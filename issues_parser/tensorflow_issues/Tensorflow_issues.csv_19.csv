Issue Number,Issue Title,Issue Body
44776,Efficient Ragged Tensor support on TFlite,"**System information**
- TensorFlow version (you are using): 2.5.0.dev20201106
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**
![image](https://user-images.githubusercontent.com/26551254/98852580-36a13f00-249b-11eb-8fb6-1c8147ed8fda.png)
While tf.RaggedTensor use int64 tensor as its row_splilts, TFLite does not support int64 indexing or slicing.
Therefore the model using RaggedTensor as input is converted like the figure above, using unnecessary flexStridedSlices.
Further, I don't understand why RaggedTensor operation should be converted into whole lot of StridedSlices.
It should be fused into single operation for efficiency.
So I argue that RaggedTensor support on TFLite should be revised.

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
The one who use RaggedTensor for their TFLite model. For example, the one who are developing natural language processing models.

**Any Other info.**
"
44775,unable to install 2.4.0rc1 from pip,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 2.4.0rc1
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip

```
ERROR: Could not find a version that satisfies the requirement tensorflow-gpu-estimator<2.5.0,>=2.4.0rc0 (from tensorflow-gpu==2.4.0rc1) (from versions: 2.1.0,2.2.0, 2.3.0)
ERROR: No matching distribution found for tensorflow-gpu-estimator<2.5.0,>=2.4.0rc0 (from tensorflow-gpu==2.4.0rc1)
```"
44774,Restored SavedModel + saved_model_cli raise exception when the object is deleted,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.8.6
- CUDA/cuDNN version: no
- GPU model and memory: cpu

**Current and expected behavior**

I'm exporting using a `tf.Module` two graphs created by decorating two method with `@tf.function`. I expect the SavedModel to be correctly exported and to not have a crash. Instead

- I guess the SavedModel is not correctly created, since in the ""serving_default"" I can find only the information of one method and I don't know how to call the other method I'm exporting.
- When I use `saved_model_cli show --all --dir at` I got an exception (see below)
- I can get the same exception if I re-load (using `tf.saved_model.load(""at"")`) the model and I delete it (the exception is when the object goes out of scope, and not when the load method is invoked).

**Standalone code to reproduce the issue**

```python
import sys

import tensorflow as tf
import tensorflow.keras as k


class ActivityTracker(tf.Module):
    def __init__(self):
        super().__init__()

        self.num_classes = 6  # activities in the training set
        self.mapping = tf.lookup.StaticHashTable(
            tf.lookup.KeyValueTensorInitializer(
                keys=tf.range(self.num_classes, dtype=tf.int32),
                values=[
                    ""Walking"",
                    ""Jogging"",
                    ""Upstairs"",
                    ""Downstairs"",
                    ""Sitting"",
                    ""Standing"",
                ],
            ),
            ""Unknown"",
        )

        self.num_features = 3  # sensor (x,y,z)
        self.batch_size = 32

        # 33,Jogging,49106062271000,5.012288,11.264028,0.95342433;
        self._model = k.Sequential(
            [
                k.layers.Input(
                    shape=(1, self.num_features), batch_size=self.batch_size
                ),
                # Note the stateful=True
                k.layers.LSTM(64, stateful=True),
                k.layers.Dense(self.num_classes),
            ]
        )

        self._global_step = tf.Variable(0, dtype=tf.int32, trainable=False)
        self._optimizer = k.optimizers.SGD(learning_rate=1e-4)
        # Sparse, so we can feed the scalar and get the one hot representation
        # From logits so we can feed the unscaled (linear activation fn)
        # directly to the loss
        self._loss = k.losses.SparseCategoricalCrossentropy(from_logits=True)

        self._last_tracked_activity = tf.Variable(-1, dtype=tf.int32, trainable=False)

    @tf.function(
        input_signature=[
            tf.TensorSpec(shape=(None, 1, 3), dtype=tf.float32),
            tf.TensorSpec(shape=(None,), dtype=tf.int32),
        ]
    )
    def learn(self, sensor_data, labels):
        # All the sensor data should be about the same activity
        tf.assert_equal(labels, tf.zeros_like(labels) + labels[0])

        # If the activity changes, we must reset the RNN state since the last update
        # and the current update are not related.

        if tf.not_equal(self._last_tracked_activity, labels[0]):
            tf.print(
                ""Resetting states. Was: "",
                self._last_tracked_activity,
                "" is "",
                labels[0],
            )
            self._last_tracked_activity.assign_sub(labels[0])
            self._model.reset_states()

        self._global_step.assign_add(1)
        with tf.GradientTape() as tape:
            loss = self._loss(labels, self._model(sensor_data))
            tf.print(self._global_step, "": loss: "", loss)

        gradient = tape.gradient(loss, self._model.trainable_variables)
        self._optimizer.apply_gradients(zip(gradient, self._model.trainable_variables))
        return loss

    @tf.function(input_signature=[tf.TensorSpec(shape=(None, 1, 3), dtype=tf.float32)])
    def predict(self, sensor_data):
        predictions = self._model(sensor_data)
        predicted = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)
        tf.print(self.mapping.lookup(predicted))
        return predicted


def main() -> int:
    at = ActivityTracker()

    # Executing an invocation of every graph we want to export is mandatory
    at.learn(
        tf.zeros((at.batch_size, 1, 3), dtype=tf.float32),
        tf.zeros((at.batch_size), dtype=tf.int32),
    )
    at.predict(tf.zeros((at.batch_size, 1, 3), dtype=tf.float32))

    tf.saved_model.save(at, ""at"")

    restored = tf.saved_model.load(""at"")
    return 0


if __name__ == ""__main__"":
    sys.exit(main())
```

**Exception and SavedModel** (that I guess is wrong)

```
saved_model_cli show --all --dir at/           
2020-11-11 18:16:54.735051: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-11-11 18:16:54.735089: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['__saved_model_init_op']:
  The given SavedModel SignatureDef contains the following input(s):
  The given SavedModel SignatureDef contains the following output(s):
    outputs['__saved_model_init_op'] tensor_info:
        dtype: DT_INVALID
        shape: unknown_rank
        name: NoOp
  Method name is: 
2020-11-11 18:16:57.323153: W tensorflow/core/common_runtime/graph_constructor.cc:808] Node 'while' has 11 outputs but the _output_shapes attribute specifies shapes for 20 outputs. Output shapes may be inaccurate.
2020-11-11 18:16:57.817962: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-11-11 18:16:57.817991: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-11-11 18:16:57.818017: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (i3): /proc/driver/nvidia/version does not exist

Defined Functions:
  Function Name: 'learn'
    Option #1
      Callable with:
        Argument #1
          sensor_data: TensorSpec(shape=(None, 1, 3), dtype=tf.float32, name='sensor_data')
        Argument #2
          labels: TensorSpec(shape=(None,), dtype=tf.int32, name='labels')

  Function Name: 'predict'
    Option #1
      Callable with:
        Argument #1
          sensor_data: TensorSpec(shape=(None, 1, 3), dtype=tf.float32, name='sensor_data')
Exception ignored in: <function CapturableResourceDeleter.__del__ at 0x7f7b9f8af700>
Traceback (most recent call last):
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py"", line 202, in __del__
    self._destroy_resource()
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 823, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 696, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2855, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3213, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3065, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 600, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/function_deserialization.py"", line 237, in restored_function_body
    return _call_concrete_function(function, inputs)
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/function_deserialization.py"", line 74, in _call_concrete_function
    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 105, in _call_flat
    return super(_WrapperFunction, self)._call_flat(args, captured_inputs,
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1938, in _call_flat
    flat_outputs = forward_function.call(ctx, args_with_tangents)
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 573, in call
    outputs = functional_ops.partitioned_call(
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/ops/functional_ops.py"", line 1192, in partitioned_call
    f.add_to_graph(graph)
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 495, in add_to_graph
    g._add_function(self)
  File ""/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 3344, in _add_function
    pywrap_tf_session.TF_GraphCopyFunction(self._c_graph, function._c_func.func,
tensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null
WARNING:tensorflow:Unresolved object in checkpoint: (root).mapping._initializer
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
```"
44773,EfficientNet loading slowly from SavedModel format,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04.5 LTS** (see colab notebook for details)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **No**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below):  **v2.3.0-0-gb36436b087 2.3.0**
- Python version: **3.6.9**
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**
When saving an instance of the EfficientNet to disk in SavedModel format and loading the same model again the loading routine takes (depending on the variation of EfficientNet) ~20 to **~120** seconds. As (please correct me if I am mistaken here) the SavedModel format is supposed to replace the frozen graphs in TensorFlow 2.0 I assume that this format is encouraged to deploy models for inference using a GPU and should be loadable quickly. It especially seems strange to me that **loading the model takes orders of magnitude longer than building it from scratch**. Please see the exact loading times for the EfficientNet architectures in [this colab notebook](https://colab.research.google.com/gist/soyers/49c40641d97e91aa3191733f123ac799/efficientnetloadingtimessavedmodel.ipynb).
As an alternative I had a look at the TensorFlow Lite format which, sadly, does not feature support for NVidia GPUs. My investigation on the loading times, however, show that EfficientNet models in TensorFlow Lite format are loaded much faster (<<< 1 second) than from SavedModel format. Please see [this colab notebook](https://colab.research.google.com/gist/soyers/f54d60badf1374349222c3ae9b7481c2/efficientnetloadingtimestflite.ipynb) for TFLite loading times.
I would appreciate any advice here as the loading times are essential in our use cases. Thank you!

**Describe the expected behavior**
The model is created at least comparably fast from SavedModel format as it takes to build it from scratch.

**Standalone code to reproduce the issue**
https://colab.research.google.com/gist/soyers/49c40641d97e91aa3191733f123ac799/efficientnetloadingtimessavedmodel.ipynb
"
44772,Missing Windows binaries,"I was wondering why `tensorflow_framework.dll` and `tensorflow_framework.lib` are missing from the released binaries for windows [here](https://www.tensorflow.org/install/lang_c). They are included for the other platforms. I'm not sure what to change to include them in a PR, but can investigate if needed. cc @alextp 
"
44770,tf.keras.preprocessing.image_dataset_from_directory ignores labels passed in 'labels' argument,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
tensorflow-gpu 2.3.0
- Python version:
3.8.3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
CUDA Version: 10.1
- GPU model and memory:
4 x Tesla P100 16GB


**Describe the current behavior**
tf.keras.preprocessing.image_dataset_from_directory creates labels based on the subdirectories despite labels being passed explicitly. Despite labels from 6 different classes being passed, only 3 are recognized (from the folder structure). The issue persists if I only have one subfolder in the directory.
**Describe the expected behavior**
The expectation is for the labels not to be ignored, and to be used instead of trying to infer them from the folder structure.
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
train = tf.keras.preprocessing.image_dataset_from_directory(
                    '/home/jupyter/JM/images', labels=list(np.uint8(np.round(np.random.random(6613)*5))),
    image_size=(1024, 1024), validation_split=0.3, subset = 'training', seed = 42, label_mode = 'categorical')

file path is irrelevant - just choose a viable file path with subdirectories and replace the 6613 with the number of files in the subdirectories

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Found 6613 files belonging to 3 classes."
44768,tf.distribute.experimental.CommunicationOptions does not work at all,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4-rc1
- Python version: 3.7

**Describe the current behavior**

The implementation of `tf.distribute.experimental.CommunicationOptions` tries to use some trickery (for unknown reason) and fails to do anything in its constructor. See https://github.com/tensorflow/tensorflow/blob/bd3a5259f5a1b07a5e899bf86a47d8a4141c2374/tensorflow/python/distribute/collective_util.py#L84 and https://github.com/tensorflow/tensorflow/blob/bd3a5259f5a1b07a5e899bf86a47d8a4141c2374/tensorflow/python/distribute/collective_util.py#L114

This makes it impossible to use the non-deprecated `tf.distribute.MultiWorkerMirroredStrategy` and specify the communication backend as that requires to create such an options instance

**Describe the expected behavior**

Creating an instance of that class initializes the members

**Standalone code to reproduce the issue**

```
import tensorflow as tf
tf.distribute.experimental.CommunicationOptions(bytes_per_pack=-1) # This should fail!
communication = tf.distribute.experimental.CommunicationImplementation.NCCL
o = tf.distribute.experimental.CommunicationOptions(implementation=communication)
assert o.implementation == communication # Error: no attribute 'implementation'
```

**Other info / logs**

IMO this is critical for the 2.4 release and the fix is rather simple: Don't have an extra Options class but simply put that code into `_OptionsExported` and let `Hints` derive from that. To not disturb other (internal) code renaming `_OptionsExported` to `Options` would be wise.
"
44767,**kwargs weights - Initialization Layer,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

When a Layer is initialized, or a subclassed layer as Dense, with 'weights' among the **kwargs, no weights are stored apparently. I believe the bug is in line 2658 in the base_layer.py file.

**Describe the expected behavior**

I would expect to have weights set as per initialization, when calling Layer.get_weights()

**Standalone code to reproduce the issue**

import tensorflow as tf

weights = tf.random.uniform(shape = [20,10]), tf.random.uniform(shape = [10,])

layer = tf.keras.layers.Layer(weights = weights)
dense = tf.keras.layers.Dense(10, weights = weights)

layer.get_weights()
dense.get_weights()

Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44766,Could not compute output tensor even when correct inputs are being passed to keras model,"I was trying to create frozen graph for the below mentioned model but when I call get_concrete_function i always get assertion error:

  **AssertionError: Could not compute output Tensor(""alpha_beta/add:0"", shape=(None, 2), dtype=float32)**
Not sure what am I missing here, I have verified the number of inputs are correct and model gives no errors while training with same inputs.
TensorFlow version- 2.3
Any help here would be much appreciated!


def get_model(features:List[F.BaseInputFeature], **kwargs) -> keras.Model:
    inputs, concats = zip(*[feature.get_input() for feature in features])

    def model_block(size, postfix):
        """"""
        Helper function for creating a dense block.
        Use a fully connected layer followed by Batch Norm and Dropout for 
        Regularization and faster training.
        """"""
        def result(input):
            model = keras.layers.Dense(size, activation=tf.nn.relu, name='inner_Dense-%s' % postfix)(input)
            model = keras.layers.BatchNormalization(name='inner_BatchNorm-%s' % postfix)(model)
            return keras.layers.Dropout(0.2, name='inner_Dropout-%s' % postfix)(model)
        return result
    print(inputs)
    output = keras.layers.Concatenate(name='inner_concat')(list(concats))
    for i, size in enumerate([256, 256, 256, 128, 128, 128, 64, 64, 64]):
        output = model_block(size, str(i))(output)

    dist = keras.layers.Dense(2, name='alpha_beta', activation=custom_activation)(output)
    print('dist-', dist)
    from keras.utils.generic_utils import get_custom_objects
    from keras.layers import Activation
    from tensorflow.python.tools import freeze_graph
    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2

    get_custom_objects().update({'custom_activation': Activation(custom_activation)})
    model = tf.keras.Model(inputs=list(inputs), outputs=[dist])


    model.compile(optimizer=keras.optimizers.Adam(lr=1e-2), loss=[loss], metrics=[metric])

    # Convert Keras model to ConcreteFunction
    full_model = tf.function(lambda x: model(x))
    print(full_model)
    full_model = full_model.get_concrete_function(
        tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))
    # Get frozen ConcreteFunction
    frozen_func = convert_variables_to_constants_v2(full_model)
    frozen_func.graph.as_graph_def()
    layers = [op.name for op in frozen_func.graph.get_operations()]
    print(""-"" * 50)
    print(""Frozen model layers: "")
    for layer in layers:
        print(layer)
    print(""-"" * 50)
    print(""Frozen model inputs: "")
    print(frozen_func.inputs)
    print(""Frozen model outputs: "")
    print(frozen_func.outputs)
    # Save frozen graph from frozen ConcreteFunction to hard drive
    tf.io.write_graph(graph_or_graph_def=frozen_func.graph,
                      logdir=""./frozen_models"",
                      name=""frozen_graph.pb"",
                      as_text=False)

    return model
"
44765,ImportError: cannot import name 'np_utils' in ImageDataGenerator,"The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#examples_2

## Description of issue (what needs changing):

### Clear description: `np_utils` is no longer available. It should be replaced with `utils`. Reproducible code is mentioned in [this Colab](https://colab.research.google.com/gist/rmothukuru/ebaf4d013cfaaba3816353cb5b8e3604/bug_fix_imagedatagen_flow.ipynb).

For example, why should someone use this method? How is it useful? : Developers using `ImageDataGenerator.flow` use this code.

### Correct links

Is the link to the source code correct? : Yes

### Parameters defined

Are all parameters defined and formatted correctly? : Yes

### Returns defined

Are return values defined? : Yes

### Raises listed and defined

Are the errors defined? No

### Usage example

Is there a usage example? : Yes

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
44763,Failed to serialize the input pipeline graph: PyFunc is stateful. [Op:DatasetToGraphV2],"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 20.04):
- TensorFlow installed from (source or binary): binary build from branch `2.3.0`
- TensorFlow version (use command below): 2.3.0
- Python version: 3.8.5
- Bazel version (if compiling from source): 3.1.0
- CUDA/cuDNN version: 11.1, 8.0.4
- GPU model and memory: Nvidia GeForce 1060 6Gb

**Describe the current behavior**
I am using a custom model that inherits from `tf.Module`. Also for some logic I am using `tf-addons==0.11.2`
Everything trains ok, without problem, however when I am using `tf.saved_model.save(custom_model, path)` I get the error below in logs section.
I found this issue after tensorflow update 2.1.0 source to 2.3.0 binary.

**Standalone code to reproduce the issue**
Below is my train code:
```
import tensorflow as tf
import tensorflow_addons as tfa

class NERTrainer(tf.Module):
    ..........
    def _helpers_(self):
        pass
    ..........
    def train(self):
        ix = 0
        for words, chars, labels, sentence_lengths in self._training_dataset:
            with tf.GradientTape() as tape:
                embeddings = self._add_embeddings_transformations(words, chars, ix)
                logits = self._add_logits(embeddings, ix)
                labels = tf.cast(labels, tf.int32)
                sentence_lengths = tf.cast(sentence_lengths, tf.int32)

                log_likelihood, transition_parameters = tfa.text.crf_log_likelihood(
                    logits, labels, sentence_lengths
                )
                log_likelihood = tf.reduce_mean(-log_likelihood)
            gradients = tape.gradient(log_likelihood, self._trainables)
            self.optimizer.apply_gradients(zip(gradients, self._trainables))
            ix += 1

        self._transition_parameters = tf.constant(transition_parameters.numpy())
        scores = self._run_evaluation(train_dataset=False)
        return {'avg_f1': np.mean(scores['f1'])}

if '__name__' == '__main__':
    nn_trainer = NERTrainer()
    num_epochs = 10
    for ix in range(num_epochs):
            print('Epoch #{}'.format(ix))
            logger.info('Epoch #{}'.format(ix))
    
            nn_trainer.train()
            nn_trainer.lr *= nn_trainer.lr_decay
            
            tf.saved_model.save(nn_trainer, target_save_path)
```

**Other info / logs** Include any logs or source code that would be helpful to
All logs while training:
```
2020-11-11 14:21:33.948277: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-11-11 14:21:33.948441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-11 14:21:33.948969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:0a:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1
coreClock: 1.8095GHz coreCount: 10 deviceMemorySize: 5,93GiB deviceMemoryBandwidth: 178,99GiB/s
2020-11-11 14:21:33.948983: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.1
2020-11-11 14:21:33.950141: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2020-11-11 14:21:33.950597: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-11-11 14:21:33.950717: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-11-11 14:21:33.951851: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.11
2020-11-11 14:21:33.952087: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2020-11-11 14:21:33.952157: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2020-11-11 14:21:33.952243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-11 14:21:33.952820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-11 14:21:33.953190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-11-11 14:21:33.953398: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-11-11 14:21:33.980819: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3792500000 Hz
2020-11-11 14:21:33.981555: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x791e0e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-11 14:21:33.981578: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-11 14:21:34.038911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-11 14:21:34.039310: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x78fa260 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-11-11 14:21:34.039330: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1060 6GB, Compute Capability 6.1
2020-11-11 14:21:34.039561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-11 14:21:34.040024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:0a:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1
coreClock: 1.8095GHz coreCount: 10 deviceMemorySize: 5,93GiB deviceMemoryBandwidth: 178,99GiB/s
2020-11-11 14:21:34.040053: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.1
2020-11-11 14:21:34.040099: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2020-11-11 14:21:34.040122: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-11-11 14:21:34.040141: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-11-11 14:21:34.040160: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.11
2020-11-11 14:21:34.040178: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2020-11-11 14:21:34.040196: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2020-11-11 14:21:34.040286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-11 14:21:34.040790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-11 14:21:34.041208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-11-11 14:21:34.041240: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.1
2020-11-11 14:21:34.268942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-11 14:21:34.268983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-11-11 14:21:34.268992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-11-11 14:21:34.269251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-11 14:21:34.269571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-11 14:21:34.269829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5006 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:0a:00.0, compute capability: 6.1)
Epoch #0
2020-11-11 14:21:34.496695: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2020-11-11 14:21:34.824964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2020-11-11 14:22:03.651363: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
Traceback (most recent call last):
  File ""ner.py"", line 358, in <module>
    tf.saved_model.save(nn_trainer, target_save_path)
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py"", line 975, in save
    _, exported_graph, object_saver, asset_info = _build_meta_graph(
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py"", line 1061, in _build_meta_graph
    _ = _SaveableView(checkpoint_graph_view)
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py"", line 216, in __init__
    function._list_all_concrete_functions_for_serialization())  # pylint: disable=protected-access
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 1030, in _list_all_concrete_functions_for_serialization
    concrete_functions = self._list_all_concrete_functions()
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 1012, in _list_all_concrete_functions
    self.get_concrete_function()
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 1167, in get_concrete_function
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 1073, in _get_concrete_function_garbage_collected
    self._initialize(args, kwargs, add_initializers_to=initializers)
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 696, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2855, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3213, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3065, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 600, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py"", line 262, in _creator
    resource = self._create_resource()
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 205, in <lambda>
    lambda: weak_self._trace_variant_creation()()),  # pylint: disable=unnecessary-lambda,protected-access
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 277, in _trace_variant_creation
    self._as_serialized_graph(external_state_policy=distribute_options
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 248, in _as_serialized_graph
    return gen_dataset_ops.dataset_to_graph_v2(
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 1122, in dataset_to_graph_v2
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/kirill/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 6843, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.FailedPreconditionError: Failed to serialize the input pipeline graph: PyFunc is stateful. [Op:DatasetToGraphV2]
```
I see this error the first time. Didn't find something similar.
Could you help me with this issue please?
"
44762,in nocopts attribute of cc_library rule @jpeg//:simd_none: This attribute was removed.,"- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.7 (19H2)
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.14
- Python version: 
python -V
Python 2.7.16
python3 -V
Python 3.7.7
- Installed using virtualenv? pip? conda?: -
- Bazel version (if compiling from source):
bazel --version
bazel 3.7.0-homebrew
- GCC/Compiler version (if compiling from source):
clang -v
Apple clang version 11.0.3 (clang-1103.0.32.29)
Target: x86_64-apple-darwin19.6.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin
- CUDA/cuDNN version: -
- GPU model and memory: -


```
git clone --single-branch --branch r1.14 git@github.com:tensorflow/tensorflow.git
cd tensorflow
bazel build tensorflow/tools/graph_transforms:summarize_graph
```

```
ERROR: /private/var/tmp/_bazel_my_user/839731be594c2770db7eceb921cc3596/external/jpeg/BUILD.bazel:488:11: in nocopts attribute of cc_library rule @jpeg//:simd_none: This attribute was removed. See https://github.com/bazelbuild/bazel/issues/8706 for details.
ERROR: /private/var/tmp/_bazel_my_user/839731be594c2770db7eceb921cc3596/external/jpeg/BUILD.bazel:488:11: in nocopts attribute of cc_library rule @jpeg//:simd_none: This attribute was removed. See https://github.com/bazelbuild/bazel/issues/8706 for details.
ERROR: Analysis of target '//tensorflow/tools/graph_transforms:summarize_graph' failed; build aborted: Analysis of target '@jpeg//:simd_none' failed
FAILED: Build did NOT complete successfully (82 packages loaded, 2745 targets configured)
```
"
44761,Could not load dynamic library 'libcuda.so.1',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3.1
- Python version: 3.7.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 10.1 cudnn7
- GPU model and memory: K80



**Describe the problem**
```
$ python -c ""import tensorflow as tf; hello = tf.constant('hello world');""
2020-11-11 10:40:12.930534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-11-11 10:40:14.016970: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2020-11-11 10:40:14.017024: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-11-11 10:40:14.017053: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (16c54693e7f2): /proc/driver/nvidia/version does not exist
2020-11-11 10:40:14.017450: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-11-11 10:40:14.041823: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300060000 Hz
2020-11-11 10:40:14.042236: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558b3bbe0d40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
```
"
44760,ERROR: /data/workspace/willy_sung/tensorflow/tensorflow/python/tools/BUILD:282:1 ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04.7 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no
- TensorFlow installed from (source or binary):source
- TensorFlow version: git checkout r2.2
- Python version:3.6.12
- Installed using virtualenv? pip? conda?:no
- Bazel version (if compiling from source):3.1.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version:V10.2.89/ 7.5.0
- GPU model and memory:GeForce GTX 1080 Ti 11GB  for 4 cards



**Describe the problem**
I want to use tensorboard on CUDA 10.2 but I found if I want tensorflow 2.2 on CUDA 10.2 with the tensorboard version
I have to build from source,so I follow the instruction from [here](https://tensorflow.google.cn/install/source)
ERROR: /data/workspace/willy_sung/tensorflow/tensorflow/python/tools/BUILD:282:1                                   
C++ compilation of rule '//tensorflow/core/kernels:sparse_tensor_dense_matmul_op_gpu' failed (Exit 1)
**Provide the exact sequence of commands / steps that you executed before running into the problem**
`git checkout r.2.2`
`./configures`

```
You have bazel 3.1.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]:


Found possible Python library paths:
  /usr/local/lib/python3.6/dist-packages
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.6/dist-packages]

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.

Found CUDA 10.2 in:
    /usr/local/cuda-10.2/targets/x86_64-linux/lib
    /usr/local/cuda-10.2/targets/x86_64-linux/include
Found cuDNN 7 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1,6.1,6.1,6.1]: 7.5


Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished

```
then 
`bazel build //tensorflow/tools/pip_package:build_pip_package`

```
external/eigen_archive/Eigen/src/Core/Ref.h(232): warning: __host__ annotation i                                  s ignored on a function(""Ref"") that is explicitly defaulted on its first declara                                  tion

external/eigen_archive/Eigen/src/Core/Ref.h(232): warning: __device__ annotation                                   is ignored on a function(""Ref"") that is explicitly defaulted on its first decla                                  ration

external/eigen_archive/Eigen/src/Core/Block.h(111): warning: __host__ annotation                                   is ignored on a function(""Block"") that is explicitly defaulted on its first dec                                  laration

external/eigen_archive/Eigen/src/Core/Block.h(111): warning: __device__ annotati                                  on is ignored on a function(""Block"") that is explicitly defaulted on its first d                                  eclaration

external/eigen_archive/Eigen/src/Core/Block.h(161): warning: __host__ annotation                                   is ignored on a function(""BlockImpl"") that is explicitly defaulted on its first                                   declaration

external/eigen_archive/Eigen/src/Core/Block.h(161): warning: __device__ annotati                                  on is ignored on a function(""BlockImpl"") that is explicitly defaulted on its fir                                  st declaration

external/eigen_archive/Eigen/src/Core/Block.h(181): warning: __host__ annotation                                   is ignored on a function(""BlockImpl_dense"") that is explicitly defaulted on its                                   first declaration

external/eigen_archive/Eigen/src/Core/Block.h(181): warning: __device__ annotati                                  on is ignored on a function(""BlockImpl_dense"") that is explicitly defaulted on i                                  ts first declaration

external/eigen_archive/Eigen/src/Core/Block.h(341): warning: __host__ annotation                                   is ignored on a function(""BlockImpl_dense"") that is explicitly defaulted on its                                   first declaration

external/eigen_archive/Eigen/src/Core/Block.h(341): warning: __device__ annotati                                  on is ignored on a function(""BlockImpl_dense"") that is explicitly defaulted on i                                  ts first declaration

external/eigen_archive/Eigen/src/Core/IndexedView.h(114): warning: __host__ anno                                  tation is ignored on a function(""IndexedView"") that is explicitly defaulted on i                                  ts first declaration

external/eigen_archive/Eigen/src/Core/IndexedView.h(114): warning: __device__ an                                  notation is ignored on a function(""IndexedView"") that is explicitly defaulted on                                   its first declaration

external/eigen_archive/Eigen/src/Core/Reshaped.h(103): warning: __host__ annotat                                  ion is ignored on a function(""Reshaped"") that is explicitly defaulted on its fir                                  st declaration

external/eigen_archive/Eigen/src/Core/Reshaped.h(103): warning: __device__ annot                                  ation is ignored on a function(""Reshaped"") that is explicitly defaulted on its f                                  irst declaration

external/eigen_archive/Eigen/src/Core/Reshaped.h(137): warning: __host__ annotat                                  ion is ignored on a function(""ReshapedImpl"") that is explicitly defaulted on its                                   first declaration

external/eigen_archive/Eigen/src/Core/Reshaped.h(137): warning: __device__ annot                                  ation is ignored on a function(""ReshapedImpl"") that is explicitly defaulted on i                                  ts first declaration

external/eigen_archive/Eigen/src/Core/Reshaped.h(155): warning: __host__ annotat                                  ion is ignored on a function(""ReshapedImpl_dense"") that is explicitly defaulted                                   on its first declaration

external/eigen_archive/Eigen/src/Core/Reshaped.h(155): warning: __device__ annot                                  ation is ignored on a function(""ReshapedImpl_dense"") that is explicitly defaulte                                  d on its first declaration

external/eigen_archive/Eigen/src/Core/Reshaped.h(215): warning: __host__ annotat                                  ion is ignored on a function(""ReshapedImpl_dense"") that is explicitly defaulted                                   on its first declaration

external/eigen_archive/Eigen/src/Core/Reshaped.h(215): warning: __device__ annot                                  ation is ignored on a function(""ReshapedImpl_dense"") that is explicitly defaulte                                  d on its first declaration

external/eigen_archive/Eigen/src/Core/Transpose.h(66): warning: __host__ annotat                                  ion is ignored on a function(""Transpose"") that is explicitly defaulted on its fi                                  rst declaration

external/eigen_archive/Eigen/src/Core/Transpose.h(66): warning: __device__ annot                                  ation is ignored on a function(""Transpose"") that is explicitly defaulted on its                                   first declaration

external/eigen_archive/Eigen/src/Core/Transpose.h(126): warning: __host__ annota                                  tion is ignored on a function(""TransposeImpl"") that is explicitly defaulted on i                                  ts first declaration

external/eigen_archive/Eigen/src/Core/Transpose.h(126): warning: __device__ anno                                  tation is ignored on a function(""TransposeImpl"") that is explicitly defaulted on                                   its first declaration

external/eigen_archive/Eigen/src/Core/Transpose.h(157): warning: __host__ annota                                  tion is ignored on a function(""TransposeImpl"") that is explicitly defaulted on i                                  ts first declaration

external/eigen_archive/Eigen/src/Core/Transpose.h(157): warning: __device__ anno                                  tation is ignored on a function(""TransposeImpl"") that is explicitly defaulted on                                   its first declaration

external/eigen_archive/Eigen/src/Core/Transpose.h(157): warning: __host__ annota                                  tion is ignored on a function(""~TransposeImpl"") that is explicitly defaulted on                                   its first declaration

external/eigen_archive/Eigen/src/Core/Transpose.h(157): warning: __device__ anno                                  tation is ignored on a function(""~TransposeImpl"") that is explicitly defaulted o                                  n its first declaration

external/eigen_archive/Eigen/src/Core/Diagonal.h(78): warning: __host__ annotati                                  on is ignored on a function(""Diagonal"") that is explicitly defaulted on its firs                                  t declaration

external/eigen_archive/Eigen/src/Core/Diagonal.h(78): warning: __device__ annota                                  tion is ignored on a function(""Diagonal"") that is explicitly defaulted on its fi                                  rst declaration

external/eigen_archive/Eigen/src/Core/TriangularMatrix.h(222): warning: __host__                                   annotation is ignored on a function(""TriangularView"") that is explicitly defaul                                  ted on its first declaration

external/eigen_archive/Eigen/src/Core/TriangularMatrix.h(222): warning: __device                                  __ annotation is ignored on a function(""TriangularView"") that is explicitly defa                                  ulted on its first declaration

external/eigen_archive/Eigen/src/Core/TriangularMatrix.h(559): warning: __host__                                   annotation is ignored on a function(""TriangularViewImpl"") that is explicitly de                                  faulted on its first declaration

external/eigen_archive/Eigen/src/Core/TriangularMatrix.h(559): warning: __device                                  __ annotation is ignored on a function(""TriangularViewImpl"") that is explicitly                                   defaulted on its first declaration

external/eigen_archive/Eigen/src/Core/TriangularMatrix.h(560): warning: __host__                                   annotation is ignored on a function(""TriangularViewImpl"") that is explicitly de                                  faulted on its first declaration

external/eigen_archive/Eigen/src/Core/TriangularMatrix.h(560): warning: __device                                  __ annotation is ignored on a function(""TriangularViewImpl"") that is explicitly                                   defaulted on its first declaration

external/eigen_archive/Eigen/src/Core/TriangularMatrix.h(560): warning: __host__                                   annotation is ignored on a function(""~TriangularViewImpl"") that is explicitly d                                  efaulted on its first declaration

external/eigen_archive/Eigen/src/Core/TriangularMatrix.h(560): warning: __device                                  __ annotation is ignored on a function(""~TriangularViewImpl"") that is explicitly                                   defaulted on its first declaration

external/eigen_archive/Eigen/src/Core/Reverse.h(90): warning: __host__ annotatio                                  n is ignored on a function(""Reverse"") that is explicitly defaulted on its first                                   declaration

external/eigen_archive/Eigen/src/Core/Reverse.h(90): warning: __device__ annotat                                  ion is ignored on a function(""Reverse"") that is explicitly defaulted on its firs                                  t declaration

external/eigen_archive/Eigen/src/Core/ArrayWrapper.h(47): warning: __host__ anno                                  tation is ignored on a function(""ArrayWrapper"") that is explicitly defaulted on                                   its first declaration

external/eigen_archive/Eigen/src/Core/ArrayWrapper.h(47): warning: __device__ an                                  notation is ignored on a function(""ArrayWrapper"") that is explicitly defaulted o                                  n its first declaration

external/eigen_archive/Eigen/src/Core/ArrayWrapper.h(145): warning: __host__ ann                                  otation is ignored on a function(""MatrixWrapper"") that is explicitly defaulted o                                  n its first declaration

external/eigen_archive/Eigen/src/Core/ArrayWrapper.h(145): warning: __device__ a                                  nnotation is ignored on a function(""MatrixWrapper"") that is explicitly defaulted                                   on its first declaration

./tensorflow/core/platform/env.h(368): warning: overloaded virtual function ""ten                                  sorflow::Env::RegisterFileSystem"" is only partially overridden in class ""tensorf                                  low::EnvWrapper""

external/com_google_absl/absl/types/optional.h(428): warning: expression has no                                   effect
          detected during instantiation of ""const T &absl::lts_2020_02_25::optio                                  nal<T>::operator*() const & [with T=stream_executor::dnn::AlgorithmDesc]""
./tensorflow/stream_executor/dnn.h(804): here

external/com_google_absl/absl/types/optional.h(428): warning: expression has no                                   effect
          detected during instantiation of ""const T &absl::lts_2020_02_25::optio                                  nal<T>::operator*() const & [with T=size_t]""
./tensorflow/stream_executor/dnn.h(858): here

external/com_google_absl/absl/time/internal/cctz/include/cctz/civil_time_detail.                                  h: In function 'constexpr absl::lts_2020_02_25::time_internal::cctz::detail::civ                                  il_day absl::lts_2020_02_25::time_internal::cctz::detail::next_weekday(absl::lts                                  _2020_02_25::time_internal::cctz::detail::civil_day, absl::lts_2020_02_25::time_                                  internal::cctz::detail::weekday)':
external/com_google_absl/absl/time/internal/cctz/include/cctz/civil_time_detail.                                  h:567:20: error: call to non-constexpr function 'absl::lts_2020_02_25::time_inte                                  rnal::cctz::detail::civil_time<absl::lts_2020_02_25::time_internal::cctz::detail                                  ::day_tag> absl::lts_2020_02_25::time_internal::cctz::detail::operator+(absl::lt                                  s_2020_02_25::time_internal::cctz::detail::civil_time<absl::lts_2020_02_25::time                                  _internal::cctz::detail::day_tag>, absl::lts_2020_02_25::time_internal::cctz::di                                  ff_t)'
           return cd + (j - i);
                    ^
external/com_google_absl/absl/time/internal/cctz/include/cctz/civil_time_detail.                                  h: In function 'constexpr absl::lts_2020_02_25::time_internal::cctz::detail::civ                                  il_day absl::lts_2020_02_25::time_internal::cctz::detail::prev_weekday(absl::lts                                  _2020_02_25::time_internal::cctz::detail::civil_day, absl::lts_2020_02_25::time_                                  internal::cctz::detail::weekday)':
external/com_google_absl/absl/time/internal/cctz/include/cctz/civil_time_detail.                                  h:587:20: error: call to non-constexpr function 'absl::lts_2020_02_25::time_inte                                  rnal::cctz::detail::civil_time<absl::lts_2020_02_25::time_internal::cctz::detail                                  ::day_tag> absl::lts_2020_02_25::time_internal::cctz::detail::operator-(absl::lt                                  s_2020_02_25::time_internal::cctz::detail::civil_time<absl::lts_2020_02_25::time                                  _internal::cctz::detail::day_tag>, absl::lts_2020_02_25::time_internal::cctz::di                                  ff_t)'
           return cd - (j - i);
                    ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /data/workspace/willy_sung/tensorflow/tensorflow/python/tools/BUILD:282:1                                   C++ compilation of rule '//tensorflow/core/kernels:sparse_tensor_dense_matmul_o                                  p_gpu' failed (Exit 1)
INFO: Elapsed time: 804.121s, Critical Path: 101.69s
INFO: 5406 processes: 5406 local.
FAILED: Build did NOT complete successfully
```


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44759,tf.nn.depth_to_space is not support,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10 enterprise
- TensorFlow installed from (source or binary): binary (pip install tensorflow==1.14)
- TensorFlow version (or github SHA if from source): 1.14


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, BATCH_TO_SPACE_ND, CONV_2D, DEPTHWISE_CONV_2D, SPACE_TO_BATCH_ND, SPACE_TO_DEPTH. Here is a list of operators for which you will need custom implementations: DEPTH_TO_SPACE.
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

https://gist.github.com/goldflower/dec52a344de461413ae10de84a555b44


**Any other info / logs**

The code above create the pb file and reproduce the issue.

https://www.tensorflow.org/mlir/tfl_ops

Here says tfl.depth_to_space (TFL::DepthToSpaceOp) can be support but actually not?

"
44758,tf.random.log_uniform_candidate_sampler gives undesired true class,"From tensorflow's word2vec tutorial: https://www.tensorflow.org/tutorials/text/word2vec#define_loss_function_and_compile_model

Nagative sampling is conducted using `tf.random.log_uniform_candidate_sampler`. Given the context class (true class), the goal is to sample negative classes from the whole vocabulary list. To my understanding, The negative classes must differ from the given context class. However, I found that the context class may appear in the negative classes sampled by tf.random.log_uniform_candidate_sampler. Here is the code:

```
import tensorflow as tf
SEED = 42 

# encode the words
sentence = ""The wide road shimmered in the hot sun""
tokens = list(sentence.lower().split())
vocab, index = {}, 1 # start indexing from 1
vocab['<pad>'] = 0 # add a padding token 
for token in tokens:
  if token not in vocab: 
    vocab[token] = index
    index += 1
vocab_size = len(vocab)
print(vocab)
inverse_vocab = {index: token for token, index in vocab.items()}
print(inverse_vocab)


# make (hot, the) as a context pair
target_word, context_word = 6, 1
print(""target: {}, context: {}"".format(inverse_vocab[target_word], inverse_vocab[context_word]))


# negative sampling
# Set the number of negative samples per positive context. 
num_ns = 4

context_class = tf.reshape(tf.constant(context_word, dtype=""int64""), (1, 1))
negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(
    true_classes=context_class, # class that should be sampled as 'positive'
    num_true=1, # each positive skip-gram has 1 positive context class
    num_sampled=num_ns, # number of negative context words to sample
    unique=True, # all the negative samples should be unique
    range_max=vocab_size, # pick index of the samples from [0, vocab_size]
    seed=SEED, # seed for reproducibility
    name=""negative_sampling"" # name of this operation
)
print(""negative samples\' index: "", negative_sampling_candidates.numpy())
print(""negetive samples: "", [inverse_vocab[index.numpy()] for index in negative_sampling_candidates])
# ""the"" will show in negative samples, if not, run it several times.
```
The word ""the"" is the context class of word ""hot"", why it could show in the sampled negative classes? Moreover, the target word hot could also be sampled as negative class. Do I misunderstand something?"
44756,Memory leak with keras metrics,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-nightly
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**

The following test is failing, as the metric is not properly garbage collected.

```
import tensorflow as tf
from tensorflow.python.framework.test_util import assert_no_garbage_created

class TestMetric(tf.test.TestCase):
  @assert_no_garbage_created
  def test_metric(self):
    metric = tf.keras.metrics.Mean('test', dtype=tf.float32)

import unittest
unittest.main(argv=['first-arg-is-ignored'], exit=False)
```

**Describe the expected behavior**

Test should pass

**Standalone code to reproduce the issue**
See above.

**Other info / logs** "
44755,Cannot execute ./configure,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Not Applicable
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20 window subsystem.
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not Applicable
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master branch
- Python version: 3.8.2
- Bazel version (if compiling from source): Not Applicable
- GCC/Compiler version (if compiling from source): Not Applicable
- CUDA/cuDNN version: Not Applicable
- GPU model and memory: Not Applicable


**Describe the current behavior**

./configure does not execute

**Describe the expected behavior**

./configure should execute

**Standalone code to reproduce the issue**

Please clone and do `./configure` on ubuntu on window subsystem. It will not work!

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
./configure: line 2: $'\r': command not found  : invalid optione 3: set: -                                                                                             set: usage: set [-abefhkmnptuvxBCHP] [-o option-name] [--] [arg ...]   : invalid option nameset: pipefail                                                                                      ./configure: line 5: $'\r': command not found
./configure: line 16: syntax error: unexpected end of file
```

"
44754,Java API load model core dumped,"**System information**
- I written custom code:
- OS Platform and Distribution : Linux Ubuntu 18.04.5
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.3.1
- Python version: 3.8; Java version: 1.8.0_271
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I tried to load both tf forzen and saved model with .pb files in IDEA 2017.2 with  Java 8 (Linux x64 1.8.0_271) by using JAVA API , but got a error message showing the failure.
However, it works well on other machines and only mine fails.

**Describe the expected behavior**
I would like to load tf frozen and saved models with .pb file with Java API on my remote server.

**Standalone code to reproduce the issue**
```
import org.apache.commons.io.IOUtils;
import org.tensorflow.Graph;
import org.tensorflow.Session;
import org.tensorflow.Tensor;
import java.util.Random;

import java.io.FileInputStream;
import java.io.IOException;
import java.nio.FloatBuffer;

public class TFTest {

    public static void main(String[] args) throws IOException {

        String path = ""/path/to/resnet50/frozen_inference_graph.pb"";
        try (Graph graph = new Graph()) {
            byte[] graphBytes = IOUtils.toByteArray(new
                    FileInputStream(path));
            graph.importGraphDef(graphBytes);

            try(Session session = new Session(graph)){
                float[][] input = new float[1][1];
                input[0] = new float[]{10.0f};
          
                Tensor z = session.runner()
                        .feed(""x"", Tensor.create(input))
                        .fetch(""y"").run().get(0);
                float[][] zz = (float[][]) z.copyTo(new float[1][1]);
                System.out.println(""y = "" + zz[0][0]);
            }
        }
    }
}
```

**Other info / logs** 
```
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGILL (0x4) at pc=0x00007f3fe5154da9, pid=19433, tid=0x00007f402d3d3700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_271-b09) (build 1.8.0_271-b09)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.271-b09 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libtensorflow_framework.so.1+0x744da9]  _GLOBAL__sub_I_loader.cc+0x99
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#
# An error report file with more information is saved as:
# /home/qihong/TEST_TF/hs_err_pid19433.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
```
"
44753,Issue working with Nvidia rtx-3090,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.4.0-rc1
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: Cuda-11.1 cuDNN-8.05
- GPU model and memory: Nvidia rtx-3090 (24GB memory)



**Describe the problem**

when listing the GPU devices using **tf.congif.list_physical_devices('GPU')**, It states that '**Could not load dynamic library 'libcusolver.so.10**''.
I've got libcusolver.so.11 in the path where other successfully loaded files are located and tensorflow is searching for libcusolver.so.10 _(i've also copied the file and renamed it to libcusolver.so.10 to try my luck but, still facing the same issue)_

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`>>> tf.config.list_physical_devices('GPU')
2020-11-11 11:11:23.937512: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-11-11 11:11:23.937695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-11 11:11:23.938928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:2d:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2020-11-11 11:11:23.938959: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2020-11-11 11:11:23.938983: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2020-11-11 11:11:23.938996: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2020-11-11 11:11:23.939009: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2020-11-11 11:11:23.939022: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2020-11-11 11:11:23.939098: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2020-11-11 11:11:23.939114: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2020-11-11 11:11:23.939127: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2020-11-11 11:11:23.939136: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[]
`


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44752,Loading images with non-inferred labels,"**System information**
- OS: macOs Catalina
- Device: MacBook Pro 2017
- GPU: Radeon Pro 555 2 GB & Intel HD Graphics 630 1536 MB
- Memory: 16 GB 2133 MHz LPDDR3
- Python Version: 3.7.9

`tf.version.GIT_VERSION: v2.3.0-54-gfcc4b966f1`
`tf.version.VERSION: 2.3.1`

**Current behavior**
`image_dataset_from_directory()` seemingly ignores `labels` argument

**Expected behavior**
When running `train_ds = image_dataset_from_directory(my_dir, labels=my_label_list)` with `my_label_list` containing integer labels with the exact number of elements as the number of images in `my_dir` (and in alphanumerically correct order with relation to the image filenames) I would expect `train_ds.class_names` to consist of the integer labels passed.

From the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) I would expect the directory structure to be ignored since I have supplied the `labels` parameter. However, images are only loaded when I nest them in a sub-directory (in this example named `images/`). This sub-directory is then assigned as the sole class label.

```python
>>> print(train_ds.class_names)
['images']
```

**Standalone code to reproduce the issue**

Some sample images [here](https://drive.google.com/file/d/1msGXyy-8aDyeqPmsRCfeWiS3iAbzLLMm/view?usp=sharing) and follow this directory structure:
```
test_dir/
images/
```
Then run the following example:
```python
image_dir = '/foo/bar/test_dir/'
label_list = [1,2,3,1,1]
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    image_dir,
    labels=label_list,
    validation_split=0.2,
    subset='training',
    shuffle=False,
)
print(train_ds.class_names)
```"
44751,TensorFlow support for Apple Silicon (M1 Chips),"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version: 2.1+ (I don't know specifics)
- Are you willing to contribute it (Yes/No): No, not enough repository knowledge.

**Describe the feature and the current behavior/state.**
This is not a feature per se, but a question. How soon would TensorFlow be available for the Apple Silicon macs announced today with the M1 chips?

**Will this change the current api? How?**
Hmmm, I don't know. Probably not.

**Who will benefit with this feature?**
This will benefit everybody who buys an apple silicon mac right now, and in the future. This is a big change, and no one would buy intel macs after a few years.

**Any Other info.**
I know python and numpy are being personally helped by apple for this transition."
44750,Add support for CUDA 11.1 on Windows 10 for the 8.6 compute capability,"**System information**
- System: Windows 10
- TensorFlow version (you are using): 2.5.0.dev20201108
- Are you willing to contribute it (Yes/No): Yes. I can do the testing of the new build and provide any additional information.

- TensorFlow version (you are using): 2.5.0.dev20201108
- Python version: 3.8.6-amd64
- Compiler: MSVC 2019
- cuDNN: 8.0.4.30
- CUDA: 11.0.3_451.82
- ptxas: from CUDA 11.1
- NVIDIA Drivers: 456.71

Hello TensorFlow team. I recently got working on my RTX 3090 on the Windows 10 machine and decided to share my investigations with you.

Right now I got TensorFlow 2.5.0.dev20201108 work with CUDA 11.0 and ptxas.exe (PTX compiler) from CUDA 11.1, because ptxas from 11.1 supports 8.6 compute capability and on 11.0 - I'm getting error `ptxas fatal : Value 'sm_86' is not defined for option 'gpu-name'` - which is mean that 11.0 doesn't support 8.6 yet (at least I think so).

So, I don't know if it's planned, but would be great to add support of the CUDA 11.1 for the Win build (I don't know if people having the same issue on the Linux). Because my current solution is kind of hacky (using the compiler from a different version) and even though I don't see any errors right now, it could potentially cause some in the future.

Here is also detailed info on my finding: [https://dobromyslova.medium.com/making-work-tensorflow-with-nvidia-rtx-3090-on-windows-10-7a38e8e582bf](https://dobromyslova.medium.com/making-work-tensorflow-with-nvidia-rtx-3090-on-windows-10-7a38e8e582bf)

Please feel free to contact me for any additional information."
44749,run out of heap space when compiling kernel bias_op.cc,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3.1
- Python version: 2.7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 3.7.0
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA / 16GB

**Describe the problem**
### compile bias_op.cc
### fatal error C1002: compiler is out of heap space in pass 2
### how to adjust the compiler options?
### please help!
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44748,tf.eye is OOM when num_rows is over 10**5,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.0
- On google colab free GPU
- Are you willing to contribute it (Yes/No): I want to try.


**Describe the feature and the current behavior/state.**
```
element_count = 10**5
eye_tf = tf.eye(element_count, dtype=tf.float32)
```
This code will be OOM. The error is here.

```
ResourceExhaustedError: OOM when allocating tensor with shape[100000,100000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MatrixDiagV3] name: diag
```


**Will this change the current api? How?**
No. 

**Who will benefit with this feature?**
I want to calculate huge array (the num_rows is over 10**8) using tesorflow parallel processing.
This change extend the possibility of tensorflow calculation.

**Any Other info.**
"
44745,Move keras.efficientnet preprocessing layers to preprocess_input function,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.1
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Current [preprocessing layers](https://github.com/tensorflow/tensorflow/blob/fcc4b966f1265f466e82617020af93670141b009/tensorflow/python/keras/applications/efficientnet.py#L316-L317) are a part of the main model. They are also not noted in the [TF docs](https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB0). 

I would propose moving these layers to the efficientnet preprocess_input function which currently does nothing. This would be consistent with the behavior of the other ready-made keras models.

**Will this change the current api? How?**
Yes. It would move preprocessing layers into a separate function. Unfortunately this would likely lead to unexpected behavior but not errors for people currently using the built-in preprocessing layers.

**Who will benefit with this feature?**
New users of the EfficientNet ready-made models. It would make the api consistent with other ready-made models. It would also separate preprocessing for edge case applications.

**Any Other info.**
"
44743,TensorflowLite  LSTM data input format example,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):
Android, Tensorflow 2.3

**Command used to run the converter or code if youre using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.
[https://github.com/grewe/UCross/blob/main/Training/LSTM_CrosswalkNavigation_Training.ipynb](https://github.com/grewe/UCross/blob/main/Training/LSTM_CrosswalkNavigation_Training.ipynb)

```
Training performed using notebook at https://github.com/grewe/UCross/blob/main/Training/LSTM_CrosswalkNavigation_Training.ipynb

This has a CNN feature extractor that runs on a sequence of 40 images captured producing 40 Feature Vectors.   These are fed into an LSTM model and trained for a video activity recognition (directing users across crosswalk) application

```

**The output from the converter invocation**

```
I understand will create 2 TFLite models (one for Feature Extractor CNN and the other for the LSTM model).  My question is how do I repackage the 40 Feature Vectors in my sequence as input into the TFLite model on Android.   I would like to see some examples showing this for Android (as on Android do not have access to full Tensorflow api).
```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44739,Wrong path separator in error message when saved model does not exist (in keras.models.load_model),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home Single Language (build 19042.610)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.8.6
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce RTX 2070 SUPER 8 GB

**Current behavior**
When I use function `keras.models.load_model` with incorrect path, I catch IOError:
`OSError: SavedModel file does not exist at: C:\IncorrectPathToSavedModel/{saved_model.pbtxt|saved_model.pb}`
Because I use Windows, path separator ""/"" is incorrect,  and when I view code of `parse_saved_model` in ""\tensorflow\python\saved_model\loader_impl.py"" I see hardcoded symbol in error text definition.

**Expected behavior**
Path separator should be recived from `os.path.sep`, for example in Windows this error message must be:
`OSError: SavedModel file does not exist at: C:\IncorrectPathToSavedModel\{saved_model.pbtxt|saved_model.pb}`
in Linux:
`OSError: SavedModel file does not exist at: ~/IncorrectPathToSavedModel/{saved_model.pbtxt|saved_model.pb}`

**Standalone code to reproduce the issue**
```
from tensorflow import keras
model_dir = 'C:\\IncorrectPathToSavedModel'
model = keras.models.load_model(model_dir)
```"
44737,Discussion of Apollo3 TFLu PRs,"@tensorflow/micro

Hi @advaitjain, or whoever it may concern.

Thank you for the [explanation closing my last PR](https://github.com/tensorflow/tensorflow/pull/43831). I'd like to restructure and improve those features so that they can be accepted into the project. From what I understand there are two main concerns:

* keeping PRs focused to one issue/feature each
* trying to deduplicate code for examples

These should be fairly easy to do. I propose three PRs:

1. Fix existing issues with Apollo3 EVB performance
2. Update underlying AmbiqSuite SDK version to 2.5.1
3. Add targets for additional SFE Apollo3 based boards 

In the last PR I initiated deduplication efforts for the additional boards. Generally it went well but I did have issues with one target in particular. I may reach out for some guidance on that when the time comes. 

Please, let me know if this approach is acceptable.

Thank you,
oclyke

"
44736,Validation step overwrites callback's internal predict call,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.8.5
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce RTX 2080 Ti 11GB x8

I'm training a network, and I'm trying to collect metrics on two separate validation sets. I've been using the built-in validation for one of them, and I wrote a callback for the other. However, in epochs when both run, the callback seems to always get the same answer as the built-in validation set.

I have a minimal test case below. This is a completely trivial network where the output and input are equal. I've built the situation as follows:
* Let x = np.ones(5,).
* It trains on input = output = x, so the training error is always 0.
* It validates on input = x, output = 2*x, so the validation error is always 1.
* The callback validates on input = x, output = 3*x, so the callback's validation error should always be 2.
```
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model

input_data = Input(shape=(5,))
model = Model(inputs=input_data, outputs=input_data)
model.compile(loss='mae')

class valCallback(tf.keras.callbacks.Callback):
    def __init__(self, model, inputs, outputs):
        self.model = model
        self.inputs = inputs
        self.outputs = outputs
        
    def on_epoch_end(self, epoch, logs={}):
        val = self.model.evaluate(self.inputs, self.outputs, verbose=0)
        print(""\nVAL: "", val)

traindata = np.ones((5))
vc = valCallback(model, traindata, 3*traindata)  
model.fit(traindata, traindata, 
          validation_data=(traindata, 2*traindata),
          epochs=4, callbacks=[vc], validation_freq=2, verbose=0)
```

In epoch 1, the callback correctly prints 2. In epoch 2, when the validation runs, the callback prints 1 instead (!).

Somehow the validation_data is overwriting the data in the callback, and I don't know how or why. The self.model.evaluate calls in the callback aren't getting the right answer any more, after validation has happened.

Desired output:
VAL: 2
VAL: 2
VAL: 2
VAL: 2

Actual output:
VAL: 2
VAL: 1
VAL: 1
VAL: 1"
44734,Dependencies for BERT-QA android not clear,"**System information**
- OS Platform and Distribution - (Linux Ubuntu 16.04)

Gradle and android SDK version not specified in the README of Android bert-qa for Question Answering
https://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/android

It would be helpful if you could add those details.
"
44733,AttributeError: module 'tensorflow.compat.v2' has no attribute 'app',"Hi,
First of all, I'm new to github and Iwasn't sure where to post this so sorry if it's in the wrong place. I'm trying to set up an object detector for my master's but I keep on running into issues.

**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow version: 2.3.0
- CUDA: 10.1
- cuDNN:  7.6.5



I've tried many many different things but I keep on running into issues that prevent me from training a model, ""AttributeError: module 'tensorflow.compat.v2' has no attribute 'app'"" is the latest
I have followed the guide which can be found on this github page: 
https://github.com/TannerGilbert/Tensorflow-Object-Detection-API-Train-Model
I have followed this pretty much to the letter aside from the fact that I didn't download the master branch but went with r2.3.0 instead to ensure compatibility.
I ran the following command in anaconda to try and train the model:
`python model_main_tf2.py --pipeline_config_path=training/ssd_efficientdet_d0_512x512_coco17_tpu-8.config --model_dir=training --alsologtostderr`

This gave me the following error:
![image](https://user-images.githubusercontent.com/74254802/98692645-a31f1f80-236f-11eb-8b7a-d74f0b876243.png)

This is the code of the model_main_tf2.py file:
```
from absl import flags
import tensorflow.compat.v2 as tf
from object_detection import model_hparams
from object_detection import model_lib_v2

flags.DEFINE_string('pipeline_config_path', None, 'Path to pipeline config '
                    'file.')
flags.DEFINE_integer('num_train_steps', None, 'Number of train steps.')
flags.DEFINE_bool('eval_on_train_data', False, 'Enable evaluating on train '
                  'data (only supported in distributed training).')
flags.DEFINE_integer('sample_1_of_n_eval_examples', None, 'Will sample one of '
                     'every n eval input examples, where n is provided.')
flags.DEFINE_integer('sample_1_of_n_eval_on_train_examples', 5, 'Will sample '
                     'one of every n train input examples for evaluation, '
                     'where n is provided. This is only used if '
                     '`eval_training_data` is True.')
flags.DEFINE_string(
    'hparams_overrides', None, 'Hyperparameter overrides, '
    'represented as a string containing comma-separated '
    'hparam_name=value pairs.')
flags.DEFINE_string(
    'model_dir', None, 'Path to output model directory '
                       'where event and checkpoint files will be written.')
flags.DEFINE_string(
    'checkpoint_dir', None, 'Path to directory holding a checkpoint.  If '
    '`checkpoint_dir` is provided, this binary operates in eval-only mode, '
    'writing resulting metrics to `model_dir`.')

flags.DEFINE_integer('eval_timeout', 3600, 'Number of seconds to wait for an'
                     'evaluation checkpoint before exiting.')
flags.DEFINE_integer(
    'num_workers', 1, 'When num_workers > 1, training uses '
    'MultiWorkerMirroredStrategy. When num_workers = 1 it uses '
    'MirroredStrategy.')

FLAGS = flags.FLAGS


def main(unused_argv):
  flags.mark_flag_as_required('model_dir')
  flags.mark_flag_as_required('pipeline_config_path')
  tf.config.set_soft_device_placement(True)

  if FLAGS.checkpoint_dir:
    model_lib_v2.eval_continuously(
        hparams=model_hparams.create_hparams(FLAGS.hparams_overrides),
        pipeline_config_path=FLAGS.pipeline_config_path,
        model_dir=FLAGS.model_dir,
        train_steps=FLAGS.num_train_steps,
        sample_1_of_n_eval_examples=FLAGS.sample_1_of_n_eval_examples,
        sample_1_of_n_eval_on_train_examples=(
            FLAGS.sample_1_of_n_eval_on_train_examples),
        checkpoint_dir=FLAGS.checkpoint_dir,
        wait_interval=300, timeout=FLAGS.eval_timeout)
  else:
    if tf.config.get_visible_devices('TPU'):
      resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
      tf.config.experimental_connect_to_cluster(resolver)
      tf.tpu.experimental.initialize_tpu_system(resolver)
      strategy = tf.distribute.experimental.TPUStrategy(resolver)
    elif FLAGS.num_workers > 1:
      strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
    else:
      strategy = tf.compat.v2.distribute.MirroredStrategy()

    with strategy.scope():
      model_lib_v2.train_loop(
          hparams=model_hparams.create_hparams(FLAGS.hparams_overrides),
          pipeline_config_path=FLAGS.pipeline_config_path,
          model_dir=FLAGS.model_dir,
          train_steps=FLAGS.num_train_steps,
          use_tpu=FLAGS.use_tpu)

if __name__ == '__main__':
  tf.app.run()
```


I did a quick search and I found that tf.app has been removed from TF2, yet it is still used in TF2 files?
Is there an easy workaround to this? I can't seem to get it to work, but that might just be because I'm not exactly a good programmer.

Thanks.

"
44730,Bug/Issue tf.data.Dataset for Keras multi-input model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.7
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.1
- Python version: 3.7

**Describe the current behavior**
I built a multi-input model with the Keras functional API. The idea is to classify a text and its metadata. The model works fine with NumPy format inputs but fails with a tf.data.Dataset. I'm not sure how to interpret it as both inputs should be equivalent. Thanks in advance for any guidance. I attached below a dummy equivalent of my project.

**Standalone code to reproduce the issue**
**Model:**

```
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras import Input, Model, layers
from transformers import DistilBertTokenizer, TFDistilBertModel


MAX_LEN = 20

STRING_CATEGORICAL_COLUMNS = [
    ""Organization"",
    ""Sector"",
    ""Content_type"",
    ""Geography"",
    ""Themes"",
]

VOCAB = {
    ""Organization"": [""BNS"", ""FED"", ""ECB""],
    ""Sector"": [""BANK"", ""ASS"", ""MARKET""],
    ""Content_type"": [""LAW"", ""NOTES"", ""PAPER""],
    ""Geography"": [""UK"", ""FR"", ""DE"", ""CH"", ""US"", ""ES"", ""NA""],
    ""Themes"": [""A"", ""B"", ""C"", ""D"", ""E"", ""F"", ""G""],
}

DIM = {
    ""Organization"": 7,
    ""Sector"": 2,
    ""Content_type"": 3,
    ""Geography"": 4,
    ""Themes"": 5,
}


# BERT branch
tf_model = TFDistilBertModel.from_pretrained(""distilbert-base-uncased"", name=""tfbert"")

input_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=""input_ids"")
attention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=""attention_mask"")


embedding = tf_model(input_ids, attention_mask=attention_mask)[0][:, 0]

bert_input = {""input_ids"": input_ids, ""attention_mask"": attention_mask}
model_bert = Model(inputs=[bert_input], outputs=[embedding])


# meta branch
meta_inputs = {}
meta_prepocs = []

for key in VOCAB:
    inputs = Input(shape=(None,), dtype=tf.string, name=key)
    meta_inputs[key] = inputs

    vocab_list = VOCAB[key]
    vocab_size = len(vocab_list)
    embed_dim = DIM[key]

    x = layers.experimental.preprocessing.StringLookup(
        vocabulary=vocab_list, num_oov_indices=1, mask_token=""PAD"", name=""lookup_"" + key
    )(inputs)

    x = layers.Embedding(
        input_dim=vocab_size + 2,  # 2 = PAD + NA
        output_dim=embed_dim,
        mask_zero=True,
        name=""embedding_"" + key,
    )(x)

    x = layers.GlobalAveragePooling1D(
        data_format=""channels_last"", name=""poolembedding_"" + key
    )(x)

    meta_prepocs.append(x)

meta_output = layers.concatenate(meta_prepocs, name=""concatenate_meta"")
model_meta = Model(meta_inputs, meta_output)


# combining branches
combined = layers.concatenate(
    [model_bert.output, model_meta.output], name=""concatenate_all""
)
ouput = layers.Dense(128, activation=""relu"", name=""dense"")(combined)
ouput = layers.Dense(4, name=""class_output"")(ouput)
model = Model(inputs=[model_bert.input, model_meta.input], outputs=ouput)

model.compile(
    optimizer=keras.optimizers.RMSprop(1e-3),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
)
```

**Dataset**
A dummy dataset of 5 texts and respective metadata

```
# input meta
dict_meta = {
    ""Organization"": [
        [""BNS"", ""NA""],
        [""ECB"", ""PAD""],
        [""NA"", ""PAD""],
        [""NA"", ""PAD""],
        [""NA"", ""PAD""],
    ],
    ""Sector"": [
        [""BANK"", ""PAD"", ""PAD""],
        [""ASS"", ""PAD"", ""NA""],
        [""MARKET"", ""NA"", ""NA""],
        [""NA"", ""PAD"", ""NA""],
        [""NA"", ""PAD"", ""NA""],
    ],
    ""Content_type"": [
        [""NOTES"", ""PAD""],
        [""PAPER"", ""UNK""],
        [""LAW"", ""PAD""],
        [""LAW"", ""PAD""],
        [""LAW"", ""NOTES""],
    ],
    ""Geography"": [
        [""UK"", ""FR""],
        [""DE"", ""CH""],
        [""US"", ""ES""],
        [""ES"", ""PAD""],
        [""NA"", ""PAD""],
    ],
    ""Themes"": [[""A"", ""B""], [""B"", ""C""], [""C"", ""PAD""], [""C"", ""PAD""], [""G"", ""PAD""]],
}

# input text
list_text = [
    ""Trump in denial over election defeat as Biden gears up to fight Covid"",
    ""Feds seize $1 billion in bitcoins they say were stolen from Silk Road"",
    ""Kevin de Bruyne misses penalty as Manchester City and Liverpool draw"",
    ""United States nears 10 million coronavirus cases"",
    ""Fiji resort offers the ultimate in social distancing"",
]

tokenizer = DistilBertTokenizer.from_pretrained(""distilbert-base-uncased"")
params = {
    ""max_length"": MAX_LEN,
    ""padding"": ""max_length"",
    ""truncation"": True,
}
tokenized = tokenizer(list_text, **params)
dict_text = tokenized.data

#input label
label = [[1], [0], [1], [0], [1]]
```

**Training with NumPy format**

```
ds_meta = tf.data.Dataset.from_tensor_slices((dict_meta))
ds_meta = ds_meta.batch(5)
example_meta = next(iter(ds_meta))

ds_text = tf.data.Dataset.from_tensor_slices((dict_text))
ds_text = ds_text.batch(5)
example_text = next(iter(ds_text))

ds_label = tf.data.Dataset.from_tensor_slices((label))
ds_label = ds_label.batch(5)
example_label = next(iter(ds_label))

model.fit([example_text, example_meta], example_label)
```

```
1/1 [==============================] - 0s 1ms/step - loss: 2.4866
```

**Training with tf.data.Dataset**

```
ds = tf.data.Dataset.from_tensor_slices(
    (
        {
            ""attention_mask"": dict_text[""attention_mask""],
            ""input_ids"": dict_text[""input_ids""],
            ""Content_type"": dict_meta[""Organization""],
            ""Geography"": dict_meta[""Geography""],
            ""Organization"": dict_meta[""Organization""],
            ""Sector"": dict_meta[""Sector""],
            ""Themes"": dict_meta[""Themes""],
        },
        {""class_output"": label},
    )
)


ds = ds.batch(5)
model.fit(ds, epochs=1)
```

**Other info / logs** 
```
2020-11-10 14:52:47.502445: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at cast_op.cc:124 : Unimplemented: Cast string to int32 is not supported
Traceback (most recent call last):

  File ""<ipython-input-10-a894466398cd>"", line 1, in <module>
    model.fit(ds, epochs=1)

  File ""/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)

  File ""/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1098, in fit
    tmp_logs = train_function(iterator)

  File ""/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)

  File ""/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 807, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable

  File ""/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access

  File ""/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)

  File ""/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))

  File ""/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 550, in call
    ctx=ctx)

  File ""/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)

UnimplementedError:  Cast string to int32 is not supported
	 [[node functional_5/Cast (defined at <ipython-input-3-8e2b230c1da3>:17) ]] [Op:__inference_train_function_24120]

Function call stack:
train_function
```"
44728,Keras/Tensorflow train_on_batch memory leak,"Please do help

train_on_batch has a memory leak in my and many other people's code. It was thought to only be on predict and fit, which was solved with a del and gc.collect(), but train_on_batch cannot be solved by this. Please someone help!

"
44727,Running tf from docker with --read-only mode,"- TensorFlow version: 2.2.0
I'd love to run my app with docker run --read-only option, but startup fails when tf runs tests that try to create a temporary folder.
```
  import tensorflow as tf  # noqa  
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py"", line 41, in <module>  
  from tensorflow.python.tools import module_util as _module_util  
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py"", line 132, in <module>  
  from tensorflow.python.platform import test  
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/test.py"", line 24, in <module>  
  from tensorflow.python.framework import test_util as _test_util  
File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/test_util.py"", line 36, in <module>  
  from absl.testing import parameterized  
File ""/usr/local/lib/python3.6/dist-packages/absl/testing/parameterized.py"", line 186, in <module>  
  from absl.testing import absltest  
File ""/usr/local/lib/python3.6/dist-packages/absl/testing/absltest.py"", line 244, in <module>  
  flags.DEFINE_string('test_tmpdir', get_default_test_tmpdir(),  
File ""/usr/local/lib/python3.6/dist-packages/absl/testing/absltest.py"", line 180, in get_default_test_tmpdir  
  tmpdir = os.path.join(tempfile.gettempdir(), 'absl_testing')  
File ""/usr/lib/python3.6/tempfile.py"", line 437, in gettempdir  
  tempdir = _get_default_tempdir()  
File ""/usr/lib/python3.6/tempfile.py"", line 372, in _get_default_tempdir  
  dirlist)  
FileNotFoundError: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', `'/opt/proj']  
```
I wonder if it's possible to turn off these tests?
Provide param to turn off tests on starting up?
Those who are willing to run tf from docker in a read-only mode which is safer will benefit from this possibility.
If it's already possible please share how. 
"
44725,module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model',"# Convert the model to the TensorFlow Lite format without quantization
converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_TF)




AttributeError                            Traceback (most recent call last)
<ipython-input-57-448f10b1b0c8> in <module>()
     18 # Provide a representative dataset to ensure we quantize correctly.
     19 converter.representative_dataset = representative_dataset
---> 20 model_tflite = converter.convert()
     21 
     22 # Save the model to disk

F:\anaconda3\lib\site-packages\tensorflow\lite\python\lite.py in convert(self)
    745         self.inference_input_type, self.inference_output_type)
    746     if flags_modify_model_io_type:
--> 747       result = _modify_model_io_type(result, **flags_modify_model_io_type)
    748 
    749     if self._experimental_sparsify_model:

F:\anaconda3\lib\site-packages\tensorflow\lite\python\util.py in modify_model_io_type(model, inference_input_type, inference_output_type)
    833     return model
    834 
--> 835   model_object = _convert_model_from_bytearray_to_object(model)
    836 
    837   if len(model_object.subgraphs) > 1:

F:\anaconda3\lib\site-packages\tensorflow\lite\python\util.py in _convert_model_from_bytearray_to_object(model_bytearray)
    570 def _convert_model_from_bytearray_to_object(model_bytearray):
    571   """"""Converts a tflite model from a bytearray into a parsable object.""""""
--> 572   model_object = schema_fb.Model.GetRootAsModel(model_bytearray, 0)
    573   model_object = schema_fb.ModelT.InitFromObj(model_object)
    574   model_object = copy.deepcopy(model_object)

AttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'
"
44724,Compiling tensorflow Lite into Android dynamic library ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Centos 7.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):tensorflow2.3.0
- TensorFlow version:tensorflow2.3.0
- Python version:python3.5
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):Bazel3.1.0
- GCC/Compiler version (if compiling from source):GCC6.3.0
- CUDA/cuDNN version:
- GPU model and memory:
-NDK version:r21b

**Describe the problem**
I have generated the static library libtensorflow-lite.a with build_aarch64_lib.sh
I want to use static library to generate dynamic library 
I copy the static library and c++ source code into jni, make Android.mk Application.mk
when I use ndk-build, There have been a lot of mistakes 
undefined reference to 'std::ios_base::Init::~Init()'
**Provide the exact sequence of commands / steps that you executed before running into the problem**
Android.mk
LOCAL_PATH:=$(call my-dir)

include $(CLEAR_VARS)
LOCAL_MODULE:=libtensorflow-lite
LOCAL_SRC_FILES:=/home/wintone/tflitetool/tflitelib/lib/libtensorflow-lite.a
include $(PREBUILT_STATIC_LIBRARY)
include $(CLEAR_VARS)


MY_CPP_LIST:=$(wildcard $(LOCAL_PATH)/*.cpp)

LOCAL_SRC_FILES:=$(MY_CPP_LIST:$(LOCAL_PATH)/%=%)

LOCAL_C_INCLUDES:=/home/wintone/tensorflow /home/wintone/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include/

LOCAL_STATIC_LIBRARIES:=libtensorflow-lite

LOCAL_MODULE:=libtflite-android

LOCAL_LDLIBS:=-llog -landroid -fuse-ld=gold

LOCAL_CPPFLAGS:=-std=c++14 -pthread -frtti -fexceptions 

include $(BUILD_SHARED_LIBRARY)
Application.mk
APP_STL:=c++_static

APP_ABI:=arm64-v8a

APP_PLATFORM:=android-16

APP_CPPFLAGS:=-frtti -fexceptions -std=c++14 -fpermissive

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

libtensorflow-lite.a(util.o):util.cpp:function faltbuffers::ConcatPathFileName(std::__cxx11::basic_string<char,std::char_traits<char>,std::allocator<char>>const&, std::__cxx11::basic_string<char,std::char_traits<char>,std::allocator<char>>const&)error:undefind reference to 'std::__cxx11::basic_string<char,std::char_traits<char>,std::allocator<char>>::_M_erase(unsigned long, unsigned long)'
libtensorflow-lite.a(format_converter.o):format_converter.cc::function _GLOBAL__sub_I_format_converter.cc:error:undefined reference to 'std::ios_base::Init::~Init()' 
"
44723,"When using opt.apply_gradient(zip(grads,model.trainable variables))","ValueError: No gradients provided for any variable: ['dense_flipout_1/kernel_posterior_loc:0', 'dense_flipout_1/kernel_posterior_untransformed_scale:0', 'dense_flipout_1/bias_posterior_loc:0', 'dense_flipout_2/kernel_posterior_loc:0', 'dense_flipout_2/kernel_posterior_untransformed_scale:0', 'dense_flipout_2/bias_posterior_loc:0'].
"
44722,TimeDistributed layer and Nested Modules,"Hi there,
I am using time distributed layer to apply another model on each frame and processing the final features by conv 3D. the model is trained properly. but when I want to load the model and use it for test, the model acc on the same val dataset is not reproducible.
When I looked the model summary I saw that if I use TimeDistributed layer with a model I can see that model parameters in the summary, but if I use a layer combined with Lambda layer, the number of parameters for that TimeDistributed layer is zero.

- tf version '1.15.0'

- keras version '2.2.3'

```
def feature_extractor_model():
    feature_extractor = keras.applications.mobilenet.MobileNet(
        include_top=False,
        input_shape=(100, 100, 1),
        weights=None)
    for layer in feature_extractor.layers:
        layer.trainable = True
    output_layer = feature_extractor.get_layer(name='conv_pw_1_relu').output
    feature_extractor = Model(inputs=feature_extractor.inputs, outputs=output_layer, name='Feature_Extractor') 
    return feature_extractor
def base_model(inputs, N_classes,
               growth_rate=32, num_features=64,
               bn_size=4, drop_rate=0.5,
               feature_size=256, reg_type='l1'):
    feature_extractor = feature_extractor_model()
    shape = inputs.get_shape().as_list()[2:]
    init_features = TimeDistributed(feature_extractor, input_shape=shape)(inputs)
```
this code works well and I can reproduce the results, but the following code does not work and the number of parameters for the second TimeDistributed layer is Zero:

```
def Squeeze_Att_(features):
    conv_feature_att = Conv2D(filters=1, kernel_size=(1, 1), strides=(1, 1))(features)
    emphasized_features = Multiply()([features, conv_feature_att])
    emphasized_features = Activation('relu')(emphasized_features)
    return emphasized_features

def base_model(inputs, N_classes,
               growth_rate=32, num_features=64,
               bn_size=4, drop_rate=0.5,
               feature_size=256, reg_type='l1'):
    feature_extractor = feature_extractor_model()
    shape = inputs.get_shape().as_list()[2:]
    print(shape)
    init_features = TimeDistributed(feature_extractor, input_shape=shape)(inputs)
    Squeeze_Att__ = Lambda(Squeeze_Att_, name=""attention_layer_1"")
    init_features = TimeDistributed(Squeeze_Att__, input_shape=shape)(init_features)
```

it seems that TimeDistributed layer only works when the input is a model. Any Suggestion??"
44721,TFLite TensorArrayScatterV3 failed (not found),"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 (work) / Android 9.0 (inference)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Motorola Nexus 6
- TensorFlow installed from (source or binary): Source (master branch at 4625f1d87f17537a8d50dc68a213d2da875c12ce)
- TensorFlow version (use command below): 4625f1d87f17537a8d50dc68a213d2da875c12ce
- Python version: 3.8.2
- Bazel version (if compiling from source): 3.7.0
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: No GPU
- GPU model and memory: N/A

- Android SDK 30.0.2 and NDK r21d

**Describe the current behavior**

I compiled `benchmark_model_plus_flex` and ran into the same issue. I was trying to benchmark the following model:

<http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz>

with it converted into TFLite format with the following code:

```python
def do_convert(saved_model, output_file):
    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model)
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,
        tf.lite.OpsSet.SELECT_TF_OPS,
    ]
    model = converter.convert()
    with open(output_file, ""wb"") as f:
        f.write(model)
```

The exact error is

```text
native : op_kernel.cc:1763 OP_REQUIRES failed at tensor_array_ops.cc:1035 : Not found: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_774)
ERROR: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_774)
         (while executing 'TensorArrayScatterV3' via Eager)
ERROR: Node number 276 (TfLiteFlexDelegate) failed to invoke.
```

and the command line in `adb shell` is

```shell
./benchmark_model_plus_flex --graph=ssd_mobilenet_v1_coco_2018_01_28.tflite
```

**Describe the expected behavior**

The benchmark should proceed successfully.

**Standalone code to reproduce the issue**
"
44720,C++ inference with tensorflow1.15,"when I do inference with c++, this problem arised,
""2020-11-10 13:41:29.363429: F tensorflow/core/framework/op.cc:200] Non-OK-status: RegisterAlreadyLocked(deferred_[i]) status: Already exists: Op with name _Arg Aborted (core dumped)
""
tensorflow1.15 python 3.6"
44718,"lib/python3.6/site-packages/tensorflow_probability/python/experimental/auto_batching/frontend.py"", line 44, in <module>     from tensorflow.python.autograph.core import naming ImportError: cannot import name 'naming'","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): conda install
- TensorFlow version: 1.15
- Python version: 3.6.1
- Installed using virtualenv? pip? conda?: conda 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: 10
- GPU model and memory:



Getting error:
lib/python3.6/site-packages/tensorflow_probability/python/experimental/auto_batching/frontend.py"", line 44, in <module>
    from tensorflow.python.autograph.core import naming
ImportError: cannot import name 'naming'


**Provide the exact sequence of commands / steps that you executed before running into the problem**
Ran a .py file with ""python <file.py>"" command


"
44714,LeakSanitizer: detected memory leaks,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):libtensorflow.so
- TensorFlow version (use command below):
- Python version: C++
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version:  10.1
- GPU model and memory: nvidia 16GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
when inference in C++, LeakSanitizer detected memory leaks, as follow:
![image](https://user-images.githubusercontent.com/73547638/98616400-b44c3a00-2337-11eb-9501-c15c34c9420a.png)


**Describe the expected behavior**
No memory leaks
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44713,//tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test fails on s390x ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ub18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.3.1
- Python version: 3.6.9
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): Ubuntu 7.5.0-3ubuntu1~18.04
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


**Describe the current behavior**
I am running Tensorflow 2.3.1 on s390x and  `//tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test` is failing on running bazel tests.
The TC was earlier failing with error: ```Internal: TargetRegistry::lookupTarget failed: No available targets are compatible with triple ""x86_64-pc-linux""```

To fix this, I added support for s390x in [test_target_triple_helper.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/test_target_triple_helper.h)
This is the code change I did:
```
diff --git a/tensorflow/compiler/xla/service/cpu/test_target_triple_helper.h b/tensorflow/compiler/xla/service/cpu/test_target_triple_helper.h
index 857de4a814..1fb04f821a 100644
--- a/tensorflow/compiler/xla/service/cpu/test_target_triple_helper.h
+++ b/tensorflow/compiler/xla/service/cpu/test_target_triple_helper.h
@@ -21,8 +21,13 @@ limitations under the License.
 static const char kTargetCpuForHost[] = ""ppc"";
 static const char kTargetTripleForHost[] = ""ppc64le-ibm-linux-gnu"";
 #else
+#if (defined(__s390x__) && (__BYTE_ORDER__ == __ORDER_BIG_ENDIAN__))
+static const char kTargetCpuForHost[] = """";
+static const char kTargetTripleForHost[] = ""systemz-none-linux-gnu""; 
+#else
 static const char kTargetCpuForHost[] = """";
 static const char kTargetTripleForHost[] = ""x86_64-pc-linux"";
 #endif
+#endif

 #endif
```

Even after making this change,  the test case is still failing. The LLVM is identifying `systemz-none-linux-gnu` as target but not able to calculate vector_register_byte_size.
The error output looks like this:
```
==================== Test output for //tensorflow/compiler/xla/service/cpu:vectorized_reduce_with_no_vector_registers_test:
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from CodegenReduceOnArchWithNoVectorRegisters
[ RUN      ] CodegenReduceOnArchWithNoVectorRegisters.Test
2020-11-09 22:19:12.755885: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 1555500000 Hz
tensorflow/compiler/xla/service/cpu/vectorized_reduce_with_no_vector_registers_test.cc:86: Failure
Expected equality of these values:
  vector_register_byte_size_for_x86_64
    Which is: 0
  16
[  FAILED  ] CodegenReduceOnArchWithNoVectorRegisters.Test (4 ms)
[----------] 1 test from CodegenReduceOnArchWithNoVectorRegisters (4 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (4 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] CodegenReduceOnArchWithNoVectorRegisters.Test
```
**Describe the expected behavior**
The LLVM function should return correct values for vector_register_byte_size and the test case should pass.


**Other info / logs** 
I noticed the previous issue regarding XLA Testcases and have already taken the changes done in the PR [39912](https://github.com/tensorflow/tensorflow/issues/39912)"
44711,memory leak in tf.keras.Model.predict,"https://stackoverflow.com/questions/64199384/tf-keras-model-predict-results-in-memory-leak

<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44710,On-disk cache for compiled functions,"Is there a way to cache compiled tf.functions in between sessions with auto-load? As a reference, numba has the option cache=True:
https://numba.readthedocs.io/en/stable/user/jit.html?highlight=cache#cache


**System information**
- TensorFlow version (you are using):
2.3.1


"
44709,sequences_to_texts on character level tf.keras.preprocessing.text.Tokenizer adds spaces between characters,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10 2004**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **v2.3.0-54-gfcc4b966f1 2.3.1** and **v2.3.0-0-gb36436b087 2.3.0**
- Python version: **3.7.9** and **3.6.9**


**Describe the current behavior**
The `tf.keras.preprocessing.text.Tokenizer` with `char_level=True` adds spaces between characters when the `sequences_to_texts` is called. For example if our data is `[""Hi""]` and we convert them to sequence and then back to text the output will be `[""h i""]`

**Describe the expected behavior**
Spaces should not be added.
For example if our data is `[""Hi""]` -> sequence -> [""hi""].


**Standalone code to reproduce the issue**
```
from tensorflow.keras.preprocessing.text import Tokenizer

text_data=[""Hello"",""Run""]
print(text_data)

tok = Tokenizer(char_level=True)
tok.fit_on_texts(text_data)
sequences = tok.texts_to_sequences(text_data)
print(sequences)

reconstructed_text = tok.sequences_to_texts(sequences)
print(reconstructed_text)
```"
44708,TFLite set_tensor fails unexpectedly in Tensorflow version 2.3.1,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Windows 10
- Mobile device: Snapdragon 855
- TensorFlow installed from: binary
- TensorFlow version: 2.3.1
- Python version: 3.7
- Bazel version: None
- GCC/Compiler version:None
- CUDA/cuDNN version: None
- GPU model and memory: None

I have a Keras model which I am able to convert to tflite both without quantization and with full integer quantization. 
I'm then able to run inference using the interpreter and get the expected output, all using the code below. In addition, this works as expected when using the Hexagon delegate on a mobile device

When using Tensorflow version 2.2.0 this works. When using Tensorflow version 2.3.1, it does not work, with the following error:

>ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type INT8 for input 0, name: input_1

I also have the following logs of the ""input details"" of the tflite intepreter:

TF version 2.3.1

non quantized: 
input details:  [{'name': 'input_1', 'index': 0, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([ -1, 224, 224,   3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
input shape:  [  1 224 224   3]

quantized:

input details:  [{'name': 'input_1', 'index': 0, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([ -1, 224, 224,   3]), 'dtype': <class 'numpy.int8'>, 'quantization': (0.007843137718737125, -1), 'quantization_parameters': {'scales': array([0.00784314], dtype=float32), 'zero_points': array([-1]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
input shape:  [  1 224 224   3]

TF version 2.2.0
non quantized:

input details:  [{'name': 'input_1', 'index': 0, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([  1, 224, 224,   3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
input shape:  [  1 224 224   3]

quantized:

input details:  [{'name': 'input_1', 'index': 235, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([  1, 224, 224,   3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
input shape:  [  1 224 224   3]

also, see attached the input image, the reference images and the model. 

I see that with version 2.2.0, the input layer of the quantized version is not quantized. However, the quantized model works fine on Hexagon DSP with Hexagon delegate. What's going on? 

`
[norm_images.zip](https://github.com/tensorflow/tensorflow/files/5512065/norm_images.zip)
[dataset.zip](https://github.com/tensorflow/tensorflow/files/5512071/dataset.zip)
[model.zip](https://drive.google.com/file/d/12ORteaMaDInKpJl3xhlTwyoi0m_V2yvV/view?usp=sharing)







    import tensorflow as tf
    import numpy as np
    import cv2

    def read_and_process_image(fname):
        img = cv2.imread(fname)
        img = cv2.resize(img,dsize = (224,224),interpolation = cv2.INTER_LANCZOS4)
        img = img / 127.5
        img = img - 1
        img = img.astype(np.float32)
        img = img.reshape(1,224,224,3)
        return img

    def rep_data_gen0():
        a = []
        for i in range(128):
            img = np.fromfile('norm_images/image_'+str(i)+"".bin"",dtype=np.float32)
            img = img.reshape(224,224,3)
            a.append(img)
        
        a = np.array(a)
        img = tf.data.Dataset.from_tensor_slices(a).batch(1)
        for i in img.take(128):
            yield [i]


     def convert(model):
        converter = tf.lite.TFLiteConverter.from_keras_model(model)
        tflite_model = converter.convert()
        return tflite_model

    def quantize(model):
        converter = tf.lite.TFLiteConverter.from_keras_model(model)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.representative_dataset = rep_data_gen0
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
        converter.inference_input_type = tf.int8  # or tf.uint8
        converter.inference_output_type = tf.int8  # or tf.uint8

        print(""Converting using full integer quantization"")
        quant_model = converter.convert()
        return quant_model

    def excute_tflite(model,image):
        interpreter = tf.lite.Interpreter(model_content=model)
        interpreter.allocate_tensors()
        # Get input and output tensors.
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()

        input_shape = input_details[0]['shape']
        print (""input details: "", input_details)
        print(""input shape: "", input_shape)
        interpreter.set_tensor(input_details[0]['index'], image)
        interpreter.invoke()

        tflite_results = interpreter.get_tensor(output_details[0]['index'])
        tflite_results_img = tflite_results[0, :, :, 0]
        return tflite_results_img

    def main_func():
        img = read_and_process_image(""dataset/COCO_train2014_000000003860.jpg"")
        keras_model = tf.keras.models.load_model('sod_latest.hdf5')
        tf_model = convert(keras_model)
        quant01_model = quantize(keras_model)
    
        result = keras_model.predict(img)
        result_img = result[0,:,:,0]

        result_tf_img = excute_tflite(tf_model,img)
        result_quant_img0 = excute_tflite(quant01_model, img)

        print(np.min(result_img), np.max(result_img))
        print(np.min(result_tf_img), np.max(result_tf_img))
        print(np.min(result_quant_img0), np.max(result_quant_img0))

        cv2.imshow('keras',result_img)
        cv2.resizeWindow('keras', 400, 400)
        cv2.imshow('tflite', result_tf_img)
        cv2.resizeWindow('tflite', 400, 400)
        cv2.imshow('quantized tflite', result_quant_img0)
        cv2.resizeWindow('quantized tflite', 400, 400)
        cv2.waitKey(0)

    if __name__ == ""__main__"":
        print(tf.__version__)
        main_func()


`
"
44707,[TFLite] TF MatMul operator converted to an unsupported TFL 16x16 FullyConnected operator with 16x8 post-training quantization,"Hello,

When using the `tf.linalg.matmul` function with TFLite 16-bit post-training quantization the operation is sometimes transformed into a `TFL::FullyConnectedOp` operator with 16-bit inputs and 16-bit weights. Unfortunately the 16-bit version of the FC operator only supports 16-bit inputs and 8-bit weights.

The conversion occurs in two places depending on how `tf.linalg.matmul` is interpreted (as a `TF::MatMulOp` operator or a `TF::BatchMatMul(V2)Op`):
* The `TF::MatMulOp` is converted to a `TFL::FullyConnectedOp` in [legalize_tf.cc](https://github.com/tensorflow/tensorflow/blob/c86b90dc9e3dc51dfc95964449570160cb12c1fe/tensorflow/compiler/mlir/lite/transforms/legalize_tf.cc#L242).
* The `TF::BatchMatMul(V2)Op` on the other hand is converted in [unroll_batch_matmul.cc](https://github.com/tensorflow/tensorflow/blob/c86b90dc9e3dc51dfc95964449570160cb12c1fe/tensorflow/compiler/mlir/tensorflow/transforms/unroll_batch_matmul.cc#L190) to a `TF::MatMulOp` which is then converted by the previous transformation into a `TFL::FullyConnectedOp`. This conversion pass is only enabled when [`PrepareTFPass::unfold_batch_matmul_`](https://github.com/tensorflow/tensorflow/blob/c86b90dc9e3dc51dfc95964449570160cb12c1fe/tensorflow/compiler/mlir/lite/transforms/prepare_tf.cc#L1214) is true.

To disable the second conversion we could add a check on the `toco_flags.inference_type()` flag in [graphdef_to_tfl_flatbuffer.cc](https://github.com/tensorflow/tensorflow/blob/c86b90dc9e3dc51dfc95964449570160cb12c1fe/tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc#L88) (and other appropriate places) and set `pass_config.unfold_batch_matmul` to false when the inference type is int16. The `TF::BatchMatMul(V2)Op` would then be converted to a `TFL::BatchMatMulOp` which support 16x16 inputs without trouble.

The problem remains though if we have a simple 16x16 `TF::MatMulOp` operator. We can't just disable the conversion in the same way as for `TF::BatchMatMul(V2)Op` as there isn't any `TFL::MatMulOp`. 

One option would be to disable this transformation in int16 post-training quantization in the same way as for `TF::BatchMatMul(V2)Op` and add a `TF::MatMulOp` to `TF::BatchMatMul(V2)Op` conversion pass. The `TFL::BatchMatMulOp`  would then be used for the cases where we didn't converted the `TF::MatMulOp` to a `TFL::FullyConnectedOp`.

Thibaut"
44706,tf concatination layers compatible with tfp,"Since tfp does not have `tf.keras.layers.Concatenate` does tf2.x support Concatenation of tfp layers?

If some changes are required then what? "
44705,Actor-critic model with LSTM layers runs out of GPU memory eventhough it shouldn't.,"- Custom code
- Windows 10
- TensorFlow version: 2.3.1
- Python version: 3.7.9
- CUDA/cuDNN version: 10.1 / 7.6.5
- GPU model and memory: GTX 1660 6GB


Networks with LSTM layers run out of memory, using only batch at a time. Creating smaller layers don't help.By my calculations they should easily fit into the memory.

Source code to recreate the issue is the following (in a few places it uses bogus values, it was only created to test if there are issues with the fit function):

`
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers     import LSTM, Dense, Dropout, Input, concatenate, Lambda
from tensorflow.keras            import activations
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models     import Model
import tensorflow.keras.backend as K

physical_devices = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)

delta = Input(shape=[1])

input_vision = Input(shape=(2* 5+1,2* 5+1))
first_branch = LSTM(1024)(input_vision)#Input layer of the 1. branch
first_branch = Dropout(0.2)(first_branch)       #Dropout for the input layer

first_branch = Dense(512, activation=activations.relu)(first_branch)       #First layer of the 1. branch
first_branch = Dropout(0.2)(first_branch)

first_branch = Dense(256, activation=activations.relu)(first_branch)       #Second layer of the 1. branch
first_branch = Dropout(0.2)(first_branch)

first_branch = Model(inputs=input_vision, outputs=first_branch)      #The output of the first branch

input_memory  = Input(shape=(100,3))
second_branch = LSTM(1024)(input_memory)       #Input layer of the 2. branch
second_branch = Dropout(0.2)(second_branch)     #Dropout for the input layer

second_branch = Dense(512, activation=activations.relu)(second_branch)     #Second layer of the 2. branch
second_branch = Dropout(0.2)(second_branch)     #Dropout for the first layer

second_branch = Dense(256, activation=activations.relu)(second_branch)     #Third layer of the 2. branch
second_branch = Dropout(0.2)(second_branch)     #Dropout for the first layer

second_branch = Model(inputs=input_memory, outputs=second_branch)    #The output of the first branch

input_task   = Input(shape=(5, 6) )
third_branch = LSTM(1024)(input_task)    #Input layer of the 3. branch
third_branch = Dropout(0.2)(third_branch)       #Dropout for the input layer

third_branch = Dense(512, activation=activations.relu)(third_branch) #Second layer of the 3. branch
third_branch = Dropout(0.2)(third_branch)       #Dropout for the first layer

third_branch = Dense(256, activation=activations.relu)(third_branch) #Third layer of the 3. branch
third_branch = Dropout(0.2)(third_branch)       #Dropout for the first layer

third_branch = Model(inputs=input_task, outputs=third_branch)#The output of the first branch

#Combine all 3 branches into one:
#combined = tf.keras.backend.expand_dims(combined, axis=-1)
combined = concatenate([first_branch.output, second_branch.output, third_branch.output])
combined = Lambda(lambda x: tf.keras.backend.expand_dims(x, axis=-1))(combined)

#Main branch:

main = LSTM(1024)(combined)       #The 1. layer of the main branch
main = Dropout(0.2)(main)

main = Dense(512, activation=activations.relu)(main)  #The 2. layer of the main branch
main = Dropout(0.2)(main)

main = Dense(256, activation=activations.relu)(main)  #The 3. layer of the main branch
main = Dropout(0.2)(main)

#------------------------------------------------These are the outputs of the  models------------------------------------------------
#This is the output of the critic model. The input is one because we just want the value of 1 action, which was taken by the actor.
values = Dense(1, activation=activations.linear)(main)

#This is the output of the actor model. We get the probabilities of each action, that is why softmax needed.
probabilities = Dense(32, activation=activations.softmax)(main)

#This is the policy network needed to connect the actor and the critic:
#y_true is an action that the actor took, and y_pred the probability of that action
def custom_logLikelihood(y_true, y_pred):
     out     = K.clip(y_pred, 1e-8, 1-1e-8) #Clip the prediction so it can not be 0 or 1.
     log_lik = y_true * K.log(out)

     return K.sum(-log_lik * delta)

#------------------------------------------------These are the  models------------------------------------------------
actor = Model(inputs=[first_branch.input, second_branch.input, third_branch.input, delta], outputs=probabilities)
actor.compile(loss=custom_logLikelihood, optimizer=Adam(lr=0.0001), experimental_run_tf_function=False)
tf.config.experimental_run_functions_eagerly(True)  

critic = Model(inputs=[first_branch.input, second_branch.input, third_branch.input], outputs=values)
critic.compile(loss=""mse"", optimizer=Adam(lr=0.0005))

prev_state_vis=np.random.randint(0, 20, size=(11,11))
prev_state_mem=np.random.randint(0, 20, size=(100,3))
prev_state_task=np.random.randint(0, 20, size=(5,6))
curr_state_vis=np.random.randint(0, 20, size=(11,11))
curr_state_mem=np.random.randint(0, 20, size=(100,3))
curr_state_task=np.random.randint(0, 20, size=(5,6))  

prev_state_v = prev_state_vis[np.newaxis, :]
prev_state_m = prev_state_mem[np.newaxis, :]
prev_state_t = prev_state_task[np.newaxis, :]
curr_state_v = curr_state_vis[np.newaxis, :]
curr_state_m = curr_state_mem[np.newaxis, :]
curr_state_t = curr_state_task[np.newaxis, :]

critic_value_prev = critic.predict_on_batch([prev_state_v,
                                                          prev_state_m, 
                                                          prev_state_t])
critic_value_curr = critic.predict_on_batch([curr_state_v,
                                                          curr_state_m, 
                                                          curr_state_t])
target  = 10 + 0.99 * critic_value_curr * (1-int(0))
d   = target - critic_value_prev
actions = np.zeros([1, 32])
actions[np.arange(1), 0] = 1.0

for i in range(0,500*100):
    print(i)
    actor.fit(x=[prev_state_v, prev_state_m, prev_state_t, d], y=actions, verbose=0)
    critic.fit(x=[prev_state_v, prev_state_m, prev_state_t], y=target, verbose=0)
`"
44704,AttributeError: module 'tensorboard.summary._tf.summary' has no attribute '__file__',"The Docker file I used to build TensorFlow 2.3.1 is shown below along with the error trace. Everything builds properly, however, when I import TensorFlow I get the following error:

AttributeError: module 'tensorboard.summary._tf.summary' has no attribute '__file__'


    FROM nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04

    RUN apt-get update && apt-get -y upgrade
    RUN apt-get install -y --no-install-recommends \
             python3 \
             python3-pip \
             python3-dev \
             python3-setuptools \
             python3-setuptools \
             git-core \
             vim \
             libgl1-mesa-glx
    RUN python3 -m pip install --upgrade pip
    RUN pip3 install --upgrade pip
    RUN pip3 install --upgrade \
                    numpy \
                    scikit-build \
                    opencv-python \
                    jupyter \
                    psutil \
                    scikit-image \
                    pycocotools
    # Install Bazel to Install TF2 from source
    # Need to install from source to build with tflite_with_xnnpack
    RUN apt-get install -y --no-install-recommends libglib2.0-0
    RUN apt-get install -y --no-install-recommends curl
    RUN apt-get install -y --no-install-recommends openjdk-8-jdk
    RUN echo ""deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8"" | tee /etc/apt/sources.list.d/bazel.list
    RUN curl https://bazel.build/bazel-release.pub.gpg | apt-key add -
    RUN apt-get update
    RUN apt-get install -y --no-install-recommends bazel-3.1.0
    RUN link /usr/bin/python3 /usr/bin/python
    RUN git clone https://github.com/tensorflow/tensorflow.git
    RUN touch WORKSPACE
    WORKDIR tensorflow
    RUN pip3 install --upgrade keras_preprocessing
    RUN pip3 install absl-py
    RUN pip3 install --upgrade google-api-python-client
    RUN pip3 install --upgrade protobuf
    RUN pip3 install wrapt
    RUN pip3 install termcolor
    RUN pip3 install --upgrade gast
    RUN bazel-3.1.0 build --config=mkl -c opt --copt=-march=skylake-avx512 --copt=-mfpmath=sse --define tflite_with_xnnpack=true //tensorflow/tools/pip_package:build_pip_package

Here is the error log:
    File ""tensorflow_benchmarking.py"", line 11, in <module>
        import tensorflow
    File ""/s/dmerric5/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 54, in <module>
        from ._api.v2 import compat
    File ""/s/dmerric5/.local/lib/python3.6/site-packages/tensorflow/_api/v2/compat/__init__.py"", line 39, in <module>
        from . import v1
    File ""/s/dmerric5/.local/lib/python3.6/site-packages/tensorflow/_api/v2/compat/v1/__init__.py"", line 34, in <module>
        from . import compat
    File ""/s/dmerric5/.local/lib/python3.6/site-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py"", line 40, in <module>
        from . import v2
    File ""/s/dmerric5/.local/lib/python3.6/site-packages/tensorflow/_api/v2/compat/v1/compat/v2/__init__.py"", line 32, in <module>
        from tensorflow._api.v2.compat.v2 import __operators__
    File ""/s/dmerric5/.local/lib/python3.6/site-packages/tensorflow/_api/v2/compat/v2/__init__.py"", line 36, in <module>
        from . import compat
    File ""/s/dmerric5/.local/lib/python3.6/site-packages/tensorflow/_api/v2/compat/v2/compat/__init__.py"", line 40, in <module>
        from . import v2
    File ""/s/dmerric5/.local/lib/python3.6/site-packages/tensorflow/_api/v2/compat/v2/compat/v2/__init__.py"", line 329, in <module>
        [_module_util.get_parent_dir(summary)] + _current_module.__path__)
    File ""/s/dmerric5/.local/lib/python3.6/site-packages/tensorflow/python/tools/module_util.py"", line 30, in get_parent_dir
       return os.path.abspath(os.path.join(os.path.dirname(module.__file__), ""..""))
    AttributeError: module 'tensorboard.summary._tf.summary' has no attribute __file__"
44703,Support string sorting,"
**Describe the feature and the current behavior/state.**

Currently string sorting in the graph is unsupported:

### Does not work:

```python
tf.sort(['d', 'a', 'b'])
InvalidArgumentError: Value for attr 'T' of string is not in the list of allowed values: bfloat16, half, float, double, int8, int16, int32, int64, complex64, complex128
	; NodeDef: {{node Neg}}; Op<name=Neg; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]> [Op:Neg]
```

### Works (but not in the graph):

```python
>>> x = tf.constant(['d', 'a', 'b'])
>>> tf.convert_to_tensor(sorted(x.numpy()), tf.string)
<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'a', b'b', b'd'], dtype=object)>
```

Can `tf.sort` be amended to allow string sorting?

**Will this change the current api? How?**

No

**Who will benefit with this feature?**

Anyone who might need to string-sort in the graph

**Any Other info.**
"
44701, 3760 illegal hardware instruction  python3,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): KALI LINUX
- TensorFlow installed from (source or binary): PIP3 INSTALL TENSORFLOW
- TensorFlow version: 
tensorflow             2.3.1
tensorflow-estimator   2.3.0

- Python version:  Python 3.8
- Installed using virtualenv? pip? conda?:  pip3

- GPU model and memory: HP pavillion g4 intel i5 m480 6gb ram



**Describe the problem**
when ever i try to import tensorflow in my python program as tf

i got this in output as error  :-  ' 3760 illegal hardware instruction  python3 '


**Provide the exact sequence of commands / steps that you executed before running into the problem**

>>> import tensorflow as tf
[1]    3760 illegal hardware instruction  python3



"
44700,SegmentSum GPU OP has no incremental detection on segment_ids,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 8
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): r1.15
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: 10.1
- GPU model and memory: Telsa V100

**Describe the current behavior**
In `tf.segment_sum` GPU version, there is no incremental check on segment_ids. I tried an unsorted segment_ids, and get a wrong result without any exception or warning.

**Describe the expected behavior**
If the segment_ids is required in an incremental sequence,  I think there should be an exception or warning in the implementation.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```python3
import tensorflow as tf

with tf.device('/GPU:0'):
  data = tf.constant([[1,2], [3,4], [5,6], [7,8]], dtype=tf.float32)

  seg_ids = tf.constant([0,0,1,0], dtype=tf.int32)

  gpu_op = tf.segment_sum(data, seg_ids)
  #Got [[16. 20.]].
  #But an exception or warning is expected.
```
"
44699,tensorflow.python.framework.errors_impl.InvalidArgumentError,"hello, when  I run my code, the error mesage occurs:
Traceback (most recent call last):
  File ""/home/tj/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1607, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 122904 and 12 for 'mul' (op: 'Mul') with input shapes: [122904], [12].

do you know how to solve it?"
44698,Training takes forever when applying distribute strategy,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): nightly 
- Python version: 3.7
- CUDA/cuDNN version: 11.0/8.04
- GPU model and memory:3x V100 32G

`v1.12.1-45362-gec957ef376 2.5.0-dev20201108`


**Describe the current behavior**
Apply my code with `tf.distribute.MirroredStrategy()` then it take about 10 times slower to start training. And RAM keeps being filled, growing almost linearly.
**Describe the expected behavior**
-Shouldn't take so long to start, it takes about 15min to start while start almost immediately without any strategy.  
- RAM grows linearly makes it impossible to train.

**Standalone code to reproduce the issue**
```
strategy = tf.distribute.MirroredStrategy()
tf.distribute.experimental_set_strategy(strategy)
raw_image_dataset = tf.data.TFRecordDataset(...)
image_feature_description = {
            'height': tf.io.FixedLenFeature([], tf.int64),
            'width': tf.io.FixedLenFeature([], tf.int64),
            'depth': tf.io.FixedLenFeature([], tf.int64),
            'label': tf.io.FixedLenFeature([], tf.string),
            'image': tf.io.FixedLenFeature([], tf.string),
}

def augument(example_proto):
            exp = tf.io.parse_single_example(example_proto, image_feature_description)
            img = tf.io.decode_jpeg(exp['image'])
            label = tf.io.parse_tensor(exp['label'], tf.float32)
            img = tf.cast(img, tf.float32)
            img = img / 255.0
            img = tf.image.random_brightness(img, max_delta=0.1)
            img = tf.clip_by_value(img, 0.0, 1.0)
            return img, label

train_dataset = raw_image_dataset.map(augument,  num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE)
train_dataset = strategy.experimental_distribute_dataset(train_dataset)
...

def train_step(self, img):
    with tf.GradientTape() as tape:
        loss = model(img)
    variables = tape.watched_variables()
    grads = tape.gradient(loss, variables)
    optimizer.apply_gradients(zip(grads, variables))
    return tf.reduce_mean(loss)

@tf.function
def distributed_train_step(self, img):
    per_replica_losses = strategy.run(self.train_step, args=(img,))
    return strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses, axis=None)

for td in train_dataset:
    distributed_train_step(td)

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Before training, it logs:
```
INFO:tensorflow:batch_all_reduce: 1 all-reduces with algorithm = nccl, num_packs = 1
INFO:tensorflow:batch_all_reduce: 1 all-reduces with algorithm = nccl, num_packs = 1
INFO:tensorflow:batch_all_reduce: 2313 all-reduces with algorithm = nccl, num_packs = 1
INFO:tensorflow:batch_all_reduce: 2313 all-reduces with algorithm = nccl, num_packs = 1
```
The last two lines take so long to be printed. "
44697,Incorrect Cupti dll name on W10,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 20H2
- TensorFlow installed from (source or binary): Binary (pip)
- TensorFlow version (use command below): 2.4.0-rc0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0 / 8
- GPU model and memory: Nvidia 2080Ti 11Gb

**Describe the current behavior**
On Windows it looks for the dll 'cupti64_110.dll' however Cuda Toolkit 11 and others now installs as 'cupti64_2020.1.0.dll' on W10.

**Describe the expected behavior**
Correctly identify the cupti dll name to work on W10 with the default Cuda Toolkit install.

**Standalone code to reproduce the issue**
Any training code with profiling or logging on W10.

**Other info / logs** 
Can be fixed by copying/renaming the dll to 'cupti64_110.dll' in the 'Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.0\extras\CUPTI\lib64' folder.
"
44696,tensorflow/go: does not contain package go/core/protobuf/for_core_protos_go_proto,
44695,bazel build libtensorflowlite has issue during compiling,"tizen5.0_cross_toolchain_vd_kant_target_armv7l_host_x86-64/bin/armv7l-tizen-linux-gnueabi-gcc -MD -MF bazel-out/armv7-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/simple_concat.d '-frandom-seed=bazel-out/armv7-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/simple_concat.o' -iquote external/mkl_dnn -iquote bazel-out/armv7-opt/bin/external/mkl_dnn -isystem external/mkl_dnn/include -isystem bazel-out/armv7-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/armv7-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/armv7-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/armv7-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/armv7-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/armv7-opt/bin/external/mkl_dnn/src/cpu/xbyak -w -DAUTOLOAD_DYNAMIC_KERNELS -fPIC '-std=c++14' -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -c external/mkl_dnn/src/cpu/simple_concat.cpp -o bazel-out/armv7-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/simple_concat.o)
Execution platform: @local_execution_config_platform//:platform
In file included from external/mkl_dnn/src/common/mkldnn_thread.hpp:20:0,
                 from external/mkl_dnn/src/cpu/simple_concat.cpp:17:
external/mkl_dnn/src/common/utils.hpp:45:1: error: static assertion failed: Intel(R) MKL-DNN supports 64 bit only
 static_assert(sizeof(void*) == 8, ""Intel(R) MKL-DNN supports 64 bit only"");
 ^~~~~~~~~~~~~
Target //tensorflow/lite:libtensorflowlite.so failed to build
"
44694,How to avoid unsupported ops during the tf lite conversion,"**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from (source or binary): pip installed tf
- TensorFlow version (or github SHA if from source): See below for detailed info


**Command used to run the converter or code if youre using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
import torch
import torch.onnx
import tensorflow as tf
import onnx
import os

from onnx_tf.backend import prepare
from models.pytorch import models
from models import vgg
from models import googlenet
from models import lenet


output_dir = ""./results/vgg/""

network = vgg.VGG('VGG13')
# network = lenet.LeNet()


if not os.path.exists(output_dir):
    os.makedirs(output_dir)

network.eval()
input_image = torch.randn(1, 3, 32, 32, requires_grad=True)
torch_out = network(input_image)

# Export the model to ONNX
torch.onnx.export(network,               # model being run
                  input_image,                         # model input (or a tuple for multiple inputs)
                  output_dir + ""model.onnx"",   # where to save the model (can be a file or file-like object)
                  export_params=True,        # store the trained parameter weights inside the model file
                  opset_version=10,          # the ONNX version to export the model to
                  do_constant_folding=True,  # whether to execute constant folding for optimization
                  input_names = ['input'],   # the model's input names
                  output_names = ['output'], # the model's output names
                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes
                                'output' : {0 : 'batch_size'}})
# Export the model to TF
onnx_model = onnx.load(output_dir + 'model.onnx')  # load onnx model
tf_rep = prepare(onnx_model)  # prepare tf representation
tf_rep.export_graph(output_dir + 'tf_model')  # export the model
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir=output_dir+'tf_model', signature_keys=['serving_default'])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.experimental_new_converter = True

# Uncomment this line to use tfops in tflite conversion, will not run on mobile devices
# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]

tflite_model = converter.convert()


with open(output_dir + 'model.tflite', 'wb') as f:
    f.write(tflite_model)
```

**The output from the converter invocation**

```
--Under 2.2.0 (onnxtf only support >=2.2.0)
loc(""onnx_tf_prefix_Pad_25@__inference_cond_true_306_570_frozen""): error: 'tfl.pad' op failed to verify that operand 0's rank e
quals operand 1's size
Traceback (most recent call last):
  File ""c:\users\macin\.conda\envs\normaltf\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\macin\.conda\envs\normaltf\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\macin\.conda\envs\normalTF\Scripts\toco_from_protos.exe\__main__.py"", line 7, in <module>
  File ""c:\users\macin\.conda\envs\normaltf\lib\site-packages\tensorflow\lite\toco\python\toco_from_protos.py"", line 93, in mai
n
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""c:\users\macin\.conda\envs\normaltf\lib\site-packages\tensorflow\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""c:\users\macin\.conda\envs\normaltf\lib\site-packages\absl\app.py"", line 300, in run
    _run_main(main, args)
  File ""c:\users\macin\.conda\envs\normaltf\lib\site-packages\absl\app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""c:\users\macin\.conda\envs\normaltf\lib\site-packages\tensorflow\lite\toco\python\toco_from_protos.py"", line 56, in exe
cute
    enable_mlir_converter)
Exception: <unknown>:0: error: loc(""onnx_tf_prefix_Pad_25@__inference_cond_true_306_570_frozen""): 'tfl.pad' op failed to verify
 that operand 0's rank equals operand 1's size

--Under 2.3.0
....a lot of characters
E6C3C3C21133B87CD213B7AEACD3B8219B83B43EBE8BBA4F8FFBB8F16FFBBC74C313CAD77963B48FC453CA0AF66BC2CCB35BB6DC30CBCC21323BC59E28DBA11
17A83A7931A8BB15E826BCADD90EBC481C223CF6CA513C17E92B3CBA57D5B82DB97BBB4B465B3C795EC53B786FED3B69FCA2BB6E81DA3BDFFE3ABC93CA08BBB
E8068BC1152553C225124BB5DF0DD3B0EEE3DBCEF8E02BC1F3A14BC4F0A28BC3415423B7CA785BA008236BBF1A75C3CF94F503CD367A13A0DC744BC30E47239
072F77BB1B72F03B8EC518BA3A1E5D3CF5953C3CC33A4F3C0B562DBCE2334D3C2E33653C9050F2B9A71E073C9A3C26BCF63130BBA0D86A3CDD41D1BB43D045B
BDD6A883B5594113CCCA3A43B6D08B3BB9B385F3BE4D90F3C99C5B8BB6E7F583CD103493CD9EC89BBD0C511B82838D63B8A49B1BBE3D92C3B60644F3BC68B02
3C030400BCBC6F99BB2D9ECDBA25435B3C4A74BD3AEF522CBB3A67AB3B0E66183A6DEDF73B2B3A5EBB925F92BBD008DB3A0D2652BCE11C123C0FB468BC14E02
93BB8E1F43AB857093C0AC9173BF0932DBCC06BA33BC9DF31BCB37F5A3C29B24D3B006908BC3C768B3B5526103B5AB2A3BBF87A7E3B8519BA3B530B323CD898
6BBCFF61B33B413A893B9CD2B13B5AD147BB56B62FBB748E383CAECA4B3C88E0B03B069137BC8E02FAB96B5A64BC980850BCD434753B6F88D8BBFCBC093C07E
EA73BEAA240BC28B18A3BC6A851BC1351DFBB50762C3C0D6ECDBA45150ABC94635CBC486A6ABCBE735F3C5A626DBC308543BB86C86BBCE670B73B74FA1DBC16
9730BB21E4293C12C570BCFD1990BBCA8AB73B22D66C3A122870BCBF0E0FBCAEC26CBC3D9E4E3C42049D3BD41CF2BB2FD72CBCADCA2DBCD966563CCA946B3C7
DE203BCC4FA0D3CD289E83BF7439ABBB2B920BC18C1033CC3E22CBCC6883C3BB877513CDB515A3C""> : tensor<512xf32>} : () -> tensor<512xf32>
  %cst_26 = ""std.constant""() {value = dense<""0x7460CA3B75992EBC01CB64BC4859043C8B26BABBE5BE90BB89EC193BF8C9C23A98BC093C3C04E93A
2C2424BC681B0BBCE8AC8C3B2FC84DBCDD113CBC116F90BB1306B3BB5EB078BA6A0F7B3B23B6003C41648FBAC2D860BC38FD92BB581C16BCEC00CEBB44F84FB
CF35314BCB63B02BC4B7239BC9EE30BBC9B851F3CF6D260396245A3BB86D442BCCEBC62BC63F969BBEB6E563CEA56B53B28FD573C98974B3C361671BCFB9C60
3C0978603C733C4A3CDD7353BCC4782ABB0F0E6B3BF534F83BF93C8E3AB8E25D3C2493CF3AD439CE3A2428153CCAF95FBCAC4E703CF23823BC496BA83812CC1
2BCCF4A833B7FFABF3BA87BB0BB84F96B3C4EF1F33B2D9FDD3B48D304BC6AAE693C2FFEF03BB110333BE69C4C3C480143BBB902263CF7B752BB817AC1BB180A
C33A2E3819BCAD2CE93BF2313A3C34D9C73B6D5457BCBC5B403C054C09BC88276DBC42F891BBAE5F5F3C99A39ABBB9B958B9995396BBD270513C557D10BCA61
C203CF12C543C2409223C8D7ECDBB07772D3AAAD71B3C41663DBC82F2D9BB622D703C54E5FF3A0CF9293B68A80CBC6D6FA0BBE56F8ABB2F444F3C18FE54BCBA
C5473BE46AB7BA50B2B13B067E19BCDBE244BB5D7A63BB43752A3C689DC33A44986A3CB018133AFD5120BC5954F1BB0272D5BA07071ABC94BB34BC89D258BC3
D14B53A700411BA52346DBBA0EEFFBB37544CBC1E663EBC8CC3E13B81656F3A5B98773BE8DF16BC805AA53B290DA7B94CDF4DBCB7DDE53B6A4E1FBCA59934BC
B85CBDBBBA92213C3CB41A3CB58BCD3A61B782BBB80FE5BB692E65BC7D4603BC4B9A52BB7103DDBBFBF4003C07DF2CBC939D4CBC0645CF3B56F5853B0827FA3
B8FD0493C92FD303BBCFE8C3B1E175F3C9890393C961A573C03212A3CFC022A3B8687DBBB0B46EA3A4D4A33BB85C93D3CFF7233BC9BA233BC512D5B3CB784A6
3B583C12BB01121ABC2AE4C23B1ACF61BB21E2D2BBFCD36FBCC2DBB53B9380153B6A78D0BB315B683CFEB547392C4F42BCE726903B9376D8BBD37232BCD564D
23BF2264FBCEE2B633C040CCFBB2AD0303B2FA456BC774E5FBB67F7423CFB2B893AAF1A7838B08D11BCA0C63EBC0FEAF6B90299913B5E112CBCCF67353C8F14
A7BBB6DE48BC62E617BCE915C1BB920919BC7D50D23B3D0217BCCA0101BBF062B13B5A11663CD42CBD3B5D2CA03B3682983A03650D3C1C5E333CFDA46FBB59F
4373CC2B8643C4F2D4EBC033DFC3BAFCDC4BAB2A73BBCD3FBEAB97B25B9BB1F655DBCDD60CBBB9E0E9D3B59FE303C672F013C0FB009BCFFAB17BC8EB76C3B32
9327BC5477C3BB896DB9BB441FFDBB41D0A8BB8935C0BA2047B13B744CFA3B85CDDE3B22196F3A11E993BB37D0E8BB2E0B0D3B18C3A9BA40224F3B5B0C00BCD
2B646BC76463CBB131FD83B9D10723B41C2B83B41F04ABBE57533BA210E8EBAA9B8883B80FF683BE4848D3BBC11D5BB989E2C3C6BF04EBC7691523CEB74603C
037A5B3C39AB953BFEC80B3BD1448FB809BE043C41890D3C19D702BB2877513C6495E5BBEADA62BC2E65EDBBAB059ABB045A533C3629693CE91DE9BB1542A0B
B0169033BBB5B5A3C7A053A3CF69F553BBAE716BBB032463C6E35CF3BCA0430BCF54910BCEAC5783BE6B36D3B75C355BC0D75D33B407CC4B9373841BCADBD37
3C63222E3B2D337DB9FFDF51BB526EF23B519AF1B99E3F6F3CACEE2A3B14E48ABBB7CF5FB67EAB2E3CFEC447B943D65EBB2D5A563C551434BBC162ADBB5361D
93BDE17233BD58A48BC2649933B139747BC53A25DBBA94B8EBBEE5DC43BC7D16D3CD1D288BB2C30393C41552EBCF2A06D398667C9BBDA5DBD3AB6BE3DBCE79A
4F3C6538643A0B4D54BC30CA973B1D30AF3BCC062C3B6D9DE93B7B2C88BB5B8915BB37AE20BC82A1F33B498AD23BFEA8203C750B48BCF987D8B936ADAB3BE30
A54BC90AA503B13DD6ABB247F5C3BB334693909FE1E3C5AF330BC33410C3CFE509D3BB516FDBB2466C83B740C523C71812CBC20EF16BC5DB4623CE030CABB83
4F8F3B8443B0BB6263573CFBB5133C732946BCFC51E8BADC1940BC38095FBC4A400CBB6C1AE8BA34C2C73B8DC010BC5E26A5BB66724ABCBE65EBBB40003F3CA
4FB193CE73327BC14622DBC1C64503AFD4814BCBD1424BC41174139C42352BC7F5D21BC11B294BBC3CAA83B6B565E3B1707513BA92DD8BA657383BABD259CBA
716E293C681C973BC2412FBC87BF39BC25E24F3C51D7533C9319D3BB5C0C343CCEC8C3BB0759B4BB6D80503C6A804C3C1BAC2D3BADB02F3C8866AB39BCFCB0B
BDFD8B43BD171C1BB64D8393C7F8E45BB1ECF533B4E88923BA8D44FBC69F88E3AE00B473C5E81803A2739A7BBF1656EBC50F7F83B892F5BBCACAA08BC093509
BC527A93BB3D4CFEBA0F1B62BC8CC6C83BCA0CBC3A62008DBAA4AA70BC118216BCC7236FBC83173BBA636FA0BBE7F5233CE3862A3C93DF0C3C5F93503CA0F80
93C9BA4F13B70AD023C871E0C3CE113ECBBE1F2F53BCF3EA63B88F600BCA2515D3CC2493DBB04DB433BEDF719BA6807283C1B74F2BBAE4131BC123DB7BB2169
27BC2DE1B4BAD2D748BCF3C7433C70F50F3CCF6E6DBC6FA7473CE6A191BB8D59623C3C5787BB516302BCDCA7D43BFE2E19BC9F28663C2D4C663C6A944C3CAFA
4063C0074A3BB4BCEA1B920250FBCC0D3E03B799935BC0271B7BBC69F41BC6495D4BB16CC0F3CBCF1CE371D4CE53A6994303C66C149BCCFA3A83B39A669BCB9
53363C7668FF3BF57D1ABB74B4EC3B2FAB613CE05DAA3A42A4B7BB011114BC4A517A3BAFEBC0BB9079143C6D8968BC88B60CBC2A5A143C13E662BCEF34013C4
7BB52BCA99D433C5F9B02BC6738503CF605663C4238DB3B62F2093C75A43DBCBF61313CEC6CD53B""> : tensor<512xf32>} : () -> tensor<512xf32>
  %cst_27 = ""std.constant""() {value = dense<""0x87236A3B5915D9B9F4955CBC1A453A3CEE1A4ABCC4746FBC09A51ABC09960D3C9E9F3CBC8C185CBB
24CD5CBCBBF7A8BB1138603CE161053C0493A33B1D9C433C1E02193A0D4E44BC665BD5BBF2D2A33A6B231CBC11FB8EBB143A55BCBE28713C68BC3E3CA0B6C7B
BC4D3263CCBC653BAEE36F33B85284E3C170C5C3CC3A8233C01D9873B5489B5BBCF3F6DBC574269BB23565F3B6511EC3B1DEA5FBC96D535BC1030B13B3218C2
3BC006E33BB80760BC1BCC563C95572DBC414265BC99E339BCB2229F3BDB07BB3AB7E0C6BBEA4B6ABCF12E54BC92195ABC0D70483C26D69DBBC19281BBC470C
EBA7912AF3B24E31EBBD796183CC2F73BBADA9A273C8D2D263B7EEE1DBC59EB023BBF6D193C6AB762BBAFF7ACBA1EABE0BB938AB2BBBF47673CD296473C0B45
C03B31ADA6BA96A0203C83D49B3B6855CEBBDFA541BC08931BBC637A1BBCC4892FBC01A1233CCBE040BC1E93B53BE4A6593CB038DCBAB20E7B3B3A1813BCB54
30ABC2BD014BC0CE340BCEB2153BC700B5F3C120630BCC5C933B82947883B859802BAB5151ABC3DC14B3AE04DE13B6FACEF3B113EAC3B3A25353CC02BC83BE6
8CBCBB3EC7AA3BBB4A39BB89530B3CF08312BBD5013D3C8F721EBC0723203B05E94DBC8107353BCFB91439EBA6443BFB3D0EBC442AD6BB5ECA21BC672FC5BAE
85CF4BAB39207BC5378133CB1093EBC0AF0403CC746673CF9C234BCC1414D3BF08FF5BBC2013DBA32316A3C3D947CBB66D02F3A4D844DBC4983E93B2703823B
56DD3BBB206DC9BB54509FBB29931C3BCCC70F3C5C194FBBD7A109BBAA91EE3A99B168BC5FBA5DBCC8B768BB5FB787BBAA3A5FBC9FD75C3C1011E23B6E8ADB3
BD0DF7FBBC2FCF43B46DCDA3B4046C439BB1645BC946CFEBBC80BBDBB6CA66F3C530DBF3B5CA867BC205833BC8C7037BC0CFA073CE7E2823BDC887EBA2EA136
BCCCB7163CCBE0243C38CA4C3C1FDA20BBCC2CB83ACB175CBC2E6BAF3A557984B969EBC4BBD025433A557FCC3A1BC638BBA57517BC4D74D23BC968373C6D4B8
ABBEBD0683C87C1C03B1D16483C55A1D13B1352253CB530AF3BE79D1F3C7FCBC43923C52DBB2B6D5DBB00BB13BB92A608BC14D0583CAC5070BCB991E6BB218E
7B3B48AE3BBCD9645C3CF0F41A3C4995493C46A75BBC92826A3CC25DD6BA61CC38BC2D625DBC83175F3BD64227BC15281BBBE8F5F93BC1892F3C9AC7D6B97B3
9363C67269ABBC2DC483B93386D3CB984243C63F4293CC799F5BA3752B3BB8F53C1BBE0C7803B7898A33B953DBB3BA84B1F3BE19FBA3B741364BC205B1A3C7D
A0283BBB8AC03B4EFB333C4417B6BA4C81023CB98113BC60D343BBE8C5363B1DEC173C02A13ABCB173243C66E6443CAA3C6EBCBF0D20BCB57E44BC9915053C0
0DE6CBC3B0145BBC2F352BCD5E9163C611841BCF7DA543CC5F08EBB28EDA53BF0BE973BAF4D6ABBF3E25B3C4F5DB23B378159BCD273ECBAC888343C7752DFB9
5C3B1DBA5B5F2C3B24A7273CD40667BB6FCD63BCD2CB323CC252043CF58A96BB226BA4BA7DB0BEBB58D635BB5F3913BC148E0E3C5149A13BB40A68B862C5653
C9252003BCFF6293C09B8AEBB4EBB493A7799AEBA526F51BCD59A4E3C946A67BBD3CC9BBBD5D2303C23303CBC984C4E3BD582C1BBAD8FEB3B18F3803B049CB6
B9A6E3DC3B7923333B034269BC8A60E63B916BE93B6C7F5CBC24DD9E3A1A2ECEBB4C55643C0D8339BC16E9FE3B924E33BC4BD7A0B847CD6A3B997E47BCF9BFB
ABB191844BC8BC759BCFB985CBC605DA33B642ED4BB684DA9B95686C13BA53FB3BB91F16FBABED41FBC4DE26E3C437C373C6E165F3C0F4EF4BB8987543CFFBC
D73B23A39CBB85B955BCA2FBD33A39FFC53B8DC7B33AAB1A1ABC2DC206BCC8D8B03BB18B403A7AA520BC7132A13BD0FFD9BB4D5FC7BB84C62B3B07E0C4BAD72
F88BB2DC8D9BBA923493CC6DD3DBB5319143BFAD563BC1F8812BCFFDE1EBC322E453C2C9302BC8BE6C4BB573109BBFCC17EBB90351E3B1ABC3DBC95BEFDBA5D
06E3BB76B5C93A153226BBF99EF13A7ECAAF3A3EA419BC06604C3CE4EAD4BBB208FCBB636B5E3B51DB473CFCD7FDBBE457A53B848E48BC3215003B4D1F16BC8
3D334BC1278BBBB5AE6543A23CB193C74251DBCCEB7583C800530BC61D702BC6BD8EBBA56D3503CB485693CD91641BCDD5AE6BBB97A5FBC81B200BC1A1C53BC
7D6E4DB9862E0BBBEA245E3C27E1F63A6818C3BAF26B913B04EF1A3C174A13BCDF4213BCB954F9BBC21D293CF62D66BCD0102F3C742F01BC83D2773B27B55A3
C59C5EBBBC72BDFBB206E5F3CAD993F3B85B65C3C1356553CCD9F9CB919C0243C53922FBC9E761C3C3C5DD53B8F7696BA1ADAE8BB27A7073B9D0CB93B277C13
3CE21AA53A9B896D3C061AC43B2898F33A9D3B123CFC17B03BE04561BC9315A0BA492C0D3C0B4958BC01AD603C3066103CEC13E13BC90D72BB543085B9D972E
ABB18D6073C4754FC3B59115B3C823D463CA49EAF3BC38F843B8B06903B19305CBCC44B91BBFB020ABCE6A44FBCCE67F8BA2732ACBBBF0EA93BA507F73B6110
023B782E38BCF87C8DBB740524BC37DC39BB689D283C9B58903B66CC55BC5EF09D3B890990BA9FE483BBDCAA633C2D437C391379FB3BFD96263BB4CFDCBB6E3
C473C59043FBB94FF443C905851BC5DC21B3A0457323C3189183C0DE2BFBB2FA0B33BAAF64FBC5306633CC7D4E43B65EB8FBBF3FE08BCB2F94A3C5FCD47BCEF
CF70BC30E5333C26FF9FBB016970BBC105DF3B8672F43BECF9D5BB5C40143C114CA03B0D7A21BCA45C56BC5A9EB1BB9E782D3C45BB0F3B9A9623BB8BA7483C9
57CDFBB11789ABBDB3607BC37C3503BCFE8493CACD1853AA8FF153C27BB2A3C3B03823BDACFF33B""> : tensor<512xf32>} : () -> tensor<512xf32>
  %cst_28 = ""std.constant""() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_29 = ""std.constant""() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_30 = ""std.constant""() {value = dense<0> : tensor<4x2xi32>} : () -> tensor<4x2xi32>
  %0 = ""tfl.pad""(%arg0, %cst) : (tensor<?x3x32x32xf32>, tensor<4x2xi32>) -> tensor<?x3x34x34xf32>
  %1 = ""tfl.transpose""(%0, %cst_3) : (tensor<?x3x34x34xf32>, tensor<4xi32>) -> tensor<?x34x34x3xf32>
  %2 = ""tfl.split""(%cst_1, %1) {num_splits = 1 : i32} : (tensor<i32>, tensor<?x34x34x3xf32>) -> tensor<?x34x34x3xf32>
  %3 = ""tfl.conv_2d""(%2, %cst_8, %cst_18) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function
= ""RELU"", padding = ""VALID"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<?x34x34x3xf32>, tensor<64x3x3x3xf32>, tensor<64x
f32>) -> tensor<?x32x32x64xf32>
  %4 = ""tfl.transpose""(%3, %cst_2) : (tensor<?x32x32x64xf32>, tensor<4xi32>) -> tensor<?x64x32x32xf32>
  %5 = ""tfl.pad""(%4, %cst) : (tensor<?x64x32x32xf32>, tensor<4x2xi32>) -> tensor<?x64x34x34xf32>
  %6 = ""tfl.transpose""(%5, %cst_3) : (tensor<?x64x34x34xf32>, tensor<4xi32>) -> tensor<?x34x34x64xf32>
  %7 = ""tfl.split""(%cst_1, %6) {num_splits = 1 : i32} : (tensor<i32>, tensor<?x34x34x64xf32>) -> tensor<?x34x34x64xf32>
  %8 = ""tfl.conv_2d""(%7, %cst_9, %cst_19) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function
= ""RELU"", padding = ""VALID"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<?x34x34x64xf32>, tensor<64x3x3x64xf32>, tensor<6
4xf32>) -> tensor<?x32x32x64xf32>
  %9 = ""tfl.transpose""(%8, %cst_2) : (tensor<?x32x32x64xf32>, tensor<4xi32>) -> tensor<?x64x32x32xf32>
  %10 = ""tfl.transpose""(%9, %cst_3) : (tensor<?x64x32x32xf32>, tensor<4xi32>) -> tensor<?x32x32x64xf32>
  %11 = ""tfl.max_pool_2d""(%10) {filter_height = 2 : i32, filter_width = 2 : i32, fused_activation_function = ""NONE"", padding =
""VALID"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<?x32x32x64xf32>) -> tensor<?x16x16x64xf32>
  %12 = ""tfl.transpose""(%11, %cst_2) : (tensor<?x16x16x64xf32>, tensor<4xi32>) -> tensor<?x64x16x16xf32>
  %13 = ""tfl.pad""(%12, %cst) : (tensor<?x64x16x16xf32>, tensor<4x2xi32>) -> tensor<?x64x18x18xf32>
  %14 = ""tfl.transpose""(%13, %cst_3) : (tensor<?x64x18x18xf32>, tensor<4xi32>) -> tensor<?x18x18x64xf32>
  %15 = ""tfl.split""(%cst_1, %14) {num_splits = 1 : i32} : (tensor<i32>, tensor<?x18x18x64xf32>) -> tensor<?x18x18x64xf32>
  %16 = ""tfl.conv_2d""(%15, %cst_10, %cst_20) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_functi
on = ""RELU"", padding = ""VALID"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<?x18x18x64xf32>, tensor<128x3x3x64xf32>, tens
or<128xf32>) -> tensor<?x16x16x128xf32>
  %17 = ""tfl.transpose""(%16, %cst_2) : (tensor<?x16x16x128xf32>, tensor<4xi32>) -> tensor<?x128x16x16xf32>
  %18 = ""tfl.pad""(%17, %cst) : (tensor<?x128x16x16xf32>, tensor<4x2xi32>) -> tensor<?x128x18x18xf32>
  %19 = ""tfl.transpose""(%18, %cst_3) : (tensor<?x128x18x18xf32>, tensor<4xi32>) -> tensor<?x18x18x128xf32>
  %20 = ""tfl.split""(%cst_1, %19) {num_splits = 1 : i32} : (tensor<i32>, tensor<?x18x18x128xf32>) -> tensor<?x18x18x128xf32>
  %21 = ""tfl.conv_2d""(%20, %cst_11, %cst_21) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_functi
on = ""RELU"", padding = ""VALID"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<?x18x18x128xf32>, tensor<128x3x3x128xf32>, te
nsor<128xf32>) -> tensor<?x16x16x128xf32>
  %22 = ""tfl.transpose""(%21, %cst_2) : (tensor<?x16x16x128xf32>, tensor<4xi32>) -> tensor<?x128x16x16xf32>
  %23 = ""tfl.transpose""(%22, %cst_3) : (tensor<?x128x16x16xf32>, tensor<4xi32>) -> tensor<?x16x16x128xf32>
  %24 = ""tfl.max_pool_2d""(%23) {filter_height = 2 : i32, filter_width = 2 : i32, fused_activation_function = ""NONE"", padding =
""VALID"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<?x16x16x128xf32>) -> tensor<?x8x8x128xf32>
  %25 = ""tfl.transpose""(%24, %cst_2) : (tensor<?x8x8x128xf32>, tensor<4xi32>) -> tensor<?x128x8x8xf32>
  %26 = ""tfl.pad""(%25, %cst) : (tensor<?x128x8x8xf32>, tensor<4x2xi32>) -> tensor<?x128x10x10xf32>
  %27 = ""tfl.transpose""(%26, %cst_3) : (tensor<?x128x10x10xf32>, tensor<4xi32>) -> tensor<?x10x10x128xf32>
  %28 = ""tfl.split""(%cst_1, %27) {num_splits = 1 : i32} : (tensor<i32>, tensor<?x10x10x128xf32>) -> tensor<?x10x10x128xf32>
  %29 = ""tfl.conv_2d""(%28, %cst_12, %cst_22) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_functi
on = ""RELU"", padding = ""VALID"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<?x10x10x128xf32>, tensor<256x3x3x128xf32>, te
nsor<256xf32>) -> tensor<?x8x8x256xf32>
  %30 = ""tfl.transpose""(%29, %cst_2) : (tensor<?x8x8x256xf32>, tensor<4xi32>) -> tensor<?x256x8x8xf32>
  %31 = ""tfl.pad""(%30, %cst) : (tensor<?x256x8x8xf32>, tensor<4x2xi32>) -> tensor<?x256x10x10xf32>
  %32 = ""tfl.transpose""(%31, %cst_3) : (tensor<?x256x10x10xf32>, tensor<4xi32>) -> tensor<?x10x10x256xf32>
  %33 = ""tfl.split""(%cst_1, %32) {num_splits = 1 : i32} : (tensor<i32>, tensor<?x10x10x256xf32>) -> tensor<?x10x10x256xf32>
  %34 = ""tfl.conv_2d""(%33, %cst_13, %cst_23) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_functi
on = ""RELU"", padding = ""VALID"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<?x10x10x256xf32>, tensor<256x3x3x256xf32>, te
nsor<256xf32>) -> tensor<?x8x8x256xf32>
  %35 = ""tfl.transpose""(%34, %cst_2) : (tensor<?x8x8x256xf32>, tensor<4xi32>) -> tensor<?x256x8x8xf32>
  %36 = ""tfl.transpose""(%35, %cst_3) : (tensor<?x256x8x8xf32>, tensor<4xi32>) -> tensor<?x8x8x256xf32>
  %37 = ""tfl.max_pool_2d""(%36) {filter_height = 2 : i32, filter_width = 2 : i32, fused_activation_function = ""NONE"", padding =
""VALID"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<?x8x8x256xf32>) -> tensor<?x4x4x256xf32>
  %38 = ""tfl.transpose""(%37, %cst_2) : (tensor<?x4x4x256xf32>, tensor<4xi32>) -> tensor<?x256x4x4xf32>
  %39 = ""tfl.pad""(%38, %cst) : (tensor<?x256x4x4xf32>, tensor<4x2xi32>) -> tensor<?x256x6x6xf32>
  %40 = ""tfl.transpose""(%39, %cst_3) : (tensor<?x256x6x6xf32>, tensor<4xi32>) -> tensor<?x6x6x256xf32>
  %41 = ""tfl.split""(%cst_1, %40) {num_splits = 1 : i32} : (tensor<i32>, tensor<?x6x6x256xf32>) -> tensor<?x6x6x256xf32>
  %42 = ""tfl.conv_2d""(%41, %cst_14, %cst_24) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_functi
on = ""RELU"", padding = ""VALID"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<?x6x6x256xf32>, tensor<512x3x3x256xf32>, tens
or<512xf32>) -> tensor<?x4x4x512xf32>
  %43 = ""tfl.transpose""(%42, %cst_2) : (tensor<?x4x4x512xf32>, tensor<4xi32>) -> tensor<?x512x4x4xf32>
  %44 = ""tfl.pad""(%43, %cst) : (tensor<?x512x4x4xf32>, tensor<4x2xi32>) -> tensor<?x512x6x6xf32>
  %45 = ""tfl.transpose""(%44, %cst_3) : (tensor<?x512x6x6xf32>, tensor<4xi32>) -> tensor<?x6x6x512xf32>
  %46 = ""tfl.split""(%cst_1, %45) {num_splits = 1 : i32} : (tensor<i32>, tensor<?x6x6x512xf32>) -> tensor<?x6x6x512xf32>
  %47 = ""tfl.conv_2d""(%46, %cst_15, %cst_25) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_functi
on = ""RELU"", padding = ""VALID"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<?x6x6x512xf32>, tensor<512x3x3x512xf32>, tens
or<512xf32>) -> tensor<?x4x4x512xf32>
  %48 = ""tfl.transpose""(%47, %cst_2) : (tensor<?x4x4x512xf32>, tensor<4xi32>) -> tensor<?x512x4x4xf32>
  %49 = ""tfl.transpose""(%48, %cst_3) : (tensor<?x512x4x4xf32>, tensor<4xi32>) -> tensor<?x4x4x512xf32>
  %50 = ""tfl.max_pool_2d""(%49) {filter_height = 2 : i32, filter_width = 2 : i32, fused_activation_function = ""NONE"", padding =
""VALID"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<?x4x4x512xf32>) -> tensor<?x2x2x512xf32>
  %51 = ""tfl.transpose""(%50, %cst_2) : (tensor<?x2x2x512xf32>, tensor<4xi32>) -> tensor<?x512x2x2xf32>
  %52 = ""tfl.pad""(%51, %cst) : (tensor<?x512x2x2xf32>, tensor<4x2xi32>) -> tensor<?x512x4x4xf32>
  %53 = ""tfl.transpose""(%52, %cst_3) : (tensor<?x512x4x4xf32>, tensor<4xi32>) -> tensor<?x4x4x512xf32>
  %54 = ""tfl.split""(%cst_1, %53) {num_splits = 1 : i32} : (tensor<i32>, tensor<?x4x4x512xf32>) -> tensor<?x4x4x512xf32>
  %55 = ""tfl.conv_2d""(%54, %cst_16, %cst_26) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_functi
on = ""RELU"", padding = ""VALID"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<?x4x4x512xf32>, tensor<512x3x3x512xf32>, tens
or<512xf32>) -> tensor<?x2x2x512xf32>
  %56 = ""tfl.transpose""(%55, %cst_2) : (tensor<?x2x2x512xf32>, tensor<4xi32>) -> tensor<?x512x2x2xf32>
  %57 = ""tfl.pad""(%56, %cst) : (tensor<?x512x2x2xf32>, tensor<4x2xi32>) -> tensor<?x512x4x4xf32>
  %58 = ""tfl.transpose""(%57, %cst_3) : (tensor<?x512x4x4xf32>, tensor<4xi32>) -> tensor<?x4x4x512xf32>
  %59 = ""tfl.split""(%cst_1, %58) {num_splits = 1 : i32} : (tensor<i32>, tensor<?x4x4x512xf32>) -> tensor<?x4x4x512xf32>
  %60 = ""tfl.conv_2d""(%59, %cst_17, %cst_27) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_functi
on = ""RELU"", padding = ""VALID"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<?x4x4x512xf32>, tensor<512x3x3x512xf32>, tens
or<512xf32>) -> tensor<?x2x2x512xf32>
  %61 = ""tfl.transpose""(%60, %cst_2) : (tensor<?x2x2x512xf32>, tensor<4xi32>) -> tensor<?x512x2x2xf32>
  %62 = ""tfl.transpose""(%61, %cst_3) : (tensor<?x512x2x2xf32>, tensor<4xi32>) -> tensor<?x2x2x512xf32>
  %63 = ""tfl.max_pool_2d""(%62) {filter_height = 2 : i32, filter_width = 2 : i32, fused_activation_function = ""NONE"", padding =
""VALID"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<?x2x2x512xf32>) -> tensor<?x1x1x512xf32>
  %64 = ""tfl.transpose""(%63, %cst_2) : (tensor<?x1x1x512xf32>, tensor<4xi32>) -> tensor<?x512x1x1xf32>
  %65 = ""tfl.pad""(%64, %cst_30) : (tensor<?x512x1x1xf32>, tensor<4x2xi32>) -> tensor<?x512x1x1xf32>
  %66 = ""tfl.transpose""(%65, %cst_3) : (tensor<?x512x1x1xf32>, tensor<4xi32>) -> tensor<?x?x?x?xf32>
  %67 = ""tfl.average_pool_2d""(%66) {filter_height = 1 : i32, filter_width = 1 : i32, fused_activation_function = ""NONE"", paddin
g = ""VALID"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
  %68 = ""tfl.transpose""(%67, %cst_2) : (tensor<?x?x?x?xf32>, tensor<4xi32>) -> tensor<?x?x?x?xf32>
  %69 = ""tfl.shape""(%68) : (tensor<?x?x?x?xf32>) -> tensor<4xi64>
  %70 = ""tfl.gather""(%69, %cst_4) {axis = 0 : i32} : (tensor<4xi64>, tensor<i64>) -> tensor<i64>
  %71 = ""tfl.reshape""(%70, %cst_29) : (tensor<i64>, tensor<1xi32>) -> tensor<1xi64>
  %72 = ""tfl.concatenation""(%71, %cst_5) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<1xi64>, tensor<1xi64>)
-> tensor<2xi64>
  %73 = ""tfl.equal""(%72, %cst_4) : (tensor<2xi64>, tensor<i64>) -> tensor<2xi1>
  %74 = ""tfl.where""(%73) : (tensor<2xi1>) -> tensor<?x1xi64>
  %75 = ""tfl.squeeze""(%74) {squeeze_dims = [-1]} : (tensor<?x1xi64>) -> tensor<?xi64>
  %76 = ""tfl.gather""(%69, %75) {axis = 0 : i32} : (tensor<4xi64>, tensor<?xi64>) -> tensor<?xi64>
  %77 = ""tfl.sparse_to_dense""(%74, %cst_0, %76, %cst_4) : (tensor<?x1xi64>, tensor<1xi64>, tensor<?xi64>, tensor<i64>) -> tenso
r<2xi64>
  %78 = ""tf.AddV2""(%72, %77) {device = """"} : (tensor<2xi64>, tensor<2xi64>) -> tensor<2xi64>
  %79 = ""tfl.cast""(%78) : (tensor<2xi64>) -> tensor<2xi32>
  %80 = ""tfl.reshape""(%68, %79) : (tensor<?x?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
  %81 = ""tfl.shape""(%80) : (tensor<?x?xf32>) -> tensor<2xi32>
  %82 = ""tfl.strided_slice""(%81, %cst_28, %cst_29, %cst_29) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32,
 new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<
i32>
  %83 = ""tfl.pack""(%82, %cst_1) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %84 = ""tfl.reshape""(%68, %83) : (tensor<?x?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
  %85 = ""tfl.fully_connected""(%84, %cst_6, %cst_7) {fused_activation_function = ""NONE"", keep_num_dims = false, weights_format =
 ""DEFAULT""} : (tensor<?x?xf32>, tensor<10x512xf32>, tensor<10xf32>) -> tensor<?x10xf32>
  ""std.return""(%85) : (tensor<?x10xf32>) -> ()
}) {sym_name = ""main"", tf.entry_function = {control_outputs = """", inputs = ""input"", outputs = ""Identity""}, type = (tensor<?x3x3
2x32xf32>) -> tensor<?x10xf32>} : () -> ()

But the issue is with AddV2


--Under nightly
error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
	tf.AddV2 {device = """"}
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\lite\python\convert.py"", line 213, in toco_convert_protos
    enable_mlir_converter)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\lite\python\wrap_toco.py"", line 38, in wrapped_toco_convert
    enable_mlir_converter)
Exception: <unknown>:0: error: loc(callsite(callsite(""add_10@__inference___call___388"" at ""PartitionedCall@__inference_signature_wrapper_441"") at ""PartitionedCall"")): 'tf.AddV2' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""PartitionedCall""): called from
<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
	tf.AddV2 {device = """"}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/macin/Documents/GitHub/torchToTFLite/converter.py"", line 49, in <module>
    tflite_model = converter.convert()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\lite\python\lite.py"", line 739, in convert
    result = _convert_saved_model(**converter_kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\lite\python\convert.py"", line 637, in convert_saved_model
    enable_mlir_converter=True)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\lite\python\convert.py"", line 216, in toco_convert_protos
    raise ConverterError(str(e))
tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(callsite(callsite(""add_10@__inference___call___388"" at ""PartitionedCall@__inference_signature_wrapper_441"") at ""PartitionedCall"")): 'tf.AddV2' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""PartitionedCall""): called from
<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
	tf.AddV2 {device = """"}
```

**Also, please include a link to the saved model or GraphDef**

```
https://drive.google.com/file/d/1UPXTbUaXzWDzHJUz8W7plO_L6lgNCNVD/view?usp=sharing
```

**Failure details**
The saved model converted from PyTorch contains ops AddV2 and pad. Pad caused problems in 2.2.0 while AddV2 causes problems in 2.3.0 and nightly.

"
44693,module 'tensorflow_lite_support.metadata.metadata_schema_py_generated' has no attribute 'ModelMetadataT' error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not used
- TensorFlow installed from (source or binary): Spyder in Anagonda Navigator (It was preinstalled)
- TensorFlow version (use command below): Tensorflow 2
- Python version: 3.8
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: None
- GPU model and memory: Not using GPU

**Describe the current behavior**
I have tried to run the sample code from https://www.tensorflow.org/lite/convert/metadata in anaconda prompt, which is

python ./metadata_writer_for_image_classifier.py \
    --model_file=./model_without_metadata/mobilenet_v1_0.75_160_quantized.tflite \
    --label_file=./model_without_metadata/labels.txt \
    --export_directory=model_with_metadata

I have downloaded ""mobilenet_v1_0.75_160_quantized.tflite"" file and placed it in Desktop/model_without_metadata, downloaded this file (https://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/metadata/metadata_writer_for_image_classifier.py) and placed it in Desktop as well.  By running this code, it gives me
module 'tensorflow_lite_support.metadata.metadata_schema_py_generated' has no attribute 'ModelMetadataT'
error.
How can I solve this?

**Describe the expected behavior**
Metadata should be added to mobilenet_v1_0.75_160_quantized.tflite in order to use it in MLkit for object classification.

"
44692,crosstool_wrapper_driver_is_not_gcc failed: error executing command,"<em>
</em>

**System information**
- Ubuntu20.04.1

- tensorflow source (master branch):
- TensorFlow version:
- Python version:3.8.5

- 3.1.0n (if compiling from source):
- GCC/Compiler version (if compiling from source):9.2
- CUDA/cuDNN version:8
- GPU model and memory:24GB
- Graph Card Diver:455.23.04
- NVIDIA Card:RTX3090
- TensorRT:TensorRT-7.2.1.6



**bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures
**

**/home/liushuai/tensorflow/tensorflow/core/grappler/costs/BUILD:146:1: C++ compilation of rule '//tensorflow/core/grappler/costs:robust_stats' failed (Exit 127): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/liushuai/.cache/bazel/_bazel_liushuai/aa8adcb58cbbd827a6e521843f232b82/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 \
    PATH=/usr/local/cuda-11.0/bin:/usr/local/python-3.7.9/bin:/usr/local/cuda-11.0/bin:/usr/local/cuda-11.0/bin:/usr/local/cuda-11.0/bin:/home/liushuai/miniconda3/bin:/home/liushuai/miniconda3/condabin:/usr/local/cuda-11.0/bin:/home/liushuai/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/liushuai/.fzf/bin:/usr/local/go/bin:/usr/local/python-3.7.9/bin:/usr/local/go/bin:/usr/local/python-3.7.9/bin:/usr/local/go/bin:/usr/local/python-3.7.9/bin:/usr/local/go/bin:/usr/local/go/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=8.6 \
    TF_CUDA_PATHS=/usr/local/cuda-11.0,/usr/local/cuda-11.0/targets/x86_64-linux,/usr/local/TensorRT-7.2.1.6 \
    TF_CUDA_VERSION=11 \
    TF_CUDNN_VERSION=8 \
    TF_NCCL_VERSION='' \
    TF_NEED_CUDA=1 \
    TF_NEED_TENSORRT=1 \
    TF_TENSORRT_VERSION=7 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/grappler/costs/_objs/robust_stats/robust_stats.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/grappler/costs/_objs/robust_stats/robust_stats.pic.o' -iquote . -iquote bazel-out/k8-opt/bin -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS '-march=native' -Wno-sign-compare '-std=c++14' -c tensorflow/core/grappler/costs/robust_stats.cc -o bazel-out/k8-opt/bin/tensorflow/core/grappler/costs/_objs/robust_stats/robust_stats.pic.o)
Execution platform: @local_execution_config_platform//:platform
/usr/bin/env: 'python': No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/liushuai/tensorflow/tensorflow/lite/toco/python/BUILD:88:1 C++ compilation of rule '//tensorflow/core/grappler/costs:robust_stats' failed (Exit 127): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/liushuai/.cache/bazel/_bazel_liushuai/aa8adcb58cbbd827a6e521843f232b82/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 \
    PATH=/usr/local/cuda-11.0/bin:/usr/local/python-3.7.9/bin:/usr/local/cuda-11.0/bin:/usr/local/cuda-11.0/bin:/usr/local/cuda-11.0/bin:/home/liushuai/miniconda3/bin:/home/liushuai/miniconda3/condabin:/usr/local/cuda-11.0/bin:/home/liushuai/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/liushuai/.fzf/bin:/usr/local/go/bin:/usr/local/python-3.7.9/bin:/usr/local/go/bin:/usr/local/python-3.7.9/bin:/usr/local/go/bin:/usr/local/python-3.7.9/bin:/usr/local/go/bin:/usr/local/go/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=8.6 \
    TF_CUDA_PATHS=/usr/local/cuda-11.0,/usr/local/cuda-11.0/targets/x86_64-linux,/usr/local/TensorRT-7.2.1.6 \
    TF_CUDA_VERSION=11 \
    TF_CUDNN_VERSION=8 \
    TF_NCCL_VERSION='' \
    TF_NEED_CUDA=1 \
    TF_NEED_TENSORRT=1 \
    TF_TENSORRT_VERSION=7 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/grappler/costs/_objs/robust_stats/robust_stats.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/grappler/costs/_objs/robust_stats/robust_stats.pic.o' -iquote . -iquote bazel-out/k8-opt/bin -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS '-march=native' -Wno-sign-compare '-std=c++14' -c tensorflow/core/grappler/costs/robust_stats.cc -o bazel-out/k8-opt/bin/tensorflow/core/grappler/costs/_objs/robust_stats/robust_stats.pic.o)**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44691,Performance regression when MKL is enabled,"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04.5

- TensorFlow installed from (source or binary):
Binary

- TensorFlow version (use command below):
1.13.2 and 1.14.0

**Describe the current behavior**
Train a ssdlite_mobilenet_v2 model with 1.13.2 and 1.14.0, the inference speed is very different. 1.14.0 one is about 50% to 100% slow with MKL enabled. When MKL is disabled, there is no obvious difference.

benchmark_model shows:
1.13.2
<pre>
             _MklFusedConv2D           55       13.732      43.384%     43.384%  45625.535         55
         NonMaxSuppressionV3           27        5.386      17.016%     60.401%     10.800         27
    _MklDepthwiseConv2dNative          33        3.161       9.987%     70.387%  20275.297         33
                    _MklToTf           72        2.027       6.404%     76.791%  23987.072         72
                         Mul           51        1.783       5.633%     82.424%     15.336         51
                         Add           81        1.772       5.598%     88.023%      0.016         81
                   _MklRelu6           33        0.731       2.309%     90.332%     30.096         33
                        NoOp            1        0.462       1.460%     91.792%      0.000          1
                        Cast            6        0.383       1.210%     93.002%   1104.316          6
                       Slice           33        0.272       0.859%     93.861%    414.072         33
                    GatherV2           69        0.227       0.717%     94.579%    253.200         69
                       Const          614        0.180       0.569%     95.147%      0.000        688
                _MklConcatV2            6        0.168       0.531%     95.678%    358.848          6
        TensorArrayScatterV3            5        0.151       0.477%     96.155%   1317.728          5
                         Sub           44        0.133       0.420%     96.575%     15.336         44
                     Sigmoid            1        0.132       0.417%     96.992%      0.000          1
                      TopKV2            2        0.117       0.370%     97.362%     43.200          2
                StridedSlice           54        0.076       0.240%     97.602%      0.088         54
                       Shape           50        0.058       0.183%     97.785%      0.344         50
         TensorArrayGatherV3            6        0.057       0.180%     97.965%   1082.416          6
               TensorArrayV3           11        0.051       0.161%     98.127%      2.596         11
                        Pack           24        0.046       0.145%     98.272%     30.956         24
              ResizeBilinear            1        0.045       0.142%     98.414%   1080.000          1
                    ConcatV2           27        0.039       0.123%     98.537%     10.800         27
                      Select           33        0.038       0.120%     98.657%      0.000         33
                        Less           31        0.038       0.120%     98.777%      2.704         35
                       Enter           26        0.036       0.114%     98.891%      0.000         26
                       Merge           10        0.035       0.111%     99.002%      0.040         20
                        Fill           30        0.031       0.098%     99.100%     11.204         30
                   ZerosLike           29        0.030       0.095%     99.194%     10.808         29
                     Reshape           74        0.028       0.088%     99.283%      0.000         74
                       Split            3        0.026       0.082%     99.365%    129.600          3
                      Unpack            5        0.023       0.073%     99.438%     92.044          5
               _MklTranspose            3        0.020       0.063%     99.501%     92.016          3
                      Switch           10        0.019       0.060%     99.561%      0.000         20
                       Range           14        0.018       0.057%     99.618%      1.244         14
               NextIteration           10        0.014       0.044%     99.662%      0.000         10
                     Squeeze            4        0.010       0.032%     99.694%      0.000          4
          TensorArrayWriteV3            6        0.009       0.028%     99.722%      0.000          6
                     Minimum            6        0.009       0.028%     99.750%      0.000          6
                     Greater            6        0.009       0.028%     99.779%      2.705          6
                  ExpandDims            7        0.009       0.028%     99.807%      0.000          7
                     _Retval            4        0.007       0.022%     99.829%      0.000          4
                         Exp            2        0.007       0.022%     99.852%      0.000          2
           TensorArraySizeV3            6        0.006       0.019%     99.870%      0.024          6
                     Maximum            4        0.006       0.019%     99.889%      0.000          4
                       Where            1        0.005       0.016%     99.905%     21.600          1
           TensorArrayReadV3            5        0.005       0.016%     99.921%      0.000          5
                         Sum            1        0.004       0.013%     99.934%      0.004          1
                  LogicalAnd            2        0.004       0.013%     99.946%      0.000          4
                         Pad            3        0.003       0.009%     99.956%      0.000          3
                GreaterEqual            1        0.003       0.009%     99.965%      2.700          1
                       Equal            2        0.003       0.009%     99.975%      0.002          2
                        Size            2        0.002       0.006%     99.981%      0.008          2
                    LoopCond            2        0.002       0.006%     99.987%      0.000          4
                      Assert            3        0.002       0.006%     99.994%      0.000          3
                        _Arg            1        0.001       0.003%     99.997%      0.000          1
                        Tile            1        0.001       0.003%    100.000%      0.000          1
                    Identity            4        0.000       0.000%    100.000%      0.000          4           
                        Exit            6        0.000       0.000%    100.000%      0.000          6
Timings (microseconds): count=283 first=29518 curr=32211 min=29448 max=39857 avg=32586.2 std=1784
</pre>

1.14.0
<pre>
                  _MklConv2D           43       12.516      28.381%     28.381%  43239.328         43
    _MklDepthwiseConv2dNative          33        7.580      17.188%     45.569%  20275.297         33
         NonMaxSuppressionV3           27        5.294      12.005%     57.574%     10.800         27  
                         Add          114        5.088      11.537%     69.111%      0.016        114 
                    _MklToTf          100        4.871      11.045%     80.156%  52036.352        100 
                   _MklRelu6           59        2.116       4.798%     84.955%     53.808         59  
                         Mul           51        1.774       4.023%     88.977%     15.336         51  
             _MklFusedConv2D           12        1.282       2.907%     91.884%   2386.208         12  
                        NoOp            1        0.563       1.277%     93.161%      0.000          1   
                        Cast            6        0.374       0.848%     94.009%   1104.316          6   
                       Slice           33        0.294       0.667%     94.676%    414.072         33  
                       Const          601        0.293       0.664%     95.340%      0.000        677 
                    GatherV2           69        0.232       0.526%     95.866%    253.200         69  
                _MklConcatV2            6        0.174       0.395%     96.261%    358.848          6   
        TensorArrayScatterV3            5        0.173       0.392%     96.653%   1317.728          5   
                        AddN           10        0.142       0.322%     96.975%      0.000         10  
                         Sub           44        0.138       0.313%     97.288%     15.336         44  
                     Sigmoid            1        0.132       0.299%     97.587%      0.000          1   
                      TopKV2            2        0.121       0.274%     97.862%     43.200          2   
                StridedSlice           54        0.085       0.193%     98.054%      0.088         54  
                     Reshape           74        0.063       0.143%     98.197%      0.000         74  
         TensorArrayGatherV3            6        0.060       0.136%     98.333%   1082.416          6   
                       Shape           50        0.056       0.127%     98.460%      0.344         50  
              ResizeBilinear            1        0.055       0.125%     98.585%   1080.000          1   
                    ConcatV2           27        0.053       0.120%     98.705%     10.800         27  
               TensorArrayV3           11        0.052       0.118%     98.823%      2.596         11  
                        Pack           24        0.051       0.116%     98.939%     30.956         24  
                      Select           33        0.043       0.098%     99.036%      0.000         33  
                       Enter           26        0.042       0.095%     99.132%      0.000         26  
                        Less           31        0.040       0.091%     99.222%      2.704         35  
                       Merge           10        0.037       0.084%     99.306%      0.040         20  
                        Fill           30        0.032       0.073%     99.379%     11.204         30  
                   ZerosLike           29        0.031       0.070%     99.449%     10.808         29  
                       Split            3        0.027       0.061%     99.510%    129.600          3   
                      Unpack            5        0.025       0.057%     99.567%     92.044          5   
                      Switch           10        0.022       0.050%     99.617%      0.000         20  
               _MklTranspose            3        0.020       0.045%     99.662%     92.016          3   
                       Range           14        0.017       0.039%     99.701%      1.244         14  
               NextIteration           10        0.017       0.039%     99.739%      0.000         10  
          TensorArrayWriteV3            6        0.012       0.027%     99.766%      0.000          6   
                     Squeeze            4        0.010       0.023%     99.789%      0.000          4   
                     Minimum            6        0.009       0.020%     99.810%      0.000          6   
                     Greater            6        0.009       0.020%     99.830%      2.705          6   
                  ExpandDims            7        0.009       0.020%     99.850%      0.000          7   
                     _Retval            4        0.008       0.018%     99.868%      0.000          4   
           TensorArrayReadV3            5        0.007       0.016%     99.884%      0.000          5   
                         Exp            2        0.007       0.016%     99.900%      0.000          2   
           TensorArraySizeV3            6        0.006       0.014%     99.914%      0.024          6   
                  LogicalAnd            2        0.006       0.014%     99.927%      0.000          4   
                       Where            1        0.005       0.011%     99.939%     21.600          1   
                     Maximum            4        0.005       0.011%     99.950%      0.000          4   
                         Sum            1        0.004       0.009%     99.959%      0.004          1   
                         Pad            3        0.003       0.007%     99.966%      0.000          3   
                GreaterEqual            1        0.003       0.007%     99.973%      2.700          1   
                       Equal            2        0.003       0.007%     99.980%      0.002          2   
                        Size            2        0.002       0.005%     99.984%      0.008          2   
                    LoopCond            2        0.002       0.005%     99.989%      0.000          4   
                      Assert            2        0.002       0.005%     99.993%      0.000          2   
                        _Arg            1        0.001       0.002%     99.995%      0.000          1   
                        Tile            1        0.001       0.002%     99.998%      0.000          1   

Timings (microseconds): count=207 first=39464 curr=48545 min=38998 max=65787 avg=45005.9 std=4312
</pre>"
44689,[Docs should enhance] The docs not tell me: How does a TPU cluster allocate memory for the model?,"My issue in short words, I'm creating a model just a single LSTM layer (`layers.LSTM(units=16298, input_shape=(1, 16298))`), and my google cloud vm connect to the `v3-8` tpu which has total 128GB RAM, why I still got `ResourceExhaustedError` exception? It seems only use one TPU core (16GB RAM) to create model, not the total (16 * 8 = 128GB).

Can some one tell me: How does a TPU cluster allocate memory for the model?

This is my issue details: https://stackoverflow.com/questions/64737657/how-much-memory-do-i-need-for-my-lstm-model"
44688,Memory leak in TF 2.3.1 when using Multi-worker training,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Tensorflow 2.3.1 container
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Tensorflow 2.3.1 container
- TensorFlow version (use command below): Tensorflow 2.3.1 container
- Python version: Tensorflow 2.3.1 container

**Describe the current behaviour**
When using MultiWorkerMirroredStrategy, there is a memory leak happening during training. The memory leak is not present when only one node is used.

Here is the memory consumption of a single node trainning
<img width=""1641"" alt=""single"" src=""https://user-images.githubusercontent.com/10571507/98474204-94146200-21f5-11eb-85bf-1e1c02f196f0.png"">

Here is the memory consumption of multi-node training (1 chief and 1 worker)
<img width=""1643"" alt=""chief"" src=""https://user-images.githubusercontent.com/10571507/98474883-c625c400-21f5-11eb-9803-c0f529f717b5.png"">

<img width=""1635"" alt=""worker"" src=""https://user-images.githubusercontent.com/10571507/98474979-dd64b180-21f5-11eb-80c8-d2b1e34c7fc4.png"">

The code is identical for both cases. The only thing that changes is the TF_CONFIG which is provided be GCP AI Platform.

**Describe the expected behaviour**

The memory consumption should not be increasing by time (epochs).

**Standalone code to reproduce the issue**
The code is available at [this](https://github.com/kalosisz/tf-gcp-multi-worker-example) repo.

"
44687,How can I use micro_mutable_op_resolver.h instead of all_ops_resolver.h to run TensorFlow Lite for Microcontrollers exapmle?,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows subsystem for Linux, Ubuntu 18.04
- TensorFlow installed from (source or binary): 
- Tensorflow version (commit SHA if source): 
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): My own PC

**Describe the problem**

I'm running test on the hello_world example in TensorFlow Lite for Microcontrollers, on my own develop machine.
In [https://www.tensorflow.org/lite/microcontrollers/library](https://www.tensorflow.org/lite/microcontrollers/library) I found an option to use **micro_mutable_op_resolver.h** instead of **all_ops_resolver.h**, so that I can only pull in the ops I need.
I've tried to change some lines in hello_world_test.cc, but still can't get it right.

**Please provide the exact sequence of commands/steps when you ran into the problem**

I've change these lines in tensorflow\lite\micro\examples\hello_world\hello_world_test.cc:
line 16: 
`#include ""tensorflow/lite/micro/all_ops_resolver.h""`
to : 
`#include ""tensorflow/lite/micro/micro_mutable_op_resolver.h""`
line 41: 
`tflite::AllOpsResolver resolver;`
to: 
```
const unsigned int tOpCount = 5;
tflite::MicroMutableOpResolver<tOpCount> resolver;
```

then I run the command: 
`sudo make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test `
But It seems to pull in and compile all ops. Also, after printing out""~~~ALL TESTS PASSED~~~'"", the process doesn't end.

Could anyone help me please?

"
44686,Typo in tf.keras.callbacks.EarlyStopping example,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping

## Description of issue (what needs changing):
The following is from the `EarlyStopping` page on TensorFlow website.

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)

This callback will stop the training when there is no improvement in the validation loss for three consecutive epochs.

In the above line, it should have been ""loss"" instead of ""validation loss"". 

### Clear description
Check above. Please check [this gist](https://colab.research.google.com/drive/1IL5tVIR_ulSeUQeBtDgrj0F5vZm7drYE#scrollTo=L9UTqnI3qd08) for an example.

### Correct links
https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping

### Submit a pull request?     
Yes


"
44685,Arguments have an HTML format issue,"## URL(s) with the issue: https://www.tensorflow.org/api_docs/python/tf/image/extract_patches

The arguments need to be reformatted:

![Screen Shot 2020-11-08 at 17 28 21](https://user-images.githubusercontent.com/8287111/98470714-dafb5b00-21e7-11eb-8221-96eb105b5e52.png)

"
44683,"Not creating XLA devices, tf_xla_enable_xla_devices not set","Hi,

I have recently upgraded my system to the following configuration:

OS: ubuntu 18.04
gcc: 7.5.0
cuda: 10.2
cuDNN:7.6.5
TensorRT: 6.0.1.8
Tensorflow:2.5.0
My GPU spec: device: 0, name: GeForce GTX 1060 6GB

Once Tensorflow installation is completed, i checked the following cpde:

`with tf.device('/gpu:0'):
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
with tf.Session() as sess:
   print (sess.run(c))
`

When I execute it in a terminal, I find the following:

`>>> import tensorflow as tf
2020-11-08 13:00:32.053030: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2
>>> with tf.device('/gpu:0'):
...     a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
...     b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
...     c = tf.matmul(a, b)
... 
2020-11-08 13:00:33.123388: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-11-08 13:00:33.123967: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2020-11-08 13:00:33.137540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-08 13:00:33.137915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1
coreClock: 1.7085GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s
2020-11-08 13:00:33.137933: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2
2020-11-08 13:00:33.139254: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2020-11-08 13:00:33.139295: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2020-11-08 13:00:33.140475: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2020-11-08 13:00:33.140641: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2020-11-08 13:00:33.141883: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2020-11-08 13:00:33.142541: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2020-11-08 13:00:33.145144: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2020-11-08 13:00:33.145247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-08 13:00:33.145551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-08 13:00:33.145778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1866] Adding visible gpu devices: 0
2020-11-08 13:00:33.146034: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-11-08 13:00:33.146315: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-11-08 13:00:33.146377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-08 13:00:33.146602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1
coreClock: 1.7085GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s
2020-11-08 13:00:33.146616: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2
2020-11-08 13:00:33.146645: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2020-11-08 13:00:33.145247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-08 13:00:33.145551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-08 13:00:33.145778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1866] Adding visible gpu devices: 0
2020-11-08 13:00:33.146034: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-11-08 13:00:33.146315: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-11-08 13:00:33.146377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

`

I would like to know how to resolve the `xla_devices not set` and `SysFS had negative value (-1)`issues.

any suggestions?

regards,"
44682,Tensorflow support for Nvidia geforce rtx 3090 - CUDA-11.1,"The rtx 3090 has been a beast in deeplearning performance and yet tensorflow has no support for training on CUDA-11.1 and cuDNN-8.

it would be great if added soon."
44681,How to implement custom delegate that supports dynamic-sized tensor,"Hi,

I'm implementing custom TFLite delegation for my accelerator.
My implemented the delegation largely referring [dummy delegate example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/utils/dummy_delegate)
But when I modify the graph using my delegate, I got following error.
```
ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.
```
So I want to enable dynamic-sized tensor support in my delegate.
But I couldn't find any guidelines for that.

**What should I do to support dynamic-sized tensors in my custom delegate?**
Do tensorflow have such an example?

Thanks in advance!"
44680,Install tf-nightly 2.0,"I need install ```tf-nightly 2.0```.  Where can I find older versions of the library?. On this site https://pypi.org/project/tf-nightly/#history the oldest is 2.4.0
"
44678,How to install tensorflow gpu on rtx 3070,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):both
- TensorFlow version:2.3.0
- Python version:3.7
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):3.1.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:11.1;8.0
- GPU model and memory:rtx 3070 8gb



**Describe the problem**
 I have brought a rtx 3070 and I want to use tensorflow gpu on it. To use it I need to use cuda 11.1
**Provide the exact sequence of commands / steps that you executed before running into the problem**
So tried building from source but that did not work , issue is here https://github.com/tensorflow/tensorflow/issues/44671. And I also tried using tf nightly but that has a bug in it of numpy falling sanity check.


"
44677,"Fail for tf.image.flip_left_right(img_input) where im_input is Input(shape=(None, None, 3), name='image')","I want to dump graph of tta for model.h5. But seems tf.image.flip_left_right not support dynamic input size. (tf version 2.3)

    import tensorflow as tf 
    from tensorflow.keras.layers import Input
    img_input = Input(shape=(None, None, 3), name='image')  
    tf.image.flip_left_right(img_input)    


OperatorNotAllowedInGraphError            Traceback (most recent call last)
<ipython-input-4-6faed2643326> in <module>()
----> 1 tf.image.flip_left_right(img_input)

/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    199     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    200     try:
--> 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a

/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py in flip_left_right(image)
    489     ValueError: if the shape of `image` not supported.
    490   """"""
--> 491   return _flip(image, 1, 'flip_left_right')
    492
    493

/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py in _flip(image, flip_index, scope_name)
    548   with ops.name_scope(None, scope_name, [image]):
    549     image = ops.convert_to_tensor(image, name='image')
--> 550     image = _AssertAtLeast3DImage(image)
    551     shape = image.get_shape()
    552     if shape.ndims == 3 or shape.ndims is None:

/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py in _AssertAtLeast3DImage(image)
    195   """"""
    196   return control_flow_ops.with_dependencies(
--> 197       _CheckAtLeast3DImage(image, require_static=False), image)
    198
    199

/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py in _CheckAtLeast3DImage(image, require_static)
    230         check_ops.assert_positive(
    231             array_ops.shape(image)[-3:],
--> 232             [""inner 3 dims of 'image.shape' ""
    233              'must be > 0.']),
    234         check_ops.assert_greater_equal(

/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    199     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    200     try:
--> 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a

/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py in assert_positive(x, data, summarize, message, name)
    510           'x (%s) = ' % name, x]
    511     zero = ops.convert_to_tensor(0, dtype=x.dtype)
--> 512     return assert_less(zero, x, data=data, summarize=summarize)
    513
    514

/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    199     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    200     try:
--> 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a

/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py in assert_less(x, y, data, summarize, message, name)
    899 def assert_less(x, y, data=None, summarize=None, message=None, name=None):
    900   return _binary_assert('<', 'assert_less', math_ops.less, np.less, x, y, data,
--> 901                         summarize, message, name)
    902
    903

/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py in _binary_assert(sym, opname, op_func, static_func, x, y, data, summarize, message, name)
    333       test_op = op_func(x, y)
    334       condition = math_ops.reduce_all(test_op)
--> 335       if condition:
    336         return
    337

/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __bool__(self)
    875       `TypeError`.
    876     """"""
--> 877     self._disallow_bool_casting()
    878
    879   def __nonzero__(self):

/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _disallow_bool_casting(self)
    488     else:
    489       # Default: V1-style Graph execution.
--> 490       self._disallow_in_graph_mode(""using a `tf.Tensor` as a Python `bool`"")
    491
    492   def _disallow_iteration(self):

/home/gezi/env/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _disallow_in_graph_mode(self, task)
    477     raise errors.OperatorNotAllowedInGraphError(
    478         ""{} is not allowed in Graph execution. Use Eager execution or decorate""
--> 479         "" this function with @tf.function."".format(task))
    480
    481   def _disallow_bool_casting(self):

OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.

"
44675,Bad performance of tf.data.Dataset API when dealing with windowed inputs / timeseries problems / 3D arrays,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): 2.4.0-dev20201006
Python version: 3.7

**Describe the current behavior**
I work on timeserie forecasting, involving manipulating 3-D arrays as inputs of my models (batch_size, nb_timesteps, nb_features).
I used to rely on Keras ""Sequence"" objects to feed the ""fit"" function.
Yet, I recently chose to move on tf.data.Dataset API to have a more robust input pipeline. In addition, this allow using named features, which is a huge advantage. Nonetheless, I found that usage and performances of tf datasets could be improved when it comes to window data:

1.  windowing a dataset of named features is slow and not very intuitive. I thought this could maybe be improved (see issue https://github.com/tensorflow/tensorflow/issues/43703)

2. datasets of windows tend to be heavy. While shuffling, the dataset ""shuffle"" method necessarily pre-buffers all samples to use for training, resulting in huge memory usage, which makes training of such models not tractable with the Dataset API. Note that for timeseries problems, as we start from chronologically-ordered data, it is very important to get a ""perfect"" shuffling, hence we cannot reduce the shuffle buffer size (it is necessary to set it equal or greater than the dataset cardinality). I was thinking: maybe the ""Shuffle"" method could be improved so that it does not pre-buffer all possible samples, but rather does the following:
           - index the whole dataset, in case cardinality can be known
           - shuffle the index list
           - pick-up a new batch based on the shuffled index list, at each training step
This would be very similar to the way the Keras Sequence works. However, I think the Keras Sequence lack a lot of other interesting features brought by the Dataset API. So it would be good if we could get the best of the two in the Dataset API, which now seems to be the standard for data input pipelines in tf.

**Describe the expected behavior**
improved performance and architecture for windowed datasets, using tf.data.Dataset API"
44674,metrics=[tf.keras.metrics.Accuracy()] gives ValueError,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
**Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**Manjaro 20.2 Nibia, Kernel: x86_64 Linux 5.8.18-1-MANJARO**
- TensorFlow installed from (source or binary):
**pip**
- TensorFlow version (use command below):
**2.3.1**
- Python version:
**3.8.6**


**Describe the current behavior**
metrics=[tf.keras.metrics.Accuracy()] gives ValueError(""Shapes %s and %s are incompatible"" % (self, other))
while metrics=['accuracy'] works.

**Describe the expected behavior**
metrics=[tf.keras.metrics.Accuracy()]  should work similar to metrics=['accuracy'] 

**Standalone code to reproduce the issue**
```
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(10)
])
model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[tf.keras.metrics.Accuracy()]
)
model.fit(x_train, y_train, epochs=5)
```

**Other info / logs**:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-24-af322e093f31> in <module>
      1 #Train the model
----> 2 model.fit(x_train, y_train, epochs=5)

/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--> 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1096                 batch_size=batch_size):
   1097               callbacks.on_train_batch_begin(step)
-> 1098               tmp_logs = train_function(iterator)
   1099               if data_handler.should_sync:
   1100                 context.async_wait()

/usr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = ""nonXla""
--> 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

/usr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    821       # This is the first call of __call__, so we have to initialize.
    822       initializers = []
--> 823       self._initialize(args, kwds, add_initializers_to=initializers)
    824     finally:
    825       # At this point we know that the initialization is complete (or less

/usr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    694     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)
    695     self._concrete_stateful_fn = (
--> 696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
    697             *args, **kwds))
    698 

/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2853       args, kwargs = None, None
   2854     with self._lock:
-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2856     return graph_function
   2857 

/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-> 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3063     arg_names = base_arg_names + missing_arg_names
   3064     graph_function = ConcreteFunction(
-> 3065         func_graph_module.func_graph_from_py_func(
   3066             self._name,
   3067             self._python_function,

/usr/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--> 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    599         # the function a weak reference to itself to avoid a reference cycle.
--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    601     weak_wrapped_fn = weakref.ref(wrapped_fn)
    602 

/usr/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    971           except Exception as e:  # pylint:disable=broad-except
    972             if hasattr(e, ""ag_error_metadata""):
--> 973               raise e.ag_error_metadata.to_exception(e)
    974             else:
    975               raise

ValueError: in user code:

    /usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *
        return step_function(self, iterator)
    /usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **
        outputs = model.train_step(data)
    /usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:759 train_step
        self.compiled_metrics.update_state(y, y_pred, sample_weight)
    /usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:409 update_state
        metric_obj.update_state(y_t, y_p, sample_weight=mask)
    /usr/lib/python3.8/site-packages/tensorflow/python/keras/utils/metrics_utils.py:90 decorated
        update_op = update_state_fn(*args, **kwargs)
    /usr/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py:176 update_state_fn
        return ag_update_state(*args, **kwargs)
    /usr/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py:612 update_state  **
        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)
    /usr/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py:3208 accuracy  **
        y_pred.shape.assert_is_compatible_with(y_true.shape)
    /usr/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:1134 assert_is_compatible_with
        raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))

    ValueError: Shapes (32, 10) and (32, 1) are incompatible
```
"
44673,Separate TF 1.x from 2.x in PyPi,"Let's separate TF 1.x from 2.x package names and PyPI repos  to enable auto-updates to latest versions of TF 1.x. (without the need to manually monitor the latest available 1.x version).

Possibly just rename it to `tensorflow1==tensorflow` and `tensorflow2`, like with python 2.x and 3.x. These major versions of TF of equally incompatible, so they should not have the same package name IMHO.

The automatic code converter `tf_upgrade_v2` does not always work for Github data science projects and converting Kaggle Kernels and other Jupyter notebooks to `.py` format is an additional manual step required in practice (only to find that conversion did not work).

Many thanks!"
44672,TensorFlow Hub BigGAN cannot be converted with TFLiteConverterV2,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary from PyPI
- TensorFlow version (or github SHA if from source): 2.3.1


**Command used to run the converter or code if youre using the Python API**

```py
import tensorflow as tf
import tensorflow_hub as hub

tf.compat.v1.disable_v2_behavior()


@tf.function(
    input_signature=[
        tf.TensorSpec(shape=(1, 128), dtype=tf.float32, name=""z""),
        tf.TensorSpec(shape=(), dtype=tf.float32, name=""truncation""),
        tf.TensorSpec(shape=(1, 1000), dtype=tf.float32, name=""y""),
    ]
)
def sample(z, truncation, y):
    f = hub.Module('https://tfhub.dev/deepmind/biggan-deep-128/1')
    x = dict(y=y, z=z, truncation=truncation)
    y = f(x)
    return y


f = sample.get_concrete_function()
converter = tf.lite.TFLiteConverter.from_concrete_functions([f])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
with open(""biggan.tflite"", ""wb"") as f:
    f.write(tflite_model)
```

**The output from the converter invocation**

```
2020-11-07 11:52:43.193133: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/
2020-11-07 11:52:43.193153: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
WARNING:tensorflow:From /home/carl/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2020-11-07 11:52:46.345076: W tensorflow/core/common_runtime/graph_constructor.cc:1517] Importing a graph with a lower producer version 27 into an existing graph with producer version 440. Shape inference will have run different parts of the graph with different producer versions.
2020-11-07 11:52:49.611710: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-11-07 11:52:49.645586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-07 11:52:49.645959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:09:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-11-07 11:52:49.646016: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/
2020-11-07 11:52:49.647140: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-11-07 11:52:49.648287: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-11-07 11:52:49.648431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-11-07 11:52:49.649664: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-11-07 11:52:49.650368: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-11-07 11:52:49.650409: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/
2020-11-07 11:52:49.650416: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-11-07 11:52:49.650622: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-11-07 11:52:49.655923: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3493685000 Hz
2020-11-07 11:52:49.656708: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x25aa5450 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-07 11:52:49.656725: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-07 11:52:49.657718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-07 11:52:49.657726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
WARNING:absl:Using TF1 Hub format while building a function: sample. This can lead to errors if the function is not pruned.
2020-11-07 12:01:32.785968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-07 12:01:32.786372: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-11-07 12:01:32.786452: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-11-07 12:01:32.872693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-07 12:01:32.873135: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4ebe0b70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-11-07 12:01:32.873177: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2020-11-07 12:01:32.873442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-07 12:01:32.874172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:09:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-11-07 12:01:32.874300: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/
2020-11-07 12:01:32.874321: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-11-07 12:01:32.874334: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-11-07 12:01:32.874347: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-11-07 12:01:32.874360: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-11-07 12:01:32.874372: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-11-07 12:01:32.874423: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/
2020-11-07 12:01:32.874433: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-11-07 12:01:32.874451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-07 12:01:32.874459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-11-07 12:01:32.874466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-11-07 12:01:36.376693: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-11-07 12:01:36.376728: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.006ms.
2020-11-07 12:01:36.376734: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-11-07 12:02:43.747428: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2020-11-07 12:02:43.747463: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.
Traceback (most recent call last):
  File ""/home/carl/.local/lib/python3.8/site-packages/tensorflow/lite/python/convert.py"", line 196, in toco_convert_protos
    model_str = wrap_toco.wrapped_toco_convert(model_flags_str,
  File ""/home/carl/.local/lib/python3.8/site-packages/tensorflow/lite/python/wrap_toco.py"", line 32, in wrapped_toco_convert
    return _pywrap_toco_api.TocoConvert(
Exception: Merge node module/collect/body/cond/Merge has input that's not in any CondContext.
        for node {{node module/collect/body/cond/Merge}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""convert_savedmodel_to_tflite.py"", line 24, in <module>
    tflite_model = converter.convert()
  File ""/home/carl/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 1076, in convert
    return super(TFLiteConverterV2, self).convert()
  File ""/home/carl/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 899, in convert
    return super(TFLiteFrozenGraphConverterV2,
  File ""/home/carl/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 629, in convert
    result = _toco_convert_impl(
  File ""/home/carl/.local/lib/python3.8/site-packages/tensorflow/lite/python/convert.py"", line 569, in toco_convert_impl
    data = toco_convert_protos(
  File ""/home/carl/.local/lib/python3.8/site-packages/tensorflow/lite/python/convert.py"", line 202, in toco_convert_protos
    raise ConverterError(str(e))
tensorflow.lite.python.convert.ConverterError: Merge node module/collect/body/cond/Merge has input that's not in any CondContext.
        for node {{node module/collect/body/cond/Merge}}
```

**Also, please include a link to the saved model or GraphDef**

```
https://tfhub.dev/deepmind/biggan-deep-128/1
```

**Failure details**
Crashes on attempted conversion."
44671,Problem building from source with cuda 11.1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):Source 
- TensorFlow version:2.3.0
- Python version:3.7
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):3.1.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:11.1,8.0
- GPU model and memory:RTX 3070



**Describe the problem**
I I have brought a RTX 3070. To use it with tensorflow I need to use cuda 11.1.So I built it from source. the problem is that the compiler is asking for files that do not exist in CUDA 11.1. You can see does file in the log below
**Provide the exact sequence of commands / steps that you executed before running into the problem**
I followed all the commands from the https://www.tensorflow.org/install/source_windows guide and at it worked fine.


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
.\bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
WARNING: Output user root ""C:/Users/pc/_bazel_my new pc"" contains a space. This will probably break the build. You should set a different --output_user_root.
WARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=210
INFO: Reading rc options for 'build' from c:\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=F:/anaconda/envs/tf/python.exe
INFO: Reading rc options for 'build' from c:\tensorflow\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2
INFO: Reading rc options for 'build' from c:\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=F:/anaconda/python.exe --action_env PYTHON_LIB_PATH=F:/anaconda/lib/site-packages --python_path=F:/anaconda/python.exe --config=xla --action_env TF_CUDA_VERSION=11.1 --action_env TF_CUDNN_VERSION=8 --action_env TF_CUDA_PATHS=C:Program FilesNVIDIA GPU Computing ToolkitCUDAv11.1,C:cuda --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.6 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:v2 in file c:\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file c:\tensorflow\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file c:\tensorflow\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file c:\tensorflow\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain
INFO: Found applicable config definition build:opt in file c:\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX --define with_default_optimizations=true
INFO: Found applicable config definition build:cuda in file c:\tensorflow\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file c:\tensorflow\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain
INFO: Found applicable config definition build:windows in file c:\tensorflow\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Repository local_config_cuda instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule cuda_configure defined at:
  C:/tensorflow/third_party/gpus/cuda_configure.bzl:1399:18: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1369
                _create_local_cuda_repository(<1 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 955, in _create_local_cuda_repository
                _get_cuda_config(repository_ctx, <1 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 657, in _get_cuda_config
                find_cuda_config(repository_ctx, <2 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 635, in find_cuda_config
                _exec_find_cuda_config(<3 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 629, in _exec_find_cuda_config
                execute(repository_ctx, <1 more arguments>)
        File ""C:/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
Could not find any cublas_api.h matching version '' in any subdirectory:
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
of:
INFO: Repository rules_cc instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule tf_http_archive defined at:
  C:/tensorflow/third_party/repo.bzl:134:19: in <toplevel>
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule git_repository defined at:
  C:/users/pc/_bazel_my new pc/xv6zejqw/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1369
                _create_local_cuda_repository(<1 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 955, in _create_local_cuda_repository
                _get_cuda_config(repository_ctx, <1 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 657, in _get_cuda_config
                find_cuda_config(repository_ctx, <2 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 635, in find_cuda_config
                _exec_find_cuda_config(<3 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 629, in _exec_find_cuda_config
                execute(repository_ctx, <1 more arguments>)
        File ""C:/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
Could not find any cublas_api.h matching version '' in any subdirectory:
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
of:
WARNING: Target pattern parsing failed.
ERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1369
                _create_local_cuda_repository(<1 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 955, in _create_local_cuda_repository
                _get_cuda_config(repository_ctx, <1 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 657, in _get_cuda_config
                find_cuda_config(repository_ctx, <2 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 635, in find_cuda_config
                _exec_find_cuda_config(<3 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 629, in _exec_find_cuda_config
                execute(repository_ctx, <1 more arguments>)
        File ""C:/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
Could not find any cublas_api.h matching version '' in any subdirectory:
        ''
        'include'
        'include/cuda'
        'include/*-linux-gnu'
        'extras/CUPTI/include'
        'include/cuda/CUPTI'
of:
INFO: Elapsed time: 1.046s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package"
44670,BUG: Keras SaveModel does not properly save optimizer state,"EDIT: looks like this is a dupe of #42749, I'll leave this up for now in case since that issue does not have as reproducible high level example, but feel free to close.

This happens at least for Adam (does not apply to SGD for example, did not test with others).

Tested on `tf-nightly` and `tf==2.3.0`.

TL;DR: running a `tf.kerasModel` through `tf.keras.models.load(model.save)` does not properly preserve the state of optimizers for certain optimizers (see #42749 for more details).

The [docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model#the_savefile_includes_2) read:

> The savefile includes:
> * The model architecture, allowing to re-instantiate the model.
> * The model weights.
> * The state of the optimizer, allowing to resume training exactly where you left off.

Full example:

```python3
import numpy as np
import tensorflow as tf
from tensorflow import keras


# Define a minimal model
inp = keras.layers.Input((1, ))
out = keras.layers.Dense(1)(inp)
m1 = keras.Model(inp, out)
m1.compile(loss=""mae"", optimizer=""adam"")

# Create some test data
X, y = np.random.random((100, )), np.random.random((100, ))

# Fit the model to the test data to get everything initialized
m1.fit(X, y, verbose=0)


def roundtrip(model: keras.Model) -> keras.Model:
    save_dir = ""/tmp/mymodel""
    model.save(save_dir)
    restored = keras.models.load_model(save_dir)
    return restored


# Create a copy of the fitted m1
m2 = roundtrip(m1)

# Weights are preserved correctly, this passes
np.testing.assert_allclose(m1.predict(X), m2.predict(X))

# New lets train once more round
m1.fit(X, y, verbose=0)
# Since optimizer weights/state is not preserved, this fit call
# results in different weights in m2, which makes the predictions differ
m2.fit(X, y, verbose=0)
try:
    np.testing.assert_allclose(m1.predict(X), m2.predict(X), rtol=0.1)  # large relative tolerance
except AssertionError:
    print(""AssertionError: model predictions differ"")

# Diagnosis: optimizer weights are not preserved
weights1 = m1.optimizer.get_weights()
m3 = roundtrip(m1)
weights3 = m3.optimizer.get_weights()

try:
    assert weights1 == weights3
except AssertionError:
    print(""AssertionError: optimizer weights differ"")
    print(f""    {weights1}\n    vs\n    {weights3}"")

# Further, we can't even restore the weights without training!
try:
    m3.optimizer.set_weights(weights1)
except Exception as e:
    print(str(e).split("" Provided weights:"")[0])
```"
44669,_pywrap_tensorflow_internal not found,"System information:
- OS: windows 10
- TensorFlow installed from (source or binary):
- TensorFlow version: tensorflow-cpu 2.3.0
- Python version: 3.8.3
- Installed using  pip from https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow_cpu-2.3.0-cp38-cp38-win_amd64.whl

Log:
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\Hp\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Hp\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Hp\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\Hp\Anaconda3\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""C:\Users\Hp\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Hp\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Hp\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
44668,"Unable to ""import tensorflow"" using 2.4.0rc0 from within conda environment","
**System information**
- OS: Ubuntu 20.04
- TensorFlow binary installed using pip
- TensorFlow version 2.4.0rc0
- Python version 3.8.6
- CUDA/cuDNN version: 11.0/8.0.4
- GPU model: GTX 1070




**Describe the current behavior**

Attempting to import tensorflow produces an ""Illegal instruction"" error.

**Describe the expected behavior**

Import tensorflow without error.

**Standalone code to reproduce the issue**
I created new conda environment and then installed tensorflow using pip from within the new environment:

```
conda create -n tf24 -c conda-forge python=3.8 pip
conda activate tf24
pip install tensorflow==2.4.0rc0
```
The following command exits with an ""Illegal instruction"" error:
```
python -c ""import tensorflow as tf""
```
Note that this is not an issue with the last development version of 2.4.0 (2.4.0.dev20201023) from tf-nightly, installed with:

```
pip install tf-nightly==2.4.0.dev20201023
```
"
44664,Register Custom Operator Prototypes in TFLite C API,"**System information**
- TensorFlow version (you are using): v2.4.0-rc0
  - TFLite C API on android arm/arm64 JNI
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

I am specifically interested in the RandomStandardNormal operator. It was added as a custom op prototype in v2.4.0-rc0: https://github.com/tensorflow/tensorflow/commit/11823c6179a5e2ca6ac4f8480aa00c42c772885e

I am looking for a way to register this operator prototype in the TFLite C API. I am able to call the function `TfLiteInterpreterOptionsAddCustomOp` from `tensorflow/lite/c/c_api_experimental.h`, but I am unable to call `tflite::ops::custom::Register_RANDOM_STANDARD_NORMAL()` to pass a registration pointer. 

**Will this change the current api? How?**

Potentially if we add a extern ""C"" wrapper to `tensorflow/lite/kernels/custom_ops_register.h` it will expose more functions in the C API.

**Who will benefit with this feature?**

People who want to use the currently implemented custom operator prototypes via the C API. Also, people who want to learn more about how they can develop their own custom operators and use them in the C API.

**Any Other info.**

TFLite mailing list discussion with @abattery https://groups.google.com/a/tensorflow.org/g/tflite/c/1zfZtRtVfpY

## My attempt at getting this to work
### My Code (android JNI project, I'm calling this `load_model` function from C)
#### load-model.h

```C++
#ifndef SYNC_LOAD_MODEL_H
#define SYNC_LOAD_MODEL_H

#include <tensorflow/lite/c/common.h>
#include <tensorflow/lite/c/c_api.h>
#include <tensorflow/lite/c/c_api_experimental.h>

#ifdef __cplusplus
extern ""C"" {
#endif  // __cplusplus
    TfLiteInterpreter* load_model(const char* model_path);
#ifdef __cplusplus
};
#endif  // __cplusplus

#endif //SYNC_LOAD_MODEL_H
```

#### load-model.cc
```C++
#include <load-model.h>

#include <tensorflow/lite/kernels/custom_ops_register.h>

// load the model and return pointer to the interpreter
extern ""C"" {
    TfLiteInterpreter* load_model(const char* model_path)
    {
        TfLiteModel *model = TfLiteModelCreateFromFile(model_path);
        TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();

        // register custom ops
        TfLiteInterpreterOptionsAddCustomOp(options, ""RandomStandardNormal"", tflite::ops::custom::Register_RANDOM_STANDARD_NORMAL(), 1, 1);

        // create the interpreter
        TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);
        TfLiteInterpreterAllocateTensors(interpreter);

        return interpreter;
    }
}
```
#### Android.mk
```
APP_ABI         := arm64-v8a

LOCAL_PATH := $(call my-dir)

include $(CLEAR_VARS)

LOCAL_MODULE    := streaming
LOCAL_SRC_FILES := stream-server.c stream-client.c stream-denoise.c
LOCAL_SHARED_LIBRARIES := load_model gstreamer_android tensorflowlite_c
LOCAL_LDLIBS := -llog
include $(BUILD_SHARED_LIBRARY)

include $(CLEAR_VARS)

LOCAL_CPP_EXTENSION := .cc
LOCAL_MODULE    := load_model
LOCAL_SRC_FILES := load-model.cc
LOCAL_SHARED_LIBRARIES := tensorflowlite_c
LOCAL_CPPFLAGS := -std=c++14
include $(BUILD_SHARED_LIBRARY)

ifndef GSTREAMER_ROOT_ANDROID
$(error GSTREAMER_ROOT_ANDROID is not defined!)
endif

ifeq ($(TARGET_ARCH_ABI),armeabi)
GSTREAMER_ROOT        := $(GSTREAMER_ROOT_ANDROID)/arm
else ifeq ($(TARGET_ARCH_ABI),armeabi-v7a)
GSTREAMER_ROOT        := $(GSTREAMER_ROOT_ANDROID)/armv7
else ifeq ($(TARGET_ARCH_ABI),arm64-v8a)
GSTREAMER_ROOT        := $(GSTREAMER_ROOT_ANDROID)/arm64
else ifeq ($(TARGET_ARCH_ABI),x86)
GSTREAMER_ROOT        := $(GSTREAMER_ROOT_ANDROID)/x86
else ifeq ($(TARGET_ARCH_ABI),x86_64)
GSTREAMER_ROOT        := $(GSTREAMER_ROOT_ANDROID)/x86_64
else
$(error Target arch ABI not supported: $(TARGET_ARCH_ABI))
endif

GSTREAMER_NDK_BUILD_PATH  := $(GSTREAMER_ROOT)/share/gst-android/ndk-build/
GSTREAMER_PLUGINS         := coreelements audiotestsrc audioconvert audioresample audiobuffersplit udp rtp rtpmanager opensles opus nice dtls srtp webrtc playback
GSTREAMER_EXTRA_DEPS      := gstreamer-net-1.0 gstreamer-sdp-1.0 gstreamer-webrtc-1.0
GSTREAMER_EXTRA_LIBS      := -liconv
include $(GSTREAMER_NDK_BUILD_PATH)/gstreamer-1.0.mk

include $(CLEAR_VARS)

LOCAL_MODULE    := tensorflowlite_c
ifeq ($(TARGET_ARCH_ABI),armeabi-v7a)
LOCAL_SRC_FILES := $(LOCAL_PATH)/libtensorflowlite_c/armv7/libtensorflowlite_c.so
else ifeq ($(TARGET_ARCH_ABI),arm64-v8a)
LOCAL_SRC_FILES := $(LOCAL_PATH)/libtensorflowlite_c/arm64/libtensorflowlite_c.so
else ifeq ($(TARGET_ARCH_ABI),x86)
LOCAL_SRC_FILES := $(LOCAL_PATH)/libtensorflowlite_c/x86/libtensorflowlite_c.so
else ifeq ($(TARGET_ARCH_ABI),x86_64)
LOCAL_SRC_FILES := $(LOCAL_PATH)/libtensorflowlite_c/x86_64/libtensorflowlite_c.so
else
$(error Target arch ABI not supported: $(TARGET_ARCH_ABI))
endif

LOCAL_EXPORT_C_INCLUDES := $(LOCAL_PATH)/tensorflow/tensorflow/lite/
LOCAL_CFLAGS := -I$(LOCAL_PATH)/tensorflow/tensorflow/lite/c/ -I$(LOCAL_PATH)/tensorflow/tensorflow/lite/kernels/
include $(PREBUILT_SHARED_LIBRARY)
```
### Tensorflow modifications for `libtensorflowlite_c.so`

#### tensorflow/lite/c/BUILD
```bazel
tflite_cc_shared_object(
    name = ""tensorflowlite_c"",
    linkopts = select({
        ""//tensorflow:ios"": [
            ""-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)"",
        ],
        ""//tensorflow:macos"": [
            ""-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)"",
        ],
        ""//tensorflow:windows"": [],
        ""//conditions:default"": [
            ""-z defs"",
            ""-Wl,--version-script,$(location //tensorflow/lite/c:version_script.lds)"",
        ],
    }),
    per_os_targets = True,
    deps = [
        "":c_api"",
        "":c_api_experimental"",
        "":exported_symbols.lds"",
        "":version_script.lds"",
        ""//tensorflow/lite/kernels:custom_ops"", # added this
    ],
)
```

#### tensorflow/lite/c/version_scripts.lds
```
VERS_1.0 {
  # try to make everything global for now
  global:
    *;
};
```
#### tensorflow/lite/c/exported_symbols.lds
```
# try to make everything global for now
*
```
#### tensorflow/lite/kernels/custom_ops_register.h
```C++
#ifndef TENSORFLOW_LITE_KERNELS_CUSTOM_OPS_REGISTER_H_
#define TENSORFLOW_LITE_KERNELS_CUSTOM_OPS_REGISTER_H_

#include ""tensorflow/lite/c/common.h""

namespace tflite {
namespace ops {
namespace custom {

TfLiteRegistration* Register_HASHTABLE();
TfLiteRegistration* Register_HASHTABLE_FIND();
TfLiteRegistration* Register_HASHTABLE_IMPORT();
TfLiteRegistration* Register_HASHTABLE_SIZE();
TfLiteRegistration* Register_IMAG();
TfLiteRegistration* Register_MULTINOMIAL();
extern ""C"" TfLiteRegistration* Register_RANDOM_STANDARD_NORMAL(); # added `extern ""C""`
TfLiteRegistration* Register_REAL();
TfLiteRegistration* Register_RFFT2D();

}  // namespace custom
}  // namespace ops
}  // namespace tflite

#endif  // TENSORFLOW_LITE_KERNELS_CUSTOM_OPS_REGISTER_H_
```
#### bazel build command
```
$ bazel build --verbose_failures --config=monolithic --config android_arm64 -c opt //tensorflow/lite/c:libtensorflowlite_c.so

INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=289
INFO: Reading rc options for 'build' from /Users/corey/Desktop/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/corey/Desktop/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /Users/corey/Desktop/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Users/corey/.pyenv/versions/3.7.3/bin/python3 --action_env PYTHON_LIB_PATH=/Users/corey/.pyenv/versions/3.7.3/lib/python3.7/site-packages --python_path=/Users/corey/.pyenv/versions/3.7.3/bin/python3 --config=xla --action_env ANDROID_NDK_HOME=/Users/corey/library/Android/sdk/ndk-bundle --action_env ANDROID_NDK_API_LEVEL=21 --action_env ANDROID_BUILD_TOOLS_VERSION=29.0.3 --action_env ANDROID_SDK_API_LEVEL=29 --action_env ANDROID_SDK_HOME=/Users/corey/library/Android/sdk --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /Users/corey/Desktop/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/corey/Desktop/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /Users/corey/Desktop/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:monolithic in file /Users/corey/Desktop/tensorflow/.bazelrc: --define framework_shared_object=false
INFO: Found applicable config definition build:android_arm64 in file /Users/corey/Desktop/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
INFO: Found applicable config definition build:android in file /Users/corey/Desktop/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++14 --host_cxxopt=-std=c++14
INFO: Analyzed target //tensorflow/lite/c:libtensorflowlite_c.so (1 packages loaded, 108 targets configured).
INFO: Found 1 target...
Target //tensorflow/lite/c:libtensorflowlite_c.so up-to-date:
  bazel-bin/tensorflow/lite/c/libtensorflowlite_c.so
INFO: Elapsed time: 1.380s, Critical Path: 0.93s
INFO: 1 process: 1 local.
INFO: Build completed successfully, 2 total actions
```
## Error
```
[arm64-v8a] Prebuilt       : libtensorflowlite_c.so <= ../app/src/main/jni/libtensorflowlite_c/arm64/
[arm64-v8a] SharedLibrary  : libload_model.so

../app/build/intermediates/ndkBuild/debug/obj/local/arm64-v8a/objs-debug/load_model/load-model.o: In function `load_model':
../app/src/main/jni/load-model.cc:13: undefined reference to `Register_RANDOM_STANDARD_NORMAL'
clang++: error: linker command failed with exit code 1 (use -v to see invocation)
```

"
44658,tf.linalg.pinv also for complex matrices,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

Right now, as stated in [the docs](https://www.tensorflow.org/api_docs/python/tf/linalg/pinv#args), the `pinv` operator accepts only float-like matrices.
It would be nice to have it accept complex matrices as well, like `numpy` does.

**Will this change the current api? How?**

No

**Who will benefit with this feature?**

People working with complex matrices.
I typically work with MRI data which is naturally complex, but I guess this can be applied to other domains.

**Any Other info.**

I checked in the latest [`tf.experimental.numpy`](https://www.tensorflow.org/api_docs/python/tf/experimental/numpy), and it's absent. But maybe the complex-supported `pinv` should be better there, I don't know. "
44657,InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.,"**System information**

- Linux Ubuntu 16.04
- TensorFlow 2.2
- Python 3

**Describe the current behavior**
```

user_input = Input( shape=(None, 1), dtype=tf.int32 )
# same error with: user_input = Input( shape=(1,), dtype=tf.int32 )

MF_Embedding_User = tf.Variable(tf.random.normal([8176, 20], stddev=0.05), dtype=tf.float32, name='user_embedding')

user_embedding = tf.nn.embedding_lookup(MF_Embedding_User, user_input)
#same error with: tf.gather

user_input = [0,1,2,3,4,5]
model = Model( user_input, user_embedding )
```

**Describe the expected behavior**

I would like to define an Input vector of indices that I can use whether in a tf.ebedding_lookup or a tf.gather
It used to work in TF1 with placeholders and not anymore.

I don't know how i can Input my index vectors in my model
I know it is possible to define a @tf.function but idk how to do this and still be abble to train my model
I am really stuck.

Thank you for your help :)"
44656,ERROR: Could not find a version that satisfies the requirement tensorflow==2.3 (from versions: none),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44654,Version requirements for Python dependencies too strict,"**System information**
- TensorFlow version (you are using): 2.4.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

Up until 2.3.1 the version requirements in the setup.py file where of the form `foo >=x.y` or `foo >=x.y,<z.q`, i.e. there was a minimum and a potential maximum version specified. This allows for TensorFlow to be installed alongside other Python packages with own requirements.
However https://github.com/tensorflow/tensorflow/commit/aafe25d3f97d645bca91b34a9476eb770d1abf90 changed that significantly and IMO to the worse: It basically pins a specific major and minor version.

I think this is overly eager. For example TF works fine with protobuf 3.9.2 or 3.10 but now requires 3.13.x. Similar the wheel version doesn't really matter much (a 0.26 is already enough) but it now requires a 0.35.x

This is really bad for forward and backward compatibility. For example the system Python (e.g. on a HPC cluster, which can't be changed) could include a newer wheel (say 0.36.0) for e.g. security reasons. Now TF cannot be installed in that environment any longer even though there is no actual reason but that arbitrary limitation to a version which might have been the most recent at some point in time. Also an already installed wheel 0.33.0 can't be used although everything would work.

And also cross compatibility is affected. Let's say a user needs to install a software which needs a feature from wheel 0.36, this can now not be installed because TF limited the version to 0.35

**Will this change the current api? How?**

No.

**Who will benefit with this feature?**

Everyone using TF in existing environments and/or with other packages

**Any Other info.**

My suggestion is to revert the version list back to what it was using min and max versions and be quite liberal there too (e.g. numpy >=1.16 works just fine and even just limiting to either pre 1.19 or post 1.19 would exclude whole classes of other software). The `~=` notation can be used where it makes sense, e.g. for `tensorboard` whose major and minor version must match the TF version."
44659,"Fatal Exception: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: OpenCL library not loaded - dlopen failed: library ""libOpenCL-pixel.so"" not found","**System information**

- Mobile device (vivo 1904, Pixel 2, etc),
- tenserflow lite
- build.gradle

```
'lite'    : 'org.tensorflow:tensorflow-lite:2.0.0',
'liteGpu': 'org.tensorflow:tensorflow-lite-gpu:2.0.0'
```

**Describe the current behavior**
Exception is thrown on invoking `Interpreter.runForMultipleInputsOutputs()` on the model collected from https://storage.googleapis.com/download.tensorflow.org/models/tflite/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite

**Describe the expected behavior**

Successful recognition of posture.

**Standalone code to reproduce the issue**
I am getting this on crash analysis tool. Since this is not reproducible on my devices I am not able to produce a standalone code.


**Other info / logs** 

```
Fatal Exception: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: OpenCL library not loaded - dlopen failed: library ""libOpenCL-pixel.so"" not found
Falling back to OpenGL
TfLiteGpuDelegate Invoke: Write to buffer failed. Source data is larger than buffer.
Node number 31 (TfLiteGpuDelegateV2) failed to invoke.

       at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java)
       at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:171)
       at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)
       at a.b.c.e.Posenet.estimateSinglePose(Posenet.java:296)
       at a.b.c.d.ImageAnalyser.processImage(ImageAnalyser.java:176)
       at a.b.c.d.ImageAnalyser.access$processImage(ImageAnalyser.java:26)
       at a.b.c.d.ImageAnalyser$processImageForAnalysis$2.invokeSuspend(ImageAnalyser.java:141)
       at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(BaseContinuationImpl.java:33)
       at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.java:56)
       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
       at java.lang.Thread.run(Thread.java:764)
```

I am following this sample https://github.com/tensorflow/examples/blob/master/lite/examples/posenet/android/posenet/src/main/java/org/tensorflow/lite/examples/posenet/lib/Posenet.kt
"
44651,Tensorflow lite Makefile: add support of gpu ,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

**System information**
- TensorFlow version (you are using):  2.3
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**
Currently we cannot use GPU delegate when compiling tensorflow lite using the Makefile

**Will this change the current api? How?** No

**Who will benefit with this feature?** 

**Any Other info.**
"
44650,A listing of API symbols exported and supported in a machine readable form,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs

## Description of issue (what needs changing):

### Clear description

I would like to know if there is a listing of API symbols available and exported in a release (public API), I'm interested in Python code specifically. I see docs on the main page, but I would like to access symbols exported per release in a machine-readable form (for example in a JSON format). I couldn't find any related documentation, sorry if there are already such files. Do you plan to provide/support such a listing to the community?

I hope the issue is addressed correctly in this repo and thanks for any advice in advance.
"
44649,ValueError: All inputs to `ConcreteFunction`s must be Tensors,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip 
- TensorFlow version (use command below): 2.1.2
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1/7.4
- GPU model and memory: gtx1080Ti

**Describe the current behavior**
Tf.train_on_batch cant take numpy array as input, asks for tensor and even using tf.convert_to_tensor doesn't help. It works in Tf2.3, but Tf2.1 is a requirement for compatibility with another library.

**Describe the expected behavior**
Should train without issues, tested on Tf2.3 and it works, but Tf2.1 is a requirement.

**Standalone code to reproduce the issue**


    import tensorflow as tf
    import numpy as np
    from sklearn.model_selection import train_test_split
    from tensorflow.keras.layers import (Conv2D,Input)
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam
    import tensorflow_addons as tfa #version 0.9.1

    opt = Adam(lr=1e-4)
    tf.config.experimental_run_functions_eagerly(True)


    def custom_mean_squared_error(y_true, y_pred):
        return(tf.reduce_mean(tf.math.squared_difference(y_true, y_pred)))


    def firstStream(ip):
            layer = Conv2D(64, kernel_size=(5, 5), padding='same')(ip)
            op = Conv2D(3, (5, 5), activation='sigmoid', padding='same')(layer)
            return op
        
    def secStream(ip):
            layer = Conv2D(8, kernel_size=(5, 5), padding='same')(ip)
            flow = Conv2D(2, (5, 5), activation='sigmoid', padding='same')(layer)
            return flow


    def main():

        ip = Input(shape=(None, None, 3))
        op = firstStream(ip)
        flow = secStream(ip)
        warped = tf.expand_dims(tfa.image.dense_image_warp(op,flow,name='warp'),axis=1)
        model = Model(ip,warped)
        model.compile(optimizer=opt, loss=custom_mean_squared_error)
            
        numEpochs = 1
 
        for epochNo in range(numEpochs):

                print(""Running epoch : %d"" % epochNo)
                batch_ip=np.ones((1,64,64,3))
                      
                curr_loss = model.train_on_batch(batch_ip, batch_ip)
                print(curr_loss)


    main()




**Other info / logs** 

main()
  File ""/home/mohana/work/simple.py"", line 46, in main
    curr_loss = model.train_on_batch(batch_ip, batch_ip)
  File ""/home/mohana/virtualenvs/tfcomp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1078, in train_on_batch
    standalone=True)
  File ""/home/mohana/virtualenvs/tfcomp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 433, in train_on_batch
    output_loss_metrics=model._output_loss_metrics)
  File ""/home/mohana/virtualenvs/tfcomp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py"", line 312, in train_on_batch
    output_loss_metrics=output_loss_metrics))
  File ""/home/mohana/virtualenvs/tfcomp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py"", line 269, in _process_single_batch
    grads = tape.gradient(scaled_total_loss, trainable_weights)
  File ""/home/mohana/virtualenvs/tfcomp/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py"", line 1029, in gradient
    unconnected_gradients=unconnected_gradients)
  File ""/home/mohana/virtualenvs/tfcomp/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py"", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File ""/home/mohana/virtualenvs/tfcomp/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1256, in _backward_function_wrapper
    processed_args, remapped_captures)
  File ""/home/mohana/virtualenvs/tfcomp/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1684, in _call_flat
    ""Tensor."" % (self._func_graph.name, i, str(arg)))
ValueError: All inputs to `ConcreteFunction`s must be Tensors; on invocation of __backward__defun_call_1331, the 0-th input (IndexedSlices(indices=tf.Tensor([  65   65   66 ... 4028 4029 4030], shape=(16384,), dtype=int32), values=tf.Tensor(
[[-0.0000000e+00 -0.0000000e+00 -0.0000000e+00]
 [-0.0000000e+00 -0.0000000e+00 -0.0000000e+00]
 [-0.0000000e+00 -0.0000000e+00 -0.0000000e+00]
 ...
 [-2.7218828e-05 -2.6730117e-05 -2.4183297e-05]
 [-2.7333013e-05 -2.7423557e-05 -2.2896325e-05]
 [-2.1508897e-05 -2.1834361e-05 -1.8788463e-05]], shape=(16384, 3), dtype=float32), dense_shape=tf.Tensor([4096    3], shape=(2,), dtype=int32))) was not a Tensor.


"
44647,Python/Django,"I just wanted to make a new application on Django. There were no problems during installation. But when I decided to create a new project, I received an unknown error. I didnt find a solution in google. Here is what I wrote: 


_django-admin startproject mysite_

This is what it gave me:


 _Traceback (most recent call last):
  File ""c:\users\danii\appdata\local\programs\python\python38-32\lib\runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""c:\users\danii\appdata\local\programs\python\python38-32\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\Users\danii\AppData\Local\Programs\Python\Python38-32\Scripts\django-admin.exe\__main__.py"", line 4, in <module>
  File ""c:\users\danii\appdata\local\programs\python\python38-32\lib\site-packages\django\core\management\__init__.py"", line 12, in <module>
    from django.conf import settings
  File ""c:\users\danii\appdata\local\programs\python\python38-32\lib\site-packages\django\conf\__init__.py"", line 19, in <module>
    from django.core.validators import URLValidator
  File ""c:\users\danii\appdata\local\programs\python\python38-32\lib\site-packages\django\core\validators.py"", line 8, in <module>
    from django.utils.encoding import punycode
  File ""c:\users\danii\appdata\local\programs\python\python38-32\lib\site-packages\django\utils\encoding.py"", line 8, in <module>
    from django.utils.deprecation import RemovedInDjango40Warning
  File ""c:\users\danii\appdata\local\programs\python\python38-32\lib\site-packages\django\utils\deprecation.py"", line 1, in <module>
    import asyncio
  File ""c:\users\danii\appdata\local\programs\python\python38-32\lib\asyncio\__init__.py"", line 8, in <module>
    from .base_events import *
  File ""c:\users\danii\appdata\local\programs\python\python38-32\lib\asyncio\base_events.py"", line 40, in <module>
    from . import events
  File ""c:\users\danii\appdata\local\programs\python\python38-32\lib\asyncio\events.py"", line 783, in <module>
    from _asyncio import (_get_running_loop, _set_running_loop,
AttributeError: module 'traceback' has no attribute 'extract_stack'_

Python 3.9
Django  3.1.3
This error can be very stupid, but I started doing this recently.If you have additional questions, I'm ready to answer
Thanks"
44646,Bug when a custom tf.keras.models.Model has multiple class inheritance,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows and Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.3
- Python version: conda env with Python 3.7.9
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce RTX 2080 Super with Max-Q Design 8GB


**Describe the current behavior**
Creating a custom model that inherit of at least one other class than `tf.keras.models.Model`, the following exception is raised:
```
File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\training\tracking\base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 255, in __init__
    inject_functional_model_class(self.__class__)
  File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 144, in inject_functional_model_class
    cls.__bases__ = tuple(inject_functional_model_class(base)
  File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 144, in <genexpr>
    cls.__bases__ = tuple(inject_functional_model_class(base)
  File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 144, in inject_functional_model_class
    cls.__bases__ = tuple(inject_functional_model_class(base)
  File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 144, in <genexpr>
    cls.__bases__ = tuple(inject_functional_model_class(base)
  File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 144, in inject_functional_model_class
    cls.__bases__ = tuple(inject_functional_model_class(base)
  File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 144, in <genexpr>
    cls.__bases__ = tuple(inject_functional_model_class(base)
  File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 144, in inject_functional_model_class
    cls.__bases__ = tuple(inject_functional_model_class(base)
TypeError: can't set attributes of built-in/extension type 'object'
```

**Describe the expected behavior**
Being able to create a custom model with different mixins.

**Standalone code to reproduce the issue**
Here a simple piece of code to reproduce the issue:
```
import tensorflow as tf

class PrintMixin:
    def custom_print(self):
        print(""Hello world"")

class CustomModel(tf.keras.models.Model, PrintMixin):
    def __init__(self, *args, **kwargs):
        my_input = tf.keras.layers.Input(shape=(16,))
        dense = tf.keras.layers.Dense(32, activation='relu')
        output = dense(my_input)
        outputs = {""output"": output}

        super().__init__(inputs=[my_input], outputs=outputs, *args, **kwargs)


my_model = CustomModel()
```

**Other info / logs**
Apparently when giving the `inputs` and `outputs` parameters, TensorFlow tries to inject an attribute to all the classes and super classes until reaching `tf.keras.models.Model`. Here the piece of code from the file `training.py` line 136:
```
def inject_functional_model_class(cls):
  """"""Inject `Functional` into the hierarchy of this class if needed.""""""
  from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top
  from tensorflow.python.keras.engine import training_v1  # pylint: disable=g-import-not-at-top
  if cls == Model or cls == training_v1.Model:
    return functional.Functional

  cls.__bases__ = tuple(inject_functional_model_class(base)
                        for base in cls.__bases__)
  # Trigger any `__new__` class swapping that needed to happen on `Functional`
  # but did not because functional was not in the class hierarchy.
  cls.__new__(cls)

  return cls
```

But when it tries to check the superclass of my mixin class, which is `object` an error is raised saying that we cannot add an attribute to the `object` type. For me the following update of the method fix the issue:

```
def inject_functional_model_class(cls):
  """"""Inject `Functional` into the hierarchy of this class if needed.""""""
  from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top
  from tensorflow.python.keras.engine import training_v1  # pylint: disable=g-import-not-at-top
  if cls == Model or cls == training_v1.Model:
    return functional.Functional
  if cls == 'object':
    return cls

  cls.__bases__ = tuple(inject_functional_model_class(base)
                        for base in cls.__bases__)
  # Trigger any `__new__` class swapping that needed to happen on `Functional`
  # but did not because functional was not in the class hierarchy.
  cls.__new__(cls)

  return cls
```
Here we return the `object` class as it is. But I don't know if it is a proper fix that won't bring another error elsewhere.

First, I wanted to know if it is really a bug?
If not, how I could do a proper custom model with mixin classes and my inputs/outputs.
If, yes, is the fix I proposed ok and if needed I can open a PR with it.

Thanks!"
44645,AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'swapaxes',"I am building a CNN model. model compile fine but when i fit the model it does not work: The code fine in my jupyttor but not in kaggle
#metric

```
def iou_metric_batch(y_true_in, y_pred_in):
    batch_size = y_true_in.shape[0]
    metric = []
    for batch in range(batch_size):
        value = iou_metric(y_true_in[batch], y_pred_in[batch])
        metric.append(value)
    return np.array(np.mean(metric), dtype=np.float32)

def my_iou_metric(label, pred):
    metric_value = tf.py_function(iou_metric_batch, [label, pred], tf.float32)
    return metric_value

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[my_iou_metric])
tf.compat.v1.disable_eager_execution()
model_path = ""nuclei_finder_unet_1.h5""

checkpoint = ModelCheckpoint(model_path,
                             monitor=""val_loss"",
                             mode=""min"",
                             save_best_only = True,
                             verbose=1)

earlystop = EarlyStopping(monitor = 'val_loss', 
                          min_delta = 0, 
                          patience = 5,
                          verbose = 1,
                          restore_best_weights = True)

# Fit our model 
results = model.fit(X_train, Y_train, validation_split=0.1,
                    batch_size=16, epochs=10,callbacks=[earlystop, checkpoint])



```Erroe:
```
nknownError: AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'swapaxes'
Traceback (most recent call last):

  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 242, in __call__
    stateful=None,

  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py"", line 131, in __call__
    """"""

  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 302, in wrapper

  File ""<ipython-input-99-1bbcb510438c>"", line 58, in iou_metric_batch
    value = iou_metric(y_true_in[batch], y_pred_in[batch])

  File ""<ipython-input-99-1bbcb510438c>"", line 2, in iou_metric
    labels = label(y_true_in > 0.5)

  File ""/opt/conda/lib/python3.7/site-packages/skimage/measure/_label.py"", line 93, in label
    return clabel(input, neighbors, background, return_num, connectivity)

  File ""skimage/measure/_ccomp.pyx"", line 348, in skimage.measure._ccomp.label_cython

  File ""skimage/measure/_ccomp.pyx"", line 322, in skimage.measure._ccomp.reshape_array

  File ""skimage/measure/_ccomp.pyx"", line 299, in skimage.measure._ccomp._apply_swaps

AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'swapaxes'


	 [[{{node metrics_10/my_iou_metric/EagerPyFunc}}]]
```"
44644,tf.keras.models.load_model fails if tf.linalg.band_part is used.,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Arch Linux (5.9.4-arch1-1)
- TensorFlow installed from: binary
- TensorFlow version: 2.3.1
- Python version: 3.8.6
- CUDA/cuDNN version: 11.1/8.0.4
- GPU model and memory: NVIDIA GeForce GTX 1080 Ti/10.91GiB

**Describe the current behavior**
`tf.keras.models.load_model` fails when `tf.linalg.band_part` is used.
~~~
ValueError: Inconsistent values for attr 'Tindex' DT_INT32 vs. DT_INT64 while building NodeDef 'tf_op_layer_MatrixBandPart/MatrixBandPart' using Op<name=MatrixBandPart; signature=input:T, num_lower:Tindex, num_upper:Tindex -> band:T; attr=T:type; attr=Tindex:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
~~~

**Describe the expected behavior**
Model is loaded without error.

**Standalone code to reproduce the issue**
~~~
import tensorflow as tf

x = tf.keras.Input(shape=(3, 3))
y = tf.linalg.band_part(x, -1, 0)  # getting lower triangle part. I need band_part() for making look-ahead-mask on Transformer.
# y = tf.reshape(x, (-1, 9))

model = tf.keras.Model(x, y)
print(model.predict([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],
                     [[9, 8, 7], [6, 5, 4], [3, 2, 1]]]))
model.save('model')

tf.keras.models.load_model('model')
~~~

**Other info / logs** Include any logs or source code that would be helpful to
~~~
tensorflow.python.framework.errors_impl.InvalidArgumentError: Inconsistent values for attr 'Tindex' DT_INT32 vs. DT_INT64 while building NodeDef 'tf_op_layer_MatrixBandPart/MatrixBandPart' using Op<name=MatrixBandPart; signature=input:T, num_lower:Tindex, num_upper:Tindex -> band:T; attr=T:type; attr=Tindex:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test_band_part.py"", line 11, in <module>
    tf.keras.models.load_model('model')
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py"", line 187, in load_model
    return saved_model_load.load(filepath, compile, options)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 120, in load
    model = tf_load.load_internal(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 632, in load_internal
    loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 194, in __init__
    super(KerasObjectLoader, self).__init__(*args, **kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 130, in __init__
    self._load_all()
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 221, in _load_all
    self._finalize_objects()
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 530, in _finalize_objects
    self._reconstruct_all_models()
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 548, in _reconstruct_all_models
    self._reconstruct_model(model_id, model, layers)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py"", line 588, in _reconstruct_model
    created_layers) = functional_lib.reconstruct_from_config(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 1214, in reconstruct_from_config
    process_node(layer, node_data)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 1162, in process_node
    output_tensors = layer(input_tensors, **kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 925, in __call__
    return self._functional_construction_call(inputs, args, kwargs,
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1117, in _functional_construction_call
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 3099, in call
    return self._make_op(inputs)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 3121, in _make_op
    c_op = ops._create_c_op(graph, node_def, inputs, control_inputs=[])
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1815, in _create_c_op
    raise ValueError(str(e))
ValueError: Inconsistent values for attr 'Tindex' DT_INT32 vs. DT_INT64 while building NodeDef 'tf_op_layer_MatrixBandPart/MatrixBandPart' using Op<name=MatrixBandPart; signature=input:T, num_lower:Tindex, num_upper:Tindex -> band:T; attr=T:type; attr=Tindex:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
~~~

Same as #42301. I just wrote standalone code."
44642,test with random fail on graph mode,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
v1.12.1-44421-g5e5730ba9d 2.4.0-rc0
- Python version:
3.7.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
11.1
- GPU model and memory:
960m cuda

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
running the followin:
```python
import tensorflow as tf
from tensorflow.python.framework import test_util


class WeirdTest(tf.test.TestCase):

  @test_util.run_in_graph_and_eager_modes
  def test_test(self):
    with self.cached_session():
      x = tf.random.normal([])
      self.assertAllClose(x, x)


if __name__ == '__main__':
  tf.random.set_seed(42)
  tf.test.main()
```

fails with assertion error (the x is not equal to itself!)
(eager mode works fine)

**Describe the expected behavior**
the test should pass

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

attached above (not sure how to run tests on colab)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
Failure
Traceback (most recent call last):
  File ""C:\Program Files\Python37\lib\unittest\case.py"", line 59, in testPartExecutor
    yield
  File ""C:\Program Files\Python37\lib\unittest\case.py"", line 628, in run
    testMethod()
  File ""D:\python3_venvs\tf_2_4\lib\site-packages\tensorflow\python\framework\test_util.py"", line 1196, in decorated
    f(self, *args, **kwargs)
  File ""C:\Users\amitp\dev\github\amitport\federated-learning-research\federated_learning_research\v2\encoders\sign_encoder_test.py"", line 11, in test_encode
    self.assertAllClose(x, x)
  File ""D:\python3_venvs\tf_2_4\lib\site-packages\tensorflow\python\framework\test_util.py"", line 1236, in decorated
    return f(*args, **kwds)
  File ""D:\python3_venvs\tf_2_4\lib\site-packages\tensorflow\python\framework\test_util.py"", line 2711, in assertAllClose
    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
  File ""D:\python3_venvs\tf_2_4\lib\site-packages\tensorflow\python\framework\test_util.py"", line 2671, in _assertAllCloseRecursive
    (path_str, path_str, msg)))
  File ""D:\python3_venvs\tf_2_4\lib\site-packages\tensorflow\python\framework\test_util.py"", line 2606, in _assertArrayLikeAllClose
    a, b, rtol=rtol, atol=atol, err_msg=""\n"".join(msgs), equal_nan=True)
  File ""D:\python3_venvs\tf_2_4\lib\site-packages\numpy\testing\_private\utils.py"", line 1528, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File ""D:\python3_venvs\tf_2_4\lib\site-packages\numpy\testing\_private\utils.py"", line 840, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=1e-06, atol=1e-06
Mismatched value: a is different from b. 
not close lhs = 0.4722212255001068
not close rhs = -1.5251041650772095
not close dif = 1.9973254203796387
not close tol = 2.525104165077209e-06
dtype = float32, shape = ()
Mismatched elements: 1 / 1 (100%)
Max absolute difference: 1.9973254
Max relative difference: 1.3096322
 x: array(0.472221, dtype=float32)
 y: array(-1.525104, dtype=float32)
```
"
44641,Tensorflow 2+ framework class exception,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: custom code
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro 64 bit
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: N/A
-   **TensorFlow installed from (source or binary)**: Binary
-   **TensorFlow version (use command below)**: 2.1.0
-   **Python version**:3.7.9
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: 11.1
-   **GPU model and memory**: GeForce MX130, 2048 MiB
-   **Exact command to reproduce**: python main.py in the tree/master/src dir after downloading the master from https://github.com/githubharald/SimpleHTR/ (I had another error that I rectified by replacing tf.placeholder with tf.Variable in several places.)

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
The src/Model.py line 34 --> tf.Variable(tf.bool, name='is_train') starts the problem-chain culminating in ""Attempt to convert a value (tf.bool) with an unsupported type (<class 'tensorflow.python.framework.dtypes.DType'>) to a Tensor."" Is there something amiss by replacing ..placeholder by ..Variable, as mentioned above?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

PS D:\github\word_beam_search_related\SimpleHTR\src>  & 'C:\Users\Xxxx\anaconda3\envs\tf-gpu\python.exe' 'c:\Users\Xxxx\.vscode\extensions\ms-python.python-2020.10.332292344\pythonFiles\lib\python\debugpy\launcher' '57165' '--' 'c:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\framework\constant_op.py'
2020-11-06 10:16:00.520456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll  
PS D:\github\word_beam_search_related\SimpleHTR\src> conda activate tf-gpu
PS D:\github\word_beam_search_related\SimpleHTR\src> python main.py
2020-11-06 10:16:54.911411: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
Validation character error rate of saved model: 10.624916%
2020-11-06 10:16:56.574477: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-11-06 10:16:57.531951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0
coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s
2020-11-06 10:16:57.540214: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-11-06 10:16:57.546025: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll   
2020-11-06 10:16:57.552043: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-11-06 10:16:57.568518: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-11-06 10:16:57.590344: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-11-06 10:16:57.624576: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-11-06 10:16:57.638909: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-11-06 10:16:57.647300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-06 10:16:57.662287: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2020-11-06 10:16:57.680211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0
coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s
2020-11-06 10:16:57.698051: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll  
2020-11-06 10:16:57.700475: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll   
2020-11-06 10:16:57.711009: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-11-06 10:16:57.715823: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll   
2020-11-06 10:16:57.729503: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-11-06 10:16:57.734204: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll 
2020-11-06 10:16:57.744820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-11-06 10:16:57.749826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-06 10:17:00.001152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-06 10:17:00.007966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-11-06 10:17:00.011319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N
2020-11-06 10:17:00.012945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1377 MB memory) -> physical GPU (device: 0, name: GeForce MX130, pci bus id: 0000:02:00.0, compute capability: 5.0)
Traceback (most recent call last):
  File ""main.py"", line 145, in <module>
    main()
  File ""main.py"", line 140, in main
    model = Model(open(FilePaths.fnCharList).read(), decoderType, mustRestore=True, dump=args.dump)
  File ""D:\github\word_beam_search_related\SimpleHTR\src\Model.py"", line 34, in __init__
    self.is_train = tf.Variable(tf.bool, name='is_train')
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\variables.py"", line 260, in __call__
    return cls._variable_v2_call(*args, **kwargs)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\variables.py"", line 254, in _variable_v2_call        
    shape=shape)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\variables.py"", line 235, in <lambda>
    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\variable_scope.py"", line 2645, in default_variable_creator_v2
    shape=shape)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\variables.py"", line 262, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py"", line 1411, in __init__    
    distribute_strategy=distribute_strategy)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py"", line 1543, in _init_from_a    name=""initial_value"", dtype=dtype)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1314, in convert_to_tensor       
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 317, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 258, in constant
    allow_broadcast=True)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 266, in _constant_impl   
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 96, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: Attempt to convert a value (tf.bool) with an unsupported type (<class 'tensorflow.python.framework.dtypes.DType'>) to a Tensor.       
PS D:\github\word_beam_search_related\SimpleHTR\src> python main.py
2020-11-06 10:17:31.484639: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
Validation character error rate of saved model: 10.624916%
2020-11-06 10:17:33.172380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-11-06 10:17:34.133112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0
coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s
2020-11-06 10:17:34.144513: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll  
2020-11-06 10:17:34.151353: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-11-06 10:17:34.168700: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-11-06 10:17:34.182794: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-11-06 10:17:34.199075: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-11-06 10:17:34.214206: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll 
2020-11-06 10:17:34.237372: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-11-06 10:17:34.244143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-06 10:17:34.249031: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2020-11-06 10:17:34.264222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0
coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s
2020-11-06 10:17:34.279950: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll  
2020-11-06 10:17:34.282239: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll   
2020-11-06 10:17:34.292704: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-11-06 10:17:34.296967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll   
2020-11-06 10:17:34.299030: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll 
2020-11-06 10:17:34.310039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-11-06 10:17:34.314767: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll     
2020-11-06 10:17:34.326801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-06 10:17:35.667289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-06 10:17:35.671078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0
2020-11-06 10:17:35.672623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N
2020-11-06 10:17:35.674178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1377 MB memory) -> physical GPU (device: 0, name: GeForce MX130, pci bus id: 0000:02:00.0, compute capability: 5.0)
Traceback (most recent call last):
  File ""main.py"", line 145, in <module>
    main()
  File ""main.py"", line 140, in main
    model = Model(open(FilePaths.fnCharList).read(), decoderType, mustRestore=True, dump=args.dump)
  File ""D:\github\word_beam_search_related\SimpleHTR\src\Model.py"", line 34, in __init__
    self.is_train = tf.Variable(tf.bool, name='is_train')
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\variables.py"", line 260, in __call__
    return cls._variable_v2_call(*args, **kwargs)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\variables.py"", line 254, in _variable_v2_call        
    shape=shape)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\variables.py"", line 235, in <lambda>
    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\variable_scope.py"", line 2645, in default_variable_creator_v2
    shape=shape)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\variables.py"", line 262, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py"", line 1411, in __init__    
    distribute_strategy=distribute_strategy)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py"", line 1543, in _init_from_args
    name=""initial_value"", dtype=dtype)
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 317, in _constant_tensor_conversion_function
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 258, in constant
    allow_broadcast=True)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 266, in _constant_impl   
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 96, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: Attempt to convert a value (tf.bool) with an unsupported type (<class 'tensorflow.python.framework.dtypes.DType'>) to a Tensor.       
PS D:\github\word_beam_search_related\SimpleHTR\src>  cd 'd:\github\word_beam_search_related\SimpleHTR\src'; & 'C:\Users\Xxxx\anaconda3\envs\tf-gpu\python.exe' 'c:\Users\Xxxx\.vscode\extensions\ms-python.python-2020.10.332292344\pythonFiles\lib\python\debugpy\launcher' '57192' '--' 'c:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\framework\constant_op.py' 
2020-11-06 10:18:08.558201: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll  
PS D:\github\word_beam_search_related\SimpleHTR\src>  cd 'd:\github\word_beam_search_related\SimpleHTR\src'; & 'C:\Users\Xxxx\anaconda3\envs\tf-gpu\python.exe' 'c:\Users\Xxxx\.vscode\extensions\ms-python.python-2020.10.332292344\pythonFiles\lib\python\debugpy\launcher' '57226' '--' 'd:\github\word_beam_search_related\SimpleHTR\src\main.py' 
2020-11-06 10:20:06.521317: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
Validation character error rate of saved model: 10.624916%
2020-11-06 10:20:08.993191: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-11-06 10:20:09.962429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0
coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s
2020-11-06 10:20:09.969303: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll  
2020-11-06 10:20:09.975677: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-11-06 10:20:09.989720: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-11-06 10:20:10.004558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll   
2020-11-06 10:20:10.012081: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-11-06 10:20:10.027702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-11-06 10:20:10.061178: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-11-06 10:20:10.071325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-06 10:20:10.087199: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2020-11-06 10:20:10.105630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0
coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s
2020-11-06 10:20:10.121203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll  
2020-11-06 10:20:10.123716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll   
2020-11-06 10:20:10.135764: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-11-06 10:20:10.141182: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll   
2020-11-06 10:20:10.152498: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-11-06 10:20:10.158452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll 
2020-11-06 10:20:10.168864: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll     
2020-11-06 10:20:10.173096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-11-06 10:20:11.486600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-06 10:20:11.492343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0
2020-11-06 10:20:11.502868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-11-06 10:20:11.507228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1377 MB memory) -> physical GPU (device: 0, name: GeForce MX130, pci bus id: 0000:02:00.0, compute capability: 5.0)
We've got an error while stopping in unhandled exception: <class 'ValueError'>.
Traceback (most recent call last):
  File ""c:\Users\Xxxx\.vscode\extensions\ms-python.python-2020.10.332292344\pythonFiles\lib\python\debugpy\_vendored\pydevd\pydevd.py"", line 1994, in do_stop_on_unhandled_exception
    self.do_wait_suspend(thread, frame, 'exception', arg, EXCEPTION_TYPE_UNHANDLED)
  File ""c:\Users\Xxxx\.vscode\extensions\ms-python.python-2020.10.332292344\pythonFiles\lib\python\debugpy\_vendored\pydevd\pydevd.py"", line 1855, in do_wait_suspend
    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)
  File ""c:\Users\Xxxx\.vscode\extensions\ms-python.python-2020.10.332292344\pythonFiles\lib\python\debugpy\_vendored\pydevd\pydevd.py"", line 1890, in _do_wait_suspend
    time.sleep(0.01)
KeyboardInterrupt
Traceback (most recent call last):
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""c:\Users\Xxxx\.vscode\extensions\ms-python.python-2020.10.332292344\pythonFiles\lib\python\debugpy\__main__.py"", line 45, in <module> 
    cli.main()
  File ""c:\Users\Xxxx\.vscode\extensions\ms-python.python-2020.10.332292344\pythonFiles\lib\python\debugpy/..\debugpy\server\cli.py"", line 430, in main
    run()
  File ""c:\Users\Xxxx\.vscode\extensions\ms-python.python-2020.10.332292344\pythonFiles\lib\python\debugpy/..\debugpy\server\cli.py"", line 267, in run_file
    runpy.run_path(options.target, run_name=compat.force_str(""__main__""))
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\runpy.py"", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""d:\github\word_beam_search_related\SimpleHTR\src\main.py"", line 145, in <module>
    main()
  File ""d:\github\word_beam_search_related\SimpleHTR\src\main.py"", line 140, in main
    model = Model(open(FilePaths.fnCharList).read(), decoderType, mustRestore=True, dump=args.dump)
  File ""d:\github\word_beam_search_related\SimpleHTR\src\Model.py"", line 34, in __init__
    self.is_train = tf.Variable(tf.bool, name='is_train')
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\variables.py"", line 260, in __call__
    return cls._variable_v2_call(*args, **kwargs)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\variables.py"", line 254, in _variable_v2_call        
    shape=shape)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\variables.py"", line 235, in <lambda>
    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\variable_scope.py"", line 2645, in default_variable_creator_v2
    shape=shape)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\variables.py"", line 262, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py"", line 1411, in __init__    
    distribute_strategy=distribute_strategy)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py"", line 1543, in _init_from_a    name=""initial_value"", dtype=dtype)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1314, in convert_to_tensor       
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 317, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 258, in constant
    allow_broadcast=True)
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""C:\Users\Xxxx\anaconda3\envs\tf-gpu\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 96, in convert_to_eager_t    return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: Attempt to convert a value (tf.bool) with an unsupported type (<class 'tensorflow.python.framework.dtypes.DType'>) to a Tensor.     "
44640,tf.keras.experimental.WideDeepModel  can not accept two  tf.keras.Model  as input?,"tf.keras.experimental.WideDeepModel claims:


`tf.keras.experimental.WideDeepModel(linear_model, dnn_model, activation=None, **kwargs)`
Args:
linear_mode | a premade LinearModel, its output must match the output of the dnn model.
dnn_model   | atf.keras.Model, its output must match the output of the linear model.
activation     | Activation function. Set it to None to maintain a linear activation.
**kwargs      | The keyword arguments that are passed on to BaseLayer.init. Allowed keyword arguments includename.

I viewd the source code and can not find the difference between linear model and deep model.
When i used this WideDeepModel with two ' tf.keras.Model' , i got some problem.

```
# input_dict key is the feature name,  value is keras Input Layer
deep_embedding_output=layers_utils.get_embed_output(input_dict, embedding_size=16)
deep_embedding_flat_inp = layers.Flatten(deep_embedding_output)
deep_mlp_out= layers_utils.mlp(units=[128,64,16])(deep_embedding_flat_inp)
deep_out=layers.Dense(1)(deep_mlp_out)
deep_channel = Model(inputs=input_dict,output=[deep_out])

wide_embedding_output=layers_utils.get_embed_output(input_dict, embedding_size=1)
wide_embedding_flat_inp = layers.Flatten(wide_embedding_output)
wide_out=layers.Dense(1)(wide_embedding_flat_inp)
deep_channel = Model(inputs=input_dict,output=[wide_out])
# label feature name is label
model = WideDeepModel(wide_channel, deep_channel, act=""sigmoid"", name=label)

model.compile(optimizer=['ftrl', 'adam'], 'binary_cross_entropy', ['auc'])
# `tf.data.Dataset` that contains a single tensor
combined_model.fit(dataset, epochs)
#dataset = tf.data.Dataset.from_tensors((features, label))
```
I met the problem:
linear model expected 94 ,but given 95;
94 is the input feature size, it seems like that the target label was not recognized?



"
44638,cv2.cuda_GpuMat <-> TensorFlow vector / TensorFlow NumPy,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TF  2.3
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Recently presented [API between Numpy and TensorFlow](https://www.tensorflow.org/api_docs/python/tf/experimental/numpy), allows to speed up Numpy functions by utilinzf CUDA supporting GPU's, as demonstrated [here](https://www.youtube.com/watch?v=DpOq0PJ0H38&t). 
Many Computer Vision tools use OpenCV, Numpy and  TensorFlow functions to resolve Computer Vision tasks.
OpenCV has a class allowing moving calculations intenstcive functions to GPU, via the [CUDA-accelerated Computer Vision](https://docs.opencv.org/master/d1/d1e/group__cuda.html). 
Pyhton API is less documented but an introduction can be seen [here](https://www.learnopencv.com/getting-started-opencv-cuda-modul/). 
Currnlty  OpenCV cuda_GpuMat, TensorFlow vectors / TensorFlow NumPy ndArrays are stored in CUDA memory.
An efficient and easy to use type conversion  between OpenCV cuda_GpuMat and TensorFlow vector / TensorFlow NumPy in both directions will allow development of efficient applications that utilize OpenCV, Numpy and  TensorFlow without obsolete memory copies.
 
**Will this change the current api? How?**
Add new 0 copy convertions between tf.experimental.numpy and cv2.cuda_GpuMat classescv2.cuda_GpuMat

**Who will benefit with this feature?**
Computer Vision developers

**Any Other info.**"
44637,False out of resource error for multi-gpu machine,"Tensorflow 2.3.1 on Nvidia Docker image `tensorflow/tensorflow:latest-gpu-jupyter` 

I can train an LSTM in a Jupyter notebook using 2-out-of-4 of my GPUs, but when I increase to 4 I get the following NCCL error:

```
InternalError: 4 root error(s) found.
  (0) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
	 [[node NcclAllReduce (defined at <ipython-input-17-7105c4ec4d5e>:2) ]]
  (1) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
	 [[node NcclAllReduce (defined at <ipython-input-17-7105c4ec4d5e>:2) ]]
	 [[GroupCrossDeviceControlEdges_1/Nadam/Nadam/update_0/Const/_529]]
  (2) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
	 [[node NcclAllReduce (defined at <ipython-input-17-7105c4ec4d5e>:2) ]]
	 [[GroupCrossDeviceControlEdges_2/Nadam/Nadam/update_0/Const/_509]]
  (3) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.
	 [[node NcclAllReduce (defined at <ipython-input-17-7105c4ec4d5e>:2) ]]
	 [[GroupCrossDeviceControlEdges_0/Nadam/Nadam/update_0/Const/_493]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_44874]
```

Looking at the NCCL_DEBUG logs, I find an out of memory error:

![image](https://user-images.githubusercontent.com/3875113/98307202-17298400-1f7a-11eb-9e2d-0bcbf88aa36f.png)

But this is wrong, I have plenty on space left on all my devices, as shown here with nvidia-smi and TF_FORCE_GPU_ALLOW_GROWTH

![image](https://user-images.githubusercontent.com/3875113/98307003-bc902800-1f79-11eb-9d2e-19365d22c3b7.png)

Dummy code:
```python3
def get_compiled_model():
    
    the_input = keras.layers.Input(shape=(window_length, input_length))
    embeddings = keras.layers.Embedding(num_addresses, embedding_dim)(the_input)

    encoded = keras.layers.Bidirectional(keras.layers.LSTM(NUM_HIDDEN))(embeddings)

    reconstr = keras.layers.RepeatVector(window_length)(encoded)
    reconstr = keras.layers.Bidirectional(keras.layers.LSTM(NUM_HIDDEN, return_sequences=True))(reconstr)
    
    reconstr = keras.layers.TimeDistributed(keras.layers.Dense(output_dim, activation='relu'), 
                                                    name=""the_output"")(reconstr)
    
    losses = {
        ""the_output"": keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    }
    lossWeights = LOSS_WEIGHTS
        
    model = keras.Model([
        the_input, 
    ], [
        reconstr
    ])

    model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=LEARNING_RATE),
                  loss=losses)
    
    return model

gpu_devices = [device for device in tf.config.list_physical_devices() if device.device_type=='GPU']
if len(gpu_devices):
    strategy = tf.distribute.MirroredStrategy(devices=['/gpu:0', '/gpu:1', '/gpu:2', '/gpu:3'])
#     strategy = tf.distribute.MirroredStrategy()
    with strategy.scope():
        model = get_compiled_model()
else:
    model = get_compiled_model()

# train_generator is a custom keras.utils.Sequence 
model.fit(x=train_generator, epochs=200)
```
"
44631,TypeError: An op outside of the function building code is being passed,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: MacOs Catalina 10.15
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from: pip
- TensorFlow version: v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.7.7

**Describe the current behavior**
The modelCheckpoint callback seems to trigger the issue, everything run smoothly without it. 


**Standalone code to reproduce the issue**

```
import pandas as pd
import numpy as np
import os
import time

import tensorflow as tf
import tensorflow_hub as hub

def make_dataset(path, n_samples):
    df = pd.read_csv(path, usecols=[6,9], nrows=n_samples)
    df.columns = ['ratings', 'title']

    text = df['title'].tolist()
    text = [str(t).encode('ascii', 'replace') for t in text]
    text = np.array(text, dtype='object')[:]

    labels = df['ratings'].tolist()
    labels = [1 if i>= 4 else 0 if i == 3 else -1 for i in labels]

    labels = np.array(pd.get_dummies(labels), dtype=int)[:]

    return labels, text


def get_model():
    embed = hub.KerasLayer(""https://tfhub.dev/google/nnlm-en-dim50/1"",
     output_shape=[50],
     input_shape=[],
     dtype=tf.string,
     name='input',
     trainable=False)

    model = tf.keras.Sequential()
    model.add(embed)
    model.add(tf.keras.layers.Dense(16, activation='relu'))
    model.add(tf.keras.layers.Dense(3, activation='softmax', name='output'))
    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])
    model.summary()
    return model

def train(epochs=5, bs=32):
    y_train, x_train = make_dataset('reviews_train.csv', n_samples=100000)
    y_val, x_val = make_dataset('reviews_test.csv', n_samples=10000)
    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
                                filepath='model_checkpoint',
                                save_weights_only=False,
                                monitor='val_acc',
                                mode='max',
                                save_best_only=True)
    model = get_model()
    model.fit(x_train, y_train, batch_size=bs, epochs=epochs, verbose=1,
         validation_data=(x_val, y_val),
         callbacks=[model_checkpoint_callback])



if __name__ == ""__main__"":
    train()

```
**Other info / logs** 
`Traceback (most recent call last):
  File ""train.py"", line 58, in <module>
    train()
  File ""train.py"", line 53, in train
    callbacks=[model_checkpoint_callback])
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1137, in fit
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py"", line 412, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py"", line 1249, in on_epoch_end
    self._save_model(epoch=epoch, logs=logs)
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py"", line 1301, in _save_model
    self.model.save(filepath, overwrite=True, options=self._options)
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1979, in save
    signatures, options)
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py"", line 134, in save_model
    signatures, options)
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save.py"", line 80, in save
    save_lib.save(model, filepath, signatures, options)
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py"", line 985, in save
    options=ckpt_options)
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/training/tracking/util.py"", line 1200, in save
    file_prefix_tensor, object_graph_tensor, options)
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/training/tracking/util.py"", line 1145, in _save_cached_when_graph_building
    save_op = saver.save(file_prefix, options=options)
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/training/saving/functional_saver.py"", line 293, in save
    tf_function_save()
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 846, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 550, in call
    ctx=ctx)
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 75, in quick_execute
    raise e
  File ""/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: embeddings/part_0/Read/ReadVariableOp:0`
"
44630,tf-nightly container has CUDA error on 'docker run',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.6 LTS
- TensorFlow installed from (source or binary): Docker
- Docker image: tensorflow/tensorflow:nightly-gpu-jupyter
- GPU model and memory: GeForce GTX 108 / 261MiB / 11178MiB

**Describe the problem**
Error regarding CUDA>=11.0 when running most recent nightly container

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Dockerfile
```
FROM tensorflow/tensorflow:nightly-gpu-jupyter

RUN pip install -U pip

# install packages
RUN apt-get update -qq && apt-get install --no-install-recommends -y \    
    libsm6 \
    libxext6 \
    libxrender-dev \
 && apt-get clean \
 && rm -rf /var/lib/apt/lists/*

COPY requirements.txt /tmp/
RUN pip install --requirement /tmp/requirements.txt
COPY . /tmp/

WORKDIR /tf/

CMD jupyter lab --ip=0.0.0.0 --notebook-dir=/tf --allow-root
```

requirements.txt
```
tensorflow-hub
imbalanced-learn==0.7.0
jupyterlab==2.1.2
opencv-python==4.2.0.34
pandas==1.0.3
Pillow==7.1.2
ptvsd==4.3.2
scikit-learn==0.23.2
scipy==1.4.1
seaborn==0.11.0
sklearn==0.0
xlrd==1.2.0
```

Build image
```
$ docker build --no-cache=true -t tf:tf-nightly .
```

Run container:
```
$ nvidia-docker run --rm -d -v /home:/tf -p 9896:8888 tf:tf-nightly
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Error:
```
docker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused ""process_linux.go:430: container init caused \""process_linux.go:413: running prestart hook 1 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: requirement error: unsatisfied condition: cuda>=11.0\\\\n\\\""\"""": unknown.
```"
44628,Move EthosU custom op out of AllOpsResolver,"@tensorflow/micro

The EthosU custom op https://github.com/tensorflow/tensorflow/blob/33dbf934d3758dfee07c0b5f6f4a6a00939b1b42/tensorflow/lite/micro/all_ops_resolver.cc#L86-L92

should ideally only be used for EthosU applications and not be part of the AllOpsResolver.

Creating this issue to figure out how we might achieve that."
44626,ambiguous template instantiation error with Eigen code,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.7
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.4-rc0
- Bazel version (if compiling from source): 3.4.1

**Describe the problem**

2.4 introduces a regression in the compilation fails on POWER systems with ""ambiguous template instantiation"".
The reason is that TF adds specializations for Eigen internal classes which conflict with specializations done by Eigen itself.

In this case https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/eigen_spatial_convolutions-inl.h adds `Eigen::internal::gemm_pack_rhs` specializations, e.g. at https://github.com/tensorflow/tensorflow/blob/300b62ff6a53a554bf8699f0632a0f517419b02c/tensorflow/core/kernels/eigen_spatial_convolutions-inl.h#L1044

However the updated Eigen (in particular commit https://gitlab.com/libeigen/eigen/-/commit/6fe88a3c9db27c00a3817e391cf70116451bf046) adds an own specialization for POWER architectures, see e.g. https://gitlab.com/libeigen/eigen/-/commit/6fe88a3c9db27c00a3817e391cf70116451bf046#b3394fe72fd1b6f1213ded1197953458e6851483_23_2622

This then leads to the ambiguous template instantiation of 
```
template<class Scalar, class Index, class DataMapper, int nr, bool Conjugate, bool PanelMode>
struct Eigen::internal::gemm_pack_rhs<Scalar, Index, DataMapper, nr, 0, Conjugate, PanelMode>
```
because the Eigen version specializes the `Scalar`, while TF specializes all but Scalar, Index and nr.

For reference the types that this is tried to be instantiated are (taken from the log, slightly formatted): 

```
Scalar = double;
Index = long int;
DataMapper = Eigen::internal::TensorContractionSubMapper<double, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const double, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::ThreadPoolDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 2, true, true, 0, Eigen::MakePointer>;
int nr = 4;
bool Conjugate = false;
bool PanelMode = false
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Compile on POWER according to instructions

**Any other info / logs**

While this currently may only happen on POWER there is always the risk of breaking things when adding to internals of another library. So it should be checked, if that is really required.
If it is, I'd suggest to extract the 4 implementations of `gemm_pack_rhs` into 4 functions. Then specializations can be added for each datatype that Eigen also specializes for which avoids the ambiguity.

A short term solution might also be to simply remove the Eigen specializations via a patch."
44625,TFLite quantization aware converter - 'std.constant' op requires attribute's type ('tensor<1x64xf32>') to match op's return type,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): Source
- TensorFlow version (or github SHA if from source): 2.3.0


**Command used to run the converter or code if youre using the Python API**
https://github.com/UNCG-DAISY/TinyDuneCollision/blob/master/src/TinyML_End_to_End.ipynb

See Quantization aware training

```
converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
```

**The output from the converter invocation**

```
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
~/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    198                                                  debug_info_str,
--> 199                                                  enable_mlir_converter)
    200       return model_str

~/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/lite/python/wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
     37       debug_info_str,
---> 38       enable_mlir_converter)
     39 

Exception: /home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/layers/ops/core.py:53:0: error: 'std.constant' op requires attribute's type ('tensor<1x64xf32>') to match op's return type ('tensor<*xf32>')
/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/layers/core.py:1198:0: note: called from
/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize_wrapper.py:162:0: note: called from
/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/autograph/impl/api.py:302:0: note: called from
/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py:985:0: note: called from
/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/engine/functional.py:508:0: note: called from
/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/engine/functional.py:386:0: note: called from
/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/engine/sequential.py:372:0: note: called from
/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py:985:0: note: called from
/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/saving/saving_utils.py:134:0: note: called from
/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/layers/ops/core.py:53:0: note: see current operation: %cst_13 = ""std.constant""() {value = dense<[[0.000000e+00, -0.330023378, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, -0.330023378, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, -0.330023378, 0.000000e+00, 0.000000e+00, 0.330023378, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.282877177, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.282877177, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00]]> : tensor<1x64xf32>} : () -> tensor<*xf32>
```

**Model Summary**

![Screen Shot 2020-11-05 at 11 03 24 AM](https://user-images.githubusercontent.com/2720456/98265106-afae0d00-1f56-11eb-95ca-2d9f2126ff8b.png)


**Failure details**
Converter is not able to create TFLite model



"
44624,cuDNN LSTMs not utilized after loading a model using `tf.saved_model.load`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **2.3.1**
- Python version: **3.7**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **10.1/7.6.2**
- GPU model and memory: **GeForce GTX 1080 Ti computeCapability: 6.1 coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s**

**Describe the current behavior**

Training a (keras) LSTM model on GPU utilizes (as expected) the cuDNN version of the LSTM cells.
After saving this model using `tf.saved_model.save` and reloading it using `tf.saved_model.load`, it utilizes the 
NON cuda optimized LSTM cells in inference. (loading it using `tf.keras.models.load_model` it is utillizing the cuDNN cells)

**Describe the expected behavior**

The save/load process shouldn't change the graph. The model should use the same cells as in training after save/load.  I know that I can't expect the same functionality as if I use the higher level keras load/save methods. Consequently, it is acceptable if the automatic device detection does not work. However, it is highly not expected that the used cells change after save/load. If the automatic device detection is not supported, the cells, which were active at the moment of saving, should be loaded by default. Like it is now, it is not possible to create a saved_model in a straightforward way which utilizes the cuDNN cells, if NOT loaded using keras (which is not possible in other APIs: cc, java, ...)

**Standalone code to reproduce the issue**

```python
import os
import time
import shutil

import tensorflow as tf
from tensorflow import keras

assert len(tf.config.list_physical_devices('GPU'))>0, ""You have to run this on a GPU machine, otherwise you do not see the effect.""
print(tf.config.list_physical_devices('GPU'))

model_save_path='tmp'

lstm_layer = keras.layers.LSTM(64, input_shape=(None, 28))
model = keras.models.Sequential([lstm_layer, keras.layers.Dense(10)])

mnist = keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        optimizer=""sgd"", metrics=[""accuracy""])
model.fit(x_train[0:128], y_train[0:128], validation_data=(x_test, y_test), batch_size=64, epochs=1)
tf.saved_model.save(model, model_save_path)

# Keras load + inference
model_reload_keras = tf.keras.models.load_model(model_save_path)
inp = tf.ones(shape=[8, 512, 28])
for i in range(501):
    # Avoid load offset.
    if i == 1:
        startT = int(round(time.time() * 1000))
    model_reload_keras.predict_on_batch(inp)
endT = int(round(time.time() * 1000))
print('Keras load -- inference time: ' + str(endT-startT) + 'ms')

# TF load + inference
model_reload_tf = tf.saved_model.load(str(model_save_path))
infer = model_reload_tf.signatures['serving_default']
for i in range(501):
    # Avoid load offset.
    if i == 1:
        startT = int(round(time.time() * 1000))
    infer(inp)
endT = int(round(time.time() * 1000))
print('TF load -- inference time (should be as fast as previous run [on GPU]): ' + str(endT - startT) + 'ms')

shutil.rmtree(model_save_path)
```

Kind regards & Thanks
Tobias
"
44623,TensorFlow 2.4 & roadmap,"Hello,

It's been released ver. 2.4-rc0 of TensorFlow, can we expect ver.2.4.x this year?

Thank you.

 "
44622,Tensorflow 2.3.1 is not compatible with cuda 10.2 on Windows,"**System information**

- OS Platform: Windows 20H2 19042.572 64bit;
- TensorFlow installed from (source or binary): source;
- TensorFlow version: 2.3.1;
- Python version: 3.8.6 64bit;
- Bazel version (if compiling from source): 3.1.0;
- Compiler version: MSVC 2019 16.7.7
- CUDA/cuDNN version: CUDA 10.2 with cuDNN 7.6.5.


**Describe the problem**

Build failed with cuda support:


**Any other info / logs**

```
.\tensorflow/core/kernels/cuda_sparse.h(39): error: identifier ""cusparseDnMatDescr_t"" is undefined

.\tensorflow/core/kernels/cuda_sparse.h(40): error: identifier ""cusparseSpMatDescr_t"" is undefined

.\tensorflow/core/kernels/cuda_sparse.h(41): error: identifier ""cusparseSpMMAlg_t"" is undefined

tensorflow/core/kernels/tridiagonal_solve_op_gpu.cu.cc(98): warning: extra "";"" ignored

3 errors detected in the compilation of ""C:/Users/SPINDE~1/AppData/Local/Temp/nvcc_inter_files_tmp_dir/tmp3nb_ygoq/tridiagonal_solve_op_gpu.cu.cpp1.ii"".
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 6783.098s, Critical Path: 4048.81s
INFO: 5532 processes: 5532 local.
FAILED: Build did NOT complete successfully
```

**Analysis**

These problematic symbols are used in `tensorflow/core/kernels/cuda_sparse.h` (line 38-42): 

```cpp {.line-numbers}
#if CUDA_VERSION >= 10020
using gpusparseDnMatDescr_t = cusparseDnMatDescr_t;
using gpusparseSpMatDescr_t = cusparseSpMatDescr_t;
using gpusparseSpMMAlg_t = cusparseSpMMAlg_t;
#endif
```

But in cuda 10.2 `cusparse.h` (`C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\include\cusparse.h`), these symbols are defined in a `#if !defined(_WIN32) ... #endif` block (line 7341-7386, 7700):

```cpp
//##############################################################################
//# SpMM APIs
//##############################################################################

#if !defined(_WIN32)

typedef enum {
    CUSPARSE_MM_ALG_DEFAULT = 0,
    CUSPARSE_COOMM_ALG1 = 1, // non-deterministc results
    CUSPARSE_COOMM_ALG2 = 2, // deterministic results
    CUSPARSE_COOMM_ALG3 = 3, // non-deterministc results, for large matrices
    CUSPARSE_CSRMM_ALG1 = 4
} cusparseSpMMAlg_t;

typedef struct cusparseSpMatDescr* cusparseSpMatDescr_t;
typedef struct cusparseDnMatDescr* cusparseDnMatDescr_t;

#endif // !defined(_WIN32)
```

According to the [Microsoft documentation](https://docs.microsoft.com/en-us/cpp/preprocessor/predefined-macros?view=msvc-160):

```
_WIN32 Defined as 1 when the compilation target is 32-bit ARM, 64-bit ARM, x86, or x64. Otherwise, undefined.
```

So these symbols are not defined when the compilation target is x64 on Windows and the build always fails.

By the way, the latest version in `master` branch has the same problem."
44616,Plans to release 2.4.0 as a stable release?,"Hello, I was just wondering if a date when 2.4.0 will be released as a stable version is currently available?"
44615,Could not find a version that satisfies the requirement tensorflow (from versions: none),"System information

* OS Platform and Distribution: masOS Big Sur
* TensorFlow installed from (source or binary): binary
* TensorFlow version: none
* Python version: 3.8.2
* Installed using virtualenv? pip? conda?: virtualenv + pip

Describe the problem
Could not find a version that satisfies the requirement tensorflow (from versions: none)

Provide the exact sequence of commands / steps that you executed before running into the problem
```bash
  ~ python -m pip install --upgrade tensorflow
Defaulting to user installation because normal site-packages is not writeable
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensor flow
```
```bash
  ~ python3 -m pip install --upgrade tensorflow==2.3.0                                        
Defaulting to user installation because normal site-packages is not writeable
ERROR: Could not find a version that satisfies the requirement tensorflow==2.3.0 (from versions: none)
ERROR: No matching distribution found for tensorflow==2.3.0
```"
44614,Different behavior of model.predict and .call if training=True at dropout layer with pre-set seed,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.7

**Describe the current behavior**
Different behavior of `model.predict `and `.call (model(...)),` if `training=True` is used at dropout layer. It seems that the call method does not take into account pre-set dropout seeds, while the predict method does.

**Describe the expected behavior**
Same behavior of the predict and call method if the same dropout seed is set and `training=True`.

**Standalone code to reproduce the issue**
```
import numpy as np
import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras.models import Model

def build_toy():
    
    inputs = layers.Input(shape=(1,), name=""inputs"")
    x = layers.Dense(1, activation=""linear"", kernel_initializer=tf.keras.initializers.Constant(value=1), name=""dense"") (inputs)
    x = layers.Dropout(0.5, seed=12, name=""dropout"") (x,  training=True)
    out = x
    
    model = Model(inputs, out)
    
    return model

data = np.array(([1]))
    
predict_model = build_toy()
print(predict_model.predict(data))

call_model = build_toy()
print(call_model(data))

```
`predict_model` always predicts the same value every time the model is newly built, while `call_model(data) ` may changes.
"
44613,[TF 2.4] KerasTensor breaks typing compatibility,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-rc0 and tf-nightly
- Python version: 3.8

## The Problem
Since version 2.4 functional Keras models use `KerasTensor` instead of `tf.Tensor` as layer output types. Unfortunately `KerasTensor` doesn't subclass `tf.Tensor` which breaks `isinstance(x, tf.Tensor)` checks:
https://github.com/tensorflow/tensorflow/blob/6d9e0887f6bce8fbeeb430364e520d05350d96d5/tensorflow/python/keras/engine/keras_tensor.py#L63

The release notes recommend to use `tf.is_tensor` instead.
In my opinion this is not really Pythonic and breaks compatibility with the [TF Types RFC](https://github.com/tensorflow/community/blob/master/rfcs/20200211-tf-types.md) (/cc @mdanatg) wich even mentiones that `tf.is_tensor` is expected to be deprecated eventually.

Concretely, switching from `isinstance(x, tf.Tensor)` to `tf.is_tensor` is also not possible in all cases. E.g. this breaks usage of static typecheckers like `pytype` or `typeguard`:

A common pattern which can also be found in TensorFlow Addons (/cc @seanpmorgan) is the following:

```python
from typeguard import typechecked
import tensorflow as tf

@typechecked
def foo(x: tf.Tensor):
    print(x.dtype)

foo(tf.keras.Input(shape=(32, 32, 3)))  # Throws in TF 2.4 since `isintance` is used for typechecking
```

## Possible solutions

1. Make `KerasTensor` a subclass of `tf.Tensor`. @mihaimaruseac @fchollet is there a reason why this isn't currently the case?

2. Make `KerasTensor` a subclass of [`types.Tensor`](https://github.com/tensorflow/tensorflow/blob/6d9e0887f6bce8fbeeb430364e520d05350d96d5/tensorflow/python/types/core.py#L40-L54). I don't see any disadvantage of doing this in general, but it wouldn't really fix this issue since `types.Tensor` is not exposed as part of the public API yet so users would need to rely on private TensorFlow APIs

3. In usercode this could be fixed by directly relying on `KerasTensor` to replace the usage of `tf.Tensor` with:
   ```python
   from typing import Union
   from tensorflow.python.keras.engine.keras_tensor import KerasTensor

   TensorType = Union[tf.Tensor, KerasTensor]
   ```
   I do not think this is a proper solution since it requires users to rely on internal APIs and implementation details that might change in the future.

I am currious to hear back from you on what the bestpractices for type checking of Tensors are, or whether I am just missing somthing obvious here."
44612,Slow Interpreter creation with GpuDelegate option,"I use TFLite 2.3.0 on Android device

**System information**
- Mobile device Xiaomi Redmi 5, GPU: Adreno(TM) 506

TensorFlow libraries imported in standard way: 
```
    implementation 'org.tensorflow:tensorflow-lite:2.3.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu:2.3.0'
```
Instance creation:

```
    init {
        val options = Interpreter.Options().apply {
            if (compatList.isDelegateSupportedOnThisDevice) {
                addDelegate(GpuDelegate(compatList.bestOptionsForThisDevice))
            } else {
                setNumThreads(DEFAULT_THREADS_COUNT)
                setUseNNAPI(DEFAULT_USE_NNAPI_OPTIONS)
            }
        }
        interpreter = Interpreter(loadModelFile(model), options)
    }

    private fun loadModelFile(data: ByteArray): ByteBuffer {
        return ByteBuffer.allocateDirect(data.size).put(data).order(ByteOrder.nativeOrder())
    }
```

**Describe the current behavior**
`Interpreter(loadModelFile(model), options)` takes 2400ms with GpuDelegate instead of several hundreds ms without it. I have measured `loadModelFile(model)` separately and this fun did not takes significant time.

**Describe the expected behavior**
I do not know, may be it is normal but it is not so convenient that instance creation takes so much time, all performance improvement that get from gpu using leveled by this delay."
44611,Where is the TRTEngine saved? ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
source
- TensorFlow version (use command below):
2.3.0
- Python version:
3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
11.1
- GPU model and memory:

I followed this answer https://stackoverflow.com/a/60744486/3265888 on stackoverflow and expect to see TRTEngine being serialized into the assets folder. I followed the doc here: https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter and used the `convert.build(input_fn)` method and everything execute without error. But when I check the savedmodel, assets folder is empty. Also when I try to run that model on jetson, the loading takes really long so I'm guessing it's generating TRTEngine on the fly? Is it a bug? or what's the correct expectation should be? 

Apart from the above, I also don't know if the `maximum_cached_engines` option takes effect or not cus when I loaded the model and do inference, when I dynamically change the input shape (the batch_size), it seems it cached more than 1 even I'm using 1 for that option cus the second inference with that batch size is much faster than the first one. I'm not sure if batch size is counted as different shape? or only the actual shape counts as that? 


This is how I load the model and do inference:
```
  saved_model_loaded = tf.saved_model.load(
      trt_model_dir, tags=[tag_constants.SERVING])
  graph_func = saved_model_loaded.signatures[
      signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
  graph_func = convert_to_constants.convert_variables_to_constants_v2(graph_func)

  graph_func(input)
```"
44610,Convert Tensorflow model to TFLite model - problem with boxes after quantization,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google colab
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source): tf-nightly==2.5.0.dev20201104



I am currently working on converting a tensorflow model to a TFLite model. By helping people from the Tensorflow community, in particular @amahendrakar.  Currently, after many attempts, I am able to convert the tensorflow model to the TFLite model
However, there was another problem with my work. Description of the problem:

**STEP 1**
In the first step, I download the ```ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8``` model and I run object detector for test image. Result (Everything is OK  but colors on the image are different than in original image):
![p1](https://user-images.githubusercontent.com/28406311/98230792-dcb0ee80-1f5b-11eb-954c-ee48938fe982.jpg)

**STEP 2**
Export model to tflite graph using
```
!python object_detection/export_tflite_graph_tf2.py \
    --pipeline_config_path ""/content/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config"" \
    --trained_checkpoint_dir ""/content/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint""\
    --output_directory ""/content/tflite_graph""
```
and convert tflite graph to tflite file using
```
converter = tf.lite.TFLiteConverter.from_saved_model('/content/tflite_graph/saved_model')
tflite_model = converter.convert()

with open('/content/tflite_model/model.tflite', 'wb') as f:
  f.write(tflite_model)
```
Run object detector. Result (Everything is OK):
![p2](https://user-images.githubusercontent.com/28406311/98231056-35808700-1f5c-11eb-8114-425909f24bfe.jpg)

**STEP 3**
Convert tflite graph to tflite file with ```tf.lite.Optimize.DEFAULT```:
```
converter = tf.lite.TFLiteConverter.from_saved_model('/content/tflite_graph/saved_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
model_quantization_1 = converter.convert()

with open('/content/tflite_model/model_quantization_1.tflite', 'wb') as f:
  f.write(model_quantization_1)
```
Run object detector. Result (Everything is OK):
![p3](https://user-images.githubusercontent.com/28406311/98231389-a758d080-1f5c-11eb-86b9-8d3ef85fbd51.jpg)

**STEP 4**
Convert tflite graph to tflite file with quantization:
```
converter = tf.lite.TFLiteConverter.from_saved_model('/content/tflite_graph/saved_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.representative_dataset = representative_data_gen
model_quantization_2 = converter.convert()

with open('/content/tflite_model/model_quantization_2.tflite', 'wb') as f:
  f.write(model_quantization_2)
```
Run object detector. Result:
![p4](https://user-images.githubusercontent.com/28406311/98231549-d707d880-1f5c-11eb-9e1a-2862bd6deb46.jpg)

As we can see the is a problem with boxes. The size od model is the same after run ```converter.convert()``` but there is a problem with boxes. Nest step will be run final TFLite model in google coral but now I would like to solve the current problem. 

Full code to reproduce the problem: 
[ssd_modilenet_to_google_coral_step_1](https://colab.research.google.com/drive/1UBikIZmoJTuxWMXhyggBugM6dF7AWK4c?usp=sharing)

I'm not sure, but it seems to me that the problem lies in the ````representative_data_gen()``` function and in the preprocessing of the imege in the object detector.

But, there is one but. If i run model from step 2 the calculation are very fast unlike of step 3 and 4.


"
44609,Make TensorFlow Lite available as Swift Package Manager package,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): Lite 2.3.0
- Are you willing to contribute it (Yes/No): yes



**Describe the feature and the current behavior/state.**
Right now, TensorFlow Lite is available as a CocoaPod or as source for iOS developers. CocoaPods cannot be used from inside a Swift Package Manager (SPM) package. Since SPM is the official package manager from Apple and the way forward, it would be nice to have TensorFlow Lite available as an SPM package or an XCFramwork, in addition to the current CocoaPods distribution.

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
iOS developers who are using SPM for module manager or who are not using CocoaPods anymore.

**Any Other info.**
I have tried creating my own SPM package or XCFramework from the source but could not get it to work because of the native code. But I'm confident it should be possible."
44608,Graph disconnected when using transfer learning,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: Python 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: (nvcc) 10.1, V10.1.243; 
- GPU model and memory: Nvidia 2080 Ti

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

When trying to obtain values for intermediete layers of a pre-trained model, I get a ""Graph disconnected"" error, even though it is connected. I can train it, evaluate it, predict images, etc.

Fragment of code (the whole code is in a Colab - below):

```
img_size = (299, 299)
IMG_SHAPE = (*img_size, 3)
base_model = tf.keras.applications.InceptionResNetV2(input_shape=IMG_SHAPE,
                                                    include_top=False,
                                                    weights='imagenet')
base_model.trainable = False
preprocess_input = tf.keras.applications.inception_resnet_v2.preprocess_input

data_augmentation = [
  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal_and_vertical'),
  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2, fill_mode=""constant"")  
]

data_augmentation = tf.keras.Sequential(data_augmentation)

global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

top_layers = []
top_layers.extend([
    tf.keras.layers.Dense(512, activation=""relu"", 
                          kernel_initializer=""glorot_normal"", 
                          bias_initializer=""glorot_uniform""),
    tf.keras.layers.Dropout(0.2)
])

prediction_layer = tf.keras.layers.Dense(1, activation=""sigmoid"", 
                                         kernel_initializer=""glorot_normal"", 
                                         bias_initializer=""glorot_uniform"")

inputs = tf.keras.Input(shape=(*img_size, 3))
x = data_augmentation(inputs)
x = preprocess_input(x)
x = base_model(x, training=False)
x = global_average_layer(x)
x = tf.keras.layers.Dropout(0.2)(x)
for layer in top_layers:
    x = layer(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)

(...)

model2 = tf.keras.Model(inputs=model.input, 
                        outputs=model.get_layer('inception_resnet_v2').output)
```

**Describe the expected behavior**

Possibility to obtain values of intermediete layers of a pre-trained model - maybe possibility to see all intermediate layers in model.summary(), because currently it looks like this:

```
Model: ""functional_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 299, 299, 3)]     0         
_________________________________________________________________
sequential (Sequential)      (None, 299, 299, 3)       0         
_________________________________________________________________
tf_op_layer_RealDiv (TensorF [(None, 299, 299, 3)]     0         
_________________________________________________________________
tf_op_layer_Sub (TensorFlowO [(None, 299, 299, 3)]     0         
_________________________________________________________________
inception_resnet_v2 (Functio (None, 8, 8, 1536)        54336736  
_________________________________________________________________
global_average_pooling2d (Gl (None, 1536)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 1536)              0         
_________________________________________________________________
dense (Dense)                (None, 512)               786944    
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 513       
=================================================================
Total params: 55,124,193
Trainable params: 787,457
Non-trainable params: 54,336,736
_________________________________________________________________
```

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/drive/1cgwJho63-QtQYS3Tdk4O5jioDu_szYjl?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-9-a7165f00310b> in <module>()
      1 model2 = tf.keras.Model(inputs=model.input, 
----> 2                         outputs=model.get_layer('inception_resnet_v2').output)

5 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in __new__(cls, *args, **kwargs)
    240       # Functional model
    241       from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top
--> 242       return functional.Functional(*args, **kwargs)
    243     else:
    244       return super(Model, cls).__new__(cls, *args, **kwargs)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py in __init__(self, inputs, outputs, name, trainable)
    113     #     'arguments during initialization. Got an unexpected argument:')
    114     super(Functional, self).__init__(name=name, trainable=trainable)
--> 115     self._init_graph_network(inputs, outputs)
    116 
    117   @trackable.no_automatic_dependency_tracking

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py in _init_graph_network(self, inputs, outputs)
    189     # Keep track of the network's nodes and layers.
    190     nodes, nodes_by_depth, layers, _ = _map_graph_network(
--> 191         self.inputs, self.outputs)
    192     self._network_nodes = nodes
    193     self._nodes_by_depth = nodes_by_depth

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py in _map_graph_network(inputs, outputs)
    929                              'The following previous layers '
    930                              'were accessed without issue: ' +
--> 931                              str(layers_with_complete_input))
    932         for x in nest.flatten(node.outputs):
    933           computable_tensors.add(id(x))

ValueError: Graph disconnected: cannot obtain value for tensor Tensor(""input_1:0"", shape=(None, 299, 299, 3), dtype=float32) at layer ""conv2d"". The following previous layers were accessed without issue: []
"
44607,tensorflow lite Makefile issue : nnapi_delegate_provider.cc is not compiled and nnapi cannot be used from command line of benchmarck_model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): yocto build system
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): compiled from source
- TensorFlow version: 2.3
- Python version: NA
- Installed using virtualenv? pip? conda?: NA
- Bazel version (if compiling from source): Not using bazel but Make
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: NA 
- GPU model and memory: NA

**Describe the problem**
I am trying to compile tensoflow-lite using the Makfile. I want to use nnapi. 
The issue is: nnapi_delegate_provider.cc is not compiled so we cannot use the switch `--use_nnapi=1` with `benchmark_model`

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`make -f tensorflow/lite/tools/make/Makefile BUILD_WITH_NNAPI=true`

**Any other info / logs**
Here is a quick patch that solve the issue : [0001-Add-nnapi_delegate_provider.cc-to-the-list-of-source.txt](https://github.com/tensorflow/tensorflow/files/5492886/0001-Add-nnapi_delegate_provider.cc-to-the-list-of-source.txt)
I would be very happy to open a merge request on it. But I need  feedback before, because I think the same problem happens for all delegates (I only tested nnapi).  I don't see why we filter out all delegates provider from `BENCHMARK_LIB_SRCS` variable:

- right now, they are not added anywhere, so it does nothing (or I missed it inside the Makefile)
- filter them out is not what should be done (I guess), but we should instead add the providers

 as needed

What do you think? Maybe I completely miss something.. 
Best 

"
44606,Functionality of writing back Tflite model with multiple subgraphs that is currently loaded in the Tflite Interpreter.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.1
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
We can currently write model that have single subgraph using **tflite::SubgraphWriter** present in  **tensorflow/lite/experimental/writer/writer_lib.h** . For my use case I need to update some model weights without converting a new model. So I load model in interpreter, update its weights and write it back to the disk. 
Below is example of loading a model and writing it back. And I can use the rewritten model for future inference.

    //load model
    std::unique_ptr<tflite::FlatBufferModel> model = 
    tflite::FlatBufferModel::BuildFromFile(""model.tflite"");

    //build interpreter
    tflite::ops::builtin::BuiltinOpResolver resolver;
    std::unique_ptr<tflite::Interpreter> interpreter;
    tflite::InterpreterBuilder(*model,resolver)(&interpreter);

    interpreter->AllocateTensors();

    //write out subgraph
    std::cout<<""writing out single subgraph""<<std::endl;
    
    tflite::SubgraphWriter(interpreter->subgraph(0)).Write(""model.tflite"");

But problem occurs when the model has multiple subgraphs e.g when model contains while_loop. In that case I am unable to write it back completely as writer api is made to write a single subgraph. If I write back a model containing while_loop  same code as above and load the model again and allocate tensors I get following error.
```
   ERROR: tensorflow/lite/kernels/while.cc:136 op_data->cond_subgraph_index < subgraphs->size() was not true.
   ERROR: Node number 0 (WHILE) failed to prepare.
```

The script for model containing while is:

   ```
import tensorflow as tf 
   import tensorflow.compat.v1 as tfv1

   tfv1.disable_eager_execution()
   with tfv1.Session(graph = tf.Graph()) as sess:
      i = tfv1.placeholder(shape = (5,1),dtype = tf.float32)
      a = tfv1.placeholder(shape = (5,1),dtype = tf.float32)
      z = np.zeros(1)
      k = tf.constant(z,dtype = tf.int32,shape=(1,))
      c = lambda i,a,k: tf.less(k,10)
      b = lambda i,a,k:(tf.add(i,a),tf.add(a,a),(k+1),)
      r = tf.while_loop(c,b,[i,a,k])
      converter = tfv1.lite.TFLiteConverter.from_session(sess,[i,a],[r[0]])
      model = converter.convert()
      with open('model.tflite','wb') as model_file:
         model_file.write(model)
```

**Will this change the current api? How?**
In the writer_lib there would be an extra api that would take interpreter as input (or all interpreter subgraphs), and write back a tflite model that is represented by its subgraphs.
@abattery, @tensorflower-gardener could be please give any insights in this or any workaround to this problem.

**Who will benefit with this feature?**
Use cases that needs to update weights of the model can do that by just passing updated parameters instead of converting and downloading the whole model. 

**Any Other info.**
"
44605,Compiling tensorflow for 12 hours is ok?,"

**System information**
- Windows 10 
- I download tensorflow by GIT day before yesterday
- TensorFlow version: last from GIT
- Python version: 3.7.9
- Im use pip
- Bazel version 3.7.0
- VS2019
- CUDA/cuDNN version: 11.1/8
- GPU model and memory:GeForce 3090, 24GB
CPU i7 10700 RAM 32GB, ssd



I following instructions for installation from sourses from website tensorflow.org (for windows 10). Now pass 12 hours from start time building the tensorflow. During this time I have 100% usage the CPU, and using 95% memory from 32GB. Its normally? Dont sure...((
Thanx!
![compile](https://user-images.githubusercontent.com/29018860/98205473-47a4fa00-1f49-11eb-9527-560ab1e18c01.jpg)

"
44604,keras epsilon doesn't change ,"Windows, TF 2.3.1

I'm trying to change the epsilon so my MAPE doesn't blow up. When I do:

$tensorflow.keras.backend.set_epsilon = 1e-3
and then:
$tensorflow.keras.backend.epsilon()
I get:
>>>> 1e-07

"
44603,tf.estimator.BoostedTreesClassifier - AttributeError: 'NoneType' object has no attribute 'is_compatible_with',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),


**System information**

- Running my code on Google Colab notebook

Am using the tf nightly build as:

`!pip install tf-nightly`  

output of pip install :

Building wheel for tensorflow-docs (setup.py) ... done
Requirement already satisfied: tf-nightly in /usr/local/lib/python3.6/dist-packages (2.5.0.dev20201104)
Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.19.4)
Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)
Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.35.1)
Requirement already satisfied: tb-nightly~=2.4.0.a in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.4.0a20201104)
Requirement already satisfied: protobuf~=3.13.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.13.0)
Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12)
Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.10.0)
Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)
Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)
Requirement already satisfied: tf-estimator-nightly~=2.4.0.dev in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.4.0.dev2020102301)
Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.7.4.3)
Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.32.0)
Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)
Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)
Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)
Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)
Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.3.0)
Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)
Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.4.0.a->tf-nightly) (1.7.0)
Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.4.0.a->tf-nightly) (1.0.1)
Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.4.0.a->tf-nightly) (2.23.0)
Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.4.0.a->tf-nightly) (0.4.2)
Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.4.0.a->tf-nightly) (3.3.3)
Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.4.0.a->tf-nightly) (1.17.2)
Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.4.0.a->tf-nightly) (50.3.2)
Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.4.0.a->tf-nightly) (3.0.4)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.4.0.a->tf-nightly) (1.24.3)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.4.0.a->tf-nightly) (2.10)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.4.0.a->tf-nightly) (2020.6.20)
Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.4.0.a->tf-nightly) (1.3.0)
Requirement already satisfied: importlib-metadata; python_version < ""3.8"" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly~=2.4.0.a->tf-nightly) (2.0.0)
Requirement already satisfied: rsa<5,>=3.1.4; python_version >= ""3"" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.4.0.a->tf-nightly) (4.6)
Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.4.0.a->tf-nightly) (0.2.8)
Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.4.0.a->tf-nightly) (4.1.1)
Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.4.0.a->tf-nightly) (3.1.0)
Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < ""3.8""->markdown>=2.6.8->tb-nightly~=2.4.0.a->tf-nightly) (3.4.0)
Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= ""3""->google-auth<2,>=1.6.3->tb-nightly~=2.4.0.a->tf-nightly) (0.4.8)


**Describe the current behavior**

I am using the tf.estimator.BoostedTreeClassifier

```
est = tf.estimator.BoostedTreesClassifier(feature_columns, **params)
est.train(train_input_fn, max_steps=100)

```

I switched to the tf-nightly build to use the fix for this issue: https://github.com/tensorflow/tensorflow/issues/40063
getting for following error since Nov 4 with the tf nightly build:

`est.train(train_input_fn, max_steps=100)`

is failing as follows:

```
INFO:tensorflow:Using default config.
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp1dthkfom
INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp1dthkfom', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
INFO:tensorflow:Calling model_fn.
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-33-961616a6c72a> in <module>()
     14 
     15 # n_batches_per_layer=10, n_trees=n_trees, n_classes=9)
---> 16 est.train(train_input_fn, max_steps=100)
     17 
     18 print(est.evaluate(eval_input_fn))

13 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parallel_for/gradients.py in batch_jacobian(output, inp, use_pfor, parallel_iterations)
    111   """"""
    112   output_shape = output.shape
--> 113   if not output_shape[0].is_compatible_with(inp.shape[0]):
    114     raise ValueError(""Need first dimension of output shape (%s) and inp shape ""
    115                      ""(%s) to match."" % (output.shape, inp.shape))

AttributeError: 'NoneType' object has no attribute 'is_compatible_with'

```

I switched back to stable version of tf=2.3.0 , dont get the error above , but it fails on issue #40063
so it seems to be problem with the nightly build. I did not hit this error with earlier to Nov 3th nightly build

**Describe the expected behavior**

expect 

```
est = tf.estimator.BoostedTreesClassifier(feature_columns, **params)
est.train(train_input_fn, max_steps=100)
```
this to run 

I did not have this issues till the Nov 3 nightly build, 
One was to address it and move forward for now would be to use a pre Nov 3 tf nightly build. is there a way to specify the version of the nightly build to pip install. that would solve my problem for now.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
params = {
  'n_trees': 100,
  'max_depth': 10,
  'n_batches_per_layer': 1,
  'n_classes': 9,
}
est = tf.estimator.BoostedTreesClassifier(feature_columns, **params)
est.train(train_input_fn, max_steps=100)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44602,fatal error: tensorflow/core/framework/types.pb.h: No such file or directory,"**System information**
- OS Platform and Distribution : Linux Ubuntu 18.04.5 LTS
- TensorFlow installed from source
- TensorFlow version : r2.2
- Python version : py3.6
- Bazel version : 2.0.0
- GCC/Compiler version : 7.5.0

 
steps to produce error:

- sudo apt install python3-dev python3-pip
- pip install -U --user pip numpy wheel
- pip install -U --user keras_preprocessing --no-deps
- git clone https://github.com/tensorflow/tensorflow.git
- cd tensorflow
- git checkout r2.2
- ./configure

You have bazel 2.0.0 installed.
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3

Found possible Python library paths:
/usr/lib/python3/dist-packages
/usr/local/lib/python3.6/dist-packages
Please input the desired Python library path to use. Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]:

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
--config=mkl # Build with MKL support.
--config=monolithic # Config for mostly static monolithic build.
--config=ngraph # Build with Intel nGraph support.
--config=numa # Build with NUMA support.
--config=dynamic_kernels # (Experimental) Build kernels into separate shared objects.
--config=v2 # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
--config=noaws # Disable AWS S3 filesystem support.
--config=nogcp # Disable GCP support.
--config=nohdfs # Disable HDFS support.
--config=nonccl # Disable NVIDIA NCCL support.
Configuration finished

-> bazel build -c opt tensorflow:libtensorflow_cc.so
-> then i created CMakeLists.txt file in tensorflow/tensorflow/examples/speech_commands/ 
-> cd /tensorflow/examples/speech_commands/
-> mkdir build
-> cd build
-> cmake ..
-> make
Then i am facing this issue.
ERROR:
[ 50%] Building CXX object CMakeFiles/speech_recog.dir/label_wav.cc.o
In file included from /home/sbojja/Qual-sdm/TF/tensorflow/tensorflow/core/framework/tensor.h:23:0,
from /home/sbojja/Qual-sdm/TF/tensorflow/tensorflow/examples/speech_commands/label_wav.cc:19:
/home/sbojja/Qual-sdm/TF/tensorflow/tensorflow/core/framework/tensor_shape.h:22:10: fatal error: tensorflow/core/framework/types.pb.h: No such file or directory
#include ""tensorflow/core/framework/types.pb.h""
^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
CMakeFiles/speech_recog.dir/build.make:62: recipe for target 'CMakeFiles/speech_recog.dir/label_wav.cc.o' failed
make[2]: *** [CMakeFiles/speech_recog.dir/label_wav.cc.o] Error 1
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/speech_recog.dir/all' failed
make[1]: *** [CMakeFiles/speech_recog.dir/all] Error 2
Makefile:83: recipe for target 'all' failed
make: *** [all] Error 2


This is the cmake file
[CMakeLists.txt](https://github.com/tensorflow/tensorflow/files/5491688/CMakeLists.txt)
"
44601,Why is the memory(no gpu) used so different between windows and Ubuntu,"PS: memory is not GPU-memory, is PC-memory
There are 6 small models, inference with 6 multiprocessing in python3.7 and tensorflow1.13.1.  
In windows, use 4G memory, but in Ubuntu use 12G memory, don't know why.  
How to reduce memory in ubuntu?"
44600,cstdio: No such file or directory,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arduino Create - online IDE
- TensorFlow installed from (source or binary): ARDUINO_TENSORFLOWLITE default library
- Tensorflow version (commit SHA if source): 2.1.0 alpha-precompiled
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arduino Nano

**Describe the problem**
Using the Arduino Create online IDE, example hello_world from the above library, verification step produces the following error:
In file included from /home/builder/opt/libraries/latest/arduino_tensorflowlite_2_1_0_alpha_precompiled/src/tensorflow/lite/micro/all_ops_resolver.h:16:0,

from /tmp/347074429/hello_world/hello_world.ino:20:

/home/builder/opt/libraries/latest/arduino_tensorflowlite_2_1_0_alpha_precompiled/src/tensorflow/lite/micro/micro_mutable_op_resolver.h:18:10: fatal error: cstdio: No such file or directory

#include cstdio

^~~~~~~~

compilation terminated.

exit status 1"
44599,esp32,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Command used to run the converter or code if youre using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
# Copy and paste here the exact command
```

**The output from the converter invocation**

```
# Copy and paste the output here.
```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44598,AssertionError t.graph == cond_graph using tf.GradientTape,"Hi, 

I posted a [similar question](https://stackoverflow.com/questions/64661822/what-is-the-meaning-of-assertion-error-assert-t-graph-cond-grap-using-tf-gra) on SO, but I am not sure anymore if the problem is on my side or somewhere in TF (hence no MRE).  Similar problem [here ](https://stackoverflow.com/questions/62911595/assertionerror-assert-t-graph-cond-graph-when-generating-heatmap-for-locally). I have been succesfully using `keras.backend.gradients` as for now but I have recently made a switch to `tensorflow.keras`  and since `tf.gradient` is not allowed in eager execution mode, I had to use `tf.GradientTape()`. 

So basically I changed all calls from this:

```python
grads = keras.backend.gradients(loss=target_tensor, variables=[var1, var2, var3])
```
to:
```python
with tf.GradientTape(persistent=True) as tape:
   # any calulations involving target_tensor, and var1, var2, ...
grads = tape.gradient(target=target_tensor, sources=[var1, var2, var3])
``` 

and I am getting this on any call (even if target and source is the same):

```console
c:\users\tomas\miniconda3\envs\tftest\lib\site-packages\tensorflow\python\ops\cond_v2.py in _resolve_grad_inputs(cond_graph, grad_graph)
    411     # `cond_graph`.
    412     if t.graph != grad_graph.outer_graph:
--> 413       assert t.graph == cond_graph
    414       # `internal_captures` are not treated as intermediates and hence not added
    415       # to If op outputs. So we get the outer tensor corresponding to those

AssertionError: 
```

I tried to dig into the TF code but I cannot really tell what is it that I am doing wrong.  There are no problems with toy-examples from docs but I cannot calculate any gradient in my graph. What does this assertion check exactly? How I can resolve this? 

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- TensorFlow installed from (source or binary): pip 
- TensorFlow version (use command below): 2.3.1, but also 2.2.0
- Python version: 3.8
"
44597,TensorFlow serving on GPUs using Docker 19.03 needs gpus flag,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://www.tensorflow.org/tfx/serving/building_with_docker#running_a_container


## Description of issue (what needs changing):
Since Docker 19.03, you need to install nvidia-container-toolkit package and then use the --gpus all flag. The docker command to run the container with GPUs needs --gpus=all parameter so that the GPU devices are visible within the container. The command should instead be (if we want to assign all available GPUs to docker)

sudo docker run -it -p 8500:8500 --gpus all tensorflow/serving:latest-devel-gpu

If you want to assign specific device then change it to --gpus device=0

### Clear description

If we don't make this change then users will run into a (cryptic) error, and won't really know how to handle it. 

W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-11-02 21:06:24.739827: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
44596,tensorflow.python.framework.errors_impl.UnknownError: Access is denied.,"Hi again!

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary):  binary i think
- TensorFlow version: 2.5.0-dev20201028
- Python version: Python 3.8.3
- Installed using virtualenv? pip? conda?: pip

**Describe the problem**
I can't train the model to recognize custom object.

```
So, my structure folder is like these:
-SistemiDigitali
    - ImageCollector
        - images
             - test
                    -images with xml file
             - train
                    -images with xml file
             - test.record
             - test_labels.csv
             - train.record
             - train_labels.csv
    - model (the folder where tensorflow model is located)
    - models-master (folder from git cloning the models repo)

```

So i have changed the pipeline inside model like this:
```

model {
  ssd {
    num_classes: 1
    image_resizer {
      fixed_shape_resizer {
        height: 320
        width: 320
      }
    }
    feature_extractor {
      type: ""ssd_mobilenet_v2_fpn_keras""
      depth_multiplier: 1.0
      min_depth: 16
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 3.9999998989515007e-05
          }
        }
        initializer {
          random_normal_initializer {
            mean: 0.0
            stddev: 0.009999999776482582
          }
        }
        activation: RELU_6
        batch_norm {
          decay: 0.996999979019165
          scale: true
          epsilon: 0.0010000000474974513
        }
      }
      use_depthwise: true
      override_base_feature_extractor_hyperparams: true
      fpn {
        min_level: 3
        max_level: 7
        additional_layer_depth: 128
      }
    }
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
        use_matmul_gather: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    box_predictor {
      weight_shared_convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 3.9999998989515007e-05
            }
          }
          initializer {
            random_normal_initializer {
              mean: 0.0
              stddev: 0.009999999776482582
            }
          }
          activation: RELU_6
          batch_norm {
            decay: 0.996999979019165
            scale: true
            epsilon: 0.0010000000474974513
          }
        }
        depth: 128
        num_layers_before_predictor: 4
        kernel_size: 3
        class_prediction_bias_init: -4.599999904632568
        share_prediction_tower: true
        use_depthwise: true
      }
    }
    anchor_generator {
      multiscale_anchor_generator {
        min_level: 3
        max_level: 7
        anchor_scale: 4.0
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        scales_per_octave: 2
      }
    }
    post_processing {
      batch_non_max_suppression {
        score_threshold: 9.99999993922529e-09
        iou_threshold: 0.6000000238418579
        max_detections_per_class: 100
        max_total_detections: 100
        use_static_shapes: false
      }
      score_converter: SIGMOID
    }
    normalize_loss_by_num_matches: true
    loss {
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_loss {
        weighted_sigmoid_focal {
          gamma: 2.0
          alpha: 0.25
        }
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    encode_background_as_zeros: true
    normalize_loc_loss_by_codesize: true
    inplace_batchnorm_update: true
    freeze_batchnorm: false
  }
}
train_config {
  batch_size: 128
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    random_crop_image {
      min_object_covered: 0.0
      min_aspect_ratio: 0.75
      max_aspect_ratio: 3.0
      min_area: 0.75
      max_area: 1.0
      overlap_thresh: 0.0
    }
  }
  sync_replicas: true
  optimizer {
    momentum_optimizer {
      learning_rate {
        cosine_decay_learning_rate {
          learning_rate_base: 0.07999999821186066
          total_steps: 50000
          warmup_learning_rate: 0.026666000485420227
          warmup_steps: 1000
        }
      }
      momentum_optimizer_value: 0.8999999761581421
    }
    use_moving_average: false
  }
  fine_tune_checkpoint: ""D:/Davide/Uni/SistemiDigitali/model/checkpoint/ckpt-0""
  num_steps: 50000
  startup_delay_steps: 0.0
  replicas_to_aggregate: 8
  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
  fine_tune_checkpoint_type: ""detection""
  fine_tune_checkpoint_version: V2
}
train_input_reader {
  label_map_path: ""D:/Davide/Uni/SistemiDigitali/ImageCollector/label_map.pbtxt""
  tf_record_input_reader {
    input_path: ""D:/Davide/Uni/SistemiDigitali/ImageCollector/images/train.record""
  }
}
eval_config {
  metrics_set: ""coco_detection_metrics""
  use_moving_averages: false
}
eval_input_reader {
  label_map_path: ""D:/Davide/Uni/SistemiDigitali/ImageCollector/label_map.pbtxt""
  shuffle: false
  num_epochs: 1
  tf_record_input_reader {
    input_path: ""D:/Davide/Uni/SistemiDigitali/ImageCollector/images/test.record""
  }
}
```


But when i run this command:

`python models-master/research/object_detection/model_main_tf2.py --pipeline_config_path=model/pipeline.config --model_dir=model --alsologtostderr`

I get this exception

```
WARNING:tensorflow:model\checkpoint: Checkpoint ignored
W1104 21:34:18.945756  6844 checkpoint_management.py:296] model\checkpoint: Checkpoint ignored
Traceback (most recent call last):
  File ""models-master/research/object_detection/model_main_tf2.py"", line 113, in <module>
    tf.compat.v1.app.run()
  File ""C:\Users\Davide\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\Users\Davide\AppData\Local\Programs\Python\Python38\lib\site-packages\absl\app.py"", line 300, in run
    _run_main(main, args)
  File ""C:\Users\Davide\AppData\Local\Programs\Python\Python38\lib\site-packages\absl\app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""models-master/research/object_detection/model_main_tf2.py"", line 104, in main
    model_lib_v2.train_loop(
  File ""C:\Users\Davide\AppData\Local\Programs\Python\Python38\lib\site-packages\object_detection-0.1-py3.8.egg\object_detection\model_lib_v2.py"", line 630, in train_loop
    manager.save()
  File ""C:\Users\Davide\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\training\checkpoint_management.py"", line 824, in save
    self._record_state()
  File ""C:\Users\Davide\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\training\checkpoint_management.py"", line 727, in _record_state
    update_checkpoint_state_internal(
  File ""C:\Users\Davide\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\training\checkpoint_management.py"", line 247, in update_checkpoint_state_internal
    file_io.atomic_write_string_to_file(coord_checkpoint_filename,
  File ""C:\Users\Davide\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 573, in atomic_write_string_to_file
    rename(temp_pathname, filename, overwrite)
  File ""C:\Users\Davide\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 532, in rename
    rename_v2(oldname, newname, overwrite)
  File ""C:\Users\Davide\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 548, in rename_v2
    _pywrap_file_io.RenameFile(
tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: model\checkpoint.tmp2ddac8459ee545f28c333f066a5292cc to: model\checkpoint : Access is denied.
; Input/output error
```"
44595,How to implement Bidirectional GRU in Encoder,"I am currently trying to implement a bidirectional GRU within an Encoder class:

```
def gru_cell(enc_units):
  return tf.keras.layers.GRUCell(enc_units, recurrent_initializer='glorot_uniform')

class Encoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
    super(Encoder, self).__init__()
    self.batch_sz = batch_sz
    self.enc_units = enc_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.bid_gru = tf.keras.layers.Bidirectional(gru_cell(self.enc_units))

   

  def call(self, x, hidden):
    x = self.embedding(x)
    concatenated, forward, backward = self.bid_gru(x, initial_state=[hidden, hidden])
    return concatenated, forward, backward

  def initialize_hidden_state(self):
    return tf.zeros((self.batch_sz, self.enc_units))
```
But I am getting the following Key Error: 'go_backwards'
![image](https://user-images.githubusercontent.com/43485111/98159495-ad788e00-1f17-11eb-94e9-3e11c792e065.png)
How can I resolve this?"
44592,Make timeseries_dataset_from_array() more intuitive,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
``timeseries_dataset_from_array()`` was introduced to replace ``tf.keras.preprocessing.sequence.TimeseriesGenerator``. But it is not a drop in replacement. In fact, as I will show below, it takes a bit of work to make ``timeseries_dataset_from_array()`` behave like ``TimeseriesGenerator``. Issue #40343 was created regarding this. But the resolution and suggested work around are problematic. This is why I am opening this issue.

Let's explain the issues with an example.

```python
features = np.arange(1, 10)
labels = features * -1

print(""Features:"", features)
print(""Labels:"", labels)
```

This prints:

```
Features: [1 2 3 4 5 6 7 8 9]
Labels: [-1 -2 -3 -4 -5 -6 -7 -8 -9]
```

We can use ``TimeseriesGenerator`` as follows.

```python
sequence_length=3

gen = tf.keras.preprocessing.sequence.TimeseriesGenerator(features, labels, sequence_length, batch_size=1)

for inputs, targets in gen:
  print(""Input:"", inputs, ""Target:"", targets)
```

This correctly prints:

```
Input: [[1 2 3]] Target: [-4]
Input: [[2 3 4]] Target: [-5]
Input: [[3 4 5]] Target: [-6]
Input: [[4 5 6]] Target: [-7]
Input: [[5 6 7]] Target: [-8]
Input: [[6 7 8]] Target: [-9]
```

Now, let's use ``timeseries_dataset_from_array()`` as a drop in replacement.

```python
ds = tf.keras.preprocessing.timeseries_dataset_from_array(features, labels, sequence_length, batch_size=1)

for inputs, targets in ds:
  print(""Input:"", inputs.numpy(), ""Target:"", targets.numpy())
```

This prints:

```
Input: [[1 2 3]] Target: [-1]
Input: [[2 3 4]] Target: [-2]
Input: [[3 4 5]] Target: [-3]
Input: [[4 5 6]] Target: [-4]
Input: [[5 6 7]] Target: [-5]
Input: [[6 7 8]] Target: [-6]
Input: [[7 8 9]] Target: [-7]
```

This is not expected. To be honest, I am not quite sure in what scenarios one would want the above behavior.

The [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array) shows a way around this. We can follow the ""Example 2"" in the documentation and re-write our test like this:

```python
ds = tf.keras.preprocessing.timeseries_dataset_from_array(features[:-sequence_length], labels[sequence_length:], sequence_length, batch_size=1)

for inputs, targets in ds:
  print(""Input:"", inputs.numpy(), ""Target:"", targets.numpy())
```

This prints:

```
Input: [[1 2 3]] Target: [-4]
Input: [[2 3 4]] Target: [-5]
Input: [[3 4 5]] Target: [-6]
Input: [[4 5 6]] Target: [-7]
```

This is good. But we are now missing the last two sequences:

```
Input: [[5 6 7]] Target: [-8]
Input: [[6 7 8]] Target: [-9]
```

Issue #40343 suggests the use of ``np.roll()`` as a work around. The resolution comment says that ``np.roll()`` was added to the documentation. But I fail to see it there. Also the way ``np.roll()`` is used in that issue seems problematic. Below we re-write our test using the way I think ``np.roll()`` should be used.

```python
ds = tf.keras.preprocessing.timeseries_dataset_from_array(
  features[:-1], np.roll(labels, -sequence_length)[:-1], sequence_length, batch_size=1)

for inputs, targets in ds:
  print(""Input:"", inputs.numpy(), ""Target:"", targets.numpy())
```

This prints:

```
Input: [[1 2 3]] Target: [-4]
Input: [[2 3 4]] Target: [-5]
Input: [[3 4 5]] Target: [-6]
Input: [[4 5 6]] Target: [-7]
Input: [[5 6 7]] Target: [-8]
Input: [[6 7 8]] Target: [-9]
```

This is now functionally equivalent to ``TimeseriesGenerator``.

**Will this change the current api? How?**
The issue can be addressed either by enhancing the documentation or by enhancing the API. I will prefer the later. Perhaps a flag can be supplied to ``timeseries_dataset_from_array()`` that will make it behave like ``TimeseriesGenerator``.

**Who will benefit with this feature?**
Currently ``timeseries_dataset_from_array()`` requires a bit of work to make it work for what I will consider as most common time series problems. This makes the API unintuitive. At minimum the documentation needs to be enhanced. It will be better if the API can be changed so one can easily use it in most common scenarios.

**Any Other info.**
"
44591,BUG!! tf.keras.model.summary() output is wrong,"**System information**

- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow version: 2.3.0
- Python version: 3.8.5
- GPU model and memory: Nvidia GeForce RTX 2080 Ti

**Describe the current behavior**
```
class DenseNormLayer(tf.keras.layers.Layer):
    def __init__(self, units, activation, name=None):
        super(DenseNormLayer, self).__init__(name=name)
        self.dense_layer = tf.keras.layers.Dense(units=units, activation=activation)
        self.batch_norm = tf.keras.layers.BatchNormalization()
    
    def call(self, x):
        x = self.batch_norm(x)
        x = self.dense_layer(x)
        return x
```

```
class BaselineModel(tf.keras.Model):
    def __init__(self, targets, name=""BaselineModel""):
        super(BaselineModel, self).__init__(name=name)
        self.block1 = DenseNormLayer(units=1024, activation=""relu"", name=""block1"")
        self.block2 = DenseNormLayer(units=1024, activation=""relu"", name=""block2"")
        self.d1 = tf.keras.layers.Dense(units=512, activation=""relu"", name=""d1"")
        self.d2 = tf.keras.layers.Dense(units=1024, activation=""relu"", name=""d2"")
        self.dp = tf.keras.layers.Dropout(rate=0.2, name=""dropout_layer"")
        self.sigmoid_layer = tf.keras.layers.Dense(units=targets, activation=""sigmoid"", name=""sigmoid_layer"")
    
    def call(self, X):
        x = self.block1(X)
        x = self.block2(x)
        x = self.d1(x)
        x = self.dp(x)
        x = self.d2(x)
        x = self.sigmoid_layer(x)
        return x
    
    def build_graph(self, dim):
        x = tf.keras.Input(shape=dim)
        return tf.keras.Model(inputs=[x], outputs=self.call(x))
```

Output summary:
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
block1 (DenseNormLayer)      multiple                  103824    
_________________________________________________________________
block2 (DenseNormLayer)      multiple                  1053696   
_________________________________________________________________
d1 (Dense)                   multiple                  524800    
_________________________________________________________________
d2 (Dense)                   multiple                  525312    
_________________________________________________________________
dropout_layer (Dropout)      multiple                  0         
_________________________________________________________________
sigmoid_layer (Dense)        multiple                  10250     
=================================================================
Total params: 2,217,882
Trainable params: 2,215,634
Non-trainable params: 2,248
_________________________________________________________________
```

Run this to get summary:
```
test_model = BaselineModel(targets=100)
test_model.build(input_shape=(None, 100))
test_model.summary()
```
As one can see in above summary, the `dropout_layer` is present below `d2` layer which is incorrect. It should be in between of `d1` and `d2` layers.
I tried cleaning python cache and also tried in Jupyter Notebook but no luck.

**Describe the expected behavior**

When plotting with the code below, output is correct.
```
import os
tf.keras.utils.plot_model(
    test_model.build_graph(dim=100), to_file=os.path.join(""."", ""model.png""),
    dpi=96, show_shapes=True, show_layer_names=True, expand_nested=False
)
````
![image](https://user-images.githubusercontent.com/23133817/98150766-303c2180-1ef5-11eb-8c73-2d5cdbf38ebf.png)

"
44590,Layer.add_loss has wrong behaviour when building Sequential model in for loop,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1     2.3.1
- Python version: 3.6.5
- CUDA/cuDNN version: Not using CUDA
- GPU model and memory: Not using GPU



This code:
```
    import tensorflow as tf

    tf.random.set_seed(0)
    model = tf.keras.Sequential()
    inputs = tf.keras.Input(shape=(1,))
    model.add(inputs)
    layer1 = tf.keras.layers.Dense(1)
    layer1.add_loss(lambda :tf.reduce_sum(layer1.kernel))
    model.add(layer1)
    layer2 = tf.keras.layers.Dense(1)
    layer2.add_loss(lambda :tf.reduce_sum(layer2.kernel))
    model.add(layer2)

    print(model.get_weights())
    print(model.losses)
```

Correctly produces:
   [array([[-0.7206192]], dtype=float32), array([0.], dtype=float32), array([[0.19195998]], dtype=float32), array([0.], 
   dtype=float32)]
   [<tf.Tensor: shape=(), dtype=float32, numpy=-0.7206192>, <tf.Tensor: shape=(), dtype=float32, numpy=0.19195998>]

I.e. the first kernel weight is the first loss and the second kernel weight is the second loss.


I would expect the following code to produce the same result:

```
    tf.random.set_seed(0)
    model = tf.keras.Sequential()
    inputs = tf.keras.Input(shape=(1,))
    model.add(inputs)
    for i in range(2):
        layer = tf.keras.layers.Dense(1)
        layer.add_loss(lambda :tf.reduce_sum(layer.kernel))
        model.add(layer)

    print(model.get_weights())
    print(model.losses)
```

But it produces:
    [array([[-0.7206192]], dtype=float32), array([0.], dtype=float32), array([[0.19195998]], dtype=float32), array([0.], 
    dtype=float32)]
    [<tf.Tensor: shape=(), dtype=float32, numpy=0.19195998>, <tf.Tensor: shape=(), dtype=float32, numpy=0.19195998>]
I.e. it takes the second kernel weight for both losses.


"
44587,Behaviour of ImageDataGenerator with subset='validation',"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator
https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow

## Description of issue:

The documentation for `keras.preprocessing.image.ImageDataGenerator` doesn't specify how it handles validation data.

### Clear description

In particular, documentation for the `.flow()` method doesn't specify whether transformations are applied to augment validation data, when `validation_split` has been given in the constructor and `subset='validation'` is passed.

### Usage example

There are no usage examples that use the parameter `validation_split` with a previously undivided dataset. All examples employ a dataset that is already divided in training/validation and then create two `ImageDataGenerator` objects with different parameters. 

### Other

I have read the [source code](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/preprocessing/image.py#L808) in order to understand this behaviour. However, I end up in the definition of [NumpyArrayIterator](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/preprocessing/image.py#L400), a class that apparently inherits from itself, and I haven't been able to find its actual implementation or the meaning of this strange inheritance."
44586,ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.,"
![image](https://user-images.githubusercontent.com/19899607/98122360-7b930780-1ed6-11eb-8def-ae981ef57dcb.png)

Getting this error while importing TensorFlow.

It installed successfully.

CPU : Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz
OS : Windows server 2016
python: 3.7.6
TensorwFlow : 2.3.0 and 2.2.0 both gives same error

Checked parameters : 
-Installed Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019
-msvcp140_1.dll present
-AVX is enabled

TensorFlow 2.0.0 already working on this system.
This server doesn't have internet access so to upgrade Tensorflow I have tried 2 ways.
-Create conda environment pack in local machine and copy and run environment.
-Download .whl files using pip in local machine and installed it on a remote server

In both these ways, I am able to install tensorflow successfully but while importing it gives this error.
So, Please suggest how can I upgrade TensorFlow 2.0.0 to 2.3.0

 
"
44585,next() not raising StopIteration in tf.python.keras.preprocessing.image.DirectoryIterator,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code: no
- OS Platform and Distribution: Linux Ubuntu 18.04.1
- Mobile device: n/a
- TensorFlow installed from: binary
- TensorFlow version: v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.8.6
- Bazel version: n/a
- GCC/Compiler version: n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
I would like to iterate over image files in a directory using `flow_from_directory()` but calling `next()` does not raise `StopIteration` at the end of the data. Instead, it produces an infinite loop. Is this expected the behavior?

This relates to issues [45](https://github.com/keras-team/keras-preprocessing/issues/45), [226](https://github.com/keras-team/keras-preprocessing/issues/226), and [269](https://github.com/keras-team/keras-preprocessing/issues/269) in the keras preprocessing repo. I understand development has moved to tensorflow, hence posting it here.


**Describe the expected behavior**
I expected `StopIteration` to be raised.

**Standalone code to reproduce the issue**
The code below was adapted from [here](https://github.com/keras-team/keras-preprocessing/issues/226#issuecomment-516648880).

```
from tensorflow.keras.preprocessing.image import ImageDataGenerator, save_img
import numpy as np
import os

batch_size = 3
n_images = 5

directory = '/tmp/test'
classes = ['dog', 'cat']

for i in range(n_images):
    label = np.random.choice(classes)
    os.makedirs(os.path.join(directory, label), exist_ok=True)
    filename = os.path.join(directory, label, str(i) + '.jpg')
    save_img(filename, np.ones((64, 64, 3)))

gen = ImageDataGenerator().flow_from_directory(
    '/tmp/test',
    class_mode=None,
    batch_size=batch_size
)

for _ in range(gen.__len__()):
    imgs = next(gen)
    print(imgs.shape)

gen.reset()
while True:
    imgs = next(gen)
    print(imgs.shape)
```

**Other info / logs**
n/a"
44583,Could not load dynamic library 'libcudart.so.11.0',"Anybody can help me to solve this problem:
![image](https://user-images.githubusercontent.com/37329790/98122003-fcf49500-1eea-11eb-8809-c068104943a5.png)


Infos:
    
1. tf_nightly_gpu-2.4.0.dev20201023
2.     cuda version:  11.1
3.     cudnn version :  8.0.4

these question cause:  can't not use GPU to train my model, Thank's all of you!"
44582,flex delegate cross-compilation failed for riscv because of boringssl,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: commit 218caef97666384211ea9171dc6029ec3ec7bdd6
- Python version: 3.8
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): using riscv-unknown-linux-gnu-gcc 9.2.0
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the problem**
I want to cross-compile tflite with flex delegate for riscv ISA
While I build tensorflow/lite/delegates/flex:delegate I got error from BoringSSL like below
```
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=179
INFO: Reading rc options for 'build' from /home/jhjang/ml-accelerator/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/jhjang/ml-accelerator/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /home/jhjang/ml-accelerator/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/jhjang/.virtualenvs/ml/bin/python3 --action_env PYTHON_LIB_PATH=/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages --python_path=/home/jhjang/.virtualenvs/ml/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:monolithic in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define framework_shared_object=false
INFO: Found applicable config definition build:linux in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Build options --compilation_mode, --cpu, --crosstool_top, and 3 more have changed, discarding analysis cache.
INFO: Analyzed target //tensorflow/lite/tools:list_flex_ops_no_kernel_main (0 packages loaded, 746 targets configured).
INFO: Found 1 target...
Target //tensorflow/lite/tools:list_flex_ops_no_kernel_main up-to-date:
  bazel-bin/tensorflow/lite/tools/list_flex_ops_no_kernel_main
INFO: Elapsed time: 0.364s, Critical Path: 0.00s
INFO: 0 processes.
INFO: Build completed successfully, 1 total action
~/ml-accelerator/tensorflow/tmp ~/ml-accelerator/tensorflow
WARNING: The following configs were expanded more than once: [noaws]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=179
INFO: Reading rc options for 'build' from /home/jhjang/ml-accelerator/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/jhjang/ml-accelerator/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /home/jhjang/ml-accelerator/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/jhjang/.virtualenvs/ml/bin/python3 --action_env PYTHON_LIB_PATH=/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages --python_path=/home/jhjang/.virtualenvs/ml/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:noaws in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=no_aws_support=true
INFO: Found applicable config definition build:noaws in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=no_aws_support=true
INFO: Found applicable config definition build:nohdfs in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=no_hdfs_support=true
INFO: Found applicable config definition build:riscv64 in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --crosstool_top=@local_config_riscv64//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=riscv64
INFO: Found applicable config definition build:linux in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Build options --compilation_mode, --cpu, --crosstool_top, and 3 more have changed, discarding analysis cache.
INFO: Analyzed target //tmp:tensorflow-lite-select-tf-ops (1 packages loaded, 16446 targets configured).
INFO: Found 1 target...
ERROR: /home/jhjang/.cache/bazel/_bazel_jhjang/8a72e618241ede75353c494f725f5447/external/boringssl/BUILD:130:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1)
In file included from external/boringssl/src/include/openssl/asn1.h:61,
                 from external/boringssl/src/crypto/x509/a_sign.c:57:
external/boringssl/src/include/openssl/base.h:122:2: error: #error ""Unknown target CPU""
  122 | #error ""Unknown target CPU""
      |  ^~~~~
In file included from external/boringssl/src/include/openssl/asn1.h:68,
                 from external/boringssl/src/crypto/x509/a_sign.c:57:
external/boringssl/src/include/openssl/bn.h:165:2: error: #error ""Must define either OPENSSL_32_BIT or OPENSSL_64_BIT""
  165 | #error ""Must define either OPENSSL_32_BIT or OPENSSL_64_BIT""
      |  ^~~~~
external/boringssl/src/include/openssl/bn.h:219:44: error: unknown type name 'BN_ULONG'
  219 | OPENSSL_EXPORT int BN_set_word(BIGNUM *bn, BN_ULONG value);
      |                                            ^~~~~~~~
external/boringssl/src/include/openssl/bn.h:309:16: error: unknown type name 'BN_ULONG'
  309 | OPENSSL_EXPORT BN_ULONG BN_get_word(const BIGNUM *bn);
      |                ^~~~~~~~
external/boringssl/src/include/openssl/bn.h:377:43: error: unknown type name 'BN_ULONG'
  377 | OPENSSL_EXPORT int BN_add_word(BIGNUM *a, BN_ULONG w);
      |                                           ^~~~~~~~
external/boringssl/src/include/openssl/bn.h:390:43: error: unknown type name 'BN_ULONG'
  390 | OPENSSL_EXPORT int BN_sub_word(BIGNUM *a, BN_ULONG w);
      |                                           ^~~~~~~~
external/boringssl/src/include/openssl/bn.h:399:44: error: unknown type name 'BN_ULONG'
  399 | OPENSSL_EXPORT int BN_mul_word(BIGNUM *bn, BN_ULONG w);
      |                                            ^~~~~~~~
external/boringssl/src/include/openssl/bn.h:417:16: error: unknown type name 'BN_ULONG'
  417 | OPENSSL_EXPORT BN_ULONG BN_div_word(BIGNUM *numerator, BN_ULONG divisor);
      |                ^~~~~~~~
external/boringssl/src/include/openssl/bn.h:417:56: error: unknown type name 'BN_ULONG'
  417 | OPENSSL_EXPORT BN_ULONG BN_div_word(BIGNUM *numerator, BN_ULONG divisor);
      |                                                        ^~~~~~~~
external/boringssl/src/include/openssl/bn.h:434:49: error: unknown type name 'BN_ULONG'
  434 | OPENSSL_EXPORT int BN_cmp_word(const BIGNUM *a, BN_ULONG b);
      |                                                 ^~~~~~~~
external/boringssl/src/include/openssl/bn.h:448:53: error: unknown type name 'BN_ULONG'
  448 | OPENSSL_EXPORT int BN_abs_is_word(const BIGNUM *bn, BN_ULONG w);
      |                                                     ^~~~~~~~
external/boringssl/src/include/openssl/bn.h:457:49: error: unknown type name 'BN_ULONG'
  457 | OPENSSL_EXPORT int BN_is_word(const BIGNUM *bn, BN_ULONG w);
      |                                                 ^~~~~~~~
external/boringssl/src/include/openssl/bn.h:516:16: error: unknown type name 'BN_ULONG'
  516 | OPENSSL_EXPORT BN_ULONG BN_mod_word(const BIGNUM *a, BN_ULONG w);
      |                ^~~~~~~~
external/boringssl/src/include/openssl/bn.h:516:54: error: unknown type name 'BN_ULONG'
  516 | OPENSSL_EXPORT BN_ULONG BN_mod_word(const BIGNUM *a, BN_ULONG w);
      |                                                      ^~~~~~~~
external/boringssl/src/include/openssl/bn.h:629:48: error: unknown type name 'BN_ULONG'
  629 | OPENSSL_EXPORT int BN_rand_range_ex(BIGNUM *r, BN_ULONG min_inclusive,
      |                                                ^~~~~~~~
external/boringssl/src/include/openssl/bn.h:921:52: error: unknown type name 'BN_ULONG'
  921 | OPENSSL_EXPORT int BN_mod_exp_mont_word(BIGNUM *r, BN_ULONG a, const BIGNUM *p,
      |                                                    ^~~~~~~~
external/boringssl/src/include/openssl/bn.h:960:3: error: unknown type name 'BN_ULONG'
  960 |   BN_ULONG *d;
      |   ^~~~~~~~
external/boringssl/src/include/openssl/bn.h:991:3: error: unknown type name 'BN_ULONG'
  991 |   BN_ULONG n0[2];  // least significant words of (R*Ri-1)/N
      |   ^~~~~~~~
external/boringssl/src/include/openssl/bn.h:994:42: error: unknown type name 'BN_ULONG'
  994 | OPENSSL_EXPORT unsigned BN_num_bits_word(BN_ULONG l);
      |                                          ^~~~~~~~
Target //tmp:tensorflow-lite-select-tf-ops failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1.357s, Critical Path: 0.31s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
```

*Can I somehow exclude boringssl from compilation?*

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I am modifying `tensorflow/lite/tools/build_aar.sh for riscv
The error is from `execute generate_flex_aar`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44579,Floating-point models,"Can tensorflow Lite for Microcontrollers run fp models or just quantized ones? if so, are there examples to run fp models?"
44578,tf.data.dataset.batch returns different shape depending on drop_remainder parameter,"Hi,
I notice that when drop_reminder is not set in tf.data.dataset.batch , the dataset returned as None shape, but if drop_reminder is set, the batch_size is set as the shape of the returned dataset:

**With drop_remainder=False:**

`train_ds = train_ds.batch(32) 

print(train_ds) 

_<BatchDataset shapes: ({age: (None,), sex: (None,), cp: (None,), trestbps: (None,), chol: (None,), fbs: (None,), restecg: (None,), thalach: (None,), exang: (None,), oldpeak: (None,), slope: (None,), ca: (None,), thal: (None,)}, (None,)), types: ({age: tf.int64, sex: tf.int64, cp: tf.int64, trestbps: tf.int64, chol: tf.int64, fbs: tf.int64, restecg: tf.int64, thalach: tf.int64, exang: tf.int64, oldpeak: tf.float64, slope: tf.int64, ca: tf.int64, thal: tf.string}, tf.int64)>_`

**With drop_remainder=True,** 

`train_ds = train_ds.batch(32, drop_remainder=True)

print(train_ds)

<BatchDataset shapes: ({age: (32,), sex: (32,), cp: (32,), trestbps: (32,), chol: (32,), fbs: (32,), restecg: (32,), thalach: (32,), exang: (32,), oldpeak: (32,), slope: (32,), ca: (32,), thal: (32,)}, (32,)), types: ({age: tf.int64, sex: tf.int64, cp: tf.int64, trestbps: tf.int64, chol: tf.int64, fbs: tf.int64, restecg: tf.int64, thalach: tf.int64, exang: tf.int64, oldpeak: tf.float64, slope: tf.int64, ca: tf.int64, thal: tf.string}, tf.int64)>`

Is this the expected behaviour?
"
44577,custom ops coredump when tf update from 2.2.0 to 2.3.0,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.8.2003 (Core)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0, 2.3.0
- Python version: Python 3.8.3
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I am updating my project from tf-2.2.0 to tf-2.3.0, but it coredump while register_op. 

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```c++
// core/ops/balance_dataset_ops.cc
#include ""tensorflow/core/framework/common_shape_fns.h""
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""

using namespace tensorflow;

REGISTER_OP(""BalanceDataset"")
    .Input(""input_dataset: variant"")
    .Output(""handle: variant"")
    .Attr(""output_types: list(type) >= 1"")
    .Attr(""output_shapes: list(shape) >= 1"")
    .SetDoNotOptimize()
    .SetShapeFn(shape_inference::ScalarShape)
    .Doc(R""doc(balance input data between datasets
    )doc"");
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
#0  0x00007f238f5a6387 in raise () from /lib64/libc.so.6
#1  0x00007f238f5a7a78 in abort () from /lib64/libc.so.6
#2  0x00007f238f5e8ed7 in __libc_message () from /lib64/libc.so.6
#3  0x00007f238f5f1299 in _int_free () from /lib64/libc.so.6
#4  0x00007f235bc3e031 in google::protobuf::internal::ArenaStringPtr::DestroyNoArena (this=0x7ffe0d537d48, 
    default_value=0x7f235d82c2c0 <google::protobuf::internal::fixed_address_empty_string[abi:cxx11]>)
    at external/com_google_protobuf/src/google/protobuf/arenastring.h:325
#5  0x00007f235c2dc44e in tensorflow::OpDef::SharedDtor (this=0x7ffe0d537cd0)
    at bazel-out/k8-dbg/bin/external/org_tensorflow/tensorflow/core/framework/op_def.pb.cc:1629
#6  0x00007f235c2d5780 in tensorflow::OpDef::~OpDef (this=0x7ffe0d537cd0, __in_chrg=<optimized out>)
    at bazel-out/k8-dbg/bin/external/org_tensorflow/tensorflow/core/framework/op_def.pb.cc:1623
#7  0x00007f235bbfc19a in tensorflow::OpRegistrationData::~OpRegistrationData (this=0x7ffe0d537cd0, __in_chrg=<optimized out>)
    at external/org_tensorflow/tensorflow/core/framework/op_def_builder.h:39
#8  0x00007f235bbfc222 in tensorflow::OpDefBuilder::~OpDefBuilder (this=0x7ffe0d537cd0, __in_chrg=<optimized out>)
    at external/org_tensorflow/tensorflow/core/framework/op_def_builder.h:53
#9  0x00007f235bbfc5fc in tensorflow::register_op::OpDefBuilderWrapper<true>::~OpDefBuilderWrapper (this=0x7ffe0d537cd0, 
    __in_chrg=<optimized out>) at external/org_tensorflow/tensorflow/core/framework/op.h:225
#10 0x00007f235bbfbdbf in __static_initialization_and_destruction_0 (__initialize_p=1, __priority=65535) at core/ops/balance_dataset_ops.cc:23
#11 0x00007f235bbfbf76 in _GLOBAL__sub_I_balance_dataset_ops.cc(void) () at core/ops/balance_dataset_ops.cc:28
#12 0x00007f238fb699b3 in _dl_init_internal () from /lib64/ld-linux-x86-64.so.2
#13 0x00007f238fb6e58e in dl_open_worker () from /lib64/ld-linux-x86-64.so.2
#14 0x00007f238fb697c4 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2
#15 0x00007f238fb6db7b in _dl_open () from /lib64/ld-linux-x86-64.so.2
#16 0x00007f238f36cfab in dlopen_doit () from /lib64/libdl.so.2
#17 0x00007f238fb697c4 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2
#18 0x00007f238f36d5ad in _dlerror_run () from /lib64/libdl.so.2
#19 0x00007f238f36d041 in dlopen@@GLIBC_2.2.5 () from /lib64/libdl.so.2
#20 0x000056226d8191ad in ?? ()
#21 0x00007f238fcbc1e0 in ?? ()
#22 0xffffffffffffffff in ?? ()
#23 0x00007f238fcd7290 in ?? ()
#24 0x0000000000000000 in ?? ()
```
"
44576,is For Loop supported? or proper way of Iterating over a 1d tensor?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
- TensorFlow installed from (source or binary):pip install
- TensorFlow version (or github SHA if from source):2.3.1

```
import os
import tempfile

import numpy as np
import tensorflow as tf
tmpdir = tempfile.mkdtemp()

class CustomModule(tf.Module):

  def __init__(self):
    super(CustomModule, self).__init__()
    self.v = tf.Variable(1, dtype=tf.int64)
    self.const = tf.constant(np.arange(10))

  @tf.function(input_signature=[tf.TensorSpec((), dtype=tf.int64)])
  def __call__(self, x):
    for ele in self.const:
      self.v.assign(ele)
    return x * self.const

module = CustomModule()

module_path = os.path.join(tmpdir)
print(module(tf.constant(2, dtype=tf.int64)))
print('Saving model...')
tf.saved_model.save(module, module_path)

imported = tf.saved_model.load(module_path)
print(imported(tf.constant(4, dtype=tf.int64)))

converter = tf.lite.TFLiteConverter.from_saved_model(module_path)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
```
error:
```
tf.Tensor([ 0  2  4  6  8 10 12 14 16 18], shape=(10,), dtype=int64)
Saving model...
...
tf.Tensor([ 0  4  8 12 16 20 24 28 32 36], shape=(10,), dtype=int64)
...
tensorflow.lite.python.convert.ConverterError: input resource[0] expected type resource != int64, the type of while_assignvariableop_resource_0[0]
        In {{node while/AssignVariableOp}}
```"
44575,tflite Converting unsupported operation: MatrixInverse,
44574,Could not find any cudnn.h matching version '8' in any subdirectory,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 1.14
- Python version: 3.6
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.1
- GPU model and memory: RTX 3090



**Describe the problem**

For some specific purpose, I am building TF 1.14 from source with CUDA 11.1, CUDNN 8.0.4 & TensorRT 7.2 on Ubuntu 16.04 but getting an error:-
![image](https://user-images.githubusercontent.com/51915348/98066809-20352b00-1e7e-11eb-8365-906eeb4dfa5b.png)

I have made various checks like LD_LIBRARY_PATH, PATH are properly set and installation using tar, deb file of CUDNN but the error persists. Even if CUDA 11.1, CUDNN 8.0.4 is incompatible with TF 1.14, why I'm getting this error as this error shows unable to find the file. Maybe incompatibility error will generate during compilation.


**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44573,Dataset.from_tensor_slices regression?,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.15.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0, 2.2, and 2.3.0
- Python version: 3.6, 3.7, 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

v2.2.0-rc4-8-g2b96f3662b 2.2.0

**Describe the current behavior**
I'm supporting someone trying to build a model. In Tensorflow 2.1.0, the code below works correctly. In version 2.2.0+, the last line fails. I am wondering if there is a regression or if there is a fix and 2.1.0 shouldn't have worked. 
**Describe the expected behavior**
I expect the dataset to be created correctly. 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf
import pandas as pd
import numpy as np


data = [
    [
        np.array(['k1', '', '']), np.array(['t1', 't2', 't3']), np.array([.03, .02, .03])
    ],
    [
        np.array(['k1', 'k2', 'k3']), np.array(['t4', 't5', 't6']), np.array([.03, .02, .03])
    ]
]
train = pd.DataFrame(data, columns=['kwd', 'title', 'labels'])
feature_cols = ['kwd', 'title']
labels = train.pop('labels')
features = {
    col: train[col] for col in feature_cols
}
batch = tf.data.Dataset.from_tensor_slices((features, labels))

```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
Traceback (most recent call last):
  File ""/array_example.py"", line 17, in <module>
    batch = tf.data.Dataset.from_tensor_slices((train, labels))
  File ""/Users/oshiv/miniconda3/envs/py37tf22/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 640, in from_tensor_slices
    return TensorSliceDataset(tensors)
  File ""/Users/oshiv/miniconda3/envs/py37tf22/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2858, in __init__
    element = structure.normalize_element(element)
  File ""/Users/oshiv/miniconda3/envs/py37tf22/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py"", line 98, in normalize_element
    ops.convert_to_tensor(t, name=""component_%d"" % i))
  File ""/Users/oshiv/miniconda3/envs/py37tf22/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1341, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/Users/oshiv/miniconda3/envs/py37tf22/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 321, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/Users/oshiv/miniconda3/envs/py37tf22/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 262, in constant
    allow_broadcast=True)
  File ""/Users/oshiv/miniconda3/envs/py37tf22/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 270, in _constant_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""/Users/oshiv/miniconda3/envs/py37tf22/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 96, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).
```"
44572,Can't get custom output name from tfserving,"I want the outputs from tfserving is:  `{""outputs"":{""scores"":[0.936071157,0.0527787767]}}`
but I got : `{ ""outputs"": [ 0.936071038, 0.0527787916 ] }`

Environment:
python 3.6
tensorflow 2.3.0

save model code:
```
class MyModel(tf.keras.Model):
    def __init__(self):
        super(MyModel, self).__init__()
        self.pmodel = model.model
        self.out = keras.layers.Lambda(lambda x:x[:,1])
        
    @tf.function(input_signature=[[tf.TensorSpec(shape=[None,1], dtype=tf.string, name='input_q'), tf.TensorSpec(shape=[None,1], dtype=tf.string, name='input_a')]])
    def call(self, inputs):
        output = self.pmodel(inputs)
        out = self.out(output)
        return {""scores"": out} 

NM = MyModel()

# test
# print(NM([qi, ai]))
# {'scores': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.93607116, 0.05277878], dtype=float32)>}

# save model
tf.saved_model.save(NM, 'saved_model/202011032114')
```
By the way, model.model is tf.keras.Model :
![image](https://user-images.githubusercontent.com/9495054/97988975-d108df00-1e18-11eb-8915-d3d1fc35a8ed.png)

SignatureDefs:
![image](https://user-images.githubusercontent.com/9495054/97989755-f34f2c80-1e19-11eb-8edd-b624bdf2d0cd.png)

Start tfserving
![image](https://user-images.githubusercontent.com/9495054/97989821-0c57dd80-1e1a-11eb-87ec-ab4f9a59aa2b.png)

What I got :(
![image](https://user-images.githubusercontent.com/9495054/97990261-a1f36d00-1e1a-11eb-8708-8673f0422c96.png)

**But!**, If model output two keys like `{""scores"": out, ""give_up"":out}`, it works well.
```
class MyModel(tf.keras.Model):
    def __init__(self):
        super(MyModel, self).__init__()
        self.pmodel = model.model
        self.out = keras.layers.Lambda(lambda x:x[:,1])
        
    @tf.function(input_signature=[[tf.TensorSpec(shape=[None,1], dtype=tf.string, name='input_q'), tf.TensorSpec(shape=[None,1], dtype=tf.string, name='input_a')]])
    def call(self, inputs):
        output = self.pmodel(inputs)
        out = self.out(output)
        return {""scores"": out, ""give_up"":out}

NM = MyModel()

# test
# print(NM([qi, ai]))
# {'scores': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.93607116, 0.05277878], dtype=float32)>, 'give_up': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.93607116, 0.05277878], dtype=float32)>}

# save model
tf.saved_model.save(NM, 'saved_model/202011032126')
```
![image](https://user-images.githubusercontent.com/9495054/97990853-83da3c80-1e1b-11eb-94c9-30e83dcf9c3a.png)

![image](https://user-images.githubusercontent.com/9495054/97990822-74f38a00-1e1b-11eb-8928-51d1d77747bb.png)

And the result:
![image](https://user-images.githubusercontent.com/9495054/97990916-9a809380-1e1b-11eb-9cee-05e8924455b6.png)

I don't know why and didn't find any solution, Is there any kind person to help me ? Many thanks !
"
44571,TFRecordWriter create in parent process can't work properly in child process,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.1
- Python version: 3.6.9
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I generate tfrecord from COCO semantic segmentation dataset with the following code. I get the error message ""tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 12"" when I load the generated tfrecord.

```python
#!/usr/bin/python3

from os.path import join;
from math import ceil;
from multiprocessing import Process, Lock;
from pycocotools.coco import COCO;
import numpy as np;
import cv2;
import tensorflow as tf;

PROCESS_NUM = 80;

def parse_function(serialized_example):

  feature = tf.io.parse_single_example(
    serialized_example,
    features = {
      'image': tf.io.FixedLenFeature((), dtype = tf.string, default_value = ''),
      'shape': tf.io.FixedLenFeature((3,), dtype = tf.int64),
      'label': tf.io.VarLenFeature(dtype = tf.float32)
    }
  );
  shape = tf.cast(feature['shape'], dtype = tf.int32);
  data = tf.io.decode_jpeg(feature['image']);
  data = tf.reshape(data, shape);
  data = tf.cast(data, dtype = tf.float32);
  label = tf.sparse.to_dense(feature['label'], default_value = 0);
  label = tf.reshape(label, (shape[0], shape[1])); # label.shape = (h, w)
  return data, label;

def create_dataset(image_dir, label_dir, trainset = True):

  anno = COCO(join(label_dir, 'instances_train2017.json' if trainset else 'instances_val2017.json'));
  writer = tf.io.TFRecordWriter('trainset.tfrecord' if trainset else 'testset.tfrecord');
  if writer is None:
    print('invalid output file!');
    exit(1);
  imgs_for_each = ceil(len(anno.getImgIds()) / PROCESS_NUM);
  handlers = list();
  lock = Lock();
  for i in range(PROCESS_NUM):
    handlers.append(Process(target = worker, args = (anno, writer, image_dir, anno.getImgIds()[i * imgs_for_each:(i+1) * imgs_for_each] if i != PROCESS_NUM - 1 else anno.getImgIds()[i * imgs_for_each:], lock)));
    handlers[-1].start();
  for handler in handlers:
    handler.join();
  writer.close();

def worker(anno, writer, image_dir, image_ids, lock):
  for image in image_ids:
    img_info = anno.loadImgs([image])[0];
    img = cv2.imread(join(image_dir, img_info['file_name']));
    if img is None:
      print('can\'t open image %s' % (join(image_dir, img_info['file_name'])));
      continue;
    mask = np.zeros((img_info['height'], img_info['width']));
    for category in anno.getCatIds():
      annIds = anno.getAnnIds(imgIds = image, catIds = category);
      anns = anno.loadAnns(annIds);
      for ann in anns:
        # for every instance of category in current image
        instance_mask = anno.annToMask(ann);
        mask = np.maximum(mask, instance_mask * category);
    trainsample = tf.train.Example(features = tf.train.Features(
      feature = {
        'image': tf.train.Feature(bytes_list = tf.train.BytesList(value = [tf.io.encode_jpeg(img).numpy()])),
        'shape': tf.train.Feature(int64_list = tf.train.Int64List(value = list(img.shape))),
        'label': tf.train.Feature(float_list = tf.train.FloatList(value = tf.reshape(mask, (-1,))))
      }
    ));
    lock.acquire();
    writer.write(trainsample.SerializeToString());
    lock.release();

if __name__ == ""__main__"":

  assert tf.executing_eagerly();
  from sys import argv;
  if len(argv) != 4:
    print('Usage: %s <train image dir> <test image dir> <anno dir>' % (argv[0]));
    exit(1);
  create_dataset(argv[1], argv[3], True);
  create_dataset(argv[2], argv[3], False);

```

**Describe the expected behavior**

the tfrecord should be loaded without problem.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

the above code can generate tfrecord from COCO2017. the generated tfrecord can't be loaded correctly. you may want to comment the line generating trainset.tfrecord. the smaller testset.tfrecord can be generated more quickly.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

>Traceback (most recent call last):
  File ""/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/eager/context.py"", line 2102, in execution_mode
    yield
  File ""/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 758, in _next_internal
    output_shapes=self._flat_output_shapes)
  File ""/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2610, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 6843, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 12 [Op:IteratorGetNext]

>During handling of the above exception, another exception occurred:

>Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 736, in __next__
    return self.next()
  File ""/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 772, in next
    return self._next_internal()
  File ""/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 764, in _next_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/eager/context.py"", line 2105, in execution_mode
    executor_new.wait()
  File ""/home/Xieyi/.local/lib/python3.6/site-packages/tensorflow/python/eager/executor.py"", line 67, in wait
    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 12
"
44570,TF2.4 Build - An error occurred during the fetch of repository 'io_bazel_rules_go':,"**System information**
- OS Platform and Distribution : Windows 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.4
- Python version: 3.7
- Bazel version (if compiling from source): 3.7.0
- GCC/Compiler version (if compiling from source): MSVC 2019
- CUDA/cuDNN version: Cuda 11.0 / cuDNN 8.0.4
- GPU model and memory: 2070 Max-Q

When building from source, I receive the following error when running the following command.

bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

ERROR: An error occurred during the fetch of repository 'io_bazel_rules_go':
   Traceback (most recent call last):
        File ""C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_tools/tools/build_defs/repo/git.bzl"", line 181, column 30, in _git_repository_implementation
                update = _clone_or_update(ctx)
        File ""C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_tools/tools/build_defs/repo/git.bzl"", line 36, column 20, in _clone_or_update
                git_ = git_repo(ctx, directory)
        File ""C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_tools/tools/build_defs/repo/git_worker.bzl"", line 91, column 12, in git_repo
                _update(ctx, git_repo)
        File ""C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_tools/tools/build_defs/repo/git_worker.bzl"", line 101, column 9, in _update
                init(ctx, git_repo)
        File ""C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_tools/tools/build_defs/repo/git_worker.bzl"", line 118, column 15, in init
                _error(ctx.name, cl, st.stderr)
        File ""C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_tools/tools/build_defs/repo/git_worker.bzl"", line 190, column 9, in _error
                fail(""error running '%s' while working with @%s:\n%s"" % (command_text, name, stderr))
Error in fail: error running 'git init C:/users/adam/_bazel_adam/ahelnrbq/external/io_bazel_rules_go' while working with @io_bazel_rules_go:
java.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(""git"" init C:/users/adam/_bazel_adam/ahelnrbq/external/io_bazel_rules_go): The system cannot find the file specified.
 (error: 2)
INFO: Repository com_google_protobuf instantiated at:
  C:/sdks/tensorflow-dev/WORKSPACE:19:16: in <toplevel>
  C:/sdks/tensorflow-dev/tensorflow/workspace.bzl:585:20: in tf_repositories
Repository rule tf_http_archive defined at:
  C:/sdks/tensorflow-dev/third_party/repo.bzl:131:34: in <toplevel>
INFO: Repository envoy_api instantiated at:
  C:/sdks/tensorflow-dev/WORKSPACE:116:10: in <toplevel>
  C:/users/adam/_bazel_adam/ahelnrbq/external/com_github_grpc_grpc/bazel/grpc_deps.bzl:235:21: in grpc_deps
Repository rule http_archive defined at:
  C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>
ERROR: no such package '@io_bazel_rules_go//go': error running 'git init C:/users/adam/_bazel_adam/ahelnrbq/external/io_bazel_rules_go' while working with @io_bazel_rules_go:
java.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(""git"" init C:/users/adam/_bazel_adam/ahelnrbq/external/io_bazel_rules_go): The system cannot find the file specified.
 (error: 2)
INFO: Elapsed time: 28.671s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    Fetching @upb; fetching
    Fetching @build_bazel_rules_apple; fetching
    Fetching @build_bazel_apple_support; fetching
    Fetching ...al/upb; Extracting C:/users/adam/_bazel_adam/ahelnrbq/external/upb/temp630520761\
6925763744/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz
    Fetching ..._apple; Extracting C:/users/adam/_bazel_adam/ahelnrbq/external/build_bazel_rules\
_apple/temp16001111932413110489/5131f3d46794bf227d296c82f30c2499c9de3c5b.tar.gz
    Fetching ...upport; Extracting C:/users/adam/_bazel_adam/ahelnrbq/external/build_bazel_apple\
_support/temp14862261056236700618/501b4afb27745c4813a88ffa28acd901408014e4.tar.gz"
44569,Build v2.1.0 from source on Windows [ERROR: Linking of rule '//tensorflow/lite/python/optimize:_tensorflow_lite_wrap_calibration_wrapper.so' failed],"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 enterprise 1909 18363.1139
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.1.0
- Python version: 3.6.12
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source): VS 2019 Community
- CUDA/cuDNN version:  CUDA: v10.1.243 / cuDNN: 7.6.5
- GPU model and memory: CPU: Intel Core i7-4710HQ / Memory: 16G

**Describe the problem**

I encountered the following error message while building Tensorflow v2.1.0 from source on Windows 10
The detailed messages can be found in attached text file. 
```
ERROR: D:/kelvinwu/code/tensorflow/tensorflow/lite/python/optimize/BUILD:29:1: Linking of rule '//tensorflow/lite/python/optimize:_tensorflow_lite_wrap_calibration_wrapper.so' failed (Exit 1120)
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Setup the environment following the instructions on Tensorflow page [[link](https://www.tensorflow.org/install/source_windows)].
2. Git clone Tenorflow and checkout to v2.1.0
3. Run <code>python configure.py</code>, the settings are shown below:
```
You have bazel 0.29.1 installed.
Please specify the location of python. [Default is D:\Users\Asus_user\anaconda3\envs\build-tensorflow-2.1\python.exe]:


Found possible Python library paths:
  D:\Users\Asus_user\anaconda3\envs\build-tensorflow-2.1\lib\site-packages
Please input the desired Python library path to use.  Default is [D:\Users\Asus_user\anaconda3\envs\build-tensorflow-2.1\lib\site-packages]

Do you wish to build TensorFlow with XLA JIT support? [y/N]: N
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Found CUDA 10.1 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include
Found cuDNN 7 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y
```
4. In order to fetch the http_archives without ERRORs, I had to add the following http_archives in WORKSPACE:
```
http_archive(
    name = ""com_google_protobuf"",
    sha256 = ""b9e92f9af8819bbbc514e2902aec860415b70209f31dfc8c4fa72515a5df9d59"",
    # This protobuf release is based on protobuf 3.8.0.
    strip_prefix = ""protobuf-310ba5ee72661c081129eb878c1bbcec936b20f0"",
    urls = [
        ""https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz"",
        ""https://github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz"",
    ],
)
http_archive(
    name = ""gif"",
    build_file = ""//third_party:gif.BUILD"",
    sha256 = ""31da5562f44c5f15d63340a09a4fd62b48c45620cd302f77a6d9acf0077879bd"",
    strip_prefix = ""giflib-5.2.1"",
    urls = [
        ""https://storage.googleapis.com/mirror.tensorflow.org/pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz"",
        ""http://pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz"",
    ],
)
http_archive(
    name = ""com_google_absl"",
    build_file = ""//third_party:com_google_absl.BUILD"",
    sha256 = ""acd93f6baaedc4414ebd08b33bebca7c7a46888916101d8c0b8083573526d070"",
    strip_prefix = ""abseil-cpp-43ef2148c0936ebf7cb4be6b19927a9d9d145b8f"",
    urls = [
        ""https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/43ef2148c0936ebf7cb4be6b19927a9d9d145b8f.tar.gz"",
        ""http://github.com/abseil/abseil-cpp/archive/43ef2148c0936ebf7cb4be6b19927a9d9d145b8f.tar.gz"",
    ],
)
http_archive(
        name = ""cub_archive"",
        build_file = ""//third_party:cub.BUILD"",
        sha256 = ""6bfa06ab52a650ae7ee6963143a0bbc667d6504822cbd9670369b598f18c58c3"",
        strip_prefix = ""cub-1.8.0"",
        urls = [
            ""https://storage.googleapis.com/mirror.tensorflow.org/github.com/NVlabs/cub/archive/1.8.0.zip"",
            ""https://github.com/NVlabs/cub/archive/1.8.0.zip"",
        ],
    )
http_archive(
        name = ""eigen_archive"",
        build_file = ""//third_party:eigen.BUILD"",
        sha256 = ""65d732985b593b553c20566e1f236f48dcc626730c418aed7b2aa1d0e3f1a0af"",
        strip_prefix = ""eigen-4e696901f873a2347f76d931cf2f701e31e15d05"",
        urls = [
            ""https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen/-/archive/4e696901f873a2347f76d931cf2f701e31e15d05/eigen-4e696901f873a2347f76d931cf2f701e31e15d05.tar.gz"",
            ""https://gitlab.com/libeigen/eigen/-/archive/4e696901f873a2347f76d931cf2f701e31e15d05/eigen-4e696901f873a2347f76d931cf2f701e31e15d05.tar.gz"",
        ],
    )
http_archive(
    name = ""png"",
    build_file = ""//third_party:png.BUILD"",
    sha256 = ""ca74a0dace179a8422187671aee97dd3892b53e168627145271cad5b5ac81307"",
    strip_prefix = ""libpng-1.6.37"",
    urls = [
        ""https://storage.googleapis.com/mirror.tensorflow.org/github.com/glennrp/libpng/archive/v1.6.37.tar.gz"",
        ""https://github.com/glennrp/libpng/archive/v1.6.37.tar.gz"",
    ],
)
http_archive(
    name = ""icu"",
    strip_prefix = ""icu-release-64-2"",
    sha256 = ""10cd92f1585c537d937ecbb587f6c3b36a5275c87feabe05d777a828677ec32f"",
    urls = [
        ""https://storage.googleapis.com/mirror.tensorflow.org/github.com/unicode-org/icu/archive/release-64-2.zip"",
        ""https://github.com/unicode-org/icu/archive/release-64-2.zip"",
    ],
    build_file = ""//third_party/icu:BUILD.bazel""
)
```
5. Run <code>bazel build //tensorflow/tools/pip_package:build_pip_package</code> and the ERROR showed up after hours of compiling

**Any other info / logs**
The complete messages shown on console: [error_messages_20201104.txt](https://github.com/tensorflow/tensorflow/files/5484927/error_messages_20201104.txt)
"
44568,Inconsistency in loss on SAME data for train and validation modes,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux (Colab)
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.9
- CUDA/cuDNN version: Colab
- GPU model and memory: Colab has k80 I think


**Describe the current behavior**
I'm implementing a semantic segmentation model with images. As a good practice I tested my training pipeline with just one image and tried to over-fit that image. To my surprise, when training with the exactly the same images, the loss goes to 0 as expected but when evaluating THE SAME IMAGES, the loss is much much higher, and it keeps going up as the training continues. So the segmentation output is garbage when `training=False`, but when run with `training=True` is works perfectly.

To be able to anyone to reproduce this I took the official [segmentation tutorial](https://www.tensorflow.org/tutorials/images/segmentation
) and modified it a little for training a convnet from scratch and just 1 image. The model is very simple, just a sequence of Conv2D with batch normalization and Relu. The results are the following

![Screenshot from 2020-11-03 18-04-17](https://user-images.githubusercontent.com/8033598/98050621-18cf3c80-1e00-11eb-93b0-0470054ddc50.png)

As you see the loss and eval_loss are really different, and making inference to the image gives perfect result in training mode and in eval mode is garbage.

**Describe the expected behavior**
I know Batchnormalization behaves differently in inference time since it uses the averaged statistics calculated whilst training. Nonetheless, since we are training with just 1 same image and evaluating in the same image, this shouldn't happen right? Moreover I implemented the same architecture with the same optimizer in Pytorch and this does not happen there. With pytorch it trains and eval_loss converges to train loss

**Standalone code to reproduce the issue**
Here you can find the above mentioned https://colab.research.google.com/drive/18LipgAmKVDA86n3ljFW8X0JThVEeFf0a#scrollTo=TWDATghoRczu
and at the end also the Pytorch implementation
"
44567,tf-nightly-gpu looking for cusolver64_10.dll on a Cuda 11.1 / cuDNN 8.0.4 RTX3080 Build,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.5.0-dev20201103
- Python version: 3.8.5
- Installed using: pip
- CUDA/cuDNN version: Cuda 11.1 cuDNN 8.0.4
- GPU model and memory: Nvidia RTX 3080

**Describe the problem**
After installing Cuda 11.1 / cuDNN 8.0.4 and tf-nightly-gpu (today's 1132020), I tried to test that tensorflow sees my gpu, but it throws an error saying it 'Could not load dynamic library 'cusolver64_10.dll''

I do have cusolver64_11.dll in my Nvidia bin folder, but for some reason TF is looking for the Cuda 10 version of that one specific file (it found and opened all the other Cuda11 dlls) 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
python
import tensorflow as tf
assert tf.test.is_gpu_available()

"
44566,Enforce single optimized kernel implementation,"Currently we can provide potentially multiple optimized kernel implementations via the `TAGS` parameter to the TFLM Makefile.

While this was originally supported, we don't have any use-case for it and are moving towards enforcing a single optimized kernel implementation. This will also reduce some complexity in the makefile and allow for improved debugging.

Corresponding internal issue: http://b/170501366"
44565,from_tensor_slices not compatible with sparse data,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab / TF 2.3.0

**Describe the current behavior**
It's stated in the documentation that `Dataset` is able to handle `SpareTensors` on top of `Ragged` tensors and the other standard data types. 

Issue is `from_tensor_slices` is not able to handle a list of `SparseTensors`, but `from_tensors` which accepts a single datapoint is able to instantiate it. Currently `Dataset.from_generator` is not able to handle `Sparse` datatype either (in the current release at least) https://github.com/tensorflow/tensorflow/pull/41981 so I'm not sure how one is supposed to handle datasets that are compromised of both Sparse and dense data. 


**Describe the expected behavior**
`from_tensor_slices` should accept a list of `SparseTensors` given that there is exactly the same number of items in the list as the other features passed into `from_tensor_slices` 


**Standalone code to reproduce the issue**
[Colab Link](https://colab.research.google.com/drive/1952M7pHEgpl0KSFPkHz7gu9BpxGejdfu?usp=sharing
)
**Other info / logs** 

I'm not sure if it's just me, but when I read this [part of the documentation](https://www.tensorflow.org/guide/data#dataset_structure) I was under the impression that this feature would be supported. Thus you may identify this as a `feature_request` rather than a bug if I've misunderstood the documentarian. 
"
44564,TF 2.3 S3 client having permission issues with `tf.data.TFRecordDataset()`,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.3
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:N/A
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
TF 2.3 `tf.data.TFRecordDataset()` failed to load files from S3.
Side note: 
`tf_file_io` is working properly for the same setup.

```
>> from tensorflow.python.lib.io import file_io as tf_file_io
>> tf_file_io.file_exists('SOME_S3_FILE')

True
```

See error log below.
**Describe the expected behavior**
Same API and environments works well with TF 2.1 and TF 1.15.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
dataset = tf.data.TFRecordDataset('SOME_S3_PATH')
for raw_record in dataset.take(10):
  print(repr(raw_record))
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/default_user/.conda/envs/user/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 736, in __next__
    return self.next()
  File ""/home/default_user/.conda/envs/user/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 772, in next
    return self._next_internal()
  File ""/home/default_user/.conda/envs/user/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 764, in _next_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File ""/home/default_user/.conda/envs/user/lib/python3.6/contextlib.py"", line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/default_user/.conda/envs/user/lib/python3.6/site-packages/tensorflow/python/eager/context.py"", line 2105, in execution_mode
    executor_new.wait()
  File ""/home/default_user/.conda/envs/user/lib/python3.6/site-packages/tensorflow/python/eager/executor.py"", line 67, in wait
    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.FailedPreconditionError: AWS Credentials have not been set properly. Unable to access the specified S3 location
```"
44563,Error using TensorBoard callback in graph mode in TF 2.4,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.0rc0
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Attempting to use `tf.keras.callbacks.TensorBoard` with eager execution disabled results in an error in TF 2.4.

**Describe the expected behavior**

There should be no error, callback should work as normal.

**Standalone code to reproduce the issue**

``` python
import tensorflow as tf
import numpy as np

tf.compat.v1.disable_eager_execution()

inp = tf.keras.Input((1,))
out = tf.keras.layers.Dense(1)(inp)

model = tf.keras.Model(inp, out)

model.predict(
    np.zeros((32, 1)),
    callbacks=[tf.keras.callbacks.TensorBoard(log_dir=""test"")],
)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
Traceback (most recent call last):
  File "".../tmp.py"", line 11, in <module>
    model.predict(
  File ""...\site-packages\tensorflow\python\keras\engine\training_v1.py"", line 982, in predict
    return func.predict(
  File ""...\site-packages\tensorflow\python\keras\engine\training_arrays_v1.py"", line 706, in predict
    return predict_loop(
  File ""...\site-packages\tensorflow\python\keras\engine\training_arrays_v1.py"", line 217, in model_iteration
    callbacks = cbks.configure_callbacks(
  File ""...\site-packages\tensorflow\python\keras\callbacks.py"", line 115, in configure_callbacks
    callback_list = CallbackList(callbacks)
  File ""...\site-packages\tensorflow\python\keras\callbacks.py"", line 237, in __init__
    self._should_call_train_batch_hooks = any(
  File ""...\site-packages\tensorflow\python\keras\callbacks.py"", line 238, in <genexpr>
    cb._implements_train_batch_hooks() for cb in self.callbacks)
  File ""...\site-packages\tensorflow\python\keras\callbacks.py"", line 2310, in _implements_train_batch_hooks
    return self._should_trace  # Only call batch hooks when tracing is enabled
AttributeError: 'TensorBoard' object has no attribute '_should_trace'
```"
44561,Convert TensorFlow to TensorFlow Lite - very small model after conversion,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source): 2.3.0

Following my previous issues https://github.com/tensorflow/tensorflow/issues/44091 https://github.com/tensorflow/tensorflow/issues/44435 of converting the Tensorflow model to Tensorflow Lite I prepared a notebook inspired by the notebook from the link [Object_Detection_in_TFLite](https://github.com/sayakpaul/E2E-Object-Detection-in-TFLite/blob/master/Object_Detection_in_TFLite.ipynb). From notebook  we can see that after converting model ```ssd_mobilenet_v2_320x320_coco17_tpu-8``` [Training a pets detector model within minutes with TFOD API](https://colab.research.google.com/github/sayakpaul/TF-2.0-Hacks/blob/master/Training_a_pets_detector_model_within_minutes_with_TFOD_API.ipynb) to TFLite the model size is 5711632 Bytes. In my cases the size of model is 516 bytes [mobilenet_v2_to_tflite](https://colab.research.google.com/drive/1GYKWFR7A6WknexnFa312eCa3ZST58i_3?usp=sharing)

Where Is the problem? I try to convert the model up to a month and the results are always the same. Model size is around 500 bytes. I thought it was a bug due to the fact that there is something wrong with training the model but I can see that even for the standard model with the repository the error is the same.

"
44560,Custom LearningRateSchedule not called within a Mirrored Strategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
**From the tutorial [Custom training with tf.distribute.Strategy](https://www.tensorflow.org/tutorials/distribute/custom_training) with a Custom LearningRateSchedule**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Colab**
- TensorFlow version (use command below): **2.3.0**
- Python version: **3.6.9**
- GPU model and memory: **GPU from Colab**

**Describe the current behavior**
In a distributed environment, when adding a custom LearningRateSchedule to Adam optimizer to decay the learning rate over epochs, the learning rate is not decayed as expected.
I tested the same code in an environment without the distributed scope and the LR decay over the epochs.

**Describe the expected behavior**
The optimizer to call the LearningRateSchedule __call__ method to decay the learning rate after applying the gradients

**Standalone code to reproduce the issue**
Please check this Colab from the TF tutorial: https://colab.research.google.com/drive/1sT3MZ5hdGvsPhFQDrCFXzMR760HPCSsc?usp=sharing

See chapter: **Training loop**

**Other info / logs** 
```
class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self):
      super(CustomSchedule, self).__init__()
      self.lr = 0.01

    def __call__(self, step):
      self.lr = self.lr/10
      print(""NEW LR:"", self.lr)
      return self.lr

with strategy.scope():
    model = create_model()
    optimizer = tf.keras.optimizers.Adam(learning_rate=CustomSchedule())
    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
```
The `learning_rate=CustomSchedule()` is the only line I changed from the tutorial.

See the Colab for the **train_step**"
44559,Cannot install latest nightly version via pip install tf-nightly,"Try `pip install tf-nightly` will install 2.5.0.dev20201029

Try `pip install tf-nightly==2.5.0.dev20201102` will output
```
ERROR: Could not find a version that satisfies the requirement tf-nightly==2.5.0.dev20201102 (from versions: 2.4.0.dev20200903, 2.4.0.dev20200904, 2.4.0.dev20200905, 2.4.0.dev20200906, 2.4.0.dev20200907, 2.4.0.dev20200908, 2.4.0.dev20200909, 2.4.0.dev20200910, 2.4.0.dev20200911, 2.4.0.dev20200912, 2.4.0.dev20200913, 2.4.0.dev20200914, 2.4.0.dev20200915, 2.4.0.dev20200916, 2.4.0.dev20200917, 2.4.0.dev20200918, 2.4.0.dev20200919, 2.4.0.dev20200920, 2.4.0.dev20200921, 2.4.0.dev20200922, 2.4.0.dev20200923, 2.4.0.dev20200924, 2.4.0.dev20200925, 2.4.0.dev20200926, 2.4.0.dev20200927, 2.4.0.dev20200928, 2.4.0.dev20200929, 2.4.0.dev20200930, 2.4.0.dev20201001, 2.4.0.dev20201002, 2.4.0.dev20201003, 2.4.0.dev20201004, 2.4.0.dev20201005, 2.4.0.dev20201007, 2.4.0.dev20201008, 2.4.0.dev20201010, 2.4.0.dev20201011, 2.4.0.dev20201012, 2.4.0.dev20201014, 2.4.0.dev20201015, 2.4.0.dev20201016, 2.4.0.dev20201017, 2.4.0.dev20201018, 2.4.0.dev20201019, 2.4.0.dev20201020, 2.4.0.dev20201021, 2.4.0.dev20201022, 2.4.0.dev20201023, 2.5.0.dev20201024, 2.5.0.dev20201025, 2.5.0.dev20201026, 2.5.0.dev20201027, 2.5.0.dev20201028, 2.5.0.dev20201029)
ERROR: No matching distribution found for tf-nightly==2.5.0.dev20201102
```"
44557,"ValueError: No gradients provided for any variable: ['Variable:0', 'Variable_1:0']","I am facing the same issue gradient issue, I google it but not found any working solution. can anyone suggest me how to
deal with this problem..
```
import tensorflow as tf    version is 2.1.0
print('training data shape    ', X.shape)
print('training labels shape  ', y.shape)
print('validation data shape  ', X_v.shape)
print('validation labels shape', y_v.shape)
print('test data shape        ', X_t.shape)
""""""
print following
training data shape     (10000, 784)
training labels shape   (10000, 10) # 10 classes label one hot encoded
validation data shape   (2000, 784) 
validation labels shape (2000, 10) # 10 classes label one hot encoded
test data shape         (4000, 784)
""""""
def build_computation_graph(X, y, X_val, y_val, X_test):
    graph = tf.Graph()
    IMAGE_SIZE, NUM_CLASSES, SEED = 28, 10, 42
    learning_rate = 0.5
    
    with graph.as_default():    
        ############### Data sets conversion into tensorflow constant ##########################
        tf_train_dataset = tf.constant(X, dtype=tf.float64)
        tf_valid_dataset = tf.constant(X_val, dtype=tf.float64)
        tf_test_dataset = tf.constant(X_test, dtype=tf.float64)
        tf_train_label = tf.constant(y, dtype=tf.float64)
        tf_validation_label = tf.constant(y_val, dtype=tf.float64)

        ###################### weight matrix and Biases initialization ####################################

        # Initialized By using random values following a (truncated) normal distribution
        weights = tf.Variable(tf.random.truncated_normal((IMAGE_SIZE * IMAGE_SIZE, NUM_CLASSES),
                                                         seed=tf.random.set_seed(SEED), dtype=tf.float64),
                                                         dtype=tf.float64, trainable=True)

        biases = tf.Variable(tf.zeros([NUM_CLASSES] , dtype=tf.float64), trainable=True)

        # multiply weight matrix and add Bias
        logit = tf.add(tf.matmul(tf_train_dataset, weights), biases)

        # generate loss function
        # labels: Each row labels[i] must be a valid probability distribution
        # logits: Unscaled log probabilities.
        loss =  lambda : tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_label, logits=logit))

        # optimizer
        #optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(loss)
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate).minimize(loss, var_list=[weights,biases])

        train_prediction = tf.nn.softmax(logit)
        valid_prediction = tf.nn.softmax(tf.nn.matmul(tf_valid_dataset, weights) + biases)
        test_prediction = tf.nn,softmax(tf.nn.matmul(tf_test_dataset, weights) + biases)
        return graph
# End

build_computation_graph(X, y, X_v, y_v, X_t)


 #########################   Error ##################################################
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-36-311f05332c36> in <module>
----> 1 build_computation_graph(X, y, X_v, y_v, X_t)

<ipython-input-35-f0e7793c1da2> in build_computation_graph(X, y, X_val, y_val, X_test)
     31         # optimizer
     32         #optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(loss)
---> 33         optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate).minimize(loss, var_list=[weights,biases])
     34 
     35         train_prediction = tf.nn.softmax(logit)

C:\Anaconda\lib\site-packages\tensorflow_core\python\keras\optimizer_v2\optimizer_v2.py in minimize(self, loss, var_list, grad_loss, name)
    316         loss, var_list=var_list, grad_loss=grad_loss)
    317 
--> 318     return self.apply_gradients(grads_and_vars, name=name)
    319 
    320   def _compute_gradients(self, loss, var_list, grad_loss=None):

C:\Anaconda\lib\site-packages\tensorflow_core\python\keras\optimizer_v2\optimizer_v2.py in apply_gradients(self, grads_and_vars, name)
    424       ValueError: If none of the variables have gradients.
    425     """"""
--> 426     grads_and_vars = _filter_grads(grads_and_vars)
    427     var_list = [v for (_, v) in grads_and_vars]
    428 

C:\Anaconda\lib\site-packages\tensorflow_core\python\keras\optimizer_v2\optimizer_v2.py in _filter_grads(grads_and_vars)
   1037   if not filtered:
   1038     raise ValueError(""No gradients provided for any variable: %s."" %
-> 1039                      ([v.name for _, v in grads_and_vars],))
   1040   if vars_with_empty_grads:
   1041     logging.warning(

ValueError: No gradients provided for any variable: ['Variable:0', 'Variable_1:0'].
```
All data value lies between [-1, 1]

**System information**
- OS Platform - window 10
- tensorFlow version (use command below):- 2.1.0
- Python version: 3.7.3
- GPU model and memory: - No GPU support
-"
44556,Second derivative of reduce_prod returns NaN,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.8.5
- CUDA/cuDNN version: executing on CPU
- GPU model and memory: executing on CPU

**Describe the current behavior**

Computing the second derivative of a function containing `tf.reduce_prod`, where one of the arguments is zero, returns NaN.

**Describe the expected behavior**

The second derivative should be finite in those cases.

**Standalone code to reproduce the issue**

```
def grad(f):
    x = tf.Variable([0.])
    with tf.GradientTape() as t:
        y = f(x)
    g = t.gradient(y, x)
    return g

def grad2(f): 
    x = tf.Variable([0.])
    with tf.GradientTape() as t2:
        with tf.GradientTape() as t1:
             y = f(x)
        g = t1.gradient(y, x)
    g2 = t2.jacobian(g, x)
    return g2

grad(lambda x: tf.reduce_prod(tf.constant([2.,0.])*x*x)) # returns 0 as expected
grad2(lambda x: tf.reduce_prod(tf.constant([2.,0.])*x*x)) # returns NaN
grad2(lambda x: 2.*x*x) # returns 4 as expected
```"
44555,tf.convert_to_tensor is slow when converting list of numpy arrays,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04 (via docker on Ubuntu 20.04)
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: - 
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: v2.3.0-rc2-23-gb36436b087 2.3.0
-   **Python version**: 3.6.9
-   **Bazel version (if compiling from source)**: -
-   **GCC/Compiler version (if compiling from source)**: -
-   **CUDA/cuDNN version**: 11.0
-   **GPU model and memory**: Quadro M2200, 4035MiB
-   **Exact command to reproduce**:
```python3
import tensorflow as tf
import numpy as np
import timeit

# make some random ""images""
np_list = [np.random.randint(0, 256, (200, 200, 3)) for _ in range(1000)]

def convert_as_list():
    tensor = tf.convert_to_tensor(np_list)

def convert_as_single_array():
    tensor = tf.convert_to_tensor(np.asarray(np_list))

# just some operation to initialize
tf.convert_to_tensor([1])

print(f""convert_as_list: {timeit.Timer(convert_as_list).timeit(1)} s"")
print(f""convert_as_single_array: {timeit.Timer(convert_as_single_array).timeit(1)} s"")
```

### Describe the problem
Using lists of numpy arrays instead of a single numpy array results in significantly slower execution time of `tf.convert_to_tensor()`.
The toy example above gives the following output on my machine, which represents a ~600 % slowdown:
```
convert_as_list: 36.3590992190002 s
convert_as_single_array: 0.6024578830001701 s
```
Using numpy arrays that are `float` instead of `int` shows the same general behaviour.

### Source code / logs

<details>
  <summary>Complete script output including tensorflow logs. Click to expand!</summary>

```
2020-11-03 14:41:56.450816: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-11-03 14:41:58.019674: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-11-03 14:41:58.027457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-03 14:41:58.027779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: Quadro M2200 computeCapability: 5.2
coreClock: 1.036GHz coreCount: 8 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 82.08GiB/s
2020-11-03 14:41:58.027817: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-11-03 14:41:58.029158: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-11-03 14:41:58.030388: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-11-03 14:41:58.030662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-11-03 14:41:58.032077: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-11-03 14:41:58.032935: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-11-03 14:41:58.035874: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-11-03 14:41:58.036017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-03 14:41:58.036329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-03 14:41:58.036605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-11-03 14:41:58.036921: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-11-03 14:41:58.041972: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2899885000 Hz
2020-11-03 14:41:58.042290: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3ee49520 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-03 14:41:58.042306: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-03 14:41:58.066763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-03 14:41:58.067127: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3e7aa050 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-11-03 14:41:58.067145: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro M2200, Compute Capability 5.2
2020-11-03 14:41:58.067303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-03 14:41:58.067560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: Quadro M2200 computeCapability: 5.2
coreClock: 1.036GHz coreCount: 8 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 82.08GiB/s
2020-11-03 14:41:58.067583: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-11-03 14:41:58.067599: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-11-03 14:41:58.067615: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-11-03 14:41:58.067632: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-11-03 14:41:58.067650: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-11-03 14:41:58.067667: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-11-03 14:41:58.067683: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-11-03 14:41:58.067747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-03 14:41:58.068006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-03 14:41:58.068224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-11-03 14:41:58.068247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-11-03 14:41:58.459390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-03 14:41:58.459438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-11-03 14:41:58.459448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-11-03 14:41:58.459633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-03 14:41:58.459972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-03 14:41:58.460242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2394 MB memory) -> physical GPU (device: 0, name: Quadro M2200, pci bus id: 0000:01:00.0, compute capability: 5.2)
convert_as_list: 36.3590992190002 s
convert_as_single_array: 0.6024578830001701 s

Process finished with exit code 0
```
</details>
"
44554,""" ImportError: cannot import name 'anchor_generator_pb2' from 'object_detection.protos' "" when trying to convert a custom model for use in the Object Detection Android example app","Hi, I'm want to use the Android example app for Object Detection with a model with greater resolution than the standard 300x300. I have downloaded the **ssd_resnet_50_fpn_coco** model from [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md](url), and I'm following the guide in the section _Custom model used_ on [https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android](url).

When running:
`python object_detection/export_tflite_ssd_graph.py --pipeline_config_path ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/pipeline.config --trained_checkpoint_prefix ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/model.ckpt --output_directory exported_model`

I get the output:
`2020-11-03 15:25:55.004086: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2020-11-03 15:25:55.009217: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""object_detection/export_tflite_ssd_graph.py"", line 97, in <module>
    from object_detection import export_tflite_ssd_graph_lib
  File ""C:\Users\Lucas\.conda\envs\python37\lib\site-packages\object_detection-0.1-py3.7.egg\object_detection\export_tflite_ssd_graph_lib.py"", line 27, in <module>
    from object_detection import exporter
  File ""C:\Users\Lucas\.conda\envs\python37\lib\site-packages\object_detection-0.1-py3.7.egg\object_detection\exporter.py"", line 24, in <module>
    from object_detection.builders import model_builder
  File ""C:\Users\Lucas\.conda\envs\python37\lib\site-packages\object_detection-0.1-py3.7.egg\object_detection\builders\model_builder.py"", line 20, in <module>
    from object_detection.builders import anchor_generator_builder
  File ""C:\Users\Lucas\.conda\envs\python37\lib\site-packages\object_detection-0.1-py3.7.egg\object_detection\builders\anchor_generator_builder.py"", line 27, in <module>
    from object_detection.protos import anchor_generator_pb2
ImportError: cannot import name 'anchor_generator_pb2' from 'object_detection.protos' (C:\Users\Lucas\.conda\envs\python37\lib\site-packages\object_detection-0.1-py3.7.egg\object_detection\protos\__init__.py)`

I have also tried this with the model **ssd_mobilenet_v2_oid_v4** as mentioned in the guide, which yields the same output.

Some extra info:
- Windows 10
- Python 3.7.9
- Tensorflow 1.15.0"
44553,tf.keras.model fit() CUDA crash when using generator inputs,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 33
-   **TensorFlow installed from (source or binary)**: pip3.7 (binary)
-   **TensorFlow version (use command below)**: v2.3.0-54-gfcc4b966f1 2.3.1
-   **Python version**: 3.7
-   **CUDA/cuDNN version**: 10.1
-   **GPU model and memory**: NVIDIA RTX 2070 Max-Q 8GB
-   **Exact command to reproduce**: See below

### Describe the problem
For academic purposes, I'm trying to preprocess CIFAR10 images before training a keras sequential model. CIFAR10 training/test images are 32x32x3 by default, I want to resize them to 244x244x3. This is not possible with a single `tf.image.resize(..)` call, as it quickly crashes due to OOM. Thus, I am using a generator function to feed the input data to the `model.fit(..)` `x` parameter.

### Source code / logs
Here is my generator function:
```python
def data_generator(images, labels, w, h):
    batch_size = 10

    i = 0
    while i < len(images):
        remaining = len(images) - i
        batch = batch_size
        if batch >= remaining:
            batch = remaining
        print(""### generator {}-{} of {}"".format(i, i+batch, len(images)))
        image_batch = tf.image.resize(images[i:i+batch], (w, h))
        #image_batch = images[i:i+batch]
        label_batch = labels[i:i+batch]
        i += batch

        yield image_batch, label_batch
```

This is how I load the images:
```python
    # get the CIFAR10 dataset
    (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

    # normalize pixel values to be between 0 and 1
    train_images, test_images = train_images / 255.0, test_images / 255.0
```

This is how I train the model:
```python
    train_generator = data_generator(train_images, train_labels, input_shape[0], input_shape[1])
    validation_generator = data_generator(test_images, test_labels, input_shape[0], input_shape[1])
    history = model.fit(x=train_generator, epochs=10,
                        validation_data=validation_generator,
                        use_multiprocessing=True)
```

... and finally, this is the crash message:
```
2020-11-03 14:26:31.048556: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error
```"
44552,Bilinear upsampling layer shift,"**System information**
- OS Platform and Distribution: Windows 10 x64
- TensorFlow installed from: pypy
- TensorFlow version: 2.3.0
- Python version: 3.8.5

**Describe the current behavior**
Inference with Opencv Dnn has Mask shift that apperas while tensorflow model inference, if model has Resize Bilinear Layer. There's no shift if ResizeBilinear layer replaced with ResizeNearest layer. Also tensorflow inference engine does not have this issue and works well with Bilinear and Nearest resize. 
I agree, that this looks like Opencv issue, but i found this bug (https://github.com/tensorflow/tensorflow/issues/29856) that took place in TF 2.0 beta and it looks similar to this issue. Now tensorflow inference works fine, but, probably, only inference was fixed, but layer still works unproperly.


Here you can see results of inference:
<details>
 <summary>Inference results</summary>

OpenCV DNN Bilinear
![test_res_bilin](https://user-images.githubusercontent.com/48096792/97987008-a3597100-1deb-11eb-9e38-050c5a83e6bb.png)

![test_res_bilin_line](https://user-images.githubusercontent.com/48096792/97987117-d0a61f00-1deb-11eb-88fd-5e4094a4abb0.png)

OpenCV DNN Nearest
![test_res_nearest](https://user-images.githubusercontent.com/48096792/97987148-dac81d80-1deb-11eb-9975-920a494c94c2.png)

Tensorflow Bilinear
![tf_res_bilin](https://user-images.githubusercontent.com/48096792/97987168-e3b8ef00-1deb-11eb-9a31-b59582d42508.png)

Tensorflow Nearest
![tf_res_nearest](https://user-images.githubusercontent.com/48096792/97987186-eadffd00-1deb-11eb-8c3b-f5d364223174.png)
</details>

##### Steps to reproduce
[reproduce.zip](https://github.com/tensorflow/tensorflow/files/5495370/reproduce.zip)

"
44551,Does TensorFlow Micro support self define output type for each layer?,"@tensorflow/micro

**Describe the problem**
In our project, we want to deploy a CNN model in QualComm sensorhub. We quantize the model firstly, the input of the model is int8 data. But in order to improve inference accuracy, we want to use uint8 data to receive the first convolution layer output and input float data to FullyConnected layer. Therefore, the data type change process is as follow: int8 input -> first conv -> uint8 output -> second conv -> ... last conv -> float output -> fullyconnected. Dose TensorFlow Micro support this? 

"
44550,fatal error: tensorflow/core/framework/types.pb.h: No such file or directory,"
when i compiling file with CMakeLists.txt file it giving error.
[CMakeLists.txt](https://github.com/tensorflow/tensorflow/files/5481075/CMakeLists.txt)
compiling file : tensorflow/tensorflow/examples/speech_commands/label_wav.cc
tensor flow version is r2.2
my machine is Ubuntu 18.04.5 LTS


error:
[ 50%] Building CXX object CMakeFiles/speech_recog.dir/label_wav.cc.o
In file included from /home/sbojja/Qual-sdm/TF/tensorflow/tensorflow/core/framework/tensor.h:23:0,
                 from /home/sbojja/Qual-sdm/TF/tensorflow/tensorflow/examples/speech_commands/label_wav.cc:19:
/home/sbojja/Qual-sdm/TF/tensorflow/tensorflow/core/framework/tensor_shape.h:22:10: fatal error: tensorflow/core/framework/types.pb.h: No such file or directory
 #include ""tensorflow/core/framework/types.pb.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
CMakeFiles/speech_recog.dir/build.make:62: recipe for target 'CMakeFiles/speech_recog.dir/label_wav.cc.o' failed
make[2]: *** [CMakeFiles/speech_recog.dir/label_wav.cc.o] Error 1
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/speech_recog.dir/all' failed
make[1]: *** [CMakeFiles/speech_recog.dir/all] Error 2
Makefile:83: recipe for target 'all' failed
make: *** [all] Error 2"
44548,RuntimeError: tensorflow/lite/kernels/conv.cc:238 input->dims->data[3] != filter->dims->data[3] (32 != 1)Node number 1 (CONV_2D) failed to prepare.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):
Ubuntu 16.04
TensorFlow version 2.3.1

**Command used to run the converter or code if youre using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
# Copy and paste here the exact command
```

**The output from the converter invocation**

```
# Copy and paste the output here.
```

**Also, please include a link to the saved model or GraphDef**

model
https://drive.google.com/file/d/1KKDE7RwQ8qVXm4AdFnoYCadcQuI1x4O2/view?usp=sharing

tflite
https://drive.google.com/file/d/1mB557Fg5eh86Hhr8GSxWzVe7rufuNq87/view?usp=sharing

h5 file
https://drive.google.com/file/d/1xvMLkTO6jlsBIHEj8LSMOIHI51FuNyqI/view?usp=sharing

```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)

2020-11-03 20:43:17.312502: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2020-11-03 20:43:17.312653: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""C:/Users/daniel/PycharmProjects/MobileNeXt-3DMPPE-Extra-Experiment/tflite/check_tflite.py"", line 5, in <module>
    interpreter.allocate_tensors()
  File ""C:\Users\daniel\Anaconda3\envs\3DMPPE_POSENET_RELEASE-master\lib\site-packages\tensorflow_core\lite\python\interpreter.py"", line 244, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File ""C:\Users\daniel\Anaconda3\envs\3DMPPE_POSENET_RELEASE-master\lib\site-packages\tensorflow_core\lite\python\interpreter_wrapper\tensorflow_wrap_interpreter_wrapper.py"", line 106, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: tensorflow/lite/kernels/conv.cc:238 input->dims->data[3] != filter->dims->data[3] (32 != 1)Node number 1 (CONV_2D) failed to prepare.

**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44547,No Keras callback for sumary database writer,"**System information**
- TensorFlow version (you are using): 2.3.1
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**

Currently there is no `tf.keras.callbacks.Callback` writing tensorboard summary data into a sql database, but only which writes it into a log folder.

**Will this change the current api? How?**

Either the option  to specify a database URI for `tensorflow.python.ops.summary_ops_v2.create_db_writer()` instead of a logdir should be added to the `tf.keras.callbacks.TensorBoard` class or a new subclass of `tf.keras.callbacks.Callback`, which saves the summary in a sql database, should be created.

**Who will benefit with this feature?**
Everyone how prefers saving the summary data in databases over all kinds of folders and files. (Me and @Informa-Tiger)"
44546,Install tensorflow in python 3,"Hello!
I have easy question, how i can install tensorflow?

Required Info | 
-- | --
Operating System | Ubuntu 18.04
Language | Python 3.6
Platform | NVIDIA Jetson Nano
CPU | Quad-core ARM A57
GPU | 128-core Maxwell

### Issue Description
I performed a million actions in the console and now I'm at this stage:
pip3 install tensorflow => no error
import tensorflow => 
```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header

Failed to load the native TensorFlow runtime.
```
"
44545,Keras: custom data validation callback on training data always returns validation data results,"Just for information: I already asked for help regarding this problem on [stackoverflow](https://stackoverflow.com/questions/64645579/keras-custom-data-validation-callback-on-training-data-always-returns-validatio). Since I did not get an answer yet, I assume that this is a non-trivial problem and therefore open an issue here.

**System information**
`python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` returned
v2.3.0-54-gfcc4b966f1 2.3.1

**Describe the current behavior**
When calling model.evaluate on training data (or any other data set) within a callback, always the results of the validation data set are returned. 

**Describe the expected behavior**
I expected to get results for the data set passed to model.evaluate - no matter if called from within a callback or not.

**Standalone code to reproduce the issue**
I created a notebook that shows the problem:
https://colab.research.google.com/drive/1H-3ULqyRZCpaasXpU1foLkEg12fYNkYK?usp=sharing"
44544,Cannot export Keras sub-classed model with 2 args as a SavedModel,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.3.1 (also tested on 2.2)
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Attempting to save the model produces the error:
`ValueError: Structure of Python function inputs does not match input_signature:`

If I remove one of the args, everything works fine. I looked at https://github.com/tensorflow/tensorflow/issues/32488 and https://github.com/tensorflow/tensorflow/issues/28165, and those also fail for me.

**Describe the expected behavior**

The model should be able to save correctly.

**Standalone code to reproduce the issue**

```python
import tensorflow as tf
from tensorflow import (
    float32,
    function,
    TensorSpec,
)
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense

class TestModel(Model):
    def __init__(
        self
    ):
        super(TestModel, self).__init__()

        self.dense = Dense(100)

    @function(
        input_signature=[
            TensorSpec([None, 512], float32, name=""responses""),
            TensorSpec([None, 512], float32, name=""contexts""),
        ]
    )
    def call(
        self,
        responses,
        contexts
    ):
        return self.dense(responses + contexts)

model = TestModel()
x = tf.random.normal((1, 512))
_ = model(x, x)
tf.saved_model.save(model, ""directory"")
```

**Other info / logs**

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _convert_inputs_to_signature(inputs, input_signature, flat_input_signature)
   2687         expand_composites=True,
-> 2688         check_types=False)  # lists are convert to tuples for `tf.data`.
   2689   except ValueError:

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/util/nest.py in flatten_up_to(shallow_tree, input_tree, check_types, expand_composites)
    951                            check_types=check_types,
--> 952                            expand_composites=expand_composites)
    953   # Discard paths returned by _yield_flat_up_to.

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/util/nest.py in assert_shallow_structure(shallow_tree, input_tree, check_types, expand_composites)
    853             _STRUCTURES_HAVE_MISMATCHING_LENGTHS.format(
--> 854                 input_length=len(input_tree), shallow_length=len(shallow_tree)))
    855       elif len(input_tree) < len(shallow_tree):

ValueError: The two structures don't have the same sequence length. Input structure has length 1, while shallow structure has length 2.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-7-d373a86dbc49> in <module>
----> 1 tf.saved_model.save(model, ""directory"")

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures, options)
    974
    975   _, exported_graph, object_saver, asset_info = _build_meta_graph(
--> 976       obj, export_dir, signatures, options, meta_graph_def)
    977   saved_model.saved_model_schema_version = constants.SAVED_MODEL_SCHEMA_VERSION
    978

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py in _build_meta_graph(obj, export_dir, signatures, options, meta_graph_def)
   1045   if signatures is None:
   1046     signatures = signature_serialization.find_function_to_export(
-> 1047         checkpoint_graph_view)
   1048
   1049   signatures, wrapped_functions = (

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_serialization.py in find_function_to_export(saveable_view)
     73   # If the user did not specify signatures, check the root object for a function
     74   # that can be made into a signature.
---> 75   functions = saveable_view.list_functions(saveable_view.root)
     76   signature = functions.get(DEFAULT_SIGNATURE_ATTR, None)
     77   if signature is not None:

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py in list_functions(self, obj, extra_functions)
    143     if obj_functions is None:
    144       obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access
--> 145           self._serialization_cache)
    146       self._functions[obj] = obj_functions
    147     if extra_functions:

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _list_functions_for_serialization(self, serialization_cache)
   2588     self.predict_function = None
   2589     functions = super(
-> 2590         Model, self)._list_functions_for_serialization(serialization_cache)
   2591     self.train_function = train_function
   2592     self.test_function = test_function

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _list_functions_for_serialization(self, serialization_cache)
   3017   def _list_functions_for_serialization(self, serialization_cache):
   3018     return (self._trackable_saved_model_saver
-> 3019             .list_functions_for_serialization(serialization_cache))
   3020
   3021   def __getstate__(self):

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py in list_functions_for_serialization(self, serialization_cache)
     85         `ConcreteFunction`.
     86     """"""
---> 87     fns = self.functions_to_serialize(serialization_cache)
     88
     89     # The parent AutoTrackable class saves all user-defined tf.functions, and

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in functions_to_serialize(self, serialization_cache)
     77   def functions_to_serialize(self, serialization_cache):
     78     return (self._get_serialized_attributes(
---> 79         serialization_cache).functions_to_serialize)
     80
     81   def _get_serialized_attributes(self, serialization_cache):

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in _get_serialized_attributes(self, serialization_cache)
     93
     94     object_dict, function_dict = self._get_serialized_attributes_internal(
---> 95         serialization_cache)
     96
     97     serialized_attr.set_and_validate_objects(object_dict)

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py in _get_serialized_attributes_internal(self, serialization_cache)
     49     # cache (i.e. this is the root level object).
     50     if len(serialization_cache[constants.KERAS_CACHE_KEY]) == 1:
---> 51       default_signature = save_impl.default_save_signature(self.obj)
     52
     53     # Other than the default signature function, all other attributes match with

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in default_save_signature(layer)
    203   original_losses = _reset_layer_losses(layer)
    204   fn = saving_utils.trace_model_call(layer)
--> 205   fn.get_concrete_function()
    206   _restore_layer_losses(original_losses)
    207   return fn

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)
   1165       ValueError: if this object has not yet been called on concrete values.
   1166     """"""
-> 1167     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
   1168     concrete._garbage_collector.release()  # pylint: disable=protected-access
   1169     return concrete

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)
   1071       if self._stateful_fn is None:
   1072         initializers = []
-> 1073         self._initialize(args, kwargs, add_initializers_to=initializers)
   1074         self._initialize_uninitialized_variables(initializers)
   1075

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    695     self._concrete_stateful_fn = (
    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 697             *args, **kwds))
    698
    699     def invalid_creator_scope(*unused_args, **unused_kwds):

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2853       args, kwargs = None, None
   2854     with self._lock:
-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2856     return graph_function
   2857

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3211
   3212       self._function_cache.missed.add(call_context_key)
-> 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3073             arg_names=arg_names,
   3074             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3075             capture_by_value=self._capture_by_value),
   3076         self._function_attributes,
   3077         function_spec=self.function_spec,

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985
--> 986       func_outputs = python_func(*func_args, **func_kwargs)
    987
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    599         # the function a weak reference to itself to avoid a reference cycle.
--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    601     weak_wrapped_fn = weakref.ref(wrapped_fn)
    602

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/saving/saving_utils.py in _wrapped_model(*args)
    132     with base_layer_utils.call_context().enter(
    133         model, inputs=inputs, build_graph=False, training=False, saving=True):
--> 134       outputs = model(inputs, training=False)
    135
    136     # Outputs always has to be a flat dict.

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    983
    984         with ops.enable_auto_cast_variables(self._compute_dtype_object):
--> 985           outputs = call_fn(inputs, *args, **kwargs)
    986
    987         if self._activity_regularizer:

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = ""nonXla""
--> 780         result = self._call(*args, **kwds)
    781
    782       new_tracing_count = self._get_tracing_count()

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    805       # In this case we have created variables on the first call, so we run the
    806       # defunned version which is guaranteed to never create variables.
--> 807       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
    808     elif self._stateful_fn is not None:
    809       # Release the lock early so that multiple threads can perform the call

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   2826     """"""Calls a graph function specialized to the inputs.""""""
   2827     with self._lock:
-> 2828       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
   2829     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   2830

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3169     if self.input_signature is None or args is not None or kwargs is not None:
   3170       args, kwargs = self._function_spec.canonicalize_function_inputs(
-> 3171           *args, **kwargs)
   3172
   3173     cache_key = self._cache_key(args, kwargs)

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/function.py in canonicalize_function_inputs(self, *args, **kwargs)
   2620           inputs,
   2621           self._input_signature,
-> 2622           self._flat_input_signature)
   2623       return inputs, {}
   2624

~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _convert_inputs_to_signature(inputs, input_signature, flat_input_signature)
   2690     raise ValueError(""Structure of Python function inputs does not match ""
   2691                      ""input_signature:\n%s"" %
-> 2692                      format_error_message(inputs, input_signature))
   2693
   2694   need_packing = False

ValueError: Structure of Python function inputs does not match input_signature:
  inputs: (
    [<tf.Tensor 'responses:0' shape=(None, 512) dtype=float32>, <tf.Tensor 'contexts:0' shape=(None, 512) dtype=float32>])
  input_signature: (
    TensorSpec(shape=(None, 512), dtype=tf.float32, name='responses'),
    TensorSpec(shape=(None, 512), dtype=tf.float32, name='contexts'))
```"
44543,ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. ERROR: Node number 484 (FlexFusedBatchNormV3) failed to prepare.,"I use the version tf-nightly==2.5.0-dev20201029 and tested the tflite model

I converted the model into tflite and the python code below can run successfully. 
```
interpreter = tf.lite.Interpreter(model_path=""model.tflite"")
for samples in data_queue:
            input_details = interpreter.get_input_details()
            output_details = interpreter.get_output_details()
            interpreter.resize_tensor_input(input_details[0]['index'], samples[""input""].shape)
            interpreter.allocate_tensors()
            interpreter.set_tensor(input_details[0]['index'], samples[""input""])
            interpreter.invoke()
            features = interpreter.get_tensor(output_details[0]['index'])
            self.vocoder(features.numpy()) 
```

However, when I use C++ code to test the tflite model, the erros below showed:

ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
ERROR: Node number 484 (FlexFusedBatchNormV3) failed to prepare."
44541,unable to trace dense layer in GNN,"**Describe the current behavior**
Hi,
I'm trying to implement GNN using tensorflow and export it to TFlite.
My implementation is largely inspired from pytorch_geometric.

After training my model I found tensorflow is not tracing my model's execution appropriately
The message was below.
```
WARNING:absl:Found untraced functions such as dense_layer_call_and_return_conditional_losses, dense_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.
WARNING:absl:Found untraced functions such as dense_layer_call_and_return_conditional_losses, dense_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.
```

My model consists of 2 graph convolutional layers and each layer computes dense layer and then some kind of scatter & gather operation.
**Describe the expected behavior**
I want tensorflow to trace my model's execution properly, so that I can painlessly export my model to tflite and execute it.

**Standalone code to reproduce the issue**
Below gist is simplified version of my code.
It is a bit long but I couldn't simplify it more to reproduce my error. sorry
https://colab.research.google.com/gist/junhyk/3744c6f54074a20b4972eada113aa76d/untitled1.ipynb
"
44539,tensorflow/go@v2.3.1: but does not contain package go/core/protobuf/for_core_protos_go_proto,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```sh
Step 21/27 : RUN $go test github.com/tensorflow/tensorflow/tensorflow/go
 ---> Running in b85702eb3116
go: finding github.com/tensorflow/tensorflow v2.3.1+incompatible
go: downloading github.com/tensorflow/tensorflow v2.3.1+incompatible
go: extracting github.com/tensorflow/tensorflow v2.3.1+incompatible
/go/go/packages/pkg/mod/github.com/tensorflow/tensorflow@v2.3.1+incompatible/tensorflow/go/saved_model.go:24:2: cannot find package
/go/go/packages/pkg/mod/github.com/tensorflow/tensorflow@v2.3.1+incompatible/tensorflow/go/saved_model.go:25:2: module github.com/tensorflow/tensorflow@latest found (v2.3.1+incompatible), but does not contain package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto
```

"
44538,OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.(Using Tensorflow data pipeline map function with tfrecords),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.5
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: NO
-   **TensorFlow installed from (source or binary)**: pip install tf-nightly 
-   **TensorFlow version (use command below)**:'2.4.0-dev20201019'
-   **Python version**: 3.7.5
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: NA
-   **GPU model and memory**:NA
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
I converted ms-coco format data into tfrecords, loaded the same using   

 autotune = tf.data.experimental.AUTOTUNE
data = tf.data.TFRecordDataset(path)
data = data.map(_decode_record,num_parallel_calls=autotune)
data = data.map(preprocess_data,num_parallel_calls= autotune)
## I've pasted the functions used below in the source code block

preprocess function have some condition check, but when using map all are executed in eager mode so gives error :

OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.

tried @tf.function, tf.py_function, functool.partial but no success, Before using tf nightly it was throwing error : Tensor' object has no attribute 'numpy'


### Source code / logs
###Creating tfrecords code:
with tf.io.TFRecordWriter(my_path) as writer:
    def _bytes_feature(value):
        if isinstance(value, type(tf.constant(0))):
            value = value.numpy()
        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
    def _convert_and_serialize(value):
        value = tf.convert_to_tensor(value)
        value = tf.io.serialize_tensor(value)
        return value
    image = np.asarray(PIL.Image.open(img_path))
   feature = {}
    image = _convert_and_serialize(image)
   feature['image'] = _bytes_feature(image)tf_example = tf.train.Example(features=tf.train.Features(feature=feature))
   writer.write(tf_example.SerializeToString())


### Reading tfrecords 
""""""
def _decode_record(record):
    feature_description = {
    'image': tf.io.FixedLenFeature([], tf.float64, default_value=0),
    }
    parsed = tf.io.parse_single_example(record,feature_description)
    return parsed
""""""
#preprocess function
""""""
def random_flip_horizontal(image, boxes):
    """"""Flips image and boxes horizontally with 50% chance

    Arguments:
      image: A 3-D tensor of shape `(height, width, channels)` representing an
        image.
      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes,
        having normalized coordinates.

    Returns:
      Randomly flipped image and boxes
    """"""
    if tf.random.uniform(()) > 0.5:
        image = tf.image.flip_left_right(image)
        boxes = tf.stack(
            [1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1
        )
    return image, boxes


def resize_and_pad_image(
    image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0
):
    """"""Resizes and pads image while preserving aspect ratio.

    1. Resizes images so that the shorter side is equal to `min_side`
    2. If the longer side is greater than `max_side`, then resize the image
      with longer side equal to `max_side`
    3. Pad with zeros on right and bottom to make the image shape divisible by
    `stride`

    Arguments:
      image: A 3-D tensor of shape `(height, width, channels)` representing an
        image.
      min_side: The shorter side of the image is resized to this value, if
        `jitter` is set to None.
      max_side: If the longer side of the image exceeds this value after
        resizing, the image is resized such that the longer side now equals to
        this value.
      jitter: A list of floats containing minimum and maximum size for scale
        jittering. If available, the shorter side of the image will be
        resized to a random value in this range.
      stride: The stride of the smallest feature map in the feature pyramid.
        Can be calculated using `image_size / feature_map_size`.

    Returns:
      image: Resized and padded image.
      image_shape: Shape of the image before padding.
      ratio: The scaling factor used to resize the image
    """"""
    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)
    if jitter is not None:
        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)
    ratio = min_side / tf.reduce_min(image_shape)
    if ratio * tf.reduce_max(image_shape) > max_side:
        ratio = max_side / tf.reduce_max(image_shape)
    image_shape = ratio * image_shape
    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))
    padded_image_shape = tf.cast(
        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32
    )
    image = tf.image.pad_to_bounding_box(
        image, 0, 0, padded_image_shape[0], padded_image_shape[1]
    )
    return image, image_shape, ratio


def preprocess_data(sample):
    """"""Applies preprocessing step to a single sample

    Arguments:
      sample: A dict representing a single training sample.

    Returns:
      image: Resized and padded image with random horizontal flipping applied.
      bbox: Bounding boxes with the shape `(num_objects, 4)` where each box is
        of the format `[x, y, width, height]`.
      class_id: An tensor representing the class id of the objects, having
        shape `(num_objects,)`.
    """"""
    image = sample[""image""]
    bbox = swap_xy(sample[""objects""][""bbox""])
    class_id = tf.cast(sample[""objects""][""label""], dtype=tf.int32)

    image, bbox = random_flip_horizontal(image, bbox)
    image, image_shape, _ = resize_and_pad_image(image)

    bbox = tf.stack(
        [
            bbox[:, 0] * image_shape[1],
            bbox[:, 1] * image_shape[0],
            bbox[:, 2] * image_shape[1],
            bbox[:, 3] * image_shape[0],
        ],
        axis=-1,
    )
    bbox = convert_to_xywh(bbox)
    return image, bbox, class_id
""""""


#error log 

<ipython-input-66-b4ebcba6c559> in preprocess_data(sample)
     91 
     92 #     image, bbox = random_flip_horizontal(image, bbox)
---> 93     image, image_shape, _ = resize_and_pad_image(image)
     94     image_shape = image.shape
     95 

<ipython-input-66-b4ebcba6c559> in resize_and_pad_image(image, min_side, max_side, jitter, stride)
     58     print('ratio',ratio,type(ratio))
     59     print(tf.reduce_max(image_shape),type(tf.reduce_max(image_shape)))
---> 60     if ratio * tf.reduce_max(image_shape) > max_side:
     61         ratio = max_side / tf.reduce_max(image_shape)
     62     image_shape = ratio * image_shape

env/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in __bool__(self)
    883       `TypeError`.
    884     """"""
--> 885     self._disallow_bool_casting()
    886 
    887   def __nonzero__(self):

env/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _disallow_bool_casting(self)
    490     else:
    491       # Default: V1-style Graph execution.
--> 492       self._disallow_in_graph_mode(""using a `tf.Tensor` as a Python `bool`"")
    493 
    494   def _disallow_iteration(self):

env/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _disallow_in_graph_mode(self, task)
    479     raise errors.OperatorNotAllowedInGraphError(
    480         ""{} is not allowed in Graph execution. Use Eager execution or decorate""
--> 481         "" this function with @tf.function."".format(task))
    482 
    483   def _disallow_bool_casting(self):

OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.

If there is better way to do the same then also let me know.

Basically i wanted to replicate the same as given below example : https://keras.io/examples/vision/retinanet/
But with tf-nightly this also gives same error.

Thanks,"
44536,Can't reproduce the same result: tensorflow/tensorflow/lite/micro/examples/person_detection_experimental/ model.,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18
- TensorFlow installed from (source or binary): binary
- Tensorflow version (commit SHA if source): 1.14
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arduino Nano 33

**Describe the problem**
I am trying to reproduce the same result for a [person detection](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection_experimental) by following all instruction in this [readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection_experimental/training_a_model.md) doc successfully. But, I can't get the expected result. 

Here is the output of the serial monitor:

```
Starting capture
Image captured
Reading 3080 bytes from ArduCAM
Finished reading
Decoding JPEG and converting to greyscale
Image decoded and processed
Person score: -1 No person score: 1

```
For all capture frame, the model gives the same output` Person score: -1 No person score: 1` even there is a person on a frame.

Please help to resolve this issue.

Thanks
Bhavika

**Please provide the exact sequence of commands/steps when you ran into the problem**

"
44535,tfp.stats.histogram does not print out that it has no gradients,"tfp.stats.histogram does not print out that it has no gradients.  I would expect there would be a message indicating gradients aren't flowing like when you use tf.round().

Verified no gradients via

```
x = tf.Variable([1,2,3,4,5,6,7,8])
with tf.GradientTape() as tape:
    tmp = tfp.stats.histogram(x, [0,10])
grad = tape.gradient(tmp, x)
grad is None
```"
44533,Colab feature_columns.ipynb cannot save and reload Keras model,"[Gist issue](https://colab.research.google.com/gist/Silb78dg/ec40d3472e6b67ad399c24320d671973/feature_columns.ipynb#scrollTo=RHNfDYLRv8o_) from [Colab feature_columns.ipynb](https://www.tensorflow.org/tutorials/structured_data/feature_columns)


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
```
2020-11-03 01:52:53.863478: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
v2.3.0-0-gb36436b087 2.3.0
```


**Describe the current behavior**
Cannot reload a model saved in keras format
```
ValueError: You are trying to load a weight file containing 4 layers into a model with 0 layers.
```

**Describe the expected behavior**
Being able to reload the previously saved model

**Standalone code to reproduce the issue**
[gist](https://colab.research.google.com/gist/Silb78dg/ec40d3472e6b67ad399c24320d671973/feature_columns.ipynb#scrollTo=RHNfDYLRv8o_)
Steps:

1. Run [Colab feature_columns.ipynb](https://www.tensorflow.org/tutorials/structured_data/feature_columns)
2. Add one cell
```
model.save('pet_finder.h5', include_optimizer=False)
reload_keras_model = tf.keras.models.load_model('pet_finder.h5')
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-75-876b76fb8d70> in <module>()
      1 model.save('pet_finder.h5', include_optimizer=False)
----> 2 reload_keras_model = tf.keras.models.load_model('pet_finder.h5')
      3 loss, accuracy = reload_keras_model.evaluate(test_ds)
      4 print(""Accuracy"", accuracy)

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py in load_weights_from_hdf5_group(f, layers)
    684                      'containing ' + str(len(layer_names)) +
    685                      ' layers into a model with ' + str(len(filtered_layers)) +
--> 686                      ' layers.')
    687 
    688   # We batch weight value assignments in a single backend call

ValueError: You are trying to load a weight file containing 4 layers into a model with 0 layers.
```

Thank you."
44532,CONV_2d convert to DEPTHWISE_CONV when input depth=1,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):16.04
- TensorFlow installed from (source or binary):pip
- TensorFlow version (or github SHA if from source):1.15.0


**Command used to run the converter or code if youre using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
# Copy and paste here the exact command
```
tflite_convert  --output_file=centerfacembv2fix.tflite  \
                --graph_def_file=freeze.pb   \
                --inference_type=QUANTIZED_UINT8    \
                --input_arrays=input    \
                --input_shapes=1,320,320,1   \
                --std_dev_values=255 \
                --mean_values=0 \
                --output_arrays=scores,scores_1,scores_2,detector/wh_1,detector/reg_1,detector/angle_reg

**The output from the converter invocation**

```
# Copy and paste the output here.
```
![image](https://user-images.githubusercontent.com/30410113/97936501-a0905900-1db6-11eb-9bc9-33f1d0d74c09.png)


**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```
![image](https://user-images.githubusercontent.com/30410113/97936536-cae21680-1db6-11eb-9b08-1c6f9e33cadd.png)
here is my pb model 
**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)
the first layer is CON2D in pb,but converted to tflite it is depthwiseConv

**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44531,'list' object consisting of multiple inputs has no attribute 'ndim',"hey! guys.
I have been in trouble, the error below was thrown when the model with double inputs predicted.
```bash
Traceback (most recent call last):
  File ""practice.py"", line 279, in <module>
    action = np.argmax([0.1, 1, 0.2]*agent.get_qs(current_state))
  File ""practice.py"", line 186, in get_qs
    return self.model.predict(state)[0]
  File ""C:\Users\liuzhen\.conda\envs\python37\lib\site-packages\keras\engine\training.py"", line 1380, in predict
    x, _, _ = self._standardize_user_data(x)
  File ""C:\Users\liuzhen\.conda\envs\python37\lib\site-packages\keras\engine\training.py"", line 757, in _standardize_user_data
    exception_prefix='input')
  File ""C:\Users\liuzhen\.conda\envs\python37\lib\site-packages\keras\engine\training_utils.py"", line 95, in standardize_input_data
    data = [standardize_single_array(x) for x in data]
  File ""C:\Users\liuzhen\.conda\envs\python37\lib\site-packages\keras\engine\training_utils.py"", line 95, in <listcomp>
    data = [standardize_single_array(x) for x in data]
  File ""C:\Users\liuzhen\.conda\envs\python37\lib\site-packages\keras\engine\training_utils.py"", line 30, in standardize_single_array
    elif x.ndim == 1:
AttributeError: 'list' object has no attribute 'ndim'
```
the 'state' is a list of two nd-arrays there
```python
model = Model(inputs=[input1, input2], outputs=predictions)
```
I would really appreciate it if anyone is willing to give some  #tips

_Originally posted by @tinmodeHuang in https://github.com/tensorflow/tensorflow/issues/20698#issuecomment-720506308_"
44529,Tflite model inference bug,"**System information**
- Linux Ubuntu 18.04
- TensorFlow installed from source
- TensorFlow version 2.3.0

**Describe the current behavior**
I use the following code to test the model after tflite conversion
```
def test_conversion(model_path):
    # Load TFLite model and allocate tensors.
    interpreter = tf.lite.Interpreter(model_path=model_path)
    interpreter.allocate_tensors()

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    input_shape = input_details[0]['shape']
    input_data = np.array(np.random.random_sample(input_shape), dtype=np.float64)
    interpreter.set_tensor(input_details[0]['index'], input_data)  # <- ValueError: Cannot set tensor: Got value of type NOTYPE but expected type FLOAT64 for input 0, name: input_1 

    interpreter.invoke()

    output_data = interpreter.get_tensor(output_details[0]['index'])
    print(output_data)
```

While I print input_details I get the following:
`[{'name': 'input_1', 'index': 0, 'shape': array([ 64, 224, 224,   3], dtype=int32), 'shape_signature': array([ 64, 224, 224,   3], dtype=int32), 'dtype': <class 'numpy.float64'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]`

Despite the fact, that the input_data dtype matches with expected type (np.float64) I still get the following error:
```
Traceback (most recent call last):
  File ""test_tflite.py"", line 55, in <module>
    main()
  File ""test_tflite.py"", line 51, in main
    test_conversion(input_data)
  File ""test_tflite.py"", line 30, in test_conversion
    interpreter.set_tensor(input_details[0]['index'], input_data)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py"", line 407, in set_tensor
    self._interpreter.SetTensor(tensor_index, value)
ValueError: Cannot set tensor: Got value of type NOTYPE but expected type FLOAT64 for input 0, name: input_1 
```
"
44528,LocallyConnected1D layer description,"The documentation of the [LocallyConnected1D layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LocallyConnected1D) mentions the following for the strides argument: ""Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1."" 

I assume this sentence was just copied from the Conv1D layer and should be removed since there is no dilation_rate argument for the LocallyConnected1D layer?
"
44526,Unable to parse stateful RNN in C API of Tensorflow 2.3.1,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.3.1
- Python version: 3.6.9
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): gcc 8.4.0
- CUDA/cuDNN version: CUDA 10.2, cuDNN 7.6
- GPU model and memory: GTX 1050, 4GB

**Describe the current behavior**
I have created this RNN model:
```
import tensorflow as tf
from tensorflow import keras
input = keras.layers.Input(shape=[12, 100], batch_size=1)
x = keras.layers.Dense(20, activation='relu')(input)
x = keras.layers.BatchNormalization()(x)
fc1 = keras.layers.Dense(10, activation=None)(x)
gru1 = keras.layers.LSTM(20, return_sequences=True, stateful=True)(fc1)
gru2 = keras.layers.LSTM(20, return_sequences=True, stateful=True)(gru1)
model = keras.Model(inputs=[input], outputs=[fc1, gru2])
```
When I try to use the C API of TensorFlow 2.3.1 (which I built myself with CUDA 10.2) to run inference on this model, I get the error:
```
2020-11-01 07:12:48.086124: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: ""tensorflow::TensorList"".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?
2020-11-01 07:12:48.086856: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at constant_op.cc:82 : Invalid argument: Cannot parse tensor from proto: dtype: DT_VARIANT
tensor_shape {
}
variant_val {
  type_name: ""tensorflow::TensorList""
  metadata: ""\014\000\001\002\003\004\005\006\007\010\t\n\013\001\377\377\377\377\377\377\377\377\377\001\022\002\010\001\022\002\010\024""
}

{{function_node __inference_standard_lstm_1329_specialized_for_StatefulPartitionedCall_StatefulPartitionedCall_functional_1_lstm_PartitionedCall_at_tf_graph_specialized_for_StatefulPartitionedCall_StatefulPartitionedCall_functional_1_lstm_PartitionedCall_at_tf_graph}} {{function_node __inference_standard_lstm_1329_specialized_for_StatefulPartitionedCall_StatefulPartitionedCall_functional_1_lstm_PartitionedCall_at_tf_graph_specialized_for_StatefulPartitionedCall_StatefulPartitionedCall_functional_1_lstm_PartitionedCall_at_tf_graph}} Cannot parse tensor from proto: dtype: DT_VARIANT
tensor_shape {
}
variant_val {
  type_name: ""tensorflow::TensorList""
  metadata: ""\014\000\001\002\003\004\005\006\007\010\t\n\013\001\377\377\377\377\377\377\377\377\377\001\022\002\010\001\022\002\010\024""
}
```
Here is the C code I used to serve the model:
```
    //********* Read model
    TF_Graph* Graph = TF_NewGraph();
    TF_Status* Status = TF_NewStatus();

    TF_SessionOptions* SessionOpts = TF_NewSessionOptions();
    TF_Buffer* RunOpts = NULL;

    const char* saved_model_dir = ""../model/"";
    const char* tags = ""serve""; // default model serving tag; can change in future
    int ntags = 1;

    TF_Session* Session = TF_LoadSessionFromSavedModel(SessionOpts, RunOpts, saved_model_dir, &tags, ntags, Graph, NULL, Status);
    if(TF_GetCode(Status) == TF_OK)
    {
        printf(""TF_LoadSessionFromSavedModel OK\n"");
    }
    else
    {
        printf(""%s"",TF_Message(Status));
    }

    //****** Get input tensor
    //TODO : need to use saved_model_cli to read saved_model arch
    int NumInputs = 1;
    TF_Output* Input = (TF_Output*)malloc(sizeof(TF_Output) * NumInputs);

    TF_Output t0 = {TF_GraphOperationByName(Graph, ""serving_default_input_1""), 0};
    if(t0.oper == NULL)
        printf(""ERROR: Failed TF_GraphOperationByName serving_default_input_1\n"");
    else
	printf(""TF_GraphOperationByName serving_default_input_1 is OK\n"");
    
    Input[0] = t0;
    
    //********* Get Output tensor
    int NumOutputs = 1;
    TF_Output* Output = (TF_Output*)malloc(sizeof(TF_Output) * NumOutputs);

    TF_Output t2 = {TF_GraphOperationByName(Graph, ""StatefulPartitionedCall""), 0};
    if(t2.oper == NULL)
        printf(""ERROR: Failed TF_GraphOperationByName StatefulPartitionedCall\n"");
    else	
	printf(""TF_GraphOperationByName StatefulPartitionedCall is OK\n"");
    
    Output[0] = t2;

    //********* Allocate data for inputs & outputs
    TF_Tensor** InputValues = (TF_Tensor**)malloc(sizeof(TF_Tensor*)*NumInputs);
    TF_Tensor** OutputValues = (TF_Tensor**)malloc(sizeof(TF_Tensor*)*NumOutputs);

    int ndims = 3;
    int64_t dims[] = {1,12,100};
    float data[1*12*100];
    for(int i=0; i< (1*12*100); i++)
    {
        data[i] = 1.00;
    }
    int ndata = sizeof(float)*1*12*100 ;

    TF_Tensor* int_tensor = TF_NewTensor(TF_FLOAT, dims, ndims, data, ndata, &NoOpDeallocator, 0);
    if (int_tensor != NULL)
    {
        printf(""TF_NewTensor is OK\n"");
    }
    else
	printf(""ERROR: Failed TF_NewTensor\n"");
    
    InputValues[0] = int_tensor;
    
    // //Run the Session
    TF_SessionRun(Session, NULL, Input, InputValues, NumInputs, Output, OutputValues, NumOutputs, NULL, 0,NULL , Status);

    if(TF_GetCode(Status) == TF_OK)
    {
        printf(""Session is OK\n"");
    }
    else
    {
        printf(""%s"",TF_Message(Status));
    }

    // //Free memory
    TF_DeleteGraph(Graph);
    TF_DeleteSession(Session, Status);
    TF_DeleteSessionOptions(SessionOpts);
    TF_DeleteStatus(Status);
```
The C code is a slightly modified version of https://github.com/AmirulOm/tensorflow_capi_sample/blob/master/main.c
It seems like the problem is with input = keras.layers.Input(shape=[12, 100], batch_size=1): if I change batch_size=1 to batch_size=None, then the inference runs fine. This lead me to suspect that maybe some settings or parameters may be incorrect when invoking the C API. 

**Describe the expected behavior**
When I run the same model (with batch_size=1) in Tensorflow 2.3.1 using Python (3.6.9) API, everything works fine. Since The Python API calls the C/C++ code under the hood, I would expect the C API to be able to handle stateful LSTM mode.l  

"
44525,The README file in this repo has a bad link - [404:NotFound],"The README file in this repo has a bad link - [404:NotFound]

Status code [404:NotFound] - Link: https://chat.stackoverflow.com/rooms/216694/tensorflow

This was found by an new experimental hobby project that I have just created: https://github.com/MrCull/GitHub-Repo-ReadMe-Dead-Link-Finder
If this has been in any way helpful then please consider giving the above Repo a Star.
"
44520,tf.lite.Interpreter can not be used on the newest nightly version,"I use the version tf-nightly==2.5.0-dev20201029 and tested the tflite model

when I use the code below to test the performance of the fastspeech tflite model, the first input can be converted to audio correctly. However, the second would be converted to audio full of noise. Only when I reload the interpreter, the model can be used for the next input.

the next audio full of noise:
```
interpreter = tf.lite.Interpreter(model_path=""model.tflite"")
for samples in data_queue:
            input_details = interpreter.get_input_details()
            output_details = interpreter.get_output_details()
            interpreter.resize_tensor_input(input_details[0]['index'], samples[""input""].shape)
            interpreter.allocate_tensors()
            interpreter.set_tensor(input_details[0]['index'], samples[""input""])
            interpreter.invoke()
            features = interpreter.get_tensor(output_details[0]['index'])
            self.vocoder(features.numpy()) 
```


correct:
```
for samples in data_queue:
            interpreter = tf.lite.Interpreter(model_path=""model.tflite"")
            input_details = interpreter.get_input_details()
            output_details = interpreter.get_output_details()
            interpreter.resize_tensor_input(input_details[0]['index'], samples[""input""].shape)
            interpreter.allocate_tensors()
            interpreter.set_tensor(input_details[0]['index'], samples[""input""])
            interpreter.invoke()
            features = interpreter.get_tensor(output_details[0]['index'])
            self.vocoder(features.numpy()) 
```"
44519,need some help building on windows,"**System information**
- OS Platform and Distribution:Windows 7 x64
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.15.4
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.26
- GCC/Compiler version (if compiling from source): MSVC2017
- CUDA/cuDNN version: 10.0/7.4
- GPU model and memory: K40m 12G

im trying to compile TF using /LTCG but bazel's def_parser.exe crashes I guess he doesn't understand LTO data in the .lib files, I tried to build TF twice once with LTO and once without and copying the .def files manually but this doesn't prevent bazel from rerunning the def_parser.exe, if I delete def_parser.exe bazel gives an error saying that he is corrupt and will not build anything at all, same problem if I try to modify the def_parser.exe into other exe file :x"
44518,Some variables are not restored only when MirroredStragy is used.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): official docker image <tensorflow/tensorflow:2.3.1-gpu>
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): I've tried every version (>=2.3.0) and I could reproduce this bug in each of them.
- Python version: I could reproduce in 3.6, 3.7, and 3.8
- GPU model and memory: Not relevant. Could reproduce this issue with and without GPU.

**Description**
`tf.keras.models.Model.load_weights` method doesn't restore the optimizer slot variables only when `MirroredStrategy` is used.

Here's the minimum code to demonstrate this bug.
```python3
import tensorflow as tf


def prepare_model():
    model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(1),
    ])
    model.compile(optimizer='adam', loss='mse')
    model.build([None, 1])
    return model


def print_opt_weights(tag):
    print(tag, list(map(lambda x: x.name, model.optimizer.weights)))
    return


labels = inputs = tf.random.uniform([5, 1])
model = prepare_model()
print_opt_weights('After model creation:')
model.fit(x=inputs, y=labels, batch_size=1)
print_opt_weights('After training:')
model.save_weights('save')

print('------------Without Distribute------------')
model = prepare_model()
print_opt_weights('After model creation:')
status = model.load_weights('save')
print_opt_weights('After load weight:')
# model.fit(x=inputs, y=labels, batch_size=1)
# print_opt_weights('After retraining:')
# status.assert_consumed()

print('------------With Distribute------------')
with tf.distribute.MirroredStrategy().scope():
    model = prepare_model()
    print_opt_weights('After model creation:')
    status = model.load_weights('save')
    print_opt_weights('After load weight:')
    # model.fit(x=inputs, y=labels, batch_size=1)
    # print_opt_weights('After retraining:')
    # status.assert_consumed()


'''
The commented out lines will make optimizers in
<without distribute> and <with distribute>
have the same number of weights.

Upon `load_weights` call, all the slot variables in the optimizer
will be created when it's not using distribution strategy,
while they are not created until the model is actually retrained
with a distribution strategy in use.
'''
```
The above snippet will produce the output like below:
```
After model creation: []
After training: ['Adam/iter:0', 'Adam/dense/kernel/m:0', 'Adam/dense/bias/m:0', 'Adam/dense/kernel/v:0', 'Adam/dense/bias/v:0']
------------Without Distribute------------
After model creation: []
After load weight: ['dense_1/kernel/m:0', 'dense_1/kernel/v:0', 'dense_1/bias/m:0', 'dense_1/bias/v:0']
------------With Distribute------------
After model creation: []
After load weight: []
```

As we can see from the output, the optimizer slot variables are correctly restored without `MirroredStrategy`, while they are not restored with `MirroredStrategy`.
In my code, I was calling `assert_consumed` method just after calling `load_weights` to make sure everything is loaded correctly before proceeding to any other operations. And due to this unexpected difference in the behavior, it was crushing when it uses `MirroredStrategy`.

In either case (with/without distribution strategy), all the variables are (probably) restored after retraining (calling `fit` again).
So the easiest solution will be to just ignore this difference, but I want to know what makes this difference and how we can fix this.

Thank you so much for taking the time to take a look at this issue.
Any comments or suggestions will be very helpful."
44517,Custome loop distributed learning examples [proposed Label] comp:dist-start,"Hello,
I saw you provide the latest strategies to write customer loop for distributed learning in [this video](https://www.youtube.com/watch?v=jKV53r9-H14).
However I am trying to find examples code in GitHub, but it seems these new style codes are not available.

Would you please let me know if you have published those codes or not? or any similar ones?

Thank you "
44516,keras preprocessing data_utils.py not working for non-'inferred' labels,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/5473994/tf_env.txt)

You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
data_utils works for labels= 'inferred', but not for any provided list of labels

**Describe the expected behavior**

It should work. I changed it as per:

[data_utils.txt](https://github.com/tensorflow/tensorflow/files/5474032/data_utils.txt)
which works as far as I can tell.
If this hasn't already been fixed in a later version, and if the changes meet with whatever standards you have, feel free to adopt any part of it for future versions.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44515,attr return values handled differently than dict by tf.distribute.Strategy.run,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.3.1
- Python version: 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.2
- GPU model and memory: 1050 Ti 4GB

**Describe the current behavior**
If I run the following snippet,
```python
import attr
import tensorflow as tf

gpus = tf.config.list_physical_devices('GPU')
tf.config.set_logical_device_configuration(
    gpus[0],
    [tf.config.LogicalDeviceConfiguration(memory_limit=1024),
     tf.config.LogicalDeviceConfiguration(memory_limit=1024)])

@attr.s
class WrappedTensor(object):
    tensor = attr.ib()

@tf.function
def func_dict(x):
    def func_dict_per_replica(x):
        return x
    print('Result of Strategy.run(func_dict_per_replica):',
          tf.distribute.get_strategy().run(func_dict_per_replica, (x,)))

@tf.function
def func_attr(x):
    def func_attr_per_replica(x):
        return x
    print('Result of Strategy.run(func_attr_per_replica):',
          tf.distribute.get_strategy().run(func_attr_per_replica, (x,)))

with tf.distribute.MirroredStrategy().scope():
    data = tf.data.Dataset.from_tensors(tf.constant(0, shape=[2]))
    data = tf.distribute.get_strategy().experimental_distribute_dataset(data)
    v = next(iter(data))
    print()
    x = {'tensor': v}
    print('Input signature of func_dict:', func_dict.get_concrete_function(x).structured_input_signature)
    print()
    x = WrappedTensor(v)
    print('Input signature of func_attr:', func_attr.get_concrete_function(x).structured_input_signature)
```
I get the following output:
```
Result of Strategy.run(func_dict_per_replica): {'tensor': PerReplica:{
  0: <tf.Tensor 'x:0' shape=(1,) dtype=int32>,
  1: <tf.Tensor 'x_1:0' shape=(1,) dtype=int32>
}}
Input signature of func_dict: (({'tensor': PerReplicaSpec(TensorSpec(shape=(1,), dtype=tf.int32, name=None), TensorSpec(shape=(1,), dtype=tf.int32, name=None))},), {})

Result of Strategy.run(func_attr_per_replica): PerReplica:{
  0: WrappedTensor(tensor=<tf.Tensor 'x:0' shape=(1,) dtype=int32>),
  1: WrappedTensor(tensor=<tf.Tensor 'x_1:0' shape=(1,) dtype=int32>)
}
Input signature of func_attr: ((WrappedTensor(tensor=PerReplicaSpec(TensorSpec(shape=(1,), dtype=tf.int32, name=None), TensorSpec(shape=(1,), dtype=tf.int32, name=None))),), {})
```

**Describe the expected behavior**
As shown in the output, the input signatures of both functions are generated by replacing values in the structures with the respective `PerReplicaSpec`, so here `attr` objects are treated like a nested structure similar to a `dict`. One would then expect the result of the two functions follow a similar treatment. However, the result of `func_attr` is `WrappedTensor` objects stored inside `PerReplica` objects, as opposed to `PerReplica` objects wrapped inside `WrappedTensor` objects, so `attr` is again not treated like a nested structure here, inconsistent with the input signatures.

I personally think tensorflow should treat `attr` objects like a nested structure (that is, change the behavior of `func_attr` to match that of `func_dict`), because then you can write member functions that handle distributed computation automatically when the attributes of a `attr` object are `DistribtuedValue`s.

**Standalone code to reproduce the issue**
See the code snippet above.

**Other info / logs**
N/A"
44513,crashed at TfLiteInterpreterCreate ,"- iPhone11 pro
- Podfile
pod 'TensorFlowLiteSelectTfOps', '~> 0.0.1-nightly'
pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['Metal']

- crashed at TfLiteInterpreterCreate.
My custom model is used at StyleTranfer demo project.

guard let cInterpreter = TfLiteInterpreterCreate(model.cModel, cInterpreterOptions) else {
--> Thread 8: EXC_BAD_ACCESS (code=1, address=0x0)

Thread 8 Queue : org.tensorflow.examples.lite.style_transfer (serial)
#0	0x00000001f0bce618 in _platform_memmove ()
#1	0x00000001007d406c in ___lldb_unnamed_symbol7040$$TFL Style Transfer ()
#2	0x00000001007c82e4 in ___lldb_unnamed_symbol6806$$TFL Style Transfer ()
#3	0x00000001007d0f90 in ___lldb_unnamed_symbol6968$$TFL Style Transfer ()
#4	0x00000001007c3744 in ___lldb_unnamed_symbol6746$$TFL Style Transfer ()
#5	0x00000001007c4304 in ___lldb_unnamed_symbol6748$$TFL Style Transfer ()
#6	0x00000001007b94bc in ___lldb_unnamed_symbol6574$$TFL Style Transfer ()
#7	0x0000000100429cfc in ___lldb_unnamed_symbol129$$TFL Style Transfer ()
#8	0x0000000100429924 in ___lldb_unnamed_symbol128$$TFL Style Transfer ()
#9	0x0000000100429678 in ___lldb_unnamed_symbol127$$TFL Style Transfer ()
#10	0x00000001007b9420 in ___lldb_unnamed_symbol6573$$TFL Style Transfer ()
#11	0x000000010042cc80 in ___lldb_unnamed_symbol169$$TFL Style Transfer ()
#12	0x00000001005638fc in ___lldb_unnamed_symbol2108$$TFL Style Transfer ()
#13	0x0000000100524554 in ___lldb_unnamed_symbol1773$$TFL Style Transfer ()
#14	0x000000010052431c in TfLiteInterpreterCreate ()
#15	0x000000010040e854 in Interpreter.init(modelPath:options:delegates:) at /Users/user/git/examples-master/lite/examples/style_transfer/ios/Pods/TensorFlowLiteSwift/tensorflow/lite/experimental/swift/Sources/Interpreter.swift:98
#16	0x000000010040e144 in Interpreter.__allocating_init(modelPath:options:delegates:) ()
#17	0x00000001003fe3a0 in closure #1 in static Inpainting.newInstance(inpaintingModel:useMetalDelegate:completion:) at /Users/user/git/examples-master/lite/examples/style_transfer/ios/StyleTransfer/Inpainting.swift:97
#18	0x00000001003fee34 in thunk for @escaping @callee_guaranteed () -> () ()
#19	0x000000010096bb68 in _dispatch_call_block_and_release ()
#20	0x000000010096d5f0 in _dispatch_client_callout ()
#21	0x0000000100974fa8 in _dispatch_lane_serial_drain ()
#22	0x0000000100975cb4 in _dispatch_lane_invoke ()
#23	0x0000000100981e38 in _dispatch_workloop_worker_thread ()
#24	0x00000001f0bd4908 in _pthread_wqthread ()
"
44512,TypeError: call() missing 2 required positional arguments: 'features' and 'hidden',"**System information**
- OS Platform and Distribution :CentOS Linux release 7.7.1908
-TensorFlow version:2.3.0

I try to convert [the tensorflow offical image caption model ](https://www.tensorflow.org/tutorials/text/image_captioning?hl=en)to TFLite model.Here is the[ gist](https://colab.research.google.com/gist/DavidInWuhanChina/f83e3e11009211f3469436bbc069b18a/43753.ipynb).

I try to convert the `tf.keras.Model `'s encoder and decoder model as following:

```
encoder_converter = tf.lite.TFLiteConverter.from_keras_model(encoder)
decoder_converter = tf.lite.TFLiteConverter.from_keras_model(decoder)

encoder_model = encoder_converter.convert()
decoder_model = decoder_converter.convert()
```


but the eorror is 

```
INFO:tensorflow:Assets written to: /tmp/tmpgvx51gsa/assets

INFO:tensorflow:Assets written to: /tmp/tmpgvx51gsa/assets

---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

<ipython-input-55-9f0e9ba5bb99> in <module>()
      2 decoder_converter = tf.lite.TFLiteConverter.from_keras_model(decoder)
      3 encoder_model = encoder_converter.convert()
----> 4 decoder_model = decoder_converter.convert()

11 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    300   def wrapper(*args, **kwargs):
    301     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
--> 302       return func(*args, **kwargs)
    303 
    304   if inspect.isfunction(func) or inspect.ismethod(func):

TypeError: call() missing 2 required positional arguments: 'features' and 'hidden'

```
"
44511,TFLite memory not released,"
**Describe the current behavior**

I am running a TFLite model on a edge device, and what we have observed is that when we invoke the first time, the RAM usage went up 30-40MB which is correct and that is the right amount of RAM needed to do inference. However, the memory does not get released after that. The second and many more times I run invoke, the overall RAM usage stays the same and there is no leak, but the overall RAM usage never goes down. Is there anything I need to do to release the RAM?

"
44510,tf.device scope not working correctly,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y es
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ppc64le-linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary)
- TensorFlow version (use command below):
- Python version: python3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1.243
- GPU model and memory: V100 16GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

tf.device command does not correctly assign a GPU device to tf.keras layers on node with 4 GPUs so cannot implement model parallelism. All layers appear on device GPU:0 with the exception of some IO based on output of tf.debugging.set_log_device_placement(True)

**Describe the expected behavior**
tf.keras layers are correctly assigned to a device.

**Standalone code to reproduce the issue**

import tensorflow as tf
from tensorflow import keras

tf.debugging.set_log_device_placement(True)

print(""On GPU:1"")
inputs = keras.Input(shape=(784,))
with tf.device(""/device:GPU:1""): # Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.
   x = keras.layers.Dense(256, activation=""relu"")(inputs)
   print(x)
   assert x.device.endswith(""/GPU:1"")

Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

I have a larger test problem that will run on a 4 GPU node. If you turn off the assert statement, then using nvidia-smi you can see that all memory and computational work is happening on GPU:0 and almost none is assigned to other GPUs. Happy to supply this code if needed.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Tensor(""dense/Identity:0"", shape=(None, 256), dtype=float32)
Traceback (most recent call last):
  File ""py_test.py"", line 11, in <module>
    assert x.device.endswith(""/GPU:1"")
AssertionError
"
44509,"```tf.keras.callbacks.EarlyStopping``` doesn't set correct mode when ```monitor=""val_auc"",mode=""auto""```","**Describe the current behavior**
[https://colab.research.google.com/drive/1y-gYAIMHYokd9K-OUDXuD3PGTh4qE1xN?usp=sharing](https://colab.research.google.com/drive/1y-gYAIMHYokd9K-OUDXuD3PGTh4qE1xN?usp=sharing)
```
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_auc',
    verbose=1,
    )
```
is equivalent to
```
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_auc',
    verbose=1,
    mode=""min""
    )
```
**Describe the expected behavior**
It should be equivalent to
```
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_auc',
    verbose=1,
    mode=""max""
    )
```

"
44508,Building TensorFlow Lite library with tensorflow ops failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): source
- TensorFlow version: master
- Python version: 3.5
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: No
- GPU model and memory: No



**Describe the problem**
I want to build a TensorFlow Lite C++ shared library that supports full tensorflow ops. I'm using cross-compile, buiding on PC and the target platform is arm64. But the build process failed. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Modify tensorflow/lite/BUILD to include ""//tensorflow/lite/delegates/flex:delegate"" in the last part: deps of tflite_cc_shared_object. 
Then run: 
bazel build --config=elinux_aarch64 --config=monolithic -c opt //tensorflow/lite:libtensorflowlite.so


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
ERROR: /home/xxx/tensorflow/tensorflow/core/lib/strings/BUILD:62:1: C++ compilation of rule '//tensorflow/core/lib/strings:proto_text_util' failed (Exit 1)
In file included from external/com_google_protobuf/src/google/protobuf/stubs/port.h:49,
                 from external/com_google_protobuf/src/google/protobuf/stubs/macros.h:34,
                 from external/com_google_protobuf/src/google/protobuf/stubs/logging.h:34,
                 from external/com_google_protobuf/src/google/protobuf/arena_impl.h:40,
                 from external/com_google_protobuf/src/google/protobuf/arena.h:53,
                 from ./tensorflow/core/platform/protobuf.h:33,
                 from ./tensorflow/core/lib/strings/proto_text_util.h:22,
                 from tensorflow/core/lib/strings/proto_text_util.cc:16:
external/com_google_protobuf/src/google/protobuf/port_def.inc:74:2: error: #error PROTOBUF_DEPRECATED was previously defined
 #error PROTOBUF_DEPRECATED was previously defined
  ^~~~~
```
The full log is too long and I omit the middle part... And the last part is: 
```
Target //tensorflow/lite:libtensorflowlite.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 24.871s, Critical Path: 24.02s
INFO: 81 processes: 81 local.
FAILED: Build did NOT complete successfully
```

Could you help me analyze this? Many thanks. "
44506,Training with TPU Out of range ... end of sequence at beginning of secund epoch,"**System information**
- Have I written custom code : Yes
- Version of Tensorflow: 2.3
- OS Platform and Distribution : Kaggle TPU
- TPU model  :v3-8
- dataset: [Raven_center_single](https://www.kaggle.com/datendnker/raven-center-single)

**Describe the current behavior**
My model is training and evaluating the first epoch as expected. At the beginning of second epoch Error raises: OutOfRangeError 

**Describe the expected behavior**
It should train Without running out of data

Here you can find the [Kaggle Notebook](https://www.kaggle.com/datendnker/notebook045427c7cb)


**Complete error log** 
---------------------------------------------------------------------------
OutOfRangeError                           Traceback (most recent call last)
<ipython-input-11-c2c56d1c82d6> in <module>
      5           validation_data=val_dataset,
      6           validation_steps=(config.entrys * config.test_size)/ config.batch_size,
----> 7           callbacks=[WandbCallback()])
      8 
      9 model.save_weights('./RAVEN-center-single_large_model.h5', overwrite=True)

/opt/conda/lib/python3.7/site-packages/wandb/integration/keras/keras.py in new_v2(*args, **kwargs)
    118             for cbk in cbks:
    119                 set_wandb_attrs(cbk, val_data)
--> 120         return old_v2(*args, **kwargs)
    121 
    122     training_arrays.orig_fit_loop = old_arrays

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--> 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1135           epoch_logs.update(val_logs)
   1136 
-> 1137         callbacks.on_epoch_end(epoch, epoch_logs)
   1138         training_logs = epoch_logs
   1139         if self.stop_training:

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in on_epoch_end(self, epoch, logs)
    413       else:
    414         if numpy_logs is None:  # Only convert once.
--> 415           numpy_logs = tf_utils.to_numpy_or_python_type(logs)
    416         callback.on_epoch_end(epoch, numpy_logs)
    417 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py in to_numpy_or_python_type(tensors)
    535     return t  # Don't turn ragged or sparse tensors to NumPy.
    536 
--> 537   return nest.map_structure(_to_single_numpy_or_python_type, tensors)
    538 
    539 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)
    633 
    634   return pack_sequence_as(
--> 635       structure[0], [func(*x) for x in entries],
    636       expand_composites=expand_composites)
    637 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in <listcomp>(.0)
    633 
    634   return pack_sequence_as(
--> 635       structure[0], [func(*x) for x in entries],
    636       expand_composites=expand_composites)
    637 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py in _to_single_numpy_or_python_type(t)
    531   def _to_single_numpy_or_python_type(t):
    532     if isinstance(t, ops.Tensor):
--> 533       x = t.numpy()
    534       return x.item() if np.ndim(x) == 0 else x
    535     return t  # Don't turn ragged or sparse tensors to NumPy.

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in numpy(self)
   1061     """"""
   1062     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.
-> 1063     maybe_arr = self._numpy()  # pylint: disable=protected-access
   1064     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr
   1065 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _numpy(self)
   1029       return self._numpy_internal()
   1030     except core._NotOkStatusException as e:  # pylint: disable=protected-access
-> 1031       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
   1032 
   1033   @property

/opt/conda/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

OutOfRangeError: 9 root error(s) found.
  (0) Out of range: {{function_node __inference_train_function_5573}} End of sequence
	 [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]
	 [[strided_slice_18/_202]]
  (1) Out of range: {{function_node __inference_train_function_5573}} End of sequence
	 [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]
	 [[tpu_compile_succeeded_assert/_16956405043851590776/_5/_185]]
  (2) Out of range: {{function_node __inference_train_function_5573}} End of sequence
	 [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]
  (3) Out of range: {{function_node __inference_train_function_5573}} End of sequence
	 [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]
	 [[Const_4/_212]]
  (4) Out of range: {{function_node __inference_train_function_5573}} End of sequence
	 [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]
	 [[cond_14/switch_pred/_140/_90]]
  (5) Out of range: {{function_node __inference_train_function_5573}} End of sequence
	 [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]
	 [[Pad_11/paddings/_154]]
  (6) Out of range: {{function_node __inference_train_function_5573}} End of sequence
	 [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]
	 [[cond_15/switch_pred/_151/_92]]
  (7) Out of range: {{function_node __inference_train_function_5573}} End of sequence
	 [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]
	 [[strided_slice_15/_192]]
  (8) Out of range: {{function_node __inference_train_function_5573}} End of sequence
	 [[{{node cond_8/else/_73/cond_8/IteratorGetNext}}]]
	 [[Shape_22/_124]]
0 successful operations.
0 derived errors ignored.
"
44505,"tensorflow.keras.callback.TensorflowCallback turns off recording for tf.summary, and stops other callbacks from recording summaries","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary (colab)
- TensorFlow version (use command below): 2.3
- Python version: 3+
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: 
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
### Steps to reproduce
Any custom keras callback that writes using tf.summary at batch_start or batch_end methods will have their `tf.summary` writing methods such as `tf.summary.scalar` not write anything to the protobuf.
### Root cause
`tensorflow.keras.callback.TensorflowCallback` modifies the private summary field of tensorflow.python.ops.summary_ops_v2:

[summary_state = summary_ops_v2._summary_state  # pylint: disable=protected-access](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/callbacks.py#L2059)


**Describe the expected behavior**
TensorflowCallback should not impact the recording of other callbacks. Instead of updating the global summary_state, it should locally guard the writing of summaries.
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
https://colab.research.google.com/drive/1UPm1m3kNZCFaJZHHYT5rbx1Xcx0VZ7gR

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Custom callback which can show the issue:
```python
class CustomCallback(tf.keras.callbacks.Callback):
    def __init__(self):
      self.step = 1
    
    def on_train_batch_end(self, batch, logs=None):
        if tf.summary.should_record_summaries():
          print(f""Warning!!!!!!!!!!!!: can only record summaries for step: {self.step}"")
        self.step += 1

tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq=100)
custom_callback = CustomCallback()
model.fit(x=x_train, 
            y=y_train, 
            epochs=epochs, 
            validation_data=(x_test, y_test), 
            callbacks=[tensorboard_callback, custom_callback])

```
The above code segment outputs:

```bash
  95/1875 [>.............................] - ETA: 7s - loss: 0.9137 - accuracy: 0.6734
Warning!!!!!!!!!!!!: can record summaries for step: 100
 191/1875 [==>...........................] - ETA: 7s - loss: 0.7903 - accuracy: 0.7202
Warning!!!!!!!!!!!!: can record summaries for step: 200
 292/1875 [===>..........................] - ETA: 6s - loss: 0.7092 - accuracy: 0.7464
Warning!!!!!!!!!!!!: can record summaries for step: 300
 396/1875 [=====>........................] - ETA: 6s - loss: 0.6668 - accuracy: 0.7633
Warning!!!!!!!!!!!!: can record summaries for step: 400
 493/1875 [======>.......................] - ETA: 5s - loss: 0.6315 - accuracy: 0.7762
Warning!!!!!!!!!!!!: can record summaries for step: 500
 595/1875 [========>.....................] - ETA: 5s - loss: 0.6107 - accuracy: 0.7834
Warning!!!!!!!!!!!!: can record summaries for step: 600
 692/1875 [==========>...................] - ETA: 4s - loss: 0.5928 - accuracy: 0.7891
Warning!!!!!!!!!!!!: can record summaries for step: 700
 792/1875 [===========>..................] - ETA: 4s - loss: 0.5800 - accuracy: 0.7931
Warning!!!!!!!!!!!!: can record summaries for step: 800
 892/1875 [=============>................] - ETA: 4s - loss: 0.5656 - accuracy: 0.7972
Warning!!!!!!!!!!!!: can record summaries for step: 900
 995/1875 [==============>...............] - ETA: 3s - loss: 0.5549 - accuracy: 0.8015
Warning!!!!!!!!!!!!: can record summaries for step: 1000
1092/1875 [================>.............] - ETA: 3s - loss: 0.5439 - accuracy: 0.8059
Warning!!!!!!!!!!!!: can record summaries for step: 1100
1194/1875 [==================>...........] - ETA: 2s - loss: 0.5351 - accuracy: 0.8083
Warning!!!!!!!!!!!!: can record summaries for step: 1200
1293/1875 [===================>..........] - ETA: 2s - loss: 0.5289 - accuracy: 0.8101
Warning!!!!!!!!!!!!: can record summaries for step: 1300
1388/1875 [=====================>........] - ETA: 2s - loss: 0.5231 - accuracy: 0.8121
Warning!!!!!!!!!!!!: can record summaries for step: 1400
1490/1875 [======================>.......] - ETA: 1s - loss: 0.5148 - accuracy: 0.8151
Warning!!!!!!!!!!!!: can record summaries for step: 1500
1591/1875 [========================>.....] - ETA: 1s - loss: 0.5102 - accuracy: 0.8163
Warning!!!!!!!!!!!!: can record summaries for step: 1600
1686/1875 [=========================>....] - ETA: 0s - loss: 0.5060 - accuracy: 0.8174
Warning!!!!!!!!!!!!: can record summaries for step: 1700
1796/1875 [===========================>..] - ETA: 0s - loss: 0.5002 - accuracy: 0.8193
Warning!!!!!!!!!!!!: can record summaries for step: 1800
```"
44503,IOS: TensorFlow Lite Error: Didn't find op for builtin opcode 'RESIZE_BILINEAR' version '3',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No - It's the TensorFlow Lite IOS Object Detection example...I just tried to use my own converted Darknet Tiny Yolo v4 to TFLite
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  NA
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: IOS 13.7
- TensorFlow installed from (source or binary): I ran the POD install command on my mac
- TensorFlow version (use command below): unknown...whatever POD install installed 
- Python version: NA
- Bazel version (if compiling from source) NA:
- GCC/Compiler version (if compiling from source) NA:
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
From XCODE -> Play the solution to deploy to my IOS IPhone device.  Fails on the following line:
 interpreter = try Interpretor(modelPath: modelPath, options: options)  

**Describe the expected behavior**
I expected it to run the normal TFLite IOS object detection example program on my phone but with my model

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44502,Jacobian fails on gradient of tf.function with if-elif-else or nested tf.cond,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.8.5
- CUDA/cuDNN version: executing on CPU
- GPU model and memory: executing on CPU

**Describe the current behavior**

Computing the `GradientTape.jacobian` of the gradient (i.e. the Hessian) of a `tf.function` with either an if-elif-else construct or nested `tf.cond` results in a crash:

```
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Trying to add unsupported dtype 10
         [[node gradients/AddN_2 (defined at debug.py:44) ]] [Op:__inference___backward___backward_f_bad_373_1158_1489]
```
See full trace attached as [trace_without_pfor.txt](https://github.com/tensorflow/tensorflow/files/5471053/trace_without_pfor.txt).

The computation works fine when disabling  `tf.functions`.

First-order derivatives (gradient or jacobian) work fine too.

**Describe the expected behavior**

Hessian evaluates to `tf.Tensor([[2.]], shape=(1, 1), dtype=float32)`.

**Standalone code to reproduce the issue**

The following works if `use_function = False`, but fails for both `f_bad` and `f_bad_cond` when using `use_function = True`. Both `f_good` and `f_good_const` always work fine.

```
import tensorflow as tf

use_function = True
use_pfor = False

tf.config.run_functions_eagerly(not use_function)

@tf.function
def f_bad(x):
    if x < -1.:
        return tf.pow(x, 2)
    elif x <= 1.:
        return tf.pow(x, 2)
    else:
        return tf.pow(x, 2)

@tf.function
def f_bad_cond(x):
    return tf.cond(x < -1.,
                   lambda: tf.pow(x, 2),
                   lambda: tf.cond(x <= 1.,
                                   lambda: tf.pow(x, 2),
                                   lambda: tf.pow(x, 2)))

@tf.function
def f_good(x):
    if x < -1.:
        return tf.pow(x, 2)
    else:
        return tf.pow(x, 2)

@tf.function
def f_good_cond(x):
    return tf.cond(x < -1.,
                   lambda: tf.pow(x, 2),
                   lambda: tf.pow(x, 2))

f = f_bad

x = tf.Variable([0.])
with tf.GradientTape(persistent=not use_pfor) as t2:
    with tf.GradientTape() as t1:
        y = f(x)
    g_y = t1.gradient(y, x)
hess = t2.jacobian(g_y, x, experimental_use_pfor=use_pfor)
print(hess)
```

Note that using `use_pfor = True` with `use_function = True` crashes for all four functions (see the attached [trace_with_pfor.txt](https://github.com/tensorflow/tensorflow/files/5471052/trace_with_pfor.txt)), but that is probably an entirely different issue...
"
44501,Image artifact with tf.keras data augmentation,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): RUN pip install --upgrade tensorflow tensorflow-gpu
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: Python 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: V10.1.243
- GPU model and memory: GeForce GTX 108 / 261MiB / 11178MiB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

When using the following code: 
```
data_augmentation = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),
  tf.keras.layers.experimental.preprocessing.RandomRotation(90),
])
```
I see black lines around the edges of the augmented images. I do not see this when I use plain Keras:

```
datagen = ImageDataGenerator(
    rotation_range=model_params['rotation_range'],  # randomly rotate images in the range (degrees, 0 to 180)  
    horizontal_flip=model_params['horizontal_flip'],  # randomly flip images
    vertical_flip=model_params['vertical_flip'],

)  
```

Also, when I upgrade to tf-nightly it goes away. 

**Describe the expected behavior**
I would expect the black lines to not be there. 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/drive/1nRpmyMrXkV43W82nI2fP2F05JndeGdg0?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

I guess my question is whether or not these black lines are truly being fed into the network when using data augmentation."
44500,FAILED: Build did NOT complete successfully,"Hello ,


- OS : pc windows 10
- CPU : i7-5820K
- RAM: 16G
- TensorFlow installed from (source or binary): source git master
- TensorFlow version: git Master
- Python version: 3.8.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): [bazel release 3.7.0]
- GCC/Compiler version (if compiling from source): Visual Studio Build Tools 2019 and Visual Studio Community 2019
- CUDA/cuDNN version: cuda_10.1.105_418.96_win10/ cudnn-10.1-windows10-x64-v7.6.5.32
- GPU model and memory: Nvidia getforce GTX1070Ti / 8192 MB



in build verbose :
`
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/9bb9b737c5573cf3850230bc4db8dac7be0e1e85.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
`
and


`ERROR: C:/tf/tensorflow/tensorflow/core/kernels/BUILD:1157:18: C++ compilation of rule '//tensorflow/core/kernels:reverse_op_gpu' failed (Exit 2): python.exe failed: error executing command`

[Full log.txt](https://github.com/tensorflow/tensorflow/files/5470759/Full.log.txt)


**Provide the exact sequence of commands / steps that you executed before running into the problem**

i search to build tensorflow-gpu with support AVX2
build command in MSYS :

 bazel build --define=no_tensorflow_py_deps=true --local_ram_resources=12288 //tensorflow/tools/pip_package:build_pip_package

"
44497,the tf.repeat function can not be converted to tflite?,"I tested the newest tf-nightly==2.5.0-dev20201029,
the tf.repeat function would cause the errors below:

tensorflow.lite.python.convert.ConverterError: ...../lib/python3.6/site-packages/tensorflow/python/saved_model/load.py:890:0: error: 'tf.Reshape' op requires 'shape' to have at most one dynamic dimension, but got multiple dynamic dimensions at indices 1 and 2

........

Exception ignored in: <bound method Buckets.del of <tensorflow.python.eager.monitoring.ExponentialBuckets object at 0x7f6e852aa4c8>>
Traceback (most recent call last):
File ""......./lib/python3.6/site-packages/tensorflow/python/eager/monitoring.py"", line 407, in del
AttributeError: 'NoneType' object has no attribute 'TFE_MonitoringDeleteBuckets'


The code has been shown in https://github.com/tensorflow/tensorflow/issues/40504

At the same time, according to the colab(https://colab.research.google.com/drive/1HudLLpT9CQdh2k04c06bHUwLubhGTWxA?usp=sharing)
when using tf-nightly==2.4.0-dev20200630, tf.repeat can be converted. "
44495,Screen freezes when jupyter starts loading keras and tensorflow,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
44494,"TensorBoard say Launching TensorBoard..., but nothing show","![9DAroMg0xc](https://user-images.githubusercontent.com/4510984/97798399-a116db80-1c60-11eb-8f2f-05a6bb148e39.gif)

![image](https://user-images.githubusercontent.com/4510984/97798449-269a8b80-1c61-11eb-95d1-b4f872a6d570.png)
"
44493,"Error while setting serial port parameters: 9,600 N 8 1: when run the ""tensorflow/tensorflow/lite/micro/examples/person_detection/"" example on Arduino Nano 33 BLE sense","### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: tensorflow/tensorflow/lite/micro/examples/person_detection/
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: 
-   **TensorFlow version (use command below)**: 1.14
-   **Python version**:3.6
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:


### Describe the problem
When I load the person detection example code in Arduino IDE, it works perfectly. 
But, for my custom dog detection model, I re-implement all steps as defined in [this ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection/training_a_model.md)article. This custom dog detection model trained successfully and also able to generate .cc model file. But, when I replace person_detect_model_data.cpp file with my dog_model, it throw the below error message when I click on serial monitor.

Can someone help me to solve my issue?

Thanks a lot,
Bhavika

### Source code / logs
Sketch uses 428728 bytes (43%) of program storage space. Maximum is 983040 bytes.
Global variables use 127600 bytes (48%) of dynamic memory, leaving 134544 bytes for local variables. Maximum is 262144 bytes.
Device       : nRF52840-QIAA
Version      : Arduino Bootloader (SAM-BA extended) 2.0 [Arduino:IKXYZ]
Address      : 0x0
Pages        : 256
Page Size    : 4096 bytes
Total Size   : 1024KB
Planes       : 1
Lock Regions : 0
Locked       : none
Security     : false
Erase flash

Done in 0.000 seconds
Write 428736 bytes to flash (105 pages)
[==============================] 100% (105/105 pages)
Done in 17.794 seconds
Error while setting serial port parameters: 9,600 N 8 1

"
44491,deterministic `Dataset.map`s aren't,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution: default colab

**Describe the current behavior**
`tf.data.Dataset.map` has a `determinstic` argument which is documented as allowing performance to be traded for determinism. Behaviour is not deterministic when `num_parallel_calls != 1` if `map_func` uses `tf.random` ops or methods from `tf.random.Generator` instances, even when `deterministic=True`.

**Describe the expected behavior**
Datasets mapped with `deterministic=True` should be deterministic, even with `num_parallel_calls != 1`. If this is not possible, documentation should not say that it is.

**Standalone code to reproduce the issue**
See [colab](https://colab.research.google.com/drive/1u9N87y5BsFFGXvTO23pIjRTjheQekPEJ?usp=sharing) (code reproduced below).

```python
import tensorflow as tf
import numpy as np

import time
import numpy as np
import tensorflow as tf

seed = 0
tf.random.set_seed(seed)


def map_fn(x):
  return tf.random.uniform((100,),)


ds = tf.data.Dataset.range(10).map(
    map_fn, num_parallel_calls=2, deterministic=True)

tf.random.set_seed(seed)
vals0 = [el.numpy() for el in ds]
tf.random.set_seed(seed)
vals1 = [el.numpy() for el in ds]

np.testing.assert_equal(vals0, vals1)
print(""Passed"")
```

**Other info / logs** Include any logs or source code that would be helpful to
```txt
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-3-9c92a75ab4eb> in <module>()
     18 vals1 = [el.numpy() for el in ds]
     19 
---> 20 np.testing.assert_equal(vals0, vals1)
     21 print(""Passed"")
     22 

3 frames
/usr/local/lib/python3.6/dist-packages/numpy/testing/_private/utils.py in assert_array_compare(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)
    844                                 verbose=verbose, header=header,
    845                                 names=('x', 'y'), precision=precision)
--> 846             raise AssertionError(msg)
    847     except ValueError:
    848         import traceback

AssertionError: 
Arrays are not equal
item=3

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 0.8390552
Max relative difference: 70.61441
 x: array([0.839809, 0.390264, 0.704193, 0.916514, 0.008297, 0.778485,
       0.316561, 0.052142, 0.876872, 0.176245, 0.275351, 0.204759,
       0.999374, 0.40232 , 0.895112, 0.604971, 0.688866, 0.333363,...
 y: array([0.595369, 0.867852, 0.033959, 0.962069, 0.560696, 0.772698,
       0.060957, 0.602406, 0.495115, 0.442416, 0.500883, 0.282592,
       0.324962, 0.593064, 0.731267, 0.349757, 0.635136, 0.251549,...
```"
44490,tf-nightly2.4  run error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu18.04
- TensorFlow version (use command below):tf-nightly2.4
- Python version:3.7.4
- CUDA/cuDNN version:CUDA11.1
- GPU model and memory:RTX3090


**Describe the current behavior**
In my current environment, I use tf-nightly2.4 to run the bert model. After I run an epoch, I reported an error.
  value error: layer model expects 3 input(s), but it received 4 input tensors.
It seems that the output layer of the model is also regarded as the input layer.
But there is no problem with my code, it can run completely on CUDA10.1, so what is going on?

"
44487,"TPUStrategy.run fails on non-primary thread with ""No OpKernel was registered"", ""TPUReplicatedInput""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, included here
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): (a) Google Collab, (b) Debian 10 (Buster), Google Cloud VM TF2.3.1 image
- TensorFlow installed from (source or binary): (a) Google Collab, (b) Google Cloud VM TF2.3.1 image
- TensorFlow version (use command below): (a) Google Collab, (b) v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: (a) Google Collab, (b) 3.7.3 (Google Cloud VM TF2.3.1 image)
- GPU model and memory: (a) Google Collab TPU, (b) Google Cloud TPU, us-central1-f, v2-8, 2.3.1

**Describe the current behavior**

Running TPUStrategy.run on a secondary thread crashes

**Describe the expected behavior**

Running TPUStrategy.run on a secondary thread behaves the same as on the primary thread, succeeding.

This is breaking for applications like inference from AlphaZero-style MCTS self-play originating from C++, using TPUStrategy.experimental_distribute_dataset, where either the primary C++ thread is required to perform other housekeeping, or where multiple prediction threads help to keep the inference pipeline more full. However, this minimal Python-only example gives the same error. The error differs slightly when experimental_distribute_dataset is used, but the final ""Op:__inference_tpu_function_177"" is common.

**Standalone code to reproduce the issue**

This snippet runs in a Google Collab notebook:

```
import tensorflow as tf
import threading

resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.TPUStrategy(resolver)

@tf.function
def double(x):
  return x * 2.0

def test():
  input = tf.range(5, dtype=tf.float32)
  strategy.run(double, args=(input,))

test()
print(""TPUStrategy.run works on primary thread"")

thread = threading.Thread(target=test)
thread.start()
thread.join()
```

**Other info / logs**

Exception in thread Thread-5:
Traceback (most recent call last):
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""<ipython-input-2-c35bbe6196e2>"", line 15, in test
    strategy.run(double, args=(input,))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py"", line 279, in run
    return self.extended.tpu_run(fn, args, kwargs, options)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py"", line 1095, in tpu_run
    return func(args, kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py"", line 814, in _call
    results = self._stateful_fn(*args, **kwds)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 550, in call
    ctx=ctx)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'TPUReplicatedInput' used by {{node input0}} with these attrs: [T=DT_INT32, index=0, is_mirrored_variable=false, N=8, is_packed=false]
Registered devices: [CPU, TPU, TPU_SYSTEM, XLA_CPU]
Registered kernels:
  <no registered kernels>

	 [[input0]] [Op:__inference_tpu_function_177]
"
44485,Support Python 3.9,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.1
- Are you willing to contribute it (Yes/No): No (willing to do trivial patches)



**Describe the feature and the current behavior/state.**
TensorFlow should be supported on Python 3.9.

**Will this change the current api? How?** Significant changes will likely need to be made.

**Who will benefit with this feature?**
Anyone on Python 3.9

**Any Other info.**
"
44483,Error after installing tensorflow,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: installed via pip3
-   **TensorFlow version (use command below)**: tensorflow-2.3.1
-   **Python version**: Python3.5
-   **Bazel version (if compiling from source)**: -
-   **GCC/Compiler version (if compiling from source)**: -
-   **CUDA/cuDNN version**: -
-   **GPU model and memory**: -
-   **Exact command to reproduce**: import tensorflow as tf

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh


### Describe the problem

Hello,
I have just installed tensorflow on my laptop which uses Ubuntu 16.04 and i installed it via pip3.
I just wanted to test if its working properly but even just the import resulted in following error message:

2020-10-31 18:31:40.413625: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/kinetic/lib:/opt/ros/kinetic/lib/x86_64-linux-gnu
2020-10-31 18:31:40.413651: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

Have i done something wrong when i installed it?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
44482,tf.logging documentation leads to 404,"## URL(s) with the issue:
As far as I can tell all articles describing tf.logging namespace. Such as:
[https://www.tensorflow.org/api_docs/python/tf/logging/TaskLevelStatusMessage](https://www.tensorflow.org/api_docs/python/tf/logging/TaskLevelStatusMessage)
or
[https://www.tensorflow.org/api_docs/python/tf/logging](https://www.tensorflow.org/api_docs/python/tf/logging)

## Description of issue (what needs changing):
Despite showing up in search results these articles are not accessible. 

### Clear description
Search results for `logging` and related queries contain links to nonexistent articles. 
Leading to confusion and potentially avoidable issues. 

### Submit a pull request?

Not at this time. 
"
44481,TFlite Custom trained model Issue (IndexError: invalid index to scalar variable),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and Raspbian
- TensorFlow installed from (source or binary): TF
- TensorFlow version (or github SHA if from source): 2.3.1


**Command used to run the converter or code if youre using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
python /content/models/research/object_detection/exporter_main_v2.py \
    --trained_checkpoint_dir training \
    --output_directory inference_graph \
    --pipeline_config_path training/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.config
```

**The output from the converter invocation**

```
Output succeeded. (not sure what to put here)
```

**Also, please include a link to the saved model or GraphDef**

```
Model: https://drive.google.com/drive/folders/17iEVFzQjqhI6zP4zUJP0uTAnBy2TnWPR?usp=sharing
TFLiteModel :https://drive.google.com/drive/folders/1BCcWG-89OHts-510sev1VJepbDQkt9s0?usp=sharing
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:

I trained a SSD_MobilenetV2_FPN_680x680 to detect only black bears successfully and converted to tflite by following this guide: https://github.com/TannerGilbert/Tensorflow-Object-Detection-API-Train-Model.

I then followed EdjeElectronics' guide on how to run tflite on Raspberry Pi: https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Raspberry_Pi_Guide.md.

But, when I try to run TFLite_detection_webcam.py It gives an error:

pi@raspberrypi:~ $ source tflite1-env/bin/activate
bash: tflite1-env/bin/activate: No such file or directory
pi@raspberrypi:~ $ cd tflite1
pi@raspberrypi:~/tflite1 $ source tflite1-env/bin/activate
(tflite1-env) pi@raspberrypi:~/tflite1 $ python3 TFLite_detection_webcam.py --modeldir=Bear_Detector_TFLite_model
Traceback (most recent call last):
  File ""TFLite_detection_webcam.py"", line 185, in <module>
    boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects
IndexError: invalid index to scalar variable.



I run his sample tflite model and it works.
I'm guessing my the SSD_MobilenetV2_fpn model isnt compatible for Raspberry PI?

Im sorry if this post is not in the correct place or if I'm missing key info for help. I'm pretty new to all this stuff.

**Any other info / logs**

**TFLite_detection_webcam.py**
https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_webcam.py

"
44478,"I can't use tensorflow very well by using RTX 3090,When will you updata your new version to support the new RTX3090?","I can't use tensorflow very well by using RTX 3090,When will you updata your version to support the new RTX3090?
![666](https://user-images.githubusercontent.com/73739019/97775960-a5f26700-1b5c-11eb-8cdd-f54eae8ee9d4.png)

"
44476,tensorflow.contrib.image import transform as H_transform,where is tensorflow.contrib.image import transform in tensorflow2.0?
44473,How can I clear GPU memory in tensorflow 2.2?,
44472,ByteBuffer is not a valid flatbuffer model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): *Ubuntu 18.04.5 LTS [Google Colab]*
- TensorFlow installed from (source or binary): *nightly*
- TensorFlow version (or github SHA if from source): *2.5.0-dev20201029*


**Command used to run the converter or code if youre using the Python API**
I am trying to covert custom efficientdet d2 object detection model that I trained for Android.
I successfully trained it and also was able to convert it to tflite by using various github issue threads.
I followed this [colab file](https://colab.research.google.com/gist/ravikyram/c2832e117a14c172c75146275381a8fc/untitled282.ipynb#scrollTo=U8oeT58f3qmr) to convert and was able to convert also used `tf_convert` and was able to convert

```bash
# did ran this after running export_tflite_graph_tf2.py
tflite_convert --saved_model_dir /content/model_nightly/saved_model --output_file /content/tflite-conversion.tflite
```
and also
```python
import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_saved_model('/content/model_nightly/saved_model/',signature_keys=['serving_default'])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.experimental_new_converter = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]

tflite_model = converter.convert()

with tf.io.gfile.GFile('/content/model.tflite', 'wb') as f:
  f.write(tflite_model)

```

**The output from the converter invocation**
**from command 1**
```
W1030 22:40:38.686931 139654416934784 function_deserialization.py:416] Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.
W1030 22:40:38.687373 139654416934784 function_deserialization.py:416] Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.
W1030 22:40:38.687821 139654416934784 function_deserialization.py:416] Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.
2020-10-30 22:40:56.738889: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.
2020-10-30 22:40:56.738961: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.
2020-10-30 22:40:56.738973: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:325] Ignored change_concat_input_ranges.
2020-10-30 22:40:56.740050: I tensorflow/cc/saved_model/reader.cc:32] Reading SavedModel from: /content/model_graph_100K_nightly/saved_model
2020-10-30 22:40:57.034144: I tensorflow/cc/saved_model/reader.cc:55] Reading meta graph with tags { serve }
2020-10-30 22:40:57.034227: I tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: /content/model_graph_100K_nightly/saved_model
2020-10-30 22:40:57.034311: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-10-30 22:40:57.901184: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)
2020-10-30 22:40:58.067181: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.
2020-10-30 22:40:58.215865: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300000000 Hz
2020-10-30 22:41:00.181931: I tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: /content/model_graph_100K_nightly/saved_model
2020-10-30 22:41:01.017615: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 4277564 microseconds.
2020-10-30 22:41:04.164837: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:194] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2020-10-30 22:41:06.474280: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
Exception ignored in: <bound method Buckets.__del__ of <tensorflow.python.eager.monitoring.ExponentialBuckets object at 0x7f037e83e798>>
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/monitoring.py"", line 407, in __del__
AttributeError: 'NoneType' object has no attribute 'TFE_MonitoringDeleteBuckets'
```

**from command 2**
```
WARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.
```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
- While using it in the android application from the repository [here](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/README.md), I get the following error.
```
E/tensorflow: CameraActivity: Exception!
    java.lang.RuntimeException: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model
        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:153)
        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:99)
        at org.tensorflow.lite.examples.detection.CameraActivity.onPreviewFrame(CameraActivity.java:200)
        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1221)
        at android.os.Handler.dispatchMessage(Handler.java:107)
        at android.os.Looper.loop(Looper.java:214)
        at android.app.ActivityThread.main(ActivityThread.java:7356)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:492)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:930)
     Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model
        at org.tensorflow.lite.NativeInterpreterWrapper.createModelWithBuffer(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:62)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:277)
        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:148)
        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:99)
        at org.tensorflow.lite.examples.detection.CameraActivity.onPreviewFrame(CameraActivity.java:200)
        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1221)
        at android.os.Handler.dispatchMessage(Handler.java:107)
        at android.os.Looper.loop(Looper.java:214)
        at android.app.ActivityThread.main(ActivityThread.java:7356)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:492)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:930)
```
I am trying to do this for my college project."
44470,per_image_standardization Breaks Keras Model Loading,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu: 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

Here is a small example script that demonstrates the issue:

```python
import tensorflow as tf
from tensorflow.keras import layers


inputs = tf.keras.Input(shape=(100, 100, 3))
normalized = layers.Lambda(tf.image.per_image_standardization)(inputs)
dense1 = layers.Dense(100, activation=""relu"")(normalized)
dense2 = layers.Dense(10, activation=""softmax"")(dense1)
model = tf.keras.Model(inputs=inputs, outputs=dense2)

model.save(""my_model.hd5"", save_format=""h5"")

new_model = tf.keras.models.load_model(""my_model.hd5"")  # <-- Error here
```

To my knowledge, this should work. However, when I run it, I get an error on the last line:

```
Traceback (most recent call last):
  File ""/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
TypeError: 'str' object is not callable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""min_loading_bug_example.py"", line 13, in <module>
    new_model = tf.keras.models.load_model(""my_model.hd5"")
  File ""/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py"", line 182, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File ""/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 177, in load_model_from_hdf5
    model = model_config_lib.model_from_config(model_config,
  File ""/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/model_config.py"", line 55, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/layers/serialization.py"", line 171, in deserialize
    return generic_utils.deserialize_keras_object(
  File ""/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py"", line 354, in deserialize_keras_object
    return cls.from_config(
  File ""/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 616, in from_config
    input_tensors, output_tensors, created_layers = reconstruct_from_config(
  File ""/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 1214, in reconstruct_from_config
    process_node(layer, node_data)
  File ""/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py"", line 1162, in process_node
    output_tensors = layer(input_tensors, **kwargs)
  File ""/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 925, in __call__
    return self._functional_construction_call(inputs, args, kwargs,
  File ""/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1117, in _functional_construction_call
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py"", line 903, in call
    result = self.function(inputs, **kwargs)
  File ""/home/daniel/git/cotton_counter/.venv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 205, in wrapper
    result = dispatch(wrapper, args, kwargs)
TypeError: 'module' object is not callable
```

**Describe the expected behavior**

I expect this to load the model without issue.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Poking around in the debugger, the issue seems to be with [this code](https://github.com/tensorflow/tensorflow/blob/5681c179eff80bce00e526303950b67b23cad14c/tensorflow/python/util/dispatch.py#L205) in `python/util/dispatch.py`. Specifically, it looks like the `dispatch` variable is being set to the `dispatch` module instead of the `dispatch` function inside that module. I'm not an expert on the idiosyncrasies of Tensorflow or Python closures, but it seems likely that something weird is going on with scoping here. For some reason, it seems like it's using the global definition of `dispatch` from `image_ops_impl` (which would be active when the `add_dispatch_support` decorator is applied) instead of the `dispatch` function.
"
44468,Can not enable XNNPack android as there is no setUseXNNPACK option. ,"System information

OS Platform and Distribution: Android v9.0
Mobile device: armv7 and v8

TensorFlow version: 2.3.0 or nightly

Installed using virtualenv?
    def tfl_version = ""0.0.0-nightly"";
    implementation (""org.tensorflow:tensorflow-lite:${tfl_version}"") { changing = true }
    implementation (""org.tensorflow:tensorflow-lite-gpu:${tfl_version}"") { changing = true }

Describe the problem

I am using tflite on android using below repos. I have tried both version 2.3.0 as well as nightly builds. According to https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html I can enable XNNPack by setUseXNNPACK. But it can not find the symbol/option.
"
44467,h5py==3.0.0 causes issues with keras model loads in tensorflow 2.1.0,"h5py released version 3.0.0 today and it causes this code to fail:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/hdf5_format.py#L182
with error:

```
File ""/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py"", line 146, in load_model
return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
File ""/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 166, in load_model_from_hdf5
model_config = json.loads(model_config.decode('utf-8'))
AttributeError: 'str' object has no attribute 'decode'
```

It looks like in version 2.1.0 the `h5py` version is not pinned (it is pinned in master), which is causing the issue. "
44465,"""training"" argument change after model is defined","**System information**
- TensorFlow version (you are using): 2.3.0

Say I define two models.

model1

```
input_shape = (224, 224, 3)
inputs = keras.Input(shape=input_shape)
x = tf.keras.layers.experimental.preprocessing.RandomRotation(0.5)(inputs, training = True)
x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255.)(x)
x = tf.keras.layers.Convolution2D(32,(3,3))(x)
x = tf.keras.layers.MaxPool2D((2,2))(x)
x = tf.keras.layers.Flatten()(x)
out = tf.keras.layers.Dense(2,activation = 'softmax')(x)
model1 = keras.Model(inputs, out)
```
model2
```
input_shape = (224, 224, 3)
inputs = keras.Input(shape=input_shape)
x = tf.keras.layers.experimental.preprocessing.RandomRotation(0.5)(inputs, training = False)
x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255.)(x)
x = tf.keras.layers.Convolution2D(32,(3,3))(x)
x = tf.keras.layers.MaxPool2D((2,2))(x)
x = tf.keras.layers.Flatten()(x)
out = tf.keras.layers.Dense(2,activation = 'softmax')(x)
model2= keras.Model(inputs, out)
```

Where the only difference is that in model2 the preprocessing augmentation layer has ""training"" set to False. There have been many instances where I simply want to change this parameter to False/True, after the model was defined, and I can find no way to do this besides creating a new model. First time I encountered this issue was in fine tuning, but more recently when trying to generate gradcam explanations. I imagine this has countless other occurrences, and might also happen with layers like BatchNorm.

Is there already a way to switch the training state of these types of layers? (Note I don't mean the layer.trainable attribute).
If not, could this be implemented? It would be very useful."
44463,Add WinRT (UWP) as a supported platform,"Feature request: Add WinRT as a supported target platform for the native library. This would allow using TF on Microsoft Store apps and would benefit projects like https://github.com/SciSharp/TensorFlow.NET.

I've performed a fair amount of analysis work in the scope of the vcpkg project (see https://github.com/microsoft/vcpkg/issues/14252 and https://github.com/microsoft/vcpkg/pull/13028), so this looks realistically feasible.

I realize that this feature has been requested already in #16514 long ago, but unfortunately it was closed without further explanation."
44462,GradCAM and nested models,"- TensorFlow version: 2.3.0
- Python version: 3.7.6

I'm trying to reproduce the Keras GradCAM example (https://keras.io/examples/vision/grad_cam/) using a custom model.
My custom model is relatively simple and inspired by the TF/Keras fine tuning tutorial (https://www.tensorflow.org/guide/keras/transfer_learning).

The problem is, adding a model such as vgg16 with the functional API, as a layer to my main model, seems to ""break the graph"" and not allow part of the GradCAM tutorial code to work properly. The model itself works fine for prediction and etc, but upon trying to create a 'last_conv_layer_model' I keep getting a ""Graph disconnected"" error.

I have tried flattening the model, so that vgg16 is not nested within the main model anymore, but this didn't change the error.

I've seen many people trying to figure out this problem and apparently the only solution is to never use the functional API in this way (allowing for nested models to happen), but that seems like a problem especially for already trained models.  I thought I'd create an issue here since maybe someone has a more elegant solution.

I can't upload my actual model but I made a notebook that reproduces the error and all of the described process with a similar architecture. 

https://github.com/palatos/mynotes/blob/main/gradcam-with-nested-models-error.ipynb

(The model is not fine tuned yet in the notebook but that doesn't matter. We could generate GradCAM heatmaps even for gibberish, non-fine tuned, predictions)

Thanks in advance for any clarification on this issue.


"
44461,cannot fin symbol method setUseNNAPI(boolean),"Hi!
I'm trying to run a model i have created on tensorflow lite for android and i got this  error
![Captura de Tela 2020-10-30 as 10 59 16](https://user-images.githubusercontent.com/2678092/97714896-7ed65f80-1aa0-11eb-91ce-3ade0e9d5c4b.png)

How can i solve this?
"
44460,[RNN] TfLite conversion issue in LSTM models,"I have not found any pretrained lstm models to work with . 
Did tfLite provided any pretrained lstm models ? 
I tried to create tflite model but facing issues while conversion .
Could you provide exact script to create tfLite model ? 
Does tfLite has any script for creating tfLite LSTM models with latest version ?
This is my script to create tfLite model. But it has few issues 


```
import numpy as np
import tensorflow as tf



model = tf.keras.Sequential()
# Add an Embedding layer expecting input vocab of size 1000, and
# output embedding dimension of size 64.
model.add(tf.keras.layers.Embedding(input_dim=1000, output_dim=64))

# Add a LSTM layer with 128 internal units.
model.add(tf.keras.layers.LSTM(128))

# Add a Dense layer with 10 units.
model.add(tf.keras.layers.Dense(10))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])
model.summary()
#model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps), num_epochs,
#                        validation_data=valid_data_generator.generate(),
#                        validation_steps=len(valid_data)//(batch_size*num_steps), callbacks=[checkpointer])
tf.saved_model.save(model, ""saved_model_keras_dir"")

model.save('my_lstm_model')
# x_train = 
#(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
#x_train, x_test = x_train / 255.0, x_test / 255.0

# Cast x_train & x_test to float32.
#x_train = x_train.astype(np.float32)
#x_test = x_test.astype(np.float32)

#model.fit(x_train, y_train, epochs=5)
#model.evaluate(x_test, y_test)

converter = tf.lite.TFLiteConverter.from_keras_model(model)

#tflite_model = converter.convert()

# Step 3: Convert the Keras model to TensorFlow Lite model.
sess = tf.compat.v1.keras.backend.get_session()
input_tensor = sess.graph.get_tensor_by_name('embedding_1:0')
output_tensor = sess.graph.get_tensor_by_name('dense_1:0')
converter = tf.lite.TFLiteConverter.from_session(
    sess, [input_tensor], [output_tensor])

tflite = converter.convert()
print('Model converted successfully!')


# Save the model.
with open('lstmmodel.tflite', 'wb') as f:
  f.write(tflite_model) 



```"
44459,Freeze / crash occurring when batch size is reduced,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10 Home 1909**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **2.3.1**
- Python version: **3.7.5**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **10.1 / cudnn-10.1-windows10-x64-v7.6.5.32**
- GPU model and memory: **GeForce GTX 1660 Ti computeCapability: 7.5, coreClock: 1.455GHz coreCount: 24 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 268.26GiB/s**

**Describe the current behavior**
When running with large batch sizes in `model.fit()`, code completes successfully. With smaller batch sizes, the training crashes/freezes in the middle of an epoch.

**Describe the expected behavior**
Smaller batch sizes do not cause TF to hang or crash.

**Standalone code to reproduce the issue**

```
# -*- coding: utf-8 -*-
""""""Reproduce crash during fit""""""
import numpy as np
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential


model = Sequential()
model.add(Dense(9, activation='relu', input_dim=125))
model.add(Dense(31, activation='softmax'))
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
               metrics=['accuracy'])

X_train=np.random.rand(3225, 125)
y_train=np.random.rand(3225, 31)

# This works
model.fit(X_train[:,:], y_train[:,:], 
          epochs=100, 
          batch_size=X_train.shape[0], 
          verbose=1)

# This crashes
model.fit(X_train[:,:], y_train[:,:], 
          epochs=100, 
          batch_size=100, 
          verbose=1)
```

The exact point it freezes seems to vary based on the random seed, here is an example. The shell (`cmd.exe`) is frozen and must be killed through task manager.

```
Epoch 11/100
33/33 [==============================] - 1s 18ms/step - loss: 607.7234 - accuracy: 0.0332
Epoch 12/100
24/33 [====================>.........] - ETA: 0s - loss: 645.0050 - accuracy: 0.0288
```"
44455,Renode tests of CMSIS-NN kernels tests are not working for all,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source): 5b5960f4fda6a6ae9cbf5233873b9ea6910b3e4e
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**
CMSIS-NN kernel tests conv and softmax are not working.

**Please provide the exact sequence of commands/steps when you ran into the problem**

make -j4 -f tensorflow/lite/micro/tools/make/Makefile TAGS=cmsis-nn TARGET=stm32f4  test_kernel_conv_test
make -j4 -f tensorflow/lite/micro/tools/make/Makefile TAGS=cmsis-nn TARGET=stm32f4  test_kernel_softmax_test
"
44454,Error while running TF Lite Model Maker Text Classification Tutorial,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/lite/tutorials/model_maker_text_classification#end-to-end_workflow

## Description of issue (what needs changing): 
The below code is resulting in error

```python
model = text_classifier.create(train_data, model_spec=spec)
```

Error Trace is shown below:

```python
INFO:tensorflow:Retraining the models...
INFO:tensorflow:Retraining the models...
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-7-89009ac171ed> in <module>()
----> 1 model = text_classifier.create(train_data, model_spec=spec)

1 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_examples/lite/model_maker/core/task/text_classifier.py in train(self, train_data, validation_data, epochs, batch_size)
    132     """"""Feeds the training data for training.""""""
    133     if batch_size is None:
--> 134       batch_size = self.model_spec.default_batch_size
    135 
    136     if train_data.size < batch_size:

AttributeError: 'BertClassifierModelSpec' object has no attribute 'default_batch_size'
```

### Clear description:

This tutorial is useful to customize the `TF Lite Text Classification` project with our Dataset.

### Correct links

Is the link to the source code correct?  : NA

### Parameters defined

Are all parameters defined and formatted correctly? : NA

### Returns defined

Are return values defined? : NA

### Raises listed and defined : NA

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example? : NA

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content? : NA

### Submit a pull request? : No

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
44453,BrokenPipeError: [Errno 32] Broken pipe,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.12.3
- Python version:3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)
- CUDA/cuDNN version:cuda 9.0.176/cudnn 7
- GPU model and memory:Tesla K80/11GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When I was training resnet50 model with some epochs,especially when the classification is reaching below 0.09, the following error appears. 
![1](https://user-images.githubusercontent.com/49138243/97657415-be9f4600-1aa4-11eb-8cfb-579b1a27427d.PNG)
![2](https://user-images.githubusercontent.com/49138243/97657426-c65eea80-1aa4-11eb-8814-a38052035dcb.PNG)

**Describe the expected behavior**
What's the error reason and how to solve this problem?

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44449,Printing on TPUs,"**System information**
- TensorFlow version (you are using): 2.3
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
I would like to be able to print on TPUs. Currently printing on TPUs in functions decorated with tf.function does not work (tf.print doesn't work). 

**Will this change the current api? How?**
I don't think the external api would need to be affected, as tf.print should work. 

**Who will benefit with this feature?**
Anyone who wants to debug their code running on TPUs easily!

**Any Other info.**
This issue https://github.com/tensorflow/tensorflow/issues/43705 raised the same concern, but the solution provided doesn't work. I've provided a minimal example here: 
https://colab.research.google.com/drive/1kFdR9tpj7zJjwHIbWRVjVSiA9zwviErK?usp=sharing

I wasn't sure where the config statement should be, so I tried placing it the three obvious scopes (the tf.function, normal python, and the TPU strategy) to no avail."
44448,Possible memory leak in tf.keras.layers.experimental.preprocessing.TextVectorization,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: Python 3.6.10 :: Anaconda, Inc.
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NVIDIA Driver 450.51.05
- GPU model and memory: Could not be determined (Deep Learning AMI (Ubuntu 18.04) Version 35.0 - ami-01aad86525617098d ; g3.8xlarge)

**Describe the current behavior**

A minimal TextVectorization builder works fine when run for the first time in a Jupyter notebook kernel. When the same minimal code is run in a parallel kernel, the below error is produced. The Jupyter notebook often needs to be restarted entirely (not just a kernel shutdown) in order for TextVectorization to work again.

**Describe the expected behavior**

Expected to work properly across multiple notebook instances without need for kernel shutdown or Jupyter notebook restart. I have tried `tf.keras.backend.clear_session()` and `gc.collect()` before and after calls to `TextVectorization` but this has not worked.

**Standalone code to reproduce the issue**

```
num_words = 100000
tag_processor = tf.keras.layers.experimental.preprocessing.TextVectorization(output_mode='int',
                                                                              max_tokens=num_words,
                                                                              name='tag_processor',
                                                                              standardize=None)
tag_processor.adapt(data['keywords_processed'].tolist())
vectorized_text = tag_processor(data['keywords_processed'].tolist()[:100])
print(vectorized_text)


# Example of data['keywords_processed'].tolist(): ['12 remove in disable\\how iphoneipad 14 disabled fix apple disable ios 2020 to how icloud iphone se',
# 'uruguayo san gana se york estados cuesta new en vs 2020 cuanto ca espaol nueva woker neuva california francisco unidos vivir',
# 'quero homem masculina namorado relacionamentos vono luiza comoconquistarumhomem que coach mente os serio pensam de pense gostam como do querem oqueoshomensquerem eles homens um meta buscam crush relacionamento coaching amor conquistar o',
# 'is hoa back tu twin and runs sun ha double through tunnel rail beautiful goes meeting watch 2020 red disappeared black tau beautifully cross station train of the appeared rushed trains to monsters nice',
# ' exercise for  \u200b and    intergral grade  \u200b\u200b\u200b\u200b studying   calculate math limite learning imo  answers brain  integral mathematics seamo algebra study of how tips    9 to',
# '2 depresso jesus da dia orao hoje liberta tarde madrugada 91 de edifica nosso barros cura deus salmo palavra do pai dulcineia noite libertao reis que',
# ...
# ]
#
# len(data.index) == 3907552

# Rest of code cannot be shared, but above example provides size of data frame and format of input to TextVectorization
```

**Other info / logs** 

jupyter core     : 4.6.1
jupyter-notebook : 6.0.3
qtconsole        : 4.6.0
ipython          : 7.12.0
ipykernel        : 5.1.4
jupyter client   : 5.3.4
jupyter lab      : 1.2.6
nbconvert        : 5.6.1
ipywidgets       : 7.5.1
nbformat         : 5.0.4
traitlets        : 4.3.3

---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-21-cfaee3ea4157> in <module>
      5                                                                               name='tag_processor',
      6                                                                               standardize=None)
----> 7 tag_processor.adapt(data['keywords_processed'].tolist())
      8 vectorized_text = tag_processor(data['keywords_processed'].tolist()[:100])
      9 print(vectorized_text)

~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/layers/preprocessing/text_vectorization.py in adapt(self, data, reset_state)
    373         data = array_ops.expand_dims(data, axis=-1)
    374       self.build(data.shape)
--> 375       preprocessed_inputs = self._preprocess(data)
    376     elif isinstance(data, dataset_ops.DatasetV2):
    377       # TODO(momernick): Replace this with a more V2-friendly API.

~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/keras/layers/preprocessing/text_vectorization.py in _preprocess(self, inputs)
    546         # This treats multiple whitespaces as one whitespace, and strips leading
    547         # and trailing whitespace.
--> 548         inputs = ragged_string_ops.string_split_v2(inputs)
    549       elif callable(self._split):
    550         inputs = self._split(inputs)

~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    199     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    200     try:
--> 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/ragged/ragged_string_ops.py in string_split_v2(input, sep, maxsplit, name)
    522       return ragged_tensor.RaggedTensor.from_value_rowids(
    523           values=sparse_result.values,
--> 524           value_rowids=sparse_result.indices[:, 0],
    525           nrows=sparse_result.dense_shape[0],
    526           validate=False)

~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    199     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    200     try:
--> 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py in _slice_helper(tensor, slice_spec, var)
   1022         ellipsis_mask=ellipsis_mask,
   1023         var=var,
-> 1024         name=name)
   1025 
   1026 

~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    199     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    200     try:
--> 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py in strided_slice(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)
   1194       ellipsis_mask=ellipsis_mask,
   1195       new_axis_mask=new_axis_mask,
-> 1196       shrink_axis_mask=shrink_axis_mask)
   1197 
   1198   parent_name = name

~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py in strided_slice(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)
  10318       return _result
  10319     except _core._NotOkStatusException as e:
> 10320       _ops.raise_from_not_ok_status(e, name)
  10321     except _core._FallbackException:
  10322       pass

~/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6841   message = e.message + ("" name: "" + name if name is not None else """")
   6842   # pylint: disable=protected-access
-> 6843   six.raise_from(core._status_to_exception(e.code, message), None)
   6844   # pylint: enable=protected-access
   6845 

~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run StridedSlice: Dst tensor is not initialized. [Op:StridedSlice] name: strided_slice/
"
44447,Compiler Test cases with tf-mlir-translate pass/crash with specific build flag on s390x architecture,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.3.1
- Python version: 3.6.9
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): Ubuntu 7.5.0-3ubuntu1~18.04
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When running test case `//tensorflow/compiler/mlir/xla/tests/translate:while.hlotxt.test` on s390x machine, if I include the build flag: `--per_file_copt=mlir,llvm-project@-UNDEBUG` the Test case passes, but if I remove the build flag, it fails with a `bad alloc` crash. Backtrace is attached below.
Another test case:
 `//tensorflow/compiler/tf2xla:fused_batchnorm_reserve_space_test`, unlike `while.hlotxt.test`, fails with this build flag but passes without it.

There are multiple test case failures in `//tensorflow/compiler/...` with similar crash. The command I am using to test:
```
bazel --host_jvm_args=""-Xms1024m"" --host_jvm_args=""-Xmx2048m"" test --host_javabase=""@local_jdk//:jdk"" --test_tag_filters=-gpu,-benchmark-test,-v1only,-no_oss,-oss_serial  -k --test_timeout 300,450,1200,3600 --build_tests_only --test_output=errors --per_file_copt=mlir,llvm-project@-UNDEBUG -- //tensorflow/compiler/...
```
Please note that there is no regression in the compiler test cases with `--per_file_copt=mlir,llvm-project@-UNDEBUG` on x86 machine.

**Describe the expected behavior**
Test case should pass and test case behaviour should not vary with build flag.


**Other info / logs** 
[while.hlotxt.test.log](https://github.com/tensorflow/tensorflow/files/5461275/while.hlotxt.test.log)

"
44440,tfl_quantizer,I want to use this [tfl_quantizer](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/mlir/lite/quantization/lite) application. How to build and use it?
44438,Tensorflow: TFLiteConverter (Saved Model -> TFLite) in SSD_mobilenet_v2_2,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from (source or binary):  pip install
- TensorFlow version (or github SHA if from source): 2.3.1

I have installed tensorflow 2.3.1, my ssd_mobile_net_v2_2 was downloaded from https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2. I want to convert this model to tf_lite version. My code is:
![image](https://user-images.githubusercontent.com/43801775/97602599-76513b00-1a46-11eb-84ea-680d8b6180fd.png)

But error occurs:

tensorflow.lite.python.convert.ConverterError: :0: error: loc(""Func/StatefulPartitionedCall/input/_0""): requires all operands and results to have compatible element types :0: note: loc(""Func/StatefulPartitionedCall/input/_0""): see current operation: %1 = ""tf.Identity""(%arg0) {device = """"} : (tensor<1x320x320x3x!tf.quint8>) -> tensor<1x320x320x3xui8>
![image](https://user-images.githubusercontent.com/43801775/97602651-836e2a00-1a46-11eb-9764-cac65a8198ae.png)

it seems I need to set the input-data-type, but I don't know how to do that.

Thanks a lot for your replying.


"
44437,Cannot build TFLite flex:delegate package for armhf,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: tensorflow-gpu 2.3.0
- Python version: 3.8
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): bazel 3.5.0
- GCC/Compiler version (if compiling from source):  gcc 9.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**

Cannot build TFLite flex:delegate package for armhf. 

Attempting to build TFLite flex:delegate for armhf results in the following error:

(base) trinity@trinity-blade:~/Documents/my-team/my-project/git/tensorflow$ bazel build -c opt --config=elinux_armhf --config=monolithic //tensorflow/lite/delegates/flex:delegate
WARNING: Ignoring JAVA_HOME, because it must point to a JDK, not a JRE.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=211
INFO: Reading rc options for 'build' from /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /home/trinity/Documents/my-team/my-project/git/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/trinity/miniconda3/bin/python3 --action_env PYTHON_LIB_PATH=/home/trinity/miniconda3/lib/python3.8/site-packages --python_path=/home/trinity/miniconda3/bin/python3 --config=xla
INFO: Found applicable config definition build:short_logs in file /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:elinux_armhf in file /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc: --config=elinux --cpu=armhf
INFO: Found applicable config definition build:elinux in file /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc: --crosstool_top=@local_config_embedded_arm//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
INFO: Found applicable config definition build:monolithic in file /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc: --define framework_shared_object=false
INFO: Found applicable config definition build:linux in file /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr/arm-linux-gnueabihf --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/trinity/Documents/my-team/my-project/git/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/lite/delegates/flex:delegate (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/trinity/.cache/bazel/_bazel_trinity/ec49567c9bd407ca66e6e2b6d734da8f/external/icu/BUILD.bazel:33:1: C++ compilation of rule '@icu//:icuuc' failed (Exit 1)
In file included from external/icu/icu4c/source/common/usetiter.cpp:10:
external/icu/icu4c/source/common/unicode/uniset.h:504:29: error: invalid covariant return type for 'virtual icu_66::UnicodeFunctor* icu_66::UnicodeSet::clone() const'
     virtual UnicodeFunctor* clone() const;
                             ^~~~~
In file included from external/icu/icu4c/source/common/unicode/uniset.h:17,
                 from external/icu/icu4c/source/common/usetiter.cpp:10:
/usr/include/unicode/unifilt.h:80:28: note: overridden function is 'virtual icu_66::UnicodeFilter* icu_66::UnicodeFilter::clone() const'
     virtual UnicodeFilter* clone() const = 0;
                            ^~~~~
Target //tensorflow/lite/delegates/flex:delegate failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 12.828s, Critical Path: 12.51s
INFO: 31 processes: 31 local.
FAILED: Build did NOT complete successfully


**Provide the exact sequence of commands / steps that you executed before running into the problem**

From tensorflow repo root:

$ bazel build -c opt --config=elinux_armhf --config=monolithic //tensorflow/lite/delegates/flex:delegate

.bazelrc is attached. 
[bazelrc.txt](https://github.com/tensorflow/tensorflow/files/5459724/bazelrc.txt)

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44435,[ssd mobileNet v2 model] ValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina
- TensorFlow installed from (source or binary): Source
- TensorFlow version (or github SHA if from source): 2.3.1


**Command used to run the converter or code if youre using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
python object_detection/export_tflite_graph_tf2.py \
    --pipeline_config_path ~/pretrained_models/ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config \
    --trained_checkpoint_dir~/pretrained_models/ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint \
    --output_directory ~/output_dir
```

Traceback (most recent call last):
  File ""/Users/jaribido/Library/Python/3.7/lib/python/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 52, in __init__
    _calibration_wrapper.CalibrationWrapper(model_content))
TypeError: pybind11::init(): factory function returned nullptr

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/user_name/Desktop/Dev/models/convert_file.py"", line 32, in <module>
    tflite_model = converter.convert()
  File ""/Users/user_name/Library/Python/3.7/lib/python/site-packages/tensorflow/lite/python/lite.py"", line 710, in convert
    output_tensors)
  File ""/Users/user_name/Library/Python/3.7/lib/python/site-packages/tensorflow/lite/python/lite.py"", line 638, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File ""/Users/user_name/Library/Python/3.7/lib/python/site-packages/tensorflow/lite/python/lite.py"", line 440, in _calibrate_quantize_model
    calibrate_quantize = _calibrator.Calibrator(result)
  File ""/Users/user_name/Library/Python/3.7/lib/python/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 54, in __init__
    raise ValueError(""Failed to parse the model: %s."" % e)
ValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.

```
# Copy and paste the output here.
```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**
I downloaded ssd mobileNet v2 model from tensorflow zoo. Then I generated the TFLite inference graph savedModel. After which I downloaded coco dataset images and ran the code for TFlite conversion to Full integer quantization. Then I got the error documented above.
"
44431,"This model does not contain associated files, and is not a Zip file","So, i'm trying to use SSD ResNet101 V1 FPN 640x640 (RetinaNet101) found [here ](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)model for my android application.
After converting the model using this script:

```
converter = tf.lite.TFLiteConverter.from_saved_model('/content/drive/My Drive/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/saved_model/',signature_keys=['serving_default'])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.experimental_new_converter = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]

tflite_model = converter.convert()

with tf.io.gfile.GFile('model.tflite', 'wb') as f:
  f.write(tflite_model)

```

I tried to use it inside the android application, but doesn't work.
While the application works just fine using the model from this example

[Example](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android)
```

E/AndroidRuntime: FATAL EXCEPTION: main
    Process: com.example.sd_detect, PID: 23556
    java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.sd_detect/com.example.sd_detect.MainActivity}: java.lang.IllegalStateException: This model does not contain associated files, and is not a Zip file.
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3374)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3513)
        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:83)
        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135)
        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2109)
        at android.os.Handler.dispatchMessage(Handler.java:107)
        at android.os.Looper.loop(Looper.java:214)
        at android.app.ActivityThread.main(ActivityThread.java:7682)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:516)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:950)

```

"
44430,Cannot save a subclassed keras model that relies on AutoGraph,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.3.1
- Python version: 3.7.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
When a subclassed model's ```call()``` method relies on AutoGraph, e.g. it is iterating over a tensor, ```model.save()``` fails with the following exception:

```
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph is disabled in this function. Try decorating it directly with @tf.function.
```

This is inconsistent with ```model.fit()``` which does not require any explicit ```@tf.function``` decorations. The error happens because of the following code in saving_utils.py:

```
  # TODO(mdan): Should the model's call be autographed by default?
  @def_function.function(input_signature=input_signature, autograph=False)
  def _wrapped_model(*args):
    """"""A concrete tf.function that wraps the model's call function.""""""
```

Do you think this code can be changed to say ```autograph=True``` (or just removing ```autograph=False``` since the default value is True) as suggested in the TODO comment? This will make it consistent with the code in ```make_train_function()``` in training.py which uses autograph by default. 

**Describe the expected behavior**
```model.save()``` should be consistent with ```model.fit()``` and not require any explicit ```@tf.function``` decorations.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np

class MyModel(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.layer = tf.keras.layers.Dense(1)

    def call(self, input):
        x = input
        for i in tf.range(2):
            x = self.layer(x)
        return x

if __name__ == ""__main__"":
    x = np.arange(10.)
    y = 2 * x

    model = MyModel()
    model.compile(loss=""mse"", optimizer=""adam"")
    model.fit(x, y, epochs=1)
    print(""Done training"")

    model.save(""saved_model"")
```
"
44429,"In the codelab, ""Build a handwritten digit classifier app with TensorFlow Lite"" , At step number 4, the interpreter is not initialized with model, which is causing an NPE.","https://codelabs.developers.google.com/codelabs/digit-classifier-tflite#3

Check the above link, In step number 4, The interpreter is not initialized using the model.
Which is why it is giving NPE on interpreter instance."
44428,OP_REQUIRES failed at constant_op.cc,"There is a mistake,
In Python, I use tf.while_loop to send LSTM the initial state. The following error occurred when freezing the model to call C language. 

```

2020-10-29 17:19:01.449470: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: ""tensorflow::TensorList"".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?
2020-10-29 17:19:01.451491: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at constant_op.cc:82 : Invalid argument: Cannot parse tensor from tensor_proto.
2020-10-29 17:19:02.134350: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: ""tensorflow::TensorList"".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?
2020-10-29 17:19:02.194184: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at constant_op.cc:82 : Invalid argument: Cannot parse tensor from proto: dtype: DT_VARIANT
```

Is it necessary to compile source code to support it? Or is my usage incorrect?

"
44427,ValueError: Structure of Python function inputs does not match input_signature:,"**System information**
- OS Platform and Distribution :CentOS Linux release 7.7.1908
-TensorFlow version:2.3.0

I try to convert [the tensorflow offical image caption model ](https://www.tensorflow.org/tutorials/text/image_captioning?hl=en)to TFLite model 

I try to convert the `tf.keras.Model `'s encoder and decoder model as following:

```
import tensorflow as tf

embedding_dim = 256
units = 512
top_k = 5000
vocab_size = top_k + 1
features_shape = 2048
attention_features_shape = 64

class BahdanauAttention(tf.keras.Model):
    def __init__(self, utils):
        super(BahdanauAttention, self).__init__()
        self.W1 = tf.keras.layers.Dense(utils)
        self.W2 = tf.keras.layers.Dense(utils)
        self.V = tf.keras.layers.Dense(1)
    def call(self, features, hidden):
        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)

        # hidden shape == (batch_size, hidden_size)
        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)
        hidden_with_time_axis_shape = tf.expand_dims(hidden, 1)

        # score shape == (batch_size, 64, hidden_size)
        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis_shape))

        # attention_weights shape == (batch_size, 64, 1)
        # you get 1 at the last axis because you are applying score to self.V
        attention_weights = tf.nn.softmax(self.V(score), axis=1)

        # context_vector shape after sum == (batch_size, hidden_size)
        context_vector = attention_weights * features
        context_vector = tf.reduce_sum(context_vector, axis=1)

        return context_vector, attention_weights

class CNN_Encoder(tf.keras.Model):
    #pickle
    #
    def __init__(self, embedding):
        super(CNN_Encoder, self).__init__()
        # shape after fc == (batch_size, 64, embedding_dim)
        self.fc = tf.keras.layers.Dense(embedding_dim)

    # @tf.function(input_signature=[tf.TensorSpec(shape=(1, 64, features_shape),dtype=tf.float32)])
    @tf.function
    def call(self, x):
        x = self.fc(x)
        x = tf.nn.relu(x)
        return x

class RNN_Decoder(tf.keras.Model):
    def __init__(self, embedding_dim, units, vocab_size):
        super(RNN_Decoder, self).__init__()
        self.units = units

        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.gru = tf.keras.layers.GRU(self.units,
                                       return_sequences=True,
                                       return_state=True,
                                       recurrent_initializer='glorot_uniform',
                                       unroll = True)
        self.fc1 = tf.keras.layers.Dense(self.units)
        self.fc2 = tf.keras.layers.Dense(vocab_size)

        self.attention = BahdanauAttention(self.units)


    @tf.function(input_signature=[tf.TensorSpec(shape=[1, 1], dtype=tf.int32, name='x'),
                                  tf.TensorSpec(shape=[1, 64, 256], dtype=tf.float32, name='feature'),
                                  tf.TensorSpec(shape=[1, 512], dtype=tf.float32, name='hidden')])
    @tf.function
    def call(self, x , features, hidden):
        #
        context_vector, attention_weights = self.attention(features, hidden)

        #x shape after passing through embedding == (batch_size, 1, embedding_dim)
        x = self.embedding(x)

        #x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)
        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)

        #concatedGRU
        output, state = self.gru(x)

        #shape == (batch_size, max_length, hidden_size)
        x = self.fc1(output)

        #x shape == (batch_size, max_length, hidden_size)
        x = tf.reshape(x, (-1, x.shape[2]))

        # output shape == (batch_size * max_length, vocab)
        x = self.fc2(x)

        return x, state, attention_weights

    def reset_states(self, batch_size):
        return tf.zeros((batch_size, self.units))

encoder = CNN_Encoder(embedding_dim)
decoder = RNN_Decoder(embedding_dim, units, vocab_size)

encoder._set_inputs(tf.TensorSpec(shape=(1, 64, features_shape),dtype=tf.float32))
decoder._set_inputs([tf.TensorSpec(shape=[1, 1], dtype=tf.int32, name='x'),
                                  tf.TensorSpec(shape=[1, 64, 256], dtype=tf.float32, name='feature'),
                                  tf.TensorSpec(shape=[1, 512], dtype=tf.float32, name='hidden')])


encoder_converter = tf.lite.TFLiteConverter.from_keras_model(encoder)
decoder_converter = tf.lite.TFLiteConverter.from_keras_model(decoder)

encoder_model = encoder_converter.convert()
decoder_model = decoder_converter.convert()

open(""encoder_model.tflite"", ""wb"").write(encoder_model)
open(""decoder_model.tflite"", ""wb"").write(decoder_model)
```

The error messge is 
```
ValueError: Structure of Python function inputs does not match input_signature:
  inputs: (
    [<tf.Tensor 'x:0' shape=(1, 1) dtype=int32>, <tf.Tensor 'feature:0' shape=(1, 64, 256) dtype=float32>, <tf.Tensor 'hidden:0' shape=(1, 512) dtype=float32>])
  input_signature: (
    TensorSpec(shape=(1, 1), dtype=tf.int32, name='x'),
    TensorSpec(shape=(1, 64, 256), dtype=tf.float32, name='feature'),
    TensorSpec(shape=(1, 512), dtype=tf.float32, name='hidden'))
```
I think the function input is the same as the input signature.How can I fix the problem?"
44426,Why XLA need new kernels about op? ,"In TensorFlow code about XLA, I see kernels about many OPs like `[compiler/tf2xla/kernels/concat_op](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/kernels/concat_op.cc)`.  It seems like a repetition of `[core/kernels/concat_op](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/concat_op.cc)`.
Why Ops like `compiler/tf2xla/kernels/concat_op` are needed? And why not just replace it with `core/kernels/concat_op` to save code? "
44424,Encountered unresolved custom op: TensorListReserve and failed to prepare,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source): training, export model used tensorflow 1.15.0, inference model used tensorflow lite CPP 2.3.1 (master) and 1.15.0 as well.

**Provide the text output from tflite_convert**
Added tf.enable_control_flow_v2() to model before exporting.
Used command (tensorflow 1.15.0) : tflite_convert --saved_model_dir=superpoint_640x480x_control_flow --enable_select_tf_ops --allow_custom_ops --output_file=testing.tflite
```
2020-10-29 14:18:08.106202: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-10-29 14:18:08.127674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.127922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2020-10-29 14:18:08.128040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-10-29 14:18:08.128856: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-10-29 14:18:08.129575: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-10-29 14:18:08.129739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-10-29 14:18:08.130698: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-10-29 14:18:08.131440: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-10-29 14:18:08.133886: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-10-29 14:18:08.133976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.134418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.134628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-10-29 14:18:08.134848: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-10-29 14:18:08.158045: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz
2020-10-29 14:18:08.158687: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b335094940 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-29 14:18:08.158725: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-29 14:18:08.158925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.159353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2020-10-29 14:18:08.159391: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-10-29 14:18:08.159408: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-10-29 14:18:08.159426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-10-29 14:18:08.159441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-10-29 14:18:08.159456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-10-29 14:18:08.159471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-10-29 14:18:08.159486: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-10-29 14:18:08.159550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.159921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.160242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-10-29 14:18:08.160279: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-10-29 14:18:08.270633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-29 14:18:08.270654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-10-29 14:18:08.270659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-10-29 14:18:08.270764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.271031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.271270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.271500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2968 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-10-29 14:18:08.272684: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b337854f60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-10-29 14:18:08.272696: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1
WARNING:tensorflow:From /home/anaconda3/envs/dynim/lib/python3.7/site-packages/tensorflow_core/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
W1029 14:18:08.273251 140612085802816 deprecation.py:323] From /anaconda3/envs/dynim/lib/python3.7/site-packages/tensorflow_core/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
INFO:tensorflow:Restoring parameters from superpoint_640x480x_control_flow/variables/variables
I1029 14:18:08.345254 140612085802816 saver.py:1284] Restoring parameters from superpoint_640x480x_control_flow/variables/variables
INFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}
I1029 14:18:08.741633 140612085802816 convert_saved_model.py:80] The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}
INFO:tensorflow:input tensors info: 
I1029 14:18:08.741893 140612085802816 convert_saved_model.py:99] input tensors info: 
INFO:tensorflow:Tensor's key in saved_model's tensor_map: image
I1029 14:18:08.742036 140612085802816 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: image
INFO:tensorflow: tensor name: superpoint/image:0, shape: (1, 480, 640, 1), type: DT_FLOAT
I1029 14:18:08.742112 140612085802816 convert_saved_model.py:43]  tensor name: superpoint/image:0, shape: (1, 480, 640, 1), type: DT_FLOAT
INFO:tensorflow:output tensors info: 
I1029 14:18:08.742195 140612085802816 convert_saved_model.py:101] output tensors info: 
INFO:tensorflow:Tensor's key in saved_model's tensor_map: descriptors
I1029 14:18:08.742318 140612085802816 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: descriptors
INFO:tensorflow: tensor name: superpoint/descriptors:0, shape: (1, 480, 640, 256), type: DT_FLOAT
I1029 14:18:08.742387 140612085802816 convert_saved_model.py:43]  tensor name: superpoint/descriptors:0, shape: (1, 480, 640, 256), type: DT_FLOAT
INFO:tensorflow:Tensor's key in saved_model's tensor_map: descriptors_raw
I1029 14:18:08.742443 140612085802816 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: descriptors_raw
INFO:tensorflow: tensor name: superpoint/descriptors_raw:0, shape: (1, 60, 80, 256), type: DT_FLOAT
I1029 14:18:08.742491 140612085802816 convert_saved_model.py:43]  tensor name: superpoint/descriptors_raw:0, shape: (1, 60, 80, 256), type: DT_FLOAT
INFO:tensorflow:Tensor's key in saved_model's tensor_map: prob_nms
I1029 14:18:08.742545 140612085802816 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: prob_nms
INFO:tensorflow: tensor name: superpoint/prob_nms:0, shape: (1, 480, 640), type: DT_FLOAT
I1029 14:18:08.742592 140612085802816 convert_saved_model.py:43]  tensor name: superpoint/prob_nms:0, shape: (1, 480, 640), type: DT_FLOAT
INFO:tensorflow:Tensor's key in saved_model's tensor_map: pred
I1029 14:18:08.742646 140612085802816 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: pred
INFO:tensorflow: tensor name: superpoint/pred:0, shape: (1, 480, 640), type: DT_INT32
I1029 14:18:08.742693 140612085802816 convert_saved_model.py:43]  tensor name: superpoint/pred:0, shape: (1, 480, 640), type: DT_INT32
INFO:tensorflow:Tensor's key in saved_model's tensor_map: logits
I1029 14:18:08.742746 140612085802816 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: logits
INFO:tensorflow: tensor name: superpoint/logits:0, shape: (1, 60, 80, 65), type: DT_FLOAT
I1029 14:18:08.742793 140612085802816 convert_saved_model.py:43]  tensor name: superpoint/logits:0, shape: (1, 60, 80, 65), type: DT_FLOAT
INFO:tensorflow:Tensor's key in saved_model's tensor_map: prob
I1029 14:18:08.742845 140612085802816 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: prob
INFO:tensorflow: tensor name: superpoint/prob:0, shape: (1, 480, 640), type: DT_FLOAT
I1029 14:18:08.742892 140612085802816 convert_saved_model.py:43]  tensor name: superpoint/prob:0, shape: (1, 480, 640), type: DT_FLOAT
2020-10-29 14:18:08.743259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.743479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2020-10-29 14:18:08.743506: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-10-29 14:18:08.743519: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-10-29 14:18:08.743530: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-10-29 14:18:08.743540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-10-29 14:18:08.743551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-10-29 14:18:08.743561: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-10-29 14:18:08.743572: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-10-29 14:18:08.743610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.743803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.743984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-10-29 14:18:08.744021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-29 14:18:08.744028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-10-29 14:18:08.744034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-10-29 14:18:08.744104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.744316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.744505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2968 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from superpoint_640x480x_control_flow/variables/variables
I1029 14:18:08.809329 140612085802816 saver.py:1284] Restoring parameters from superpoint_640x480x_control_flow/variables/variables
2020-10-29 14:18:08.899389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.899605: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-10-29 14:18:08.899660: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-10-29 14:18:08.900032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.900221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2020-10-29 14:18:08.900248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-10-29 14:18:08.900261: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-10-29 14:18:08.900272: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-10-29 14:18:08.900283: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-10-29 14:18:08.900295: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-10-29 14:18:08.900306: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-10-29 14:18:08.900317: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-10-29 14:18:08.900352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.900544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.900711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-10-29 14:18:08.900730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-29 14:18:08.900736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-10-29 14:18:08.900742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-10-29 14:18:08.900797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.900991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.901166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2968 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-10-29 14:18:08.922872: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2020-10-29 14:18:08.922897: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: Graph size after: 675 nodes (0), 1166 edges (0), time = 5.041ms.
2020-10-29 14:18:08.922905: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: Graph size after: 675 nodes (0), 1166 edges (0), time = 4.969ms.
2020-10-29 14:18:08.922911: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: superpoint_pred_tower0_map_while_body_529
2020-10-29 14:18:08.922918: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-10-29 14:18:08.922924: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-10-29 14:18:08.922930: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: superpoint_pred_tower0_map_while_cond_528
2020-10-29 14:18:08.922936: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-10-29 14:18:08.922942: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0ms.
WARNING:tensorflow:From /home/anaconda3/envs/dynim/lib/python3.7/site-packages/tensorflow_core/lite/python/util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W1029 14:18:08.937568 140612085802816 deprecation.py:323] From /home/anaconda3/envs/dynim/lib/python3.7/site-packages/tensorflow_core/lite/python/util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /home/anaconda3/envs/dynim/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W1029 14:18:08.937732 140612085802816 deprecation.py:323] From /home/anaconda3/envs/dynim/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 72 variables.
I1029 14:18:08.979997 140612085802816 graph_util_impl.py:334] Froze 72 variables.
INFO:tensorflow:Converted 72 variables to const ops.
I1029 14:18:08.988762 140612085802816 graph_util_impl.py:394] Converted 72 variables to const ops.
2020-10-29 14:18:08.996008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.996231: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-10-29 14:18:08.996275: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-10-29 14:18:08.996592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.996781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
2020-10-29 14:18:08.996805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-10-29 14:18:08.996814: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-10-29 14:18:08.996821: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-10-29 14:18:08.996830: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-10-29 14:18:08.996838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-10-29 14:18:08.996846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-10-29 14:18:08.996853: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-10-29 14:18:08.996885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.997076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.997241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-10-29 14:18:08.997260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-29 14:18:08.997265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-10-29 14:18:08.997268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-10-29 14:18:08.997320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.997593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-29 14:18:08.997811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2968 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-10-29 14:18:09.044065: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2020-10-29 14:18:09.044089: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 184 nodes (-67), 191 edges (-60), time = 21.371ms.
2020-10-29 14:18:09.044094: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 184 nodes (0), 191 edges (0), time = 6.549ms.
2020-10-29 14:18:09.044098: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: superpoint_pred_tower0_map_while_body_529
2020-10-29 14:18:09.044102: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 43 nodes (0), 49 edges (0), time = 0.965ms.
2020-10-29 14:18:09.044106: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 43 nodes (0), 49 edges (0), time = 0.767ms.
2020-10-29 14:18:09.044110: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: superpoint_pred_tower0_map_while_cond_528
2020-10-29 14:18:09.044114: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 10 nodes (0), 7 edges (0), time = 0.403ms.
2020-10-29 14:18:09.044119: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 10 nodes (0), 7 edges (0), time = 0.141ms.
```
Same issue when use TFLiteConverter:
```
saved_model_dir = ""superpoint_640x480x_control_flow""

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, input_arrays=[""superpoint/image""] , input_shapes={""superpoint/image"" : [1, 480, 640, 1]}, output_arrays=['superpoint/logits', 'superpoint/prob', 'superpoint/descriptors_raw', 'superpoint/descriptors', 'superpoint/prob_nms', 'superpoint/pred'])
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
converter.allow_custom_ops = True
converter.experimental_new_converter = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float32]

tflite_model = converter.convert()
open(""testing.tflite"", ""wb"").write(tflite_model)
```

**Standalone code to reproduce the issue** 
Reproduce with cpp code :
```
#include <iostream>

#include <tensorflow/tensorflow/lite/interpreter.h>
#include <tensorflow/tensorflow/lite/kernels/register.h>
#include <tensorflow/tensorflow/lite/string_util.h>
#include <tensorflow/tensorflow/lite/model.h>

int main()
{
    std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(""testing.tflite"");
    if (!model)
    {
        std::cerr << ""Failed to mmap tflite model"" << std::endl;
        exit(0);
    }
    tflite::ops::builtin::BuiltinOpResolver resolver;
    std::unique_ptr<tflite::Interpreter> interpreter;
    if (tflite::InterpreterBuilder(*model.get(), resolver)(&interpreter) != kTfLiteOk)
    {
        std::cerr << ""Failed to interpreter tflite model"" << std::endl;
        exit(0);
    }
    interpreter->AllocateTensors();
}
```

Also, please include a link to a GraphDef or the model if possible.

**Output Logs**
ERROR: Encountered unresolved custom op: TensorListReserve.
ERROR: Node number 1 (TensorListReserve) failed to prepare.

I couldn't allocate input tensor memory, probably because of interpreter->AllocateTensors() failed.
"
44423,Questions about quantization aware trained tflite model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source): 2.3


**Command used to run the converter or code if youre using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf

frozen_graph='./tflite_graph.pb'
input_arrays=[""normalized_input_image_tensor""]
output_arrays=['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']
input_shapes={""normalized_input_image_tensor"":[1,300,600,3]}
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_graph,input_arrays,output_arrays,input_shapes=input_shapes)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.allow_custom_ops=True

tflite_quant_model = converter.convert()

# Save the model.
with open('ssd_mobilenet_v2_traffic_quanaware_int8.tflite', 'wb') as f:
    f.write(tflite_quant_model)

```

**The output from the converter invocation**




[converted_model.zip](https://github.com/tensorflow/tensorflow/files/5456642/converted_model.zip)




**Also, please include a link to the saved model or GraphDef**


[model.zip](https://github.com/tensorflow/tensorflow/files/5456644/model.zip)



**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)

I trained a mobilenet v2 ssd using quantization aware training, and the pb file contains FakeQuantWithMinMaxVars nodes.

I have three questions:

1. The size of the tflite model I converted using Python API is bigger than the size of the pb file. Is this correct?
2. Is the quantized tflite model with int8 weights and uint8 activations?
3. If I use the TFLite converter(Python API), do the FakeQuantWithMinMaxVars nodes be replaced with the quantize and dequantize nodes?


"
44422,"I have been trying to implement a custom tflite model on android studio. Example object detection works. But when I change the tflite file to my model, app gets installed. App closes as soon as it opens saying ""app keeps stopping"". Please help me. I am pretty new to this.","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44421,how to control the number of variables.data ?,"I use tensorflow 2.2.0 ,python 3.6.9

when I saved my model, how to control the number of files 'variables.data' ?

in my machine with titan Xp, saved my model has 2 variables.data files that variables.data-00000-of-00002, variables.data-00001-of-00002.

but in gcp machine with telsa T4, saved my model has 1 variables.data file that variables.data-00000-of-00001

it can be control by user?"
44420,how to use tensorflow_docs after installed ,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
44419,Downgrading the arm linux gnueabihf toolchain causes a linking error when buidling the label_image C++ example,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device: n/a
- TensorFlow installed from: source
- TensorFlow version: master (669993e)
- Python version: 3.7
- Installed using: Bazel
- Bazel version: 3.1.0
- GCC/Compiler version: gcc-linaro-7.3.1-2018.05-x86_64_arm-linux-gnueabihf.tar.xz
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the problem**
Building the TensorFlow Lite `label_image` C++ example with the default _elinux_armhf_  bazel config results in the following error when running the program in the target environment:

`/lib/libm.so.6: version 'GLIBC_2.27' not found (required by [...] label_image)`

In the target environment, doing `strings /lib/libm.so.6 | grep GLIBC` lists versions 2.4, 2.15, 2.18, 2.23, 2.24, and 2.25. Upgrading the environment is not an option. 

To resolve this, I want to downgrade the arm linux gnu toolchain so that I can compile something that can run on the target environment. I build `label_image` after downgrading to the [7.3-2018.05 linaro toolchain](https://releases.linaro.org/components/toolchain/binaries/7.3-2018.05/) but the build fails due to a linking error:

```
ERROR: /home/georges/dev/geo/tensorflow/tensorflow/lite/examples/label_image/BUILD:15:1: Linking of rule '//tensorflow/lite/examples/label_image:label_image' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command
```
I do not know how to investigate this further and I am not certain that the 7.3-2018.05 linaro toolchain is the toolchain I need to begin with. An attempt to compile with the 6.1-2016.08 and 6.5-2018.12 linaro toolchains produces similar linking errors. Branches for those attempts are also available [here](https://github.com/georgeslabreche/tensorflow/tree/toolchain-arm-linux-6.1-2016.08) and [here](https://github.com/georgeslabreche/tensorflow/tree/toolchain-arm-linux-6.5-2018.12).
 
**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Downgrade the arm linux gnu toolchain [in the workspace.bzl and cc_config.bzl.tpl files](https://github.com/georgeslabreche/tensorflow/commit/e64b86ac5091e0853d67ff36368a5cf5a49b3b37). Branch available [here](https://github.com/georgeslabreche/tensorflow/tree/toolchain-arm-linux-7.3-2018.05).
2. `bazel build --verbose_failures --config=elinux_armhf -c opt //tensorflow/lite/examples/label_image:label_image`
3. Error will be thrown during linking.

**Any other info / logs**
```
georges@oppenheimer:~/dev/geo/tensorflow$ bazel build --verbose_failures --config=elinux_armhf -c opt //tensorflow/lite/examples/label_image:label_image
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=245
INFO: Reading rc options for 'build' from /home/georges/dev/geo/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/georges/dev/geo/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Found applicable config definition build:short_logs in file /home/georges/dev/geo/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/georges/dev/geo/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:elinux_armhf in file /home/georges/dev/geo/tensorflow/.bazelrc: --config=elinux --cpu=armhf
INFO: Found applicable config definition build:elinux in file /home/georges/dev/geo/tensorflow/.bazelrc: --crosstool_top=@local_config_embedded_arm//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
INFO: Found applicable config definition build:linux in file /home/georges/dev/geo/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/georges/dev/geo/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/lite/examples/label_image:label_image (20 packages loaded, 1240 targets configured).
INFO: Found 1 target...
ERROR: /home/georges/dev/geo/tensorflow/tensorflow/lite/examples/label_image/BUILD:15:1: Linking of rule '//tensorflow/lite/examples/label_image:label_image' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command
  (cd /home/georges/.cache/bazel/_bazel_georges/711a8914001bc37d15d9e59048761a9c/execroot/org_tensorflow && \
  exec env - \
    PATH='/home/georges/bin:/home/georges/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Program Files/WindowsApps/CanonicalGroupLimited.Ubuntu16.04onWindows_1604.2019.523.0_x64__79rhkp1fndgsc:/mnt/c/Program Files (x86)/Common Files/Oracle/Java/javapath:/mnt/c/Program Files (x86)/Atmel/AVR Tools/AVR32 Toolchain/bin:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0/:/mnt/c/Windows/System32/OpenSSH/:/mnt/c/Program Files (x86)/Calibre2/:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/Microsoft SQL Server/130/Tools/Binn/:/mnt/c/Program Files/Microsoft SQL Server/Client SDK/ODBC/170/Tools/Binn/:/mnt/c/Program Files/dotnet/:/mnt/c/Program Files/Docker/Docker/resources/bin:/mnt/c/ProgramData/DockerDesktop/version-bin:/mnt/c/Users/Georges/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/Georges/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/Georges/AppData/Local/GitHubDesktop/bin:/mnt/c/Users/Georges/Development/Tools/MongoShell/bin:/mnt/c/Users/Georges/.dotnet/tools:/snap/bin' \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /home/georges/.cache/bazel/_bazel_georges/711a8914001bc37d15d9e59048761a9c/external/armhf_linux_toolchain/bin/arm-linux-gnueabihf-gcc @bazel-out/armhf-opt/bin/tensorflow/lite/examples/label_image/label_image-2.params)
Execution platform: @local_execution_config_platform//:platform
bazel-out/armhf-opt/bin/tensorflow/lite/nnapi/libnnapi_implementation.a(nnapi_implementation.o): In function `(anonymous namespace)::ASharedMemory_create(char const*, unsigned int)':
nnapi_implementation.cc:(.text._ZN12_GLOBAL__N_120ASharedMemory_createEPKcj+0x14): warning: the use of `tmpnam' is dangerous, better use `mkstemp'
bazel-out/armhf-opt/bin/tensorflow/lite/kernels/internal/libneon_tensor_utils.a(neon_tensor_utils.o): In function `void ruy::RunPack<(ruy::Path)16, ruy::FixedKernelLayout<(ruy::Order)0, 16, 2>, signed char, signed char>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':
neon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x9e): undefined reference to `ruy::Pack8bitRowMajorForNeon(unsigned char const*, int, int, int, int, int, int, signed char*, int, int, int*, int, int)'
neon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x16a): undefined reference to `ruy::Pack8bitColMajorForNeonOutOfOrder2Cols(ruy::PackParams8bit const&)'
neon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x1e4): undefined reference to `ruy::Pack8bitColMajorForNeonOutOfOrder2Cols(ruy::PackParams8bit const&)'
neon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x286): undefined reference to `ruy::Pack8bitColMajorForNeonOutOfOrder2Cols(ruy::PackParams8bit const&)'
neon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi2EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x2f2): undefined reference to `ruy::Pack8bitColMajorForNeonOutOfOrder2Cols(ruy::PackParams8bit const&)'
bazel-out/armhf-opt/bin/tensorflow/lite/kernels/internal/libneon_tensor_utils.a(neon_tensor_utils.o): In function `void ruy::RunPack<(ruy::Path)16, ruy::FixedKernelLayout<(ruy::Order)0, 16, 4>, signed char, signed char>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':
neon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0xa8): undefined reference to `ruy::Pack8bitRowMajorForNeon(unsigned char const*, int, int, int, int, int, int, signed char*, int, int, int*, int, int)'
neon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x196): undefined reference to `ruy::Pack8bitColMajorForNeonOutOfOrder4Cols(ruy::PackParams8bit const&)'
neon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x22e): undefined reference to `ruy::Pack8bitColMajorForNeonOutOfOrder4Cols(ruy::PackParams8bit const&)'
neon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x2fc): undefined reference to `ruy::Pack8bitColMajorForNeonOutOfOrder4Cols(ruy::PackParams8bit const&)'
neon_tensor_utils.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi16ELi4EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x382): undefined reference to `ruy::Pack8bitColMajorForNeonOutOfOrder4Cols(ruy::PackParams8bit const&)'
bazel-out/armhf-opt/bin/tensorflow/lite/kernels/internal/libneon_tensor_utils.a(neon_tensor_utils.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)16, signed char, signed char, int, int> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':
neon_tensor_utils.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x150): undefined reference to `ruy::Kernel8bitNeonOutOfOrder(ruy::KernelParams8bit<4, 2> const&)'
neon_tensor_utils.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x192): undefined reference to `ruy::Kernel8bitNeonOutOfOrder1Col(ruy::KernelParams8bit<4, 2> const&)'
collect2: error: ld returned 1 exit status
Target //tensorflow/lite/examples/label_image:label_image failed to build
INFO: Elapsed time: 294.772s, Critical Path: 62.65s
INFO: 1243 processes: 1243 local.
FAILED: Build did NOT complete successfully
```
"
44418,Can tflite.interpreter.set_tensor set an inter tensor,"**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from: binary
- TensorFlow version: 1.15.0

**Problem Description**
I have converted my model using float fallback quantization. The figure below is the input part of my float fallback model.

![image](https://user-images.githubusercontent.com/49788188/97520323-e7a4d580-19d5-11eb-8c2e-f9b790d69d55.png)

Firstly inference: I used tflite.interpreter.set_tensor to set my test_data to the input tensor ""in_tensor"", and stored the value of quantified input data (the output port of ""quantize"" in the figure). 

Secondly inference: I set the data stored above to the tensor which is the output of ""quantize"" in the figure( same as the tensor data stored). 

The result of the first reasoning is correct, and the result of the second reasoning is incorrect.

I think it might be tflite.interpreter.set_tensor can only set an input tensor but not an internal tensor. Hoping you giving me some guidance."
44417,Random error with conv_grad_filter_ops,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): TF 2.2
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0.194 / 8.0.1
- GPU model and memory: GeForce RTX 2070 SUPER


**Describe the current behavior**

We have a repo, with a few hundreds ""unit"" tests.
During the CI, in 5-10% of the runs, one of the largest tests (always the same one) is failing with the following log (repeated many times):

`tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at conv_grad_filter_ops.cc:1101 : Not found: No algorithm worked!`

The test suite is a mix of eager execution and graph mode tests.

This particular failing test is building a network built with CNNs (Resnet backbone, then object detection), and it's using the estimator API. Its only exotic feature is the use of RaggedTensors.
Another test is very similar (but no RaggedTensor) and has no issue.

Both these tests are running towards the end of the suite, so after a lot of models/variables have been instantiated and deleted.


**Describe the expected behavior**

A more informative error message.

**Standalone code to reproduce the issue**

Unfortunately, this error can be reproduced only randomly and with the whole repo (even changing the order of the tests can have an impact).
I'm aware this ticket is not super helpful, but I hope someone will have an idea about what's happening ...


**Other info / logs**

Stack trace looks like:
```
[2020-10-27T07:40:30.393Z] tensorflow.python.framework.errors_impl.NotFoundError: No algorithm worked!
[2020-10-27T07:40:30.393Z] 	 [[node SGD/gradients/gradients/.../Conv2D_grad/Conv2DBackpropFilter (defined at ...) ]]
[2020-10-27T07:40:30.393Z] 
[2020-10-27T07:40:30.393Z] Errors may have originated from an input operation.
[2020-10-27T07:40:30.393Z] Input Source operations connected to node SGD/gradients/gradients/.../Conv2D_grad/Conv2DBackpropFilter:
[2020-10-27T07:40:30.393Z]  .../BiasAdd (defined at ...)
```

"
44416,pip install tensorflow fails on macOS and Python 3.9,"**System information**
- OS Platform and Distribution: macOS 10.15.7 (19H2)
- TensorFlow version: tf-nightly
- Python version: 3.9.0
- Installed using virtualenv? pip? conda?: pip, venv

**Describe the problem**

```
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```

**Provide the exact sequence of commands that you executed before running into the problem**

```
python3 -m venv ./test
source ./test/bin/activate
python3 -m pip install --quiet --upgrade pip
python3 -m pip install --upgrade tensorflow
```

```
python3 -m venv ./test
source ./test/bin/activate
python3 -m pip install --quiet --upgrade pip
python3 -m pip install --upgrade nightly
```

**Any other info / logs**

Homebrew recently updated to Python 3.9.

@mihaimaruseac @goldiegadde @Saduf2019"
44414,"Model not deterministic, even though os.environ['TF_DETERMINISTIC_OPS'] = '1' is set","in reference of this [kaggle notebook](https://www.kaggle.com/devang/transfer-learning-with-keras-and-efficientnets) i've written this [notebook](https://drive.google.com/file/d/1VyaUGXgaCDIbWbt0-F0iHTIdokxWesJH/view?usp=sharing) to try transfer learning through multiple datasets.

With multiple soft runs on the notebook, I came to found out that I can't get deterministic/reproducible model out of it. 

Weird thing is, with the same virtual-environment, I can get deterministic/reproducible from a plain [CNN-MNIST](https://drive.google.com/file/d/1GpTupofffdaEufpMSvDkqnTpjiqP3mwC/view?usp=sharing) code. 

Spent a week on researching and can't find the solution. Any guidance or suggestions are much appreciated. 

[my environment](https://drive.google.com/file/d/1lfM5y2vylh9HkuduargAon9arNQKB9kQ/view?usp=sharing)
"
44409,num_units in GRU and LSTM layers confuse meaning,"In tf.keras.layers.LSTM tf.keras.layers.GRU layers there is a parameter called num_units. I saw a lot of questions over the internet about this parameter. and there is not clear answer for what this parameter mean expect for the obvious meaning which is the shape of the output. some say that its mean that in each layer there num_units of LSTM or GRU units, some say that it is only one unit of LSTM or GRU, but with num_units hidden units (num_units of tanh, and sigmoids for each  gate and so on..) inside the LSTM or GRU layer. 
I thought it might be propright that the developers of TF will answer one and for all what is the meaning of this parameter.

Thank You   "
44407,axis with int64 is not supported by tflite for strided_slice op,"Hi,
Recentlly, I converted my pytorch model to pb using onnx-tf. I can convert successfully. But during inference, I found that in my pb model, the axis of strided_slice is int64. How can I convert int64 to int32 axis?

Here is the error:
> ERROR: tensorflow/lite/kernels/strided_slice.cc:155 op_context.begin->type != kTfLiteInt32 (4 != 2)
> ERROR: Node number 22 (STRIDED_SLICE) failed to prepare.

https://github.com/playbar/tfcmake/blob/e5c8319827fd06146f1edcc1557f0189e50da89d/tensorflow/contrib/lite/kernels/strided_slice.cc#L157

I think this because the axis in slice op is int64 and tflite does not support int64.  Is there a method to solve this problem?


 - Python version: 3.6
 - ONNX version:   1.7
 - ONNX-TF version: 1.6
 - Tensorflow version: 1.15 




"
44404,Keras Sequential Model Not taking IN the data due to Matrix size-incompatible,"I am using the beginner template code from TensorFlow. 

https://www.tensorflow.org/tutorials/quickstart/beginner

The code is 

```


#x_train, y_train, x_test, y_test -- All defined before with images size 28 28 
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])
predictions = model(x_train[:1]).numpy()
predictions
```

When I run the following code I expect this 
array([[-0.2858531 , -0.37892324,  0.25575462, -0.16782898,  0.17269424,
        -0.4606699 , -0.15211505,  0.11725096,  0.58891714, -0.2277292 ]],
      dtype=float32)


The error I get is
InvalidArgumentError: Matrix size-incompatible: In[0]: [28,56], In[1]: [784,1] [Op:MatMul]


The partial code to create the images is --
```

x = img.crop([matrix[0][0]-2, matrix[0][1], matrix[2][0]+2, matrix[2][1]+2])
img = x.resize((28, 28))
display(img.convert('LA'))
lstOfArrays.append(numpy.array(img.convert('LA')))                        

```

---------------------------------------------------------------------------

WARNING:tensorflow:Model was constructed with shape Tensor(""flatten_3_input:0"", shape=(None, 28, 28), dtype=float32) for input (None, 28, 28), but it was re-called on a Tensor with incompatible shape (28, 28, 2).
WARNING:tensorflow:Layer flatten_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-180-6c835adf6b59> in <module>
----> 1 predictions = model(x_train[:1]).numpy()
      2 predictions

~\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py in __call__(self, inputs, *args, **kwargs)
    820           with base_layer_utils.autocast_context_manager(
    821               self._compute_dtype):
--> 822             outputs = self.call(cast_inputs, *args, **kwargs)
    823           self._handle_activity_regularization(inputs, outputs)
    824           self._set_mask_metadata(inputs, outputs, input_masks)

~\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\sequential.py in call(self, inputs, training, mask)
    265       if not self.built:
    266         self._init_graph_network(self.inputs, self.outputs, name=self.name)
--> 267       return super(Sequential, self).call(inputs, training=training, mask=mask)
    268 
    269     outputs = inputs  # handle the corner case where self.layers is empty

~\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\network.py in call(self, inputs, training, mask)
    715     return self._run_internal_graph(
    716         inputs, training=training, mask=mask,
--> 717         convert_kwargs_to_constants=base_layer_utils.call_context().saving)
    718 
    719   def compute_output_shape(self, input_shape):

~\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\network.py in _run_internal_graph(self, inputs, training, mask, convert_kwargs_to_constants)
    889 
    890           # Compute outputs.
--> 891           output_tensors = layer(computed_tensors, **kwargs)
    892 
    893           # Update tensor_dict.

~\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py in __call__(self, inputs, *args, **kwargs)
    820           with base_layer_utils.autocast_context_manager(
    821               self._compute_dtype):
--> 822             outputs = self.call(cast_inputs, *args, **kwargs)
    823           self._handle_activity_regularization(inputs, outputs)
    824           self._set_mask_metadata(inputs, outputs, input_masks)

~\Anaconda3\lib\site-packages\tensorflow_core\python\keras\layers\core.py in call(self, inputs)
   1140         outputs = sparse_ops.sparse_tensor_dense_matmul(inputs, self.kernel)
   1141       else:
-> 1142         outputs = gen_math_ops.mat_mul(inputs, self.kernel)
   1143     if self.use_bias:
   1144       outputs = nn.bias_add(outputs, self.bias)

~\Anaconda3\lib\site-packages\tensorflow_core\python\ops\gen_math_ops.py in mat_mul(a, b, transpose_a, transpose_b, name)
   5614         pass  # Add nodes to the TensorFlow graph.
   5615     except _core._NotOkStatusException as e:
-> 5616       _ops.raise_from_not_ok_status(e, name)
   5617   # Add nodes to the TensorFlow graph.
   5618   if transpose_a is None:

~\Anaconda3\lib\site-packages\tensorflow_core\python\framework\ops.py in raise_from_not_ok_status(e, name)
   6604   message = e.message + ("" name: "" + name if name is not None else """")
   6605   # pylint: disable=protected-access
-> 6606   six.raise_from(core._status_to_exception(e.code, message), None)
   6607   # pylint: enable=protected-access
   6608 

~\Anaconda3\lib\site-packages\six.py in raise_from(value, from_value)

InvalidArgumentError: Matrix size-incompatible: In[0]: [28,56], In[1]: [784,1] [Op:MatMul]"
44403,tensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1 
- Python version: 3.7.6

**Describe the expected behavior**
This is probably a duplicate of https://github.com/tensorflow/tensorflow/issues/42239 (which I cannot reopen), but I was able to isolate it as standalone tf.test.TestCase.

The following test itself is successful, but after it completes, an error is logged. (If you run it in an IDE, the output is not attributes to the test case, but visible in the full log)

While examining the original problem in our full model, I added some logging and I think a function named ""__inference__destroyer_15572"" was involved - I guess this is _destroyer() created in _list_functions_for_serialization() of CapturableResource which is a superclass of StaticHashTable.

I couldn't catch it with a python breakpoint, probably because it happens when objects are released. Given that memory management is probably involved, I suspect the python version is important (which would explain why the example in the other issue was not reproducible)

It also still happens with tf-nightly v1.12.1-44401-g11bbaed857 2.4.0-dev20201023.

It doesn't appear with v2.1.0-rc2-17-ge5bf8de410 2.1.0

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
import os
import shutil


class MyLookup(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.table_init = tf.lookup.KeyValueTensorInitializer(
            key_dtype=tf.int64,
            keys=[0, 1, 2],
            value_dtype=tf.string,
            values=[""A"", ""B"", ""C""],
            name=""table_init"")
        self.index_to_kw = tf.lookup.StaticHashTable(self.table_init, ""?"")

    def call(self, inputs, **kwargs):
        return self.index_to_kw.lookup(inputs)


class TestSaveProblem(tf.test.TestCase):

    def determine_and_clear_test_workdir(self):
        testname = self.id()
        result = os.path.abspath(os.path.join(
            os.path.dirname(__file__), ""tmp_test_workdir"", testname))
        shutil.rmtree(result, ignore_errors=True)
        return result

    def testSaveProblem(self):
        export_dir = self.determine_and_clear_test_workdir() + ""/saved_model""

        exampledata = [1, 2]

        input = tf.keras.layers.Input(shape=1, dtype=tf.int64)
        output = MyLookup(name='result')(input)
        model = tf.keras.Model(inputs=[input], outputs=[output])

        # save and load
        model.save(export_dir, save_format='tf', include_optimizer=False)
        loaded_model = tf.saved_model.load(export_dir, [tf.saved_model.SERVING]).signatures[
            'serving_default']

        # test after saving and loading (works!)
        loaded_result = loaded_model(tf.constant(exampledata, dtype=tf.int64))['result']
        self.assertAllEqual([b""B"", b""C""], loaded_result)

if __name__ == '__main__':
    tf.test.main()
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
(tf2.3.1) till@mac-brychcyt [/Users/till/test] 21:08% python test_saveproblem.py 
Running tests under Python 3.7.6: /Users/till/tf2.3.1/bin/python
[ RUN      ] TestSaveProblem.testSaveProblem
2020-10-28 21:08:59.759328: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-28 21:08:59.770427: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe5afe7a390 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-28 21:08:59.770444: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-28 21:08:59.868931: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
INFO:tensorflow:Assets written to: /Users/till/test/tmp_test_workdir/__main__.TestSaveProblem.testSaveProblem/saved_model/assets
I1028 21:09:00.045232 4522530240 builder_impl.py:775] Assets written to: /Users/till/test/tmp_test_workdir/__main__.TestSaveProblem.testSaveProblem/saved_model/assets
INFO:tensorflow:time(__main__.TestSaveProblem.testSaveProblem): 0.37s
I1028 21:09:00.122746 4522530240 test_util.py:1973] time(__main__.TestSaveProblem.testSaveProblem): 0.37s
[       OK ] TestSaveProblem.testSaveProblem
[ RUN      ] TestSaveProblem.test_session
[  SKIPPED ] TestSaveProblem.test_session
----------------------------------------------------------------------
Ran 2 tests in 0.371s

OK (skipped=1)
Exception ignored in: <function CapturableResourceDeleter.__del__ at 0x12f193170>
Traceback (most recent call last):
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py"", line 202, in __del__
    self._destroy_resource()
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 823, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 697, in _initialize
    *args, **kwds))
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2855, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3213, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3075, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 600, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/saved_model/function_deserialization.py"", line 237, in restored_function_body
    return _call_concrete_function(function, inputs)
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/saved_model/function_deserialization.py"", line 74, in _call_concrete_function
    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py"", line 106, in _call_flat
    cancellation_manager)
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1938, in _call_flat
    flat_outputs = forward_function.call(ctx, args_with_tangents)
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 579, in call
    executor_type=executor_type)
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/ops/functional_ops.py"", line 1192, in partitioned_call
    f.add_to_graph(graph)
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 495, in add_to_graph
    g._add_function(self)
  File ""/Users/till/tf2.3.1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3345, in _add_function
    gradient)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null
```
"
44400,Pycharm 3.9 not supporting Tensorflow,"ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
Hi!, i'm someone thats completley new to coding, now i was instructed to download tensorflow thru pycharm (i prefer to have things stored physically on my device rather than online) as i tried all of the guides i realized that for most platforms i was getting the exact same answer, i was searching breifly and saw that there wasnt another thread about this topic, so thank you to anyone that interacts with this!
![image](https://user-images.githubusercontent.com/73615107/97494015-c7bcd580-1933-11eb-86b9-6fd88744b189.png)
"
44396,DLL Error,"Traceback (most recent call last):
  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train_audio.py"", line 4, in <module>
    import tensorflow as tf
  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.


Please can someone help me with this!"
44394,How are batch gradients computed on embedding layers?,"Consider the following model, which is more or less a 12-dimensional vector lookup table with 10 rows, initialized to all zeros.

```
model = keras.models.Sequential()
model.add(keras.layers.Embedding(input_dim=10, output_dim=12, embeddings_initializer=keras.initializers.zeros))
model.compile(optimizer=keras.optimizers.SGD(),loss=keras.losses.MeanSquaredError())
```

I simply want it to train to the following data:
```
x = numpy.append(numpy.zeros(10000), numpy.ones(10000))
y = numpy.append(numpy.random.multivariate_normal(numpy.zeros(12), numpy.diag(numpy.ones(12)), 10000),
                 numpy.random.multivariate_normal(numpy.ones(12)*2, numpy.diag(numpy.ones(12)), 10000), axis=0)
model.fit(x,y,epochs=1,batch_size=1)
```

When the batch size is 1, the model behaves predictably; using stochastic gradient descent, we train the weights of the embedding layer towards the means of the two conditional distribution - i.e. model(0) tends towards [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], and model(1) tends toward [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2].

However, for batch sizes larger than 1, the weights of the model tend toward the average value of y, without respect for the value of x - i.e. both model(0) and model(1) tend toward [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1].

Why does this happen? If we just want a lookup table, we can alternately implement it using a dense layer with no bias term; this simply requires that we one-hot encode the input.

```
model = keras.models.Sequential()
model.add(keras.layers.Dense(12, input_shape=(10,), use_bias=False, kernel_initializer=keras.initializers.zeros))
model.compile(optimizer=keras.optimizers.SGD(),loss=keras.losses.MeanSquaredError())

x = tensorflow.one_hot(numpy.append(numpy.zeros(10000), numpy.ones(10000)), 10)
y = numpy.append(numpy.random.multivariate_normal(numpy.zeros(12), numpy.diag(numpy.ones(12)), 10000),
                 numpy.random.multivariate_normal(numpy.ones(12)*2, numpy.diag(numpy.ones(12)), 10000), axis=0)
model.fit(x,y,epochs=16,batch_size=128)
```

This behaves as I would expect, even when using mini-batch for training. So why doesn't it work correctly using the embedding layer?"
44393,tflite_convert produce 1kb tensorflow lite file,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source if i remember correctly
- TensorFlow version (or github SHA if from source): 2.3.1


**Command used to run the converter or code if youre using the Python API**
So, i'm trying to use [SSD ResNet101 V1 FPN 640x640 (RetinaNet101) found here][1] model for my android application. 
I download the model on desktop and then i use this command

`    python object_detection/export_tflite_graph_tf2.py --pipeline_config_path C:\Users\Davide\Desktop\ssd_resnet101_v1_fpn_640x640_coco17_tpu-8\pipeline.config --trained_checkpoint_dir C:\Users\Davide\Desktop\ssd_resnet101_v1_fpn_640x640_coco17_tpu-8\checkpoint --output_directory C:\Users\Davide\Desktop\fine_tuned`

After a bit on desktop a new folder ""fine_tuned"" appear, after that i use the second command:

    `tflite_convert --saved_model_dir=C:\Users\Davide\Desktop\fine_tuned\saved_model --output_file=C:\Users\Davide\Desktop\mobilenet.tflite`

And the file appear on the desktop, the problem is that it's 1kb. When i try to use it inside my application i get this error:

        `java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.sd_detect/com.example.sd_detect.MainActivity}: java.lang.IllegalStateException: This model does not contain associated files, and is not a Zip file.`

  [1]: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md

output from tflite_convert

   ```
 2020-10-26 15:11:42.511698: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
    2020-10-26 15:11:42.511837: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
    2020-10-26 15:12:01.023376: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found
    2020-10-26 15:12:01.023482: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
    2020-10-26 15:12:01.030610: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-II0SFO3
    2020-10-26 15:12:01.030829: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-II0SFO3
    2020-10-26 15:12:01.042164: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x17c13580870 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
    2020-10-26 15:12:01.042253: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
    I1026 15:14:04.769973  1808 lite.py:624] Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False
    2020-10-26 15:14:13.437234: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
    2020-10-26 15:14:13.437349: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.
    2020-10-26 15:14:13.439460: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: C:\Users\Davide\Desktop\fine_tuned\saved_model
    2020-10-26 15:14:13.642525: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
    2020-10-26 15:14:13.642639: I tensorflow/cc/saved_model/loader.cc:250] Reading SavedModel debug info (if present) from: C:\Users\Davide\Desktop\fine_tuned\saved_model
    2020-10-26 15:14:14.428409: I tensorflow/cc/saved_model/loader.cc:215] Restoring SavedModel bundle.
    2020-10-26 15:14:16.534036: I tensorflow/cc/saved_model/loader.cc:199] Running initialization op on SavedModel bundle at path: C:\Users\Davide\Desktop\fine_tuned\saved_model
    2020-10-26 15:14:17.186241: I tensorflow/cc/saved_model/loader.cc:319] SavedModel load for tags { serve }; Status: success: OK. Took 3746762 microseconds.
```

"
44391,TensorFlow 2.2 and 2.3 not detecting the Titan XP GPU,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- TensorFlow installed from (source or binary): Binary in Virtual environment as per https://www.tensorflow.org/install/pip and https://www.tensorflow.org/install/gpu
- TensorFlow version: 2.2 and 2.3 in two virtual environments
- Python version:  Tried both 3.6 and 3.8 in virtual environments
- Installed using virtualenv? pip? conda?: PIP

- CUDA: 
`nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243 `

-cuDNN version: 
`#define CUDNN_MAJOR 7
#define CUDNN_MINOR 6
#define CUDNN_PATCHLEVEL 5
--
#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)
`
- GPU model and memory: Titan XP (Details from nvidia-smi, shared below)
`Wed Oct 28 13:05:46 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  TITAN Xp            Off  | 00000000:01:00.0  On |                  N/A |
| 23%   31C    P5    17W / 250W |    371MiB / 12192MiB |      0%      Default |
|                               |                      |                  N/A `


**Describe the problem**
**TensorFlow 2.2 and 2.3 not detecting the Titan XP GPU**
**Provide the exact sequence of commands / steps that you executed before running into the problem**
Created virtual environments using : `python3.8 -m venv --system-site-packages ./TF_2` (same step for python 3.6)
Sourced: source TF_2/bin/activate
Code and outputs from terminal with python: 
`>>> **import tensorflow as tf**
2020-10-28 12:56:39.039879: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
>>> **tf.config.list_physical_devices('GPU')**
2020-10-28 12:56:45.248971: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-10-28 12:56:45.282612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-28 12:56:45.283010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: TITAN Xp computeCapability: 6.1
coreClock: 1.582GHz coreCount: 30 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 510.07GiB/s
2020-10-28 12:56:45.283029: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-28 12:56:45.283144: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
2020-10-28 12:56:45.284267: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-28 12:56:45.284454: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-28 12:56:45.285753: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-28 12:56:45.286488: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-28 12:56:45.289239: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-28 12:56:45.289253: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
**Skipping registering GPU devices...
[]**
`


**Any other info / logs**
Note the path was set in the ~/.bashrc 
`export PATH=/usr/local/cuda-10.1/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
export PATH=~/Downloads/protobuf${PATH:+:${PATH}}`
"
44390,"Tensorflow release files missing from PyPi, fallback to local hashing ","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.2
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip 20.2.4


**Describe the problem**
When using pip-compile to get recreate a requirements file with a hash pointer to tensorflow, the hash must be generated each time. It would be create it the required files/settings were included 


```md
tensorflow-gpu
  Missing release files on PyPI
  Couldn't get hashes from PyPI, fallback to hashing files
  Hashing tensorflow_gpu-2.2.0-cp38-win_amd64.whl
tensorflow-gpu
    Missing release files on PyPI
    Couldn't get hashes from PyPI, fallback to hashing files
    Hashing tensorflow_gpu-2.2.0-cp38-cp38-win_amd64.whl
    Hashing tensorflow_gpu-2.2.0-cp37-cp37m-win_amd64.whl
    Hashing tensorflow_gpu-2.2.0-cp38-cp38-manylinux2010_x86_64.whl
    Hashing tensorflow_gpu-2.2.0-cp35-cp35m-manylinux2010_x86_64.whl
    Hashing tensorflow_gpu-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl
    Hashing tensorflow_gpu-2.2.0-cp35-cp35m-win_amd64.whl
    Hashing tensorflow_gpu-2.2.0-cp36-cp36m-win_amd64.whl
    Hashing tensorflow_gpu-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```shell-script
echo ""tensorflow==2.2"" > tensorflow.in
pip install pip-tools
pip-compile -v --allow-unsafe --generate-hashes tensorflow.in
```

**Any other info / logs**
Nice to have... not sure how simple it is? But every other package I am working with has this setup properly, so i only notice Tensorflow taking its time."
44389,fatal error: fatbinary_section.h: No such file or directory,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3
- Python version: 3.6, 3.7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 7.5
- CUDA/cuDNN version: 11.1 , 8
- GPU model and memory: RTX 2080 Ti

**Describe the problem**

I am installing TF from the source following steps https://www.tensorflow.org/install/source.
and
https://towardsdatascience.com/how-to-compile-tensorflow-2-3-with-cuda-11-1-8cbecffcb8d3
The build ended up with the error in the following.

INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/nccl_archive/BUILD.bazel:53:1: C++ compilation of rule '@nccl_archive//:device_dlink' failed (Exit 1)
bazel-out/k8-opt/bin/external/nccl_archive/device_dlink.cc:69:10: fatal error: fatbinary_section.h: No such file or directory
 #include <fatbinary_section.h>
          ^~~~~~~~~~~~~~~~~~~~~
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /root/tensorflow/tensorflow/lite/toco/python/BUILD:84:1 C++ compilation of rule '@nccl_archive//:device_dlink' failed (Exit 1)
INFO: Elapsed time: 120.484s, Critical Path: 45.36s
INFO: 989 processes: 989 local.

Please let me know how to fix it as google returns zero search results for it.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

 bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package

before this I had the following error and fixed it as shown:
Error:
ERROR: /root/tensorflow/tensorflow/stream_executor/cuda/BUILD:144:1: C++ compilation of rule '//tensorflow/stream_executor/cuda:cudart_stub' failed (Exit 1)
tensorflow/stream_executor/cuda/cudart_stub.cc:19:10: fatal error: third_party/gpus/cuda/include/cuda_runtime_api.h: No such file or directory
 #include ""third_party/gpus/cuda/include/cuda_runtime_api.h""
 
compilation terminated.
 
Solution:
sudo apt-get install nvidia-cuda-toolkit
 
export CPATH=/usr/local/cuda-11.1/targets/x86_64-linux/include:$CPATH
export LD_LIBRARY_PATH=/usr/local/cuda-11.1/targets/x86_64-linux/lib:$LD_LIBRARY_PATH
export PATH=/usr/local/cuda-11.1/bin:$PATH


I also fixed other errors: 
Edit
third_party/gpus/cuda_configure.bzl
 
 ""cudart"": _check_cuda_lib_params(
            ""cudart"",
            cpu_value,
            cuda_config.config[""cuda_library_dir""],
            # Comment this out since cudart is still on 11.0 instead of
            # the same as cuda 11.1
            # cuda_config.cuda_version,
            ""11.0"",
            static = False,
        ),
        ""cudart_static"": _check_cuda_lib_params(
            ""cudart_static"",
            cpu_value,
            cuda_config.config[""cuda_library_dir""],
            # Comment this out since cudart is still on 11.0 instead of
            # the same as cuda 11.1
            # cuda_config.cuda_version,
            ""11.0"",
            static = True,
        ),
        ""cublas"": _check_cuda_lib_params(
            ""cublas"",
            cpu_value,
            cuda_config.config[""cublas_library_dir""],
            # Comment this out since cudart is still on 11.0 instead of
            # the same as cuda 11.1
            # cuda_config.cuda_version,
            ""11"",
            static = False,
        ),

"
44387,Tensorflow keras `save_weights` and `load_weights` produce random evaluation result on some dataset,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10 and Mac OS 10.15.7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): `pip install tensorflow`
- TensorFlow version (use command below): 2.3.1
- Python version: Window 10 (3.7.4), Mac OS (3.8.5)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Has CUDA only on Window 10 (CUDA 10.1, cuDNN 7.6.5) 
- GPU model and memory: MSI GeForce GTX1080 8GB


**Describe the current behavior**
When `save_weights` and `load_weights` on keras model, seem to work fine in the same python session with training.
But after stop that python session and try calling `load_weights` on a new python session, some dataset produce a random evaluation result.

**Describe the expected behavior**
Evaluation result after `load_weights` should be the same across python session

**Standalone code to reproduce the issue**
[This ](https://github.com/sainttail/tensorflow-load-weights-problem)is the full standalone reproducible problem when `load_weights` each time get you a random evaluation loss and accuracy

"
44386,tf.where raises TypeError for a RaggedTensor argument 'condition',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): `pip install tensorflow`
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.8.2 64-bit ('venv')

**Describe the current behavior**
Calling tf.where raises TypeError for a RaggedTensor argument 'condition' `TypeError: Expected bool passed to parameter 'condition' of op 'SelectV2', got tf.RaggedTensor`.

**Describe the expected behavior**
tf.where should support RaggedTensor

**Other info / logs** 
Source code snippet:

`input_action_mask = tf.where(tf.math.equal(action_mask, 0), NO_ZERO_INPUT_VALUE, action_mask, name=""input_action_mask"")`

`action_mask` is a RaggedTensor, `NO_ZERO_INPUT_VALUE` is a float (I think broadcasting should not be a problem here)

```
2020-10-28 15:26:31,076 ERROR worker.py:1018 -- Possible unhandled error from worker: ray::RolloutWorker.__init__() (pid=3809, ip=172.28.211.149)
  File ""/home/user/venv/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py"", line 268, in inner
    _check_failed(v)
  File ""/home/user/venv/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py"", line 249, in _check_failed
    raise ValueError(v)
ValueError: tf.RaggedTensor(values=Tensor(""tu_policy/Equal_1:0"", shape=(?, 1, 133), dtype=bool), row_splits=Tensor(""tu_policy/Placeholder_12:0"", shape=(?,), dtype=int64))

During handling of the above exception, another exception occurred:

ray::RolloutWorker.__init__() (pid=3809, ip=172.28.211.149)
  File ""/home/user/venv/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"", line 465, in _apply_op_helper
    values = ops.convert_to_tensor(
  File ""/home/user/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1499, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/user/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 338, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/user/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 263, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File ""/home/user/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 280, in _constant_impl
    tensor_util.make_tensor_proto(
  File ""/home/user/venv/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py"", line 456, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""/home/user/venv/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py"", line 335, in _AssertCompatible
    raise TypeError(""Expected %s, got %s of type '%s' instead."" %
TypeError: Expected bool, got tf.RaggedTensor(values=Tensor(""tu_policy/Equal_1:0"", shape=(?, 1, 133), dtype=bool), row_splits=Tensor(""tu_policy/Placeholder_12:0"", shape=(?,), dtype=int64)) of type 'RaggedTensor' instead.

During handling of the above exception, another exception occurred:

ray::RolloutWorker.__init__() (pid=3809, ip=172.28.211.149)
  File ""python/ray/_raylet.pyx"", line 479, in ray._raylet.execute_task
  File ""python/ray/_raylet.pyx"", line 483, in ray._raylet.execute_task
  File ""python/ray/_raylet.pyx"", line 484, in ray._raylet.execute_task
  File ""python/ray/_raylet.pyx"", line 438, in ray._raylet.execute_task.function_executor
  File ""/home/user/venv/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 416, in __init__
    self._build_policy_map(policy_dict, policy_config)
  File ""/home/user/venv/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py"", line 1008, in _build_policy_map
    policy_map[name] = cls(obs_space, act_space, merged_conf)
  File ""/home/user/venv/lib/python3.8/site-packages/ray/rllib/policy/tf_policy_template.py"", line 206, in __init__
    DynamicTFPolicy.__init__(
  File ""/home/user/venv/lib/python3.8/site-packages/ray/rllib/policy/dynamic_tf_policy.py"", line 198, in __init__
    self.model = ModelCatalog.get_model_v2(
  File ""/home/user/venv/lib/python3.8/site-packages/ray/rllib/models/catalog.py"", line 339, in get_model_v2
    raise e
  File ""/home/user/venv/lib/python3.8/site-packages/ray/rllib/models/catalog.py"", line 324, in get_model_v2
    instance = model_cls(obs_space, action_space,
  File ""/mnt/c/Users/user/Desktop/KI_Galvanik/MARL_for_Galvanic_per_second_2_TUs/articleSchedulingModel.py"", line 41, in __init__
    input_action_mask = tf.where(tf.math.equal(action_mask, 0), NO_ZERO_INPUT_VALUE, action_mask, name=""input_action_mask"")
  File ""/home/user/venv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/home/user/venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py"", line 4461, in where_v2
    return gen_math_ops.select_v2(condition=condition, t=x, e=y, name=name)
  File ""/home/user/venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 8874, in select_v2
    _, _, _op, _outputs = _op_def_library._apply_op_helper(
  File ""/home/user/venv/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"", line 475, in _apply_op_helper
    raise TypeError(
TypeError: Expected bool passed to parameter 'condition' of op 'SelectV2', got tf.RaggedTensor(values=Tensor(""tu_policy/Equal_1:0"", shape=(?, 1, 133), dtype=bool), row_splits=Tensor(""tu_policy/Placeholder_12:0"", shape=(?,), dtype=int64)) of type 'RaggedTensor' instead. Error: Expected bool, got tf.RaggedTensor(values=Tensor(""tu_policy/Equal_1:0"", shape=(?, 1, 133), dtype=bool), row_splits=Tensor(""tu_policy/Placeholder_12:0"", shape=(?,), dtype=int64)) of type 'RaggedTensor' instead.
```"
44385,InvalidArgumentError on List of Dicts inside TensorFlow Dataset,"# Problem Statement

I have to read from Google Cloud Storage, several JSON Lines files `.jsonl`. In order to do this, I have created a dataset from the records I want to read, which is a `numpy array` containing `[[<gs:// url>, id], ...]` where `id` is row number to check which line is train/test/validation.

# Code

The main function, which creates the `TF Dataset` from a `generator` which yields the previously described `np.ndarray` and then runs a map function to download and parse the file is:

```python
def load_dataset(records: np.ndarray) -> tf.data.Dataset:
    """"""Create Tensorflow Dataset MapDataset (generator) from a list of gs:// data URL.

    Args:
        records (np.ndarray): List of strings, which are gs://<foo>/foo<N>/*.jsonl.gz files

    Returns:
        tf.data.Dataset: MapDataset generator which can be used for training Keras models.
    """"""
    dataset = tf.data.Dataset.from_generator(lambda: _generator(records), (tf.string, tf.int8))
    return dataset


def _generator(records):
    for r in records:
        yield r[0], r[1]
```

As you can see, the `generator` is simply iterating through the `np.ndarray` to get `url` and a `'line index'`

Then I have to `load and preprocess` the file from the URL to get a list of the `json -> Dict` objects.

```python
def _load_and_preprocess(filepath, selected_sample):
    """"""Read a file GCS or local path and process it into a tensor

    Args:
        path (tensor): path string, pointer to GCS or local path

    Returns:
        tensor: processed input
    """"""
    sample_raw_input = tf.io.read_file(filepath)
    uncompressed_inputs = tf.py_function(_get_uncompressed_inputs, [sample_raw_input], tf.string)
    sample = tf.py_function(_load_sampled_sample, [uncompressed_inputs, selected_sample], tf.float32) #This `tf.float32` is definitely wrong
    return sample #This is not a tensor, but a List of Dictionaries which I will process later


def _get_uncompressed_inputs(record):
    return zlib.decompress(record.numpy(), 16 + zlib.MAX_WBITS)


def _load_sampled_sample(inputs: Iterable, selected_sample: List[int]) -> List[Dict[str, str]]:
    if not tf.executing_eagerly():
        raise RuntimeError(""TensorFlow must be executing eagerly."")
    inputs = inputs.numpy()
    selected_sample = selected_sample.numpy()
    sample = _load__sampled_sample_from_jsonl(inputs, selected_sample)
    return sample


def _load__sampled_sample_from_jsonl(jsonl: bytes, selected_sample: List[int]) -> List[Dict[str, str]]:
    json_lines = _read_jsonl(jsonl).split(""\n"")
    sample = list()
    for n, sample_json in enumerate(json_lines):
        sample_obj = _read_json(sample_json) if n in selected_sample else None
        if sample_obj:
            sample.append(sample_obj)
    return sample


def _read_jsonl(jsonl: bytes) -> str:
    return jsonl.decode()

```

# Executing

I then create the dataset with the above code, and try to retrieve a single sample from it to test.

```python
val_ds = load_dataset(validation_records)
samples = tf.data.experimental.get_single_element(
    val_ds
) # This should be a list of Dicts
```

Which `raises`:

```
InvalidArgumentError: ValueError: Attempt to convert a value ({...}) with an unsupported type (<class 'dict'>) to a Tensor.
# ... are the dict values, which is really big so I've shortened it to `...`
Traceback (most recent call last):

  File ""/home/victor/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py"", line 242, in __call__
    return func(device, token, args)

  File ""/home/victor/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py"", line 140, in __call__
    outputs = [

  File ""/home/victor/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py"", line 141, in <listcomp>
    _maybe_copy_to_context_device(self._convert(x, dtype=dtype),

  File ""/home/victor/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py"", line 120, in _convert
    return ops.convert_to_tensor(value, dtype=dtype)

  File ""/home/victor/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1499, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)

  File ""/home/victor/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 338, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)

  File ""/home/victor/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 263, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,

  File ""/home/victor/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 275, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)

  File ""/home/victor/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 300, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)

  File ""/home/victor/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 98, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)

ValueError: Attempt to convert a value ({...}) with an unsupported type (<class 'dict'>) to a Tensor.
# ... are the dict values, which is really big so I've shortened it to `...`


	 [[{{node EagerPyFunc_1}}]] [Op:DatasetToSingleElement]
```

# Conclusion

Is there any way I can work with List of Dicts without Eager execution (which is not allowed from TF Dataset)?

This list of dicts is not the input for my model, however, I simply cannot work with it in the `preprocessing` function because this error is raised before passing the values to any other function.


# Additional Informations:

* Python Version: `3.8`
* Tensorflow Version: `2.3.1`"
44381,TF Nightly fails to find most recent cusolver64,"**System information**
- Windows 10
- TF-Nightly 2.5.0.dev20201027
- Python version: 3.8
- Pip install
- CUDA/cuDNN version: 11
- GPU model and memory: Quadro M2000M

**Describe the problem**

TF-Nightly still fails to import all required libraries.  All dll's other than cusolver appear to link to version 11, while cusolver tries to find a version 10 and ignores 11.


2020-10-28 12:07:52.945021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-10-28 12:08:06.077965: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-10-28 12:08:06.080459: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2020-10-28 12:08:06.596743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: Quadro M2000M computeCapability: 5.0
coreClock: 1.137GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s
2020-10-28 12:08:06.600978: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-10-28 12:08:06.612213: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-10-28 12:08:06.612951: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-10-28 12:08:06.620033: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-10-28 12:08:06.623163: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-10-28 12:08:06.625507: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found
2020-10-28 12:08:06.632009: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-10-28 12:08:06.634138: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-10-28 12:08:06.634754: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1761] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-10-28 12:08:24.017842: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-28 12:08:24.143577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1265] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-28 12:08:24.144363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1271]      
2020-10-28 12:08:24.144755: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-10-28 12:08:24.303626: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2"
44380,GPU kernel for SparseSegmentReduction ops.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): r.1.5.2
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Hi, I have noticed that the SparseSegmentReduction ops do not have GPU implementation. I was wondering is there any consideration for not implementing GPU version SparseSegmentReduction ops? And will the feature be supported in future version?


**Will this change the current api? How?**
It will not change the current API.

**Who will benefit with this feature?**
People who use GPU to train the sparse networks.

**Any Other info.**
"
44379,tf.data.Dataset.from_tensor_slices requests same shape tensors,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
As of now, tf.data.Dataset.from_tensor_slices requests the input tensors to be of the same shape, which is not working very well with the new text preprocessing function tf.keras.preprocessing.text.Tokenizer

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
NLP practionners will benefit from this feature.

**Any Other info.**
Currently it is not possible to create a tensorflow dataset immidiately after using tf.keras.preprocessing.text.Tokenizer because the sequences of idexes generated are'nt all of the same length. This can be solved by using tf.keras.preprocessing.sequence.pad_sequences but it is not very memory friendly as many sequences will be padded with a great deal of unnecessary zeros.
Adding this feature will make it possible to preprocess text data, encode it, put it in a tensor dataset, split between train and test and then pad using .padded_batch method, which way more memory friendly and will make training the model faster as well.
"
44378,"Tflite ops: TensorListReserve, TensorListSetItem, TensorListStack","**System information**
Linux Ubuntu 18.04
TensorFlow installed from source
TensorFlow version 2.1.0


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, FULLY_CONNECTED. Here is a list of operators for which you will need custom implementations: TensorListReserve, TensorListSetItem, TensorListStack.
```

**Standalone code to reproduce the issue** 
https://colab.research.google.com/drive/1jEUZxqM4Olr3Hbiu9WoCpYiVyGGx1ko8?usp=sharing

"
44377,Error during extract ,"I installed using Nvidia CPU, my Nvidia is (GeForce GTX 1070) after finish installation no issue, continue to open the file and able to use the software.

After selecting the video and clicking extract i got an error below, and the same as well with S3fd

Setting Faceswap backend to NVIDIA
10/28/2020 18:35:58 INFO     Log level set to: INFO
10/28/2020 18:36:00 INFO     Output Directory: C:\Users\Daniel-PC\Videos\Model B

10/28/2020 18:36:00 INFO     Loading Detect from Mtcnn plugin...
10/28/2020 18:36:00 INFO     Loading Align from Fan plugin...
10/28/2020 18:36:00 INFO     Loading Mask from Components plugin...
10/28/2020 18:36:00 INFO     Loading Mask from Extended plugin...
10/28/2020 18:36:00 INFO     Starting, this may take a while...
10/28/2020 18:36:00 INFO     Initializing MTCNN (Detect)...
10/28/2020 18:36:02 INFO     Initialized MTCNN (Detect) with batchsize of 8
10/28/2020 18:36:02 INFO     Initializing FAN (Align)...

2020-10-28 18:36:04.204035: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:725] failed to record completion event; therefore, failed to create inter-stream dependency
2020-10-28 18:36:04.204401: E tensorflow/stream_executor/stream.cc:334] Error recording event in stream: Error recording CUDA event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.
2020-10-28 18:36:04.204967: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2020-10-28 18:36:04.205426: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1
Process exited.

10/28/2020 18:43:50 INFO     Loading Detect from S3Fd plugin...
10/28/2020 18:43:50 INFO     Loading Align from Fan plugin...
10/28/2020 18:43:50 INFO     Loading Mask from Components plugin...
10/28/2020 18:43:50 INFO     Loading Mask from Extended plugin...
10/28/2020 18:43:50 INFO     Starting, this may take a while...
10/28/2020 18:43:50 INFO     Initializing S3FD (Detect)...
10/28/2020 18:43:51 INFO     Initialized S3FD (Detect) with batchsize of 4
10/28/2020 18:43:51 INFO     Initializing FAN (Align)...

2020-10-28 18:43:54.494508: E tensorflow/stream_executor/cuda/cuda_driver.cc:910] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2020-10-28 18:43:54.494821: E tensorflow/stream_executor/gpu/gpu_timer.cc:55] Internal: Error destroying CUDA event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2020-10-28 18:43:54.495199: E tensorflow/stream_executor/gpu/gpu_timer.cc:60] Internal: Error destroying CUDA event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2020-10-28 18:43:54.495492: E tensorflow/stream_executor/stream.cc:5485] Internal: Failed to enqueue async memset operation: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2020-10-28 18:43:54.495879: E tensorflow/stream_executor/cuda/cuda_driver.cc:575] failed to load PTX text as a module: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2020-10-28 18:43:54.496149: E tensorflow/stream_executor/cuda/cuda_driver.cc:580] error log buffer (1024 bytes):
2020-10-28 18:43:54.496344: E tensorflow/stream_executor/cuda/cuda_driver.cc:575] failed to load PTX text as a module: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2020-10-28 18:43:54.496580: E tensorflow/stream_executor/cuda/cuda_driver.cc:580] error log buffer (1024 bytes):
2020-10-28 18:43:54.521217: F .\tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: an illegal memory access was encountered


I am still new to this and please help, i tried uninstalling and reinstall but still the same issue."
44376,Error during extract (Help),"After clicking extract i got an error below, and the same as well with S3fd

Setting Faceswap backend to NVIDIA
10/28/2020 18:35:58 INFO     Log level set to: INFO
10/28/2020 18:36:00 INFO     Output Directory: C:\Users\Daniel-PC\Videos\Model B

10/28/2020 18:36:00 INFO     Loading Detect from Mtcnn plugin...
10/28/2020 18:36:00 INFO     Loading Align from Fan plugin...
10/28/2020 18:36:00 INFO     Loading Mask from Components plugin...
10/28/2020 18:36:00 INFO     Loading Mask from Extended plugin...
10/28/2020 18:36:00 INFO     Starting, this may take a while...
10/28/2020 18:36:00 INFO     Initializing MTCNN (Detect)...
10/28/2020 18:36:02 INFO     Initialized MTCNN (Detect) with batchsize of 8
10/28/2020 18:36:02 INFO     Initializing FAN (Align)...

2020-10-28 18:36:04.204035: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:725] failed to record completion event; therefore, failed to create inter-stream dependency
2020-10-28 18:36:04.204401: E tensorflow/stream_executor/stream.cc:334] Error recording event in stream: Error recording CUDA event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.
2020-10-28 18:36:04.204967: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2020-10-28 18:36:04.205426: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1
Process exited.

10/28/2020 18:43:50 INFO     Loading Detect from S3Fd plugin...
10/28/2020 18:43:50 INFO     Loading Align from Fan plugin...
10/28/2020 18:43:50 INFO     Loading Mask from Components plugin...
10/28/2020 18:43:50 INFO     Loading Mask from Extended plugin...
10/28/2020 18:43:50 INFO     Starting, this may take a while...
10/28/2020 18:43:50 INFO     Initializing S3FD (Detect)...
10/28/2020 18:43:51 INFO     Initialized S3FD (Detect) with batchsize of 4
10/28/2020 18:43:51 INFO     Initializing FAN (Align)...

2020-10-28 18:43:54.494508: E tensorflow/stream_executor/cuda/cuda_driver.cc:910] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2020-10-28 18:43:54.494821: E tensorflow/stream_executor/gpu/gpu_timer.cc:55] Internal: Error destroying CUDA event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2020-10-28 18:43:54.495199: E tensorflow/stream_executor/gpu/gpu_timer.cc:60] Internal: Error destroying CUDA event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2020-10-28 18:43:54.495492: E tensorflow/stream_executor/stream.cc:5485] Internal: Failed to enqueue async memset operation: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2020-10-28 18:43:54.495879: E tensorflow/stream_executor/cuda/cuda_driver.cc:575] failed to load PTX text as a module: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2020-10-28 18:43:54.496149: E tensorflow/stream_executor/cuda/cuda_driver.cc:580] error log buffer (1024 bytes):
2020-10-28 18:43:54.496344: E tensorflow/stream_executor/cuda/cuda_driver.cc:575] failed to load PTX text as a module: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2020-10-28 18:43:54.496580: E tensorflow/stream_executor/cuda/cuda_driver.cc:580] error log buffer (1024 bytes):
2020-10-28 18:43:54.521217: F .\tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: an illegal memory access was encountered


I am still new to this and please help, i tried uninstalling and reinstall but still the same issue.

"
44375,DeepLabV3+ Xception SemanticPredictions output layer corrupt,"I can not do inference on a frozen custom trained Xception model. Resnet/Mobilenet is working ok. What might be the difference then? Is there anything special to know about the Xception model output?
During training loss and images look very good. However when using the frozen model the output is always empty.

**System information**
Platform: Linux Ubuntu 20.04 / Nvidia GPU 8 GB
Python: 3.7
Tensorflow: v1.15.2-30-g4386a66
Model: xception_41

**Describe the current behavior**
Output tensor for SemanticPredictions is always the same value.

**Describe the expected behavior**
Output tensor for SemanticPredictions should contain the classes prediction value.

**Other info / logs** Include any logs or source code that would be helpful to

The model was trained:
```
python deeplab/train.py --logtostderr --training_number_of_steps=96400 \
  --train_split=""trainval""  \
  --train_crop_size=""384,384"" --train_batch_size=4 --dataset=""vehicle"" --save_interval_secs=300 \
  --save_summaries_secs=300 --save_summaries_images=True --log_steps=100 --train_logdir=training_xception_41 \
  --dataset_dir=${PATH_TO_DATASET} \
  --scale_factor_step_size=0.05 \
  --min_scale_factor=0.60 \
  --max_scale_factor=0.75 \
  --scale_factor_step_size=0.05 \
  --output_stride=8 --base_learning_rate=0.125 \
  --weight_decay=0.00125 \
  --model_variant=""xception_41""
```

The model was frozen:
```
python deeplab/export_model.py \
--model_variant=""xception_41"" \
--num_classes=${CLASSES} \
--dataset=""vehicle"" \
--checkpoint_path=${toFreeze} \
--crop_size=384 \
--crop_size=384 \
--export_path=graph_xception_41.pb \
--output_stride=8
```

I can see a difference in output graphs (left Resnet / right Xception):
[![https://imgur.com/pPswXVo.png](https://imgur.com/pPswXVo.png)](https://imgur.com/pPswXVo.png)"
44372,InvalidZoneinfoFile when trying to create a GMT pendulum object,"Hello, I encountered the issue below when trying to execute this code

**System information**
- Windows 10
- python 3.7
- pendulum 2.0.2
- pytzdata: 2020.1 (dependency installed by pendulum)

**Code**
`import pendulum` 
`gmt_time = pendulum.now('GMT')`


**Workaround**
This [issue](https://github.com/sdispater/pendulum/issues/300) was closed  but it seem that with my current config it re-appears.
I downgraded my pytzdata version to an old one `2018.3` as a workaround.

**Traceback**
```Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\pnouhaud\.conda\envs\asbooker_prod_py37\lib\site-packages\pendulum\__init__.py"", line 212, in now
    tz = _safe_timezone(tz)
  File ""C:\Users\pnouhaud\.conda\envs\asbooker_prod_py37\lib\site-packages\pendulum\__init__.py"", line 82, in _safe_timezone
    return timezone(obj)
  File ""C:\Users\pnouhaud\.conda\envs\asbooker_prod_py37\lib\site-packages\pendulum\tz\__init__.py"", line 36, in timezone
    tz = _Timezone(name, extended=extended)
  File ""C:\Users\pnouhaud\.conda\envs\asbooker_prod_py37\lib\site-packages\pendulum\tz\timezone.py"", line 30, in __init__
    tz = read(name, extend=extended)
  File ""C:\Users\pnouhaud\.conda\envs\asbooker_prod_py37\lib\site-packages\pendulum\tz\zoneinfo\__init__.py"", line 9, in read
    return Reader(extend=extend).read_for(name)
  File ""C:\Users\pnouhaud\.conda\envs\asbooker_prod_py37\lib\site-packages\pendulum\tz\zoneinfo\reader.py"", line 52, in read_for
    return self.read(file_path)
  File ""C:\Users\pnouhaud\.conda\envs\asbooker_prod_py37\lib\site-packages\pendulum\tz\zoneinfo\reader.py"", line 64, in read
    return self._parse(fd)
  File ""C:\Users\pnouhaud\.conda\envs\asbooker_prod_py37\lib\site-packages\pendulum\tz\zoneinfo\reader.py"", line 115, in _parse
    type_idx = self._parse_type_idx(fd, hdr.transitions)
  File ""C:\Users\pnouhaud\.conda\envs\asbooker_prod_py37\lib\site-packages\pendulum\tz\zoneinfo\reader.py"", line 198, in _parse_type_idx
    buff = self._check_read(fd, n)
  File ""C:\Users\pnouhaud\.conda\envs\asbooker_prod_py37\lib\site-packages\pendulum\tz\zoneinfo\reader.py"", line 77, in _check_read
    nbytes, fd.name, len(result) if result else 0
pendulum.tz.zoneinfo.exceptions.InvalidZoneinfoFile: Expected 0 bytes reading C:\Users\pnouhaud\.conda\envs\asbooker_prod_py37\lib\site-packages\pytzdata\zoneinfo\GMT, but got 0


"
44371,the moving_mean and moving_variance in batchnormalization going to very big after 20 epoch ,"tensorFlow version (use command below):  tensorflow 2.2
Python version: 3.7.9
CUDA/cuDNN version: 10.1

 I'm traing a resnet50 model,but the moving_mean and moving_variance in batchnormalization going to very big after 20 epoch ,and when i use this model to predit the mean and variance of the same layer is different with the moving_mean and moving_variance.

bn_conv1 is the first batchnormalization. bn5c_branch2b (BatchNormalization) in the last identity_block:
bn_conv1 (BatchNormalization)
![1](https://user-images.githubusercontent.com/52857358/97404807-a22ecc80-1931-11eb-9c06-f3a288cf8d04.png)
bn5c_branch2b (BatchNormalization):
![2](https://user-images.githubusercontent.com/52857358/97404847-b2df4280-1931-11eb-9a74-8f8ecc2cae6d.png)

               
__________________________________________________________________________________________________
conv1 (Conv2D)                  (None, 402, 6, 32)   224         reshape[0][0]                    
__________________________________________________________________________________________________
bn_conv1 (BatchNormalization)   (None, 402, 6, 32)   128         conv1[0][0]                      
__________________________________________________________________________________________________
leaky_re_lu (LeakyReLU)         (None, 402, 6, 32)   0           bn_conv1[0][0]                   
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 200, 6, 32)   0           leaky_re_lu[0][0]                
........
........

res5c_branch2b (Conv2D)         (None, 13, 6, 128)   147584      leaky_re_lu_46[0][0]             
__________________________________________________________________________________________________
bn5c_branch2b (BatchNormalizati (None, 13, 6, 128)   512         res5c_branch2b[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_47 (LeakyReLU)      (None, 13, 6, 128)   0           bn5c_branch2b[0][0]              
__________________________________________________________________________________________________
res5c_branch2c (Conv2D)         (None, 13, 6, 1024)  132096      leaky_re_lu_47[0][0]             
__________________________________________________________________________________________________
bn5c_branch2c (BatchNormalizati (None, 13, 6, 1024)  4096        res5c_branch2c[0][0]             
__________________________________________________________________________________________________
add_15 (Add)                    (None, 13, 6, 1024)  0           bn5c_branch2c[0][0]              
                                                                 leaky_re_lu_45[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_48 (LeakyReLU)      (None, 13, 6, 1024)  0           add_15[0][0]                     
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 4, 2, 1024)   0           leaky_re_lu_48[0][0]             
__________________________________________________________________________________________________
flatten (Flatten)               (None, 8192)         0           max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
output_Dense (Dense)            (None, 1)            8193        flatten[0][0]                    
===================================================================================




"
44369,How to get detailed performance profiling results of TFLite for Apple Metal on iOS devices?,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS 14.0.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone xs Max
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): github master with commit: 6f58b4d46ee25633052a844531c3151affdf3635
- Python version:  3.7.6
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source):  Apple clang version 12.0.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
The iOS benchmark app could show detailed performance profiling results for cpu. But with metal delegate, no detailed results are shown.

**Describe the expected behavior**
Shwoing defailed profiling results for metal gpu delegate like CPU

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
The output in xcode  console output section for cpu is shown below:

> Profile (4 threads):
> 
> Thread 0 (932 samples)
> 
> * 70.71% Conv
>   * 65.77% cpu_backend_gemm::Gemm
>     * 63.73% Mul
>       * 28.33% matmul shape: 512x512x196
>         * 28.33% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))
>           * 28.33% TrMulImpl, general case
>             * 16.09% Kernel (kNeon, optimized for out-of-order cores)
>             * 10.41% [other]
>             * 1.82% Pack (kNeon, optimized for out-of-order cores)
>       * 6.87% matmul shape: 1024x1024x49
>         * 6.87% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))
>           * 6.87% TrMulImpl, general case
>             * 3.86% Kernel (kNeon, optimized for out-of-order cores)
>             * 2.15% Pack (kNeon, optimized for out-of-order cores)
>             * 0.86% [other]
>       * 6.12% matmul shape: 256x256x784
>         * 6.12% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))
>           * 6.12% TrMulImpl, general case
>             * 3.65% Kernel (kNeon, optimized for out-of-order cores)
>             * 2.36% [other]
>             * 0.11% Pack (kNeon, optimized for out-of-order cores)
>       * 4.51% matmul shape: 128x128x3136
>         * 4.51% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))
>           * 4.51% TrMulImpl, general case
>             * 3.54% Kernel (kNeon, optimized for out-of-order cores)
>             * 0.54% [other]
>             * 0.43% Pack (kNeon, optimized for out-of-order cores)
>       * 3.76% matmul shape: 64x32x12544
>         * 3.76% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))
>           * 3.76% TrMulImpl, general case
>             * 2.90% Kernel (kNeon, optimized for out-of-order cores)
>             * 0.86% [other]
>       * 3.43% matmul shape: 1024x512x49
>         * 3.43% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))
>           * 3.43% TrMulImpl, general case
>             * 1.61% Kernel (kNeon, optimized for out-of-order cores)
>             * 0.97% Pack (kNeon, optimized for out-of-order cores)
>             * 0.86% [other]
>       * 2.90% matmul shape: 512x256x196
>         * 2.90% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))
>           * 2.90% TrMulImpl, general case
>             * 1.50% Kernel (kNeon, optimized for out-of-order cores)
>             * 1.07% [other]
>             * 0.32% Pack (kNeon, optimized for out-of-order cores)
>       * 2.90% matmul shape: 256x128x784
>         * 2.90% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))
>           * 2.90% TrMulImpl, general case
>             * 1.61% [other]
>             * 1.29% Kernel (kNeon, optimized for out-of-order cores)
>       * 2.68% matmul shape: 128x64x3136
>         * 2.68% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))
>           * 2.68% TrMulImpl, general case
>             * 1.93% Kernel (kNeon, optimized for out-of-order cores)
>             * 0.54% [other]
>             * 0.21% Pack (kNeon, optimized for out-of-order cores)
>       * 2.25% matmul shape: 32x27x12544
>         * 2.25% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))
>           * 2.25% TrMulImpl, general case
>             * 1.29% Kernel (kNeon, optimized for out-of-order cores)
>             * 0.86% [other]
>             * 0.11% GetBlockMatrixCoords
>     * 2.04% cpu_backend_gemm::Gemm: CustomGemv
>   * 4.83% Im2col
>     * 3.00% ExtractPatchIntoBufferColumn
>     * 1.82% [other]
>   * 0.11% [other]
> * 29.08% DepthwiseConv
>   * 22.85% [other]
>   * 6.22% DepthwiseConv/float/DepthwiseConvImpl
>     * 4.83% void tflite::optimized_ops::FloatDepthwiseConvAccumRow(int, int, int, int, const float *, int, int, int, const float *, int, int, int, float *) [kAllowStrided = true, kFixedInputDepth = 0, kFixedDepthMultiplier = 1]
>     * 1.39% [other]
> * 0.21% AveragePool
> 
> Thread 1 (718 samples)
> 
> * 67.55% Kernel (kNeon, optimized for out-of-order cores)
> * 25.63% DepthwiseConv/float/DepthwiseConvImpl
>   * 21.31% void tflite::optimized_ops::FloatDepthwiseConvAccumRow(int, int, int, int, const float *, int, int, int, const float *, int, int, int, float *) [kAllowStrided = true, kFixedInputDepth = 0, kFixedDepthMultiplier = 1]
>   * 4.32% [other]
> * 6.69% Pack (kNeon, optimized for out-of-order cores)
> * 0.14% GetBlockMatrixCoords
> 
> Thread 2 (681 samples)
> 
> * 65.49% Kernel (kNeon, optimized for out-of-order cores)
> * 26.73% DepthwiseConv/float/DepthwiseConvImpl
>   * 21.44% void tflite::optimized_ops::FloatDepthwiseConvAccumRow(int, int, int, int, const float *, int, int, int, const float *, int, int, int, float *) [kAllowStrided = true, kFixedInputDepth = 0, kFixedDepthMultiplier = 1]
>   * 5.29% [other]
> * 7.64% Pack (kNeon, optimized for out-of-order cores)
> * 0.15% GetBlockMatrixCoords
> 
> Thread 3 (645 samples)
> 
> * 66.82% Kernel (kNeon, optimized for out-of-order cores)
> * 25.27% DepthwiseConv/float/DepthwiseConvImpl
>   * 20.16% void tflite::optimized_ops::FloatDepthwiseConvAccumRow(int, int, int, int, const float *, int, int, int, const float *, int, int, int, float *) [kAllowStrided = true, kFixedInputDepth = 0, kFixedDepthMultiplier = 1]
>   * 5.12% [other]
> * 7.60% Pack (kNeon, optimized for out-of-order cores)
> * 0.16% GetBlockByIndex
> * 0.16% GetBlockMatrixCoords

While for metal gpu delegates, the output is:

> Duplicate flags: num_threads
> Log parameter values verbosely: [0]
> Min num runs: [20]
> Inter-run delay (seconds): [-1]
> Benchmark name: [mobile_net_benchmark]
> Min warmup runs: [1]
> Graph: [/private/var/containers/Bundle/Application/29E02AE1-F22E-4454-A049-042C04929CBE/TFLiteBenchmark.app/mobilenet_v1_1.0_224.tflite]
> Input layers: [input]
> Input shapes: [1,224,224,3]
> Use gpu: [1]
> GPU delegate wait type: [aggressive]
> Loaded model /private/var/containers/Bundle/Application/29E02AE1-F22E-4454-A049-042C04929CBE/TFLiteBenchmark.app/mobilenet_v1_1.0_224.tflite
> 2020-10-28 14:21:10.624179+0800 TFLiteBenchmark[3858:924590] Initialized TensorFlow Lite runtime.
> 2020-10-28 14:21:10.632180+0800 TFLiteBenchmark[3858:924590] Created TensorFlow Lite delegate for Metal.
> 2020-10-28 14:21:10.635001+0800 TFLiteBenchmark[3858:924590] Metal GPU Frame Capture Enabled
> 2020-10-28 14:21:10.638319+0800 TFLiteBenchmark[3858:924590] Metal API Validation Enabled
> 2020-10-28 14:21:10.876638+0800 TFLiteBenchmark[3858:924590] Following operations are not supported by GPU delegate:
> SQUEEZE: Operation is not supported.
> 29 operations will run on the GPU, and the remaining 2 operations will run on the CPU.
> 2020-10-28 14:21:11.164313+0800 TFLiteBenchmark[3858:924590] [Metal Compiler Warning] Warning: Compilation succeeded with: 
> 
> program_source:46:11: warning: unused variable 'gid'
>     uint3 gid = uint3(0u, 0u, uint(linear_index));
>           ^
> Explicitly applied GPU delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.
> The input model file size (MB): 16.9008
> Initialized session in 558.992ms.
> Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
> count=86 first=9867 curr=4858 min=4704 max=9867 avg=5813.1 std=1163
> 
> Running benchmark for at least 20 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
> count=183 first=5456 curr=5688 min=4690 max=29198 avg=5447.84 std=1850
> 
> Inference timings in us: Init: 558992, First inference: 9867, Warmup (avg): 5813.1, Inference (avg): 5447.84
> 
> Profile (0 threads):

Deos the TFLite iOS benchmark app supports profiling metal operator performance? If not, how to profile the operator performance on metal?"
44368,Datatype mismatch error,"I am trying to run the TFLite recommendation system example code but getting the following error while running the ""Training Model"" commands. Please suggest if I am missing something?

Documentation Link: https://github.com/tensorflow/examples/tree/master/lite/examples/recommendation/ml

TypeError: Value passed to parameter 'indices' has DataType float32 not in list of allowed values: uint8, int32, int64

#######################
Full snippet:

INFO:tensorflow:Setting up train and eval input_fns.
I1027 21:22:29.326874 140339508344576 app.py:251] Setting up train and eval input_fns.
INFO:tensorflow:Build keras model for mode: train_and_eval.
I1027 21:22:29.326983 140339508344576 app.py:251] Build keras model for mode: train_and_eval.
2020-10-27 21:22:29.336519: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-10-27 21:22:29.340825: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 4007930000 Hz
2020-10-27 21:22:29.341035: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563fc6e69aa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-27 21:22:29.341056: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-27 21:22:29.341118: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2020-10-27 21:22:29.577370: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.
Epoch 1/10000
Traceback (most recent call last):
  File ""/home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/rajesh/Documents/Privacy/TFLite/Tutorial/Recommendation_Google/model/recommendation_model_launcher_keras.py"", line 253, in <module>
    app.run(main)
  File ""/home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/rajesh/Documents/Privacy/TFLite/Tutorial/Recommendation_Google/model/recommendation_model_launcher_keras.py"", line 240, in main
    eval_steps=FLAGS.num_eval_steps)
  File ""/home/rajesh/Documents/Privacy/TFLite/Tutorial/Recommendation_Google/model/recommendation_model_launcher_keras.py"", line 180, in train_and_eval
    callbacks=callbacks)
  File ""/home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 848, in fit
    tmp_logs = train_function(iterator)
  File ""/home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 580, in __call__
    result = self._call(*args, **kwds)
  File ""/home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 506, in _initialize
    *args, **kwds))
  File ""/home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2667, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:

    /home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /home/rajesh/Documents/Privacy/TFLite/Tutorial/Recommendation_Google/model/keras_losses.py:83 call  *
        full_labels = tf.one_hot(
    /home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:180 wrapper  **
        return target(*args, **kwargs)
    /home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:4010 one_hot
        name)
    /home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:6200 one_hot
        off_value=off_value, axis=axis, name=name)
    /home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:578 _apply_op_helper
        param_name=input_name)
    /home/rajesh/anaconda3/envs/tf-2-gpu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:61 _SatisfiesTypeConstraint
        "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))

    TypeError: Value passed to parameter 'indices' has DataType float32 not in list of allowed values: uint8, int32, int64
"
44364,Bazel / FAILED: Build did NOT complete successfully ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.15 
- Python version: 3.6
- Installed using virtualenv? pip? conda?: Conda
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source):



**Describe the problem**
Hi ,I'm trying to convert an object detection model to tflite through bazel. I had been following the EdjeElectronics tutorial from ""https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi#part-1---how-to-train-convert-and-run-custom-tensorflow-lite-object-detection-models-on-windows-10""but I got stuck in errors when I execute:

bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package 
 
I tried two solutions : First add ""const"" to bfloat16.cc file as #41086 mentions and later I tried downgrade numpy (#41061 (comment)).Neither of them worked. 


**Provide the exact sequence of commands / steps that you executed before running into the problem**

(tensorflow-build) C:\tensorflow-build\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/User/Anaconda3/envs/tensorflow-build/python.exe
INFO: Reading rc options for 'build' from c:\tensorflow-build\tensorflow\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --copt=-w --config=v1
INFO: Reading rc options for 'build' from c:\tensorflow-build\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/User/Anaconda3/envs/tensorflow-build/python.exe --action_env PYTHON_LIB_PATH=C:/Users/User/Anaconda3/envs/tensorflow-build/lib/site-packages --python_path=C:/Users/User/Anaconda3/envs/tensorflow-build/python.exe --config monolithic --copt=-w --host_copt=-w --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --verbose_failures --distinct_host_configuration=false --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:v1 in file c:\tensorflow-build\tensorflow\.bazelrc: --define=tf_api_version=1 --action_env=TF2_BEHAVIOR=0
INFO: Found applicable config definition build:monolithic in file c:\tensorflow-build\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Found applicable config definition build:opt in file c:\tensorflow-build\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX --define with_default_optimizations=true
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:647:12: in srcs attribute of cc_library rule //tensorflow/core:lib_proto_parsing: please do not import '//tensorflow/core/platform:protobuf.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:1093:12: in srcs attribute of cc_library rule //tensorflow/core:framework_lite: please do not import '//tensorflow/core/platform:default/integral_types.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:1093:12: in srcs attribute of cc_library rule //tensorflow/core:framework_lite: please do not import '//tensorflow/core/platform:default/mutex.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:1093:12: in srcs attribute of cc_library rule //tensorflow/core:framework_lite: please do not import '//tensorflow/core/platform:default/mutex_data.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:1091:1: in linkstatic attribute of cc_library rule //tensorflow/core:framework_lite: setting 'linkstatic=1' is recommended if there are no object files
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:abi.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:byte_order.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:context.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:cord.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:cpu_feature_guard.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:cpu_info.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:cuda_libdevice_path.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:demangle.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:denormal.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:dynamic_annotations.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:env.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:env_time.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:error.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:file_statistics.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:file_system.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:file_system_helper.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:fingerprint.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:grpc_services.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:host_info.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:human_readable_json.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:init_main.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:load_library.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:logger.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:logging.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:macros.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:mem.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:monitoring.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:mutex.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:net.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:notification.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:null_file_system.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:numa.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:platform.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:platform_strings.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:platform_strings_computed.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:prefetch.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:profile_utils/android_armv7a_cpu_utils_helper.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:profile_utils/clock_cycle_profiler.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:profile_utils/cpu_utils.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:profile_utils/i_cpu_utils_helper.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:protobuf.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:protobuf_compiler.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:protobuf_internal.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:regexp.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:rocm_rocdl_path.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:setround.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:snappy.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:stacktrace.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:stacktrace_handler.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:stream_executor_no_cuda.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:stringpiece.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:strong_hash.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:subprocess.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:tensor_coding.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:test.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:test_benchmark.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:thread_annotations.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:tracing.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:tstring.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:types.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:unbounded_work_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:str_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/bfloat16:bfloat16.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:array_slice.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:cleanup.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:compactptrset.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:edit_distance.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:flatmap.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:flatrep.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:flatset.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:inlined_vector.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:int_type.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:iterator_range.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:manual_constructor.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:map_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:optional.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:priority_queue_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:stl_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:subtle/map_traits.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/gtl:top_n.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/random:distribution_sampler.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/random:exact_uniform_int.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/random:philox_random.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/random:philox_random_test_utils.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/random:random.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/random:random_distributions.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/random:simple_philox.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/random:weighted_picker.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:base64.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:numbers.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:ordered_code.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:proto_serialization.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:proto_text_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:scanner.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:str_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:strcat.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:stringprintf.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/math:math_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:default/monitoring.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:default/mutex.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:default/stacktrace_handler.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:default/tracing.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:default/unbounded_work_queue.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:posix/error.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:windows/env.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:windows/error.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:windows/net.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:windows/port.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:windows/windows_file_system.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:cpu_feature_guard.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:denormal.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:env.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:file_system.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:file_system_helper.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:profile_utils/android_armv7a_cpu_utils_helper.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:profile_utils/clock_cycle_profiler.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:profile_utils/cpu_utils.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:protobuf_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:setround.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:tensor_coding.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/platform:tracing.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/random:distribution_sampler.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/random:random.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/random:random_distributions.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/random:simple_philox.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/random:weighted_picker.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:base64.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:numbers.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:ordered_code.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:proto_serialization.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:proto_text_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:scanner.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:strcat.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:abi.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:byte_order.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:context.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:cord.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:cpu_feature_guard.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:cpu_info.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:cuda_libdevice_path.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:demangle.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:denormal.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:dynamic_annotations.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:env.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:env_time.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:error.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:file_statistics.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:file_system.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:file_system_helper.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:fingerprint.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:grpc_services.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:host_info.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:human_readable_json.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:init_main.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:load_library.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:logger.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:logging.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:macros.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:mem.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:monitoring.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:mutex.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:net.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:notification.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:null_file_system.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:numa.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:platform.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:platform_strings.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:platform_strings_computed.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:prefetch.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:profile_utils/android_armv7a_cpu_utils_helper.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:profile_utils/clock_cycle_profiler.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:profile_utils/cpu_utils.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:profile_utils/i_cpu_utils_helper.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:protobuf.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:protobuf_compiler.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:protobuf_internal.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:regexp.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:rocm_rocdl_path.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:setround.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:snappy.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:stacktrace.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:stacktrace_handler.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:stream_executor_no_cuda.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:stringpiece.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:strong_hash.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:subprocess.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:tensor_coding.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:test.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:test_benchmark.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:thread_annotations.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:tracing.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:tstring.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:types.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:unbounded_work_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/platform:str_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/bfloat16:bfloat16.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:array_slice.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:cleanup.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:compactptrset.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:edit_distance.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:flatmap.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:flatrep.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:flatset.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:inlined_vector.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:int_type.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:iterator_range.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:manual_constructor.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:map_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:optional.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:priority_queue_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:stl_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:subtle/map_traits.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/gtl:top_n.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/random:distribution_sampler.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/random:exact_uniform_int.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/random:philox_random.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/random:philox_random_test_utils.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/random:random.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/random:random_distributions.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/random:simple_philox.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/random:weighted_picker.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/strings:base64.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/strings:numbers.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/strings:ordered_code.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/strings:proto_serialization.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/strings:proto_text_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/strings:scanner.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/strings:str_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/strings:strcat.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/strings:stringprintf.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2430:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal: please do not import '//tensorflow/core/lib/math:math_util.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2428:1: in linkstatic attribute of cc_library rule //tensorflow/core:lib_internal: setting 'linkstatic=1' is recommended if there are no object files
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:472:12: in srcs attribute of cc_library rule //tensorflow/core:logger: please do not import '//tensorflow/core/platform:logger.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2973:1: in srcs attribute of cc_library rule //tensorflow/core:stream_executor: please do not import '//tensorflow/core/platform:stream_executor.h' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'tf_cuda_library', the error might have been caused by the macro implementation in C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2973:1
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2511:12: in srcs attribute of cc_library rule //tensorflow/core:gif_internal: please do not import '//tensorflow/core/platform:gif.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/core/BUILD:2531:12: in srcs attribute of cc_library rule //tensorflow/core:jpeg_internal: please do not import '//tensorflow/core/platform:jpeg.h' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:3750:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: C:/tensorflow-build/tensorflow/tensorflow/contrib/metrics/BUILD:17:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:87:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: C:/tensorflow-build/tensorflow/tensorflow/contrib/learn/BUILD:16:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: C:/tensorflow-build/tensorflow/tensorflow/contrib/learn/BUILD:16:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: C:/tensorflow-build/tensorflow/tensorflow/contrib/bayesflow/BUILD:18:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: C:/tensorflow-build/tensorflow/tensorflow/contrib/BUILD:12:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (370 packages loaded, 18091 targets configured).
INFO: Found 1 target...
ERROR: C:/users/user/_bazel_user/j7bi4x5j/external/llvm/BUILD.bazel:3380:1: C++ compilation of rule '@llvm//:support' failed (Exit 2): cl.exe failed: error executing command
  cd C:/users/user/_bazel_user/j7bi4x5j/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt;C:\Program Files (x86)\Windows Kits\8.1\include\shared;C:\Program Files (x86)\Windows Kits\8.1\include\um;C:\Program Files (x86)\Windows Kits\8.1\include\winrt;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\WINDOWS\Microsoft.NET\Framework64\;C:\Program Files (x86)\Windows Kits\8.1\bin\x64;C:\Program Files (x86)\Windows Kits\8.1\bin\x86;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/User/Anaconda3/envs/tensorflow-build/python.exe
    SET PYTHON_LIB_PATH=C:/Users/User/Anaconda3/envs/tensorflow-build/lib/site-packages
    SET TEMP=C:\Users\User\AppData\Local\Temp
    SET TF2_BEHAVIOR=0
    SET TF_CONFIGURE_IOS=0
    SET TMP=C:\Users\User\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/llvm /Ibazel-out/x64_windows-opt/genfiles/external/llvm /Ibazel-out/x64_windows-opt/bin/external/llvm /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/llvm/include /Ibazel-out/x64_windows-opt/genfiles/external/llvm/include /Ibazel-out/x64_windows-opt/bin/external/llvm/include /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLLVM_ENABLE_STATS /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DLLVM_BUILD_GLOBAL_ISEL /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=""redacted"" -D__TIMESTAMP__=""redacted"" -D__TIME__=""redacted"" /Gy /Gw -w -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX -Zc:inline -Zc:strictStrings -Zc:rvalueCast -Oi -wd4141 -wd4146 -wd4180 -wd4244 -wd4258 -wd4267 -wd4291 -wd4345 -wd4351 -wd4355 -wd4456 -wd4457 -wd4458 -wd4459 -wd4503 -wd4624 -wd4722 -wd4800 -wd4100 -wd4127 -wd4512 -wd4505 -wd4610 -wd4510 -wd4702 -wd4245 -wd4706 -wd4310 -wd4701 -wd4703 -wd4389 -wd4611 -wd4805 -wd4204 -wd4577 -wd4091 -wd4592 -wd4319 -wd4324 -w14062 -we4238 /Fobazel-out/x64_windows-opt/bin/external/llvm/_objs/support/Timer.obj /c external/llvm/lib/Support/Timer.cpp
Execution platform: @bazel_tools//platforms:host_platform
external/llvm/include\llvm/Support/Compiler.h(79): fatal error C1189: #error:  LLVM requires at least MSVC 2017.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 20,792s, Critical Path: 0,72s
INFO: 0 processes.
FAILED: Build did NOT complete successfully


"
44362,`LLVM ERROR: Cannot select` issue when compiling LLVM module on s390x architecture,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.3.1
- Python version: 3.8.5
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): Ubuntu 9.3.0-17ubuntu1~20.04
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When running the test case `//tensorflow/python/keras/optimizer_v2:adam_test` on s390x machine, please see below for the full error stack. Apparently, it is an issue within the compiler code and the same issue is causing a lot of other test cases failures under `//tensorflow/compiler/...` test suite.

I investigated the issue a little bit, and I noticed that the error is thrown when executing this line https://github.com/tensorflow/tensorflow/blob/fcc4b966f1265f466e82617020af93670141b009/tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc#L178, i.e., when compiling the LLVM IR code into machine code. Since LLVM is supporting s390x architecture (as ""SystemZ"") and I manually merged the [commit](https://github.com/tensorflow/tensorflow/commit/a3196a68f5d85d6210fab3e2654d7463c62079bf) to include it into build target, it should be able to execute the compilation without any issue. Since the error message does not have much readable info about the error, I am kind of stuck in figuring out the cause of the issue. If anyone who is familiar with LLVM could help with debugging this issue, I will really appreciate it, thanks.

**Describe the expected behavior**
The compilation should work, and the test case should pass.

**Other info / logs** 
[adam_test.log](https://github.com/tensorflow/tensorflow/files/5447886/adam_test.log)
"
44361,tensorflow 2.3.1 issue causing python crash on macOS upon index update(PyCharm),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution macOS Catalina 10.15.7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from binary
- TensorFlow version v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.8.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

A problem occurs upon index update conducted by PyCharm which happens at PyCharm startup. It goes away by uninstalling tensorflow, however when I reinstall, the same problem recurs.

**Describe the expected behavior**

Python shouldn't crash

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
Just open pycharm after installing tf 2.3.1

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
Process:               Python [3010]
Path:                  /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/Resources/Python.app/Contents/MacOS/Python
Identifier:            Python
Version:               3.8.6 (3.8.6)
Code Type:             X86-64 (Native)
Parent Process:        Python [3003]
Responsible:           pycharm [1713]
User ID:               501

Date/Time:             2020-10-27 22:14:10.562 +0200
OS Version:            Mac OS X 10.15.7 (19H2)
Report Version:        12
Anonymous UUID:        EFCF7A5F-9255-DB43-BCFD-10EB5A7EDDB8

Sleep/Wake UUID:       B7EEB186-7CDE-40C5-961A-74850C2FCA28

Time Awake Since Boot: 11000 seconds
Time Since Wake:       4100 seconds

System Integrity Protection: enabled

Crashed Thread:        0  Dispatch queue: com.apple.main-thread

Exception Type:        EXC_CRASH (SIGABRT)
Exception Codes:       0x0000000000000000, 0x0000000000000000
Exception Note:        EXC_CORPSE_NOTIFY

Application Specific Information:
/usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/compiler/tf2xla/ops/_xla_ops.so
abort() called

Thread 0 Crashed:: Dispatch queue: com.apple.main-thread
0   libsystem_kernel.dylib        	0x00007fff7393733a __pthread_kill + 10
1   libsystem_pthread.dylib       	0x00007fff739f3e60 pthread_kill + 430
2   libsystem_c.dylib             	0x00007fff738be808 abort + 120
3   libtensorflow_framework.2.dylib	0x0000000131097c4c tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 44
4   libtensorflow_framework.2.dylib	0x0000000131097c80 tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 16
5   libtensorflow_framework.2.dylib	0x0000000130bd164f tensorflow::OpRegistry::Register(std::__1::function<tensorflow::Status (tensorflow::OpRegistrationData*)> const&) + 191
6   libtensorflow_framework.2.dylib	0x0000000130bd362b tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&) + 123
7   _xla_ops.so                   	0x00000001415071be _GLOBAL__sub_I_xla_ops.cc + 782
8   dyld                          	0x00000001111341d3 ImageLoaderMachO::doModInitFunctions(ImageLoader::LinkContext const&) + 535
9   dyld                          	0x00000001111345de ImageLoaderMachO::doInitialization(ImageLoader::LinkContext const&) + 40
10  dyld                          	0x000000011112effb ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, char const*, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 493
11  dyld                          	0x000000011112d0b4 ImageLoader::processInitializers(ImageLoader::LinkContext const&, unsigned int, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 188
12  dyld                          	0x000000011112d154 ImageLoader::runInitializers(ImageLoader::LinkContext const&, ImageLoader::InitializerTimingList&) + 82
13  dyld                          	0x000000011111eef2 dyld::runInitializers(ImageLoader*) + 82
14  dyld                          	0x0000000111128e8f dlopen_internal + 609
15  libdyld.dylib                 	0x00007fff737dad8a dlopen + 171
16  org.python.python             	0x000000010f64f8b7 _PyImport_FindSharedFuncptr + 288
17  org.python.python             	0x000000010f62962b _PyImport_LoadDynamicModuleWithSpec + 540
18  org.python.python             	0x000000010f629007 _imp_create_dynamic + 291
19  org.python.python             	0x000000010f59c3a9 cfunction_vectorcall_FASTCALL + 169
20  org.python.python             	0x000000010f56dd09 PyVectorcall_Call + 108
21  org.python.python             	0x000000010f60b7dc _PyEval_EvalFrameDefault + 31274
22  org.python.python             	0x000000010f60f197 _PyEval_EvalCodeWithName + 1947
23  org.python.python             	0x000000010f56e427 _PyFunction_Vectorcall + 227
24  org.python.python             	0x000000010f60e67d call_function + 346
25  org.python.python             	0x000000010f60b257 _PyEval_EvalFrameDefault + 29861
26  org.python.python             	0x000000010f56e2c2 function_code_fastcall + 106
27  org.python.python             	0x000000010f60e67d call_function + 346
28  org.python.python             	0x000000010f60b23b _PyEval_EvalFrameDefault + 29833
29  org.python.python             	0x000000010f56e2c2 function_code_fastcall + 106
30  org.python.python             	0x000000010f60e67d call_function + 346
31  org.python.python             	0x000000010f60b314 _PyEval_EvalFrameDefault + 30050
32  org.python.python             	0x000000010f56e2c2 function_code_fastcall + 106
33  org.python.python             	0x000000010f60e67d call_function + 346
34  org.python.python             	0x000000010f60b314 _PyEval_EvalFrameDefault + 30050
35  org.python.python             	0x000000010f56e2c2 function_code_fastcall + 106
36  org.python.python             	0x000000010f60e67d call_function + 346
37  org.python.python             	0x000000010f60b314 _PyEval_EvalFrameDefault + 30050
38  org.python.python             	0x000000010f56e2c2 function_code_fastcall + 106
39  org.python.python             	0x000000010f56f75a object_vacall + 352
40  org.python.python             	0x000000010f56f89d _PyObject_CallMethodIdObjArgs + 219
41  org.python.python             	0x000000010f62815f PyImport_ImportModuleLevelObject + 1577
42  org.python.python             	0x000000010f600785 builtin___import__ + 122
43  org.python.python             	0x000000010f56df8e cfunction_call_varargs + 171
44  org.python.python             	0x000000010f56da7d _PyObject_MakeTpCall + 274
45  org.python.python             	0x000000010f60e847 call_function + 804
46  org.python.python             	0x000000010f60b314 _PyEval_EvalFrameDefault + 30050
47  org.python.python             	0x000000010f60f197 _PyEval_EvalCodeWithName + 1947
48  org.python.python             	0x000000010f56e427 _PyFunction_Vectorcall + 227
49  org.python.python             	0x000000010f56dd09 PyVectorcall_Call + 108
50  org.python.python             	0x000000010f60b5d4 _PyEval_EvalFrameDefault + 30754
51  org.python.python             	0x000000010f60f197 _PyEval_EvalCodeWithName + 1947
52  org.python.python             	0x000000010f56e427 _PyFunction_Vectorcall + 227
53  org.python.python             	0x000000010f56dd09 PyVectorcall_Call + 108
54  org.python.python             	0x000000010f60b5d4 _PyEval_EvalFrameDefault + 30754
55  org.python.python             	0x000000010f56e2c2 function_code_fastcall + 106
56  org.python.python             	0x000000010f60e67d call_function + 346
57  org.python.python             	0x000000010f60b23b _PyEval_EvalFrameDefault + 29833
58  org.python.python             	0x000000010f60f197 _PyEval_EvalCodeWithName + 1947
59  org.python.python             	0x000000010f56e427 _PyFunction_Vectorcall + 227
60  org.python.python             	0x000000010f60e67d call_function + 346
61  org.python.python             	0x000000010f60b23b _PyEval_EvalFrameDefault + 29833
62  org.python.python             	0x000000010f56e2c2 function_code_fastcall + 106
63  org.python.python             	0x000000010f60e67d call_function + 346
64  org.python.python             	0x000000010f60b314 _PyEval_EvalFrameDefault + 30050
65  org.python.python             	0x000000010f60f197 _PyEval_EvalCodeWithName + 1947
66  org.python.python             	0x000000010f56e427 _PyFunction_Vectorcall + 227
67  org.python.python             	0x000000010f60e67d call_function + 346
68  org.python.python             	0x000000010f60b3df _PyEval_EvalFrameDefault + 30253
69  org.python.python             	0x000000010f60f197 _PyEval_EvalCodeWithName + 1947
70  org.python.python             	0x000000010f603d0f PyEval_EvalCode + 51
71  org.python.python             	0x000000010f63cf9d run_eval_code_obj + 102
72  org.python.python             	0x000000010f63c3ec run_mod + 82
73  org.python.python             	0x000000010f63b3ec PyRun_StringFlags + 120
74  org.python.python             	0x000000010f63b337 PyRun_SimpleStringFlags + 69
75  org.python.python             	0x000000010f651d91 Py_RunMain + 424
76  org.python.python             	0x000000010f65267e pymain_main + 306
77  org.python.python             	0x000000010f6526cc Py_BytesMain + 42
78  libdyld.dylib                 	0x00007fff737efcc9 start + 1

Thread 1:
0   libsystem_kernel.dylib        	0x00007fff73933882 __psynch_cvwait + 10
1   libsystem_pthread.dylib       	0x00007fff739f4425 _pthread_cond_wait + 698
2   libopenblasp-r0.3.7.dylib     	0x000000013b485c3b blas_thread_server + 619
3   libsystem_pthread.dylib       	0x00007fff739f4109 _pthread_start + 148
4   libsystem_pthread.dylib       	0x00007fff739efb8b thread_start + 15

Thread 0 crashed with X86 Thread State (64-bit):
  rax: 0x0000000000000000  rbx: 0x00000001111e3dc0  rcx: 0x00007ffee06b6718  rdx: 0x0000000000000000
  rdi: 0x0000000000000307  rsi: 0x0000000000000006  rbp: 0x00007ffee06b6740  rsp: 0x00007ffee06b6718
   r8: 0x000000000000015a   r9: 0x0000000000000007  r10: 0x00000001111e3dc0  r11: 0x0000000000000246
  r12: 0x0000000000000307  r13: 0x00007ffee06b6af0  r14: 0x0000000000000006  r15: 0x0000000000000016
  rip: 0x00007fff7393733a  rfl: 0x0000000000000246  cr2: 0x00000001305f4005
  
Logical CPU:     0
Error Code:      0x02000148
Trap Number:     133


Binary Images:
       0x10f542000 -        0x10f545fff +org.python.python (3.8.6 - 3.8.6) <4F8ACF44-9B66-372B-90AA-C97286D786A8> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/Resources/Python.app/Contents/MacOS/Python
       0x10f551000 -        0x10f728fff +org.python.python (3.8.6, [c] 2001-2019 Python Software Foundation. - 3.8.6) <731774E1-F4F4-3BD1-B103-61340A56CAF0> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/Python
       0x10f9b5000 -        0x10f9b8fff +_heapq.cpython-38-darwin.so (0) <51B3B4E3-CBB1-3C92-A651-9A3AE7C37B7F> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_heapq.cpython-38-darwin.so
       0x10fb05000 -        0x10fb0cff3 +_struct.cpython-38-darwin.so (0) <B4AF514F-84FA-374A-A2AE-E7D29EE71F5E> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_struct.cpython-38-darwin.so
       0x10fb19000 -        0x10fb28ff7 +_pickle.cpython-38-darwin.so (0) <DA84C88C-D1DF-325B-B593-A30C8CB558D9> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_pickle.cpython-38-darwin.so
       0x10fb79000 -        0x10fb88ff3 +_socket.cpython-38-darwin.so (0) <E3FB0F8D-A942-36E3-B83B-F63332BC5490> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_socket.cpython-38-darwin.so
       0x10fb95000 -        0x10fb9cffb +math.cpython-38-darwin.so (0) <2BA5E392-386D-3147-A53A-302E8C99A6DA> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/math.cpython-38-darwin.so
       0x10fba9000 -        0x10fbb0ff7 +select.cpython-38-darwin.so (0) <67CAC50E-5966-3332-9829-B27B9161C5B4> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/select.cpython-38-darwin.so
       0x10fbbd000 -        0x10fbc4ff3 +array.cpython-38-darwin.so (0) <B43E8134-3561-38B4-AF15-B3FFB8D84848> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/array.cpython-38-darwin.so
       0x10fc11000 -        0x10fc14ff3 +_posixsubprocess.cpython-38-darwin.so (0) <53691B1D-780C-3DD6-9626-309199E1BE10> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_posixsubprocess.cpython-38-darwin.so
       0x10fc21000 -        0x10fc24ffb +_multiprocessing.cpython-38-darwin.so (0) <172A618C-1A06-3FF3-B73A-BE1E6D8944C4> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_multiprocessing.cpython-38-darwin.so
       0x10fc31000 -        0x10fc34ff7 +_posixshmem.cpython-38-darwin.so (0) <FEB4733A-DD59-332B-BBE2-BBEC2697D3E4> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_posixshmem.cpython-38-darwin.so
       0x10fcc1000 -        0x10fcc8ffb +zlib.cpython-38-darwin.so (0) <356E4220-DD01-3C05-8B97-DCE7E92304F4> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/zlib.cpython-38-darwin.so
       0x10fcd5000 -        0x10fcd8fff +_bz2.cpython-38-darwin.so (0) <EDC4EBFA-A15E-3747-B90D-8E593DC3C999> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_bz2.cpython-38-darwin.so
       0x10fce5000 -        0x10fcecfff +_lzma.cpython-38-darwin.so (0) <A70E8FD5-D4C5-3DFD-B4AF-95E4DA8695ED> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_lzma.cpython-38-darwin.so
       0x10fcf9000 -        0x10fd14fff +liblzma.5.dylib (0) <E4406E42-7BC4-3945-A1A4-E9B6874EF052> /usr/local/opt/xz/lib/liblzma.5.dylib
       0x10fd1b000 -        0x10fd1effb +grp.cpython-38-darwin.so (0) <3A609192-DECA-3783-A061-A23741A53440> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/grp.cpython-38-darwin.so
       0x10fd6b000 -        0x10fd72ffb +_json.cpython-38-darwin.so (0) <9C43A119-F0CD-34D8-89EE-8FBDA07F5C5B> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_json.cpython-38-darwin.so
       0x10fdbf000 -        0x10fdc6fff +_hashlib.cpython-38-darwin.so (0) <EDC76022-E1A3-3A36-A506-9DDC4892EAF8> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_hashlib.cpython-38-darwin.so
       0x10fdd3000 -        0x10fe22ff7 +libssl.1.1.dylib (0) <47223953-51A0-3BBA-9098-75BF3B9E5853> /usr/local/opt/openssl@1.1/lib/libssl.1.1.dylib
       0x10fe4f000 -        0x10fe56ff3 +_blake2.cpython-38-darwin.so (0) <93180342-0E6B-3915-B5F9-E44CE532B9ED> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_blake2.cpython-38-darwin.so
       0x10fe63000 -        0x10fe76ffb +_sha3.cpython-38-darwin.so (0) <4C828006-3E90-3F30-87AE-81DE2C260E84> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_sha3.cpython-38-darwin.so
       0x10fe83000 -        0x10fe86ff7 +_opcode.cpython-38-darwin.so (0) <C307C1C4-DE08-367A-BF49-F98B2B4B66D7> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_opcode.cpython-38-darwin.so
       0x10fe93000 -        0x10fe96fff +_bisect.cpython-38-darwin.so (0) <33A4D2EF-8A3B-3A7B-9931-51D5D95ECF05> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_bisect.cpython-38-darwin.so
       0x10fea3000 -        0x10feaafff +_sha512.cpython-38-darwin.so (0) <BA2E8F16-3CD6-325F-8328-3CA749200AF9> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_sha512.cpython-38-darwin.so
       0x10feb7000 -        0x10febaff3 +_random.cpython-38-darwin.so (0) <B2D321A0-CEB0-36DB-AF30-23D5DA34C4B0> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_random.cpython-38-darwin.so
       0x10fec7000 -        0x10fedaff7 +_ctypes.cpython-38-darwin.so (0) <68E0FC90-CBCC-3E03-B974-DF234CFB42C3> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_ctypes.cpython-38-darwin.so
       0x10feeb000 -        0x10feeefff +termios.cpython-38-darwin.so (0) <634E9969-F009-3FFA-A4B7-B068B55F127F> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/termios.cpython-38-darwin.so
       0x10fefb000 -        0x10fefbff3 +_api_implementation.cpython-38-darwin.so (0) <493330A1-C47F-3ADD-8ACD-EF866819892B> /usr/local/Cellar/protobuf/3.13.0/libexec/lib/python3.8/site-packages/google/protobuf/internal/_api_implementation.cpython-38-darwin.so
       0x10ff3f000 -        0x1100feb3f +libcrypto.1.1.dylib (0) <B07413C6-70F9-330D-9D30-16208A090C5B> /usr/local/opt/openssl@1.1/lib/libcrypto.1.1.dylib
       0x1101d7000 -        0x1101deffb +_csv.cpython-38-darwin.so (0) <8BF679CB-9B09-3662-9708-53758FF09A23> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_csv.cpython-38-darwin.so
       0x11022b000 -        0x11022eff3 +fcntl.cpython-38-darwin.so (0) <711873A6-5688-3D99-B418-2F8C1268A17F> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/fcntl.cpython-38-darwin.so
       0x11023b000 -        0x11024aff3 +_datetime.cpython-38-darwin.so (0) <B5F7B11B-1744-361B-8EB0-F33C6268B6CB> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_datetime.cpython-38-darwin.so
       0x110397000 -        0x1103acff7 +libgcc_s.1.dylib (0) <7C6D7CB7-82DB-3290-8181-07646FEA1F80> /usr/local/lib/python3.8/site-packages/numpy/.dylibs/libgcc_s.1.dylib
       0x1103b7000 -        0x1103c4ff7 +_multiarray_tests.cpython-38-darwin.so (0) <ACCA8133-D875-3786-9491-E3E8F50AE677> /usr/local/lib/python3.8/site-packages/numpy/core/_multiarray_tests.cpython-38-darwin.so
       0x110417000 -        0x110775fe7 +_multiarray_umath.cpython-38-darwin.so (0) <4D269D8B-E945-3FAB-9B27-071C6EDD79D1> /usr/local/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so
       0x110889000 -        0x1109a0ff7 +libgfortran.3.dylib (0) <9ABE5EDE-AD43-391A-9E54-866711FAC32A> /usr/local/lib/python3.8/site-packages/numpy/.dylibs/libgfortran.3.dylib
       0x110a04000 -        0x110a3afff +libquadmath.0.dylib (0) <7FFA409F-FB04-3B64-BE9A-3E3A494C975E> /usr/local/lib/python3.8/site-packages/numpy/.dylibs/libquadmath.0.dylib
       0x110c09000 -        0x110c0afff +lapack_lite.cpython-38-darwin.so (0) <321E5F11-D3E6-380B-8CEE-2E6806AE89CA> /usr/local/lib/python3.8/site-packages/numpy/linalg/lapack_lite.cpython-38-darwin.so
       0x110c0e000 -        0x110c27ffb +_umath_linalg.cpython-38-darwin.so (0) <1F7A4336-6B80-3409-ADCE-C7DBD8D5477D> /usr/local/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-darwin.so
       0x110cf6000 -        0x110d29fff +_decimal.cpython-38-darwin.so (0) <26BDD26C-0DAC-3876-82F0-57CD8EB6E037> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_decimal.cpython-38-darwin.so
       0x110d7e000 -        0x110d8fffb +_pocketfft_internal.cpython-38-darwin.so (0) <F3F11219-6ACF-3689-A48E-969559212D9A> /usr/local/lib/python3.8/site-packages/numpy/fft/_pocketfft_internal.cpython-38-darwin.so
       0x110dd3000 -        0x110e3dfff +mtrand.cpython-38-darwin.so (0) <EDDA7534-B549-308F-BB96-F8452FDDBB2C> /usr/local/lib/python3.8/site-packages/numpy/random/mtrand.cpython-38-darwin.so
       0x110e91000 -        0x110eaefff +_bit_generator.cpython-38-darwin.so (0) <19109E76-8BCB-3B51-90E8-F9B0C8B7BF74> /usr/local/lib/python3.8/site-packages/numpy/random/_bit_generator.cpython-38-darwin.so
       0x110ec9000 -        0x110ef8ff3 +_common.cpython-38-darwin.so (0) <D290B4A9-BC33-3FF3-8F41-28D2CF3EF277> /usr/local/lib/python3.8/site-packages/numpy/random/_common.cpython-38-darwin.so
       0x110f0d000 -        0x110f14ff3 +binascii.cpython-38-darwin.so (0) <FE2B1D01-120D-35DE-BF46-8DEF395E46EA> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/binascii.cpython-38-darwin.so
       0x110f21000 -        0x110f75fff +_bounded_integers.cpython-38-darwin.so (0) <8CB882D6-79D2-3A47-88FE-92A3F9173CFC> /usr/local/lib/python3.8/site-packages/numpy/random/_bounded_integers.cpython-38-darwin.so
       0x110f97000 -        0x110fa9ff7 +_mt19937.cpython-38-darwin.so (0) <2795FA1D-CD55-3496-98CD-49DAC2CD4F21> /usr/local/lib/python3.8/site-packages/numpy/random/_mt19937.cpython-38-darwin.so
       0x110fb5000 -        0x110fc1ffb +_philox.cpython-38-darwin.so (0) <BC7653EE-D166-31F6-8CC9-44B7DA001817> /usr/local/lib/python3.8/site-packages/numpy/random/_philox.cpython-38-darwin.so
       0x110fcc000 -        0x110fd6fff +_pcg64.cpython-38-darwin.so (0) <0C4DE74B-83B2-341F-89F2-8256CB73D13D> /usr/local/lib/python3.8/site-packages/numpy/random/_pcg64.cpython-38-darwin.so
       0x110fe1000 -        0x110fe8ff3 +_sfc64.cpython-38-darwin.so (0) <1377DE30-C98F-393B-A0D8-1736FF284C59> /usr/local/lib/python3.8/site-packages/numpy/random/_sfc64.cpython-38-darwin.so
       0x110ff2000 -        0x111073ff7 +_generator.cpython-38-darwin.so (0) <ECFF10CC-C3D7-302D-9D37-F8C1B7E0BB24> /usr/local/lib/python3.8/site-packages/numpy/random/_generator.cpython-38-darwin.so
       0x11110c000 -        0x11110effb +_SHA256.cpython-38-darwin.so (0) <C2CD768E-6FCF-3EE5-8AD3-C9E55AB9EDF9> /usr/local/lib/python3.8/site-packages/Crypto/Hash/_SHA256.cpython-38-darwin.so
       0x111111000 -        0x111112ffb +libsz.2.dylib (0) <C3E13A34-DBC5-3EBD-85E2-2E68FAE23D68> /usr/local/lib/python3.8/site-packages/h5py/.dylibs/libsz.2.dylib
       0x111115000 -        0x111116fff +libXau.6.dylib (0) <F40D7B27-9464-30FC-AC72-204164AFEE8D> /usr/local/lib/python3.8/site-packages/PIL/.dylibs/libXau.6.dylib
       0x111119000 -        0x1111aaf47  dyld (750.6) <1D318D60-C9B0-3511-BE9C-82AFD2EF930D> /usr/lib/dyld
       0x11121e000 -        0x114c89ae7 +libopenblasp-r0.3.7.dylib (0) <9914A383-F8C9-3559-BC88-B4DD28689BC5> /usr/local/lib/python3.8/site-packages/numpy/.dylibs/libopenblasp-r0.3.7.dylib
       0x116f89000 -        0x116facff3 +pyexpat.cpython-38-darwin.so (0) <4FBBB0B6-C3E6-3440-8F2C-46D09EF28EEB> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/pyexpat.cpython-38-darwin.so
       0x1172fd000 -        0x117323fff +_message.cpython-38-darwin.so (0) <5D9A7A3F-E8AF-33AF-8640-C693DFC760BF> /usr/local/Cellar/protobuf/3.13.0/libexec/lib/python3.8/site-packages/google/protobuf/pyext/_message.cpython-38-darwin.so
       0x11736b000 -        0x1174acfeb +libprotobuf.24.dylib (0) <73225B4E-CE33-38D7-B9EF-4916EE5C9594> /usr/local/Cellar/protobuf/3.13.0/lib/libprotobuf.24.dylib
       0x1175ef000 -        0x123603aff +_pywrap_tensorflow_internal.so (0) <E5B5423E-6634-38EB-83AC-0B3AF250AA62> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_tensorflow_internal.so
       0x130411000 -        0x13157e83f +libtensorflow_framework.2.dylib (0) <7A1D4E69-89F0-3CF3-9354-D50AC25EC423> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/libtensorflow_framework.2.dylib
       0x132005000 -        0x132175ff7 +_pywrap_tfe.so (0) <DA36B49A-9481-3BC3-9F89-57174C61CCBF> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_tfe.so
       0x13223d000 -        0x132397fef +_pywrap_tf_session.so (0) <483D0B9E-F8EF-30A7-ABA3-E121D775D2C8> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_tf_session.so
       0x132410000 -        0x132434ff7 +_tf_stack.so (0) <7AA43485-C7F2-3693-9E2A-4E78B37AFF08> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_tf_stack.so
       0x13248c000 -        0x132490fff +_wrappers.cpython-38-darwin.so (0) <9ACF8142-8E4D-3770-B695-51095FE7C41E> /usr/local/lib/python3.8/site-packages/wrapt/_wrappers.cpython-38-darwin.so
       0x1324db000 -        0x1324eeff3 +_pywrap_utils.so (0) <4EB215EE-101A-3715-9E5E-03FC74BF780B> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_utils.so
       0x13253b000 -        0x13257affb +_pywrap_py_exception_registry.so (0) <64BF6907-CC12-3D61-BF5C-DADC04FFF409> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_py_exception_registry.so
       0x1325dc000 -        0x1325ebffb +_pywrap_bfloat16.so (0) <2F4AEF20-0358-37B6-A92D-B2007392FCF9> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_bfloat16.so
       0x1325f4000 -        0x132620ff7 +_dtypes.so (0) <9FDFDDDB-428D-33BB-87C0-FBE2B04FC8E5> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_dtypes.so
       0x1326fa000 -        0x132709ff7 +fast_tensor_util.so (0) <AC1CB9EA-2A88-3D47-A2DC-0A319431698C> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/framework/fast_tensor_util.so
       0x132712000 -        0x132832fe3 +_pywrap_traceme.so (0) <6A2A5A40-6CB0-3BC8-BA96-937CCA6431B8> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/profiler/internal/_pywrap_traceme.so
       0x1328b9000 -        0x1329d4ff7 +_op_def_registry.so (0) <07938B6E-DFEE-32AB-9BAF-6F20E5FF12CD> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_op_def_registry.so
       0x132f19000 -        0x132f1cff3 +_queue.cpython-38-darwin.so (0) <CC3308A3-8C7B-3697-8D1D-805536A72BB9> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_queue.cpython-38-darwin.so
       0x132f69000 -        0x13308afff +_pywrap_events_writer.so (0) <E003AE8C-48F4-3792-BBDB-DAD756043BEB> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_events_writer.so
       0x1330d2000 -        0x1330d5fff +_uuid.cpython-38-darwin.so (0) <D23D3A7B-479B-3341-A800-4BDA8DB1EF61> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_uuid.cpython-38-darwin.so
       0x1330e2000 -        0x13320bfff +_pywrap_file_io.so (0) <3BF60581-AF2B-3291-9D65-477F98A173EF> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_file_io.so
       0x133259000 -        0x13383ea77 +_pywrap_profiler.so (0) <D922CD87-17E9-3D11-8405-8D5454213716> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/profiler/internal/_pywrap_profiler.so
       0x133d45000 -        0x133e6effb +_pywrap_record_io.so (0) <4A814088-C2B0-356C-978B-E65690C3B828> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_record_io.so
       0x133efd000 -        0x13402cff3 +_pywrap_checkpoint_reader.so (0) <8B567F90-67AE-3110-9F80-A1E9FF128C48> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_checkpoint_reader.so
       0x13417f000 -        0x13418effb +_pywrap_py_func.so (0) <40D3E2BC-AE52-34C8-A8A0-BF12724AA875> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_py_func.so
       0x134217000 -        0x13433afeb +_pywrap_server_lib.so (0) <49EF72BB-141A-3386-91DF-2CF3D2D9F482> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/data/experimental/service/_pywrap_server_lib.so
       0x134503000 -        0x134512ff3 +_pywrap_tf32_execution.so (0) <51B4EEFE-F417-31D6-9D3A-64A7265A8960> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_tf32_execution.so
       0x13459b000 -        0x1345aefff +_ssl.cpython-38-darwin.so (0) <2E0EB6F1-9289-3B73-944E-8891AA50A064> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_ssl.cpython-38-darwin.so
       0x134b03000 -        0x134e53a7b +_openssl.abi3.so (0) <D1458ACA-8DFA-35C5-866D-7F23AB4E888F> /usr/local/lib/python3.8/site-packages/cryptography/hazmat/bindings/_openssl.abi3.so
       0x134f75000 -        0x134f93ffb +_cffi_backend.cpython-38-darwin.so (0) <4652037D-440F-385E-90E7-2B88EB0FEB0C> /usr/local/lib/python3.8/site-packages/_cffi_backend.cpython-38-darwin.so
       0x135228000 -        0x135283fc7 +libgmp.10.dylib (0) <6DACA437-97FF-3C9A-8F1A-6542952C4DD5> /usr/local/Cellar/gmp/6.2.0/lib/libgmp.10.dylib
       0x1353d0000 -        0x1354ebfff +_pywrap_device_lib.so (0) <294D611F-5AB7-3920-A327-7646239D7B24> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_device_lib.so
       0x1356f0000 -        0x1356f3fff +_contextvars.cpython-38-darwin.so (0) <13EA0FC0-40B7-3670-8E7B-49F28B501D0B> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_contextvars.cpython-38-darwin.so
       0x135700000 -        0x135707fff +_asyncio.cpython-38-darwin.so (0) <911367E3-899B-30C3-8642-E41D10F042B3> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_asyncio.cpython-38-darwin.so
       0x135e98000 -        0x135e9efff +_errors.cpython-38-darwin.so (0) <57F55A50-23B0-368C-98C4-70667BD5BF40> /usr/local/lib/python3.8/site-packages/h5py/_errors.cpython-38-darwin.so
       0x135ea6000 -        0x1363c0fff +libhdf5.103.dylib (0) <753861C4-5702-3761-9242-88FCD8F6E068> /usr/local/lib/python3.8/site-packages/h5py/.dylibs/libhdf5.103.dylib
       0x136414000 -        0x136434fff +libhdf5_hl.100.dylib (0) <BEA67564-0F2E-30EC-9FF3-601F270688D3> /usr/local/lib/python3.8/site-packages/h5py/.dylibs/libhdf5_hl.100.dylib
       0x13643d000 -        0x136443ff7 +libaec.0.dylib (0) <6D76AD2E-74FD-326D-ADE4-68C426692120> /usr/local/lib/python3.8/site-packages/h5py/.dylibs/libaec.0.dylib
       0x136446000 -        0x136455fff +h5.cpython-38-darwin.so (0) <C7DEEC69-10E8-31A6-A24A-DE6EFD94EAB6> /usr/local/lib/python3.8/site-packages/h5py/h5.cpython-38-darwin.so
       0x136465000 -        0x136483ff3 +defs.cpython-38-darwin.so (0) <D57175A0-D6B4-3AE9-B62D-F3CC184B88B4> /usr/local/lib/python3.8/site-packages/h5py/defs.cpython-38-darwin.so
       0x1364dd000 -        0x1364f0ff3 +_objects.cpython-38-darwin.so (0) <9298C88A-2A3D-355B-B885-698FA5A1CF43> /usr/local/lib/python3.8/site-packages/h5py/_objects.cpython-38-darwin.so
       0x136500000 -        0x13650aff3 +_conv.cpython-38-darwin.so (0) <00D8A992-482B-3DD6-8F13-384B504485B3> /usr/local/lib/python3.8/site-packages/h5py/_conv.cpython-38-darwin.so
       0x136513000 -        0x13651bffb +h5r.cpython-38-darwin.so (0) <595813B3-09F3-33F8-93C8-AF2BDF9E2E22> /usr/local/lib/python3.8/site-packages/h5py/h5r.cpython-38-darwin.so
       0x136525000 -        0x13657bfff +h5t.cpython-38-darwin.so (0) <C245AA61-5B95-367A-A5E6-E9A8204661A9> /usr/local/lib/python3.8/site-packages/h5py/h5t.cpython-38-darwin.so
       0x1365b6000 -        0x1365bbff3 +utils.cpython-38-darwin.so (0) <F1320153-4058-3FEC-9A5C-174CDC147F85> /usr/local/lib/python3.8/site-packages/h5py/utils.cpython-38-darwin.so
       0x1365c2000 -        0x1365c7ffb +h5z.cpython-38-darwin.so (0) <3723B76B-3155-324D-97C2-D8EEC2F0A053> /usr/local/lib/python3.8/site-packages/h5py/h5z.cpython-38-darwin.so
       0x1365d0000 -        0x1365e4ff7 +h5a.cpython-38-darwin.so (0) <A92B9D14-28EF-316A-B6F5-E327CD9B3155> /usr/local/lib/python3.8/site-packages/h5py/h5a.cpython-38-darwin.so
       0x1365f6000 -        0x136607fff +h5s.cpython-38-darwin.so (0) <6E62FCBB-6F53-3721-B209-CEDCB170D484> /usr/local/lib/python3.8/site-packages/h5py/h5s.cpython-38-darwin.so
       0x136618000 -        0x13664fffb +h5p.cpython-38-darwin.so (0) <A5F17D1A-6064-3DA2-A94B-3B478FA37912> /usr/local/lib/python3.8/site-packages/h5py/h5p.cpython-38-darwin.so
       0x1366bf000 -        0x1366c5ff3 +h5ac.cpython-38-darwin.so (0) <6CC0E2A0-4CBB-34E8-B9FB-8C2C666CE47E> /usr/local/lib/python3.8/site-packages/h5py/h5ac.cpython-38-darwin.so
       0x1366cd000 -        0x1366d3ff3 +_proxy.cpython-38-darwin.so (0) <D03E1672-5FFB-3265-97B4-FA2A9BE3D74C> /usr/local/lib/python3.8/site-packages/h5py/_proxy.cpython-38-darwin.so
       0x1366d9000 -        0x1366ecffb +h5d.cpython-38-darwin.so (0) <BE8C435B-FB03-3F3C-895F-B47EE0139AF3> /usr/local/lib/python3.8/site-packages/h5py/h5d.cpython-38-darwin.so
       0x1366fd000 -        0x136708ffb +h5ds.cpython-38-darwin.so (0) <273A2066-5634-3B3A-8AE1-A1ADA939E953> /usr/local/lib/python3.8/site-packages/h5py/h5ds.cpython-38-darwin.so
       0x136714000 -        0x136726fff +h5f.cpython-38-darwin.so (0) <BEBEE29F-5FB4-320D-B01F-B5A92D070439> /usr/local/lib/python3.8/site-packages/h5py/h5f.cpython-38-darwin.so
       0x13673a000 -        0x136753fff +h5g.cpython-38-darwin.so (0) <F552936C-B41A-3E6F-A34A-53FE8710FE4F> /usr/local/lib/python3.8/site-packages/h5py/h5g.cpython-38-darwin.so
       0x136768000 -        0x13676ffff +h5i.cpython-38-darwin.so (0) <EF58DE3A-9AB0-3824-AB86-3DBA052CE993> /usr/local/lib/python3.8/site-packages/h5py/h5i.cpython-38-darwin.so
       0x136778000 -        0x136793ff3 +h5fd.cpython-38-darwin.so (0) <63EFE497-964F-3AAB-B5C8-F81C6105B82F> /usr/local/lib/python3.8/site-packages/h5py/h5fd.cpython-38-darwin.so
       0x1367a7000 -        0x1367abffb +h5pl.cpython-38-darwin.so (0) <D240146D-735A-3C08-B2D5-B650A1475EF5> /usr/local/lib/python3.8/site-packages/h5py/h5pl.cpython-38-darwin.so
       0x1367f1000 -        0x136800ff3 +h5o.cpython-38-darwin.so (0) <D9417DF8-F0BA-3AB2-A83B-6D892C125DF3> /usr/local/lib/python3.8/site-packages/h5py/h5o.cpython-38-darwin.so
       0x136811000 -        0x136820fff +h5l.cpython-38-darwin.so (0) <C4CF8BDC-A0FE-3383-84EA-59CB2FDAF084> /usr/local/lib/python3.8/site-packages/h5py/h5l.cpython-38-darwin.so
       0x1369ae000 -        0x1369b1fff +_scproxy.cpython-38-darwin.so (0) <6C197CEE-7A8A-32A6-9B5D-CC7FDD475662> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_scproxy.cpython-38-darwin.so
       0x136b7e000 -        0x136c85ff7 +unicodedata.cpython-38-darwin.so (0) <6AC22301-2BDE-31B2-B314-871AC633674E> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/unicodedata.cpython-38-darwin.so
       0x136dd2000 -        0x136ddeffb +_ccallback_c.cpython-38-darwin.so (0) <9C28AB0B-8909-36C5-91B7-68D0781FB0D3> /usr/local/lib/python3.8/site-packages/scipy/_lib/_ccallback_c.cpython-38-darwin.so
       0x136dea000 -        0x136df0fff +_uarray.cpython-38-darwin.so (0) <564F92D1-1AF0-384A-B250-A790F1DDC8ED> /usr/local/lib/python3.8/site-packages/scipy/_lib/_uarray/_uarray.cpython-38-darwin.so
       0x136df8000 -        0x136ea5ff7 +pypocketfft.cpython-38-darwin.so (0) <E55B73C5-4C27-3AEF-B369-04D78A3B244A> /usr/local/lib/python3.8/site-packages/scipy/fft/_pocketfft/pypocketfft.cpython-38-darwin.so
       0x136fa6000 -        0x1373defff +_sparsetools.cpython-38-darwin.so (0) <3C58751A-7827-3410-9625-8A759545899A> /usr/local/lib/python3.8/site-packages/scipy/sparse/_sparsetools.cpython-38-darwin.so
       0x1374bd000 -        0x137521ff7 +_csparsetools.cpython-38-darwin.so (0) <5CB63C2A-479C-372D-A43F-C6B512810613> /usr/local/lib/python3.8/site-packages/scipy/sparse/_csparsetools.cpython-38-darwin.so
       0x13758d000 -        0x1375daff7 +_shortest_path.cpython-38-darwin.so (0) <6C48E97C-3940-333D-A9E4-031B97D4C3D2> /usr/local/lib/python3.8/site-packages/scipy/sparse/csgraph/_shortest_path.cpython-38-darwin.so
       0x137603000 -        0x13761effb +_tools.cpython-38-darwin.so (0) <908680BE-1C75-311B-84A9-0F11E6C5BF40> /usr/local/lib/python3.8/site-packages/scipy/sparse/csgraph/_tools.cpython-38-darwin.so
       0x137634000 -        0x13764eff7 +_traversal.cpython-38-darwin.so (0) <3237DF82-2144-300C-9521-388E66032A29> /usr/local/lib/python3.8/site-packages/scipy/sparse/csgraph/_traversal.cpython-38-darwin.so
       0x13765e000 -        0x13767cff7 +_min_spanning_tree.cpython-38-darwin.so (0) <52B1E142-B285-37BF-9246-53E1FB513246> /usr/local/lib/python3.8/site-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-38-darwin.so
       0x137691000 -        0x1376baff7 +_flow.cpython-38-darwin.so (0) <9A79DD63-A094-34C1-A00A-2D46F5B0E699> /usr/local/lib/python3.8/site-packages/scipy/sparse/csgraph/_flow.cpython-38-darwin.so
       0x1376d6000 -        0x1376f6fff +_matching.cpython-38-darwin.so (0) <36D7C078-6ACC-361B-B59F-CCB3F8AA18E8> /usr/local/lib/python3.8/site-packages/scipy/sparse/csgraph/_matching.cpython-38-darwin.so
       0x13770c000 -        0x137737ff3 +_reordering.cpython-38-darwin.so (0) <B7CD3695-FC40-3952-BC28-EF5754277E96> /usr/local/lib/python3.8/site-packages/scipy/sparse/csgraph/_reordering.cpython-38-darwin.so
       0x1377d4000 -        0x1378c3ff3 +interval.cpython-38-darwin.so (0) <5C95FC43-11B8-306E-8799-E8AE09B63CDB> /usr/local/lib/python3.8/site-packages/pandas/_libs/interval.cpython-38-darwin.so
       0x137925000 -        0x137996ff3 +hashtable.cpython-38-darwin.so (0) <C060BDA7-AD90-3626-8DCF-9FF991D41596> /usr/local/lib/python3.8/site-packages/pandas/_libs/hashtable.cpython-38-darwin.so
       0x1379d6000 -        0x1379f9ff7 +missing.cpython-38-darwin.so (0) <3F9BEDB8-AA09-3504-B492-03A9DCCC5346> /usr/local/lib/python3.8/site-packages/pandas/_libs/missing.cpython-38-darwin.so
       0x137a17000 -        0x137a29ffb +dtypes.cpython-38-darwin.so (0) <0524B227-8CDD-3D9B-B4F9-CE8F696784F9> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslibs/dtypes.cpython-38-darwin.so
       0x137a3f000 -        0x137a70ff3 +conversion.cpython-38-darwin.so (0) <E5DE69FC-7024-3B05-8AF5-3B03812354B5> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslibs/conversion.cpython-38-darwin.so
       0x137a8e000 -        0x137a92fff +base.cpython-38-darwin.so (0) <790A69D1-16B1-33AC-9D8A-1D2B603FD242> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslibs/base.cpython-38-darwin.so
       0x137a99000 -        0x137abcff7 +nattype.cpython-38-darwin.so (0) <3A614EA4-C26C-311A-A085-8E7FEE9E5B9D> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslibs/nattype.cpython-38-darwin.so
       0x137add000 -        0x137ae5ffb +np_datetime.cpython-38-darwin.so (0) <06F079F5-C2B7-374B-AF77-96C99713A64E> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslibs/np_datetime.cpython-38-darwin.so
       0x137b2d000 -        0x137b53fff +timezones.cpython-38-darwin.so (0) <3A1B89C4-4AED-32A1-86AF-96A8DD09ECAD> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslibs/timezones.cpython-38-darwin.so
       0x137b6e000 -        0x137ba9fff +tzconversion.cpython-38-darwin.so (0) <215F9F14-2B79-3E13-9B06-89763391A84E> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslibs/tzconversion.cpython-38-darwin.so
       0x137bc7000 -        0x137bcfffb +ccalendar.cpython-38-darwin.so (0) <C81CF332-2AC2-3757-A388-EF390C973C10> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslibs/ccalendar.cpython-38-darwin.so
       0x137bda000 -        0x137c22ff7 +parsing.cpython-38-darwin.so (0) <C0E7A008-CA66-3CFA-A51B-E860DBB9CF1A> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslibs/parsing.cpython-38-darwin.so
       0x137c51000 -        0x137d0affb +offsets.cpython-38-darwin.so (0) <9EE0C7AC-703F-3133-B38C-F7B54CF58671> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslibs/offsets.cpython-38-darwin.so
       0x137db5000 -        0x137e00ff7 +timedeltas.cpython-38-darwin.so (0) <78ECDB91-86A7-36D6-A95B-6D86AF183C5F> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslibs/timedeltas.cpython-38-darwin.so
       0x137e34000 -        0x137e80ff7 +timestamps.cpython-38-darwin.so (0) <4AC792E4-7174-389A-8873-919C92109481> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslibs/timestamps.cpython-38-darwin.so
       0x137eb8000 -        0x137ee3ffb +fields.cpython-38-darwin.so (0) <44B8DF0C-8F01-344F-8350-5BBB96DA1762> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslibs/fields.cpython-38-darwin.so
       0x137f40000 -        0x137f82ff7 +strptime.cpython-38-darwin.so (0) <83F85EC9-6019-3D1D-8ABF-0DC80AB7D78C> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslibs/strptime.cpython-38-darwin.so
       0x137faf000 -        0x137fb7ff7 +properties.cpython-38-darwin.so (0) <C80919BD-7E72-3D74-A2EF-A7D45B7C72B2> /usr/local/lib/python3.8/site-packages/pandas/_libs/properties.cpython-38-darwin.so
       0x137fc1000 -        0x138006ffb +period.cpython-38-darwin.so (0) <BADB14B2-72AE-333B-BB3B-91994A52D1C2> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslibs/period.cpython-38-darwin.so
       0x138076000 -        0x13809bff7 +vectorized.cpython-38-darwin.so (0) <9F23A27B-40BE-33A6-B2E8-D3709D3B5AC8> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslibs/vectorized.cpython-38-darwin.so
       0x1380b2000 -        0x1380b8ffb +ops_dispatch.cpython-38-darwin.so (0) <D96618C6-D076-3617-9BE5-55A58E51ED94> /usr/local/lib/python3.8/site-packages/pandas/_libs/ops_dispatch.cpython-38-darwin.so
       0x1380c2000 -        0x13821bfff +algos.cpython-38-darwin.so (0) <55ABE6A9-D8CA-306F-ADD7-003DADCD0128> /usr/local/lib/python3.8/site-packages/pandas/_libs/algos.cpython-38-darwin.so
       0x138286000 -        0x1382e8ffb +lib.cpython-38-darwin.so (0) <CF84E184-CC3D-31B0-9662-A53D3AEC2145> /usr/local/lib/python3.8/site-packages/pandas/_libs/lib.cpython-38-darwin.so
       0x13832c000 -        0x13834aff3 +tslib.cpython-38-darwin.so (0) <2426ABCE-81BE-32C7-B3E2-672C5D5A2002> /usr/local/lib/python3.8/site-packages/pandas/_libs/tslib.cpython-38-darwin.so
       0x1383dc000 -        0x1383f6ff3 +hashing.cpython-38-darwin.so (0) <45770CC7-3F16-33EE-943D-322F2F9A4EBA> /usr/local/lib/python3.8/site-packages/pandas/_libs/hashing.cpython-38-darwin.so
       0x13844a000 -        0x13846effb +ops.cpython-38-darwin.so (0) <3BD85E9A-B206-3DCD-ADB7-F7BBA9089912> /usr/local/lib/python3.8/site-packages/pandas/_libs/ops.cpython-38-darwin.so
       0x1385c5000 -        0x13862dffb +index.cpython-38-darwin.so (0) <4A695E0B-2E5D-35FD-B71D-D62D7B76C62E> /usr/local/lib/python3.8/site-packages/pandas/_libs/index.cpython-38-darwin.so
       0x1386a6000 -        0x138917ffb +join.cpython-38-darwin.so (0) <206D1CD4-E7B9-3DC1-8EAE-26DBD0D16905> /usr/local/lib/python3.8/site-packages/pandas/_libs/join.cpython-38-darwin.so
       0x1389a1000 -        0x138a68ffb +sparse.cpython-38-darwin.so (0) <572A2454-1CE5-3464-BC82-8EAC924EDBE8> /usr/local/lib/python3.8/site-packages/pandas/_libs/sparse.cpython-38-darwin.so
       0x138c60000 -        0x138c8cffb +reshape.cpython-38-darwin.so (0) <16E299A3-7BE2-333A-BB47-2059A15CC630> /usr/local/lib/python3.8/site-packages/pandas/_libs/reshape.cpython-38-darwin.so
       0x138ce5000 -        0x138ceaff3 +indexing.cpython-38-darwin.so (0) <AB97662D-520A-340D-BBDE-9028E561A36A> /usr/local/lib/python3.8/site-packages/pandas/_libs/indexing.cpython-38-darwin.so
       0x138cf1000 -        0x138d10fff +writers.cpython-38-darwin.so (0) <C759D039-308C-34D0-AEFA-E061064AAAD1> /usr/local/lib/python3.8/site-packages/pandas/_libs/writers.cpython-38-darwin.so
       0x138d68000 -        0x138d91ff3 +internals.cpython-38-darwin.so (0) <A497BE5F-BDED-3C18-80CF-BC46573BE128> /usr/local/lib/python3.8/site-packages/pandas/_libs/internals.cpython-38-darwin.so
       0x138dec000 -        0x138defffb +mmap.cpython-38-darwin.so (0) <BAE3D5B0-6DF0-3ED1-8148-BAC6492B20A7> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/mmap.cpython-38-darwin.so
       0x138e3c000 -        0x138e87ff3 +aggregations.cpython-38-darwin.so (0) <71ABE3C6-C269-3660-BA49-6CB015BA9231> /usr/local/lib/python3.8/site-packages/pandas/_libs/window/aggregations.cpython-38-darwin.so
       0x138ef4000 -        0x138f0dffb +indexers.cpython-38-darwin.so (0) <96D3A334-415F-384A-900E-44534F20B57C> /usr/local/lib/python3.8/site-packages/pandas/_libs/window/indexers.cpython-38-darwin.so
       0x138fe0000 -        0x1390ecfff +groupby.cpython-38-darwin.so (0) <9D7B64F4-62A5-37F9-8CC5-78942D0D94D4> /usr/local/lib/python3.8/site-packages/pandas/_libs/groupby.cpython-38-darwin.so
       0x139136000 -        0x13916aff7 +reduction.cpython-38-darwin.so (0) <9EAD72DC-57A6-34DB-ADD8-A502E3B4171E> /usr/local/lib/python3.8/site-packages/pandas/_libs/reduction.cpython-38-darwin.so
       0x139249000 -        0x1392abffb +parsers.cpython-38-darwin.so (0) <1BB0C433-705E-3090-9D25-D485E737967A> /usr/local/lib/python3.8/site-packages/pandas/_libs/parsers.cpython-38-darwin.so
       0x139325000 -        0x139333ffb +json.cpython-38-darwin.so (0) <F542426B-5C56-30EF-9DD8-5752BD0CEE03> /usr/local/lib/python3.8/site-packages/pandas/_libs/json.cpython-38-darwin.so
       0x13947d000 -        0x13948cff3 +testing.cpython-38-darwin.so (0) <E5012E07-77A1-3D43-8508-AFCF9D20BE27> /usr/local/lib/python3.8/site-packages/pandas/_libs/testing.cpython-38-darwin.so
       0x139718000 -        0x139837ffb +_pywrap_tf_optimizer.so (0) <B1F0CD9E-35DE-3A27-8DBC-A6C9350B4A17> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_tf_optimizer.so
       0x1398be000 -        0x1399ebffb +_pywrap_tf_cluster.so (0) <8F93E2BE-C033-3B77-ADEF-23B5613124C6> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_tf_cluster.so
       0x139d79000 -        0x139e95ff7 +_pywrap_tfprof.so (0) <41AD0CF0-EB6B-3818-8B70-CE32A720534C> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_tfprof.so
       0x139fdc000 -        0x13a0f7fff +_pywrap_quantize_training.so (0) <0D2A50A2-CFF3-3BBE-97FE-D19AA6ABC1E9> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_quantize_training.so
       0x13a23c000 -        0x13a24bff3 +_pywrap_stacktrace_handler.so (0) <C95E3624-D0E8-3919-AFC0-15F46C97FD50> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_stacktrace_handler.so
       0x13a254000 -        0x13a263ffb +_pywrap_util_port.so (0) <B6638062-153E-394E-9D6F-9CDAB7DC510D> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_util_port.so
       0x13a36c000 -        0x13a48bfe7 +_pywrap_debug_events_writer.so (0) <648DC286-CE80-3853-81E5-3D445ACC028B> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_debug_events_writer.so
       0x13a4d3000 -        0x13a609feb +_pywrap_mlir.so (0) <C9B37000-C0A1-3289-968F-02C2E7167EFD> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_mlir.so
       0x13a6d9000 -        0x13a6fcff3 +_pywrap_python_op_gen.so (0) <CCF1FF5B-66D7-38DD-AF57-6D141EEFC4DA> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_python_op_gen.so
       0x13a711000 -        0x13a731fff +_audio_microfrontend_op.so (0) <3E43A62D-C6A9-3FCD-AD8B-42129D162959> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so
       0x13a749000 -        0x13a75aff7 +_pywrap_toco_api.so (0) <AA5A2977-76D0-30B1-AC96-8222C0032BA5> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/python/_pywrap_toco_api.so
       0x13a7a4000 -        0x13a7afff3 +_elementtree.cpython-38-darwin.so (0) <26BAEF44-6261-3ABD-BDB8-F7CC7F48D40F> /usr/local/Cellar/python@3.8/3.8.6/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_elementtree.cpython-38-darwin.so
       0x13a809000 -        0x13abbaff3 +_pywrap_tensorflow_interpreter_wrapper.so (0) <4D246436-5A37-322C-83C0-ADF4EBEFFCC3> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so
       0x13ad7d000 -        0x13add8ffb +_imaging.cpython-38-darwin.so (0) <ADC17727-3D14-3B8A-82CC-3382B9F7742E> /usr/local/lib/python3.8/site-packages/PIL/_imaging.cpython-38-darwin.so
       0x13ae01000 -        0x13ae39ff3 +libjpeg.9.dylib (0) <A21D37F5-39AE-31A6-A1E6-84CE883F3D7F> /usr/local/lib/python3.8/site-packages/PIL/.dylibs/libjpeg.9.dylib
       0x13ae49000 -        0x13aebfffb +libopenjp2.2.3.1.dylib (0) <070B3635-9516-3171-8E04-6F645281C96B> /usr/local/lib/python3.8/site-packages/PIL/.dylibs/libopenjp2.2.3.1.dylib
       0x13aeca000 -        0x13aee5ff7 +libz.1.2.11.dylib (0) <8F57D771-E0DD-3DA4-8241-A90A4B0AE451> /usr/local/lib/python3.8/site-packages/PIL/.dylibs/libz.1.2.11.dylib
       0x13aee9000 -        0x13af78fff +libtiff.5.dylib (0) <DE18E216-5168-3869-9376-D3240A470E69> /usr/local/lib/python3.8/site-packages/PIL/.dylibs/libtiff.5.dylib
       0x13af8a000 -        0x13afb3ff7 +libxcb.1.1.0.dylib (0) <5EF280A8-5FC6-34C1-82A1-E84D594ACA94> /usr/local/lib/python3.8/site-packages/PIL/.dylibs/libxcb.1.1.0.dylib
       0x13afc5000 -        0x13aff6ffb +liblzma.5.dylib (0) <40849847-E5EE-37D8-9141-CFD1BF39A83C> /usr/local/lib/python3.8/site-packages/PIL/.dylibs/liblzma.5.dylib
       0x13affe000 -        0x13b01fffb +_nd_image.cpython-38-darwin.so (0) <E5682B82-3E77-3927-ACFC-FDE27CBD62E8> /usr/local/lib/python3.8/site-packages/scipy/ndimage/_nd_image.cpython-38-darwin.so
       0x13b026000 -        0x13b062ff7 +_ni_label.cpython-38-darwin.so (0) <E393AE92-DFC8-37BC-A95D-2D32FC21D167> /usr/local/lib/python3.8/site-packages/scipy/ndimage/_ni_label.cpython-38-darwin.so
       0x13b0be000 -        0x13b117ff7 +_fblas.cpython-38-darwin.so (0) <119EAE21-C3F9-38D9-9F4E-B3C035453E4A> /usr/local/lib/python3.8/site-packages/scipy/linalg/_fblas.cpython-38-darwin.so
       0x13b14f000 -        0x13ebbaae7 +libopenblasp-r0.3.7.dylib (0) <9914A383-F8C9-3559-BC88-B4DD28689BC5> /usr/local/lib/python3.8/site-packages/scipy/.dylibs/libopenblasp-r0.3.7.dylib
       0x13edfa000 -        0x13ef11ff7 +libgfortran.3.dylib (0) <9ABE5EDE-AD43-391A-9E54-866711FAC32A> /usr/local/lib/python3.8/site-packages/scipy/.dylibs/libgfortran.3.dylib
       0x13ef75000 -        0x13ef8aff7 +libgcc_s.1.dylib (0) <7C6D7CB7-82DB-3290-8181-07646FEA1F80> /usr/local/lib/python3.8/site-packages/scipy/.dylibs/libgcc_s.1.dylib
       0x13ef95000 -        0x13efcbfff +libquadmath.0.dylib (0) <7FFA409F-FB04-3B64-BE9A-3E3A494C975E> /usr/local/lib/python3.8/site-packages/scipy/.dylibs/libquadmath.0.dylib
       0x140fda000 -        0x1410bbfff +_flapack.cpython-38-darwin.so (0) <53F92D23-B0E6-3E29-BB55-62958E6EAF80> /usr/local/lib/python3.8/site-packages/scipy/linalg/_flapack.cpython-38-darwin.so
       0x141196000 -        0x1411a1ff7 +_flinalg.cpython-38-darwin.so (0) <A0B69527-8AC8-30BE-A313-D108F7003106> /usr/local/lib/python3.8/site-packages/scipy/linalg/_flinalg.cpython-38-darwin.so
       0x1411a9000 -        0x1411ceff7 +_solve_toeplitz.cpython-38-darwin.so (0) <08DAABBC-1383-3FC7-AD0B-290851C6F33D> /usr/local/lib/python3.8/site-packages/scipy/linalg/_solve_toeplitz.cpython-38-darwin.so
       0x1411e7000 -        0x14121cfff +_decomp_update.cpython-38-darwin.so (0) <A30481AB-7752-3980-AE6B-98E919BEFD5B> /usr/local/lib/python3.8/site-packages/scipy/linalg/_decomp_update.cpython-38-darwin.so
       0x141232000 -        0x14125afff +cython_blas.cpython-38-darwin.so (0) <3ACA7C9A-A1B5-30D3-A06C-CB06A31C0865> /usr/local/lib/python3.8/site-packages/scipy/linalg/cython_blas.cpython-38-darwin.so
       0x141277000 -        0x1412e4ff7 +cython_lapack.cpython-38-darwin.so (0) <FCD177EE-9EB7-386F-BF19-6F9B4C158364> /usr/local/lib/python3.8/site-packages/scipy/linalg/cython_lapack.cpython-38-darwin.so
       0x141502000 -        0x141647fe3 +_xla_ops.so (0) <6D5F3CB5-1B1B-3702-AF86-C48A7B970F0A> /usr/local/lib/python3.8/site-packages/tensorflow-2.3.1-py3.8-macosx-10.15-x86_64.egg/tensorflow/compiler/tf2xla/ops/_xla_ops.so
    0x7fff3551c000 -     0x7fff3551cfff  com.apple.Accelerate (1.11 - Accelerate 1.11) <4F9977AE-DBDB-3A16-A536-AC1F9938DCDD> /System/Library/Frameworks/Accelerate.framework/Versions/A/Accelerate
    0x7fff35534000 -     0x7fff35b8afff  com.apple.vImage (8.1 - 524.2.1) <EA6F5FF2-7A1B-35D5-A5A3-D2B3386ECB75> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vImage.framework/Versions/A/vImage
    0x7fff35b8b000 -     0x7fff35df2ff7  libBLAS.dylib (1303.60.1) <C6C2D42F-7456-3DBF-8BE2-9AA06EFC78FD> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
    0x7fff35df3000 -     0x7fff362c6fef  libBNNS.dylib (144.100.2) <99C61C48-B14C-3DA6-8C31-6BF72DA0A3A9> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBNNS.dylib
    0x7fff362c7000 -     0x7fff36662fff  libLAPACK.dylib (1303.60.1) <5E3E3867-50C3-3E6A-9A2E-007CE77A4641> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib
    0x7fff36663000 -     0x7fff36678fec  libLinearAlgebra.dylib (1303.60.1) <3D433800-0099-33E0-8C81-15F83247B2C9> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLinearAlgebra.dylib
    0x7fff36679000 -     0x7fff3667eff3  libQuadrature.dylib (7) <371F36A7-B12F-363E-8955-F24F7C2048F6> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libQuadrature.dylib
    0x7fff3667f000 -     0x7fff366effff  libSparse.dylib (103) <B8A10D0C-4577-343D-B310-A3E81265D107> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libSparse.dylib
    0x7fff366f0000 -     0x7fff36702fef  libSparseBLAS.dylib (1303.60.1) <B147FEF6-A0DB-3830-BF06-45BEC58DB576> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libSparseBLAS.dylib
    0x7fff36703000 -     0x7fff368dafd7  libvDSP.dylib (735.140.1) <D63DC0A5-B8B4-3562-A574-E73BC3B57407> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvDSP.dylib
    0x7fff368db000 -     0x7fff3699dfef  libvMisc.dylib (735.140.1) <3601FDE3-B142-398D-987D-8151A51F0A96> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib
    0x7fff3699e000 -     0x7fff3699efff  com.apple.Accelerate.vecLib (3.11 - vecLib 3.11) <F6C5613D-2284-342B-9160-9731F78B4DE5> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/vecLib
    0x7fff38104000 -     0x7fff38493ffa  com.apple.CFNetwork (1128.0.1 - 1128.0.1) <07F9CA9C-B954-3EA0-A710-3122BFF9F057> /System/Library/Frameworks/CFNetwork.framework/Versions/A/CFNetwork
    0x7fff39895000 -     0x7fff39d14feb  com.apple.CoreFoundation (6.9 - 1677.104) <C0D70026-EDBE-3CBD-B317-367CF4F1C92F> /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation
    0x7fff3ac7d000 -     0x7fff3ac7dfff  com.apple.CoreServices (1069.24 - 1069.24) <AA140158-E909-34C2-B2F5-20EBC93E0056> /System/Library/Frameworks/CoreServices.framework/Versions/A/CoreServices
    0x7fff3ac7e000 -     0x7fff3ad03fff  com.apple.AE (838.1 - 838.1) <2E5FD5AE-8A7F-353F-9BD1-0241F3586181> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/AE.framework/Versions/A/AE
    0x7fff3ad04000 -     0x7fff3afe5ff7  com.apple.CoreServices.CarbonCore (1217 - 1217) <BE379206-99FA-30CD-8391-2708473A633F> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/CarbonCore.framework/Versions/A/CarbonCore
    0x7fff3afe6000 -     0x7fff3b033ffd  com.apple.DictionaryServices (1.2 - 323.6) <26B70C82-25BC-353A-858F-945B14C803A2> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/DictionaryServices.framework/Versions/A/DictionaryServices
    0x7fff3b034000 -     0x7fff3b03cff7  com.apple.CoreServices.FSEvents (1268.100.1 - 1268.100.1) <FC84DB48-A3CE-30F7-A918-B3587731ACC7> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/FSEvents.framework/Versions/A/FSEvents
    0x7fff3b03d000 -     0x7fff3b277ff6  com.apple.LaunchServices (1069.24 - 1069.24) <9A5359D9-9148-3B18-B868-56A9DA5FB60C> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/LaunchServices.framework/Versions/A/LaunchServices
    0x7fff3b278000 -     0x7fff3b310ff1  com.apple.Metadata (10.7.0 - 2076.7) <0973F7E5-D58C-3574-A3CE-4F12CAC2D4C7> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/Metadata.framework/Versions/A/Metadata
    0x7fff3b311000 -     0x7fff3b33efff  com.apple.CoreServices.OSServices (1069.24 - 1069.24) <0E4F48AD-402C-3E9D-9CA9-6DD9479B28F9> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/OSServices.framework/Versions/A/OSServices
    0x7fff3b33f000 -     0x7fff3b3a6fff  com.apple.SearchKit (1.4.1 - 1.4.1) <2C5E1D85-E8B1-3DC5-91B9-E3EDB48E9369> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/SearchKit.framework/Versions/A/SearchKit
    0x7fff3b3a7000 -     0x7fff3b3cbff5  com.apple.coreservices.SharedFileList (131.4 - 131.4) <02DE0D56-E371-3EF5-9BC1-FA435451B412> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/SharedFileList.framework/Versions/A/SharedFileList
    0x7fff3bc11000 -     0x7fff3bc17fff  com.apple.DiskArbitration (2.7 - 2.7) <0BBBB6A6-604D-368B-9943-50B8CE75D51D> /System/Library/Frameworks/DiskArbitration.framework/Versions/A/DiskArbitration
    0x7fff3bf52000 -     0x7fff3c317fff  com.apple.Foundation (6.9 - 1677.104) <7C69F845-F651-3193-8262-5938010EC67D> /System/Library/Frameworks/Foundation.framework/Versions/C/Foundation
    0x7fff3c68b000 -     0x7fff3c72fff3  com.apple.framework.IOKit (2.0.2 - 1726.140.1) <14223387-6F81-3976-8605-4BC2F253A93E> /System/Library/Frameworks/IOKit.framework/Versions/A/IOKit
    0x7fff40238000 -     0x7fff40244ffe  com.apple.NetFS (6.0 - 4.0) <4415F027-D36D-3B9C-96BA-AD22B44A4722> /System/Library/Frameworks/NetFS.framework/Versions/A/NetFS
    0x7fff42e27000 -     0x7fff42e43fff  com.apple.CFOpenDirectory (10.15 - 220.40.1) <7E6C88EB-3DD9-32B0-81FC-179552834FA9> /System/Library/Frameworks/OpenDirectory.framework/Versions/A/Frameworks/CFOpenDirectory.framework/Versions/A/CFOpenDirectory
    0x7fff42e44000 -     0x7fff42e4fffd  com.apple.OpenDirectory (10.15 - 220.40.1) <4A92D8D8-A9E5-3A9C-942F-28576F6BCDF5> /System/Library/Frameworks/OpenDirectory.framework/Versions/A/OpenDirectory
    0x7fff461ec000 -     0x7fff46535ff1  com.apple.security (7.0 - 59306.140.5) <B6F8368F-2395-379B-B768-71C53BB1B903> /System/Library/Frameworks/Security.framework/Versions/A/Security
    0x7fff46536000 -     0x7fff465beffb  com.apple.securityfoundation (6.0 - 55236.60.1) <7C69DF47-4017-3DF2-B55B-712B481654CB> /System/Library/Frameworks/SecurityFoundation.framework/Versions/A/SecurityFoundation
    0x7fff465ed000 -     0x7fff465f1ff8  com.apple.xpc.ServiceManagement (1.0 - 1) <2C62956C-F2D4-3EB0-86C7-EAA06331621A> /System/Library/Frameworks/ServiceManagement.framework/Versions/A/ServiceManagement
    0x7fff4729d000 -     0x7fff47317ff7  com.apple.SystemConfiguration (1.19 - 1.19) <84F9B3BB-F7AF-3B7C-8CD0-D3C22D19619F> /System/Library/Frameworks/SystemConfiguration.framework/Versions/A/SystemConfiguration
    0x7fff4b287000 -     0x7fff4b34cfe7  com.apple.APFS (1412.141.1 - 1412.141.1) <C86A3423-E61C-335A-9D17-0B3CE5BB6467> /System/Library/PrivateFrameworks/APFS.framework/Versions/A/APFS
    0x7fff4d057000 -     0x7fff4d066fd7  com.apple.AppleFSCompression (119.100.1 - 1.0) <466ABD77-2E52-36D1-8E39-77AE2CC61611> /System/Library/PrivateFrameworks/AppleFSCompression.framework/Versions/A/AppleFSCompression
    0x7fff4e827000 -     0x7fff4e830ff7  com.apple.coreservices.BackgroundTaskManagement (1.0 - 104) <A6877DAD-8F47-363C-983A-DC8DDE83B7B5> /System/Library/PrivateFrameworks/BackgroundTaskManagement.framework/Versions/A/BackgroundTaskManagement
    0x7fff51638000 -     0x7fff51648ff3  com.apple.CoreEmoji (1.0 - 107.1) <7C2B3259-083B-31B8-BCDB-1BA360529936> /System/Library/PrivateFrameworks/CoreEmoji.framework/Versions/A/CoreEmoji
    0x7fff51c88000 -     0x7fff51cf2ff0  com.apple.CoreNLP (1.0 - 213) <E70E2505-8078-324E-BAE1-01A2DA980E2C> /System/Library/PrivateFrameworks/CoreNLP.framework/Versions/A/CoreNLP
    0x7fff52b6d000 -     0x7fff52b9bffd  com.apple.CSStore (1069.24 - 1069.24) <C2D67667-FA0B-3DB6-AA34-6999EE4346A0> /System/Library/PrivateFrameworks/CoreServicesStore.framework/Versions/A/CoreServicesStore
    0x7fff5edf9000 -     0x7fff5eec7ffd  com.apple.LanguageModeling (1.0 - 215.1) <C456087D-5B3A-390E-A665-862FA284C59C> /System/Library/PrivateFrameworks/LanguageModeling.framework/Versions/A/LanguageModeling
    0x7fff5eec8000 -     0x7fff5ef10fff  com.apple.Lexicon-framework (1.0 - 72) <41F208B9-8255-3EC7-9673-FE0925D071D3> /System/Library/PrivateFrameworks/Lexicon.framework/Versions/A/Lexicon
    0x7fff5ef17000 -     0x7fff5ef1cff3  com.apple.LinguisticData (1.0 - 353.18) <3B92F249-4602-325F-984B-D2DE61EEE4E1> /System/Library/PrivateFrameworks/LinguisticData.framework/Versions/A/LinguisticData
    0x7fff60285000 -     0x7fff602d1fff  com.apple.spotlight.metadata.utilities (1.0 - 2076.7) <0237323B-EC78-3FBF-9FC7-5A1FE2B5CE25> /System/Library/PrivateFrameworks/MetadataUtilities.framework/Versions/A/MetadataUtilities
    0x7fff60d88000 -     0x7fff60d92fff  com.apple.NetAuth (6.2 - 6.2) <B0C03C41-87A3-352B-B130-96E1A6F94B47> /System/Library/PrivateFrameworks/NetAuth.framework/Versions/A/NetAuth
    0x7fff6a01e000 -     0x7fff6a02eff3  com.apple.TCC (1.0 - 1) <017AB27D-6821-303A-8FD2-6DAC795CC7AA> /System/Library/PrivateFrameworks/TCC.framework/Versions/A/TCC
    0x7fff6d711000 -     0x7fff6d713ff3  com.apple.loginsupport (1.0 - 1) <12F77885-27DC-3837-9CE9-A25EBA75F833> /System/Library/PrivateFrameworks/login.framework/Versions/A/Frameworks/loginsupport.framework/Versions/A/loginsupport
    0x7fff70231000 -     0x7fff70265fff  libCRFSuite.dylib (48) <5E5DE3CB-30DD-34DC-AEF8-FE8536A85E96> /usr/lib/libCRFSuite.dylib
    0x7fff70268000 -     0x7fff70272fff  libChineseTokenizer.dylib (34) <7F0DA183-1796-315A-B44A-2C234C7C50BE> /usr/lib/libChineseTokenizer.dylib
    0x7fff702fe000 -     0x7fff70300ff7  libDiagnosticMessagesClient.dylib (112) <C94F3B7B-1854-38EB-9778-834501C53B3F> /usr/lib/libDiagnosticMessagesClient.dylib
    0x7fff707d4000 -     0x7fff707d5fff  libSystem.B.dylib (1281.100.1) <0A6C8BA1-30FD-3D10-83FD-FF29E221AFFE> /usr/lib/libSystem.B.dylib
    0x7fff70862000 -     0x7fff70863fff  libThaiTokenizer.dylib (3) <4F4ADE99-0D09-3223-B7C0-C407AB6DE8DC> /usr/lib/libThaiTokenizer.dylib
    0x7fff7087b000 -     0x7fff70891fff  libapple_nghttp2.dylib (1.39.2) <07FEC48A-87CF-32A3-8194-FA70B361713A> /usr/lib/libapple_nghttp2.dylib
    0x7fff708c6000 -     0x7fff70938ff7  libarchive.2.dylib (72.140.1) <AC311FBA-F2DD-3595-AA76-769F912942B8> /usr/lib/libarchive.2.dylib
    0x7fff709d6000 -     0x7fff709d6ff3  libauto.dylib (187) <B6124448-7690-34AE-8939-ED84AAC630CE> /usr/lib/libauto.dylib
    0x7fff70a9c000 -     0x7fff70aacffb  libbsm.0.dylib (60.100.1) <00BFFB9A-2FFE-3C24-896A-251BC61917FD> /usr/lib/libbsm.0.dylib
    0x7fff70aad000 -     0x7fff70ab9fff  libbz2.1.0.dylib (44) <14CC4988-B6D4-3879-AFC2-9A0DDC6388DE> /usr/lib/libbz2.1.0.dylib
    0x7fff70aba000 -     0x7fff70b0cfff  libc++.1.dylib (902.1) <59A8239F-C28A-3B59-B8FA-11340DC85EDC> /usr/lib/libc++.1.dylib
    0x7fff70b0d000 -     0x7fff70b22ffb  libc++abi.dylib (902) <E692F14F-C65E-303B-9921-BB7E97D77855> /usr/lib/libc++abi.dylib
    0x7fff70b23000 -     0x7fff70b23fff  libcharset.1.dylib (59) <72447768-9244-39AB-8E79-2FA14EC0AD33> /usr/lib/libcharset.1.dylib
    0x7fff70b24000 -     0x7fff70b35fff  libcmph.dylib (8) <E72A20DB-2E86-378D-A237-EB9A1370F989> /usr/lib/libcmph.dylib
    0x7fff70b36000 -     0x7fff70b4dfd7  libcompression.dylib (87) <64C91066-586D-38C0-A2F3-3E60A940F859> /usr/lib/libcompression.dylib
    0x7fff70e27000 -     0x7fff70e3dff7  libcoretls.dylib (167) <770A5B96-936E-34E3-B006-B1CEC299B5A5> /usr/lib/libcoretls.dylib
    0x7fff70e3e000 -     0x7fff70e3ffff  libcoretls_cfhelpers.dylib (167) <940BF370-FD0C-30A8-AA05-FF48DA44FA4C> /usr/lib/libcoretls_cfhelpers.dylib
    0x7fff71565000 -     0x7fff71565fff  libenergytrace.dylib (21) <162DFCC0-8F48-3DD0-914F-FA8653E27B26> /usr/lib/libenergytrace.dylib
    0x7fff7158c000 -     0x7fff7158efff  libfakelink.dylib (149.1) <36146CB2-E6A5-37BB-9EE8-1B4034D8F3AD> /usr/lib/libfakelink.dylib
    0x7fff7158f000 -     0x7fff71594f57  libffi.dylib (26) <6068BAD9-0878-3491-97E4-6CF6BC813C2E> /usr/lib/libffi.dylib
    0x7fff7159d000 -     0x7fff715a2fff  libgermantok.dylib (24) <D2AE5AC0-EDCE-3216-B8C9-CF59292A545F> /usr/lib/libgermantok.dylib
    0x7fff715ad000 -     0x7fff7169dfff  libiconv.2.dylib (59) <18311A67-E4EF-3CC7-95B3-C0EDEE3A282F> /usr/lib/libiconv.2.dylib
    0x7fff7169e000 -     0x7fff718f5fff  libicucore.A.dylib (64260.0.1) <8AC2CB07-E7E0-340D-A849-186FA1F27251> /usr/lib/libicucore.A.dylib
    0x7fff7190f000 -     0x7fff71910fff  liblangid.dylib (133) <30CFC08C-EF36-3CF5-8AEA-C1CB070306B7> /usr/lib/liblangid.dylib
    0x7fff71911000 -     0x7fff71929ff3  liblzma.5.dylib (16) <C131EF18-2CDD-3271-8A30-A8760D4FE166> /usr/lib/liblzma.5.dylib
    0x7fff71941000 -     0x7fff719e8ff7  libmecab.dylib (883.11) <0D5BFD01-D4A7-3C8D-AA69-C329C1A69792> /usr/lib/libmecab.dylib
    0x7fff719e9000 -     0x7fff71c4bff1  libmecabra.dylib (883.11) <E31DE74D-1B88-377F-ACD3-D789D29C3AE7> /usr/lib/libmecabra.dylib
    0x7fff72117000 -     0x7fff72593ff5  libnetwork.dylib (1880.120.4) <BA294A54-F309-398D-B308-F97032AFF555> /usr/lib/libnetwork.dylib
    0x7fff72634000 -     0x7fff72667fde  libobjc.A.dylib (787.1) <6DF81160-5E7F-3E31-AA1E-C875E3B98AF6> /usr/lib/libobjc.A.dylib
    0x7fff7267a000 -     0x7fff7267efff  libpam.2.dylib (25.100.1) <0502F395-8EE6-3D2A-9239-06FD5622E19E> /usr/lib/libpam.2.dylib
    0x7fff72681000 -     0x7fff726b7ff7  libpcap.A.dylib (89.120.1) <A76EC076-A8EA-354C-B95F-7AB1EAFBCC65> /usr/lib/libpcap.A.dylib
    0x7fff727af000 -     0x7fff72999ff7  libsqlite3.dylib (308.5) <35A2BD9F-4E33-30DE-A994-4AB585AC3AFE> /usr/lib/libsqlite3.dylib
    0x7fff72bea000 -     0x7fff72bedffb  libutil.dylib (57) <F01467F6-23A7-37EE-A170-33CE1577B41D> /usr/lib/libutil.dylib
    0x7fff72bee000 -     0x7fff72bfbff7  libxar.1.dylib (425.2) <EE964412-9E25-30B3-BCC0-CCEFBCC8094B> /usr/lib/libxar.1.dylib
    0x7fff72c01000 -     0x7fff72ce3fff  libxml2.2.dylib (33.5) <A579D158-2E09-316C-872E-DD9D93401B2F> /usr/lib/libxml2.2.dylib
    0x7fff72ce7000 -     0x7fff72d0ffff  libxslt.1.dylib (16.9) <34A45627-DA5B-37D2-9609-65B425E0010A> /usr/lib/libxslt.1.dylib
    0x7fff72d10000 -     0x7fff72d22ff3  libz.1.dylib (76) <793D9643-CD83-3AAC-8B96-88D548FAB620> /usr/lib/libz.1.dylib
    0x7fff735d1000 -     0x7fff735d6ff3  libcache.dylib (83) <AF488D13-9E89-35E0-B078-BE37CC5B8586> /usr/lib/system/libcache.dylib
    0x7fff735d7000 -     0x7fff735e2fff  libcommonCrypto.dylib (60165.120.1) <C7912BE5-993E-3581-B2A0-6AABDC8C5562> /usr/lib/system/libcommonCrypto.dylib
    0x7fff735e3000 -     0x7fff735eafff  libcompiler_rt.dylib (101.2) <49B8F644-5705-3F16-BBE0-6FFF9B17C36E> /usr/lib/system/libcompiler_rt.dylib
    0x7fff735eb000 -     0x7fff735f4ff7  libcopyfile.dylib (166.40.1) <3C481225-21E7-370A-A30E-0CCFDD64A92C> /usr/lib/system/libcopyfile.dylib
    0x7fff735f5000 -     0x7fff73687fdb  libcorecrypto.dylib (866.140.1) <60567BF8-80FA-359A-B2F3-A3BAEFB288FD> /usr/lib/system/libcorecrypto.dylib
    0x7fff73794000 -     0x7fff737d4ff0  libdispatch.dylib (1173.100.2) <CD9C059C-91D9-30E8-8926-5B9CD0D5D4F5> /usr/lib/system/libdispatch.dylib
    0x7fff737d5000 -     0x7fff7380bfff  libdyld.dylib (750.6) <789A18C2-8AC7-3C88-813D-CD674376585D> /usr/lib/system/libdyld.dylib
    0x7fff7380c000 -     0x7fff7380cffb  libkeymgr.dylib (30) <DB3337BE-01CA-3425-BD0C-87774FC0CDC0> /usr/lib/system/libkeymgr.dylib
    0x7fff7380d000 -     0x7fff73819ff3  libkxld.dylib (6153.141.2) <EE8ECB4B-2EDB-3440-BBC1-6BDDDF5F1BCE> /usr/lib/system/libkxld.dylib
    0x7fff7381a000 -     0x7fff7381aff7  liblaunch.dylib (1738.140.1) <AFBCBDD3-0B55-3ECD-8E04-A73A3A57356B> /usr/lib/system/liblaunch.dylib
    0x7fff7381b000 -     0x7fff73820ff7  libmacho.dylib (959.0.1) <AA613A9C-961A-3B67-B696-4622FA59FC4E> /usr/lib/system/libmacho.dylib
    0x7fff73821000 -     0x7fff73823ff3  libquarantine.dylib (110.40.3) <F234E51D-FD0B-3EE4-B679-AE3EE9C536C3> /usr/lib/system/libquarantine.dylib
    0x7fff73824000 -     0x7fff73825ff7  libremovefile.dylib (48) <7C7EFC79-BD24-33EF-B073-06AED234593E> /usr/lib/system/libremovefile.dylib
    0x7fff73826000 -     0x7fff7383dff3  libsystem_asl.dylib (377.60.2) <1563EE02-0657-3B78-99BE-A947C24122EF> /usr/lib/system/libsystem_asl.dylib
    0x7fff7383e000 -     0x7fff7383eff7  libsystem_blocks.dylib (74) <0D53847E-AF5F-3ACF-B51F-A15DEA4DEC58> /usr/lib/system/libsystem_blocks.dylib
    0x7fff7383f000 -     0x7fff738c6fff  libsystem_c.dylib (1353.100.2) <BBDED5E6-A646-3EED-B33A-91E4331EA063> /usr/lib/system/libsystem_c.dylib
    0x7fff738c7000 -     0x7fff738caffb  libsystem_configuration.dylib (1061.141.1) <0EE84C33-64FD-372B-974A-AF7A136F2068> /usr/lib/system/libsystem_configuration.dylib
    0x7fff738cb000 -     0x7fff738cefff  libsystem_coreservices.dylib (114) <A199156E-058D-3ABB-BCE9-4B9F20DCED0F> /usr/lib/system/libsystem_coreservices.dylib
    0x7fff738cf000 -     0x7fff738d7fff  libsystem_darwin.dylib (1353.100.2) <5B12B5DB-3F30-37C1-8ECC-49A66B1F2864> /usr/lib/system/libsystem_darwin.dylib
    0x7fff738d8000 -     0x7fff738dffff  libsystem_dnssd.dylib (1096.100.3) <EBB4C2C2-E031-3094-B40A-E67BF261D295> /usr/lib/system/libsystem_dnssd.dylib
    0x7fff738e0000 -     0x7fff738e1ffb  libsystem_featureflags.dylib (17) <29FD922A-EC2C-3F25-BCCC-B58D716E60EC> /usr/lib/system/libsystem_featureflags.dylib
    0x7fff738e2000 -     0x7fff7392fff7  libsystem_info.dylib (538) <8A321605-5480-330B-AF9E-64E65DE61747> /usr/lib/system/libsystem_info.dylib
    0x7fff73930000 -     0x7fff7395cff7  libsystem_kernel.dylib (6153.141.2) <A576A1CF-7726-3146-B04B-A26E1CDB9757> /usr/lib/system/libsystem_kernel.dylib
    0x7fff7395d000 -     0x7fff739a4fff  libsystem_m.dylib (3178) <00F331F1-0D09-39B3-8736-1FE90E64E903> /usr/lib/system/libsystem_m.dylib
    0x7fff739a5000 -     0x7fff739ccfff  libsystem_malloc.dylib (283.100.6) <8549294E-4C53-36EB-99F3-584A7393D8D5> /usr/lib/system/libsystem_malloc.dylib
    0x7fff739cd000 -     0x7fff739daffb  libsystem_networkextension.dylib (1095.140.2) <F06C65C5-2CBE-313C-96E1-A09240F9FE57> /usr/lib/system/libsystem_networkextension.dylib
    0x7fff739db000 -     0x7fff739e4ff7  libsystem_notify.dylib (241.100.2) <FA22F928-D91B-3AA5-96BB-3186AC0FB264> /usr/lib/system/libsystem_notify.dylib
    0x7fff739e5000 -     0x7fff739edfef  libsystem_platform.dylib (220.100.1) <009A7C1F-313A-318E-B9F2-30F4C06FEA5C> /usr/lib/system/libsystem_platform.dylib
    0x7fff739ee000 -     0x7fff739f8fff  libsystem_pthread.dylib (416.100.3) <62CB1A98-0B8F-31E7-A02B-A1139927F61D> /usr/lib/system/libsystem_pthread.dylib
    0x7fff739f9000 -     0x7fff739fdff3  libsystem_sandbox.dylib (1217.141.2) <051C4018-4345-3034-AC98-6DE42FB8273B> /usr/lib/system/libsystem_sandbox.dylib
    0x7fff739fe000 -     0x7fff73a00fff  libsystem_secinit.dylib (62.100.2) <F80872AA-E1FD-3D7E-8729-467656EC6561> /usr/lib/system/libsystem_secinit.dylib
    0x7fff73a01000 -     0x7fff73a08ffb  libsystem_symptoms.dylib (1238.120.1) <5820A2AF-CE72-3AB3-ABCC-273A3419FB55> /usr/lib/system/libsystem_symptoms.dylib
    0x7fff73a09000 -     0x7fff73a1fff2  libsystem_trace.dylib (1147.120) <04B47629-847B-3D74-8ABE-C05EF9DEEFE4> /usr/lib/system/libsystem_trace.dylib
    0x7fff73a21000 -     0x7fff73a26ff7  libunwind.dylib (35.4) <42B7B509-BAFE-365B-893A-72414C92F5BF> /usr/lib/system/libunwind.dylib
    0x7fff73a27000 -     0x7fff73a5cffe  libxpc.dylib (1738.140.1) <3E243A41-030F-38E3-9FD2-7B38C66C35B1> /usr/lib/system/libxpc.dylib

External Modification Summary:
  Calls made by other processes targeting this process:
    task_for_pid: 0
    thread_create: 0
    thread_set_state: 0
  Calls made by this process:
    task_for_pid: 0
    thread_create: 0
    thread_set_state: 0
  Calls made by all processes on this machine:
    task_for_pid: 6608
    thread_create: 0
    thread_set_state: 0

VM Region Summary:
ReadOnly portion of Libraries: Total=1.1G resident=0K(0%) swapped_out_or_unallocated=1.1G(100%)
Writable regions: Total=223.1M written=0K(0%) resident=0K(0%) swapped_out=0K(0%) unallocated=223.1M(100%)
 
                                VIRTUAL   REGION 
REGION TYPE                        SIZE    COUNT (non-coalesced) 
===========                     =======  ======= 
Kernel Alloc Once                    8K        1 
MALLOC                            64.7M       66 
MALLOC guard page                   16K        4 
MALLOC_LARGE (reserved)            256K        1         reserved VM address space (unallocated)
STACK GUARD                          8K        2 
Stack                             16.5M        2 
VM_ALLOCATE                       75.2M      301 
VM_ALLOCATE (reserved)            64.0M        2         reserved VM address space (unallocated)
__DATA                            19.7M      404 
__DATA_CONST                       964K       50 
__LINKEDIT                       626.2M      243 
__OBJC_RO                         32.3M        1 
__OBJC_RW                         1908K        2 
__TEXT                           476.9M      324 
__UNICODE                          564K        1 
shared memory                       12K        3 
===========                     =======  ======= 
TOTAL                              1.3G     1407 
TOTAL, minus reserved VM space     1.3G     1407 

Model: MacBookPro12,1, BootROM 192.0.0.0.0, 2 processors, Dual-Core Intel Core i5, 2.7 GHz, 8 GB, SMC 2.28f7
Graphics: kHW_IntelIris6100Item, Intel Iris Graphics 6100, spdisplays_builtin
Memory Module: BANK 0/DIMM0, 4 GB, DDR3, 1867 MHz, 0x02FE, 0x4544464132333241324D412D4A442D460000
Memory Module: BANK 1/DIMM0, 4 GB, DDR3, 1867 MHz, 0x02FE, 0x4544464132333241324D412D4A442D460000
AirPort: spairport_wireless_card_type_airport_extreme (0x14E4, 0x133), Broadcom BCM43xx 1.0 (7.77.111.1 AirPortDriverBrcmNIC-1615.2)
Bluetooth: Version 7.0.6f7, 3 services, 27 devices, 1 incoming serial ports
Network Service: Wi-Fi, AirPort, en0
Serial ATA Device: APPLE SSD SM0128G, 121.33 GB
USB Device: USB 3.0 Bus
USB Device: Bluetooth USB Host Controller
Thunderbolt Bus: MacBook Pro, Apple Inc., 27.1
```"
44360,Looking for tf-nightly 2.4.0.dev20200817,"Is there any way to get my hands on the tf-nightly 2.4.0.dev20200817 wheel for py36 / manylinux? I have a code that requires tensorflow >=2.4 but I cannot upgrade to CUDA 11, so the builds that are available on PyPI don't work for me anymore. 

Many thanks!

EDIT: looking for both tf-nightly and tf-nightly-gpu of the same version"
44359,InvalidArgumentError: In[0] is not a matrix. Instead it has shape [] [Op:MatMul],"# Tensorflow Custom Activation Func
There is a simple pure TensorFlow multi-layer perceptron with custom activation function and derivative for gradient descent
The built-in **tf.nn.relu** approach is works well and count the gradient properly

**System information**
- MAC OS Catalina 10.15.7
- Python v3.8.5
- TensorFlow v2.3.1

**Standalone code to reproduce the issue**
https://github.com/akorneychuk/tensorflow_custom_activation

**Stacktrace** 
`Traceback (most recent call last):
  File ""/private/var/root/PycharmProjects/untitled_sec/venv/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py"", line 162, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File ""/private/var/root/PycharmProjects/untitled_sec/venv/lib/python3.8/site-packages/tensorflow/python/ops/math_grad.py"", line 1694, in _MatMulGrad
    grad_a = gen_math_ops.mat_mul(grad, b, transpose_b=True)
  File ""/private/var/root/PycharmProjects/untitled_sec/venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 5624, in mat_mul
    _ops.raise_from_not_ok_status(e, name)
  File ""/private/var/root/PycharmProjects/untitled_sec/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 6843, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: In[0] is not a matrix. Instead it has shape [] [Op:MatMul]`
"
44358,Tensorflow randomly crashes running multiple consecutive images through classifier model TF_SessionRun() in c api,"Hi, 

I am hesitant to call this a bug, only because it seems like an obvious use case.  On the other hand, the c api looks pretty basic to me, so I can't figure how what I could be missing here.   Here's all the information I can think of.   Let me know if you need anything else.  I'll be more than happy to oblige.  Thank you.  

**System information**
== check os platform ===============================================
os: Linux
os kernel version: #1 SMP Debian 4.19.132-1 (2020-07-24)
os release version: 4.19.0-10-amd64
os platform: Linux-4.19.0-10-amd64-x86_64-with-debian-10.5
linux distribution: ('debian', '10.5', '')
linux os distribution: ('debian', '10.5', '')
mac version: ('', ('', '', ''), '')
uname: ('Linux', 'starkdg', '4.19.0-10-amd64', '#1 SMP Debian 4.19.132-1 (2020-07-24)', 'x86_64', '')
architecture: ('64bit', 'ELF')
machine: x86_64

== compiler ====================================================
gcc (Debian 8.3.0-6) 8.3.0

Tensorflow version: 2.3.1

**Describe the current behavior**
Performs as expected, except for random crashes on different files - oddly, never the same file. When the same
files are submitted individually, the graph session performs as expected.  There are no log messages to report from tensorflow, just a string of filesystem errors.  Like this:

```
EXT4-fs error (device sda1): ext4_find_entry:1456: inode #390913: common systemd-journal: reading directory lblock 0
```

When this occurs, the only thing possible is a hard reboot.  

A fsck on those file systems reports all fine.  

**Describe the expected behavior**

The program should read all the images in a given directory.  For each image, read the image data, run it through model 
classifier [mobilenetv2](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz) (Use the frozen.pb model in the download archive), and obtain the feature vector for that image from the model.    

**Standalone code to reproduce the issue**

Here is a basic program to reproduce the issue.  I tried to minimize it as much as possible, but it requires
opencv (v3.4.7) and Boost filesystem and system libraries (v1.67.0)  in order to retrieve all files inside a directory and the
image data for each file.   The directory of images must contain all images  that opencv installation can read. 

OpenCV  v3.4.7
Boost  v1.67.0

```
Invoke program:  ./tftest /path/to/image/files/dir
```

Put the frozen .pb model file in the same directory as the example code.  or change the `model_file` variable in the code so it can read in the model graph.

```
/** 
	g++ -otftest tftest.cpp -g -O0 -Wall -lboost_filesystem -lboost_system 
                   -lopencv_core -lopencv_imgcodecs -lopencv_imgproc -ltensorflow
 **/
#include <iostream>
#include <cstdlib>
#include <string>
#include <cassert>
#include <boost/filesystem.hpp>
#include <opencv2/core/core_c.h>
#include <opencv2/imgproc/imgproc_c.h>
#include <opencv2/imgcodecs/imgcodecs_c.h>
#include <tensorflow/c/c_api.h>

using namespace std;
using namespace cv;

namespace fs=boost::filesystem;

static const string model_file = ""mobilenet_v2_1.4_224_frozen.pb"";       //location of frozen model file 
static const string input_layer = ""input"";                                                   // input output layers
static const string output_layer = ""MobilenetV2/Logits/AvgPool"";

static int Height = 224;   // target image size 
static int Width = 224;

void NoOpDeAlloc(void *data, size_t len, void *arg){}

void freebuffer(void *data, size_t len){
	free(data);
}

CvMat* load_image(const string &filename, const int height, const int width){

	CvMat *src = cvLoadImageM(filename.c_str(), CV_LOAD_IMAGE_COLOR);
	if (src == NULL) return NULL;

	CvMat *img_prime = cvCreateMat(src->rows, src->cols, CV_32FC3);
	CvMat *img_resized = cvCreateMat(height, width, CV_32FC3);
	CvMat *dst = cvCreateMat(height, width, CV_32FC3);
	CvMat *div = cvCreateMat(height, width, CV_32FC3);

	/* convert to float type */
	cvConvertScale(src, img_prime, 1, 0);

	/* resize to heightxwidth */
	cvResize(img_prime, img_resized, CV_INTER_CUBIC);

	/* normalize values */
	CvScalar norm_constant = {255, 255, 255};
	cvSet(div, norm_constant, NULL);
	cvDiv(img_resized, div, dst, 1);

	cvReleaseMat(&src);
	cvReleaseMat(&img_resized);
	cvReleaseMat(&img_prime);
	cvReleaseMat(&div);
	return dst;
}

TF_Buffer* readfromfile(const string &filename){
	FILE *file = fopen(filename.c_str(), ""rb"");
	if (file == NULL) return NULL;
	
	fseek(file, 0, SEEK_END);
	size_t file_size = ftell(file);
	fseek(file, 0, SEEK_SET);
	assert(file_size > 0);
	
	unsigned char *data = (unsigned char*)malloc(file_size);
	size_t n = fread(data, 1, file_size, file);
	assert(n == file_size);

	TF_Buffer *buf = TF_NewBuffer();
	buf->data = data;
	buf->length = file_size;
	buf->data_deallocator = freebuffer;
	fclose(file);
	return buf;
}


int ProcessFiles(const fs::path &dirname){
	fs::recursive_directory_iterator dir(dirname), end;

	int ndims = 4;
	int64_t dims[4] = { 1, Height, Width, 3};

	cout << ""TF Model graph setup"" << endl;
	TF_Graph *graph = TF_NewGraph();
	TF_Status *status = TF_NewStatus();

	TF_ImportGraphDefOptions *opts = TF_NewImportGraphDefOptions();
	TF_Buffer *graph_def = readfromfile(model_file);
	assert(graph_def != NULL);

	TF_GraphImportGraphDef(graph, graph_def, opts, status);
	assert(TF_GetCode(status) == TF_OK);
	
	TF_SessionOptions *sess_opts = TF_NewSessionOptions();
	TF_Session *sess = TF_NewSession(graph, sess_opts, status);
	TF_Tensor *img_tensor;
	TF_Tensor *output_tensor;
	TF_Output input = { TF_GraphOperationByName(graph, input_layer.c_str()), 0 };
	TF_Output output = { TF_GraphOperationByName(graph, output_layer.c_str()), 0 };
	assert(input.oper != NULL);
	assert(output.oper != NULL);
	
	cout << ""Read files "" << endl;
	
	int count = 0;
	for (;dir!=end;++dir){
		string filename = dir->path().string();
		if (fs::is_directory(dir->status())){
			cout << ""directory: "" << filename << endl;
		} else if (fs::is_regular_file(dir->status())){
			CvMat *img = load_image(filename, Height, Width);
			if (img != NULL){
				cout << dec << ""("" << ++count << "") "" << filename << endl;

				int total = Height*Width*3;
				img_tensor = TF_NewTensor(TF_FLOAT, dims, ndims, (void*)img->data.fl,
										  total*sizeof(float), NoOpDeAlloc, NULL);
				assert(img_tensor != NULL);
				
				TF_SessionRun(sess, NULL, &input, &img_tensor, 1,
							  &output, &output_tensor, 1,
							  NULL, 0, NULL, status);

				if (TF_GetCode(status) != TF_OK){
					cout << ""unale to complete session: "" << TF_Message(status) << endl;
					break;
				}

				TF_DeleteTensor(img_tensor);
				TF_DeleteTensor(output_tensor);
				img_tensor = NULL;
				output_tensor = NULL;
				cvReleaseMat(&img);
			}
		} else {
			cout << ""special file: "" << filename << endl;
		}
	}

	TF_DeleteBuffer(graph_def);
	TF_DeleteSessionOptions(sess_opts);
	TF_DeleteImportGraphDefOptions(opts);
	TF_DeleteGraph(graph);
	TF_DeleteSession(sess, status);
	TF_DeleteStatus(status);
	return count;
}

int main (int argc, char **argv){
	assert(argc == 2);
	string dir = argv[1];


	cout << ""Tensorflow: "" << TF_Version() << endl;
	
	cout << ""directory: "" << dir << endl;
	
	int n = ProcessFiles(dir);
	
	cout << ""Processed files: "" << n << endl;
	cout << ""Done"" << endl;
	return 0;
}
```

**Other info / logs** Include any logs or source code that would be helpful to

"
44357,Couldn't find a way to convert tflite to tensorflow graph,"Hello all,

I am new to this world so hopefully my question will be as clear as possible and related to this forum.
I want to be able to convert tflite file to tensorflow graph.
I have tried several ways:

tflite --> onnx : I used
tflite2onnx.convert(tflite_path, onnx_path)
Result:
NotImplementedError: Unsupported TFLite OP: 41
It seems there is an unsupported operations (maybe more) and I was unable to convert

tflite --> pb
I looked on the net and found that it is possible only up to tf 1.9 so I downgraded my tf to 1.9
and tried following the instructions (which included using bazel, couldn't even install bazel)

Anyway Is there a known way (either one of the above or any other you can think of)
that I can use and hopefully works?

Thanks in advance
Chen"
44356,[Violation] 'requestAnimationFrame' handler took <N>ms tfjs hand model demo,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
Windows 10 x64
Chrome Version 86.0.4240.111 (Official Build) (64-bit)
tfjs version used in online demo at https://storage.googleapis.com/tfjs-models/demos/handpose/index.html

**Describe the current behavior**
1) In Chrome open https://storage.googleapis.com/tfjs-models/demos/handpose/index.html
2) Open DevTools
3) After the detection is running refresh the page
Error [Violation] 'requestAnimationFrame' handler took <N>ms is thrown after refresh

**Describe the expected behavior**
No error after refresh

"
44354,Enable control of use of caching in tf.keras.preprocessing.image_dataset_from_directory,"**System information**
- TensorFlow version (you are using): TF 2.3.0 
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
This concerns the API tf.keras.preprocessing.image_dataset_from_directory(...) I read through the doc as well as the source code, it doesn't seem like there's any control over behavior concerning cache() for the tf.data.Dataset. As best I could understand this, it won't be right to apply .cache(...) to the output tf dataset of this API since shuffling is done (esp. for train) and caching anything after, it freezes the initial shuffling order, such that it stays fixed at subsequent epochs. According to general wisdom, this is highly likely not desirable. The caching should be done before shuffling, but there's no way to specify this at API call time. The only workaround I could think of is to copy/paste the exact code, and write your own version.


**Will this change the current api? How?**
One can add this feature without affecting old behavior. Cache is simply assumed False unless supplied otherwise, and if it is false, the old code path should execute.

**Who will benefit with this feature?**
image_dataset_from_directory(...) is a nice convenient method and caching may improve performance by not repeating the image resizing processing. It can also help if your original directories are remote and may suffer outage or unpredictable lag.

**Any Other info.**
"
44353,Unreferenced buffers in flatbuffer,"When using `inference_input_type = tf.int8` or `inference_output_type = tf.int8`, there are unrefenrenced buffers leftover in the flatbuffer. The problem is in [this function](https://github.com/tensorflow/tensorflow/blob/b0784e587b62eec6967196b745bba4db3a90ab0c/tensorflow/lite/python/util.py#L632), which cleans up unreferenced *tensors* [over here](https://github.com/tensorflow/tensorflow/blob/b0784e587b62eec6967196b745bba4db3a90ab0c/tensorflow/lite/python/util.py#L710) but it does not cleanup unreferenced *buffers* (i.e. the `BufferT` object).

It can be reproduced with:

```python
import tensorflow as tf

def mymodel():
    img = tf.keras.layers.Input(shape=(96, 96, 3))
    x = img
    x = tf.quantization.fake_quant_with_min_max_vars(x, -3, 3)
    x = tf.keras.layers.Conv2D(32, 3)(x)
    x = tf.quantization.fake_quant_with_min_max_vars(x, -3, 3)
    return tf.keras.Model(img, x)

converter = tf.lite.TFLiteConverter.from_keras_model(mymodel())
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
tflite_model = converter.convert()
```

In the above example, when `inference_input_type` and `inference_output_type` are not set, the flatbuffer will contain 7 `Buffer` objects, of which `Buffer` 0 is empty and unreferenced by definition of the schema. `Buffer`s 1 and 6 are referenced by the input and output tensors of the model. When `inference_input_type` and `inference_output_type` are set to `tf.int8`, then `Buffer`s 1 and 6 remain in the flatbuffer file but are unreferenced.

@MeghnaNatraj can you take a look at this? (I saw you pushed the code related to this)

**System information**
- OS Platform and Distribution: Arch Linux
- TensorFlow installed from (source or binary): binary
- TensorFlow version: `tf-nightly` on 27 Oct 2020."
44352,InvalidArgumentError with MirroredStrategy under graph mode when using tf.summary,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):2.3.1
- Python version:3.7.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):6.3.0
- CUDA/cuDNN version: CUDA Version: 10.1 
- GPU model and memory: Tesla T4 * 2


**Describe the current behavior**
When I use tf.summary and tf.map_fn(tf.io.decode_jpeg) under distributed strategy and graph mode, the error appears.
When I remove tf.summary, it works well. 
When I remove tf.function, it works well.
When I remove tf.map_fn, it works well.
When I remove tf.io.decode_jpeg out of the serving() scope, like putting it in the __init__ func, it works well.


**Describe the expected behavior**
I guess this is the bug of tensorflow.

**Standalone code to reproduce the issue**
```
import tensorflow as tf  
from tensorflow.keras import layers  

class Evaluation:

    def __init__(self, strategy=None):  
        
        # prepare for encoded img data
        self.strategy = strategy
        H, W, C = 10, 10, 3
        imgs = tf.cast(tf.zeros([8, H, W, C]), tf.uint8)
        encodes = []
        for img in imgs:
            encode = tf.io.encode_jpeg(img)
            encodes.append(encode)
        encodes = tf.stack(encodes, axis = 0) 
        
        # convert encoded img data to tf.data
        self.dataset = tf.data.Dataset.from_tensor_slices(encodes)
        self.dataset = self.dataset.batch(2)
        self.dataset = self.strategy.experimental_distribute_dataset(self.dataset)
        with self.strategy.scope():
            self.conv = layers.Conv2D(32, (1, 1), strides=(1, 1), padding='same')
        self.parallel_iterations = 10
    
    def preprocess(self, encoded):
        # preprocess for tf.data
        image = tf.io.decode_jpeg(encoded, channels=3)
        image = tf.image.resize(image, [20,20])
        return image

    @tf.function
    def serving(self, inputs):
        # data preprocess
        image = tf.map_fn(self.preprocess,
                          inputs,
                          fn_output_signature=tf.float32,
                          parallel_iterations=self.parallel_iterations)
        
        # inference for each batch
        prediction = self.conv(image)
        return prediction

    @tf.function
    def infer(self, serve_summary_writer):
        # inference for all batches
        with serve_summary_writer.as_default():
            batch = tf.cast(0, tf.int64)
            for data in self.dataset:
                prediction_perReplica = strategy.run(self.serving, args=(data,))
                prediction_tensor = prediction_perReplica.values
                prediction_concat = tf.concat(prediction_tensor, axis = 0)
                tf.summary.write(tag=""prediction"", tensor=prediction_concat, step=batch)
                batch += 1
                
    def eval(self):
        serve_summary_writer = tf.summary.create_file_writer('save_file', max_queue=100000, flush_millis=100000)
        self.infer(serve_summary_writer)
        serve_summary_writer.close()
        tf.io.gfile.rmtree('save_file')  

if __name__ == ""__main__"":

    strategy = tf.distribute.MirroredStrategy()
    e = Evaluation(strategy)   
    e.eval()
```

**Other info / logs** 
Traceback (most recent call last):
  File ""test.py"", line 66, in <module>
    e.eval()
  File ""test.py"", line 58, in eval
    self.infer(serve_summary_writer)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 846, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 550, in call
    ctx=ctx)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 3 root error(s) found.
  (0) Invalid argument:  2 root error(s) found.
  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
         [[{{node while/body/_17/while/StatefulPartitionedCall/map/TensorArrayUnstack/TensorListFromTensor/_109}}]]
         [[while/loop_body_control/_45/_96]]
  (1) Invalid argument:  2 root error(s) found.
  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
         [[{{node while/body/_17/while/StatefulPartitionedCall/map/TensorArrayUnstack/TensorListFromTensor/_109}}]]
  (2) Invalid argument:  2 root error(s) found.
  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
         [[{{node while/body/_17/while/StatefulPartitionedCall/map/TensorArrayUnstack/TensorListFromTensor/_109}}]]
         [[Func/while/body/_17/while/StatefulPartitionedCall/replica_1/map/while/body/_181/input/_266/_187]]
0 successful operations.
0 derived errors ignored. [Op:__inference_infer_620]

Function call stack:
infer -> infer -> infer
"
44350,DeeplabV3+ training documentation improvement,"###  Description of issue (what needs changing):

Learning rate documentation and training output documentation.

### Clear description

For ML in general the parameter choice is extremely important to have success. You have done incredible work into research of challenging algorithms which help the whole world to improve. However for me as an solution worker it is incredible hard to mine all the details of your ML framework. I am missing fundamental practical documentation about detailed parameter choice involved during training as well as which algorithm to chose. Nowhere is anything said which parameters are a good start to start learning. I am speaking of parameters like weight_decay, base_learning_rate and so on.
When you don't document this all your work is nonsense and not very helpful.
The same applies to training output and interpretation, what does loss mean related to a specific algorithm? Which algorithm to chose why for what purpose and what are upsides and downsides of either?

I know we are dealing with a very dynamic field of science, every now and then there are new algorithms and approaches. However it is very important to have some kind of documentation available which is more than just here is a notebook start it and you will see. Or here is my model, it is working so cool and fast, take it but leave me alone.
Google or whomever it might affect please put some more effort in this middle layer documentation to have your code and inventions used. And let me say this, yes I read it..."
44349,Model does not learn when using the GPU,"### System information
- OS Platform and Distribution: Linux Ubuntu 18.04.5 LTS (Bionic Beaver)
- TensorFlow installed from: Conda
- TensorFlow version: 2.2.0
- Python version: 3.6.9
- CUDA/cuDNN version: 10.1.243 / 7.6.5
- GPU model and memory: RTX 3090 (24,265 MiB) (driver: 455.32.00)

### Describe the current behavior
When using tensorflow with the CPU (tensorflow) the model achieves a test accuracy of +0.90 but when running the same code with the GPU (tensorflow-gpu) the model achieves a test accuracy of ~0.10. So it seems like the CPU version learns while the GPU version does not. The same problem is present when running another simple example code with keras.

### Describe the expected behavior
I would expect the two version to have around the same test accuracy in the end. Or at least it would learn with the GPU version. 

### Code to reproduce the issue
I'm using the following code: 
```python
# Code from: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/convolutional_network.ipynb

import tensorflow as tf
from tensorflow.keras import Model, layers
import numpy as np

print(tf.__version__)
print(tf.config.list_physical_devices('GPU'))

# MNIST dataset parameters.
num_classes = 10 # total classes (0-9 digits).

# Training parameters.
learning_rate = 0.001
training_steps = 200
batch_size = 128
display_step = 10

# Network parameters.
conv1_filters = 32 # number of filters for 1st conv layer.
conv2_filters = 64 # number of filters for 2nd conv layer.
fc1_units = 1024 # number of neurons for 1st fully-connected layer.


# Prepare MNIST data.
from tensorflow.keras.datasets import mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
# Convert to float32.
x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)
# Normalize images value from [0, 255] to [0, 1].
x_train, x_test = x_train / 255., x_test / 255.

# Use tf.data API to shuffle and batch data.
train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)

# Create TF Model.
class ConvNet(Model):
    # Set layers.
    def __init__(self):
        super(ConvNet, self).__init__()
        # Convolution Layer with 32 filters and a kernel size of 5.
        self.conv1 = layers.Conv2D(32, kernel_size=5, activation=tf.nn.relu)
        # Max Pooling (down-sampling) with kernel size of 2 and strides of 2.
        self.maxpool1 = layers.MaxPool2D(2, strides=2)

        # Convolution Layer with 64 filters and a kernel size of 3.
        self.conv2 = layers.Conv2D(64, kernel_size=3, activation=tf.nn.relu)
        # Max Pooling (down-sampling) with kernel size of 2 and strides of 2.
        self.maxpool2 = layers.MaxPool2D(2, strides=2)

        # Flatten the data to a 1-D vector for the fully connected layer.
        self.flatten = layers.Flatten()

        # Fully connected layer.
        self.fc1 = layers.Dense(1024)
        # Apply Dropout (if is_training is False, dropout is not applied).
        self.dropout = layers.Dropout(rate=0.5)

        # Output layer, class prediction.
        self.out = layers.Dense(num_classes)

    # Set forward pass.
    def call(self, x, is_training=False):
        x = tf.reshape(x, [-1, 28, 28, 1])
        x = self.conv1(x)
        x = self.maxpool1(x)
        x = self.conv2(x)
        x = self.maxpool2(x)
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.dropout(x, training=is_training)
        x = self.out(x)
        if not is_training:
            # tf cross entropy expect logits without softmax, so only
            # apply softmax when not training.
            x = tf.nn.softmax(x)
        return x

# Build neural network model.
conv_net = ConvNet()


# Cross-Entropy Loss.
# Note that this will apply 'softmax' to the logits.
def cross_entropy_loss(x, y):
    # Convert labels to int 64 for tf cross-entropy function.
    y = tf.cast(y, tf.int64)
    # Apply softmax to logits and compute cross-entropy.
    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x)
    # Average loss across the batch.
    return tf.reduce_mean(loss)

# Accuracy metric.
def accuracy(y_pred, y_true):
    # Predicted class is the index of highest score in prediction vector (i.e. argmax).
    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))
    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)

# Stochastic gradient descent optimizer.
optimizer = tf.optimizers.Adam(learning_rate)


# Optimization process.
def run_optimization(x, y):
    # Wrap computation inside a GradientTape for automatic differentiation.
    with tf.GradientTape() as g:
        # Forward pass.
        pred = conv_net(x, is_training=True)
        # Compute loss.
        loss = cross_entropy_loss(pred, y)

    # Variables to update, i.e. trainable variables.
    trainable_variables = conv_net.trainable_variables

    # Compute gradients.
    gradients = g.gradient(loss, trainable_variables)

    # Update W and b following gradients.
    optimizer.apply_gradients(zip(gradients, trainable_variables))


# Run training for the given number of steps.
for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):
    # Run the optimization to update W and b values.
    run_optimization(batch_x, batch_y)

    if step % display_step == 0:
        pred = conv_net(batch_x)
        loss = cross_entropy_loss(pred, batch_y)
        acc = accuracy(pred, batch_y)
        print(""step: %i, loss: %f, accuracy: %f"" % (step, loss, acc))


# Test model on validation set.
pred = conv_net(x_test)
print(""Test Accuracy: %f"" % accuracy(pred, y_test))
```

### Other info / logs
The output from the CPU version: 
```
2.2.0
[]
2020-10-27 13:57:03.982255: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-10-27 13:57:04.003444: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 4149920000 Hz
2020-10-27 13:57:04.004916: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556240ba48c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-27 13:57:04.004927: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-27 13:57:04.004979: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
step: 10, loss: 1.765925, accuracy: 0.859375
step: 20, loss: 1.604248, accuracy: 0.906250
step: 30, loss: 1.642668, accuracy: 0.882812
step: 40, loss: 1.594136, accuracy: 0.937500
step: 50, loss: 1.557037, accuracy: 0.953125
step: 60, loss: 1.549416, accuracy: 0.937500
step: 70, loss: 1.530980, accuracy: 0.976562
step: 80, loss: 1.546553, accuracy: 0.937500
step: 90, loss: 1.518947, accuracy: 0.968750
step: 100, loss: 1.525878, accuracy: 0.953125
step: 110, loss: 1.492367, accuracy: 0.992188
step: 120, loss: 1.498649, accuracy: 0.984375
step: 130, loss: 1.515978, accuracy: 0.960938
step: 140, loss: 1.522711, accuracy: 0.976562
step: 150, loss: 1.496059, accuracy: 0.976562
step: 160, loss: 1.501745, accuracy: 0.976562
step: 170, loss: 1.488870, accuracy: 0.984375
step: 180, loss: 1.504619, accuracy: 0.992188
step: 190, loss: 1.480513, accuracy: 1.000000
step: 200, loss: 1.489994, accuracy: 0.984375
Test Accuracy: 0.976200

Process finished with exit code 0
```

The output from the GPU version:
```
2.2.0
2020-10-27 13:52:51.391410: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-10-27 13:52:51.424025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-27 13:52:51.424711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:21:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.86GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2020-10-27 13:52:51.424829: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-10-27 13:52:51.425678: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-10-27 13:52:51.426660: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-10-27 13:52:51.426788: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-10-27 13:52:51.427656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-10-27 13:52:51.428145: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-10-27 13:52:51.430066: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-10-27 13:52:51.430149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-27 13:52:51.430858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-27 13:52:51.431506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
2020-10-27 13:52:51.654160: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-10-27 13:52:51.659773: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 4149920000 Hz
2020-10-27 13:52:51.661105: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b3ed5bed50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-27 13:52:51.661115: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-27 13:52:51.661262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-27 13:52:51.661963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:21:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.86GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2020-10-27 13:52:51.661986: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-10-27 13:52:51.661992: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-10-27 13:52:51.661997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-10-27 13:52:51.662002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-10-27 13:52:51.662007: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-10-27 13:52:51.662012: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-10-27 13:52:51.662017: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-10-27 13:52:51.662051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-27 13:52:51.662717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-27 13:52:51.663356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-10-27 13:52:51.663379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-10-27 13:52:51.720749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-27 13:52:51.720769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-10-27 13:52:51.720775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-10-27 13:52:51.720899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-27 13:52:51.721550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-27 13:52:51.722176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-27 13:52:51.722790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21939 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:21:00.0, compute capability: 8.6)
2020-10-27 13:52:51.723983: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b3ee94ec90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-10-27 13:52:51.723991: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
2020-10-27 13:52:52.629936: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-10-27 13:52:54.157774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
step: 10, loss: 2.302645, accuracy: 0.101562
step: 20, loss: 2.302479, accuracy: 0.101562
step: 30, loss: 2.302398, accuracy: 0.125000
step: 40, loss: 2.302354, accuracy: 0.164062
step: 50, loss: 2.301157, accuracy: 0.140625
step: 60, loss: 2.301878, accuracy: 0.140625
step: 70, loss: 2.302341, accuracy: 0.109375
step: 80, loss: 2.303088, accuracy: 0.039062
step: 90, loss: 2.302307, accuracy: 0.085938
step: 100, loss: 2.302237, accuracy: 0.125000
step: 110, loss: 2.302458, accuracy: 0.125000
step: 120, loss: 2.301577, accuracy: 0.171875
step: 130, loss: 2.301929, accuracy: 0.109375
step: 140, loss: 2.302793, accuracy: 0.101562
step: 150, loss: 2.302621, accuracy: 0.125000
step: 160, loss: 2.302866, accuracy: 0.078125
step: 170, loss: 2.301470, accuracy: 0.164062
step: 180, loss: 2.301091, accuracy: 0.156250
step: 190, loss: 2.302170, accuracy: 0.132812
step: 200, loss: 2.302764, accuracy: 0.117188
Test Accuracy: 0.113500

Process finished with exit code 0
```

"
44348,CTC Error crashes on empty GPU batch,"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS
- TensorFlow installed from (source or binary): binary 
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.8.5
- CUDA/cuDNN version: 11.0
- GPU model and memory: GeForce RTX 2080 Ti 11GB x8

When I run the following notebook with 8 GPUs activated, it fails:
https://colab.research.google.com/drive/1yU02l0WtSW2mp7Ai-UvUF18Z2vbNPbzy?usp=sharing

If I run it with 1 GPU, or on CPU, it works. If I change ""valnum"" from 65 to 64, it works. 

It seems like the CTC Error function doesn't handle empty batches being distributed to GPUs?

The error messages when it fails are mostly variations of:
`Invalid argument:  Tried to stack elements of an empty list with non-fully-defined element_shape: [1,?]
	 [[node replica_6/functional_5/ctc/scan/TensorArrayV2Stack/TensorListStack (defined at <ipython-input-3-6220b8957bcc>:14) ]]
	 [[replica_2/functional_5/ctc/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int32_Squeeze_1/_499]]`

Full error log: [errorlog.txt](https://github.com/tensorflow/tensorflow/files/5445445/errorlog.txt)

(This has been a problem on a transfer learning application, where I have a very large training set and need to use all 8 GPUs, but a very small validation set, which crashes if I use all 8 GPUs.)"
44346,Problems generating the Hello-World-Project von ESP32,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linuxmint in VirtualBox
- TensorFlow installed from (source or binary): viva git clone https://github.com/tensorflow/tensorflow.git
- Tensorflow version (commit SHA if source):
- Target platform: ESP32 

**Describe the problem**
Hello guys,

I can't generate projects for ESP32 using make. If  I run:
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project

I get:
tensorflow/lite/micro/tools/make/Makefile:418: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
tensorflow/lite/micro/tools/make/Makefile:418: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
tensorflow/lite/micro/tools/make/Makefile:378: tensorflow/lite/micro/tools/make/targets/esp_makefile.inc: No such file or directory
make: *** No rule to make target 'tensorflow/lite/micro/tools/make/targets/esp_makefile.inc'. Stop.


Maybe this issue is alrady fixed:
https://github.com/tensorflow/tensorflow/pull/44316
If this is so, a big ""Thank you!"" do the person how did this! 
Where do I get the fixed version? (Sry, I'm new to this topic)

Thank you for all attempts! 

**Please provide the exact sequence of commands/steps when you ran into the problem**
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project
after clone git:
git clone https://github.com/tensorflow/tensorflow.git
"
44345,cmake support of 2.0.0-rc2,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0.0-rc2
- Python version: 3.5
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): 19.23.28105.4
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
## Can not compile tensorflow from source using cmake in tensorflow\contrib\cmake\CMakeLists.txt : 
## Target proto_text has dependency on tensorflow\core\framework\tensor_shape.cc when linking. 
## And this file is not included in the dependencies of proto_text , the only 2 dependencies of proto_text are tf_protos_cc and externals (zlib,png...etc.). 
## Please help!

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44343,Bug in CosineSimilarity ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Colab

while using ```CosineSimilarity``` loss  with the array,
```
    rng = jax.random.PRNGKey(42)

    y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2)
    y_pred = jax.random.uniform(rng, shape=(2, 3))
```
I'm getting the following error:
``` 
TypeError: Cannot convert 1e-12 to EagerTensor of dtype int32
```
**Colab Notebook to reproduce error**
https://colab.research.google.com/drive/1diVmzp2V-jqsH8UamIDRSmbp5gLGRAtN?usp=sharing

**Do you want to contribute?**
Yes, I want to contribute please give me some more detail about the bug and I'll definetely raise a PR to fix the bug.

"
44342,Unsupported loss function in seq2seq model.,"I am exploring the following (official Google DataLab) tensorflow example: https://github.com/googledatalab/notebooks/blob/master/samples/TensorFlow/LSTM%20Punctuation%20Model%20With%20TensorFlow.ipynb which apparently is written in tf v1, so I upgraded with the v2 upgrade script and there were three main inconsistencies: 

```
ERROR: Using member tf.contrib.rnn.DropoutWrapper in deprecated module tf.contrib. tf.contrib.rnn.DropoutWrapper cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.
ERROR: Using member tf.contrib.legacy_seq2seq.sequence_loss_by_example in deprecated module tf.contrib. tf.contrib.legacy_seq2seq.sequence_loss_by_example cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.
ERROR: Using member tf.contrib.framework.get_or_create_global_step in deprecated module tf.contrib. tf.contrib.framework.get_or_create_global_step cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.
```

So for compatibility I manually replaced `framework.get_or_create_global_step` with `tf.compat.v1.train.get_or_create_global_step`, and also `rnn.DropoutWrapper` with `tf.compat.v1.nn.rnn_cell.DropoutWrapper`.

But I was unable to find a solution on how to handle the `tf.contrib.legacy_seq2seq.sequence_loss_by_example` method, since I cannot find a backwards compatible alternative. I tried installing Tensroflow Addons and use [its seq2seq loss function](https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq/sequence_loss), but wasn't able to figure out how to adapt it to work with the rest of the code.

Stumbled across some errors like `Consider casting elements to a supported type.` or `Logits must be a [batch_size x sequence_length x logits] tensor`, because probably i am not implementing something correctly.

**My question:** How to implement supported tensorflow v2 alternative of this loss function, so it acts similarly to the code below?

```python
    output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, size])
    softmax_w = tf.compat.v1.get_variable(""softmax_w"", [size, len(TARGETS)], dtype=tf.float32)
    softmax_b = tf.compat.v1.get_variable(""softmax_b"", [len(TARGETS)], dtype=tf.float32)
    logits = tf.matmul(output, softmax_w) + softmax_b
    self._predictions = tf.argmax(input=logits, axis=1)    
    self._targets = tf.reshape(input_.targets, [-1])
    loss = tfa.seq2seq.sequence_loss(
        [logits],
        [tf.reshape(input_.targets, [-1])],
        [tf.ones([batch_size * num_steps], dtype=tf.float32)])
    self._cost = cost = tf.reduce_sum(input_tensor=loss) / batch_size
    self._final_state = state
```

*Full code [here](https://github.com/googledatalab/notebooks/blob/master/samples/TensorFlow/LSTM%20Punctuation%20Model%20With%20TensorFlow.ipynb).*"
44341,pruning object detection models,hi is there a way to prune object detection models that are frozen or in saved model format? I have taken a pre-trained model from the TensorFlow zoo and retrained with my own dataset. I would like to prune the model. I have tried several ways but nothing works. Can you help me out with the procedure to do so?. Thank you.
44339,Tensorflow auto killed my training processing.,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-nightly-2.4.0
- Python version:
- Bazel version (if compiling from source): 3.7.4
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: CUDA 11.0 / cuDNN 8.0.4
- GPU model and memory: RTX3090 24GB
- RAM: 32GB


I'm training my Faster RCNN model.And I train it in my old GPU(1080 8GB).It didn't occur any error.Last week, I changed to RTX3090 and environment. When I run my code, I can run normal in first one hour. After that keras show the code runtime getting longer. I don't get any information, just get 'killed'.Someone saies  maybe high RAM usage, and I watch my memory, it auto increasing indeed. But I can't find code bug.
So I want to know if tf-nightly-gpu version bug?Or GPU environment setting?

Thanks~
"
44337,Concrete Function output shape sometimes changes after save/load cycle,"Output of environment capture script:

```
== check python ===================================================
python version: 3.7.8
python branch:
python build version: ('default', 'Aug 10 2020 13:15:25')
python compiler version: Clang 10.0.0 (clang-1000.10.44.4)
python implementation: CPython


== check os platform ===============================================
os: Darwin
os kernel version: Darwin Kernel Version 17.7.0: Thu Dec 20 21:47:19 PST 2018; root:xnu-4570.71.22~1/RELEASE_X86_64
os release version: 17.7.0
os platform: Darwin-17.7.0-x86_64-i386-64bit
linux distribution: ('', '', '')
linux os distribution: ('', '', '')
mac version: ('10.13.6', ('', '', ''), 'x86_64')
uname: uname_result(system='Darwin', node='...', release='17.7.0', version='Darwin Kernel Version 17.7.0: Thu Dec 20 21:47:19 PST 2018; root:xnu-4570.71.22~1/RELEASE_X86_64', machine='x86_64', processor='i386')
architecture: ('64bit', '')
machine: x86_64


== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 10.0.0 (clang-1000.10.44.4)
Target: x86_64-apple-darwin17.7.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== check pips ===================================================
numpy                         1.18.5
protobuf                      3.10.0
tensorflow                    2.3.1
tensorflow-estimator          2.3.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.3.1
tf.version.GIT_VERSION = v2.3.0-54-gfcc4b966f1
tf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 147: nvidia-smi: command not found

== cuda libs  ===================================================

== tensorflow installed from info ==================
Name: tensorflow
Version: 2.3.1
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: .../python3.7/site-packages
Required-by: 

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 7, 8, 'final', 0)

== bazel version  ===============================================
```

**Describe the current behavior**

The `structured_outputs` of a concrete function might not match after saving and restoring a model containing that function via tf.saved_model. In the example below, the function doesn't change the shape of the tensor, but, when requesting a concrete function for a tensor with shape [None, 3], a more specific output [7, 3] is returned instead.

**Describe the expected behavior**

The output shape should not be more specific than the true output.

**Standalone code to reproduce the issue**
Reproduction script and stdout of `for i in $(seq 10); do echo $i; python repro.py; done` in this Gist: https://gist.github.com/gmacon/057cc64bf849c8d65974daec56a037b7

**Other info / logs**
Initially, I thought this bug was deterministic, but it's clearly actually random. Based on running the reproduction script in a loop, it appears to work correctly about half of the time. Question: does this have something to do with the traversal order of a randomized hash table?

Edit again: I initially thought it was deterministic because it was happening every time in the system I'm building. However, I was reusing the same serialized model over and over in those tests. It appears to work in about half the *saves*, not half the *loads*.

~As far as I can tell, this was working correctly in Tensorflow 2.2.~ Edit: I'm not sure about that any more.

Edit again again: I reproduced this in Tensorflow 2.0.0."
44336,[TFLite] [GPU delegate] DataLayout::DHWC4 and ObjectType::OPENGL_SSBO input does not work,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nexus 5X
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.2.0 / 2.3.1
- Python version: 2.7.16
- Bazel version (if compiling from source): 2.0.0

**Describe the current behavior**

The GPU delegate readme explains how tensors are always converted to the DHWC4 format, which I understand, and by providing the input in the correct format we can avoid a useless copy.

I'm using a slightly modified build of tensorflow (See [v2.2.0](https://github.com/deepmedia/tensorflow/commit/7401fbb4fa0c94004865c089d8c89bdd566ad747) and [v2.3.1](https://github.com/deepmedia/tensorflow/commit/ac8aad5c55838566a26ed6725c966d399319c831)) in order to inject `OPENGL_SSBO` inputs into the delegate.

**In v2.2.0**, adding a SSBO with `DataLayout::DHWC4` would fail with a ""Not supported"" error.
The error was thrown in [these lines](https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/lite/delegates/gpu/gl/api2.cc#L138-L141):

```c++
    // TODO(akulik): external object should propagate to internal.
    if (IsSameDef()) {
      return UnimplementedError(""Not supported"");
    }
```

**In v2.3.1**, a commit was added, named ""Fix propagation of objects with same defs."", which removes this error:

https://github.com/tensorflow/tensorflow/commit/3e414c752be1a259c64b1f85a24e095d02dd5e2f

But it just removes the exception, without actually implementing the propagation. I could not find any other commit that includes the actual implementation. The result is that if I try to pass a DHWC4 SSBO, I get no error, but the input is all zeros. 
Am I missing something? If so, can you point me to the code/commit that actually reuses the input SSBO id? 

**Describe the expected behavior**

We should be able to send a SSBO buffer with DataLayout::DHWC4 and avoid a useless copy. If it's not implemented, we should get an error."
44333,"why gradient of clip_by_value(x) is zero, when x is outside of range?","I agree with derivative of mathematical notation,
because output value is not changing when x is outside of range

![xnviewmp_2020-10-26_22-02-22](https://user-images.githubusercontent.com/8076202/97212824-54538080-17da-11eb-9d23-d76828e3cbd5.jpg)

But let look at it from the ML perspective.

Let `x = -2`
we can just simply decide `x = x+1` 

thus `Clip(x, -1, 1) = x + 1 = -1`

gradient for `x+y` is `1` for both

Therefore gradient of clip(x) should be 1 for any value and range ?
"
44332,TF Lite GPU delegate - Selection of GPU device,"-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Platform independent
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: X
-   **TensorFlow installed from (source or binary)**: Source
-   **TensorFlow version (use command below)**: 2.3.0
-   **Python version**: 3.8
-   **Bazel version (if compiling from source)**: Doesn't matter
-   **GCC/Compiler version (if compiling from source)**: Doesn't matter
-   **CUDA/cuDNN version**: Doesn't matter
-   **GPU model and memory**: Nvidia Titan, Jetson, etc
-   **Exact command to reproduce**: Nope

### Describe the problem
I believe there is currently no way to select which GPU device is used by TF Lite. Even though it is supposed to be used on mobile devices, we already have builds for Windows, Linux (server and embedded), Android, and platforms i686, x86_64, armv7 etc. So basically we use it as our main inference library everywhere, as it is much smaller compared to full TF (which we also integrated). 

I believe GPU selection is a feature desired by many. We are willing to put our resources into coding this feature and creating a push request. Could you point us to a proper place in code where the GPU selection should be integrated? Both for opengl and opencl. Please also suggest how to expose GPU selection in the public API, which structure to modify. We are mainly using the C API.

If we are just stupid and the feature is already present, please point us to it.

Thanks, r"
44331,Non-deterministic results if using Functional model creation style on GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux, kernel 5.8.14-arch1-1
- TensorFlow installed from (source or binary): python-tensorflow-opt-cuda package
- TensorFlow version (use command below): unknown 2.3.1
- Python version: Python 3.8.6
- CUDA/cuDNN version: CUDA 11.1.0, cuDNN 8.0.4
- GPU model and memory: Nvidia GeForce RTX 2070 8gb

**Describe the current behavior**

When using Functional interface to compose the Model results are slightly non-deterministic after several rounds of training. I've included a script that highlights the differences. If we use the Functional model creation method, results are sometimes different between runs - but if we use Sequential model, the results are always the same.

Non-determinism seems to be related to GPU - running with `CUDA_VISIBLE_DEVICES=-1` makes output deterministic.

**Describe the expected behavior**

Expected results to be deterministic for both Functional and Sequential models (or non-deterministic for both, if there are some fundamental issues with GPU optimizations). [TensoFlow Determinism](https://github.com/NVIDIA/framework-determinism) page suggests that TensorFlow 2.3 should be deterministic by default.

**Standalone code to reproduce the issue**

```python
import hashlib

# generate some training data
import numpy
numpy.random.seed(542326146)
D = numpy.random.uniform(size=(64, 8)).astype(""float32"")
L = numpy.random.binomial(n=1, p=0.5, size=64).astype(""float32"")

# ensure that data is always the same
m = hashlib.sha256()
m.update(D.tobytes())
assert(m.hexdigest() == ""96291d29976308b4f9b4a9514466a44f4d45b2944bd149951e017688d56aa380"")
m = hashlib.sha256()
m.update(L.tobytes())
assert(m.hexdigest() == ""f51400856346bd43b2f04d81f7177fab8844f572ea6a281d1a429e26713fb4f4"")

import tensorflow as tf
tf.random.set_seed(542326146)
import tensorflow.keras.layers as layers

use_functional = True # change to False to switch to Sequential model and get deterministic results
if use_functional:
    # Functional
    input_layer = layers.Input(shape=(8))
    dense1_layer = layers.Dense(8, activation=tf.nn.relu, kernel_initializer=tf.keras.initializers.he_normal())(input_layer)
    output_layer = layers.Dense(2, activation=tf.nn.softmax, kernel_initializer=tf.keras.initializers.he_normal())(dense1_layer)
    model = tf.keras.models.Model(inputs=[input_layer], outputs=[output_layer])
else:
    # Sequential
    model = tf.keras.Sequential([
        layers.Input(shape=(8)),
        layers.Dense(8, activation=tf.nn.relu, kernel_initializer=tf.keras.initializers.he_normal()),
        layers.Dense(2, activation=tf.nn.softmax, kernel_initializer=tf.keras.initializers.he_normal()),
    ])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
model.fit(x=D, y=L, epochs=4, verbose=0)

# get a hash of model weights
m = hashlib.sha256()
for w in model.get_weights():
    m.update(w.tobytes())
print(m.hexdigest())

```

Run with:
```bash
for i in $(seq 16); do python train.py; done
```

Example of output on my machine:
```
c61bf7b08f9ecaa84afa9be3f9910f4e7533faa393ec2138c1d47d62913b3c19
097afc63c6796b52dbcf4d9216fcaf56122fe6a77ffc997c35d538f644061f3e
097afc63c6796b52dbcf4d9216fcaf56122fe6a77ffc997c35d538f644061f3e
17f78cb99882f3b9d33fd90cb714e43800a1547d878f801e85044d8ba41650a0
c61bf7b08f9ecaa84afa9be3f9910f4e7533faa393ec2138c1d47d62913b3c19
c61bf7b08f9ecaa84afa9be3f9910f4e7533faa393ec2138c1d47d62913b3c19
c61bf7b08f9ecaa84afa9be3f9910f4e7533faa393ec2138c1d47d62913b3c19
c61bf7b08f9ecaa84afa9be3f9910f4e7533faa393ec2138c1d47d62913b3c19
1e3d2b84fc8de77db548a805a78e4206b30f3e4726c49ec4f16bcc681ddb27f9
097afc63c6796b52dbcf4d9216fcaf56122fe6a77ffc997c35d538f644061f3e
c61bf7b08f9ecaa84afa9be3f9910f4e7533faa393ec2138c1d47d62913b3c19
097afc63c6796b52dbcf4d9216fcaf56122fe6a77ffc997c35d538f644061f3e
097afc63c6796b52dbcf4d9216fcaf56122fe6a77ffc997c35d538f644061f3e
c61bf7b08f9ecaa84afa9be3f9910f4e7533faa393ec2138c1d47d62913b3c19
c61bf7b08f9ecaa84afa9be3f9910f4e7533faa393ec2138c1d47d62913b3c19
08d1b83a3548d180d2940e380a276ff3861604514bf9a857632a3770fc460dd7
```"
44330,TensorFlow 1.14 or 1.15 not detecting the Titan XP GPU. TensorFlow 2.x detectes the same Titan XP GPU !,"**System information**
- Have a simple code : to check the GPU and tested with the TensorFlow 1.14, 1.15 and TensorFlow 2.2 and 2.3. Code is shared below.

```
import tensorflow as tf
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
tf.config.list_physical_devices('GPU')
```

### **_Output for TensorFlow 1.14 and TensorFlow 1.15_**

` python
Python 3.6.9 (default, Oct  8 2020, 12:12:24) 
[GCC 8.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
/home/rr/TF_1_14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/rr/TF_1_14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/rr/TF_1_14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/rr/TF_1_14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/rr/TF_1_14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/rr/TF_1_14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/home/rr/TF_1_14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/rr/TF_1_14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/rr/TF_1_14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/rr/TF_1_14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/rr/TF_1_14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/rr/TF_1_14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
>>> print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))
2020-10-26 11:56:38.543745: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2020-10-26 11:56:38.570403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-26 11:56:38.570983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
2020-10-26 11:56:38.571042: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64
2020-10-26 11:56:38.571087: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64
2020-10-26 11:56:38.571131: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64
2020-10-26 11:56:38.571170: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64
2020-10-26 11:56:38.571212: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64
2020-10-26 11:56:38.571256: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64
2020-10-26 11:56:38.573773: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-10-26 11:56:38.573792: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...
**_Num GPUs Available:  0_**
`

### **_Output for TensorFlow 2.x**
` python
Python 3.6.9 (default, Oct  8 2020, 12:12:24) 
[GCC 8.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2020-10-26 11:46:11.025219: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
>>> print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))
2020-10-26 11:46:21.325573: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-10-26 11:46:21.349766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-26 11:46:21.350134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: TITAN Xp computeCapability: 6.1
coreClock: 1.582GHz coreCount: 30 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 510.07GiB/s
2020-10-26 11:46:21.350166: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-26 11:46:21.351260: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-26 11:46:21.352291: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-26 11:46:21.352479: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-26 11:46:21.353628: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-26 11:46:21.354306: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-26 11:46:21.356778: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-26 11:46:21.356860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-26 11:46:21.357494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-26 11:46:21.357935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
**_Num GPUs Available:  1_**
`



- OS Platform and Distribution: **Ubuntu 18.04**
- TensorFlow installed from binary via pip 
- TensorFlow version (use command below):1.14, 1.15, 2.2 and 2.3
- Python version: 3.6
- CUDA/cuDNN version: 10.1
- GPU model and memory: Titan XP

1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` : `v1.14.0-rc1-22-gaf24dc91b5 1.14.0`

2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`:` v2.3.0-54-gfcc4b966f1 2.3.1`

**Describe the current behavior**
The TensorFlow 1.14 and TensorFlow 1.15 not able to detect the GPU or not able use the GPU. In the same system in different virtual environment, TensorFlow 2.2 or TensorFlow 2.3 is able to detect GPU and able to use it as shown above. 

**Describe the expected behavior**
All TensorFlow should be able to detect Titan XP and use the GPU. 

"
44329,RAM exhaustion while CPU taining doesn't throw exception,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.8.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
While training on CPU, memory exhaustion doesn't throw any exceptions, just warnings.

**Describe the expected behavior**
RAM memory exhaustion will throw exception.

**Standalone code to reproduce the issue**

Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf 
import numpy as np

fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(2048 * 2048, activation='relu'),
    tf.keras.layers.Dense(10)
])

model.compile(optimizer='adam',
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'])

model.fit(train_images, train_labels, epochs=1, batch_size=100)
```

**Other info / logs**
```
2020-10-24 18:31:34.045523: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-10-24 18:31:34.058101: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-24 18:31:34.070957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-24 18:31:34.075908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]
2020-10-24 18:31:34.078952: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-10-24 18:31:43.412026: I tensorflow/core/profiler/lib/profiler_session.cc:133] Profiler session started.
2020-10-24 18:31:43.420340: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 1 GPUs
2020-10-24 18:31:43.429960: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cupti64_110.dll'; dlerror: cupti64_110.dll not found
2020-10-24 18:31:43.444796: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found
2020-10-24 18:31:43.452632: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2020-10-24 18:31:43.467569: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1522] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.
2020-10-24 18:31:44.057786: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 1)
Epoch 1/4
WARNING:tensorflow:From D:\yolov3-to-tf\python_server\server\venv_server\lib\site-packages\tensorflow\python\util\deprecation.py:574: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Use fn_output_signature instead
2020-10-24 18:32:20.966123: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 2215116800 exceeds 10% of free system memory.
2020-10-24 18:32:22.455623: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 2215116800 exceeds 10% of free system memory.
2020-10-24 18:32:36.990875: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 2215116800 exceeds 10% of free system memory.
2020-10-24 18:32:37.683320: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 2225779200 exceeds 10% of free system memory.
```

"
44328,"Tensorflow DLL leaks references to itself, preventing the DLL from unloading","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (see below).
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro 64bit (version 2004)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A.
- TensorFlow installed from (source or binary): Binary.
- TensorFlow version (use command below): 2.3.1 (CPU only).
- Python version: N/A.
- Bazel version (if compiling from source): N/A.
- GCC/Compiler version (if compiling from source): N/A.
- CUDA/cuDNN version: N/A.
- GPU model and memory: N/A.

**Describe the current behavior**

We link to the Tensorflow DLL dynamically (i.e., using ``LoadLibrary()`` directly on ``tensorflow.dll``, or on another DLL that itself links to Tensorflow). We use the Tensorflow C API to run some data through a model. Eventually, when processing is complete, the DLL is unlinked (i.e., using ``FreeLibrary()`` directly on ``tensorflow.dll``, or the other DLL that itself links to Tensorflow). For some reason unknown to us, at this stage the operating system (Windows) still believes the DLL is in use (the reference count is non-zero), thus any static resource allocated internally by Tensorflow is (probably?) not freed.

The code example below illustrates this problem.

If, within the same process, and under certain conditions, the procedure above is repeated a second time (link again to TF, run a model, etc.), we experience an access violation error in ``TF_SessionRun()``. We are not able to produce a sample code that reproduces this last error, as it only occurs when our software is used by a third party application (the source code of which we do not have access to). However, we noticed that the error would no longer happen if we forcefully unlink the Tensorflow DLL by repeatedly calling ``FreeLibrary(""tensorflow.dll"")`` until the OS holds no reference to it (this requires some 30-40 calls to ``FreeLibrary()``!). This implies that some internal state of Tensorflow from the first ""link/process/unlink"" run is still in use, but is somehow invalid.

**Describe the expected behavior**

A cycle of  ``LoadLibrary()`` / run TF / ``FreeLibrary()`` should leave the Tensorflow DLL reference count unchanged (i.e., equal to zero if TF is not used anywhere else in the program). On ``FreeLibrary()``, any internal resource held by TF should be freed (provided TF is not in use by other parts of the program). Loading the DLL again should result in a blank new internal state of TF.

**Standalone code to reproduce the issue**
```c++
#include <iostream>
#include <exception>
#include <string>
#include <windows.h>
#include <tlhelp32.h>

// Helper function to count the number of references to tensorflow.dll
std::size_t GetTensorflowRefCount() {
    auto handle = GetCurrentProcessId();
    auto s = CreateToolhelp32Snapshot(TH32CS_SNAPMODULE, handle);

    std::size_t value = 0;
    MODULEENTRY32 me32;
    me32.dwSize = sizeof(MODULEENTRY32);
    Module32First(s, &me32);
    do {
        if (std::strcmp(me32.szModule, ""tensorflow.dll"") == 0) {
            value = me32.GlblcntUsage;
            break;
        }
    } while(Module32Next(s, &me32));

    CloseHandle(s);
    return value;
}

// Manually import Tensorflow some types and functions
typedef enum TF_Code {
  TF_OK = 0,
  TF_CANCELLED = 1,
  TF_UNKNOWN = 2,
  TF_INVALID_ARGUMENT = 3,
  TF_DEADLINE_EXCEEDED = 4,
  TF_NOT_FOUND = 5,
  TF_ALREADY_EXISTS = 6,
  TF_PERMISSION_DENIED = 7,
  TF_UNAUTHENTICATED = 16,
  TF_RESOURCE_EXHAUSTED = 8,
  TF_FAILED_PRECONDITION = 9,
  TF_ABORTED = 10,
  TF_OUT_OF_RANGE = 11,
  TF_UNIMPLEMENTED = 12,
  TF_INTERNAL = 13,
  TF_UNAVAILABLE = 14,
  TF_DATA_LOSS = 15,
} TF_Code;

using TF_Status = void;
using TF_Graph = void;
using TF_SessionOptions = void;
using TF_Session = void;

TF_Code            (*TF_GetCode)(TF_Status*);
TF_Status*         (*TF_NewStatus)();
TF_Graph*          (*TF_NewGraph)();
TF_SessionOptions* (*TF_NewSessionOptions)();
TF_Session*        (*TF_NewSession)(TF_Graph*, TF_SessionOptions*, TF_Status*);
void               (*TF_DeleteSession)(TF_Session*, TF_Status*);
void               (*TF_DeleteSessionOptions)(TF_SessionOptions*);
void               (*TF_DeleteGraph)(TF_Graph*);
void               (*TF_DeleteStatus)(TF_Status*);

void ImportTensorflow(HINSTANCE handle) {
#define LOAD_FUNCTION(Function) Function = reinterpret_cast<decltype(Function)>(GetProcAddress(handle, #Function))
    LOAD_FUNCTION(TF_GetCode);
    LOAD_FUNCTION(TF_NewStatus);
    LOAD_FUNCTION(TF_NewGraph);
    LOAD_FUNCTION(TF_NewSessionOptions);
    LOAD_FUNCTION(TF_NewSession);
    LOAD_FUNCTION(TF_DeleteSession);
    LOAD_FUNCTION(TF_DeleteSessionOptions);
    LOAD_FUNCTION(TF_DeleteGraph);
    LOAD_FUNCTION(TF_DeleteStatus);
#undef LOAD_FUNCTION
}

// Helper function to throw on error
void TFStatusCheck(TF_Status* status) {
    if (TF_GetCode(status) != TF_OK) {
        throw std::runtime_error(""Error in tensorflow"");
    }
}

int main() {
    // Load Tensorflow DLL and import functions
    std::cout << ""initial: "" << GetTensorflowRefCount() << std::endl;
    auto handle = LoadLibrary(""tensorflow.dll"");
    std::cout << ""DLL loaded: "" << GetTensorflowRefCount() << std::endl;
    ImportTensorflow(handle);
    std::cout << ""functions imported: "" << GetTensorflowRefCount() << std::endl;

    // Create a dummy session
    TF_Status* status = nullptr;
    TF_Graph* graph = nullptr;
    TF_SessionOptions* options = nullptr;
    TF_Session* session = nullptr;

    try
    {
        status = TF_NewStatus();
        std::cout << ""TF_NewStatus: "" << GetTensorflowRefCount() << std::endl;
        graph = TF_NewGraph();
        std::cout << ""TF_NewGraph: "" << GetTensorflowRefCount() << std::endl;
        options = TF_NewSessionOptions();
        std::cout << ""TF_NewSessionOptions: "" << GetTensorflowRefCount() << std::endl;
        session = TF_NewSession(graph, options, status);
        TFStatusCheck(status);
        std::cout << ""TF_NewSession: "" << GetTensorflowRefCount() << std::endl;
    }
    catch (const std::exception& e)
    {
        std::cout << e.what() << std::endl;
    }

    // Free allocated resources
    if (session) TF_DeleteSession(session, status);
    std::cout << ""TF_DeleteSession: "" << GetTensorflowRefCount() << std::endl;
    if (options) TF_DeleteSessionOptions(options);
    std::cout << ""TF_DeleteSessionOptions: "" << GetTensorflowRefCount() << std::endl;
    if (graph)   TF_DeleteGraph(graph);
    std::cout << ""TF_DeleteGraph: "" << GetTensorflowRefCount() << std::endl;
    if (status)  TF_DeleteStatus(status);
    std::cout << ""TF_DeleteStatus: "" << GetTensorflowRefCount() << std::endl;

    // Free DLL
    FreeLibrary(handle);
    std::cout << ""DLL unloaded: "" << GetTensorflowRefCount() << std::endl;

    return 0;
}
```

**Other info / logs**
Simply compile the above on Windows with ``clang++ test.cpp -o test``.
Output on my computer:
```
initial: 0
DLL loaded: 1
functions imported: 1
TF_NewStatus: 1
TF_NewGraph: 2
TF_NewSessionOptions: 2
2020-10-26 13:38:48.388408: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-26 13:38:48.398942: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1ffcd6eeec0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-26 13:38:48.399288: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
TF_NewSession: 41
TF_DeleteSession: 40
TF_DeleteSessionOptions: 40
TF_DeleteGraph: 40
TF_DeleteStatus: 40
DLL unloaded: 39
```
Notice the following:
 - ``TF_NewGraph`` generates a new reference to the DLL
 - ``TF_NewSession`` generates 39(!) new references to the DLL
 - ``TF_DeleteSession`` removes one reference
 - We are left with 39 leaked references, which we did not create explicitly in our program.
 - We were able to reproduce this with a number of TF versions (1.15, 2.1.1, 2.3.1), with or without GPU support."
44327,cannot see keras properly in tf-nightly-gpu install,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: tf-nightly-gpu
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.1
- GPU model and memory: nVidia MX250, 4GB



**Describe the problem**
I installed tf-nightly-gpu in my environment and first ran into a missing dll issue (#44291) which was resolved. Now I can import keras but do not seem to see it or use anything inside it. See commands below

**Provide the exact sequence of commands / steps that you executed before running into the problem**

from tensorflow import keras
from keras import regularizers
Traceback (most recent call last):
File """", line 1, in
ModuleNotFoundError: No module named 'keras'
dir()
['annotations', 'builtins', 'doc', 'loader', 'name', 'package', 'spec', 'keras', 'tensorflow']

As you can see, keras is visible as a package after importing it, but does not seem to work properly.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44326,Error while converting to tflite model,"I am trying to convert a trained keras model to tflite. However, I am facing the error shown below. Is this likely because of the Albert layer in the model?

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (or github SHA if from source): 2.3.0


**Command used to run the converter or code if youre using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
converter = tf.lite.TFLiteConverter.from_keras_model(text_classifier_model)
tflite_model = converter.convert()
```

**The output from the converter invocation**

```
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
~/anaconda3/lib/python3.8/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    195     try:
--> 196       model_str = wrap_toco.wrapped_toco_convert(model_flags_str,
    197                                                  toco_flags_str, input_data_str,

~/anaconda3/lib/python3.8/site-packages/tensorflow/lite/python/wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
     31   """"""Wraps TocoConvert with lazy loader.""""""
---> 32   return _pywrap_toco_api.TocoConvert(
     33       model_flags_str,

Exception: fail to open input file

During handling of the above exception, another exception occurred:

ConverterError                            Traceback (most recent call last)
<ipython-input-5-c548bab089a8> in <module>
----> 1 tflite_model = converter.convert()

~/anaconda3/lib/python3.8/site-packages/tensorflow/lite/python/lite.py in convert(self)
    785         Invalid quantization parameters.
    786     """"""
--> 787     saved_model_convert_result = self._convert_as_saved_model()
    788     if saved_model_convert_result:
    789       return saved_model_convert_result

~/anaconda3/lib/python3.8/site-packages/tensorflow/lite/python/lite.py in _convert_as_saved_model(self)
    767         self._trackable_obj = _load(self.saved_model_dir,
    768                                     self._saved_model_tags)
--> 769         return super(TFLiteKerasModelConverterV2,
    770                      self).convert(meta_graph.graph_def, input_tensors,
    771                                    output_tensors)

~/anaconda3/lib/python3.8/site-packages/tensorflow/lite/python/lite.py in convert(self, graph_def, input_tensors, output_tensors)
    627 
    628     # Converts model.
--> 629     result = _toco_convert_impl(
    630         input_data=graph_def,
    631         input_tensors=input_tensors,

~/anaconda3/lib/python3.8/site-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)
    567       input_tensors, output_tensors, *args, **kwargs)
    568   debug_info_str = debug_info.SerializeToString() if debug_info else None
--> 569   data = toco_convert_protos(
    570       model_flags.SerializeToString(),
    571       toco_flags.SerializeToString(),

~/anaconda3/lib/python3.8/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    200       return model_str
    201     except Exception as e:
--> 202       raise ConverterError(str(e))
    203 
    204   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:

ConverterError: fail to open input file```

**Also, please include a link to the saved model or GraphDef**

```
# Architecture of text_classifier_model
```
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_title_word_ids (InputLaye [(None, 35)]         0                                            
__________________________________________________________________________________________________
input_title_mask (InputLayer)   [(None, 35)]         0                                            
__________________________________________________________________________________________________
segment_title_ids (InputLayer)  [(None, 35)]         0                                            
__________________________________________________________________________________________________
input_text_word_ids (InputLayer [(None, 250)]        0                                            
__________________________________________________________________________________________________
input_text_mask (InputLayer)    [(None, 250)]        0                                            
__________________________________________________________________________________________________
segment_text_ids (InputLayer)   [(None, 250)]        0                                            
__________________________________________________________________________________________________
albert_layer (KerasLayer)       [(None, 768), (None, 11683584    input_title_word_ids[0][0]       
                                                                 input_title_mask[0][0]           
                                                                 segment_title_ids[0][0]          
                                                                 input_text_word_ids[0][0]        
                                                                 input_text_mask[0][0]            
                                                                 segment_text_ids[0][0]           
__________________________________________________________________________________________________
title_repeat_vector (RepeatVect (None, 1, 768)       0           albert_layer[0][0]               
__________________________________________________________________________________________________
text_repeat_vector (RepeatVecto (None, 1, 768)       0           albert_layer[1][0]               
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 768)          0           title_repeat_vector[0][0]        
__________________________________________________________________________________________________
global_average_pooling1d_1 (Glo (None, 768)          0           text_repeat_vector[0][0]         
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 1536)         0           global_average_pooling1d[0][0]   
                                                                 global_average_pooling1d_1[0][0] 
__________________________________________________________________________________________________
repeat_vector (RepeatVector)    (None, 1, 1536)      0           concatenate[0][0]                
__________________________________________________________________________________________________
bidirectional (Bidirectional)   (None, 150)          725850      repeat_vector[0][0]              
__________________________________________________________________________________________________
dense (Dense)                   (None, 64)           9664        bidirectional[0][0]              
__________________________________________________________________________________________________
dropout (Dropout)               (None, 64)           0           dense[0][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            65          dropout[0][0]     

```
# Standalone code to reproduce issues

[Colab Notebook](https://colab.research.google.com/drive/11Jwz2YV1h1EniI4vVwaxyf3jXRNUSNys?usp=sharing)"
44322,Models for person_detect used in micro vision demo of Tensorflow Lite,"Hello,

did you upload the source models?
It would be very nice for me to get the Tensorflow Lite model and the original unquantized model file.

Thank you in advance,
Ilkay

---

They are currently only available as c++ source files.  I will have a look at adding the source model to the download.  In the meantime, a simple c++ method which dumps the array to file would re-create the tflite model.  You can find the grayscale c++ file [here](https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale.zip):  and the RGB model c++ file [here](https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data.tgz).

_Originally posted by @njeffrie in https://github.com/tensorflow/tensorflow/issues/29792#issuecomment-529726207_"
44321,Tensorflow import error,"Below is the error I see.. any suggestions will be greatful

ImportError: Traceback (most recent call last):
  File ""C:\Users\212474419\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal:     .


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
44320,Request for person_detect.tflite file used in micro vision demo of Tensorflow Lite,"Hello,

is it possible to get the Tensorflow Lite file for the person detection of the micro vision demo?
The original or unquantized model file would be helpful for my research as well.
Is there a way to get the person_detect.h5 file, if it exists?


Thank you in advance,

Ilkay
"
44319,Build up new Optimizer,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.0
- Are you willing to contribute it (Yes/No): Yes

will TensorFlow add AdaBelief ?
https://arxiv.org/abs/2010.07468


**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
44318,Illegal memory access when running large models on GPU,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux RedHat 7.6**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **Docker container**
- TensorFlow version (use command below): **I used the one from the tag 2.3.0-gpu**
- Python version: **3.6.8**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **10.1.243**
- GPU model and memory: **Tesla V100-SXM2-32GB**

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When I run a large model, it I get an illegal memory access was encountered. This occurs even when I use managed memory.

**Describe the expected behavior**
I expect to see either an OOM error or no error when I used managed memory to emulate a larger memory size.
**Standalone code to reproduce the issue**

Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

     import tensorflow as tf 
     import numpy as np
     # Uncomment this to use CUDA Managed Memory (This issue still happens regardless if this is uncommented or not).
     #config = tf.compat.v1.ConfigProto()
     #config.gpu_options.per_process_gpu_memory_fraction = 5 
     #session = tf.compat.v1.InteractiveSession(config=config)
 
     # Grab some random dataset
     fashion_mnist = tf.keras.datasets.fashion_mnist
     (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
 
     # Standard MNIST Example, except with a large model
     model = tf.keras.Sequential([
         tf.keras.layers.Flatten(input_shape=(28, 28)),
         tf.keras.layers.Dense(2048 * 2048, activation='relu'),
         tf.keras.layers.Dense(10)
      ])
 
     model.compile(optimizer='adam',
             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
             metrics=['accuracy'])
 
     model.fit(train_images, train_labels, epochs=1)


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Here is full log
[tf.log](https://github.com/tensorflow/tensorflow/files/5436897/tf.log)
This part seems relevant.
     Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: an illegal memory access was encountered
    
"
44317,TensorFlow 1.14 bug,"```
import tensorflow as tf

sess = tf.Session()

logits = tf.constant([[0.1,0.2,0.3,0.4],[0.5,0.6,0.7,0.8]],dtype=tf.float32)
topk_logits_value,topk_logits_index = tf.math.top_k(logits,k=3)
print(sess.run(topk_logits_index))

sample_index = tf.multinomial(topk_logits_value,num_samples=1)

range = tf.range(0,sample_index.get_shape()[0])
sample_index = tf.cast(sample_index,tf.int32)

sample_index = range * sample_index.get_shape()[0] + tf.reshape(sample_index,[-1])
print(sess.run(sample_index))

print(sess.run(tf.reshape(topk_logits_index,[-1])))
sample_global_index = tf.gather(tf.reshape(topk_logits_index,[-1]),indices=sample_index)
print(sess.run(sample_global_index))

sample_global_index = tf.gather(tf.reshape(topk_logits_index,[-1]),indices=tf.constant([2,4]))
print(sess.run(sample_global_index))

```

print results:

```
[[3 2 1]
 [3 2 1]]
[2 4]
[3 2 1 3 2 1]

[3 1] # should be [1,2]

[1 2]

```"
44315,Failed to build due to keras import error (Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed),"I am trying to build tensorflow by pulling recent code changes. Keras is not built as part of tensorflow.

**System information**
- OS Platform and Distribution: Linux Ubuntu 20.04
- TensorFlow installed from: source
- TensorFlow version: 2.1
- Python version: 3.8.4
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: 11
- GPU model and memory: Nvidia RTX 2070


```
from tensorflow.python.keras.preprocessing import image
  File ""/home/zero/.cache/bazel/_bazel_zero/6a745063bb93c191166f02cd77ede64a/execroot/org_tensorflow/bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/keras/preprocessing/image.py"", line 24, in <module>
    from keras_preprocessing import image
  File ""/home/zero/env/lib/python3.8/site-packages/keras_preprocessing/image.py"", line 18, in <module>
    backend = get_keras_submodule('backend')
  File ""/home/zero/env/lib/python3.8/site-packages/keras_preprocessing/__init__.py"", line 24, in get_keras_submodule
    raise ImportError('You need to first `import keras` '
ImportError: You need to first `import keras` in order to use `keras_preprocessing`. For instance, you can do:

import keras
from keras_preprocessing import image

Or, preferably, this equivalent formulation:

from keras import preprocessing


Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/zero/tensorflow/tensorflow/python/tools/BUILD:226:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)
```

"
44314,TensorFlow 1.14 bug,"```
import tensorflow as tf

sess = tf.Session()

logits = tf.constant([[0.1,0.2,0.3,0.4],[0.5,0.6,0.7,0.8]],dtype=tf.float32)
topk_logits_value,topk_logits_index = tf.math.top_k(logits,k=3)
print(sess.run(topk_logits_index))
# print(sess.run(topk_logits_value))
sample_index = tf.multinomial(topk_logits_value,num_samples=1)

range = tf.range(0,sample_index.get_shape()[0])
sample_index = tf.cast(sample_index,tf.int32)
range = tf.expand_dims(range,-1)
# print(sess.run(range))
sample_index = tf.concat([range,sample_index],-1)
print(sess.run(sample_index))

sample_global_index = tf.gather_nd(topk_logits_index,indices=sample_index)
print(sess.run(sample_global_index))

sample_global_index = tf.gather_nd(topk_logits_index,indices=tf.constant([[0,0],[1,2]]))
print(sess.run(sample_global_index))
```

print result:

```
[[3 2 1]
 [3 2 1]]
[[0 0]
 [1 2]]

[2 1] # should be [3,1]

[3 1]
```"
44313,OperatorNotAllowedInGraphError in tf.debugging.Assert and tf.keras.Input,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
    - Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
    - Linux Ubuntu 18.04.4 LTS
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
    - `pip install tensorflow`
-   **TensorFlow version (use command below)**:
    - v2.3.0-54-gfcc4b966f1 2.3.1
-   **Python version**:
    - 3.6.9
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
    - 10.1/7.6.5
-   **GPU model and memory**:
    - Geforce RTX 2080 SUPER (8GB)
-   **Exact command to reproduce**:
    - The minimal python script which can reproduce the issue is in the Source code / logs section below.

**Describe the current behavior**
I would like to check if inputs to a `Model` is valid or not by using the `tf.debugging.Assert()` function.
But the straightforward implementation results in an OperatorNotAllowedInGraphError:
```
Traceback (most recent call last):
  File ""reproduce_tfbug.py"", line 9, in <module>
    [inputs]
  File ""/home/shimaya/venv/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/home/shimaya/venv/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py"", line 247, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs),
  File ""/home/shimaya/venv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 151, in Assert
    if not condition:
  File ""/home/shimaya/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 877, in __bool__
    self._disallow_bool_casting()
  File ""/home/shimaya/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 490, in _disallow_bool_casting
    self._disallow_in_graph_mode(""using a `tf.Tensor` as a Python `bool`"")
  File ""/home/shimaya/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 479, in _disallow_in_graph_mode
    "" this function with @tf.function."".format(task))
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
```
This seems to occur only when the argument of the `Assert()` is a `tf.keras.Input` tensor.

**Describe the expected behavior**
I expect this simple assertion works. I believe such simple syntax will be helpful to many TensorFlow users.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import tensorflow.keras as K

if __name__ == ""__main__"":
    inputs = K.Input(shape=(10, 10))

    tf.debugging.Assert(
        tf.reduce_all(inputs >= 0.), 
        [inputs]
    )

    # any operations or model definitions follow
    # ...
```"
44312,docs: Could not load dynamic library 'libcublas.so.10',"## URL(s) with the issue:

https://www.tensorflow.org/install/gpu#install_cuda_with_apt

## Description of issue (what needs changing):

I followed the Ubuntu 18.04 (CUDA 10.1) instructions and have installed cuda-10-1, libcudnn7=7.6.5.32-1+cuda10.1, libcudnn7-dev=7.6.5.32-1+cuda10.1 as per the instructions.

When I train I'm seeing
```
Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory
```

In context:

```
2020-10-26 01:41:57.018284: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-26 01:41:57.018526: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory
2020-10-26 01:41:57.119177: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-26 01:41:57.144040: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-26 01:41:57.274656: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-26 01:41:57.292466: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-26 01:41:57.619082: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-26 01:41:57.619159: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
```

I'm running Ubuntu 18.04.5 LTS (Bionic Beaver)"
44311,Mysterious exception when using tf.data.Dataset.from_tensor_slices,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
?
- TensorFlow version (use command below):
2.3.0
- Python version:
3.6.9
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
?
- GPU model and memory:
?

**Describe the current behavior**
When I run `tf.data.Dataset.from_tensor_slices(data)`, it takes a very long time, but at the end, I get:
an `InvalidArgumentError` with `OpKernel 'Pack' has constraint on attr 'T' not in NodeDef '[N=0, axis=0]', KernelDef: 'op: ""Pack"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_QINT32 } } }`
*`data` is a `list` containing my data*
**Describe the expected behavior**
It should run without any exceptions. The docs don't mention that this function will raise an `InvalidArgumentError`. I have tried searching the internet for a solution, but the only thing I could find is a Chinese site with no useful information.
**Standalone code to reproduce the issue**
*This is my exact code*
https://gist.github.com/firebird52/a87558e9d6a675e3d68d22ccc48f447b
**Other info / logs** 
I think it has to do with my data. It seems to be saying that my data has to be of type `DT_QINT32`. All of my data is in the form of strings."
44309,"Model.predict() subsequent predictions returning nan (tf-nightly TF 2.4.0-dev20201023, NVIDIA-SMI 456.71, CUDA 11.1, cuDNN 8.0.4, RTX 3090)","This seems to be an interesting issue with serial predictions using RTX 3090 with updated version of tf-nightly and CUDA 11.1 (Windows 10, cuDNN 8.0.4). I tried submitting the issue on stackoverflow without any resolution. 

I'm passing a single image_array: (IMAGE_DATA TYPE <class 'numpy.ndarray'> IMAGE_DATA.SHAPE (1, 640, 640, 3)) to MODEL.predict(IMAGE_DATA). The first time the model is compiled, the predictions make sense (accurate bounding boxes, class type). However, the second time a new image or a copy of the first image is passed to MODEL.predict() it returns predictions that don't make any sense (see below). This is a YOLOv3 model.

PREDICTED_BOXES = model.predict(IMAGE_DATA)
LEN(PREDICTED_BOXES) = 25200
[[ nan nan nan nan nan nan]
[ nan nan nan nan nan nan]
[ nan nan nan nan nan nan]
...
[640. 640. inf inf 1. 1.]
[640. 640. inf inf 1. 1.]
[640. 640. inf inf 1. 1.]]

--> postprocessing of boxes returns [x1, y1, x2, y2, class_prob, class_num]:
[0.000e+00 0.000e+00 1.919e+03 1.079e+03 1.000e+00 0.000e+00]

If I run the same code/images using only my CPU on the same computer as the RTX 3090 it correctly predicts on all images. If I run the same code/images on a different computer with a GTX 1060 and TF 2.3.0, CUDA 10.1 NVIDIA-SMI 441.87 it correctly predicts on all images.

Has anyone else had a similar issue and are there any thoughts on what the problem might be or fixes/workarounds?

Thanks
"
44307,Could not load dynamic library 'libcudnn.so.7',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 20.04**
- TensorFlow installed from (source or binary): 
- TensorFlow version: **2.3.1**
- Python version: **Python 3.8.5**
- Installed using virtualenv? pip? conda?: **pip**
- CUDA/cuDNN version: **CUDA 11.1/cuDNN v8**
- GPU model and memory: **RTX 3080**

ISSUE similar to #20271 and #36426


**Describe the problem**
So I was setting up CUDA and cuDNN for my RTX 3080 on ubuntu 20.04. I followed the TF guide and installed CUDA 10.1 and cuDNN v7. But then I was running into several issues. From a similar issue #43718, I was [suggested](https://github.com/tensorflow/tensorflow/issues/43718#issuecomment-714649169) to use CUDA 11.1 and cuDNN v8.

But whenever I run `tf.config.list_physical_devices('GPU')` I get the following error:
```
2020-10-25 17:36:10.844295: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-10-25 17:36:10.879258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-25 17:36:10.879821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:2b:00.0 name: GeForce RTX 3080 computeCapability: 8.6
coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.76GiB deviceMemoryBandwidth: 707.88GiB/s
2020-10-25 17:36:10.879836: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-25 17:36:10.881041: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-25 17:36:10.881627: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-25 17:36:10.881750: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-25 17:36:10.882821: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-25 17:36:10.883332: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-25 17:36:10.883401: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.1/lib64:/usr/local/cuda-11.1/lib64:/opt/ros/noetic/lib
2020-10-25 17:36:10.883408: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[]
```
Here is my ~/.zshrc:
```
# ROS
source /opt/ros/noetic/setup.zsh

# CUDA
export PATH=/usr/local/cuda-11.1/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-11.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
# export LD_LIBRARY_PATH=/usr/lib/cuda/lib64:$LD_LIBRARY_PATH
# export LD_LIBRARY_PATH=/usr/lib/cuda/include:$LD_LIBRARY_PATH
```
"
44306,Quantized MobileBERT and ALBERT models performing very poorly,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

The Model Maker tool provides an [easy to follow guide](https://www.tensorflow.org/lite/tutorials/model_maker_text_classification) to train a custom text classifier by fine-tuning the pre-trained MobileBERT weights. The guide uses [this MobileBERT model](https://tfhub.dev/google/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT/1) from TensorFlow Hub. 

Now, once should be able to directly use this TF-Hub model as a part of a Keras model (which is exactly what Model Maker does) and train it. I tried doing the same and I have been successful but when I evaluated the TFLite variant of the model on the test set, the performance of the model significantly drops from 90% to 49%. The same happens with the [ALBERT model](https://tfhub.dev/tensorflow/albert_lite_base/1) as well. 

I DID use the proper tokenizers for the respective models as one can see in the notebooks mentioned below. I fine-tuning on the SST-2 dataset as used in the Model Maker Text Classification guide. 

What's wrong here?

**Describe the expected behavior**

The TFLite models should match the original model performance. 

**Standalone code to reproduce the issue**
* [ALBERT model training and conversion notebook](https://github.com/sayakpaul/BERT-for-Mobile/blob/master/ALBERT_Keras.ipynb)
* [TFLite ALBERT model evaluation](https://github.com/sayakpaul/BERT-for-Mobile/blob/master/Evaluation_SST_2_ALBERT.ipynb)
* [MobileBERT model training and conversion notebook](https://github.com/sayakpaul/BERT-for-Mobile/blob/master/MobileBERT_Keras.ipynb)
* [TFLite MobileBERT model evaluation](https://colab.research.google.com/gist/sayakpaul/27f97101ca06b80c14d29ecf1afdccc8/evaluation_sst-2_albert.ipynb)"
44302,Unable to install CUDA on Ubuntu 18.04.5,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): pip3 
- TensorFlow version: 
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: I am trying to install this.
- GPU model and memory: GTX 960M

**Describe the problem**

I have spent the whole day trying to install CUDA into my newly installed Ubuntu.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I followed the instruction as outlined [here](https://www.tensorflow.org/install/gpu). Before doing that, I make sure that I delete all previously installed CUDA:
```

sudo apt-get purge nvidia\*
sudo apt remove nvidia-\*
sudo rm /etc/apt/sources.list.d/cuda*
sudo apt-get autoremove && sudo apt-get autoclean
sudo rm -rf /usr/local/cuda*

```
The following commands are executed without problems:
```
# Add NVIDIA package repositories
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub
sudo dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb
sudo apt-get update
wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
sudo apt-get update

# Install NVIDIA driver
sudo apt-get install --no-install-recommends nvidia-driver-450
```
After that, I restart my machine. However, I failed to execute the following: 
```
# Install development and runtime libraries (~4GB)
sudo apt-get install --no-install-recommends \
    cuda-10-1 \
    libcudnn7=7.6.5.32-1+cuda10.1  \
    libcudnn7-dev=7.6.5.32-1+cuda10.1

```
The error I got is 
`E: Unable to locate package cuda-10-1`

I found there is at least one other having the same issue with me: https://github.com/tensorflow/tensorflow/issues/39506

I follow the advice given in that issue and instead of executing the commands that gave me errors, I do the following (as instructed [here](https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1804&target_type=deblocal)):

```
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin
sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/11.1.0/local_installers/cuda-repo-ubuntu1804-11-1-local_11.1.0-455.23.05-1_amd64.deb
sudo dpkg -icuda-repo-ubuntu1804-11-1-local_11.1.0-455.23.05-1_amd64.deb
sudo apt-key add /var/cuda-repo-ubuntu1804-11-1-local/7fa2af80.pub
sudo apt-get update
sudo apt-get -y install cuda-10-1
```

But when executing  `sudo apt-get -y install cuda-10-1` I got the following errors:
```
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 cuda-10-1 : Depends: cuda-runtime-10-1 (>= 10.1.243) but it is not going to be installed
             Depends: cuda-demo-suite-10-1 (>= 10.1.243) but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
```
I have tried other methods on stackoverflow but it seems like all paths lead to Rome, where I have some unmet dependencies.

**Any other info / logs**
When executing `sudo apt-get install --no-install-recommends nvidia-driver-450`, I got the following log:
```
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  dkms libnvidia-cfg1-450 libnvidia-common-450 libnvidia-decode-450
  libnvidia-encode-450 libnvidia-extra-450 libnvidia-fbc1-450 libnvidia-gl-450
  libnvidia-ifr1-450 nvidia-compute-utils-450 nvidia-dkms-450
  nvidia-kernel-common-450 nvidia-kernel-source-450 nvidia-utils-450
  xserver-xorg-video-nvidia-450
Suggested packages:
  menu
Recommended packages:
  nvidia-settings nvidia-prime libnvidia-compute-450:i386
  libnvidia-decode-450:i386 libnvidia-encode-450:i386 libnvidia-ifr1-450:i386
  libnvidia-fbc1-450:i386 libnvidia-gl-450:i386
The following NEW packages will be installed:
  dkms libnvidia-cfg1-450 libnvidia-common-450 libnvidia-decode-450
  libnvidia-encode-450 libnvidia-extra-450 libnvidia-fbc1-450 libnvidia-gl-450
  libnvidia-ifr1-450 nvidia-compute-utils-450 nvidia-dkms-450
  nvidia-driver-450 nvidia-kernel-common-450 nvidia-kernel-source-450
  nvidia-utils-450 xserver-xorg-video-nvidia-450
0 to upgrade, 16 to newly install, 0 to remove and 3 not to upgrade.
Need to get 0 B/76.3 MB of archives.
After this operation, 267 MB of additional disk space will be used.
Do you want to continue? [Y/n] y
Selecting previously unselected package dkms.
(Reading database ... 176145 files and directories currently installed.)
Preparing to unpack .../00-dkms_2.3-3ubuntu9.7_all.deb ...
Unpacking dkms (2.3-3ubuntu9.7) ...
Selecting previously unselected package libnvidia-cfg1-450:amd64.
Preparing to unpack .../01-libnvidia-cfg1-450_450.80.02-0ubuntu0.18.04.2_amd64.deb ...
Unpacking libnvidia-cfg1-450:amd64 (450.80.02-0ubuntu0.18.04.2) ...
Selecting previously unselected package libnvidia-common-450.
Preparing to unpack .../02-libnvidia-common-450_450.80.02-0ubuntu0.18.04.2_all.deb ...
Unpacking libnvidia-common-450 (450.80.02-0ubuntu0.18.04.2) ...
Selecting previously unselected package libnvidia-decode-450:amd64.
Preparing to unpack .../03-libnvidia-decode-450_450.80.02-0ubuntu0.18.04.2_amd64.deb ...
Unpacking libnvidia-decode-450:amd64 (450.80.02-0ubuntu0.18.04.2) ...
Selecting previously unselected package libnvidia-encode-450:amd64.
Preparing to unpack .../04-libnvidia-encode-450_450.80.02-0ubuntu0.18.04.2_amd64.deb ...
Unpacking libnvidia-encode-450:amd64 (450.80.02-0ubuntu0.18.04.2) ...
Selecting previously unselected package libnvidia-extra-450:amd64.
Preparing to unpack .../05-libnvidia-extra-450_450.80.02-0ubuntu0.18.04.2_amd64.deb ...
Unpacking libnvidia-extra-450:amd64 (450.80.02-0ubuntu0.18.04.2) ...
Selecting previously unselected package libnvidia-fbc1-450:amd64.
Preparing to unpack .../06-libnvidia-fbc1-450_450.80.02-0ubuntu0.18.04.2_amd64.deb ...
Unpacking libnvidia-fbc1-450:amd64 (450.80.02-0ubuntu0.18.04.2) ...
Selecting previously unselected package libnvidia-gl-450:amd64.
Preparing to unpack .../07-libnvidia-gl-450_450.80.02-0ubuntu0.18.04.2_amd64.deb ...
Unpacking libnvidia-gl-450:amd64 (450.80.02-0ubuntu0.18.04.2) ...
Selecting previously unselected package libnvidia-ifr1-450:amd64.
Preparing to unpack .../08-libnvidia-ifr1-450_450.80.02-0ubuntu0.18.04.2_amd64.deb ...
Unpacking libnvidia-ifr1-450:amd64 (450.80.02-0ubuntu0.18.04.2) ...
Selecting previously unselected package nvidia-compute-utils-450.
Preparing to unpack .../09-nvidia-compute-utils-450_450.80.02-0ubuntu0.18.04.2_amd64.deb ...
Unpacking nvidia-compute-utils-450 (450.80.02-0ubuntu0.18.04.2) ...
Selecting previously unselected package nvidia-kernel-source-450.
Preparing to unpack .../10-nvidia-kernel-source-450_450.80.02-0ubuntu0.18.04.2_amd64.deb ...
Unpacking nvidia-kernel-source-450 (450.80.02-0ubuntu0.18.04.2) ...
Selecting previously unselected package nvidia-kernel-common-450.
Preparing to unpack .../11-nvidia-kernel-common-450_450.80.02-0ubuntu0.18.04.2_amd64.deb ...
Unpacking nvidia-kernel-common-450 (450.80.02-0ubuntu0.18.04.2) ...
Selecting previously unselected package nvidia-dkms-450.
Preparing to unpack .../12-nvidia-dkms-450_450.80.02-0ubuntu0.18.04.2_amd64.deb ...
Unpacking nvidia-dkms-450 (450.80.02-0ubuntu0.18.04.2) ...
Selecting previously unselected package nvidia-utils-450.
Preparing to unpack .../13-nvidia-utils-450_450.80.02-0ubuntu0.18.04.2_amd64.deb ...
Unpacking nvidia-utils-450 (450.80.02-0ubuntu0.18.04.2) ...
Selecting previously unselected package xserver-xorg-video-nvidia-450.
Preparing to unpack .../14-xserver-xorg-video-nvidia-450_450.80.02-0ubuntu0.18.04.2_amd64.deb ...
Unpacking xserver-xorg-video-nvidia-450 (450.80.02-0ubuntu0.18.04.2) ...
Selecting previously unselected package nvidia-driver-450.
Preparing to unpack .../15-nvidia-driver-450_450.80.02-0ubuntu0.18.04.2_amd64.deb ...
Unpacking nvidia-driver-450 (450.80.02-0ubuntu0.18.04.2) ...
Setting up libnvidia-decode-450:amd64 (450.80.02-0ubuntu0.18.04.2) ...
Setting up libnvidia-extra-450:amd64 (450.80.02-0ubuntu0.18.04.2) ...
Setting up libnvidia-encode-450:amd64 (450.80.02-0ubuntu0.18.04.2) ...
Setting up libnvidia-fbc1-450:amd64 (450.80.02-0ubuntu0.18.04.2) ...
Setting up nvidia-utils-450 (450.80.02-0ubuntu0.18.04.2) ...
Setting up nvidia-kernel-common-450 (450.80.02-0ubuntu0.18.04.2) ...
update-initramfs: deferring update (trigger activated)
Setting up dkms (2.3-3ubuntu9.7) ...
Setting up nvidia-kernel-source-450 (450.80.02-0ubuntu0.18.04.2) ...
Setting up libnvidia-cfg1-450:amd64 (450.80.02-0ubuntu0.18.04.2) ...
Setting up nvidia-dkms-450 (450.80.02-0ubuntu0.18.04.2) ...
update-initramfs: deferring update (trigger activated)
INFO:Enable nvidia
DEBUG:Parsing /usr/share/ubuntu-drivers-common/quirks/lenovo_thinkpad
DEBUG:Parsing /usr/share/ubuntu-drivers-common/quirks/dell_latitude
DEBUG:Parsing /usr/share/ubuntu-drivers-common/quirks/put_your_quirks_here
Loading new nvidia-450.80.02 DKMS files...
Building for 5.4.0-52-generic
Building for architecture x86_64
Building initial module for 5.4.0-52-generic
Secure Boot not enabled on this system.
Done.

nvidia:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/5.4.0-52-generic/updates/dkms/

nvidia-modeset.ko:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/5.4.0-52-generic/updates/dkms/

nvidia-drm.ko:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/5.4.0-52-generic/updates/dkms/

nvidia-uvm.ko:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/5.4.0-52-generic/updates/dkms/

depmod...

DKMS: install completed.
Setting up nvidia-compute-utils-450 (450.80.02-0ubuntu0.18.04.2) ...
Warning: The home dir /nonexistent you specified can't be accessed: No such file or directory
Adding system user `nvidia-persistenced' (UID 122) ...
Adding new group `nvidia-persistenced' (GID 127) ...
Adding new user `nvidia-persistenced' (UID 122) with group `nvidia-persistenced' ...
Not creating home directory `/nonexistent'.
Setting up libnvidia-common-450 (450.80.02-0ubuntu0.18.04.2) ...
Setting up xserver-xorg-video-nvidia-450 (450.80.02-0ubuntu0.18.04.2) ...
Setting up libnvidia-gl-450:amd64 (450.80.02-0ubuntu0.18.04.2) ...
Setting up libnvidia-ifr1-450:amd64 (450.80.02-0ubuntu0.18.04.2) ...
Setting up nvidia-driver-450 (450.80.02-0ubuntu0.18.04.2) ...
Processing triggers for libc-bin (2.27-3ubuntu1.2) ...
Processing triggers for man-db (2.8.3-2ubuntu0.1) ...
Processing triggers for initramfs-tools (0.130ubuntu3.11) ...
update-initramfs: Generating /boot/initrd.img-5.4.0-52-generic
W: Possible missing firmware /lib/firmware/rtl_nic/rtl8125a-3.fw for module r8169
W: Possible missing firmware /lib/firmware/rtl_nic/rtl8168fp-3.fw for module r8169
W: Possible missing firmware /lib/firmware/i915/tgl_dmc_ver2_04.bin for module i915
W: Possible missing firmware /lib/firmware/i915/skl_guc_33.0.0.bin for module i915
W: Possible missing firmware /lib/firmware/i915/bxt_guc_33.0.0.bin for module i915
W: Possible missing firmware /lib/firmware/i915/kbl_guc_33.0.0.bin for module i915
W: Possible missing firmware /lib/firmware/i915/glk_guc_33.0.0.bin for module i915
W: Possible missing firmware /lib/firmware/i915/kbl_guc_33.0.0.bin for module i915
W: Possible missing firmware /lib/firmware/i915/icl_guc_33.0.0.bin for module i915
I: The initramfs will attempt to resume from /dev/dm-2
I: (/dev/mapper/ubuntu--vg-swap_1)
I: Set the RESUME variable to override this.
```
No errors, but some warnings..."
44301,docs: Failed to initialize NVML: Driver/library version mismatch,"## URL(s) with the issue:

https://www.tensorflow.org/install/gpu#install_cuda_with_apt

## Description of issue (what needs changing):

I'm following the Ubuntu 18.04 (CUDA 10.1) instructions. I installed the NVIDIA package repositories & NVIDIA driver 450 and rebooted, then ran nvidia-smi:
```
$ nvidia-smi
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |
| N/A   62C    P0    63W / 149W |      0MiB / 11441MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```

I then installed cuda-10-1, libcudnn7=7.6.5.32-1+cuda10.1, libcudnn7-dev=7.6.5.32-1+cuda10.1 as per the instructions. Now I get:

```
$ nvidia-smi
Failed to initialize NVML: Driver/library version mismatch
```

I'm running Ubuntu 18.04.5 LTS (Bionic Beaver)"
44300,docs: Unable to locate package libcudnn7,"## URL(s) with the issue:

https://www.tensorflow.org/install/gpu#install_cuda_with_apt

## Description of issue (what needs changing):

I'm following the Ubuntu 18.04 (CUDA 10.1) instructions. I've installed the NVIDIA package repositories & NVIDIA driver and rebooted. I now try to install cuda & libcudnn7:

```
$ sudo apt-get install --no-install-recommends \
    cuda-10-1 \
    libcudnn7=7.6.5.32-1+cuda10.1  \
    libcudnn7-dev=7.6.5.32-1+cuda10.1
Reading package lists... Done
Building dependency tree       
Reading state information... Done
E: Unable to locate package libcudnn7
E: Unable to locate package libcudnn7-dev
```

I'm running Ubuntu 18.04.5 LTS (Bionic Beaver)"
44299,Is is possible to use tf.keras.applications.VGG16/Resnet50 in under the scope of TPU?,"I am currently trying to use  tf.keras.applications.VGG16/Resnet50 to do transfer learning with TPU. I run this on a Google Colab with tensorflow 2.3.0.  I briefly use the following code to build the model:

```
with strategy.scope():
  # loading VGG
  vgg16_base = tf.keras.applications.VGG16(include_top=False, weights='imagenet',input_shape=(224,224,3))
  # freeze pretrained model
  vgg16_base.trainable = False
  vgg16 = tf.keras.models.Sequential(name='vgg16')
  vgg16.add(vgg16_base)
  vgg16.add(tf.keras.layers.Flatten())
  vgg16.add(tf.keras.layers.Dense(4096, activation='relu'))
  vgg16.add(tf.keras.layers.Dropout(0.5))
  vgg16.add(tf.keras.layers.Dense(4096, activation='relu'))
  vgg16.add(tf.keras.layers.Dropout(0.5))
  vgg16.add(tf.keras.layers.Dense(2, activation=None))
  vgg16.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        optimizer=tf.keras.optimizers.SGD(lr=0.001,momentum=0.9),metrics=['accuracy'])
  vgg16.summary()
```  
The summary can be displayed normally and correctly, but when I use the fit method with the model, it gives out the following error:s for all the roots:
```
UnavailableError: 7 root error(s) found.
  (0) Unavailable: {{function_node __inference_train_function_27953}} failed to connect to all addresses
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:{""created"":""@1603596583.299611768"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3948,""referenced_errors"":[{""created"":""@1603596583.299610117"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":394,""grpc_status"":14}]}
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
	 [[Cast_1/_26]]
```
The code is runnable when using GPU. I wonder it is supposed to be related to the image_generator or we can not use these pre-trained models in TPU currently?"
44297,Is padding also required in model.predict function?,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
44296,Strange error when I try to use tf.roll in a custom loss function,"Hello, I would love if anyoen could point me  in a good direction to find why I am getting the following error when I try to use a custom loss function like:

def lfunction(y_true, y_pred):
 
  mask = K.variable(np.array([1.0,0.0,0.0,0.0,0.0]))
  squared = (y_true - y_pred)**2
  
  mass_mal_to_bk = tf.roll(y_pred,1,axis=1)*mask
  
  add_mass_mal_to_bk = y_true * mass_mal_to_bk
 
  return squared + add_mass_mal_to_bk

The error I get is this:""

tensorflow.python.framework.errors_impl.FailedPreconditionError:  Error while reading resource variable _AnonymousVar84 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar84/class tensorflow::Var does not exist.
         [[node loss/predictions_loss/batata/mul/ReadVariableOp (defined at C:\Users\tiago\Anaconda3\envs\Aqula36\lib\site-packages\keras\backend\tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_10394]

Sorry to bother with that, but the message  gives me no clue on what is going on."
44294,Non-Gradient Descend Learning,"
**System information**
- OS Platform and Distribution Windows 10
- TensorFlow installed from binary
- TensorFlow version: 2.3.1
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I am currently developing a Radial Basis Function Network, where I use a layer calculating the radial basis functions, followed by a Dense layer.
The problem is, that the Dense-Layer should use a non-gradient-descent learning algorithm.
It should calculate the optimal weights by using the Moore-Penrose Pseudo-Inverse.
However, it does not seem to be possible to update the layers weight within the train_step of the model.
The only option, I am aware of, is through optimizer.apply_gradients.

Is is possible, to change the weights in another way than applying gradients?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
# in model:train_step

self.interpolated_dense_weights = tf.tensordot(tf.linalg.pinv(input_after_rbf_layer), tf.transpose(y), axes=1)
print(self.dense_layer.get_weights()) <-- ERROR here. So ""layer.set_weights()"" does not work


**Any other info / logs**
Error I received:

RuntimeError: Cannot get value inside Tensorflow graph function.

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44293,tf.keras.experimental.CosineDecay error,"- TensorFlow version: 2.3.0
- Python version: 3.7.6

According to https://www.tensorflow.org/api_docs/python/tf/keras/experimental/CosineDecay :
> You can pass this schedule directly into a tf.keras.optimizers.Optimizer as the learning rate. 

However when I try to use the suggested code, I get the following error:

`TypeError: '<' not supported between instances of 'CosineDecay' and 'int'`

**Standalone code to reproduce the issue**

Here is a sample code:

```
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow as tf

# Model / data parameters
num_classes = 10
input_shape = (28, 28, 1)

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Scale images to the [0, 1] range
x_train = x_train.astype(""float32"") / 255
x_test = x_test.astype(""float32"") / 255
# Make sure images have shape (28, 28, 1)
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
print(""x_train shape:"", x_train.shape)
print(x_train.shape[0], ""train samples"")
print(x_test.shape[0], ""test samples"")


# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(64, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation=""softmax""),
    ]
)

model.summary()

decay_steps = 10
lr_decayed_fn = tf.keras.experimental.CosineDecay(0.0001, decay_steps)

model.compile(
    loss='categorical_crossentropy',
    optimizer=keras.optimizers.Adam(lr=lr_decayed_fn),
    metrics=['categorical_accuracy'],
)

batch_size = 128
epochs = 5

model.compile(loss=""categorical_crossentropy"", optimizer=tf.keras.optimizers.Adam(lr=lr_decayed_fn), metrics=[""categorical_accuracy""])

model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)
```"
44292,Building TF-2.2 OpenCL triSYCL-1.2 on macOS-10.15 with GCC-4.9 and Bazel-2.0.0 Failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina (macOS-10.15)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): source
- TensorFlow version: r2.2
- Python version: 3.6.1
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): GCC-4.9
- CUDA/cuDNN version: None (OpenCL-1.2 with triSYCL-1.2)
- GPU model and memory: Intel Iris Graphics 550 1536MB



**Describe the problem**
Errors occurred when I was trying to build TensorFlow-2.2 from source code. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
(venv) localhost:src daqirui$ ./configure
WARNING: Running Bazel server needs to be killed, because the startup options are different.
You have bazel 2.0.0 installed.
Please specify the location of python. [Default is /Users/daqirui/PycharmProjects/TF2-Mac/venv/bin/python]: 


Found possible Python library paths:
  /Users/daqirui/PycharmProjects/TF2-Mac/venv/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/Users/daqirui/PycharmProjects/TF2-Mac/venv/lib/python3.6/site-packages]

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: y
OpenCL SYCL support will be enabled for TensorFlow.

Please specify which C++ compiler should be used as the host C++ compiler. [Default is /usr/bin/g++]: /usr/local/bin/c++-4.9


Please specify which C compiler should be used as the host C compiler. [Default is /usr/bin/gcc]: /usr/local/bin/gcc-4.9


Do you wish to build TensorFlow with ComputeCPP support? [Y/n]: n
No ComputeCPP support will be enabled for TensorFlow.

Please specify the location of the triSYCL include directory. (Use --config=sycl_trisycl when building with Bazel) [Default is /usr/local/triSYCL/include]: 


Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: -march=native -Wno-sign-compare -O0


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Do you wish to build TensorFlow with iOS support? [y/N]: n
No iOS support will be enabled for TensorFlow.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished
```
```
(venv) localhost:src daqirui$ /usr/local/bin/bazel build //tensorflow/tools/pip_package:build_pip_package --config=sycl_trisycl --config=opt --config=v2 --local_ram_resources=2048
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=133
ERROR: Config value sycl_trisycl is not defined in any .rc file
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I tried to deleted `--config=sycl_trisycl` but it occurred another error `ERROR: Config value opt is not defined in any .rc file`. And I also found out that the configure file `.tf_configure.bazelrc` seemed not being detected by Bazel. So I renamed it into `.bazelrc` which I deleted the prefix. This time, Bazel detected it. But it did work either that `ERROR: Config value xla is not defined in any .rc file`. By the way, I need to clarify that the reason I used `/usr/local/bin/bazel` instead of `bazel` is that I've installed two versions of Bazel. One is 3.7.0, and one is 2.0.0 because TF-2.2 needs it. I don't know why Bazelisk cannot work properly since it can't download the right version automatically. And command `/usr/local/bin/bazel` refers to 2.0.0, which `bazel` refers to 3.7.0. But both of them can't work well (same error). I hope that someone could solve my issue. This is my `.tf_configure.bazelrc` file: 
```
build --action_env PYTHON_BIN_PATH=""/Users/daqirui/PycharmProjects/TF2-Mac/venv/bin/python""
build --action_env PYTHON_LIB_PATH=""/Users/daqirui/PycharmProjects/TF2-Mac/venv/lib/python3.6/site-packages""
build --python_path=""/Users/daqirui/PycharmProjects/TF2-Mac/venv/bin/python""
build --config=xla
build --config=sycl
build --action_env HOST_CXX_COMPILER=""/usr/local/bin/c++-4.9""
build --action_env HOST_C_COMPILER=""/usr/local/bin/gcc-4.9""
build --action_env TF_NEED_COMPUTECPP=""0""
build --action_env TRISYCL_INCLUDE_DIR=""/usr/local/triSYCL/include""
build:opt --copt=-march=native
build:opt --copt=-Wno-sign-compare
build:opt --copt=-O0
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac,-v1only
build --action_env TF_CONFIGURE_IOS=""0""
```"
44291,tensorflow-nightly-gpu looking for cusolver64_10.dll on a cuDNN 11.1 installation,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: tf-nightly-gpu-2.4.0.dev20201023
- Python version: 3.7.9
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.1
- GPU model and memory: GeForceMX250



**Describe the problem**
After installing the new CUDA and cuDNN, I can now get the installation to go through. However, the first command tensorflow.test.is_gpu_available() fails because one dll is not found:

2020-10-24 11:59:36.156976: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found

In the ""NVIDIA GPU Computing Toolkit""\CUDA\v11.1\bin directory I see the following dll:

cusolver64_11.dll

However, tensorflow nightly seems to be looking for cusolver64_10.dll.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
python
import tensorflow
tensorflow.test.is_gpu_available()

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44290,missing folder absl? ,"As I see, there's a folder called absl which doesn't exist. For instance, this include won't work: #include ""absl/strings/str_join.h"" and a lot of missing file for C++

"
44289,Converted TFLite model accuracy drop,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source): 2.3


**Command used to run the converter or code if youre using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
def rep_data_gen():
    f = ""/home/pychen/Datasets/VOCdevkit/VOC0712_TRAIN/ImageSets/train.txt""
    f_name = np.loadtxt(f, dtype = np.str).reshape(-1)
    # print('Here', type(f_name))
    # print(f_name)
    NORM_H = 300
    NORM_W = 300
    for i in range(22136):
        image = next(iter(f_name))
        image = tf.io.read_file(img_dir + image + '.jpg')
        image = tf.io.decode_jpeg(image, channels=3)
        image = tf.image.resize(image, [NORM_H, NORM_W])
        image = tf.cast(image / 255., tf.float32)
        image = tf.expand_dims(image, 0)
        yield [image]

frozen_graph='./tflite_graph.pb'
input_arrays=[""normalized_input_image_tensor""]
output_arrays=['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']
input_shapes={""normalized_input_image_tensor"":[1,300,300,3]}
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_graph,input_arrays,output_arrays,input_shapes=input_shapes)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
# converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]

# Float 16
# converter.target_spec.supported_types = [tf.float16]

# Float 16 activations with int8 weights
# converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS]

converter.representative_dataset=rep_data_gen
converter.allow_custom_ops=True

tflite_quant_model = converter.convert()

# Save the model.
with open('ssd_mobilenet_v1_voc0712_int8.tflite', 'wb') as f:
    f.write(tflite_quant_model)
```




I trained a Mobilenet_ssd and converted it to a int8 quantized TFLite model by post-training quantization.
But the accuracy of int8 quantized TFLite model drop about 30% mAP for the Pascal VOC dataset.
Is there any method to fine-tune or retrain the TFLite model?
I tried the quantization-aware training, but quantization-aware training add FakeQuantWithMinMaxVars node unsupported by Arm NN.

And can I just quantize the weight to int8 and activation still remain FP32?

If I just use converter.optimizations = [tf.lite.Optimize.DEFAULT], the model will be converted to uint8 or int8?

What is the exact function of the following two commands?
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]




"
44288,"I had tried this code in Xcode with objective c but I am getting the error ""Explicit specialization of undeclared template struct 'NumTraits'","This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
44287,Heteregnous inference,"I'm wondering if there is a procedure for inference calculation within a heterogeneous environment. The goal is to run graf inference as multiple its subgraph on the various processing unit (CPU, GPU). basically to split the inference workload across multiple CPUs, GPUs ... Is there any example or guide on how to do it?"
44286,What is I and W in tensorflow logs?,"Recently I installed tensorflow 2.x on my PC and while importing I find the following logs. What does `I` and `W` mean in these logs that appear after the timestamp.

Initially, I felt `I` is used to give a human touch to the machine (`W` meaning `We`). But it does not appear so. It would be really helpful If you could clarify what it actually means. I found the similar notations used in ONNX also. I guess it is some well known shortforms. But I really could not find it on google.

I had always used tensorflow on Google Colab and AWS Cloud but never faced this.


```bash
2020-10-24 15:29:53.303267: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-24 15:29:53.303299: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-24 15:29:53.303324: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-24 15:29:53.303346: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-24 15:29:53.303370: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-24 15:29:53.303394: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-24 15:29:53.303509: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory
```"
44285,training keras Sequential model with python generator raise error: AttributeError: 'tuple' object has no attribute 'rank',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.3
- Python version: 3.7.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cu101
- GPU model and memory: Quadro M6000 24G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When training a Sequential model by fit function with python generator as inputs, it raise AttributeError: 'tuple' object has no attribute 'rank'. The python generator yield a tuple (x, y), which follows the tf document of fit function.

**Describe the expected behavior**
I want to know how to make it.

**Standalone code to reproduce the issue**
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import numpy as np

train_time = pd.read_csv(""./train.csv"")
train = pd.read_csv(""./train_v1.csv"")
x_train = train[[""year"", 'month', 'day', 'hour', 'label']]
y_train = train['label']
test = pd.read_csv(""./test_v1.csv"")


def train_dataset(x):
    for i in range(0, len(x)):
        if i < 23:
            feature = x[[""year"", 'month', 'day', 'hour', 'label']][0:i + 1]
        else:
            feature = x[[""year"", 'month', 'day', 'hour', 'label']][i - 23:i + 1]
        feature = np.asarray(feature)
        feature[i, -1] = 0
        yield tf.expand_dims(feature, 0), x[""label""][i]


def test_dataset(x):
    for i in range(0, len(x)):
        if i < 24:
            yield tf.expand_dims(x[[""year"", 'month', 'day', 'hour', 'label']][0:i], 0), 0
        else:
            yield tf.expand_dims(x[[""year"", 'month', 'day', 'hour', 'label']][1 - 24:i], 0), 0


model = keras.Sequential()
model.add(keras.layers.LSTM(200, input_shape=[None, 5]))
model.add(keras.layers.Dropout(0.3))
model.add(keras.layers.Dense(100))
model.add(keras.layers.Dense(1))

train_generator = train_dataset(train)
test_generator = test_dataset(test)
model.compile(loss=keras.losses.mean_absolute_error, metrics=keras.metrics.mean_absolute_error)

model.fit(x=train_generator, y=None, epochs=100)

pre = model.predict(test_generator)

**Other info / logs** 
Traceback (most recent call last):
  File ""D:/code/competetion/railway station/model.py"", line 43, in <module>
    model.fit(x=train_generator, y=None, epochs=100)
  File ""C:\Anaconda3\envs\deeplearning\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""C:\Anaconda3\envs\deeplearning\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1063, in fit
    steps_per_execution=self._steps_per_execution)
  File ""C:\Anaconda3\envs\deeplearning\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py"", line 1117, in __init__
    model=model)
  File ""C:\Anaconda3\envs\deeplearning\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py"", line 805, in __init__
    output_shapes = nest.map_structure(_get_dynamic_shape, peek)
  File ""C:\Anaconda3\envs\deeplearning\lib\site-packages\tensorflow\python\util\nest.py"", line 635, in map_structure
    structure[0], [func(*x) for x in entries],
  File ""C:\Anaconda3\envs\deeplearning\lib\site-packages\tensorflow\python\util\nest.py"", line 635, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File ""C:\Anaconda3\envs\deeplearning\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py"", line 801, in _get_dynamic_shape
    if shape.rank is None:
AttributeError: 'tuple' object has no attribute 'rank'
"
44284,ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.,"I am using python 3.7.2 and I have installed the packages required for a code but I have encountered an error while trying to run the code 
This is what I received

`Traceback (most recent call last):
  File ""C:\Users\Future\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 6, in <module>
    import cvlib as cv
  File ""C:\Users\Future\AppData\Local\Programs\Python\Python37\lib\site-packages\cvlib\__init__.py"", line 8, in <module>
    from .gender_detection import detect_gender
  File ""C:\Users\Future\AppData\Local\Programs\Python\Python37\lib\site-packages\cvlib\gender_detection.py"", line 3, in <module>
    from tensorflow.keras.utils import get_file
  File ""C:\Users\Future\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Future\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\Future\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""C:\Users\Future\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Future\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Future\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.`

How can this be solved?"
44283,failed dnn implementation,"import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from yahoo_fin import stock_info as si
from collections import deque

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
import os
import random

set seed, so we can get the same results after rerunning several times
np.random.seed(314)
tf.random.set_seed(314)
random.seed(314)

Hyper-parameters
import os
import time
from tensorflow.keras.layers import LSTM

Window size or the sequence length
N_STEPS = 70

Lookup step, 1 is the next day
LOOKUP_STEP = 1

test ratio size, 0.2 is 20%
TEST_SIZE = 0.2

features to use
FEATURE_COLUMNS = [""adjclose"", ""volume"", ""open"", ""high"", ""low""]

date now
date_now = time.strftime(""%Y-%m-%d"")

model parameters
N_LAYERS = 3

LSTM cell
CELL = LSTM

256 LSTM neurons
UNITS = 256

40% dropout
DROPOUT = 0.4

whether to use bidirectional RNNs
BIDIRECTIONAL = False

training parameters
mean absolute error loss
LOSS = ""mae""
huber loss
LOSS = ""huber_loss""
OPTIMIZER = ""adam""
BATCH_SIZE = 64
EPOCHS = 400

Tesla stock market
ticker = ""TSLA""
ticker_data_filename = os.path.join(""data"", f""{ticker}_{date_now}.csv"")

model name to save, making it as unique as possible based on parameters
model_name = f""{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.name}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}""
if BIDIRECTIONAL:
model_name += ""-b""

def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1,
test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):
""""""
Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.
Params:
ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.
n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50
scale (bool): whether to scale prices from 0 to 1, default is True
shuffle (bool): whether to shuffle the data, default is True
lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)
test_size (float): ratio for test data, default is 0.2 (20% testing data)
feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin
""""""
# see if ticker is already a loaded stock from yahoo finance
if isinstance(ticker, str):
# load it from yahoo_fin library
df = si.get_data(ticker)
elif isinstance(ticker, pd.DataFrame):
# already loaded, use it directly
df = ticker
else:
raise TypeError(""ticker can be either a str or a pd.DataFrame instances"")

# this will contain all the elements we want to return from this function
result = {}
# we will also return the original dataframe itself
result['df'] = df.copy()

# make sure that the passed feature_columns exist in the dataframe
for col in feature_columns:
    assert col in df.columns, f""'{col}' does not exist in the dataframe.""

if scale:
    column_scaler = {}
    # scale the data (prices) from 0 to 1
    for column in feature_columns:
        scaler = preprocessing.MinMaxScaler()
        df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))
        column_scaler[column] = scaler

    # add the MinMaxScaler instances to the result returned
    result[""column_scaler""] = column_scaler

# add the target column (label) by shifting by `lookup_step`
df['future'] = df['adjclose'].shift(-lookup_step)

# last `lookup_step` columns contains NaN in future column
# get them before droping NaNs
last_sequence = np.array(df[feature_columns].tail(lookup_step))

# drop NaNs
df.dropna(inplace=True)

sequence_data = []
sequences = deque(maxlen=n_steps)

for entry, target in zip(df[feature_columns].values, df['future'].values):
    sequences.append(entry)
    if len(sequences) == n_steps:
        sequence_data.append([np.array(sequences), target])

# get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence
# for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length
# this last_sequence will be used to predict future stock prices not available in the dataset
last_sequence = list(sequences) + list(last_sequence)
last_sequence = np.array(last_sequence)
# add to result
result['last_sequence'] = last_sequence

# construct the X's and y's
X, y = [], []
for seq, target in sequence_data:
    X.append(seq)
    y.append(target)

# convert to numpy arrays
X = np.array(X)
y = np.array(y)

# reshape X to fit the neural network
X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))

# split the dataset
result[""X_train""], result[""X_test""], result[""y_train""], result[""y_test""] = train_test_split(X, y, 
                                                                            test_size=test_size, shuffle=shuffle)
# return the result
return result
def create_model(sequence_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,
loss=""mean_absolute_error"", optimizer=""rmsprop"", bidirectional=False):
model = Sequential()
for i in range(n_layers):
if i == 0:
# first layer
if bidirectional:
model.add(Bidirectional(cell(units, return_sequences=True), input_shape=(None, sequence_length)))
else:
model.add(cell(units, return_sequences=True, input_shape=(None, sequence_length)))
elif i == n_layers - 1:
# last layer
if bidirectional:
model.add(Bidirectional(cell(units, return_sequences=False)))
else:
model.add(cell(units, return_sequences=False))
else:
# hidden layers
if bidirectional:
model.add(Bidirectional(cell(units, return_sequences=True)))
else:
model.add(cell(units, return_sequences=True))
# add dropout after each layer
model.add(Dropout(dropout))
model.add(Dense(1, activation=""linear""))
model.compile(loss=loss, metrics=[""mean_absolute_error""], optimizer=optimizer)
return model

create these folders if they does not exist
if not os.path.isdir(""results""):
os.mkdir(""results"")

if not os.path.isdir(""logs""):
os.mkdir(""logs"")

if not os.path.isdir(""data""):
os.mkdir(""data"")

load the data
data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)

save the dataframe
data[""df""].to_csv(ticker_data_filename)

construct the model
model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,
dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)

some tensorflow callbacks
checkpointer = ModelCheckpoint(os.path.join(""results"", model_name + "".h5""), save_weights_only=True, save_best_only=True, verbose=1)
tensorboard = TensorBoard(log_dir=os.path.join(""logs"", model_name))

history = model.fit(data[""X_train""], data[""y_train""],
batch_size=BATCH_SIZE,
epochs=EPOCHS,
validation_data=(data[""X_test""], data[""y_test""]),
callbacks=[checkpointer, tensorboard],
verbose=1)

model.save(os.path.join(""results"", model_name) + "".h5"")

def plot_graph(model, data):
y_test = data[""y_test""]
X_test = data[""X_test""]
y_pred = model.predict(X_test)
y_test = np.squeeze(data[""column_scaler""][""adjclose""].inverse_transform(np.expand_dims(y_test, axis=0)))
y_pred = np.squeeze(data[""column_scaler""][""adjclose""].inverse_transform(y_pred))
plt.plot(y_test[-200:], c='b')
plt.plot(y_pred[-200:], c='r')
plt.xlabel(""Days"")
plt.ylabel(""Price"")
plt.legend([""Actual Price"", ""Predicted Price""])
plt.show()

def get_accuracy(model, data):
y_test = data[""y_test""]
X_test = data[""X_test""]
y_pred = model.predict(X_test)
y_test = np.squeeze(data[""column_scaler""][""adjclose""].inverse_transform(np.expand_dims(y_test, axis=0)))
y_pred = np.squeeze(data[""column_scaler""][""adjclose""].inverse_transform(y_pred))
y_pred = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_pred[LOOKUP_STEP:]))
y_test = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_test[LOOKUP_STEP:]))
return accuracy_score(y_test, y_pred)

def predict(model, data):
# retrieve the last sequence from data
last_sequence = data[""last_sequence""][-N_STEPS:]
# retrieve the column scalers
column_scaler = data[""column_scaler""]
# reshape the last sequence
last_sequence = last_sequence.reshape((last_sequence.shape[1], last_sequence.shape[0]))
# expand dimension
last_sequence = np.expand_dims(last_sequence, axis=0)
# get the prediction (scaled from 0 to 1)
prediction = model.predict(last_sequence)
# get the price (by inverting the scaling)
predicted_price = column_scaler[""adjclose""].inverse_transform(prediction)[0][0]
return predicted_price

load the optimal model weights
model_path = os.path.join(""results"", model_name) + "".h5""
model.load_weights(model_path)

load the data with shuffle = False
data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,
feature_columns=FEATURE_COLUMNS, shuffle=False)

evaluate the model
mse, mae = model.evaluate(data[""X_test""], data[""y_test""], verbose=0)

calculate the mean absolute error (inverse scaling)
mean_absolute_error = data[""column_scaler""][""adjclose""].inverse_transform([[mae]])[0][0]
print(""Mean Absolute Error:"", mean_absolute_error)

predict the future price
future_price = predict(model, data)
print(f""Future price after {LOOKUP_STEP} days is {future_price:.2f}$"")
print(""Accuracy Score:"", get_accuracy(model, data))
plot_graph(model, data)

THIS IS THE ERROR
Train on 2023 samples, validate on 506 samples
Epoch 1/400
64/2023 [..............................] - ETA: 2:51WARNING:tensorflow:Can save best model only with val_loss available, skipping.
UnknownError Traceback (most recent call last)
in
28 validation_data=(data[""X_test""], data[""y_test""]),
29 callbacks=[checkpointer, tensorboard],
---> 30 verbose=1)
31
32 model.save(os.path.join(""results"", model_name) + "".h5"")

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
817 max_queue_size=max_queue_size,
818 workers=workers,
--> 819 use_multiprocessing=use_multiprocessing)
820
821 def evaluate(self,

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
340 mode=ModeKeys.TRAIN,
341 training_context=training_context,
--> 342 total_epochs=epochs)
343 cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
344

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
126 step=step, mode=mode, size=current_batch_size) as batch_logs:
127 try:
--> 128 batch_outs = execution_function(iterator)
129 except (StopIteration, errors.OutOfRangeError):
130 # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py in execution_function(input_fn)
96 # numpy translates Tensors to values in Eager mode.
97 return nest.map_structure(_non_none_constant_value,
---> 98 distributed_function(input_fn))
99
100 return execution_function

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\eager\def_function.py in call(self, *args, **kwds)
566 xla_context.Exit()
567 else:
--> 568 result = self._call(*args, **kwds)
569
570 if tracing_count == self._get_tracing_count():

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\eager\def_function.py in _call(self, *args, **kwds)
630 # Lifting succeeded, so variables are initialized and we can run the
631 # stateless function.
--> 632 return self._stateless_fn(*args, **kwds)
633 else:
634 canon_args, canon_kwds = \

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\eager\function.py in call(self, *args, **kwargs)
2361 with self._lock:
2362 graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 2363 return graph_function._filtered_call(args, kwargs) # pylint: disable=protected-access
2364
2365 @Property

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\eager\function.py in _filtered_call(self, args, kwargs)
1609 if isinstance(t, (ops.Tensor,
1610 resource_variable_ops.BaseResourceVariable))),
-> 1611 self.captured_inputs)
1612
1613 def _call_flat(self, args, captured_inputs, cancellation_manager=None):

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\eager\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
1690 # No tape is watching; skip to running the function.
1691 return self._build_call_outputs(self._inference_function.call(
-> 1692 ctx, args, cancellation_manager=cancellation_manager))
1693 forward_backward = self._select_forward_and_backward_functions(
1694 args,

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\eager\function.py in call(self, ctx, args, cancellation_manager)
543 inputs=args,
544 attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 545 ctx=ctx)
546 else:
547 outputs = execute.execute_with_cancellation(

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
65 else:
66 message = e.message
---> 67 six.raise_from(core._status_to_exception(e.code, message), None)
68 except TypeError as e:
69 keras_symbolic_tensors = [

~\anaconda3\envs\gputest\lib\site-packages\six.py in raise_from(value, from_value)

UnknownError: [Derived] Fail to find the dnn implementation.
[[{{node CudnnRNN}}]]
[[sequential/lstm/StatefulPartitionedCall]] [Op:__inference_distributed_function_7959]

Function call stack:
distributed_function -> distributed_function -> distributed_function"
44281,Mirrored Strategy CUDA_ERROR_ILLEGAL_ADDRESS,"**System information**
Have I written custom code: NO
OS: Windows 10
Tensorflow Version: v2.3.0-54-gfcc4b966f1 2.3.1
Python Version: Python 3.8.6
CUDA/cuDNN version: 10.1/cudnn-10.1-windows10-x64-v7.6.5.32
GPU model and memory: 2x GeForce RTX 2080 Ti 11GB

**Describe the current behavior**
Tensorflow fails with the folllowing error:

```
Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
```
Only see this error when executing using MirroredStrategy. Single GPU works just fine.

**Describe the expected behavior**
Tensorflow should execute my code without any errors.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import Xception
import numpy as np

num_samples = 1000
height = 224
width = 224
num_classes = 1000

strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    parallel_model = Xception(weights=None,
                     input_shape=(height, width, 3),
                     classes=num_classes)
    parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

# Generate dummy data.
x = np.random.random((num_samples, height, width, 3))
y = np.random.random((num_samples, num_classes))

parallel_model.summary()
parallel_model.fit(x, y, epochs=20, batch_size=16) #batch_sized changed to 16
```

**Other info / logs** 
```
2020-10-23 23:35:09.514415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.635GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s
2020-10-23 23:35:09.514583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties:
pciBusID: 0000:04:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.635GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s
2020-10-23 23:35:09.514832: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-10-23 23:35:09.518478: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-10-23 23:35:09.521479: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-10-23 23:35:09.522718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-10-23 23:35:09.526250: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-10-23 23:35:09.528215: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-10-23 23:35:09.536133: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-10-23 23:35:09.536323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
2020-10-23 23:35:09.536813: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-23 23:35:09.546403: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14c16f08870 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-23 23:35:09.546511: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-23 23:35:09.749996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.635GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s
2020-10-23 23:35:09.750156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties:
pciBusID: 0000:04:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.635GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s
2020-10-23 23:35:09.750429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-10-23 23:35:09.750544: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-10-23 23:35:09.750639: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-10-23 23:35:09.750729: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-10-23 23:35:09.750818: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-10-23 23:35:09.750874: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-10-23 23:35:09.750958: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-10-23 23:35:09.751069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
2020-10-23 23:35:10.440797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-23 23:35:10.440935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1
2020-10-23 23:35:10.441179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N
2020-10-23 23:35:10.441273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N
2020-10-23 23:35:10.441592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9418 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5)
2020-10-23 23:35:10.442616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9419 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5)
2020-10-23 23:35:10.446257: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14c88a25f70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-10-23 23:35:10.446360: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2020-10-23 23:35:10.446450: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce RTX 2080 Ti, Compute Capability 7.5
2020-10-23 23:35:11.113923: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2020-10-23 23:35:11.114521: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:220] Unexpected Event status: 1
```"
44280,tensorflow.js have tf.nn.ctc_greedy_decoder?,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
44279,Protobuf Installation Issue (?),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Fedora 32
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):  Source
- TensorFlow version:  Master
- Python version: 3.8
- Installed using virtualenv? pip? conda?:  N/A
- Bazel version (if compiling from source):3.7
- GCC/Compiler version (if compiling from source):  10.2.1
- CUDA/cuDNN version:8
- GPU model and memory: Titan XP 12 GB



**Describe the problem**
Build fails with the following error:
ERROR: /home/melrobin/.cache/bazel/_bazel_melrobin/f2116cf848eff0a1d187a7773c1fea53/external/com_google_protobuf/BUILD:155:11: C++ compilation of rule '@com_google_protobuf//:protobuf' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/descriptor_database.d ... (remaining 48 argument(s) skipped)
ccache: error: invalid size: D
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=cuda  //tensorflow/tools/pip_package:build_pip_package


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44278,Unexpected `snapshot` behaviour with `flat_map` in tf-nightly,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): '2.4.0-dev20201023'
- Python version: 3.7.7

**Describe the current behavior**
A dataset formed by `flat_map`ping multiple snapshotted datasets which have each been iterated over individually (thus producing files on disk) results in a dataset which seemingly does not use those files on disk. This is different to the behaviour of `cache` in tf-2.3 and tf-nightly and `snapshot` in tf-2.3

**Describe the expected behavior**
`snapshot` to work equivalently in 2.3 as tf-nightly, and similarly to `cache`.

**Standalone code to reproduce the issue**
Colab [here](https://colab.research.google.com/drive/13mXDdNKNg04MEDvFTwmQq0tAw8L0MY02?usp=sharing).

```python
import os
from tempfile import TemporaryDirectory

import numpy as np
import tensorflow as tf


def as_numpy(ds: tf.data.Dataset):
    return np.array([x.numpy() for x in ds])


def get_data(
    num_repeats=2,
    snap=False,
    preprocess_early=False,
    preprocess_late=False,
    del_rng=False,
):
    """"""
    Get numpy results from a data pipeline.

    The pipeline looks like:
        1. range
        2. add stateful random noise
        3. create `num_repeats` `cache`d or `snapshot`ted versions
        4. `flat_map` if num_repeats > 1

    Args:
        num_repeats: number of duplicates created in step 3 above.
        snap: use `snapshot` (otherwise use `cache`)
        preprocess_early: if True, we iterate over individually cached / snapshotted
            datasets prior to flat-mapping.
        preprocess_late: if True, we iterate over the `flat_map`ped dataset
        del_rng: if True, we delete the rng responsible for generating random noise in
            step 2. This will cause an error if this map function is called again,
            rather than using cached / snapshotted files on disk.

    Returns:
        Two iterations of the repeated dataset.
    """"""
    rng = tf.random.Generator.from_seed(0)
    dataset = tf.data.Dataset.range(10).map(
        lambda x: tf.cast(x, tf.float32) + rng.uniform(())
    )
    with TemporaryDirectory() as tmp_dir:
        paths = [os.path.join(tmp_dir, f""repeat-{i}"") for i in range(num_repeats)]
        if snap:
            datasets = [
                dataset.apply(tf.data.experimental.snapshot(path)) for path in paths
            ]
        else:
            datasets = [dataset.cache(path) for path in paths]
        if preprocess_early:
            # iterate over datasets individually to force saving to file
            for ds in datasets:
                as_numpy(ds)
        if num_repeats == 1:
            (dataset,) = datasets
        else:
            dataset = tf.data.Dataset.from_tensor_slices(datasets).flat_map(lambda x: x)
        if preprocess_late:
            # iterate over concatenated dataset to force saving to file
            as_numpy(dataset)
        if del_rng:
            # this will cause an error is the original mapped dataset is called
            del rng
        return as_numpy(dataset), as_numpy(dataset)


class SnapshotTest(tf.test.TestCase):
    def test_consistent(self):
        base0, base1 = get_data()
        np.testing.assert_equal(base0, base1)

    def test_reproducible(self):
        base0, _ = get_data()
        s0, s1 = get_data()
        np.testing.assert_equal(s0, s1)
        np.testing.assert_equal(s0, base0)

    def test_snapshot(self):
        base0, _ = get_data()
        s0, s1 = get_data(snap=True)
        np.testing.assert_equal(s0, s1)
        np.testing.assert_equal(s0, base0)

    def test_preprocess_late(self):
        base0, _ = get_data()
        s0, s1 = get_data(snap=True, preprocess_late=True)
        np.testing.assert_equal(s0, s1)
        np.testing.assert_equal(s0, base0)

    def test_preprocess_late_del_rng(self):
        base0, _ = get_data()
        s0, s1 = get_data(snap=True, preprocess_late=True, del_rng=True)
        np.testing.assert_equal(s0, s1)
        np.testing.assert_equal(s0, base0)

    def test_preprocess_early(self):
        base0, _ = get_data()
        s0, s1 = get_data(snap=True, preprocess_early=True)
        np.testing.assert_equal(s0, s1)
        np.testing.assert_equal(s0, base0)

    def test_preprocess_early_del_rng(self):
        base0, _ = get_data()
        s0, s1 = get_data(snap=True, preprocess_early=True, del_rng=True)
        np.testing.assert_equal(s0, s1)
        np.testing.assert_equal(s0, base0)

    def test_preprocess_no_repeats(self):
        # preprocess_early is equivalent to preprocess_late here
        base0, _ = get_data(num_repeats=1)
        s0, s1 = get_data(snap=True, preprocess_early=True, num_repeats=1)
        np.testing.assert_equal(s0, s1)
        np.testing.assert_equal(s0, base0)

    def test_preprocess_del_rng_no_repeats(self):
        # preprocess_early is equivalent to preprocess_late here
        base0, _ = get_data(num_repeats=1)
        s0, s1 = get_data(snap=True, preprocess_early=True, num_repeats=1, del_rng=True)
        np.testing.assert_equal(s0, s1)
        np.testing.assert_equal(s0, base0)


if __name__ == ""__main__"":
    tf.test.main()
```

**Other info / logs**
Failed test output:
```txt
======================================================================
ERROR: test_preprocess_early_del_rng (__main__.SnapshotTest)
SnapshotTest.test_preprocess_early_del_rng
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/context.py"", line 2113, in execution_mode
    yield
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 733, in _next_internal
    output_shapes=self._flat_output_shapes)
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2579, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/_AnonymousVar6/N10tensorflow3VarE does not exist.
	 [[{{node stateful_uniform/StatefulUniform}}]] [Op:IteratorGetNext]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""foob.py"", line 107, in test_preprocess_early_del_rng
    s0, s1 = get_data(snap=True, preprocess_early=True, del_rng=True)
  File ""foob.py"", line 67, in get_data
    return as_numpy(dataset), as_numpy(dataset)
  File ""foob.py"", line 9, in as_numpy
    return np.array([x.numpy() for x in ds])
  File ""foob.py"", line 9, in <listcomp>
    return np.array([x.numpy() for x in ds])
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 747, in __next__
    return self._next_internal()
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 739, in _next_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/contextlib.py"", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/context.py"", line 2116, in execution_mode
    executor_new.wait()
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/executor.py"", line 69, in wait
    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/_AnonymousVar6/N10tensorflow3VarE does not exist.
	 [[{{node stateful_uniform/StatefulUniform}}]]

======================================================================
FAIL: test_preprocess_early (__main__.SnapshotTest)
SnapshotTest.test_preprocess_early
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""foob.py"", line 103, in test_preprocess_early
    np.testing.assert_equal(s0, base0)
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/numpy/testing/_private/utils.py"", line 342, in assert_equal
    return assert_array_equal(actual, desired, err_msg, verbose)
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/numpy/testing/_private/utils.py"", line 931, in assert_array_equal
    verbose=verbose, header='Arrays are not equal')
  File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/numpy/testing/_private/utils.py"", line 840, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Arrays are not equal

Mismatched elements: 20 / 20 (100%)
Max absolute difference: 0.90819454
Max relative difference: 1.9366292
 x: array([0.91562 , 1.45509 , 2.253555, 3.829305, 4.681193, 5.65526 ,
       6.401854, 7.514806, 8.184864, 9.174181, 0.130606, 1.063369,
       2.513922, 3.190604, 4.433053, 5.044663, 6.653943, 7.007094,
       8.878403, 9.046815], dtype=float32)
 y: array([0.311793, 1.18098 , 2.761353, 3.138052, 4.027518, 5.460741,
       6.235661, 7.175892, 8.786037, 9.549028, 0.860469, 1.631952,
       2.669349, 3.255722, 4.884421, 5.066545, 6.267429, 7.34992 ,
       8.16538 , 9.955009], dtype=float32)

----------------------------------------------------------------------
Ran 10 tests in 0.849s

FAILED (failures=1, errors=1, skipped=1)
```
"
44274,Tensorflow dataset is not faster with multiple cores,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- Linux Ubuntu 18.04):
- TensorFlow installed from Colab / AWS Deep Learning AMI
- TensorFlow version 2.3.0
- Python version 3.6
- Bazel version N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
I am using tensorflow dataset to read multiple datasets in parallel. I am trying to use dataset.interleave(map_function, ...) with multiple cores to speed up reads. All of the example code I have seen uses time.sleep() to ""emulate"" a real workload in the map_function. When doing this the dataset runs faster with multiple cores. However, when the map_function has a real computational load (and not just a sleep function) using multiple cores has no improvement. 

**Describe the expected behavior**
I expect that when I use multiple cores I will get a performance improvement when using a real workload in the map_function.

**Standalone code to reproduce the issue**
```
import time
import tensorflow as tf
import numpy as np
from multiprocessing import cpu_count
        
def real_work():
    for i in range(1000000):
        2+2
    
def sleep_delay():
    time.sleep(.03)

def run_test(num_cores, delay):
    def test_gen(k):
        for i in range(10):
            delay()
            yield 0

    class TestDataset(tf.data.Dataset):

        def __new__(cls,k):
            return tf.data.Dataset.from_generator(
                test_gen,
                output_types=((tf.dtypes.float32)),
                output_shapes=((None)),
                args=(k,)
            )

    def get_test_dataset(num_cores):

        dataset = tf.data.Dataset.from_tensor_slices([str(f) for f in range(30)])
        dataset = dataset.interleave(lambda k: TestDataset(k),
                                cycle_length=num_cores,
                                num_parallel_calls=num_cores, #tf.data.experimental.AUTOTUNE,
                                block_length=1,
                                deterministic=False)
        dataset = dataset.prefetch(100)
        return dataset

    it = get_test_dataset(num_cores).as_numpy_iterator()

    for i in it:
        continue

n_cpus = cpu_count()

print('\n 1 core, sleep delay')
%time run_test(1,sleep_delay)

print(f'\n {n_cpus} core, sleep delay')
%time run_test(n_cpus,sleep_delay)

print('\n 1 core, real_work')
%time run_test(1,real_work)

print(f'\n {n_cpus} core, real_work')
%time run_test(n_cpus,real_work)
```


** OUTPUT **
 1 core, sleep delay
CPU times: user 259 ms, sys: 11 ms, total: 270 ms
Wall time: 6.55 s

 4 core, sleep delay
CPU times: user 276 ms, sys: 11.3 ms, total: 287 ms
Wall time: 1.94 s

 1 core, real_work
CPU times: user 7.55 s, sys: 4.67 ms, total: 7.55 s
Wall time: 7.42 s

 4 core, real_work
CPU times: user 7.87 s, sys: 1.8 ms, total: 7.87 s
Wall time: 7.61 s

As can be seen the sleep delay system works as intended but with a real workload multicore has no impact.

here is a colab link: 
https://colab.research.google.com/drive/1hqxHWK4cxD-ja8kV33LM56z6uxPthsjo?usp=sharing
"
44272,DLL error importing python tensorflow library in pywrap_tensorflow.py,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home, x64, version 18362.1139 in Spanish (ESPAOL)
- Mobile device: I will test in a mobile device after development.
- TensorFlow installed from (source or binary): binary, pip install.
- TensorFlow version:
C:\>pip list | findstr tensorflow
tensorflow             1.15.0
tensorflow-estimator   1.15.1
* i tried version 2
- Python version: Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)] on win32
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): I dont know
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: no cuda
- GPU model and memory:  Intel UHD Graphics, 600. 4G memory
- AVX:
AVX             -       Supports AVX instruction extensions



**Describe the problem**
Im trying some code on python that includes tensorflow, when i do import tensorflow as tf it results into an DLL error 

i just installed Visual C++ downloads and i updraded to tensorflow 2,  it didnt work

*****Provide the exact sequence of commands / steps that you executed before running into the problem*****
python command line:
import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
.... ERROR

****** Any other info / logs******

>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Error en una rutina de inicializacin de biblioteca de vnculos dinmicos (DLL).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\lenovo\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Error en una rutina de inicializacin de biblioteca de vnculos dinmicos (DLL).


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
44266,Training is slow when using tf.keras.utils.Sequence with large Numpy arrays,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux RHEL Server 7.6**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **2.3.1**
- Python version: **3.6**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **10.1/7.6.4**
- GPU model and memory: **NVIDIA V100 32GB**

**Describe the current behavior**
I'm using `tf.keras.Model.fit()` on a GPU with input as `tf.keras.utils.Sequence` generating batches of `Numpy` arrays. When the generated input or target `Numpy` arrays are large I notice that the training slows down dramatically. Part of this is of course expected due to the host (CPU) to device (GPU) transfer, but the times I'm seeing suggest an additional overhead somewhere else.

Below is a minimal example reproducing the issue. To keep things simple, the `Numpy` arrays are created before `data_sequence` starts generating batches, so there is almost no work done by `data_sequence[idx]`. Also, the Model consists of a single linear activation, which should just translate to a NoOp.

On an NVIDIA Tesla V100 GPU I get ca. 900ms/step.
```
Epoch 1/10
100/100 [==============================] - 90s 899ms/step - loss: -3.4149e-04
Epoch 2/10
100/100 [==============================] - 91s 907ms/step - loss: -3.4149e-04
...
```
In TensorFlow Profiler trace_viewer one can see most of the time, namely 700ms, is spent in IteratorGetNextOp::DoCompute (or equally Iterator::Model and Iterator::Prefetch):
![tf_profiler](https://user-images.githubusercontent.com/35227976/97080852-c66f6e00-15fe-11eb-92f4-ea0fef54e98e.png)

**Describe the expected behavior**
The input and target `Numpy` arrays per batch combined have a size of `2*N*H*W*C*sizeof(np.float32) = 320 MB`. The CPU to GPU transfer bandwidth should be around `12.4 GB/s`, e.g. according to the [sample bandwidth test](https://github.com/NVIDIA/cuda-samples/tree/v10.1/Samples/bandwidthTest) from NVIDIA:
```
Host to Device Bandwidth, 1 Device(s)
 PINNED Memory Transfers
   Transfer Size (Bytes)        Bandwidth(GB/s)
   32000000                     12.4
```
so the CPU to GPU transfer cannot explain the almost 900ms per batch alone.

**Standalone code to reproduce the issue**
```python
import tqdm
import numpy as np
import tensorflow as tf
from tensorflow.keras.utils import Sequence

class NumpySequence(Sequence):
    def __init__(self, x, y):
        self.x = x
        self.y = y

    def __len__(self):
        return len(self.x)

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

workers = 1
steps_per_epoch, epochs = 100, 10
N, H, W, C = 4, 1000, 1000, 10

x_train = []
y_train = []
rng = np.random.default_rng()
for step in tqdm.tqdm(range(steps_per_epoch)):
    x_train.append(rng.standard_normal(size=(N, H, W, C), dtype=np.float32))
    y_train.append(rng.standard_normal(size=(N, H, W, C), dtype=np.float32))

data_sequence = NumpySequence(x_train, y_train)

model = tf.keras.models.Sequential([tf.keras.layers.Activation(""linear"")])
model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy())
model.fit(data_sequence, epochs=epochs, workers=workers)
```

**Is `TensorFlow` doing some additional serialization or memory copies of the `Numpy` arrays? If so, is there any way to prevent them?**

I'm aware of `tf.data.Dataset` and the optimizations it applies. Unfortunately, my actual pre-processing pipeline, of which the above example is only a simple caricature, is too complex to port to `tf.data.Dataset`. Also, if I understand it correctly, `tf.keras.Model.fit()` internally transforms the `tf.keras.utils.Sequence` to `tf.data.Dataset` using the `from_generator` generator method.
"
44265,TensorFlow Lite Error: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. TensorFlow Lite Error: Node number 0 (FlexConv3D) failed to prepare.  Fatal error: Failed to allocate memory for input tensors.,"Hi,

I am trying to use Activity i3D model on mobile devices. Have successfully Converted the i3D model check points to tflite. Now while loading the tflite model into the mobile application using TensorFlowLite Library. Getting the following error.

GitHub link to checkpoints : [https://github.com/deepmind/kinetics-i3d](https://github.com/deepmind/kinetics-i3d)

**TensorFlow Lite Error: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
TensorFlow Lite Error: Node number 0 (FlexConv3D) failed to prepare.

Fatal error: Failed to allocate memory for input tensors.**

Please help me find anything I am missing.
Thanks :-)"
44264,[FeatureRequest]Support tilde as alias for home directory in file paths,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.2.0
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**

I was trying to use `~/dir/file` style file-path to create `Dataset` using `tf.data.Dataset.list_files`, but it failed with `NotFoundError: No such file or directory`. With an expanded home directory path it works fine. Is it possible to add this feature to automatically resolve `tilde` as home directory.

**Will this change the current api? How?**

Not sure.

**Who will benefit with this feature?**
Maybe everyone (Or at least some lazy people like me)

**Any Other info.**
"
44263,"I had tried this code in Xcode with objective c but I am getting the error ""Explicit specialization of undeclared template struct 'NumTraits'"" could anyone help me out please?","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
44261,CMSIS repository always has to be downloaded an patched,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source): 392321a9a02f98baf7d9c0e34005d19568b38848
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**
Some targets like Arduino need to patch the CMSIS repository when downloading it. Most other targets that don't need this, still always do it as default. It would be better if only those targets need patching, do that.
Also when supplying a path to an external CMSIS on the command line, the CMSIS repository is still downloaded. Actually a combination of the two different CMSIS repositories are then used. It would be better to not download when supplying an external CMSIS and only use that.

**Please provide the exact sequence of commands/steps when you ran into the problem**
For example this will still download a CMSIS repository and patch it:
make -f tensorflow/lite/micro/tools/make/Makefile CMSIS_PATH=/path/to/CMSIS_5/ TAGS=cmsis-nn TARGET=stm32f4  test
"
44260,How to force to run a function on GPU,"Hello:

I have the following code:

`config=tf.compat.v1.ConfigProto(log_device_placement=True)
config.gpu_options.visible_device_list='0'
config.gpu_options.allow_growth = True
config.gpu_options.per_process_gpu_memory_fraction = 0.9
tf.compat.v1.enable_eager_execution()
tf.compat.v1.reset_default_graph()`

`with tf.compat.v1.Session(config=config) as sess:
    x_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 1], 0,
                                                 TRAIN_SPLIT, past_history,
                                                 future_target, STEP)

    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
    c = tf.matmul(a, b)
    #print(sess.run(x_train_multi, y_train_multi))
    print(sess.run(x_train_multi, y_train_multi))
    sess.close()`

The function, that makes 

`def multivariate_data(dataset, target, start_index, end_index, history_size,
                      target_size, step, single_step=False):
  #data = []
  #labels = []
  data = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)
  labels = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)
  #data = tf.placeholder(name=""data"", shape=[None, 720, 3], dtype=tf.float32)
  #labels = tf.placeholder(name= ""labels"", shape=[None, 36], dtype=tf.float32)
  start_index = start_index + history_size
  if end_index is None:
    end_index = len(dataset) - target_size
  #print(history_size)
  for i in range(start_index, end_index):
    indices = range(i-history_size, i, step)
    for j in indices:
     data = data.write(j, dataset[j])
    #data.append(dataset[indices])
    if single_step:
      labels = labels.write(i, target[i+target_size])
      #labels.append(target[i+target_size])
    else:
       for j in range(i, i+target_size):
          labels = labels.write(j, target[j])
      #labels.append(target[i:i+target_size])
  return data.stack(), labels.stack()
  #return np.array(data), np.array(labels)`

It runs on CPU, not on GPU:
CPU - 40%, Physical Memory - 97%, GPU - 2%

"
44259,How to use tf.keras.texts_to_sequences in tf.dataset,"I have a text classification task whose feeding files are lines of text. Before, I use keras and generator to handle with massive dataset.
Now I want to use tf.dataset which can feed the data in a parallel way that is important to me as the bottleneck now is IO and text transformation.
I tried to implement my code below, like a protype, but is does not work, as it will not operate the get_keras_data function in parallel way.
I suppose that the core problem is that API such as `texts_to_sequences` `pad_sequences` can not be used for the tensor, while they just use for the numpy arrary.

So is there any other approaches to meet my expectation?

```
def get_keras_data(lines):
    random.shuffle(lines)
    infos_feat, infos_feat_sub, infos_label = [], [], []
    for line in lines:
        line = line.decode('UTF-8')
        label = line.strip().split(' ')[0].replace('__label__','')
        label_ix = app_id[label]
        feat = ' '.join(line.strip().split(' ')[1:])

        infos_feat.append(feat)
        infos_label.append(label_ix)
        infos_feat_sub.append(feat.replace('.',' '))

    content_ids_list = tok.texts_to_sequences(infos_feat)
    content_pad_array = tf.keras.preprocessing.sequence.pad_sequences(content_ids_list, maxlen=maxlen, padding='post', truncating='post',value=0)
    content_ids_list = tok_sub.texts_to_sequences(infos_feat_sub)
    content_pad_array_sub = tf.keras.preprocessing.sequence.pad_sequences(content_ids_list, maxlen=max_len_sub, padding='post', truncating='post',value=0)

    trunk = ([content_pad_array, content_pad_array_sub], \
            tf.keras.utils.to_categorical(np.array(infos_label), num_classes = 998, dtype='int' ))
    return trunk


def generator(path, batch_size):
    file_list = glob.glob(""{}/part-*.csv"".format(path))
    dataset = tf.data.TextLineDataset(file_list, num_parallel_reads=32 )
    for lines in dataset.shuffle(buffer_size=buffer_size).repeat().batch(batch_size).prefetch(20).as_numpy_iterator():
        trunk = get_keras_data(lines)
        yield trunk
```"
44258,tflite file size is 1KB ?,"**System information**
- OS Platform and Distribution (windows 10):
- TensorFlow installed from (source):
- TensorFlow version (2.3):

im trying to convert my costum model ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8 to tflite so I can deploy it on android phone

First Iam generating tflite graph

```
python export_tflite_graph_tf2.py --pipeline_config_path=D:\models\TensorFlowColored89\workspace\training_demo\models\my_ssd_mobilenet_v2_fpnlite\pipeline.config --trained_checkpoint_dir=D:\models\TensorFlowColored89\workspace\training_demo\exported-models\my_mobilenet_model\checkpoint --output_directory=D:\models\TensorFlowColored89\workspace\training_demo\tflite_exported
```
then this command for getting the model.tflite

# the exact command
```
tflite_convert --saved_model_dir=D:\models\TensorFlowColored89\workspace\training_demo\tflite_exported\saved_model --output_file=D:\models\TensorFlowColored89\workspace\training_demo\tflite_exported\TFL.tflite

```
# Copy and paste the output here.

```

(tf) PS D:\models\TensorFlowColored89\models\research\object_detection> tflite_convert --saved_model_dir=D:\models\TensorFlowColored89\workspace\training_demo\tflite_exported\saved_model --output_file=D:\models\TensorFlowColored89\workspace\training_demo\tflite_exported\TFL.tflite
2020-10-23 12:01:07.115542: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-10-23 12:01:16.832442: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-10-23 12:01:17.685510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1
coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2020-10-23 12:01:17.685853: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-10-23 12:01:17.693815: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-10-23 12:01:17.699545: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-10-23 12:01:17.702309: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-10-23 12:01:17.708703: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-10-23 12:01:17.713493: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-10-23 12:01:17.724389: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-10-23 12:01:17.724807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-10-23 12:01:17.725887: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-23 12:01:17.736662: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15884ebf8c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-23 12:01:17.736911: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-23 12:01:17.737896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1
coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2020-10-23 12:01:17.738470: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-10-23 12:01:17.739031: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-10-23 12:01:17.739685: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-10-23 12:01:17.740310: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-10-23 12:01:17.740747: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-10-23 12:01:17.741304: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-10-23 12:01:17.741781: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-10-23 12:01:17.742327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-10-23 12:01:18.332670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-23 12:01:18.332908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0
2020-10-23 12:01:18.333608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N
2020-10-23 12:01:18.334330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2989 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-10-23 12:01:18.338688: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x158b91aab40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-10-23 12:01:18.338823: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050, Compute Capability 6.1
I1023 12:02:20.237044 33924 lite.py:624] Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False
2020-10-23 12:02:23.839000: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2020-10-23 12:02:23.839271: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.
2020-10-23 12:02:23.841449: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: D:\models\TensorFlowColored89\workspace\training_demo\tflite_exported\saved_model
2020-10-23 12:02:23.929815: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
2020-10-23 12:02:23.930050: I tensorflow/cc/saved_model/loader.cc:250] Reading SavedModel debug info (if present) from: D:\models\TensorFlowColored89\workspace\training_demo\tflite_exported\saved_model
2020-10-23 12:02:23.931573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-23 12:02:23.931855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]
2020-10-23 12:02:24.264018: I tensorflow/cc/saved_model/loader.cc:215] Restoring SavedModel bundle.
2020-10-23 12:02:24.910611: I tensorflow/cc/saved_model/loader.cc:199] Running initialization op on SavedModel bundle at path: D:\models\TensorFlowColored89\workspace\training_demo\tflite_exported\saved_model
2020-10-23 12:02:25.185379: I tensorflow/cc/saved_model/loader.cc:319] SavedModel load for tags { serve }; Status: success: OK. Took 1343952 microseconds.
2020-10-23 12:02:27.159515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1
coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2020-10-23 12:02:27.160041: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-10-23 12:02:27.161714: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-10-23 12:02:27.162453: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-10-23 12:02:27.163015: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-10-23 12:02:27.163827: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-10-23 12:02:27.164425: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-10-23 12:02:27.164992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-10-23 12:02:27.165708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-10-23 12:02:27.166372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-23 12:02:27.166798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0
2020-10-23 12:02:27.167201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N
2020-10-23 12:02:27.167793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2989 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)
(tf) PS D:\models\TensorFlowColored89\models\research\object_detection>
```
the tfl.tflite file size is 1Kb


**Also, please include a link to the saved model or GraphDef**

open the file with sublime  and it contains this ;
1c00 0000 5446 4c33 0000 1200 1c00 0400
0800 0c00 1000 1400 0000 1800 1200 0000
0300 0000 1400 0000 1400 0000 8000 0000
1800 0000 2800 0000 0000 0000 0200 0000
c000 0000 7c00 0000 0400 0000 b401 0000
b001 0000 0001 0000 3800 0000 0100 0000
0c00 0000 0800 0c00 0400 0800 0800 0000
0800 0000 0300 0000 1300 0000 6d69 6e5f
7275 6e74 696d 655f 7665 7273 696f 6e00
42ff ffff 0400 0000 1000 0000 0000 0000
0000 0000 0000 0000 0000 0000 0f00 0000
4d4c 4952 2043 6f6e 7665 7274 6564 2e00
ceff ffff 1400 0000 1400 0000 1400 0000
1400 0000 1400 0000 0000 0000 0000 0000
0000 0000 0000 0000 0400 0000 4e6f 4f70
0000 0e00 1800 0400 0800 0c00 1000 1400
0e00 0000 1400 0000 1c00 0000 2000 0000
3000 0000 3000 0000 0200 0000 9800 0000
4400 0000 0100 0000 0000 0000 0400 0000
0100 0000 0100 0000 0100 0000 0100 0000
0000 0000 0400 0000 6d61 696e 0000 0600
0800 0400 0600 0000 0400 0000 0400 0000
0000 0000 beff ffff 1000 0000 0200 0000
0c00 0000 2c00 0000 0000 0000 1900 0000
5374 6174 6566 756c 5061 7274 6974 696f
6e65 6443 616c 6c3a 3300 0000 0400 0600
0400 0000 0000 0e00 1400 0400 0000 0800
0c00 1000 0e00 0000 1000 0000 0100 0000
1c00 0000 3400 0000 0400 0000 0100 0000
4001 0000 4001 0000 0300 0000 1700 0000
7365 7276 696e 675f 6465 6661 756c 745f
696e 7075 743a 3000 fcff ffff 0400 0400
0400 0000 
```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)




**Any other info / logs**

I think its failing to convert to tflite 
"
44257,"Tensorflow combining with PySpark pandas_udf yields ""triggered tf.function retracing"".","Here is a snippet of my code combining Tensorflow with Spark for online prediction:
```
@pandas_udf(StringType())
def online_predict(values: pd.Series) -> pd.Series:
    pred = Model.from_config(bc_config.value)
    pred.set_weights(bc_weights.value)
    ds = tf.data.Dataset.from_tensor_slices(values)
    ds = ds.map(preprocessing).batch(batch_size)
    res = pred.predict_step(ds)
    res = tf.norm(res, axis=1)
    # res = tf.greater(res, 5.0)
    res = tf.strings.as_string(res).numpy()
    return pd.Series(res)


spark = SparkSession.builder.appName(
    'spark_tf').master(""local[*]"").getOrCreate()
weights = np.load('./ext/weights.npy', allow_pickle=True)
config = np.load('./ext/config.npy', allow_pickle=True).item()
bc_weights = spark.sparkContext.broadcast(weights)
bc_config = spark.sparkContext.broadcast(config)

stream = spark.readStream.format('kafka') \
    .option('kafka.bootstrap.servers', 'localhost:9092') \
    .option('subscribe', 'network') \
    .load()

stream = stream.select(online_predict(col('value')).alias('value'))

x = stream.writeStream \
    .format('kafka') \
    .option(""kafka.bootstrap.servers"", 'localhost:9092') \
    .option('topic', 'dlpred') \
    .option('checkpointLocation', './kafka_checkpoint') \
    .start()

x.awaitTermination()
```

Running the code yields this warning when data come repeatedly from Kafka topic, I assume this is due to the repeat of creating dataset:

> WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ca6abf598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.



"
44256,Docs for LambdaCallback should mention that it only works in training mode,"## URL(s) with the issue:

Documentation page for `tf.keras.callbacks.LambdaCallback`:
https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LambdaCallback?hl=en

## Description of issue (what needs changing):

The docs for the base class `tf.keras.callbacks.Callback` mentions that the methods `on_epoch_begin`, `on_epoch_end`, `on_batch_begin` and `on_batch_end` are supposed to be called in training mode only, but this information is not included in the docs for `tf.keras.callbacks.LambdaCallback`. This may cause confusion when users pass this callback to the `callbacks` parameter of `tf.keras.Model.predict` or `tf.keras.Model.evaluate` and subsequently find out that the supplied functions are not being called.

I suggest that this information be added to the documentation to make it clear that `tf.keras.callbacks.LambdaCallback` does not do anything in predict and test mode.

Optionally, the functionality of `tf.keras.callbacks.LambdaCallback` could be expanded to include the rest of the available callback hooks in `tf.keras.callbacks.Callback` (`on_predict_batch_begin`, `on_predict_batch_end` etc.) so that it may be used in predict and test mode as well."
44255,Different ABI between docker version and public version in tf 2.3.0,"Here is the link, where we first discussed issues with docker tensorflow:2.3.0 and tensorflow-text==2.3.0. https://github.com/tensorflow/text/issues/385
As pointed there - Docker image with tag 2.3.0 and tensorflow==2.3.0 have different ABIs.
Could it be fixed in future updates?"
44254,tensorflow/tools/pip_package/BUILD:66:1 C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command,"### System information

-   **NVIDIA Driver**: 455.23.05
-   **OS Platform and Distribution (e.g., Linux Ubuntu 20.04)**:
-   **TensorFlow installed from binary**: from r2.3 branch
-   **TensorFlow version (use command below)**: 2.3.0
-   **Python version**: 3.8.5
-   **Bazel version (if compiling from source)**: 3.1.0
-   **GCC/Compiler version (if compiling from source)**: 9.3.0
-   **CUDA/cuDNN version**: cuda: 11.1, cuDNN: 8.0.4, TensorRT: 7.2
-   **GPU model and memory**: GTX 1060 6GB
-   **Exact command to reproduce**: `bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures` also tried `bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""`

### Describe the problem
I have the freshest version of CUDA 11.1, cuDNN 8.0.4 and TensorRT 7.2. That's why to use CUDA and GPU I decided to build tf from this link https://www.tensorflow.org/install/source
Firstly, I've build tf with the master branch and successfully built tensorflow 2.4.0 .whl file and installed it with pip. While `./configure` I've specified all actual versions and everything worked fine.
However then I needed tf 2.3 and tried to rebuild everything at `r2.3` branch. There I've begun to receive different errors during compilation. Some of them fixed (for instance I've added 11.0 to cuda, cudart in `cuda_configure.bzl`. Also I've installed numpy<1.19.0.
BTW, that what I installed: 
```
pip install -U --user pip six 'numpy<1.19.0' wheel setuptools mock 'future>=0.17.1' 'gast==0.3.3' typing_extensions
pip install -U --user keras_applications --no-deps
pip install -U --user keras_preprocessing --no-deps
```
But that didn't help. If if was the first build, I would've thought - something with cuda, nccl versions etc. However, 2.4.0 build was perfect and still no workarounds from another issues are helpful. Could you help me please?
### Source code / logs

```INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/kirill/Documents/tensorflow/tensorflow/python/BUILD:501:1: C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/kirill/.cache/bazel/_bazel_kirill/aeab598ed30f46d483876f775f0431ef/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/lib/cuda/include:/usr/lib/cuda/lib64: \
    PATH=/home/kirill/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.o' -DMLIR_CUDA_CONVERSIONS_ENABLED -DSQLITE_OMIT_DEPRECATED -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL '-DGRPC_ARES=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DOPENSSL_IS_BORINGSSL -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/host/bin -iquote external/local_config_python -iquote bazel-out/host/bin/external/local_config_python -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/host/bin/external/snappy -iquote external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/host/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/host/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/host/bin/external/aws-checksums -iquote external/mkl_dnn -iquote bazel-out/host/bin/external/mkl_dnn -iquote external/llvm-project -iquote bazel-out/host/bin/external/llvm-project -iquote external/cub_archive -iquote bazel-out/host/bin/external/cub_archive -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive -iquote external/png -iquote bazel-out/host/bin/external/png -iquote external/lmdb -iquote bazel-out/host/bin/external/lmdb -iquote external/gemmlowp -iquote bazel-out/host/bin/external/gemmlowp -iquote external/icu -iquote bazel-out/host/bin/external/icu -iquote external/org_sqlite -iquote bazel-out/host/bin/external/org_sqlite -iquote external/com_github_grpc_grpc -iquote bazel-out/host/bin/external/com_github_grpc_grpc -iquote external/upb -iquote bazel-out/host/bin/external/upb -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cusolver_headers_virtual -Ibazel-out/host/bin/external/cub_archive/_virtual_includes/cub -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/nccl -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cusparse_headers_virtual -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/DialectSymbolRegistry -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffineOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SCFIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/DerivedAttributeOpInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/VectorOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LoopPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/OpenMPOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMAVX512IncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMAVX512ConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AVX512IncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffinePassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/NVVMConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/NVVMOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToNVVMGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ParallelLoopMapperAttrGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToROCDLTGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ROCDLOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToSPIRVIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVAvailabilityIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVCanonicalizationIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpUtilsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVSerializationGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/QuantOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/QuantPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeToStandardPatternsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeTransformsPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsTransformsPassIncGen -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cupti_headers_virtual -isystem external/local_config_python/numpy_include -isystem bazel-out/host/bin/external/local_config_python/numpy_include -isystem external/local_config_python/python_include -isystem bazel-out/host/bin/external/local_config_python/python_include -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/host/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/host/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/host/bin/external/aws-checksums/include -isystem external/mkl_dnn/include -isystem bazel-out/host/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/host/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/host/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu/xbyak -isystem external/llvm-project/llvm/include -isystem bazel-out/host/bin/external/llvm-project/llvm/include -isystem external/local_config_cuda/cuda/cublas/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cublas/include -isystem external/local_config_cuda/cuda/cusolver/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cusolver/include -isystem external/png -isystem bazel-out/host/bin/external/png -isystem external/local_config_cuda/cuda/cusparse/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cusparse/include -isystem external/icu/icu4c/source/common -isystem bazel-out/host/bin/external/icu/icu4c/source/common -isystem external/llvm-project/mlir/include -isystem bazel-out/host/bin/external/llvm-project/mlir/include -isystem tensorflow/compiler/mlir/tensorflow/include -isystem bazel-out/host/bin/tensorflow/compiler/mlir/tensorflow/include -isystem tensorflow/compiler/mlir/xla/include -isystem bazel-out/host/bin/tensorflow/compiler/mlir/xla/include -isystem external/llvm-project/llvm/include/llvm/IR -isystem bazel-out/host/bin/external/llvm-project/llvm/include/llvm/IR -isystem external/llvm-project/llvm/lib/Transforms/InstCombine -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Transforms/InstCombine -isystem external/llvm-project/llvm/lib/Target/NVPTX -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/NVPTX -isystem external/llvm-project/mlir/lib/Conversions/GPUToSPIRV -isystem bazel-out/host/bin/external/llvm-project/mlir/lib/Conversions/GPUToSPIRV -isystem external/llvm-project/mlir/lib/Conversion/StandardToSPIRV -isystem bazel-out/host/bin/external/llvm-project/mlir/lib/Conversion/StandardToSPIRV -isystem external/llvm-project/llvm/lib/Target/X86 -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/X86 -isystem external/llvm-project/llvm/lib/Target/AMDGPU -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/AMDGPU -isystem external/com_github_grpc_grpc/include -isystem bazel-out/host/bin/external/com_github_grpc_grpc/include -isystem external/com_github_grpc_grpc/src/core/ext/upb-generated -isystem bazel-out/host/bin/external/com_github_grpc_grpc/src/core/ext/upb-generated -isystem external/com_github_grpc_grpc/third_party/address_sorting/include -isystem bazel-out/host/bin/external/com_github_grpc_grpc/third_party/address_sorting/include -isystem external/local_config_cuda/cuda/cuda/extras/CUPTI/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/extras/CUPTI/include -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 '-std=c++14' -c tensorflow/python/lib/core/bfloat16.cc -o bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.o)
Execution platform: @local_execution_config_platform//:platform
tensorflow/python/lib/core/bfloat16.cc: In function bool tensorflow::{anonymous}::Initialize():
tensorflow/python/lib/core/bfloat16.cc:664:36: error: no match for call to (tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [6], <unresolved overloaded function type>, const std::array<int, 3>&)
  664 |                       compare_types)) {
      |                                    ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,
      |                         ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from <unresolved overloaded function type> to PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}
tensorflow/python/lib/core/bfloat16.cc:668:36: error: no match for call to (tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [10], <unresolved overloaded function type>, const std::array<int, 3>&)
  668 |                       compare_types)) {
      |                                    ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,
      |                         ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from <unresolved overloaded function type> to PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}
tensorflow/python/lib/core/bfloat16.cc:671:77: error: no match for call to (tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [5], <unresolved overloaded function type>, const std::array<int, 3>&)
  671 |   if (!register_ufunc(""less"", CompareUFunc<Bfloat16LtFunctor>, compare_types)) {
      |                                                                             ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,
      |                         ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from <unresolved overloaded function type> to PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}
tensorflow/python/lib/core/bfloat16.cc:675:36: error: no match for call to (tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [8], <unresolved overloaded function type>, const std::array<int, 3>&)
  675 |                       compare_types)) {
      |                                    ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,
      |                         ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from <unresolved overloaded function type> to PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}
tensorflow/python/lib/core/bfloat16.cc:679:36: error: no match for call to (tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [11], <unresolved overloaded function type>, const std::array<int, 3>&)
  679 |                       compare_types)) {
      |                                    ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,
      |                         ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from <unresolved overloaded function type> to PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}
tensorflow/python/lib/core/bfloat16.cc:683:36: error: no match for call to (tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [14], <unresolved overloaded function type>, const std::array<int, 3>&)
  683 |                       compare_types)) {
      |                                    ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>
  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,
      |                         ^
tensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from <unresolved overloaded function type> to PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/kirill/Documents/tensorflow/tensorflow/tools/pip_package/BUILD:66:1 C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/kirill/.cache/bazel/_bazel_kirill/aeab598ed30f46d483876f775f0431ef/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/lib/cuda/include:/usr/lib/cuda/lib64: \
    PATH=/home/kirill/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.o' -DMLIR_CUDA_CONVERSIONS_ENABLED -DSQLITE_OMIT_DEPRECATED -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL '-DGRPC_ARES=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DOPENSSL_IS_BORINGSSL -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/host/bin -iquote external/local_config_python -iquote bazel-out/host/bin/external/local_config_python -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/host/bin/external/snappy -iquote external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/host/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/host/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/host/bin/external/aws-checksums -iquote external/mkl_dnn -iquote bazel-out/host/bin/external/mkl_dnn -iquote external/llvm-project -iquote bazel-out/host/bin/external/llvm-project -iquote external/cub_archive -iquote bazel-out/host/bin/external/cub_archive -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive -iquote external/png -iquote bazel-out/host/bin/external/png -iquote external/lmdb -iquote bazel-out/host/bin/external/lmdb -iquote external/gemmlowp -iquote bazel-out/host/bin/external/gemmlowp -iquote external/icu -iquote bazel-out/host/bin/external/icu -iquote external/org_sqlite -iquote bazel-out/host/bin/external/org_sqlite -iquote external/com_github_grpc_grpc -iquote bazel-out/host/bin/external/com_github_grpc_grpc -iquote external/upb -iquote bazel-out/host/bin/external/upb -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cusolver_headers_virtual -Ibazel-out/host/bin/external/cub_archive/_virtual_includes/cub -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/nccl -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cusparse_headers_virtual -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/DialectSymbolRegistry -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffineOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SCFIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/DerivedAttributeOpInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/VectorOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LoopPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/OpenMPOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMAVX512IncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMAVX512ConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AVX512IncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffinePassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/NVVMConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/NVVMOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToNVVMGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ParallelLoopMapperAttrGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToROCDLTGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ROCDLOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToSPIRVIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVAvailabilityIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVCanonicalizationIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpUtilsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVSerializationGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/QuantOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/QuantPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeToStandardPatternsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeTransformsPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsTransformsPassIncGen -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cupti_headers_virtual -isystem external/local_config_python/numpy_include -isystem bazel-out/host/bin/external/local_config_python/numpy_include -isystem external/local_config_python/python_include -isystem bazel-out/host/bin/external/local_config_python/python_include -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/host/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/host/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/host/bin/external/aws-checksums/include -isystem external/mkl_dnn/include -isystem bazel-out/host/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/host/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/host/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu/xbyak -isystem external/llvm-project/llvm/include -isystem bazel-out/host/bin/external/llvm-project/llvm/include -isystem external/local_config_cuda/cuda/cublas/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cublas/include -isystem external/local_config_cuda/cuda/cusolver/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cusolver/include -isystem external/png -isystem bazel-out/host/bin/external/png -isystem external/local_config_cuda/cuda/cusparse/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cusparse/include -isystem external/icu/icu4c/source/common -isystem bazel-out/host/bin/external/icu/icu4c/source/common -isystem external/llvm-project/mlir/include -isystem bazel-out/host/bin/external/llvm-project/mlir/include -isystem tensorflow/compiler/mlir/tensorflow/include -isystem bazel-out/host/bin/tensorflow/compiler/mlir/tensorflow/include -isystem tensorflow/compiler/mlir/xla/include -isystem bazel-out/host/bin/tensorflow/compiler/mlir/xla/include -isystem external/llvm-project/llvm/include/llvm/IR -isystem bazel-out/host/bin/external/llvm-project/llvm/include/llvm/IR -isystem external/llvm-project/llvm/lib/Transforms/InstCombine -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Transforms/InstCombine -isystem external/llvm-project/llvm/lib/Target/NVPTX -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/NVPTX -isystem external/llvm-project/mlir/lib/Conversions/GPUToSPIRV -isystem bazel-out/host/bin/external/llvm-project/mlir/lib/Conversions/GPUToSPIRV -isystem external/llvm-project/mlir/lib/Conversion/StandardToSPIRV -isystem bazel-out/host/bin/external/llvm-project/mlir/lib/Conversion/StandardToSPIRV -isystem external/llvm-project/llvm/lib/Target/X86 -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/X86 -isystem external/llvm-project/llvm/lib/Target/AMDGPU -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/AMDGPU -isystem external/com_github_grpc_grpc/include -isystem bazel-out/host/bin/external/com_github_grpc_grpc/include -isystem external/com_github_grpc_grpc/src/core/ext/upb-generated -isystem bazel-out/host/bin/external/com_github_grpc_grpc/src/core/ext/upb-generated -isystem external/com_github_grpc_grpc/third_party/address_sorting/include -isystem bazel-out/host/bin/external/com_github_grpc_grpc/third_party/address_sorting/include -isystem external/local_config_cuda/cuda/cuda/extras/CUPTI/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/extras/CUPTI/include -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 '-std=c++14' -c tensorflow/python/lib/core/bfloat16.cc -o bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.o)
Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 2.180s, Critical Path: 1.67s
INFO: 4 processes: 4 local.
FAILED: Build did NOT complete successfully```"
44253,tensorflow.map_fn support in Custom Layer.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary):  binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.8
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: 10.1.243 / 7.6.5
- GPU model and memory: GeForce GTX 950M 2GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I am trying to create a custom layer that calculates the forward kinematics for a robotic arm using 'DH parameters'. In my code, I am using the 6 joint angles as the input of the custom layer (Kinematics_Physics) and I am using tensorflow.map_fn to iteratively calculate the forward kinematics of each set of angles in the input. My goal is to set the 'DH parameters' as the trainable weights and train a model to optimize the 'DH parameters'.  when I test the custom layer with tf.Variable as input it works without any error. Also, when I used the custom layer wrapped in a tensorflow.keras.layers.Lambda layers, there are no errors but obviously the weights of my custom layer is not visible to the tensorflow. as can be seen from the model.summary() output. However when I try to use the custom layer as it is in the model, tensorflow throws the error as ""TypeError: 'Tensor' object cannot be interpreted as an integer"".

**Describe the expected behavior**
There could be something wrong in my implementation of the custom layer or probably the Tensorflow API doesn't support the map_fn for layers in the Functional API for building models. Is there a workaround for this problem? Any recommendations?

**Standalone code to reproduce the issue**
```
import tensorflow as tf
print(tf.__version__)
from tensorflow import math as m
from tensorflow.keras import layers

class Kinematics_Physics(layers.Layer):

    def __init__(self,):
        super(Kinematics_Physics, self).__init__()
        # define initial variables
        self.TF_PI = tf.constant(3.1415926535897932, dtype =tf.float32)
        self.TF_180 = tf.constant(180.0, dtype =tf.float32)
        self.TINY_VALUE = tf.constant(1e-6, dtype =tf.float32)
        self.out_Pose=tf.Variable([0,0,0,0,0,0],  dtype =tf.float32, trainable = False)
        self.A_Deg = tf.Variable(0,dtype = tf.float32, trainable = False)
        self.B_Deg = tf.Variable(0,dtype = tf.float32, trainable = False)
        self.C_Deg = tf.Variable(0,dtype = tf.float32, trainable = False)
        self.sA = tf.Variable(0,dtype = tf.float32, trainable = False)
        self.cA = tf.Variable(0,dtype = tf.float32, trainable = False)
        self.b = tf.Variable(tf.zeros((4,4),dtype = tf.float32),dtype = tf.float32, trainable = False)
        
        # initial dh parameters: 6 x 4 parameters for a robotic arm with 6 joints.
        dh = tf.constant([  [0 ,    180.,   -650.,  0.],
                            [270.,   90.,      0.,  0.],
                            [800.,    0.,      0.,  0.],
                            [140.,   90.,   -908.,  0.],
                            [0.,    -96.,      0.,  0.],
                            [0.,    -65.,   260.,   0.]] ,dtype = tf.float32)
        
        # convert to trainable weights?? for further optimization 
        self.dh = tf.Variable(initial_value=dh, trainable=True)

        # initialize a buffer to calculate the the modified angles
        self.dh_processed = tf.Variable(tf.zeros(self.dh.shape,dtype = tf.float32),dtype = tf.float32, trainable = False)
        # buffer for transformation matrix
        self.trans_matrix = tf.Variable(tf.zeros((4,4),dtype = tf.float32),  dtype =tf.float32, trainable = False)


    @tf.function
    def radians(self,a):
        return m.multiply(a,(tf.divide(self.TF_PI, self.TF_180 )))
    
    @tf.function
    def degrees(self,a):
        return m.multiply(a,(tf.divide(self.TF_180 , self.TF_PI)))
    
    @tf.function
    def joint_transform(self,input_values):
        '''input: format --> 1D array = [a, , d, ]'''
    
        a = input_values[0] # a
        alpha = self.radians(input_values[1]) #convert degrees to radians
        d = input_values[2] # d
        theta = self.radians(input_values[3]) #convert degrees to radians 
        
        self.trans_matrix.assign(
                                [[      m.cos(theta),             -m.sin(theta),              0,                a],
                                [m.multiply(m.sin(theta),m.cos(alpha)),  m.multiply(m.cos(theta),m.cos(alpha)), -m.sin(alpha), m.multiply(-m.sin(alpha),d)],
                                [m.multiply(m.sin(theta),m.sin(alpha)),  m.multiply(m.cos(theta),m.sin(alpha)),  m.cos(alpha),  m.multiply(m.cos(alpha),d)],
                                [        0,                             0,                     0,            1        ]])
        return self.trans_matrix
    
    @tf.function
    def to_pose(self,T):
        # converts the transformation matrix to pose [x, y, z, alpha ]
        if(m.abs(T[1,2]) <= self.TINY_VALUE and m.abs(T[2,2]) <= self.TINY_VALUE): #% singular : B = +-90 (Gimbal Lock)
            self.C_Deg.assign(tf.constant(0, dtype =tf.float32))
            self.B_Deg.assign(self.degrees(m.atan2( T[0,2] , m.divide_no_nan(T[2,2], m.cos(self.C_Deg)) ))) #convert radians to degree
            self.A_Deg.assign(self.degrees(m.atan2( T[1,0] ,  m.divide_no_nan(T[1,1], m.cos(self.C_Deg)) ))) #convert radians to degree
        
        else:
            self.A_Deg.assign(self.degrees(m.atan2(-T[0,1] , T[0,0]))) #convert radians to degree
            self.sA.assign(m.sin(self.radians(self.A_Deg)))
            self.cA.assign(m.cos(self.radians(self.A_Deg)))
            self.B_Deg.assign(self.degrees(m.atan2(T[0,2] , m.multiply(self.cA,T[0,0]) - m.multiply(self.sA,T[0,1]))))#convert radians to degree
            self.C_Deg.assign(self.degrees(m.atan2(-T[1,2] , T[2,2]))) #convert radians to degree
        
        self.out_Pose.assign([ T[0,3], T[1,3], T[2,3],self.A_Deg, self.B_Deg , self.C_Deg])
        return self.out_Pose
    
    
    @tf.function
    def forward_kinematics(self, theta):
        
        # add the dh_theta to measured theta to get the final theta
        actual_theta = m.add(self.dh[:,3] ,theta)
        
        # create new dh_table with modified theta for forward kinematics calculation
        self.dh_processed[:,0].assign(self.dh[:,0])
        self.dh_processed[:,1].assign(self.dh[:,1])
        self.dh_processed[:,2].assign(self.dh[:,2])
        self.dh_processed[:,3].assign(actual_theta)
        b = tf.eye(4, dtype = tf.float32)
        for i in tf.range(6):
            b = (tf.linalg.matmul(b,self.joint_transform(self.dh_processed[i])))
        return self.to_pose(b)
      
    def __call__(self, inputs):
        return tf.map_fn(self.forward_kinematics, inputs, parallel_iterations=True, dtype=tf.float32)
    
    def get_config(self):
        config = super(Kinematics_Physics, self).get_config()
        return config
    

if __name__ == ""__main__"":
    # create a list of six joint angles
    ang = tf.Variable([[20,10,20,10,10,20],[15,10,20,22,20,32]], dtype = tf.float32, trainable=False)
    # Call the Kinematics_physics layer
    fk = Kinematics_Physics()
    print(""trainable weights: "", len(fk.trainable_weights))
    print(""non-trainable weights: "", len(fk.non_trainable_weights))
    print(""trainable_weights: "", fk.trainable_weights)
    print(""forward kinematic transform: "", fk(ang[:10]))
    # Everything works up to this.

    # # create a model using functional API and custom layer wrapped in Lambda layer
    in_= layers.Input(shape=(6,))
    for_kin = layers.Lambda(Kinematics_Physics())(in_)
    model = tf.keras.models.Model(inputs=in_, outputs=for_kin) 
    model.summary() 
    # Works but the weights are not trainable in this case
    
    
    # create a model using functional API
    in_= layers.Input(shape=(6,))
    for_kin = Kinematics_Physics()(in_)
    model = tf.keras.models.Model(inputs=in_, outputs=for_kin) 
    model.summary() 
    # Error: expected behavior => should create a model with 24 trainable parameters
    #        but the error happens in tf.map_fn of Kinematics_Physics().__call__ function
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
2020-10-23 16:16:20.529396: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2.1.0
2020-10-23 16:16:23.763749: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-10-23 16:16:23.980052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 950M computeCapability: 5.0
coreClock: 0.928GHz coreCount: 5 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 74.65GiB/s
2020-10-23 16:16:23.980403: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-10-23 16:16:23.991313: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll 
2020-10-23 16:16:23.997168: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll  
2020-10-23 16:16:23.999492: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll 
2020-10-23 16:16:24.006531: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-10-23 16:16:24.014520: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-10-23 16:16:24.031075: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-10-23 16:16:24.035576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-10-23 16:16:24.037553: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2020-10-23 16:16:24.041785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 950M computeCapability: 5.0
coreClock: 0.928GHz coreCount: 5 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 74.65GiB/s
2020-10-23 16:16:24.043759: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-10-23 16:16:24.044399: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-10-23 16:16:24.045042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-10-23 16:16:24.045625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-10-23 16:16:24.046172: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-10-23 16:16:24.046644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-10-23 16:16:24.065410: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-10-23 16:16:24.070724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-10-23 16:16:25.336541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-23 16:16:25.337273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-10-23 16:16:25.338396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N
2020-10-23 16:16:25.358220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1370 MB memory) -> physical GPU (device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0, compute capability: 5.0)
trainable weights:  1
non-trainable weights:  9
trainable_weights:  [<tf.Variable 'Variable:0' shape=(6, 4) dtype=float32, numpy=
array([[   0.,  180., -650.,    0.],
       [ 270.,   90.,    0.,    0.],
       [ 800.,    0.,    0.,    0.],
       [ 140.,   90., -908.,    0.],
       [   0.,  -96.,    0.,    0.],
       [   0.,  -65.,  260.,    0.]], dtype=float32)>]
2020-10-23 16:16:27.011142: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
forward kinematic transform:  tf.Tensor(
[[ 548.9288   -118.251114 -527.5552     47.53804   -30.621014 -144.55815 ]
 [ 511.83533   -85.7232   -492.43372    49.052803  -46.484085 -145.28882 ]], shape=(2, 6), dtype=float32)
WARNING:tensorflow:
The following Variables were used a Lambda layer's call (lambda), but
are not present in its tracked objects:
  <tf.Variable 'Variable:0' shape=(6, 4) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
Model: ""model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 6)]               0
_________________________________________________________________
lambda (Lambda)              (None, 6)                 0
=================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
_________________________________________________________________
Traceback (most recent call last):
  File ""c:/Users/ABCD/Desktop/physics_layer.py"", line 127, in <module>
    for_kin = Kinematics_Physics()(in_)
  File ""c:/Users/ABCD/Desktop/physics_layer.py"", line 99, in __call__
    return tf.map_fn(self.forward_kinematics, inputs, parallel_iterations=True, dtype=tf.float32)
  File ""C:\ProgramData\Anaconda3\envs\tf_gpu\lib\site-packages\tensorflow_core\python\ops\map_fn.py"", line 228, in map_fn
    for elem in elems_flat]
  File ""C:\ProgramData\Anaconda3\envs\tf_gpu\lib\site-packages\tensorflow_core\python\ops\map_fn.py"", line 228, in <listcomp>
    for elem in elems_flat]
  File ""C:\ProgramData\Anaconda3\envs\tf_gpu\lib\site-packages\tensorflow_core\python\ops\tensor_array_ops.py"", line 1078, in __init__
    name=name)
  File ""C:\ProgramData\Anaconda3\envs\tf_gpu\lib\site-packages\tensorflow_core\python\ops\tensor_array_ops.py"", line 716, in __init__
    self._tensor_array = [None for _ in range(size)]
TypeError: 'Tensor' object cannot be interpreted as an integer
```
"
44252,How can I obtain the output of an intermediate layer (feature extraction) in Model Subclassing ?,"Please refer to the below link for more details on the issue.

https://stackoverflow.com/questions/64471742/skip-some-layers-in-keras-model-during-evaluation-validation-phase

My requirement is to override the Evaluation/Validation step after each epoch, with using the Existing Fit function.

Following below link does not work (when this code is written in test_step method)
https://keras.io/getting_started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer-feature-extraction
"
44251,Failed build with Cuda-11.1/TF2.3/cuDNN 8.0.4,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.3
- Python version: 3.8.6
- Installed using virtualenv? pip? conda?: venv with pip
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): MSVC 2019
- CUDA/cuDNN version: 11.1 / 8.0.4
- GPU model and memory: RTX 3090 w/ 24 GB 



**Describe the problem**
Bazel (via Baselisk) errors in `local_config_cuda`. I've referenced issues: listed [here](https://github.com/tensorflow/tensorflow/issues/38660#issuecomment-616581116) and [here](https://github.com/tensorflow/tensorflow/issues/37888). I have set the Bazel `BAZEL_VC` path, and MSYS2 is installed and pathed correctly.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
C:\Users\Kevin>.virtualenvs\tf_sourcebuild\Scripts\activate

(tf_sourcebuild) C:\Users\Kevin>cd TF_2.3_Source\tensorflow

(tf_sourcebuild) C:\Users\Kevin\TF_2.3_Source\tensorflow>configure.py
You have bazel 3.1.0 installed.
Please specify the location of python. [Default is C:\Users\Kevin\.virtualenvs\tf_sourcebuild\Scripts\python.exe]:


Found possible Python library paths:
  C:\Users\Kevin\AppData\Local\Programs\Python\Python38\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\Kevin\AppData\Local\Programs\Python\Python38\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Found CUDA 11.1 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/include
Found cuDNN 8 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.6


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]: --copt=-march=native


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.

(tf_sourcebuild) C:\Users\Kevin\TF_2.3_Source\tensorflow>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
```
WARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from c:\users\kevin\tf_2.3_source\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/Kevin/.virtualenvs/tf_sourcebuild/Scripts/python.exe
INFO: Reading rc options for 'build' from c:\users\kevin\tf_2.3_source\tensorflow\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2
INFO: Reading rc options for 'build' from c:\users\kevin\tf_2.3_source\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/Kevin/.virtualenvs/tf_sourcebuild/Scripts/python.exe --action_env PYTHON_LIB_PATH=C:/Users/Kevin/AppData/Local/Programs/Python/Python38/lib/site-packages --python_path=C:/Users/Kevin/.virtualenvs/tf_sourcebuild/Scripts/python.exe --config=xla --action_env TF_CUDA_VERSION=11.1 --action_env TF_CUBLAS_VERSION=11 --action_env TF_CUDNN_VERSION=8.0.4 --action_env TF_TENSORRT_VERSION=7.2.1 --action_env TF_CUDA_PATHS=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1 --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.6 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:v2 in file c:\users\kevin\tf_2.3_source\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file c:\users\kevin\tf_2.3_source\tensorflow\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file c:\users\kevin\tf_2.3_source\tensorflow\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file c:\users\kevin\tf_2.3_source\tensorflow\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain
INFO: Found applicable config definition build:opt in file c:\users\kevin\tf_2.3_source\tensorflow\.tf_configure.bazelrc: --copt=--copt=-march=native --define with_default_optimizations=true
INFO: Found applicable config definition build:cuda in file c:\users\kevin\tf_2.3_source\tensorflow\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file c:\users\kevin\tf_2.3_source\tensorflow\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain
INFO: Found applicable config definition build:windows in file c:\users\kevin\tf_2.3_source\tensorflow\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\users\kevin\tf_2.3_source\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Repository local_config_cuda instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule cuda_configure defined at:
  C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl:1399:18: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1369
                _create_local_cuda_repository(<1 more arguments>)
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 955, in _create_local_cuda_repository
                _get_cuda_config(repository_ctx, <1 more arguments>)
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 657, in _get_cuda_config
                find_cuda_config(repository_ctx, <2 more arguments>)
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 635, in find_cuda_config
                _exec_find_cuda_config(<3 more arguments>)
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 629, in _exec_find_cuda_config
                execute(repository_ctx, <1 more arguments>)
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
The system cannot find the path specified.
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1369
                _create_local_cuda_repository(<1 more arguments>)
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 955, in _create_local_cuda_repository
                _get_cuda_config(repository_ctx, <1 more arguments>)
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 657, in _get_cuda_config
                find_cuda_config(repository_ctx, <2 more arguments>)
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 635, in find_cuda_config
                _exec_find_cuda_config(<3 more arguments>)
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 629, in _exec_find_cuda_config
                execute(repository_ctx, <1 more arguments>)
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
The system cannot find the path specified.
WARNING: Target pattern parsing failed.
ERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1369
                _create_local_cuda_repository(<1 more arguments>)
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 955, in _create_local_cuda_repository
                _get_cuda_config(repository_ctx, <1 more arguments>)
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 657, in _get_cuda_config
                find_cuda_config(repository_ctx, <2 more arguments>)
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 635, in find_cuda_config
                _exec_find_cuda_config(<3 more arguments>)
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl"", line 629, in _exec_find_cuda_config
                execute(repository_ctx, <1 more arguments>)
        File ""C:/users/kevin/tf_2.3_source/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
The system cannot find the path specified.
INFO: Elapsed time: 0.524s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package
```
```
TF_CUDA_PATHS = ""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1, C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\include,C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\lib\x64,C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\bin""
```
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44250,"ValueError: Input 0 of layer sequential is incompatible with the layer: expected ndim=4, found ndim=3.","tensorflow: 2.2
python: 3.8
i use tf.data.dataset pipline to fit, but it has some error, i check my input shape and simplify my model, error still exists. i don`t know what`s problem happend?

train_x = np.random.uniform(0, 10, (300, 12, 24, 3))
train_y = np.random.randint(0, 2, 300)
val_x = np.random.uniform(0, 10, (100, 12, 24, 3))
val_y = np.random.randint(0, 2, 100)

model = Sequential([
    layers.Conv2D(32, 3, activation='relu', padding='same', input_shape=(12, 24, 3)),
    layers.MaxPooling2D(),
    layers.GlobalAveragePooling2D(),
    layers.Dense(1, activation='sigmoid'),
])

model.compile(loss='mse', optimizer=optimizers.SGD(1e-3), metrics=['accuracy'])

AUTOTUNE = tf.data.experimental.AUTOTUNE
train_ds = tf.data.Dataset.from_tensor_slices((train_x, train_y))
train_ds = train_ds.shuffle(300)
train_ds = train_ds.repeat()
train_ds = train_ds.batch(32)
train_ds = train_ds.prefetch(AUTOTUNE)

val_ds = tf.data.Dataset.from_tensor_slices((val_x, val_y)).prefetch(AUTOTUNE)

model.fit(train_ds, validation_data=val_ds, epochs=10, steps_per_epoch=10)

![image](https://user-images.githubusercontent.com/47907486/96959081-c9505d00-1531-11eb-9392-2ff1d8c201d6.png)
![image](https://user-images.githubusercontent.com/47907486/96959136-e2590e00-1531-11eb-903a-3f400f00dc60.png)
"
44249,Failed to find the dnn implementation,"UnknownError                              Traceback (most recent call last)
<ipython-input-7-9cfed8880631> in <module>
     14                     validation_data=(data[""X_test""], data[""y_test""]),
     15                     callbacks=[checkpointer, tensorboard],
---> 16                     verbose=1)
     17 model.save(os.path.join(""results"", model_name) + "".h5"")

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    817         max_queue_size=max_queue_size,
    818         workers=workers,
--> 819         use_multiprocessing=use_multiprocessing)
    820 
    821   def evaluate(self,

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    340                 mode=ModeKeys.TRAIN,
    341                 training_context=training_context,
--> 342                 total_epochs=epochs)
    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    344 

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    126         step=step, mode=mode, size=current_batch_size) as batch_logs:
    127       try:
--> 128         batch_outs = execution_function(iterator)
    129       except (StopIteration, errors.OutOfRangeError):
    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py in execution_function(input_fn)
     96     # `numpy` translates Tensors to values in Eager mode.
     97     return nest.map_structure(_non_none_constant_value,
---> 98                               distributed_function(input_fn))
     99 
    100   return execution_function

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\eager\def_function.py in __call__(self, *args, **kwds)
    566         xla_context.Exit()
    567     else:
--> 568       result = self._call(*args, **kwds)
    569 
    570     if tracing_count == self._get_tracing_count():

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\eager\def_function.py in _call(self, *args, **kwds)
    630         # Lifting succeeded, so variables are initialized and we can run the
    631         # stateless function.
--> 632         return self._stateless_fn(*args, **kwds)
    633     else:
    634       canon_args, canon_kwds = \

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\eager\function.py in __call__(self, *args, **kwargs)
   2361     with self._lock:
   2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   2364 
   2365   @property

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\eager\function.py in _filtered_call(self, args, kwargs)
   1609          if isinstance(t, (ops.Tensor,
   1610                            resource_variable_ops.BaseResourceVariable))),
-> 1611         self.captured_inputs)
   1612 
   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\eager\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1690       # No tape is watching; skip to running the function.
   1691       return self._build_call_outputs(self._inference_function.call(
-> 1692           ctx, args, cancellation_manager=cancellation_manager))
   1693     forward_backward = self._select_forward_and_backward_functions(
   1694         args,

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\eager\function.py in call(self, ctx, args, cancellation_manager)
    543               inputs=args,
    544               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 545               ctx=ctx)
    546         else:
    547           outputs = execute.execute_with_cancellation(

~\anaconda3\envs\gputest\lib\site-packages\tensorflow_core\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

~\anaconda3\envs\gputest\lib\site-packages\six.py in raise_from(value, from_value)

UnknownError:  [_Derived_]  Fail to find the dnn implementation.
	 [[{{node CudnnRNN}}]]
	 [[sequential/lstm/StatefulPartitionedCall]] [Op:__inference_distributed_function_7959]

Function call stack:
distributed_function -> distributed_function -> distributed_function



"
44248,tf version 1.10.0 does not have MeanSqauredError(),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- TensorFlow installed from (source or binary): Conda
- TensorFlow version:  1.10.1
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- GCC/Compiler version (if compiling from source): 5.4
- CUDA/cuDNN version: 9.0
- GPU model and memory: 1060

I am working on LSTM architecture using Keras (2.2.0) and tf (1.10.1). I have the below code:

model.compile(loss=tf.losses.MeanSquaredError(), optimizer = tf.optimizers.Adam(), metrics = [tf.metrics.MeanAbsoluteError()])

where AttributeError: module 'tensorflow.losses' has no attribute 'MeanSquaredError'. But tf version 1 is supposed to have this loss function. I checked the documentation part as well. https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError

Still can't get rid of the error. I dont want to upgrade to tf version 2. Cause i have to upgrade my CUDA drivers as well!!

How do I fix this?
Thanks and Regards
Niranjan. 

"
44247,why the scales in quantization are floats not in the format of m/2^n,"on the hardware, while we are doing quantized model inference, we use featmap*m/2^n to simulate rescaling from one layer(int32) to another(int8).
but in the tensorflow lite source code, it seems that the scale is calculate by (max-min)/2^8, while max and min are floats.
due to m and n are all integer, scale(m/2^n) is not totally equal to scale((max-min)/255)

For example, max = 3.1415 and min=1.9276, the scale=0.00476...
there is no m and n can make this scale"
44245,ERROR: No matching distribution found for tensorflow,"<em>Hello guys,
I am trying to install Tensorflow but its giving me this error.

![Capture](https://user-images.githubusercontent.com/63431339/96945131-3a721f00-14f6-11eb-8a52-42f81bb58eca.PNG)

I have Python V 3.9, Pip version 20.2
Python x64, Windows 10 x64
What I have already Tried:
I have tried creating Virtual environment using the following steps,
![Capture1](https://user-images.githubusercontent.com/63431339/96945257-ac4a6880-14f6-11eb-8c34-4e4fddf61966.PNG)
 which was successfully created... 
also tried these...
</em>

`pip install --upgrade `tensorflow``
and
`pip` install https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-2.3.0-cp38-cp38-win_amd64.whl`
 (This link was given @ Tensorflow web for Python v3.8, 3.9 not available)
I have already tried doing all these stuff with python v 3.8
My Paths are also set in Environment Variables

I have tried almost all suggestion provided on Stackoverflow and Github and even uninstalled and reinstalled Python, still
Any help will be highly appreciated!
"
44243,Unable to change tensor_content in SavedModel loader.cc on s390x,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): v1.8.0-49081-g13b54c02e4 2.2.0
- Python version: Python 3.6.9
- Bazel version (if compiling from source): 2.0.0- (@non-git)
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
On s390x architecture (big-endian),`Tensorflow Serving` is unable to serve model that were saved on little-endian systems containing `tensor_content`.  This happens because `tensor_content` data contains little-endian serialized data and there is no way of determining the endiness of the `tensor_content` data.

**Describe the expected behavior**
`Tensorflow Serving` should be able to serve models with `tensor_content` on big-endian systems.

**Standalone code to reproduce the issue**
Running this testcase on s390x `Tensorflow Serving` code will cause the problem:
```
bazel --host_jvm_args=""-Xms1024m"" --host_jvm_args=""-Xmx2048m"" test -c dbg --copt=-O -c opt --copt=-g --strip never --host_javabase=""@local_jdk//:jdk"" --test_tag_filters=-gpu,-benchmark-test,-v1only -k --test_timeout 300,450,1200,3600 --build_tests_only --test_output=errors --output_filter= -- //tensorflow_serving/model_servers:tensorflow_model_server_test
```

**Other info / logs** 
Here is another related [issue](https://github.com/tensorflow/tensorflow/issues/41652). 

As part of understanding the flow, I have been trying to prototype a patch which will byte-swap `tensor_content` field of the tensor when [loader.cc](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/cc/saved_model/loader.cc#L316) is done with loading the `MetaGraphDef` from the SavedModel.

I can byte-swap `tensor_content` field in a function but the changes are not persisted for reasons I am unable to figure out.  I have used `mutable` Protobuf calls to make these changes to no avail.  Here is a rough snippet of what I thought would have worked but it did not:
```
// Swap tensor_content of Const Op Tensors in the named functions
static Status SwapTensorContent(MetaGraphDef* meta_graph_def) {
  GraphDef graph_def = *meta_graph_def->mutable_graph_def();
  auto library = graph_def.mutable_library();
  for (auto& function : (*library->mutable_function())) {
    for (auto& node : (*function.mutable_node_def())) {
            if (node.op() == ""Const"") {
                auto node_iterator = node.mutable_attr()->find(""value"");
                if (node_iterator != node.mutable_attr()->end()) {
                    AttrValue node_value = node_iterator->second;
                    if (node_value.has_tensor()) {
                        TensorProto* tproto = node_value.mutable_tensor();
                        auto tsize = tproto->tensor_content().size();
                        if (tsize != 0)
                        {
                                Tensor parsed(tproto->dtype());
                                bool success = parsed.FromProto(*tproto);
                                DCHECK(success);
				// Try DT_INT64 first
                                if ( tproto->dtype() == DT_INT64) {
					// swap and set the tensor_content LE/BE
                                        TF_RETURN_IF_ERROR(ByteSwapTensor(&parsed));
                                        (*node.mutable_attr())[""value""].mutable_tensor()->set_tensor_content(string(reinterpret_cast<const char*>(parsed.tensor_data().data()), parsed.tensor_data().size())); //<--- this seems to work but not retained once loop exists
                                }
                        }
                    }
                }
            }
    }
  }  
  return Status::OK();
}

//loader.cc -> https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/cc/saved_model/loader.cc#L316
Status LoadSavedModelInternal(const SessionOptions& session_options,
                              const RunOptions& run_options,
                              const string& export_dir,
                              const std::unordered_set<string>& tags,
                              SavedModelBundle* const bundle) {
  const uint64 read_start_microseconds = Env::Default()->NowMicros();
  TF_RETURN_IF_ERROR(ReadMetaGraphDefFromSavedModel(export_dir, tags,
                                                    &bundle->meta_graph_def));
SwapTensorContent(&meta_graph_def);
// Expected meta_graph_def to contain swapped tensor_content but no effect???
```
I may be missing some fundamental aspects around mutability of Nodes in a `MetaGraphDef`.  Would appreciate any pointers.

Thanks.
"
44241,Metrics names get modified for multi-output models,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Linux Ubuntu 16.04.5 LTS
- TensorFlow installed from: binary, installed with pip
- TensorFlow version (use command below): 2.3.0, 2.3.1 and nightly (2.4.0-dev20201022)
- Python version: 3.6.9
- CUDA/cuDNN version: 10.1
- GPU model and memory: NVIDIA T4, 16 GB

**Describe the current behavior**
When training a multi-output model with a metric in `tf.keras.metric`, the name of the metric is modified during training (the name of the output is added as a prefix to the name of the metric). This causes the model to get saved incorrectly when using `save()` (saving to a `h5` file). When the saved model is reloaded, the name of the output is added another time to the metric. So every time we save/load the metric name gets longer. In TF 2.3.0 and 2.3.1, this problem seems to only affect metrics in `tf.keras.metric` and metrics defined by a string (e.g. `accuracy`) don't seem to be affected. However in the nightly this problem seems to have got worse and affects both types of metrics.

**Describe the expected behavior**
If a multi-output model is saved to a `h5` file and reloaded, the metric names should be the same as in the original model.

**Standalone code to reproduce the issue**
```
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Softmax
from tensorflow.keras.models import Model, load_model

# Create multi-output model
inp = Input(4)
m = Model(inp, [Softmax(name='head_0')(Dense(3)(inp)), Softmax(name='head_1')(Dense(5)(inp))])
metric = tf.keras.metrics.BinaryAccuracy()
m.compile(loss='mse', metrics = {'head_0': [metric, 'accuracy']})

print(f'Metric name: {metric.name}')

# Run one iteration
x = np.random.rand(2,4)
y = {'head_0': np.random.randint(2, size=(2,3)), 'head_1': np.random.randint(2, size=(2,5))}
m.fit(x, y, verbose=0)

print(f'Metric name: {metric.name}')

print(m.metrics_names)

# Save the model to a h5 file and reload it
model_fpath = 'model.h5'
m.save(model_fpath, save_format='h5')
m2 = load_model(model_fpath)

print(m2.metrics_names)
```

In this case we can see that `metric.name` gets modified after `fit` is called. The `metrics_names` before saving are initially:
```
['loss', 'head_0_loss', 'head_1_loss', 'head_0_binary_accuracy', 'head_0_accuracy']
```
After reloading in TF 2.3.x:
```
['loss', 'head_0_loss', 'head_1_loss', 'head_0_head_0_binary_accuracy', 'head_0_accuracy']
```
And after reloading in TF 2.4.0:
```
['loss', 'head_0_loss', 'head_1_loss', 'head_0_head_0_binary_accuracy', 'head_0_head_0_accuracy']
```"
44240,Why is calling model.build() required to load  weights for a subclassed tf.keras.Model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): tf-nightly 
- TensorFlow version (use command below):2.4
- Python version: 3.7.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A 
- GPU model and memory:N/A

**Describe the current behavior**
This issue is related to models that subclass `tf.keras.Model`. When calling `model.load_weights(""path.h5"")` why is it required to have called `model.build()` beforehand? This [line](https://github.com/tensorflow/tensorflow/blob/8bfaaccad9594e215765cf927bc51c4bf8c0aad8/tensorflow/python/keras/engine/training.py#L2221) checks whether `model.built == True` and fails to load the model if it is not. `model.built` gets set to `True` if `model.build()` is called. However the doc-string for `model.build()` makes it seem as if it is an optional call:

```
This is to be used for subclassed models, which do not know at instantiation
time what their inputs look like.
This method only exists for users who want to call `model.build()` in a
standalone way (as a substitute for calling the model on real data to
build it). It will never be called by the framework (and thus it will
never throw unexpected errors in an unrelated workflow).
```

Is there a reason that `model.built` is checked when loading model weights?

I believe this issue appeared in TF2.3."
44237,opcode 'SUB' version '3' in tfllite_runtime on Windows,"**System information**
- Ubuntu 18.04, Nvidia driver 430.4, CUDA 10.1
- tf-nightly: 2.4.0-dev20201022, and
- **tflite_runtime: 2.1.0.post1**

Converted keras-model with tf-nightly. This seems to be incompatible with tflite_runtime.

Inference test script:

```
import numpy as np
import tensorflow as tf
from tflite_runtime.interpreter import Interpreter

# interpreter = tf.lite.Interpreter(model_path=""<path_to_wts_file>.tflite"")
interpreter = Interpreter(model_path=""<path_to_wts_file>.tflite"")

input_details = interpreter.get_input_details()

interpreter.resize_tensor_input(input_details[0]['index'], (1, 64, 64, 64, 1))
input_details = interpreter.get_input_details()

interpreter.allocate_tensors()

input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)

interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()

output_details = interpreter.get_output_details()
output_data = interpreter.get_tensor(output_details[0]['index'])
```

The above works fine when using tf-nightly with:
```
import tensorflow as tf
interpreter = tf.lite.Interpreter(model_path=""<path_to_wts_file>.tflite"")
```

When trying to execute the same with **tflite_runtime**, it fails to load the model:
```
from tflite_runtime.interpreter import Interpreter
interpreter = Interpreter(model_path=""<path_to_wts_file>.tflite"")
```

Error:
```
Traceback (most recent call last):
  File ""tfliteTest.py"", line 13, in <module>
    interpreter = Interpreter(model_path=""_<path_to_wts_file>_.tflite"")
  File ""/home/venv/lib/python3.6/site-packages/tflite_runtime/interpreter.py"", line 204, in __init__
    model_path, self._custom_op_registerers))
ValueError: Didn't find op for builtin opcode 'SUB' version '3'
Registration failed.
```
Is there a nightly build of tflite_runtime?"
44235,Embedding layer with NaN input different behaviour on CPU vs GPU,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.6
- CUDA/cuDNN version: CUDA Version 10.1.243

**Describe the current behavior**

Different behaviour of embedding layer on CPU vs GPU. When running on CPU and a `nan` value is passed as input to the embedding layer, the code throws a correct `InvalidArgumentError`. Running the same code on GPU silently converts the `nan` value to 0 and does not throw any warning/exception.

**Describe the expected behavior**

Maybe an exception should be thrown (or at least a warning) if a `nan` value is passed to the embedding layer on GPU?

**Standalone code to reproduce the issue**

This code breaks (as expected) running on CPU:
```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding
print(tf.__version__)

# Disable cuda devices
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1"" 

# Make sure no cuda device is used
gpus = tf.config.experimental.list_physical_devices('GPU')
print(gpus)

# Simple toy model
n_inputs      = 10
embedding_dim = 2

input_tensor    = Input((1,))
embedded_tensor = Embedding(n_inputs, embedding_dim, name = 'embedding_layer')(input_tensor)

model = Model(inputs = input_tensor, outputs = embedded_tensor)

# Predict which should break (and does break on CPU)
model.predict([np.nan])
```
Output (`nan` gets converted to minimum value of int32 when cast to tf.int32, thus triggering the out of bound error):
```
InvalidArgumentError:  indices[0,0] = -2147483648 is not in [0, 10)
	 [[node functional_3/embedding_layer/embedding_lookup (defined at <ipython-input-7-7784f8517478>:25) ]] [Op:__inference_predict_function_295]

Errors may have originated from an input operation.
Input Source operations connected to node functional_3/embedding_layer/embedding_lookup:
 functional_3/embedding_layer/embedding_lookup/287 (defined at /usr/lib/python3.6/contextlib.py:81)

Function call stack:
predict_function
```
On the other hand, this piece of code does not break, nor throws a warning at me (running on GPU):
```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding
print(tf.__version__)

# Usa cuda device 0
gpus = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_visible_devices(gpus[0], 'GPU')

# Simple toy model
n_inputs      = 10
embedding_dim = 2

input_tensor    = Input((1,))
embedded_tensor = Embedding(n_inputs, embedding_dim, name = 'embedding_layer')(input_tensor)

model = Model(inputs = input_tensor, outputs = embedded_tensor)

# Predict which should break (and does break on CPU)
print(model.predict([np.nan]))
print(model.predict([0]))
```
Output (`nan` output is same as 0th embedding):
```
2.3.1
[[[ 0.02256938 -0.02229818]]]
[[[ 0.02256938 -0.02229818]]]
```
"
44234,"TensorFlow Lite Crashed when calling void tflite::reference_ops::Gather<long, int>","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04 and Android10
- TensorFlow installed from (source or binary):
binary tf_nightly
- TensorFlow version (or github SHA if from source):
2.4.0-dev20201005
- TensorFlow Lite version:
tensorflow-lite:0.0.0-nightly and also a debug build jni so for the backtrack in C.



**Also, please include a link to the saved model or GraphDef**
[inference.tflite.zip](https://github.com/tensorflow/tensorflow/files/5423912/inference.tflite.zip)


**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- The conversion was successful and I was able to invoke the generated tflite model on my desktop with Python API. However, it crashed when invoking with Java code in the Android App. I have compiled a debug JNI so and attached the native  backtrace below. It seems something wrong with the `Gather ` function.




**Any other info / logs**
```
signal 6 (SIGABRT), code -1 (SI_QUEUE), fault addr --------
    x0  0000000000000000  x1  00000000000078df  x2  0000000000000006  x3  0000007d53ac2410
    x4  0000007d446feb40  x5  0000007d446feb40  x6  0000007d446feb40  x7  000000010000012c
    x8  00000000000000f0  x9  17de649dc883d4e6  x10 0000000000000001  x11 0000000000000000
    x12 fffffff0fffffbdf  x13 0000000000000070  x14 0000000000000088  x15 0000007d53ac2528
    x16 0000007e4192f738  x17 0000007e4190dd20  x18 000000000000000a  x19 00000000000078af
    x20 00000000000078df  x21 00000000ffffffff  x22 0000007d53ac3350  x23 0000007db1589c22
    x24 0000000000000010  x25 0000007d53ac5020  x26 0000007da4c39cb0  x27 0000000000000004
    x28 0000007d53ac30d0  x29 0000007d53ac24b0
    sp  0000007d53ac23f0  lr  0000007e418bf404  pc  0000007e418bf430

backtrace:
      #00 pc 0000000000073430  /apex/com.android.runtime/lib64/bionic/libc.so (abort+160) (BuildId: 084c8a81b8c78e19cd9a1ff6208e77cf)
      #01 pc 0000000000238b54  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (void tflite::reference_ops::Gather<long, int>(tflite::GatherParams const&, tflite::RuntimeShape const&, long const*, tflite::RuntimeShape const, int const*, tflite::RuntimeShape const, tflite::RuntimeShape const&*)+536)
      #02 pc 0000000000236ec0  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (TfLiteStatus tflite::ops::builtin::gather::Gather<long, int>(TfLiteGatherParams const&, TfLiteTensor const*, TfLiteTensor const, TfLiteGatherParams const&*)+208)
      #03 pc 00000000002365e0  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (tflite::ops::builtin::gather::Eval(TfLiteContext*, TfLiteNode*)+448)
      #04 pc 00000000003af5b4  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (tflite::Subgraph::OpInvoke(TfLiteRegistration const&, TfLiteNode*)+96)
      #05 pc 00000000003aecd4  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (tflite::Subgraph::Invoke()+1396)
      #06 pc 0000000000335a48  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (tflite::ops::builtin::while_kernel::Eval(TfLiteContext*, TfLiteNode*)+1048)
      #07 pc 00000000003af5b4  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (tflite::Subgraph::OpInvoke(TfLiteRegistration const&, TfLiteNode*)+96)
      #08 pc 00000000003aecd4  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (tflite::Subgraph::Invoke()+1396)
      #09 pc 00000000003ba72c  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (tflite::Interpreter::Invoke()+88)
      #10 pc 00000000000d22f4  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+152)
      #11 pc 000000000013f350  /apex/com.android.runtime/lib64/libart.so (art_quick_generic_jni_trampoline+144) (BuildId: d409c858261cc9eed63528a22714bb15)
      #12 pc 00000000001365b8  /apex/com.android.runtime/lib64/libart.so (art_quick_invoke_static_stub+568) (BuildId: d409c858261cc9eed63528a22714bb15)
      #13 pc 000000000014500c  /apex/com.android.runtime/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+276) (BuildId: d409c858261cc9eed63528a22714bb15)
      #14 pc 00000000002e27cc  /apex/com.android.runtime/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+384) (BuildId: d409c858261cc9eed63528a22714bb15)
      #15 pc 00000000002dda2c  /apex/com.android.runtime/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+892) (BuildId: d409c858261cc9eed63528a22714bb15)
      #16 pc 00000000005a2824  /apex/com.android.runtime/lib64/libart.so (MterpInvokeStatic+372) (BuildId: d409c858261cc9eed63528a22714bb15)
      #17 pc 0000000000130994  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_static+20) (BuildId: d409c858261cc9eed63528a22714bb15)
      #18 pc 000000000018c28e  [anon:dalvik-classes.dex extracted in memory from /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapper.run+166)
      #19 pc 00000000005a0010  /apex/com.android.runtime/lib64/libart.so (MterpInvokeVirtual+1352) (BuildId: d409c858261cc9eed63528a22714bb15)
      #20 pc 0000000000130814  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: d409c858261cc9eed63528a22714bb15)
      #21 pc 000000000018b812  [anon:dalvik-classes.dex extracted in memory from /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/base.apk] (org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs+10)
      #22 pc 00000000005a0010  /apex/com.android.runtime/lib64/libart.so (MterpInvokeVirtual+1352) (BuildId: d409c858261cc9eed63528a22714bb15)
      #23 pc 0000000000130814  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: d409c858261cc9eed63528a22714bb15)
      #24 pc 00000000001896ae  [anon:dalvik-classes.dex extracted in memory from /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/base.apk] (com.example.android.tflitecamerademo.ImageClassifierInference.runInference+394)
      #25 pc 00000000005a0010  /apex/com.android.runtime/lib64/libart.so (MterpInvokeVirtual+1352) (BuildId: d409c858261cc9eed63528a22714bb15)
      #26 pc 0000000000130814  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: d409c858261cc9eed63528a22714bb15)
      #27 pc 0000000000189be6  [anon:dalvik-classes.dex extracted in memory from /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/base.apk] (com.example.android.tflitecamerademo.ImageClassifier.classifyFrame+70)
      #28 pc 00000000005a0010  /apex/com.android.runtime/lib64/libart.so (MterpInvokeVirtual+1352) (BuildId: d409c858261cc9eed63528a22714bb15)
      #29 pc 0000000000130814  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: d409c858261cc9eed63528a22714bb15)
      #30 pc 00000000001880c2  [anon:dalvik-classes.dex extracted in memory from /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/base.apk] (com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame+150)
      #31 pc 00000000005a231c  /apex/com.android.runtime/lib64/libart.so (MterpInvokeDirect+1100) (BuildId: d409c858261cc9eed63528a22714bb15)
      #32 pc 0000000000130914  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_direct+20) (BuildId: d409c858261cc9eed63528a22714bb15)
      #33 pc 0000000000187fcc  [anon:dalvik-classes.dex extracted in memory from /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/base.apk] (com.example.android.tflitecamerademo.Camera2BasicFragment.access$1000)
      #34 pc 00000000005a2ac0  /apex/com.android.runtime/lib64/libart.so (MterpInvokeStatic+1040) (BuildId: d409c858261cc9eed63528a22714bb15)
      #35 pc 0000000000130994  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_static+20) (BuildId: d409c858261cc9eed63528a22714bb15)
      #36 pc 00000000001878ae  [anon:dalvik-classes.dex extracted in memory from /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/base.apk] (com.example.android.tflitecamerademo.Camera2BasicFragment$8.run+34)
      #37 pc 00000000005a1830  /apex/com.android.runtime/lib64/libart.so (MterpInvokeInterface+1788) (BuildId: d409c858261cc9eed63528a22714bb15)
      #38 pc 0000000000130a14  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_interface+20) (BuildId: d409c858261cc9eed63528a22714bb15)
      #39 pc 0000000000320014  /system/framework/framework.jar (android.os.Handler.handleCallback+4)
      #40 pc 00000000005a2ac0  /apex/com.android.runtime/lib64/libart.so (MterpInvokeStatic+1040) (BuildId: d409c858261cc9eed63528a22714bb15)
      #41 pc 0000000000130994  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_static+20) (BuildId: d409c858261cc9eed63528a22714bb15)
      #42 pc 000000000031fe80  /system/framework/framework.jar (android.os.Handler.dispatchMessage+8)
      #43 pc 00000000005a0010  /apex/com.android.runtime/lib64/libart.so (MterpInvokeVirtual+1352) (BuildId: d409c858261cc9eed63528a22714bb15)
      #44 pc 0000000000130814  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: d409c858261cc9eed63528a22714bb15)
      #45 pc 0000000000344e14  /system/framework/framework.jar (android.os.Looper.loop+484)
      #46 pc 00000000005a2ac0  /apex/com.android.runtime/lib64/libart.so (MterpInvokeStatic+1040) (BuildId: d409c858261cc9eed63528a22714bb15)
      #47 pc 0000000000130994  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_static+20) (BuildId: d409c858261cc9eed63528a22714bb15)
      #48 pc 000000000031f59c  /system/framework/framework.jar (android.os.HandlerThread.run+56)
      #49 pc 00000000002b3ae0  /apex/com.android.runtime/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEbb.llvm.17460956533834400288+240) (BuildId: d409c858261cc9eed63528a22714bb15)
      #50 pc 00000000005912b8  /apex/com.android.runtime/lib64/libart.so (artQuickToInterpreterBridge+1032) (BuildId: d409c858261cc9eed63528a22714bb15)
      #51 pc 000000000013f468  /apex/com.android.runtime/lib64/libart.so (art_quick_to_interpreter_bridge+88) (BuildId: d409c858261cc9eed63528a22714bb15)
      #52 pc 0000000000136334  /apex/com.android.runtime/lib64/libart.so (art_quick_invoke_stub+548) (BuildId: d409c858261cc9eed63528a22714bb15)
      #53 pc 0000000000144fec  /apex/com.android.runtime/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+244) (BuildId: d409c858261cc9eed63528a22714bb15)
      #54 pc 00000000004afcbc  /apex/com.android.runtime/lib64/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+104) (BuildId: d409c858261cc9eed63528a22714bb15)
      #55 pc 00000000004b0dd0  /apex/com.android.runtime/lib64/libart.so (art::InvokeVirtualOrInterfaceWithJValues(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, jvalue const*)+416) (BuildId: d409c858261cc9eed63528a22714bb15)
      #56 pc 00000000004f178c  /apex/com.android.runtime/lib64/libart.so (art::Thread::CreateCallback(void*)+1176) (BuildId: d409c858261cc9eed63528a22714bb15)
      #57 pc 00000000000d6cb0  /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void*)+36) (BuildId: 084c8a81b8c78e19cd9a1ff6208e77cf)
      #58 pc 0000000000074eac  /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+64) (BuildId: 084c8a81b8c78e19cd9a1ff6208e77cf)
```
"
44232,TFlite interpreter fails to load a saved tflite model when Dropout is used!,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (or github SHA if from source): v 2.3.0


**Provide the text output from tflite_convert**

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-74-3ae2f98ec211> in <module>
----> 1 interpreter = tf.lite.Interpreter(model_path=tflite_file)
      2 translate_tflite(""este  o primeiro livro que eu fiz"",tokenizer_pt,tokenizer_en,interpreter,MAX_LENGTH)

~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py in __init__(self, model_path, model_content, experimental_delegates, num_threads)
    196       self._interpreter = (
    197           _interpreter_wrapper.CreateWrapperFromFile(
--> 198               model_path, self._custom_op_registerers))
    199       if not self._interpreter:
    200         raise ValueError('Failed to open {}'.format(model_path))

ValueError: Did not get operators, tensors, or buffers in subgraph 1.
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**It is a transformer-based machine translation. The code is the one implemented in Tensorflow tutorials page:**
https://www.tensorflow.org/tutorials/text/transformer

**The code for saving and loading the tflite model is as follows:**
```
# Load TFLite model and allocate tensors.
def evaluate_tflite(inp_sentence,tokenizer_pt,tokenizer_en,interpreter,max_length):
  start_token = [tokenizer_pt.vocab_size]
  end_token = [tokenizer_pt.vocab_size + 1]
  
  # TODO: languague change [check for erros]
  # inp sentence is lev, hence adding the start and end token
  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token
  encoder_input = tf.expand_dims(inp_sentence, 0)
  
  # as the target is english, the first word to the transformer should be the
  # english start token.
  decoder_input = [tokenizer_en.vocab_size]
  output = tf.expand_dims(decoder_input, 0)
  
  for i in range(max_length):
    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(
        encoder_input, output)
    
    # Use interpreter for inference
    # print (input_details)
#     interpreter = tf.lite.Interpreter(model_path=tflite_file)
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    interpreter.resize_tensor_input(input_details[0]['index'], encoder_input.shape)
    interpreter.resize_tensor_input(input_details[1]['index'], output.shape)
    interpreter.resize_tensor_input(input_details[3]['index'], enc_padding_mask.shape)
    interpreter.resize_tensor_input(input_details[4]['index'], combined_mask.shape)
    interpreter.resize_tensor_input(input_details[5]['index'], dec_padding_mask.shape)
    interpreter.allocate_tensors()

    # input_details = interpreter.get_input_details()
    # output_details = interpreter.get_output_details()
    interpreter.set_tensor(input_details[0]['index'], tf.cast(encoder_input, tf.float32))
    interpreter.set_tensor(input_details[1]['index'], tf.cast(output, tf.float32))
    interpreter.set_tensor(input_details[2]['index'], [False])
    interpreter.set_tensor(input_details[3]['index'], enc_padding_mask)
    interpreter.set_tensor(input_details[4]['index'], combined_mask)
    interpreter.set_tensor(input_details[5]['index'], dec_padding_mask)
    interpreter.invoke()

    # print(""Inference worked!"")
    

    # [print(d) for d in output_details]

    # The function `get_tensor()` returns a copy of the tensor data.
    # Use `tensor()` in order to get a pointer to the tensor.
    predictions = interpreter.get_tensor(output_details[0]['index'])
    attention_weights = interpreter.get_tensor(output_details[1]['index'])
    

    
    # select the last word from the seq_len dimension
    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)

    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)
    
    # return the result if the predicted_id is equal to the end token
    if predicted_id == tokenizer_en.vocab_size+1:
      return tf.squeeze(output, axis=0), attention_weights
    
    # concatentate the predicted_id to the output which is given to the decoder
    # as its input.
    output = tf.concat([output, predicted_id], axis=-1)

  return tf.squeeze(output, axis=0), attention_weights


def translate_tflite(sentence, tokenizer_pt,tokenizer_en, interpreter, max_length, plot=False):
  result, attention_weights = evaluate_tflite(sentence, tokenizer_pt,tokenizer_en, interpreter, max_length)
  
  predicted_sentence = tokenizer_en.decode([i for i in result 
                                            if i < tokenizer_en.vocab_size])  

  print('Input: {}'.format(sentence))
  print('Predicted translation: {}'.format(predicted_sentence))
  
  if plot:
    plot_attention_weights(attention_weights, sentence, result, plot)
  
  return predicted_sentence


interpreter = tf.lite.Interpreter(model_path=tflite_file)
translate_tflite(""este  o primeiro livro que eu fiz"",tokenizer_pt,tokenizer_en,interpreter,MAX_LENGTH)
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

When I remove Dropout layers from, the interpreter works well and loads and interprets the tflite model well.
Do you have any idea how I can incorporate Dropout?"
44231,Internal: No unary variant unary_op function found for unary variant op enum: 1 Variant type_name: RaggedTensorVariant for device type: GPU,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Linux Ubuntu 20.04 Focal
- TensorFlow installed from (source or binary): Source as also used in nightly build from 20201021; also tested the official tf-nightly-gpu.dev20201021 build installed with pip
- TensorFlow version (use command below): 2.4.0 (pre-release due to source build)
- Python version: 3.8.5
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: 11 / 8
- GPU model and memory: RTX 2080, 8 GB VRAM


**Describe the current behavior**

Whenever `RaggedTensor` objects are returned from a nested `tf.map_fn` call, Tensorflow is unable to find a GPU implementation or some ""unary variant unary_op function"" by given the following `InternalError`:

```
2020-10-22 12:00:57.819438: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at constant_op.cc:243 : Internal: No unary variant unary_op function found for unary variant op enum: 1 Variant type_name: RaggedTensorVariant for device type: GPU
Traceback (most recent call last):
  File ""/home/julilien/Programming/tf_gpu_issue.py"", line 23, in <module>
    optimizer.minimize(f, [x])
  File ""/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 496, in minimize
    grads_and_vars = self._compute_gradients(
  File ""/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py"", line 539, in _compute_gradients
    loss = loss()
  File ""/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 894, in _call
    return self._concrete_stateful_fn._call_flat(
  File ""/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1933, in _call_flat
    forward_backward.record(flat_outputs)
  File ""/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1458, in record
    self._functions.record(
  File ""/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1302, in record
    backward_function, to_record = self._wrap_backward_function(
  File ""/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1250, in _wrap_backward_function
    variant_zeros_like[output_index] = default_gradient.zeros_like(output)
  File ""/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/ops/default_gradient.py"", line 57, in zeros_like
    return array_ops.zeros_like(t)
  File ""/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py"", line 2921, in zeros_like
    return zeros_like_impl(tensor, dtype, name, optimize)
  File ""/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py"", line 2819, in wrapped
    tensor = fun(*args, **kwargs)
  File ""/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py"", line 2982, in zeros_like_impl
    return gen_array_ops.zeros_like(tensor, name=name)
  File ""/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 12442, in zeros_like
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: No unary variant unary_op function found for unary variant op enum: 1 Variant type_name: RaggedTensorVariant for device type: GPU [Op:ZerosLike]
```

On CPU, however, it works without any issue, e.g., when I explicitly disable the GPU support (by setting the `CUDA_VISIBLE_DEVICES` to `-1`). I've also observed the same behavior in a ""conventional"" training setting within Keras training code. According to [this](https://github.com/tensorflow/tensorflow/issues/42047#issuecomment-691208579) comment, I would have expected this error not being an issue due to the CUDA version that I've used.

**Describe the expected behavior**

I would expect to be able to run this code on both CPU and GPU providing the same results.

**Standalone code to reproduce the issue**

I was able to (successfully) run this code on my CPU but not on a GPU (Nvidia RTX 2080, Driver Version: 455.23.05, CUDA Version: 11 with cuDNN 8).

```
import tensorflow as tf

if __name__ == '__main__':
    x = tf.Variable([1., 2., 3.], name='x', trainable=True, dtype=tf.float32)


    @tf.function
    def f():
        seg_data = tf.constant([[0, 0, 1], [0, 1, 2]])

        def seg_batch_fn(loc_segs):
            return tf.math.unsorted_segment_prod(x, loc_segs, tf.reduce_max(loc_segs) + 1)

        return tf.reduce_sum(tf.map_fn(seg_batch_fn, seg_data,
                                       fn_output_signature=tf.RaggedTensorSpec(shape=[None],
                                                                               dtype=tf.float32), name=""seg_batch_fn""))


    optimizer = tf.optimizers.SGD()
    optimizer.minimize(f, [x])

```

**Other info / logs**
```
2020-10-22 11:29:44.986842: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2020-10-22 11:29:46.802265: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-10-22 11:29:46.802836: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2020-10-22 11:29:46.858113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-22 11:29:46.858476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 7.77GiB deviceMemoryBandwidth: 417.23GiB/s
2020-10-22 11:29:46.858494: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2020-10-22 11:29:46.860514: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2020-10-22 11:29:46.860564: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2020-10-22 11:29:46.861269: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2020-10-22 11:29:46.861446: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2020-10-22 11:29:46.863415: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2020-10-22 11:29:46.863890: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2020-10-22 11:29:46.863996: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2020-10-22 11:29:46.864087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-22 11:29:46.864466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-22 11:29:46.865040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-10-22 11:29:46.865065: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2020-10-22 11:29:47.286906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-22 11:29:47.286929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2020-10-22 11:29:47.286934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2020-10-22 11:29:47.287113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-22 11:29:47.287681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-22 11:29:47.287996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-22 11:29:47.288286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6942 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-10-22 11:29:50.715204: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2020-10-22 11:29:50.737811: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3600000000 Hz
Epoch 1/40
2020-10-22 11:30:03.130840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2020-10-22 11:30:04.518337: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2020-10-22 11:30:04.822802: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2020-10-22 11:30:17.122236: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at constant_op.cc:243 : Internal: No unary variant unary_op function found for unary variant op enum: 1 Variant type_name: RaggedTensorVariant for device type: GPU
```
"
44230,AttributeError: 'Tensor' object has no attribute 'numpy',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.0
- Python version:  3.7.x


**Describe the current behavior**
 got errors

**Describe the expected behavior**
print out the tensor value

**Standalone code to reproduce the issue**
```
import tensorflow as tf

def f(x, y):
  output = 1.0
  for i in range(y):
    if i > 1 and i < 5:
      output = tf.multiply(output, x)
  return output

def grad(x, y):
  with tf.GradientTape() as t:
    t.watch(x)
    out = f(x, y)
  return t.gradient(out, x)

x = tf.convert_to_tensor(2.0)
print(""result: "", grad(x, 6).numpy())
```

**Other info / logs**
```
Traceback (most recent call last):
  File ""test_tf.py"", line 17, in <module>
    print(""result: "", grad(x, 6).numpy())
AttributeError: 'Tensor' object has no attribute 'numpy'
```"
44229,Inlcude Ubuntu 20.04 in GPU Support Linux Setup,"## URL(s) with the issue:
[GPU Setup -- Linux setup](https://www.tensorflow.org/install/gpu#linux_setup)

## Description of issue (what needs changing):

### Clear description

The documentation for adding GPU support has descriptions for ubuntu 18.04 and 16.04 **but not ubuntu 20.04**. It's been a while since ubuntu 20.04 has been released and I think adding the instructions for ubuntu 20.04 will be helpful. I followed the article [Installing TensorFlow GPU in Ubuntu 20.04](https://towardsdatascience.com/installing-tensorflow-gpu-in-ubuntu-20-04-4ee3ca4cb75d) to set up GPU support on my system. Adding something along the lines would be great.

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue?

I'm willing to work on this."
44228,build tensorflow lite failed ,"Hi tensorflow lite,
    I use arm-a7 to run TFlite , but first build TFlite failed ,and log as attachment . I have build libtensorflow-lite.a ,but I did confirm it OK, because there are some error , so pls check  whether it is ok? thank you very much!


mike@ubuntu:~/working/tensorflow$ ./tensorflow/lite/tools/make/build_bbb_lib.sh
+ set -e
+++ dirname ./tensorflow/lite/tools/make/build_bbb_lib.sh
++ cd ./tensorflow/lite/tools/make
++ pwd
+ SCRIPT_DIR=/home/mike/working/tensorflow/tensorflow/lite/tools/make
+ cd /home/mike/working/tensorflow/tensorflow/lite/tools/make/../../../..
+ CC_PREFIX=/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/arm-buildroot-linux-gnueabihf-
+ make -j 4 -f tensorflow/lite/tools/make/Makefile TARGET=bbb TARGET_ARCH=armv7l
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/arm-buildroot-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/ruy -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include \
-o /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/obj/tensorflow/lite/tools/benchmark/benchmark_main.o \
   -Wl,--whole-archive /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a -Wl,--no-whole-archive -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lstdc++ -lpthread -lm -ldl -lrt 
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/arm-buildroot-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/ruy -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include \
-o /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/obj/tensorflow/lite/tools/benchmark/benchmark_tflite_performance_options_main.o \
 /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lstdc++ -lpthread -lm -ldl -lrt 
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/arm-buildroot-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/ruy -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include \
-o /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/obj/tensorflow/lite/examples/minimal/minimal.o \
 /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lstdc++ -lpthread -lm -ldl -lrt 
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(interpreter.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(interpreter.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(interpreter_builder.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(interpreter_builder.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(allocation.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(allocation.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(common.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(common.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(error_reporter.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(error_reporter.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(flatbuffer_conversions.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(flatbuffer_conversions.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(op_resolver.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(op_resolver.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(subgraph.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(subgraph.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(external_cpu_backend_context.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(external_cpu_backend_context.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(graph_info.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(graph_info.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(add_n.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(add_n.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(arena_planner.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(arena_planner.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(tensor_utils.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(tensor_utils.o)
collect2: error: ld returned 1 exit status
tensorflow/lite/tools/make/Makefile:359: recipe for target '/home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal' failed
make: *** [/home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal] Error 1
make: *** Waiting for unfinished jobs....
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(external_cpu_backend_context.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(external_cpu_backend_context.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(interpreter.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(interpreter.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(interpreter_builder.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(interpreter_builder.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(allocation.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(allocation.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(common.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(common.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(error_reporter.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(error_reporter.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(flatbuffer_conversions.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(flatbuffer_conversions.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(op_resolver.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(op_resolver.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(subgraph.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(subgraph.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(graph_info.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(graph_info.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(add_n.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(add_n.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(arena_planner.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(arena_planner.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(tensor_utils.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(tensor_utils.o)
collect2: error: ld returned 1 exit status
tensorflow/lite/tools/make/Makefile:394: recipe for target '/home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options' failed
make: *** [/home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options] Error 1
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(allocation.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(allocation.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(arena_planner.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(arena_planner.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(c_api.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(c_api.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(c_api_experimental.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(c_api_experimental.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(common.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(common.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(error_reporter.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(error_reporter.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(flatbuffer_conversions.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(flatbuffer_conversions.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(op_resolver.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(op_resolver.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(tensor_utils.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(tensor_utils.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(subgraph.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(subgraph.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(resource_variable.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(resource_variable.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(static_hashtable.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(static_hashtable.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(external_cpu_backend_context.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(external_cpu_backend_context.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(graph_info.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(graph_info.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(interpreter.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(interpreter.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(interpreter_builder.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(interpreter_builder.o)
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(add_n.o) does not
/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(add_n.o)
collect2: error: ld returned 1 exit status
tensorflow/lite/tools/make/Makefile:388: recipe for target '/home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model' failed
make: *** [/home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model] Error 1
mike@ubuntu:~/working/tensorflow$ "
44226,linux go1.12.2 tensorflow 2.3.1 installation error,"I am facing this issue with TensorFlow 2.3.1 installation in golang go1.12.2 in ubuntu 20.04 with go mod.

Error:
```
go: downloading google.golang.org/protobuf v1.23.0
go: extracting google.golang.org/protobuf v1.23.0
go build github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto: no Go files in 
```

Full error:
```
$ go get github.com/tensorflow/tensorflow/tensorflow/go


go: finding github.com/tensorflow/tensorflow/tensorflow/go latest
go: finding github.com/tensorflow/tensorflow/tensorflow latest
go: finding github.com/tensorflow/tensorflow v2.3.1+incompatible
go: downloading github.com/tensorflow/tensorflow v2.3.1+incompatible
go: extracting github.com/tensorflow/tensorflow v2.3.1+incompatible
go: finding github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto latest
go: finding github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf latest
go: finding github.com/tensorflow/tensorflow/tensorflow/go/core latest
go: finding github.com/golang/protobuf/proto latest
go: finding github.com/golang/protobuf v1.4.3
go: downloading github.com/golang/protobuf v1.4.3
go: extracting github.com/golang/protobuf v1.4.3
go: finding github.com/google/go-cmp v0.4.0
go: finding google.golang.org/protobuf v1.23.0
go: finding github.com/golang/protobuf v1.4.0
go: finding golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543
go: finding google.golang.org/protobuf v1.21.0
go: finding github.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0
go: finding google.golang.org/protobuf v1.20.1-0.20200309200217-e05f789c0967
go: finding github.com/golang/protobuf v1.4.0-rc.2
go: finding google.golang.org/protobuf v0.0.0-20200228230310-ab0ca4ff8a60
go: finding github.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208
go: finding google.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64
go: finding github.com/google/go-cmp v0.3.1
go: finding github.com/golang/protobuf v1.4.0-rc.1
go: finding google.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd
go: finding github.com/google/go-cmp v0.3.0
go: downloading google.golang.org/protobuf v1.23.0
go: extracting google.golang.org/protobuf v1.23.0
go build github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto: no Go files in 
```"
44225,"[API] Tensorflow.Tensor.Tensor(byte[ ][ ], long[ ]) constructor not exist any more","We used to use version ""1.14.0"" of package ""SciSharp.TensorFlow.Redist"" and recently we upgrade to version ""2.3.1"" and seems the constructor for Tensor take byte[][] not exists any more, so I'm wondering is there any replacement for this API?
"
44224,"does ""shared_embedding_columns_v2"" api need check executing_eagerly? ","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Does ""shared_embedding_columns_v2"" api need check executing_eagerly? 

```
@tf_export('feature_column.shared_embeddings', v1=[])
def shared_embedding_columns_v2(categorical_columns,
                                dimension,
                                combiner='mean',
                                initializer=None,
                                shared_embedding_collection_name=None,
                                ckpt_to_load_from=None,
                                tensor_name_in_ckpt=None,
                                max_norm=None,
                                trainable=True,
                                use_safe_embedding_lookup=True):
  **if context.executing_eagerly():
    raise RuntimeError('shared_embedding_columns are not supported when eager '
                       'execution is enabled.')**
```

error:
```
  File ""~/models.py"", line 92, in build_feature_columns
    group_embedding = tf.feature_column.shared_embeddings(group_encode, dimension=32)
  File ""~/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py"", line 902, in shared_embedding_columns_v2
    raise RuntimeError('shared_embedding_columns are not supported when eager '
RuntimeError: shared_embedding_columns are not supported when eager execution is enabled.
```
**Describe the expected behavior**

Just comment the check code, seems all things normal. 


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44219,How to calculate mixed partial derivatives w.r.t inputs?,"Assume I have the following neural network with two inputs:

`(x, t) ---> [neural network] ---> u(x,t)`

Is it possible to calculate the mixed partials i.e.: `d/dx[du(x,t)/dt]`?"
44218,ModelCheckpoint() verbose changed?,"- TensorFlow version (use command below): 2.3.0
- Python version: 3.7.6

**Describe the current behavior**
When using tf.keras.callbacks.ModelCheckpoint() with verbose = 1 the callback behaves weirdly.
If I monitor the validation metric, during epochs where the checkpoint is triggered, the validation metric is not shown in the training output. The callback output reads fewer information than it previously did, making it harder to understand if it's working as intended. For example, it currently says something like:
```
14/14 [==============================] - ETA: 0s - loss: 0.0253 - acc: 0.9929
Epoch 00001: saving model to model.h5
```

**Describe the expected behavior**
Checking some older code, the output used to be something like:

```
14/14 [==============================] - ETA: 0s - loss: 0.0253 - acc: 0.9929 - val_loss: 0.1335 - val_acc: 0.93469
Epoch 00001: val_acc improved from -inf to 0.93469, saving model to model.h5
```
This output makes it clearer the checkpoint is monitoring the correct quantity and when the changes are happening.

I just want to be double sure that the current ModelCheckpoint() is doing the expected thing, even if the output is more minimal?

Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

The following code should be copy-paste-able and should reproduce the problem:

```
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow as tf

# Model / data parameters
num_classes = 10
input_shape = (28, 28, 1)

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Scale images to the [0, 1] range
x_train = x_train.astype(""float32"") / 255
x_test = x_test.astype(""float32"") / 255
# Make sure images have shape (28, 28, 1)
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
print(""x_train shape:"", x_train.shape)
print(x_train.shape[0], ""train samples"")
print(x_test.shape[0], ""test samples"")


# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(64, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation=""softmax""),
    ]
)

model.summary()

my_callbacks = [
    tf.keras.callbacks.ModelCheckpoint(filepath='model.h5', monitor = 'val_categorical_accuracy', verbose =2),
]

batch_size = 128
epochs = 5

model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""categorical_accuracy""])

model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks = my_callbacks)
```
"
44217,inference time improvement using pb model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): pip3 install tensorflow-gpu==2.3.0rc0
- TensorFlow version (use command below): tensorflow-gpu==2.3.0rc0
- Python version: 3.8.6
- CUDA/cuDNN version: 10.1
- GPU model and memory: nvidia 2080ti

**Describe the current behavior**

**method 1) inference with model define and load weights**
_model = tf.keras.Model(input_layer, bbox_tensors)
utils.load_weights(model, FLAGS.weights)
pred_bbox = model.predict(image_data)_

**method 2) inference with pb model**
_saved_model_loaded = tf.saved_model.load(path_to_weight, tags=[tag_constants.SERVING])
infer = saved_model_loaded.signatures['serving_default']
batch_data = tf.constant(image_data)
pred_bbox = infer(batch_data)_

**behavior**
Inference with method2 is much faster than that of method1. The inference time was reduced from 100 ms to 50 ms when an image was analyzed. And I observed that the metric, GPU-Util, is increased from 35% to 47%.

In both inference methods, all settings including batch size is identical except the model loading part - Image is analyzed one by one. And model is uploaded or defined only once at the beginning before reading images. 

**Describe the expected behavior**
I want to know why inference with pb model increased the inference time performance."
44216,A description problem in tf.keras.layers.MaxPooling2D,"## URL(s) with the issue:
https://keras.io/api/layers/pooling_layers/max_pooling2d/


## Description of issue (what needs changing):

At the end of second paragraph, you give a fomula to compute the size of output without padding: 'output_shape = (input_shape - pool_size + 1) / strides)'. However, there is something wrong with it. The correct one should be 'output_shape = (input_shape - pool_size) / strides + 1'. 
Thank you!"
44215,TFLite model weight shape is different from original model,"I use TF2.3.0 and try two different way. One is python API and the other is command line `tflite_convert`.
**Python API**
```
import os
import sys
import absl.logging as logging
import numpy as np
import tensorflow.compat.v1 as tf

np.set_printoptions(threshold=sys.maxsize)
tf.reset_default_graph()
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)
tf.keras.backend.set_session(sess)

converter = tf.lite.TFLiteConverter.from_saved_model('best_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
converter.allow_custom_ops = True
quant_best_model = converter.convert()
with open('quant_best_model.tflite', 'wb') as w:
  w.write(quant_best_model)
```

**Command line**
```
tflite_convert \
--output_file 'tflite_convert.tflite' \
--saved_model_dir 'best_model'
```

[best_weights.zip](https://github.com/tensorflow/tensorflow/files/5418846/best_weights.zip)
[best_model.zip](https://github.com/tensorflow/tensorflow/files/5418507/best_model.zip)

Open `best_weights` and `quant_best_model.tflite` or `tflite_convert.tflite` in [netron](https://lutzroeder.github.io/netron/) and inspect the weight shape. Actually `quant_best_model.tflite` and `tflite_convert.tflite` should be same.

last conv2D layer in tflite model (the dtype is float32 because the tensor is too small to be quantized, no problem here)
![image](https://user-images.githubusercontent.com/59950011/96791211-9c655280-142a-11eb-9113-a1baa0e9ea3d.png)

last conv2D layer in original model
![image](https://user-images.githubusercontent.com/59950011/96791262-b2731300-142a-11eb-93da-fa315d56eb3c.png)

Actually it's the transpose version if you look into how weights distributed.

The tflite model just fail like the model that generate random output in my case.
Any advice? Thanks! "
44214,tensor.numpy() from GPU model prediction very slow,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux (Colab)
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.9
- CUDA/cuDNN version: Colab
- GPU model and memory: Colab has k80 I think

**Describe the current behavior**
I'm running inference with batch_size=1, calling a `tf.keras.Model` model, and when trying to convert the predictions to numpy array (moving the tensor from GPU to CPU), the inference time drops significantly. In the example given, I am just running a CNN with 10 conv2d layers. When running `.numpy()`, the inference time increases more than 2x. In the case mentioned the prediction has the same shape as the input, a tensor of shape (1,512,512,3).

**Describe the expected behavior**
One would expect that copying just an image from the GPU won't take that much. Running the same network with `pytorch` confirms that moving the tensor to the CPU can be done way faster.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
import time
def get_model(depth):
  model = tf.keras.Sequential(
      [
       tf.keras.layers.Conv2D(64,3,padding=""same"",input_shape=(512,512,3), name=""input"")]+[
       tf.keras.layers.Conv2D(64,3,padding=""same"") if i < depth -1 else tf.keras.layers.Conv2D(64,3,padding=""same"", name=""output"")
       for i in range(depth)
       ],
  )
  return model

model = get_model(10)
model_fn = tf.function(model)
image = tf.random.normal((1, 512,512,3))
# warm up
for i in range(50):
    model_fn(image)
iters = 100
images = [tf.random.normal((1, 512,512,3)) for i in range(iters)]

init = time.time()
for image in images:
    x = model_fn(image)
end = time.time() - init
print(f""FPS {1/(end/iters)}"") # FPS 28.559242310262988
print(f""Time {end/iters}"") # Time 0.03501493453979492

init = time.time()
for image in images:
    x = model_fn(image).numpy()
end = time.time() - init
print(f""FPS {1/(end/iters)}"") # FPS 13.259242909743344
print(f""Time {end/iters}"") # Time 0.07541908740997315
```
The colab is also here https://colab.research.google.com/gist/charlielito/c8a301da67b5bf1cf68a4da4afc24d8e/tf-numpy-issue.ipynb


"
44213,tf.contrib.layers.Conv2D and MaxPool2D in tf.keras.layers,"I am trying to convert the following lines of code to TensorFlow 2.0. These are written in tf.contrib.layers and now I need to put them in tf.keras.layers. 

```python
net1_h = layers.conv2d(input, 32, 3, stride=1, scope=""Enc_conv_1"") 
net2 = layers.max_pool2d(input, kernel_size=2, stride=2, scope=""Enc_maxpool_1"")
```
The documentation for tf.keras.layers.Conv2D under TF2 states that input is not listed for Conv2D and MaxPool2D does not have input or kernel_size.

Any suggestions?
"
44212,Compiling tensorflow-lib for jetson nano not working.,"I am trying to build tensorflow-lib.so for jetson nano
https://developer.nvidia.com/embedded/jetson-nano-developer-kit

This issue is a continuation on https://github.com/tensorflow/tensorflow/issues/44198

So I am trying to build the tensorflow library to use with the c api on jetson nano, I have a project that uses tensorflow r1.14 and I assume that I will need a r1.14 library with that, I can migrate to r1.15 if that would be easier since the r1.15 build gets further and from the issue #34429 it seems as if the compilation issue I am getting on r1.14 will not get fixed. 

Jetson nano comes with aarch64, ubuntu 18.04, cuda 10.2 and cudnn 8.


**Describe the problem**

bazel build fails on tensorflow r1.15 with:

```bash
INFO: From Compiling external/snappy/snappy.cc [for host]:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
ERROR: /home/jonasrsv/.cache/bazel/_bazel_jonasrsv/71718566c3609207e5a44599481f772c/external/nccl_archive/BUILD.bazel:72:1: undeclared inclusion(s) in rule '@nccl_archive//:nccl':
this rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/channel.cc':
  'bazel-out/aarch64-opt/bin/external/nccl_archive/nccl.h'
Target //tensorflow/tools/lib_package:libtensorflow failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1299.618s, Critical Path: 130.78s
INFO: 1525 processes: 1525 local.
FAILED: Build did NOT complete successfully
```

bazel build fails on r1.14 with: 

```bash
INFO: From Compiling external/highwayhash/highwayhash/sip_hash.cc:
In file included from external/highwayhash/highwayhash/arch_specific.h:39:0,
                 from external/highwayhash/highwayhash/sip_hash.h:23,
                 from external/highwayhash/highwayhash/sip_hash.cc:15:
external/highwayhash/highwayhash/state_helpers.h: In function 'void highwayhash::PaddedUpdate(highwayhash::HH_U64, const char*, highwayhash::HH_U64, State*)':
external/highwayhash/highwayhash/compiler_specific.h:52:46: warning: requested alignment 32 is larger than 16 [-Wattributes]
 #define HH_ALIGNAS(multiple) alignas(multiple)  // C++11
                                              ^
external/highwayhash/highwayhash/state_helpers.h:49:41: note: in expansion of macro 'HH_ALIGNAS'
   char final_packet[State::kPacketSize] HH_ALIGNAS(32) = {0};
                                         ^~~~~~~~~~
ERROR: /home/jonasrsv/.cache/bazel/_bazel_jonasrsv/71718566c3609207e5a44599481f772c/external/nccl_archive/BUILD.bazel:67:1: fatbinary external/nccl_archive/device_dlink_hdrs.fatbin failed (Exit 1)
fatbinary fatal   : Unknown option '-bin2c-path'
Target //tensorflow/tools/lib_package:libtensorflow failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1732.677s, Critical Path: 143.12s
INFO: 2101 processes: 2101 local.
FAILED: Build did NOT complete successfully
```
This error seems to be related to #34429

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```bash
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout r1.14
python3 configure.py

# Click yes on tensorRT and cudaNN

bazel build --config opt //tensorflow/tools/lib_package:libtensorflow
```
"
44210,Bug in loading keras model with custom losses (metrics),"**System information**
- Have I written custom code: yes, class for custom loss `tf.keras` 
- OS Platform and Distribution Linux Ubuntu 20.04:
- TensorFlow installed from binary
- TensorFlow version 2.1.0 --> 2.3.0 (it seems that the bug is present until master branch):
- Python version: 3.7.6

**Describe the current behavior**

(see code below)
1. Define a class for custom loss in keras named `BatchMeanSquaredError`
2. Create a model using `tf.keras.Sequential`
3. Train the model on random data
4. Save the model with `tf.keras.models.save_model`
5. Load the model using `tf.keras.models.load_model('model.h5', custom_objects={'BatchMeanSquaredError': BatchMeanSquaredError})`

The loading function raises the following error 

    ValueError: Unknown loss function: BatchMeanSquaredError


**Describe the expected behavior**
I think that the bug is caused by two little issues:

***1***
In `tensorflow_core/python/keras/losses.py` the function `get(identifier)` misses the parameter `custom_objects`. The same parameter should be added when calling `deserialize`. 
Now it is:

    def get(identifier):
      [...]
      if isinstance(identifier, six.string_types):
        identifier = str(identifier)
        return deserialize(identifier)
      if isinstance(identifier, dict):
        return deserialize(identifier)
      [...]


I think it should be:

    def get(identifier, custom_objects=None):
      [...]
      if isinstance(identifier, six.string_types):
        identifier = str(identifier)
        return deserialize(identifier, custom_objects=custom_objects)
      if isinstance(identifier, dict):
        return deserialize(identifier, custom_objects=custom_objects)
      [...]


***2***
In `tensorflow_core/python/keras/saving/saving_utils.py` the function `compile_args_from_training_config(training_config, custom_objects=None)` has the parameter `custom_objects` but the same parameter is not present when calling `losses.get`:

    if isinstance(loss_config, dict) and 'class_name' in loss_config:
        loss_config = losses.get(loss_config)

I think it should be:

    if isinstance(loss_config, dict) and 'class_name' in loss_config:
        loss_config = losses.get(loss_config, custom_objects=custom_objects)

***Overall***
The problem may be caused by the coexistence of the possibility to define `tf.keras` custom losses, or custom metrics, both using functions or classes. I think it should be allowed to use only classes to have a more general behavior.
 

**Standalone code to reproduce the issue**

    from tensorflow.keras.losses import Loss
    import tensorflow as tf
    import tensorflow.keras.backend as K
    import numpy as np
    
    
    class BatchMeanSquaredError(Loss):
    
        def __init__(self, reduction='auto', name='batch_mean_squared_error'):
            super().__init__(reduction=reduction, name=name)
    
        def call(self, y_true, y_pred):
            y_pred = tf.convert_to_tensor(y_pred)
            y_true = tf.cast(y_true, y_pred.dtype)
            L = K.mean((y_pred - y_true) ** 2, axis=0)
            return L
    
    X = np.random.random((1000, 3))
    y = np.ones(shape=(1000, 3))

    model = tf.keras.Sequential(
        [
            tf.keras.layers.Dense(3, activation='relu'),
            tf.keras.layers.Dense(3, activation='relu'),
            tf.keras.layers.Dense(3)
        ]
    )
    
    bmse = BatchMeanSquaredError()
    model.compile(loss=bmse, optimizer='sgd')

    model.fit(X, y, batch_size=10, epochs=5)

    tf.keras.models.save_model(model=model, filepath='model.h5')

    custom_objects = {'BatchMeanSquaredError': BatchMeanSquaredError}
    tf.keras.models.load_model('model.h5', custom_objects=custom_objects)"
44209,Cannot access a resource variable from a custom op on Mac and Windows,"I am facing an issue with accessing a resource variable from a custom op on Mac and on Windows (it works fine on Linux though). When running the attached reproducer on Mac, I get the following error:

```
Trying to access resource using the wrong type. Expected N10tensorflow3VarE got N10tensorflow3VarE [Op:CustomOp]
```

On Windows the error is similar:

```
Trying to access resource using the wrong type. Expected class tensorflow::Var got class tensorflow::Var [Op:CustomOp]
```

On Linux there is no error. The op works as expected and the attached test prints this output:

```
<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=([1., 2., 3.], dtype=float32)>
<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=([2.22, 2.22, 2.22], dtype=float32)>
```

I think this issue is due to the use of an address of a static variable ```hash_bit``` in ```TypeIndex::Make<T>``` as a hash. When the shared library with the custom op is compiled, it will have its own copy of ```TypeIndex::Make<Var>::hash_bit```, so the hash codes for the same type (e.g. Var) will be different. I suspect this still works on Linux because a STB_GNU_UNIQUE symbol is generated for ```hash_bit```, so that there is only one definition of it (per type) in the whole process. Mac and Windows don't have this feature though.

Is it possible to rewrite TypeIndex in a way that will make it work across shared libraries on all platforms? Or perhaps various instantiations of ```TypeIndex::Make<T>``` could be exported from tensorflow framework library?

A standalone reproducer along with build scripts is attached.
[custom_op.zip](https://github.com/tensorflow/tensorflow/files/5417289/custom_op.zip)

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows, Mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary (pypi)
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:"
44208,Docker build tensorflow cannot detect gpu,"Hello:

I used docker to install tensorflow but GPU cannot be detected

**System information**
- OS Platform and Distribution: Ubuntu 20.04.1 LTS
- TensorFlow installed from (source or binary): docker
- Docker Image: 2.2.1-gpu-py3-jupyter
- Python version: Python 3.8.5
- GPU: GTX1650



**Describe the problem**

After running the command: **sudo docker run --gpus all -it --rm -p 8888:8888 tensorflow/tensorflow:2.2.1-gpu-py3-jupyter**, I use the jupyter on the browser to write python codes. But I find that gpu cannot be detected by tensorflow.

```import tensorflow as tf
import numpy as np
from tensorflow.python.client import device_lib

print(tf.__version__)
print(device_lib.list_local_devices())
```

result:

```
2.2.1
[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 16222889058350606218
, name: ""/device:XLA_CPU:0""
device_type: ""XLA_CPU""
memory_limit: 17179869184
locality {
}
incarnation: 1739177089072959980
physical_device_desc: ""device: XLA_CPU device""
]
```

There are no GPU.

**Any other info / logs**
Here is results of **sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi**:


+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 1650    Off  | 00000000:01:00.0  On |                  N/A |
| N/A   44C    P8     3W /  N/A |    281MiB /  3911MiB |      1%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+

"
44207,Unable to import tensorflow,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):OS X
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):pip install tensorflow
- TensorFlow version:2.0.1
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Unable to import tensorflow

**Provide the exact sequence of commands / steps that you executed before running into the problem**
python -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""

**Any other info / logs:

(base) [deeplearning]$ python -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
Traceback (most recent call last):
  File ""/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: dlopen(/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: dlopen(/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation
  Referenced from: /Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib
  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security
 in /Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
(base) [deeplearning]$ 
"
44206,TensorFlow on Dell XPS 9300,"I recently purchased a new laptop, and after setting up the environment on pycharm and running a test code i get several warning messages:

```
2020-10-21 16:03:00.527974: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Found 24958 images belonging to 2 classes.
Found 2600 images belonging to 2 classes.
2020-10-21 16:03:01.695574: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-10-21 16:03:03.773517: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-10-21 16:03:03.773567: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fabio-XPS): /proc/driver/nvidia/version does not exist
2020-10-21 16:03:03.774077: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-21 16:03:03.802935: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 1497600000 Hz
2020-10-21 16:03:03.803743: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6270580 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-21 16:03:03.803791: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
```
what am i missing?

this is the system configuration
>                           system         XPS 13 9300 (096D)
> /0                        bus            077Y9N
> /0/1                      memory         1MiB BIOS
> /0/400                    processor      Intel(R) Core(TM) i7-1065G7 CPU @ 1.30GHz
> /0/400/701                memory         128KiB L1 cache
> /0/400/702                memory         2MiB L2 cache
> /0/400/703                memory         8MiB L3 cache
> /0/700                    memory         192KiB L1 cache
> /0/1000                   memory         16GiB System Memory
> /0/1000/0                 memory         8GiB Row of chips LPDDR4 Synchronous 4267 MHz (0.2 ns)
> /0/1000/1                 memory         8GiB Row of chips LPDDR4 Synchronous 4267 MHz (0.2 ns)
> /0/100                    bridge         Intel Corporation
> /0/100/2       /dev/fb0   display        Iris Plus Graphics G7
> /0/100/4                  generic        Intel Corporation
> /0/100/7                  bridge         Ice Lake Thunderbolt 3 PCI Express Root Port #0
> /0/100/7.2                bridge         Ice Lake Thunderbolt 3 PCI Express Root Port #2
> /0/100/d                  bus            Ice Lake Thunderbolt 3 USB Controller
> /0/100/d/0     usb1       bus            xHCI Host Controller
> /0/100/d/1     usb2       bus            xHCI Host Controller
> /0/100/d.2                generic        Ice Lake Thunderbolt 3 NHI #0
> /0/100/d.3                generic        Ice Lake Thunderbolt 3 NHI #1
> /0/100/12                 communication  Intel Corporation
> /0/100/14                 bus            Ice Lake-LP USB 3.1 xHCI Host Controller
> /0/100/14/0    usb3       bus            xHCI Host Controller
> /0/100/14/0/4             input          USB Optical Mouse
> /0/100/14/0/5             generic        FingerPrint
> /0/100/14/0/9             multimedia     Integrated_Webcam_HD
> /0/100/14/0/a             communication  Bluetooth wireless interface
> /0/100/14/1    usb4       bus            xHCI Host Controller
> /0/100/14.2               memory         RAM memory
> /0/100/14.3    wlp0s20f3  network        Killer Wi-Fi 6 AX1650i 160MHz Wireless Network Adapter (201NGW)
> /0/100/15                 bus            Ice Lake-LP Serial IO I2C Controller #0
> /0/100/15.1               bus            Ice Lake-LP Serial IO I2C Controller #1
> /0/100/16                 communication  Management Engine Interface
> /0/100/1d                 bridge         Ice Lake-LP PCI Express Root Port #9
> /0/100/1d/0               storage        SK hynix
> /0/100/1d.7               bridge         Intel Corporation
> /0/100/1d.7/0             generic        RTS525A PCI Express Card Reader
> /0/100/1e                 communication  Ice Lake-LP Serial IO UART Controller #0
> /0/100/1f                 bridge         Ice Lake-LP LPC Controller
> /0/100/1f.3               multimedia     Smart Sound Technology Audio Controller
> /0/100/1f.4               bus            Ice Lake-LP SMBus Controller
> /0/100/1f.5               bus            Ice Lake-LP SPI Controller
> /0/0                      system         PnP device PNP0c02
> /0/2                      system         PnP device PNP0b00
> /0/3                      generic        PnP device INT3f0d
> /0/4                      input          PnP device PNP0303
> /0/5                      generic        PnP device DLL096d
> /0/6                      system         PnP device PNP0c02
> /0/7                      system         PnP device PNP0c02
> /0/8                      system         PnP device PNP0c02
> /0/9                      system         PnP device PNP0c02
> /1                        power          DELL 2XXFW05
> /2                        power          To Be Filled by O.E.M.


> device_lib.list_local_devices()
> [name: ""/device:CPU:0""
> device_type: ""CPU""
> memory_limit: 268435456
> locality {
> }
> incarnation: 322266644154681505
> , name: ""/device:XLA_CPU:0""
> device_type: ""XLA_CPU"""
44205,Unable to Convert Faster RCNN to TF Lite model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10
- TensorFlow installed from (source or binary): pip package
- TensorFlow version (or github SHA if from source): 2.3.1


**Command used to run the converter or code if youre using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
converter = tf.lite.TFLiteConverter.from_saved_model(model_path)
tflite_model = converter.convert()
```

**The output from the converter invocation**

```
Traceback (most recent call last):
  File ""C:\Users\shehr\anaconda3\envs\nn_gpu\lib\site-packages\tensorflow\lite\python\convert.py"", line 199, in toco_convert_protos
    enable_mlir_converter)
  File ""C:\Users\shehr\anaconda3\envs\nn_gpu\lib\site-packages\tensorflow\lite\python\wrap_toco.py"", line 38, in wrapped_toco_convert
    enable_mlir_converter)
Exception: <unknown>:0: error: loc(""Func/StatefulPartitionedCall/input/_0""): requires all operands and results to have compatible element types
<unknown>:0: note: loc(""Func/StatefulPartitionedCall/input/_0""): see current operation: %1 = ""tf.Identity""(%arg0) {device = """"} : (tensor<1x?x?x3x!tf.quint8>) -> tensor<1x?x?x3xui8>


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:/TensorFlow/models/to_tflite.py"", line 4, in <module>
    tflite_model = converter.convert()
  File ""C:\Users\shehr\anaconda3\envs\nn_gpu\lib\site-packages\tensorflow\lite\python\lite.py"", line 1076, in convert
    return super(TFLiteConverterV2, self).convert()
  File ""C:\Users\shehr\anaconda3\envs\nn_gpu\lib\site-packages\tensorflow\lite\python\lite.py"", line 900, in convert
    self).convert(graph_def, input_tensors, output_tensors)
  File ""C:\Users\shehr\anaconda3\envs\nn_gpu\lib\site-packages\tensorflow\lite\python\lite.py"", line 633, in convert
    **converter_kwargs)
  File ""C:\Users\shehr\anaconda3\envs\nn_gpu\lib\site-packages\tensorflow\lite\python\convert.py"", line 574, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""C:\Users\shehr\anaconda3\envs\nn_gpu\lib\site-packages\tensorflow\lite\python\convert.py"", line 202, in toco_convert_protos
    raise ConverterError(str(e))
tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(""Func/StatefulPartitionedCall/input/_0""): requires all operands and results to have compatible element types
<unknown>:0: note: loc(""Func/StatefulPartitionedCall/input/_0""): see current operation: %1 = ""tf.Identity""(%arg0) {device = """"} : (tensor<1x?x?x3x!tf.quint8>) -> tensor<1x?x?x3xui8>
```


M trying to convert my custom trained Faster RCNN model to TF Lite model. Does TF Lite supports RCNN models? If not can you guys please add a list to supported model's list to the website so that one easily identify which model to use from the start. Currently I trained a model but I dont have any info which models are supported and which are not. Adding a list of models will save lot of for people.

"
44204,Illegal instruction (core dumped),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint19.03
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.3
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I am getting an error while importing the TensorFlow.
```bash
>>> import tensorflow
Illegal instruction (core dumped)
```
I tried to downgrade the TensorFlow but still it's not working.

**Describe the expected behaviour**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44201,TFLite convert supporting problem for the OP 'tf.image.crop_and_resize' in 2.3.0,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
44200,TENSORFLOW 1.14 STYLEAGN 2 PERFORMANCE ISSUE ON RTX 3090 MULTIPLE GPU,"I am running Stylegan 2 model on 4x RTX 3090 and it is taking a long time **to start up the training** than as in 1x RTX 3090. Although, as training starts, it gets finished up earlier in 4x than in 1x. I am using CUDA 11.1 and TensorFlow 1.14 in both the GPUs. 

Secondly, When I am using 1x RTX 2080ti, with CUDA 10.2 and TensorFlow 1.14, it is taking less amount **to start the training** as compared to 1x RTX 3090 with 11.1 CUDA and Tensorflow 1.14. Tentatively, it is taking 5 min in 1x RTX 2080ti, 30-35 minutes in 1x RTX 3090, and 1.5 hrs in 4x RTX 3090 **to start the training** for one of the dataset.

I'll be grateful if anyone can help me to resolve this issue.

I am using Ubuntu 16.04, Core i9-10980XE CPU, and 32 GB ram both in 2080ti and 3090 machines."
44199,error occurred when i loaded a model contain TextVectorization layer,"**System information**
- TensorFlow version:  2.3.1


**error message**


**RuntimeError: Unable to restore a layer of class TextVectorization. Layers of class TextVectorization require that the class be provided to the model loading code, either by registering the class using @keras.utils.register_keras_serializable on the class def and including that file in your program, or by passing the class in a keras.utils.CustomObjectScope that wraps this load call.**



**Standalone code to reproduce the issue** 
i built a model like below:
```
model = tf.keras.Sequential([
   tf.keras.Input(shape=(1,), dtype=tf.string),
   vectorize_layer(a TextVectorization layer that has adapt),
   layers.Embedding(max_features + 1, embedding_dim),
   layers.Dropout(0.2),
   layers.GlobalAveragePooling1D(),
   layers.Dropout(0.2),
   layers.Dense(1),
  layers.Activation('sigmoid')
])
```

and i saved model like this:
`model.save('./model/basic-text-class-export', save_format='tf')`

but when i loaded model like below, i got a error as shown before
`model = tf.keras.models.load_model('./model/basic-text-class-export')`

how can i do next?
best wishes for you"
44198,configure.py not working on r1.14 - aarch64 nivida jetson nano,"

**Describe the problem**

I am trying to build tensorflow c-library 1.14 from source on my jetson nano dev kit https://developer.nvidia.com/embedded/jetson-nano-developer-kit. I am running ubuntu 18.04 with aarch64, I have python3.6.9 installed.

I have cloned the tensorflow repo and checked out r1.14 branch, when I run

```bash
python3 configure.py
```

I get the following

```bash

python3 configure.py 
WARNING: ignoring LD_PRELOAD in environment.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.24.1- (@non-git) installed.
Please specify the location of python. [Default is /usr/bin/python3]: 


Found possible Python library paths:
  /usr/lib/python3.6/dist-packages
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.6/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3.6/dist-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: y
TensorRT support will be enabled for TensorFlow.

Traceback (most recent call last):
  File ""configure.py"", line 1571, in <module>
    main()
  File ""configure.py"", line 1448, in main
    if validate_cuda_config(environ_cp):
  File ""configure.py"", line 1340, in validate_cuda_config
    tuple(line.decode('ascii').rstrip().split(': ')) for line in proc.stdout)
ValueError: dictionary update sequence element #9 has length 1; 2 is required

````


**Provide the exact sequence of commands / steps that you executed before running into the problem**

exact sequnce:

```bash
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout r1.14
python3 configure.py
```
"
44197,Cannot convert keras.layers.Embedding,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary (by pip)
- TensorFlow version (or github SHA if from source): 2.3.0

Hi, I found simple model with keras.layers.Embedding cannot be converted to tflite.
I built my model with python API and then tried to convert it using `tflite_convert` CLI
Attached code/command can reproduce this bug.

**Command used to run the converter or code if youre using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```tflite_convert --saved_model_dir=embedding --output_file=embedding.tflite```

**The output from the converter invocation**

```
2020-10-21 06:20:51.501787: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-21 06:20:52.177539: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-10-21 06:20:52.223256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-21 06:20:52.223582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.815GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2020-10-21 06:20:52.223600: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-21 06:20:52.224542: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-21 06:20:52.225557: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-21 06:20:52.225744: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-21 06:20:52.226751: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-21 06:20:52.227345: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-21 06:20:52.229557: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-21 06:20:52.229661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-21 06:20:52.229986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-21 06:20:52.230255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-10-21 06:20:52.230467: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-21 06:20:52.234982: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3699850000 Hz
2020-10-21 06:20:52.235474: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5a1d6c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-21 06:20:52.235487: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-21 06:20:52.337971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-21 06:20:52.338757: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5a404e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-10-21 06:20:52.338789: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2020-10-21 06:20:52.339061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-21 06:20:52.339714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.815GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2020-10-21 06:20:52.339756: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-21 06:20:52.339791: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-21 06:20:52.339817: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-21 06:20:52.339841: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-21 06:20:52.339865: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-21 06:20:52.339888: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-21 06:20:52.339911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-21 06:20:52.340007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-21 06:20:52.340695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-21 06:20:52.341326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-10-21 06:20:52.341370: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-21 06:20:52.622897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-21 06:20:52.622925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-10-21 06:20:52.622930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-10-21 06:20:52.623098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-21 06:20:52.623413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-21 06:20:52.623714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7272 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-10-21 06:20:52.735265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-21 06:20:52.735499: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-10-21 06:20:52.735547: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-10-21 06:20:52.735866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-21 06:20:52.736073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.815GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s
2020-10-21 06:20:52.736097: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-21 06:20:52.736116: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-21 06:20:52.736128: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-21 06:20:52.736139: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-21 06:20:52.736151: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-21 06:20:52.736162: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-21 06:20:52.736174: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-21 06:20:52.736214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-21 06:20:52.736422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-21 06:20:52.736600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-10-21 06:20:52.736619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-21 06:20:52.736625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-10-21 06:20:52.736629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-10-21 06:20:52.736685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-21 06:20:52.736893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-21 06:20:52.737083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7272 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-10-21 06:20:52.742399: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-10-21 06:20:52.742415: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 4 nodes (0), 4 edges (0), time = 0.333ms.
2020-10-21 06:20:52.742420: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 4 nodes (0), 4 edges (0), time = 0.257ms.
2020-10-21 06:20:52.742424: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: __inference__wrapped_model_77
2020-10-21 06:20:52.742428: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-10-21 06:20:52.742432: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
Traceback (most recent call last):
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 496, in _import_graph_def_internal
    results = c_api.TF_GraphImportGraphDefWithResults(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input 1 of node StatefulPartitionedCall was passed float from unknown:0 incompatible with expected resource.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/jhjang/.virtualenvs/ml/bin/tflite_convert"", line 8, in <module>
    sys.exit(main())
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/lite/python/tflite_convert.py"", line 640, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/lite/python/tflite_convert.py"", line 623, in run_main
    _convert_tf2_model(tflite_flags)
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/lite/python/tflite_convert.py"", line 239, in _convert_tf2_model
    tflite_model = converter.convert()
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 1076, in convert
    return super(TFLiteConverterV2, self).convert()
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 877, in convert
    _convert_to_constants.convert_variables_to_constants_v2_as_graph(
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 1108, in convert_variables_to_constants_v2_as_graph
    frozen_func = _construct_concrete_function(func, output_graph_def,
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 999, in _construct_concrete_function
    new_func = wrap_function.function_from_graph_def(output_graph_def,
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 650, in function_from_graph_def
    wrapped_import = wrap_function(_imports_graph_def, [])
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 621, in wrap_function
    func_graph.func_graph_from_py_func(
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 87, in __call__
    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 93, in wrapped
    return fn(*args, **kwargs)
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 648, in _imports_graph_def
    importer.import_graph_def(graph_def, name="""")
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 400, in import_graph_def
    return _import_graph_def_internal(
  File ""/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 501, in _import_graph_def_internal
    raise ValueError(str(e))
ValueError: Input 1 of node StatefulPartitionedCall was passed float from unknown:0 incompatible with expected resource.
```

**Also, please include a link to the saved model or GraphDef**
I built my model 'embedding' with below python code
```
import tensorflow as tf
import tensorflow.keras as keras

model = keras.models.Sequential([
    keras.layers.Embedding(1000, 64, name=""embedding""),
    keras.layers.GlobalAveragePooling1D()
])

model.save('embedding')
```

**Failure details**
None

**Any other info / logs**
None
"
44195,Random ops the same for different iterations over mapped datasets,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (copy below + colab link)
- OS Platform and Distribution Linux Ubuntu 18.04:
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 2.3 / tf-nightly
- Python version: 3.7.7

**Describe the current behavior**
Random ops in `tf.data.Dataset.map`ped datasets produce the same values for repeated iterations over the same dataset (though not for datasets using `Dataset.repeat`) when `tf.random.set_seed` has been called. This nullifies the effect of any data augmentation when the dataset is looped over and the seed is set.

**Describe the expected behavior**
Looping over a dataset multpile times should have at least qualitatively equivalent behaviour to looping over a `repeat`ed dataset (potentially quantitatively too), and should be qualitatively the same with or without using `tf.random.set_seed`.

**Standalone code to reproduce the issue**
Colab [here](https://colab.research.google.com/drive/1Y9wajzK5TYer_DYOK5l_ySexq_A-5tLd?usp=sharing)

```python
import tensorflow as tf

def get_dataset():
  return tf.data.Dataset.range(2).map(
      lambda x: tf.cast(x, tf.float32) + tf.random.uniform(()))

def print_looped_epochs(num_epochs=2):
  print('Looped epochs:')
  dataset = get_dataset()
  for _ in range(num_epochs):
    print([e.numpy() for e in dataset])

def print_repeated_epochs(num_epochs=2):
  print(""Repeated epochs:"")
  dataset = get_dataset().repeat(num_epochs)
  print([e.numpy() for e in dataset])

print_looped_epochs()
print_repeated_epochs()

tf.random.set_seed(0)
print('Seed set')
print_looped_epochs()
print_repeated_epochs()
```
**Other info / logs**
```txt
Looped epochs:
[0.6892859, 1.171408]
[0.0070821047, 1.629379]
Repeated epochs:
[0.6894361, 1.446699, 0.60574067, 1.9869001]
Seed set
Looped epochs:
[0.019757032, 1.5400312]
[0.019757032, 1.5400312]
Repeated epochs:
[0.019757032, 1.5400312, 0.51667833, 1.4683528]
```"
44194,tf.keras.layers.Embedding forces CPU placement if eager execution is enabled and GPUs are present.,"**System information**
- Have I written custom code: **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Google AI Platform**
- TensorFlow installed from **binary**
- TensorFlow version: reproducible on 2.2 or 2.3
- Python version: 3.7
- CUDA/cuDNN version: Unknown
- GPU model and memory: NVIDIA Tesla K80

**Describe the current behavior**

`tf.keras.layers.Embedding` layers are pinned to the CPU when created in eager execution mode (the default), resulting in significant slowdowns compared with GPU placement. In non-toy examples, this results in a 2-3x slowdown when run on a modest training machine (AI Platform with one NVIDIA Tesla K80).

**Describe the expected behavior**

`tf.keras.layers.Embedding` layers would be placed on the GPU, avoiding unnecessary MemcpyH2D and MemcpyD2H calls (as viewed in the TensorFlow profiler) and avoiding bottlenecking the job on the CPU.

**Standalone code to reproduce the issue**

```python
import tensorflow as tf
import numpy as np

# Uncomment the following line to get proper GPU placement for the Embedding layer
# tf.compat.v1.disable_eager_execution()
tf.debugging.set_log_device_placement(True)

model = tf.keras.Sequential(
    [
        tf.keras.layers.Embedding(10, 1, input_shape=(1,)),
        tf.keras.layers.Dense(1),
    ]
)

model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(),
    metrics=[""accuracy""],
)

# Train on some dummy data
x = np.random.randint(1, 10, (10, 1))
y = np.random.uniform(0, 1, (10, 1))
model.fit(x, y, epochs=12)
```

When run on a machine with a GPU, the above code prints out all op assignments, including:

```
...
""sequential_embedding_embedding_lookup_488: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0""
...
```

...however, when eager execution is disabled entirely, the above code shows that the embedding-related ops are being assigned to the GPU instead:
```
...
embedding/Cast: (Cast): /job:localhost/replica:0/task:0/device:GPU:0
embedding/embedding_lookup: (ResourceGather): /job:localhost/replica:0/task:0/device:GPU:0
embedding/embedding_lookup/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
embedding/embedding_lookup/Identity_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
...
```

This issue seems to be caused by https://github.com/tensorflow/tensorflow/commit/a1b64cf2a6a995ffaaf384cf8643221f1c27db48, landed by @alextp a couple years ago. It seems that the assertion [in the comment of `Embedding#build`](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/layers/embeddings.py#L126-L150) (""Most sparse optimizers do not have GPU kernels defined"") may no longer hold, and that this pinning of the Embedding layer to CPU only is adversely affecting performance when no longer necessary in all cases.

It does seem to be possible to circumvent this issue by subclassing `tf.keras.layers.Embedding` and manually reimplementing the method:

```python
class GPUCompatibleEmbedding(layers.Embedding):
    @tf_utils.shape_type_conversion
    def build(self, input_shape):
        self.embeddings = self.add_weight(
            shape=(self.input_dim, self.output_dim),
            initializer=self.embeddings_initializer,
            name=""embeddings"",
            regularizer=self.embeddings_regularizer,
            constraint=self.embeddings_constraint,
        )
        self.built = True
```

However, this seems like a bit of a hacky workaround, as TensorFlow/Keras should allow Embedding layers to run on GPUs out of the box - as it does seem entirely possible to do so."
44193,exit error when using tftrt in jupyterlab,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu18.04.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0 master
- Python version: 3.8.5
- Bazel version (if compiling from source): 3.7.0
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: 11.1/8.0.4
- GPU model and memory: tesla p100 X 2

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
using jupyterlab
import tensorrt
from tensorflow import saved_model

then import saved tensorrtengine ( built from tftrt )

**Describe the expected behavior**
exit and no errors

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
using tensorflow, from any saved model, import tensorflow tensorrt, convert, build and save
and load from jupyterlab, and then exit jupyterlab


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
[C 13:57:47.406 LabApp] Shutdown confirmed
[I 13:57:47.407 LabApp] Shutting down 2 kernels
[I 13:57:47.608 LabApp] Kernel shutdown: ef64bc65-6695-40bf-ab9b-e2c7ff62e1bd
--- Logging error ---
Traceback (most recent call last):
  File ""/home/alan/anaconda3/lib/python3.8/logging/__init__.py"", line 1084, in emit
    stream.write(msg + self.terminator)
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py"", line 414, in write
    self._schedule_flush()
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py"", line 335, in _schedule_flush
    self.pub_thread.schedule(_schedule_in_thread)
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py"", line 207, in schedule
    f()
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py"", line 334, in _schedule_in_thread
    self._io_loop.call_later(self.flush_interval, self._flush)
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py"", line 602, in call_later
    return self.call_at(self.time() + delay, callback, *args, **kwargs)
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/tornado/platform/asyncio.py"", line 162, in call_at
    return self.asyncio_loop.call_later(
  File ""/home/alan/anaconda3/lib/python3.8/asyncio/base_events.py"", line 687, in call_later
    timer = self.call_at(self.time() + delay, callback, *args,
  File ""/home/alan/anaconda3/lib/python3.8/asyncio/base_events.py"", line 698, in call_at
    self._check_closed()
  File ""/home/alan/anaconda3/lib/python3.8/asyncio/base_events.py"", line 508, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
Call stack:
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py"", line 160, in __del__
    log_fn(""Unresolved object in checkpoint: {}""
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/tensorflow/python/platform/tf_logging.py"", line 178, in warning
    get_logger().warning(msg, *args, **kwargs)
Message: 'Unresolved object in checkpoint: (root).trt_engine_resources.TRTEngineOp_0_0._serialized_trt_resource_filename'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File ""/home/alan/anaconda3/lib/python3.8/logging/__init__.py"", line 1085, in emit
    self.flush()
  File ""/home/alan/anaconda3/lib/python3.8/logging/__init__.py"", line 1065, in flush
    self.stream.flush()
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py"", line 357, in flush
    self._flush()
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py"", line 383, in _flush
    self.session.send(self.pub_thread, u'stream', content=content,
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/jupyter_client/session.py"", line 751, in send
    stream.send_multipart(to_send, copy=copy)
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py"", line 214, in send_multipart
    self.schedule(lambda : self._really_send(*args, **kwargs))
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py"", line 207, in schedule
    f()
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py"", line 214, in <lambda>
    self.schedule(lambda : self._really_send(*args, **kwargs))
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py"", line 222, in _really_send
    self.socket.send_multipart(msg, *args, **kwargs)
AttributeError: 'NoneType' object has no attribute 'send_multipart'
Call stack:
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py"", line 168, in __del__
    log_fn(
  File ""/home/alan/anaconda3/lib/python3.8/site-packages/tensorflow/python/platform/tf_logging.py"", line 178, in warning
    get_logger().warning(msg, *args, **kwargs)
Message: 'A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.'
Arguments: ()
[I 13:57:48.411 LabApp] Kernel shutdown: fd4e7ab1-7f88-485f-9270-296fbbaca6c4
[I 13:57:48.411 LabApp] Shutting down 0 terminals
```"
44192,error when exit kernel,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0 master
- Python version: 3.7.9
- Bazel version (if compiling from source): 3.7.0
- GCC/Compiler version (if compiling from source):  7.5.0
- CUDA/cuDNN version: 11.1 / 8.0.4
- GPU model and memory: tesla P100

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
start python and import tensorflow and exit

**Describe the expected behavior**
no error messages

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
Python 3.7.9 (default, Aug 31 2020, 12:42:55)
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2020-10-21 13:48:47.162423: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
>>>
>>> exit()
Exception ignored in: <function Buckets.__del__ at 0x7f8b017e2830>
Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/monitoring.py"", line 407, in __del__
AttributeError: 'NoneType' object has no attribute 'TFE_MonitoringDeleteBuckets'
```


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44191,mixed_float16 + example code = horrible performance on a GeForce GTX 1660 Ti,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): docker image (binary)
- TensorFlow version (use command below): 2.3.0 (v2.3.0-rc2-23-gb36436b087)
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0
- GPU model and memory: Nvidia GeForce GTX 1660 Ti (mobile)

**Describe the current behavior**
I went to the TensorFlow guide on mixed precision, here:

https://www.tensorflow.org/guide/mixed_precision

... and copied the code/model as best I could. (I will attach my code.)

Inference performance decreases by a factor of ~4 when mixed_float16 is turned on. (GeForce GTX 1660 Ti)

I have tried several different input dimensions and model designs (including the model that I'm actually trying to optimize) and I can't find a combination where mixed_float16 doesn't decrease performance by a factor of 2-4x.

**Describe the expected behavior**
I understand that the 1660 Ti (chip name: TU116 SM) does not have Tensor Cores but it does have support for float16 and Nvidia advertises that its float16 throughput is twice that of float32, so I would expect some speedup from switching to mixed_float16. Instead I'm seeing enormous slowdowns.

**Standalone code to reproduce the issue**
I will attach my minimal-repro code to this issue.

[test.py.txt](https://github.com/tensorflow/tensorflow/files/5413328/test.py.txt)

"
44186,how save the value of a tensorflow operation in an array,"save the value of a tensorflow operation in an array.

i am using tensorflow v2 in jupyter

I have a code that calculates the loss error, I would like to save the value for each time in the gen_loss_epoch array. I try to save the error value with the following code

`gen_loss_epoch[epoch] = tf.print (gen_loss)`

while training the error value is printed eg 0.990000 0.800 0.2000 etc thanks to tf.print (gen_loss)

I thought that if I did ggg [time] = tf.print (gen_loss) it would save those loss values in the gen_loss_epoch array.

```
gen_loss_epoch=np.linspace(0,0,100)

@tf.function()
def train_step(input_image, target,epoch):
    global gen_loss_epoch
    
    with tf.GradientTape() as gen_tape, tf.GradientTape() as discr_tape:
        
        output_image = generator(input_image, training=True)

        output_gen_discr = discriminator([output_image, input_image], training=True)

        output_trg_discr = discriminator([target, input_image], training=True)

        discr_loss = discriminator_loss(output_trg_discr, output_gen_discr)
      
        gen_loss = generator_loss(output_gen_discr, output_image, target)
        
        #with tf.Session() as sess:
        tf.print(""error del discriminador="", discr_loss)
        tf.print(""error del generador="",  gen_loss)

        gen_loss_epoch[epoch]=tf.print(gen_loss)
        generator_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)
        
        discriminator_grads = discr_tape.gradient(discr_loss, discriminator.trainable_variables)
        
        generator_optimizer.apply_gradients(zip(generator_grads, generator.trainable_variables))
        
        discriminator_optimizer.apply_gradients(zip(discriminator_grads, discriminator.trainable_variables))
```
        
but when he finishes training I see that

  >> type(gen_loss_epoch) 
  <tf.Operation 'PrintV2_2' type = PrintV2>
they are not numbers

Why does it return a tf.Operation? Why doesn't it save me the error value as it shows when training and just save a <tf.Operation 'PrintV2_2' type = PrintV2>?"
44185,Multi-output custom loss model crashes: ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()  ...  Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated. 	 [[{{node PyFunc}}]],"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Docker container in CentOS Linux release 7.8.2003 (Core)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Docker container - tensorflow/tensorflow:latest-gpu
- TensorFlow version (use command below): 2.3.1
- Python version: Python 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1
- GPU model and memory: 2x NVIDIA Tesla V100, 32510MiB (~34GB) of memory each

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I'm implementing a source-separation 2-output model, and it's made with the functional API. I'm running a custom loss function which uses 2-output targets and predictions (4 in total). For my loss function to work, my model is ""wrapped"" within a subclassed model. It crashes during training.

**Describe the expected behavior**
I expect it not to crash during training.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
from scipy.io import wavfile
import scipy.signal as sg
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.layers import Input, SimpleRNN, Dense, Lambda, TimeDistributed, Layer, LSTM, Bidirectional, BatchNormalization, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.activations import relu
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
import datetime
import numpy as np
import math
import random
import json
import os
import sys



# Loss function
def discriminative_loss(piano_true, noise_true, piano_pred, noise_pred, loss_const):
    last_dim = piano_pred.shape[1] * piano_pred.shape[2]
    return (
        tf.math.reduce_mean(tf.reshape(noise_pred - noise_true, shape=(-1, last_dim)) ** 2, axis=-1) - 
        (loss_const * tf.math.reduce_mean(tf.reshape(noise_pred - piano_true, shape=(-1, last_dim)) ** 2, axis=-1)) +
        tf.math.reduce_mean(tf.reshape(piano_pred - piano_true, shape=(-1, last_dim)) ** 2, axis=-1) -
        (loss_const * tf.math.reduce_mean(tf.reshape(piano_pred - noise_true, shape=(-1, last_dim)) ** 2, axis=-1))
    )



def make_model(features, sequences, name='Model'):

    input_layer = Input(shape=(sequences, features), dtype='float32', 
                        name='piano_noise_mixed')
    piano_true = Input(shape=(sequences, features), dtype='float32', 
                       name='piano_true')
    noise_true = Input(shape=(sequences, features), dtype='float32', 
                       name='noise_true')

    x = SimpleRNN(features // 2, 
                  activation='relu', 
                  return_sequences=True) (input_layer) 
    piano_pred = TimeDistributed(Dense(features), name='piano_hat') (x)  # source 1 branch
    noise_pred = TimeDistributed(Dense(features), name='noise_hat') (x)  # source 2 branch
  
    model = Model(inputs=[input_layer, piano_true, noise_true],
                  outputs=[piano_pred, noise_pred])

    return model



# Model ""wrapper"" for many-input loss function
class RestorationModel2(Model):
    def __init__(self, model, loss_const):
        super(RestorationModel2, self).__init__()
        self.model = model
        self.loss_const = loss_const
       
    def call(self, inputs):
        return self.model(inputs)

    def compile(self, optimizer, loss):
        super(RestorationModel2, self).compile()
        self.optimizer = optimizer
        self.loss = loss

    def train_step(self, data):
        # Unpack data - what generator yeilds
        x, piano_true, noise_true = data

        with tf.GradientTape() as tape:
            piano_pred, noise_pred = self.model((x, piano_true, noise_true), training=True)
            loss = self.loss(piano_true, noise_true, piano_pred, noise_pred, self.loss_const)

        trainable_vars = self.model.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        
        return {'loss': loss}

    def test_step(self, data):
        x, piano_true, noise_true = data

        piano_pred, noise_pred = self.model((x, piano_true, noise_true), training=False)
        loss = self.loss(piano_true, noise_true, piano_pred, noise_pred, self.loss_const)
        
        return {'loss': loss}



def make_imp_model(features, sequences, loss_const=0.05, 
                   optimizer=tf.keras.optimizers.RMSprop(clipvalue=0.7),
                   name='Restoration Model', epsilon=10 ** (-10)):
    
    # NEW Semi-imperative model
    model = RestorationModel2(make_model(features, sequences, name='Training Model'),
                              loss_const=loss_const)

    model.compile(optimizer=optimizer, loss=discriminative_loss)

    return model



# MODEL TRAIN & EVAL FUNCTION
def evaluate_source_sep(train_generator, validation_generator,
                        num_train, num_val, n_feat, n_seq, batch_size, 
                        loss_const, epochs=20, 
                        optimizer=tf.keras.optimizers.RMSprop(clipvalue=0.75),
                        patience=10, epsilon=10 ** (-10)):
   
    print('Making model...')    # IMPERATIVE MODEL - Customize Fit
    model = make_imp_model(n_feat, n_seq, loss_const=loss_const, optimizer=optimizer, epsilon=epsilon)
    
    print('Going into training now...')
    hist = model.fit(train_generator,
                     steps_per_epoch=math.ceil(num_train / batch_size),
                     epochs=epochs,
                     validation_data=validation_generator,
                     validation_steps=math.ceil(num_val / batch_size),
                     callbacks=[EarlyStopping('val_loss', patience=patience, mode='min')])
    print(model.summary())



# NEURAL NETWORK DATA GENERATOR
def my_dummy_generator(num_samples, batch_size, train_seq, train_feat):

    while True:
        for offset in range(0, num_samples, batch_size):

            # Initialise x, y1 and y2 arrays for this batch
            x, y1, y2 = (np.empty((batch_size, train_seq, train_feat)),
                            np.empty((batch_size, train_seq, train_feat)),
                            np.empty((batch_size, train_seq, train_feat)))

            yield (x, y1, y2)



def main():
    epsilon = 10 ** (-10)
    train_batch_size = 5
    loss_const, epochs, val_split = 0.05, 10, 0.25
    optimizer = tf.keras.optimizers.RMSprop(clipvalue=0.9)

    TRAIN_SEQ_LEN, TRAIN_FEAT_LEN = 1847, 2049
    TOTAL_SMPLS = 60 

    # Validation & Training Split
    indices = list(range(TOTAL_SMPLS))
    val_indices = indices[:math.ceil(TOTAL_SMPLS * val_split)]
    num_val = len(val_indices)
    num_train = TOTAL_SMPLS - num_val
   
    train_seq, train_feat = TRAIN_SEQ_LEN, TRAIN_FEAT_LEN
    print('Train Input Stats:')
    print('N Feat:', train_feat, 'Seq Len:', train_seq, 'Batch Size:', train_batch_size)

    # Create data generators and evaluate model with them
    train_generator = my_dummy_generator(num_train,
                        batch_size=train_batch_size, train_seq=train_seq,
                        train_feat=train_feat)
    validation_generator = my_dummy_generator(num_val,
                        batch_size=train_batch_size, train_seq=train_seq,
                        train_feat=train_feat)

    evaluate_source_sep(train_generator, validation_generator, num_train, num_val,
                            n_feat=train_feat, n_seq=train_seq, 
                            batch_size=train_batch_size, 
                            loss_const=loss_const, epochs=epochs,
                            optimizer=optimizer, epsilon=epsilon)

if __name__ == '__main__':
    main()

```


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


Matplotlib created a temporary config/cache directory at /tmp/matplotlib-w351htm7 because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
2020-10-20 20:59:48.073656: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Train Input Stats:
N Feat: 2049 Seq Len: 1847 Batch Size: 5
Making model...
2020-10-20 20:59:49.685893: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-10-20 20:59:51.341091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:27:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0
coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s
2020-10-20 20:59:51.343325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
pciBusID: 0000:83:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0
coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s
2020-10-20 20:59:51.343415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-20 20:59:51.346449: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-20 20:59:51.349214: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-20 20:59:51.349659: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-20 20:59:51.352344: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-20 20:59:51.353860: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-20 20:59:51.359411: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-20 20:59:51.367984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
2020-10-20 20:59:51.368576: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-20 20:59:51.405603: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2245615000 Hz
2020-10-20 20:59:51.435047: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4c0cdc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-20 20:59:51.435197: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-20 20:59:51.659719: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x44a7910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-10-20 20:59:51.659822: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100S-PCIE-32GB, Compute Capability 7.0
2020-10-20 20:59:51.659849: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100S-PCIE-32GB, Compute Capability 7.0
2020-10-20 20:59:51.665940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:27:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0
coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s
2020-10-20 20:59:51.668089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
pciBusID: 0000:83:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0
coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s
2020-10-20 20:59:51.668184: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-20 20:59:51.668387: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-20 20:59:51.668620: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-20 20:59:51.668671: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-20 20:59:51.668786: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-20 20:59:51.668836: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-20 20:59:51.668883: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-20 20:59:51.677143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
2020-10-20 20:59:51.677300: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-20 20:59:52.875191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-20 20:59:52.875315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 
2020-10-20 20:59:52.875346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N Y 
2020-10-20 20:59:52.875557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   Y N 
2020-10-20 20:59:52.882402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30132 MB memory) -> physical GPU (device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:27:00.0, compute capability: 7.0)
2020-10-20 20:59:52.885327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30132 MB memory) -> physical GPU (device: 1, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:83:00.0, compute capability: 7.0)
Going into training now...
2020-10-20 20:59:53.516824: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
Epoch 1/10
9/9 [==============================] - ETA: 0s - loss: 0.0000e+00Traceback (most recent call last):
  File ""dlnn_brahms_restore_clean.py"", line 212, in <module>
    main()
  File ""dlnn_brahms_restore_clean.py"", line 209, in main
    optimizer=optimizer, epsilon=epsilon)
  File ""dlnn_brahms_restore_clean.py"", line 158, in evaluate_source_sep
    callbacks=[EarlyStopping('val_loss', patience=patience, mode='min')])
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py"", line 1137, in fit
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py"", line 416, in on_epoch_end
    callback.on_epoch_end(epoch, numpy_logs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py"", line 1664, in on_epoch_end
    if self.monitor_op(current - self.min_delta, self.best):
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
2020-10-20 21:00:05.825863: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]]
"
44181,clang: error: linker command failed with exit code 1 (use -v to see invocation),"hi, 
I was trying to run:

**_make -f tensorflow/lite/micro/tools/make/makefile test_hello_world_test_**

and had an error: **clang: error: linker command failed with exit code 1 (use -v to see invocation)**

Using make to build other source files gave me the same error. I'm using macOSX Catalina 10.15.7

Cheers



@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**

**Please provide the exact sequence of commands/steps when you ran into the problem**

"
44179,File system scheme 'gs' not implemented,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.4 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: --
- TensorFlow installed from (source or binary): source
- TensorFlow version: [2.3.1](https://github.com/tensorflow/tensorflow/releases/tag/v2.3.1)
- Python version: 3.7.5
- Installed using virtualenv? pip? conda?: pip (in virtualenv)
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: 10.2 / 7.6.5
- GPU model and memory: Tesla P100, 16GB



**Describe the problem**

Using `tensorflow.io.gfile.GFile` with the GCS filesystem results in `tensorflow.python.framework.errors_impl.UnimplementedError: File system scheme 'gs' not implemented`, but only if TF is compiled from source. Using the vanilla `pip install tensorflow==2.3.1` yields no errors.

To clarify how the TF wheel was built, in the `./configure` step I've used all default options except for CUDA support. When building the pip package, I ran `bazel build --config=cuda --config=monolithic --config=v2 //tensorflow/tools/pip_package:build_pip_package`.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Running
```
import tensorflow as tf

with tf.io.gfile.GFile('gs://foo', mode='w') as bar:
	bar.write('baz')
```

Yields
```
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/home/joao/py_3_7/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write
    self._prewrite_check()
  File ""/home/joao/py_3_7/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check
    compat.as_bytes(self.__name), compat.as_bytes(self.__mode))
tensorflow.python.framework.errors_impl.UnimplementedError: File system scheme 'gs' not implemented (file: 'gs://foo')
```

**Any other info / logs**
1. It seems like this issue is related to [this](https://github.com/tensorflow/tensorflow/issues/38477#issuecomment-657929334) comment. A few comments below it was [suggested](https://github.com/tensorflow/tensorflow/issues/38477#issuecomment-659069010) to build the gcs plugin separately, but I was unable to complete that step (no `gcs_file_system.dll` found);
2. This issue was reproduced on a different machine, with everything equal except the GPUs (GTX1080 8GB and Titan X 12GB);
3. The example above is a dummy GCS path, but the error is the same when a previously working path is given.

Thanks :)"
44178,Deprecation warnings Model.state_updates and layer.updates when saving model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): happens on 2.3.1 (v2.3.0-54-gfcc4b966f1) and 2.4.0-dev20201020 (v1.12.1-44160-g72c19e8880)
- Python version: 3.6.9

**Describe the current behavior**
Getting two deprecation warnings when saving a model (with default parameters).
```
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2334: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:1397: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`layer.updates` will be removed in a future version. '
```

**Describe the expected behavior**
No warnings!

**Standalone code to reproduce the issue**
```
import tensorflow as tf
from tensorflow import keras

model = keras.models.Sequential([
  keras.layers.Dense(1, input_shape=(1,)),
])

model.save('my_model')
```
[Colab](https://colab.research.google.com/drive/1tAx3zIJ9fKzpjitXmIhebxL_UkeFLidM?usp=sharing)
"
44177,ctc_loss generate different value between sparse label and dense label,"With tensorflow 1.15, I get different value when using sparse tensor and dense tensor as labels separately. Is there something wrong with my code? I also use tensorflow 2.x to verf this and get the same result. The two different usage would get different vaule.
```
import numpy as np
import tensorflow as tf

# params
batch_size = 3
seq_len = 15
label_len = 12
num_class = 20

# construct input
indices = np.array([[0, 0], [0, 4], [1, 9], [2, 5]], dtype=np.int64)
values = np.array([1, 1, 1, 1], dtype=np.int64)
shape = np.array([batch_size, label_len], dtype=np.int64) 
tensor = tf.SparseTensor(indices, values, shape)
tensor = tf.cast(tensor, tf.int32)
# generate label
logit = tf.random.uniform([seq_len,batch_size,num_class], dtype=tf.float32)

# two diffenret loss
loss_1 = tf.reduce_mean(
    tf.nn.ctc_loss(
        labels=tensor, inputs=logit,
        sequence_length=seq_len * np.ones(batch_size)
    ),
    name='ctc_loss1'
)
loss_2 = tf.reduce_mean(
    tf.nn.ctc_loss_v2(
        labels=tf.sparse_tensor_to_dense(tensor, default_value=num_class-1),
        logits=logit,
        logit_length=seq_len * np.ones(batch_size, dtype=np.int32),
        label_length=label_len * np.ones(batch_size, dtype=np.int32),
        blank_index=num_class-1
    ),
    name='ctc_loss2'
)

# run
sess = tf.compat.v1.Session()
print(sess.run([loss_1, loss_2]))
# output is : [39.27905, 740.8396]
```
"
44176,Massive memory leaks due to data.Dataset.shuffle,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  18.04.1-Ubuntu
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.6.9
- CUDA/cuDNN version: 10.1
- GPU model and memory: GTX 2080 Ti, ~11gb

**Describe the current behavior**
When a new iterator to a dataset containing a `shuffle()` iteration is opened after the old one became completely exhausted,
the memory held by the `ShuffleDataset` is not released / reused, resulting in massive memory leaks and ultimately in the process being killed by the OOM reaper.

For this purpose it does not matter whether we manually iterate over the dataset, use a Keras function like `Model.fit()` or chain a `Dataset.repeat()` operation at the end.
The original bug was found in production code and the condensed code below outlines roughly our original data pipeline
but perfectly reproduces the problem.

**Describe the expected behavior**
Memory usage should be constant when a new iterator to the Dataset is opened and there are no existing iterators anymore.

To be extra safe it might be desirable to immediately release any memory held by the `ShuffleDataset` when iteration is done,
so that other components can use it. (maybe introduce parameter controlling the behaviour?). This could be very important in conjunction with `Dataset.interleave()`, e.g when we iterate 36 files with a `cycle_length` of four and only have enough memory to hold 4 shuffle buffers in memory. If memory is not immediately released, we would run out of memory after the first four files have been processed.

**Standalone code to reproduce the issue**
I run the code with the `memory-profiler` package (https://pypi.org/project/memory-profiler/) to generate plots of the memory usage. By default shuffle buffers are enabled but when any additional argv is passed, shuffle buffers will be disabled:

Example usage: `mprof run --include-children test.py ` or `mprof run --include-children test.py no-shuffle` 

I recommend at least 32 GB of memory so that you can properly observe the behaviour. Otherwise feel free to tune down the memory usage in the code, for example by reducing the image size from 512x512 to 256x256.

```
import sys
import tensorflow as tf

do_shuffle = len(sys.argv) <= 1

# Simulate reading from files
filenames = tf.data.Dataset.from_tensor_slices(['{}.data'.format(i) for i in range(16)])

def read_files(files):
    # In the original code we open TFRecordDatasets here
    N = 8192 * 4

    def gen():
        for _ in range(N // 32):
            yield tf.random.normal([32, 512, 512, 1])

    rng_ds = tf.data.Dataset.from_generator(gen, tf.float32).unbatch()
    return rng_ds

readers_ds = filenames.batch(4).map(read_files, num_parallel_calls=1, deterministic=True)

def process(ds):
    # Create windows of 4 and add them as extra T dimension 
    window_size = 4
    ds = ds.window(window_size, shift=1, drop_remainder=True).flat_map(lambda x: x).batch(window_size)
    
    # buffer size = 1.07 GB (256 * 4 * 512 * 512 * 4)
    if do_shuffle:
        ds = ds.shuffle(    
            256, 
            reshuffle_each_iteration=True
        )

    return ds

# interleave will result in 4 iterators being opened in parallel
# which iterate the whole dataset (each iterates 4 files and there are 32 files in total)
ds = readers_ds.interleave(
        process,
        cycle_length=4,   # total buffer size: 1.07 GB * 4 = 4.29 GB
        block_length=1,
        num_parallel_calls=1,
        deterministic=False
    )

ds = ds.batch(32)

for e in range(30):
    print('epoch: ', e)

    # this creates a temporary iterator to the dataset
    for x in ds:
        pass
```

**Other info / logs**
The first run uses shuffling and we can clearly see the buffer filling up again after each epoch without the old memory being released (it appears that sometimes a small fraction is released though). I'm not sure why the buffers use 8gb in total opposed to the theoretical 4gb. After the fourth epoch the process is killed on my machine, because i run out of memory (32gb):

![shuffle](https://user-images.githubusercontent.com/1858546/96578075-e6d4bb00-12d4-11eb-80bf-047a826e2e90.png)

Log:
```
epoch:  0
epoch:  1
epoch:  2
epoch:  3
epoch:  4
```

-----------------------

For the second run I disabled shuffling and we can see that there is still some leakage yet much more irregularly. In previous test runs which used our original data-pipeline, I was able to achieve a flat memory usage by disabling the shuffling; I'm not sure why it doesn't work with the test script though. This might require further investigation. I manually terminated the script after a while.

![no-shuffle](https://user-images.githubusercontent.com/1858546/96579856-bd695e80-12d7-11eb-941a-6f87677c418f.png)

Log:
```
epoch:  0
epoch:  1
epoch:  2
epoch:  3
epoch:  4
epoch:  5
```"
44174,ESP32: Register_RESIZE_NEAREST_NEIGHBOR not a member ,"@tensorflow/micro

**System information**
- Ubuntu20
- TensorFlow installed from conda:
- Tensorflow version 2.3
- ESP32


I try to run a cnn on esp32 cam. When defining the operators with 
```
  micro_mutable_op_resolver.AddBuiltin(
    tflite::BuiltinOperator_RESIZE_NEAREST_NEIGHBOR,
    tflite::ops::micro::Register_RESIZE_NEAREST_NEIGHBOR(),
    1, 4);
```
and compiling I receive the error 
`'Register_RESIZE_NEAREST_NEIGHBOR' is not a member of 'tflite::ops::micro'
`

This is my code:
```
#include ""TensorFlowLite_ESP32.h""
#include ""tensorflow/lite/experimental/micro/kernels/all_ops_resolver.h""
#include ""tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h""
#include ""tensorflow/lite/experimental/micro/kernels/micro_ops.h""

#include ""tensorflow/lite/experimental/micro/micro_error_reporter.h""
#include ""tensorflow/lite/experimental/micro/micro_interpreter.h""


// Create a memory pool for the nodes in the network
constexpr int tensor_pool_size = 25 * 1024; // trial and erro approach to get the right size
uint8_t tensor_pool[tensor_pool_size];

// Define the model to be used
const tflite::Model* tflite_model;

// Define the interpreter
tflite::MicroInterpreter* interpreter;

// Input/Output nodes for the network
TfLiteTensor* input;
TfLiteTensor* output;


// define array that contains pixel values (gray scale, 0-255)
#define HEIGHT 120
#define WIDTH 160

uint8_t img_array[HEIGHT * WIDTH] = { 0 };

void setup() 
{

	// Load tflite model
	Serial.println(""Loading Tensorflow model...."");
	tflite_model = tflite::GetModel(cnn_model);
	Serial.println(""Model loaded!"");

  // Pull in only needed operations (should match NN layers)
  // Available ops:
  // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/micro_ops.h
  static tflite::MicroMutableOpResolver micro_mutable_op_resolver;
  micro_mutable_op_resolver.AddBuiltin(
    tflite::BuiltinOperator_FULLY_CONNECTED,
    tflite::ops::micro::Register_FULLY_CONNECTED(),
    1, 4);
  micro_mutable_op_resolver.AddBuiltin(
    tflite::BuiltinOperator_MAX_POOL_2D,
    tflite::ops::micro::Register_MAX_POOL_2D(),
    1, 4);
  micro_mutable_op_resolver.AddBuiltin(
    tflite::BuiltinOperator_SOFTMAX,
    tflite::ops::micro::Register_SOFTMAX(),
    1, 4);
  micro_mutable_op_resolver.AddBuiltin(
    tflite::BuiltinOperator_RESHAPE,
    tflite::ops::micro::Register_RESHAPE(),
    1, 4);
  micro_mutable_op_resolver.AddBuiltin(
    tflite::BuiltinOperator_CONV_2D,
    tflite::ops::micro::Register_CONV_2D(),
    1, 4);
  micro_mutable_op_resolver.AddBuiltin(
    tflite::BuiltinOperator_QUANTIZE,
    tflite::ops::micro::Register_QUANTIZE(),
    1, 4);
  micro_mutable_op_resolver.AddBuiltin(
    tflite::BuiltinOperator_DEQUANTIZE,
    tflite::ops::micro::Register_DEQUANTIZE(),
    1, 4);
  micro_mutable_op_resolver.AddBuiltin(
    tflite::BuiltinOperator_RESIZE_NEAREST_NEIGHBOR,
    tflite::ops::micro::Register_RESIZE_NEAREST_NEIGHBOR(),
    1, 4);
    
  // Instantiate the ErrorReporter and MicroErrorReporter
	static tflite::ErrorReporter* error_reporter;
	static tflite::MicroErrorReporter micro_error;
	error_reporter = &micro_error;

	// Instantiate the interpreter 
	static tflite::MicroInterpreter static_interpreter(
		tflite_model, micro_mutable_op_resolver, tensor_pool, tensor_pool_size, error_reporter
	);

	interpreter = &static_interpreter;

	// Allocate the the model's tensors in the memory pool that was created.
	Serial.println(""Allocating tensors to memory pool"");
	if(interpreter->AllocateTensors() != kTfLiteOk) {
		Serial.println(""There was an error allocating the memory...ooof"");
		return;
	}

	// Define input and output nodes
	input = interpreter->input(0);
	output = interpreter->output(0);
}
```

**Please provide the exact sequence of commands/steps when you ran into the problem**

"
44173,Not able to convert model to c array after quantization,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution Ubuntu 20.04:
- TensorFlow installed from source:
- Tensorflow version 2.3:
- Target platform Arduino Nano 33

**Describe the problem**
I am optimizing a model for the Arduino Nano 33. After training I quantized the model using the tf lite converter. The resulting tf-lite file is as expected much smaller, but I am not able convert it now using the xxd -i command. The non quantized model however, does work as expected and I am able to run inference with it on the microcontroller.

**Please provide the exact sequence of commands/steps when you ran into the problem**
I trained a functional keras model with tf 2.3 and gpu acceleration. After that I used the tf-lite converter class to quantize the model with my validation set and also did a conversion without quantization. Subsequently, I used the xxd -i command to convert the tflite files to a .cc file. The one without quantization looks as expected and I can copy it into my arduino code. The quantized model on the other hand is not a byte array. The file is made of lines like this:

```
00000000: 2400 0000 5446 4c33 0000 0000 0000 0000  $...TFL3........
00000010: 0000 1200 1c00 0400 0800 0c00 1000 1400  ................
00000020: 0000 1800 1200 0000 0300 0000 fc17 0000  ................
00000030: 7004 0000 5804 0000 3c00 0000 0400 0000  p...X...<.......
00000040: 0100 0000 0c00 0000 0800 0c00 0400 0800  ................
00000050: 0800 0000 0800 0000 1a00 0000 1300 0000  ................
00000060: 6d69 6e5f 7275 6e74 696d 655f 7665 7273  min_runtime_vers
```

Can anyone provide information to this problem?


"
44172,Lr keyword parameter for learning rate does not accept a learning rate schdule,"Output of system capture
```
== check python ===================================================
python version: 3.8.5
python branch: 
python build version: ('default', 'Jul 21 2020 10:42:08')
python compiler version: Clang 11.0.0 (clang-1100.0.33.17)
python implementation: CPython


== check os platform ===============================================

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 10.0.1 (clang-1001.0.46.4)
Target: x86_64-apple-darwin18.7.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== check pips ===================================================
numpy                  1.18.5
protobuf               3.13.0
tensorflow             2.3.1
tensorflow-estimator   2.3.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.3.1
tf.version.GIT_VERSION = v2.3.0-54-gfcc4b966f1
tf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
-bash: nvidia-smi: command not found

== cuda libs  ===================================================

== tensorflow installed from info ==================
Name: tensorflow
Version: 2.3.1
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /Users/nicksorros/code/notebooks/venv/lib/python3.8/site-packages
Required-by: 

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 8, 5, 'final', 0)

== bazel version  ===============================================
```

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os 10.14.16
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: Python 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
```
import tensorflow as tf

learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.01, 10, 0.9)
optimizer = tf.keras.optimizers.Adam(lr=learning_rate)
```

throws
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-7b7916db6482> in <module>
      1 learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.01, 10, 0.9)
----> 2 optimizer = tf.keras.optimizers.Adam(lr=learning_rate)

~/code/notebooks/venv/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/adam.py in __init__(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, name, **kwargs)
    113                name='Adam',
    114                **kwargs):
--> 115     super(Adam, self).__init__(name, **kwargs)
    116     self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
    117     self._set_hyper('decay', self._initial_decay)

~/code/notebooks/venv/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in __init__(self, name, **kwargs)
    303                         ""passed to optimizer: "" + str(k))
    304       # checks that all keyword arguments are non-negative.
--> 305       if kwargs[k] is not None and kwargs[k] < 0:
    306         raise ValueError(""Expected {} >= 0, received: {}"".format(k, kwargs[k]))
    307 

TypeError: '<' not supported between instances of 'ExponentialDecay' and 'int'
```

**Describe the expected behavior**
I was expecting this to work. It works as a positional argument. `optimizer = tf.keras.optimizers.Adam(learning_rate)`
"
44170,Missing compiler flags for some Cortex-M target architectures,"@tensorflow/micro

Exact reproduction steps can not be shared here since they rely on internal systems. However, it is very likely that the specific problem that I am seeing can be reproduced on a sparkfun edge as well.

High level steps:

 1. Generated a static lib for an M4F with:
     ```
     make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=cortex_m_generic TARGET_ARCH=cortex-m4+fp TAGS=cmsis-nn microlite
    ```
 1. Link into a binary and then attempt to run the binary (in my case via qemu)

 1. qemu gives an error like this:
    ```
    qemu: uncaught target signal 4 (Illegal instruction) - core dumped
    ```

In this case I isolated the problem to a missing

```TARGET_SPECIFIC_FLAGS += -D__FPU_PRESENT=1 -mfpu=fpv4-sp-d16```.

More generally, it seems like there might be such omissions for the other target architectures as well.

@mansnils @freddan80 would you be willing to set the correct flags for both arm-gcc and armclang for all the target architectures?"
44164,Ragged arange?,"I am sorry in advance if a solution for this exists. I am wondering if there is a way to create a ragged `arange` along the axis of an existing ragged tensor?

For instance, imagine I have an arbitrarily nested ragged tensor `x` I need to perform masking on. Something like:

```python
x = tf.ragged.constant([
    [[12, 9], [5]],
    [[10], [6, 8], [42]],
])
```

The easiest way for me to mask will be by index of an element. Is there a way to get a ragged arange with the same row lengths/splits like:

```python
x = tf.ragged.constant([
    [[0, 1], [2]],
    [[0], [1, 2], [3]],
])
```

It would be really cool to be able to do something like `tf.ragged.arange_along_axis(x, axis=2)` to get something like this."
44163,installation issue keras-preprocessing,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

ERROR: Could not find a version that satisfies the requirement keras-preprocessing<1.2,>=1.1.1 (from tensorflow==2.3.1) (from versions: none)
ERROR: No matching distribution found for keras-preprocessing<1.2,>=1.1.1 (from tensorflow==2.3.1)



**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44162,cpu cores not fully utilized causing gpu bottleneck,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Hi, I am finding that when running a step, only 3 out of 8 cpu cores are pegged at 100 percent and gpu is underutilized at 50 or 60 pct. other 5 cpu cores are at 0 pct. I am assuming that this may be causing cpu bottleneck in my model. 
I also see during backward process, one of compute process seems to run entire backward op (with tf profiler) but i see 3 cores getting utilized. 
I have tried setting these config but i don't see much benefit. Any pointers to if there is any config can utilize to provide more cpu parallelism.
num_threads = 8
tf.config.threading.set_inter_op_parallelism_threads(2*num_threads)
tf.config.threading.set_intra_op_parallelism_threads(2*num_threads)
As you see in below image there are lagger compute node where i see only 3 cores getting utilized and underutilized gpu. 

Also seems like lagger Execute::process correponds to backward pass as i see Adam operator running for timeframe.

<img width=""1177"" alt=""Screen Shot 2020-10-19 at 5 32 24 PM"" src=""https://user-images.githubusercontent.com/1817689/96525925-1a5e0980-1231-11eb-980b-69fc66f12b17.png"">


**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44161,Downloading pretrained Efficient Det in google colab using TensorFlow Object Detection Api gives a series of unknown warnings?,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I have created a notebook that downloads pretrained model from tf2 detection model zoo and performs prediction using saved model . The notebook is working fine on other models such as centernet, faster rcnn etc. But whenever I download efficient det using the download function below it issues a series of warnings.

MY DOWNLOAD FUNCTION
```
def download_model(model_name):
  
   
  download_url = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/'
  
  
  pretrained_model = model_name
  
  
  model_file = pretrained_model + '.tar.gz'

  
  
      
  !wget {download_url + model_file}
  tar = tarfile.open(model_file)
  tar.extractall()
  tar.close

    
  model_dir= os.getcwd() + '/' + pretrained_model        # determine the path to saved model 
  model_dir=pathlib.Path(model_dir)/'saved_model'
  model= tf.saved_model.load(str(model_dir))

  return model

```
The warnings I encounter:
```
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.
```

**Describe the expected behavior**
Eventhough my Efficient Det Model runs ok, I don't understand why is this warning being issued and how to get rid of it. I am making a notebook tutorial for others to understand TFOD but these warnings make my notebook look messy. Is there anyway I can get rid of those?
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
The warnings occur while using this code:
```
model_name= 'efficientdet_d1_coco17_tpu-32'
model= download_model(model_name)

```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44160,Normalization.adapt() not working on tf.data.Dataset(),"- TensorFlow version: 2.3.0
- Python version: 3.7.6

I am trying to use Normalization within my image classification model [ 224x224x3 shaped images, 2 classes with categorical (one hot) labels]. I have built a tf.data.Dataset through the tf.keras.preprocessing.image_dataset_from_directory() function.
Calling the element_spec() method on the resulting dataset results in:

`(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),
 TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))`

Which is consistent with what I want. However upon using Normalization.adapt() on this Dataset I get the following error:

`ValueError: as_list() is not defined on an unknown TensorShape.`

I couldn't find the solution to this anywhere. Any help is deeply appreciated. If I understand correctly from the documentation the adapt() method should be able to take tf.data.Datasets as inputs, right?"
44159,Incompatibility between versions of TF and CUDA dynamic libraries,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise 64 bit, version 10.0.17763
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tf-nightly-gpu 
- TensorFlow version: tf-nightly-gpu 2.4.0.dev20201019
- Python version: 3.8.6
- Installed using virtualenv? pip? conda?: Pip (in conda env)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 11.1/cuDNN 8.4.0.30
- GPU model and memory: NVIDIA Quadro P5000 (16 GB)


**Describe the problem**
I installed MS Visual Studio Community 2019, CUDA (express installation) and, cuDNN, as per the respective instructions.
I created a new conda env with Python 3.8.6 and activated it. I installed tf-nightly-gpu using pip. 
I launched Python and imported tensorflow, then listed GPU devices: all DLLs were found but one (cusolver64). The version of the library looked for by TF is 10 while CUDA 11.1 installs version 11.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`>>> import tensorflow as tf
2020-10-19 17:17:23.185914: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
>>> tf.config.list_physical_devices('GPU')
2020-10-19 17:17:44.311308: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-10-19 17:17:44.320150: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2020-10-19 17:17:44.361274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:65:00.0 name: Quadro P5000 computeCapability: 6.1
coreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 16.00GiB deviceMemoryBandwidth: 269.00GiB/s
2020-10-19 17:17:44.373653: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-10-19 17:17:44.392056: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-10-19 17:17:44.397830: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-10-19 17:17:44.407985: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-10-19 17:17:44.417194: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-10-19 17:17:44.424211: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found
2020-10-19 17:17:44.433236: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-10-19 17:17:44.440903: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-10-19 17:17:44.445862: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[]`


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44157,Can't allocate specific number of CPU threads when executing in graph mode,"I'm working on some benchmarking code to determine FPS depending on number of CPU cores. It seems like it isn't possible to select a certain number of CPU threads when running inference in graph mode. I can, however, select a specific number of threads when running inference with eager execution. I confirm this by running the script in the background and watching CPU allocation with 'htop' or 'top.'

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5 LTS 
- TensorFlow installed from (source or binary): pip 20.2.3
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: Python 3.6.9
- CUDA/cuDNN version: release 9.0, V9.0.176
- GPU model and memory: N/A
- CPU Model: Intel(R) Core(TM) i5-8400 CPU @ 2.80GHz, 


**Describe the current behavior**
When running in graph execution, only a single CPU core is being used (according to htop) regardless of how many threads I choose. I would expect a higher FPS when more threads are selected, however, I get the same FPS whenever eager mode is disabled. I believe graph execution is ignoring the thread variables. When running in eager execution, the correct number of threads are being used. I get different FPS for different numbers of threads.

**Describe the expected behavior**
When running in graph execution, I expect the script to use as many threads as I explicitly tell it to use. Further, I would expect to see a change in FPS when selecting a different number of threads.

**Standalone code to reproduce the issue**

<pre><code>import tensorflow as tf
import numpy as np
import time

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

ITERATIONS = 100
BATCH_SIZE = 1
INTER_THREADS = 1
INTRA_THREADS = 1

""Comment for eager execution (graph mode). Uncomment for Graph mode.""
tf.compat.v1.disable_eager_execution()

tf.config.threading.set_inter_op_parallelism_threads(INTER_THREADS)
tf.config.threading.set_intra_op_parallelism_threads(INTRA_THREADS)


@tf.function
def inference(net, input_data):
    return net(input_data, training=False)


model = tf.keras.applications.ResNet50(weights=None, classes=10)
dummy_batch = tf.random.uniform([BATCH_SIZE,224,224,3], dtype=tf.dtypes.float32)


i = 0
time_spent = np.zeros(ITERATIONS)
while (i < ITERATIONS + 1):
  if i != 0:
       ""clocking inference speeds""
       start_time = time.time()
       _ = inference(model, dummy_batch)
       time_spent[i-1] = time.time() - start_time
  else:
       ""warm up cpu / gpu""
       while(i < 50):
           _ = inference(model, dummy_batch)
           i+=1
       i = 0
  i += 1


fps = (len(time_spent) * BATCH_SIZE) / np.sum(time_spent)
print(""FPS: {}"".format(fps))
</code></pre>

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

**Output with eager execution disabled 1 inter thread, 1 intra thread
FPS: 162.246247505461

**Output with eager execution disabled 1 inter thread, 2 intra thread
FPS: 162.3569705471583

**Output with eager execution enabled 1 inter thread, 1 intra thread
FPS: 10.065427737674705

**Output with eager execution enabled 1 inter thread, 2 intra thread
FPS: 17.2407394877956
"
44154,"""WARNING:tensorflow:Model failed to initialize as JSON.Ignoring ""","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

# **System information**

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Scientific Linux 7
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
TensorFlow installed from (source or binary):binary
TensorFlow version:Desktop
Python version:3.6.2
Installed using virtualenv? pip? conda?: virtualenv and pip
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.0
GPU model and memory:

# **Describe the problem**
I am working in a remote server, creating a virtual environment on one shell with gpu.
I am updating a package for ObjectDetection, which in its original repo uses outdated tf 1.13.1, to tf 2.2.

Now i managed to make it train (yuhu!) but it seems to have issues in saving, since i get the following error message:

**""WARNING:tensorflow:Model failed to initialize as JSON.Ignoring ""**

plus also these ones (which seem to be ignorable for now, but still..)
""WARNING:tensorflow:'period' argument is deprecated.Please use 'save_freq' ect""
""WARNING:tensorflow:'epsilon' argument is deprecated. Please use 'min_delta' etc""

i believe the problem is somewhere in the def ""setTrainConfig""  attached at the end) but I cant figure it out how to solve it.

# **Provide the exact sequence of commands / steps that you executed before running into the problem**
-------------------------------------------------------------------------------------------------------------------------

**TF 2.2**
pip install --upgrade pip
python3 -m pip install tensorflow-gpu==2.2.0
python3 -m pip install opencv-python
python3 -m pip install keras
module load cuda/10.1
python3 -m pip install ImageAI-master/ # editedfrom package
python3 main.py
-------------------------------------------------------------------------------------------------------------------------
## **main.py**
from imageai.Detection.Custom import DetectionModelTrainer
import os

INPUT_DIR = ""/zhome/94/5/101974/Desktop/data/hololens/""

trainer = DetectionModelTrainer()
trainer.setModelTypeAsYOLOv3()
trainer.setDataDirectory(data_directory=INPUT_DIR)
trainer.setTrainConfig(object_names_array=[""hololens"", ""oculus""], batch_size=32, num_experiments=1, train_from_pretrained_model=""pretrained-yolov3.h5"")
trainer.trainModel()

"""""" evaluate performance""""""
trainer = DetectionModelTrainer()
trainer.setModelTypeAsYOLOv3()
trainer.setDataDirectory(data_directory=INPUT_DIR)
metrics = trainer.evaluateModel(model_path=INPUT_DIR+""models"", json_path=""hololens/json/detection_config.json"", iou_threshold=0.5, object_threshold=0.3, nms_threshold=0.5)
print(metrics)



# **Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
---------------------------------------------------------------------------------------------------------------------------------------------
## **yolo.py file**
from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, Lambda
from keras.layers.merge import add, concatenate
from keras.models import Model
from keras.engine.topology import Layer
import tensorflow as tf

"""""" 
ARI MOD 
https://stackoverflow.com/questions/59811781/tf-function-valueerror-creating-variables-on-a-non-first-call-to-a-function-de
"""""" 

batch_seen = None 

class YoloLayer(Layer):
	def __init__(self, anchors, max_grid, batch_size, warmup_batches, ignore_thresh, 
					grid_scale, obj_scale, noobj_scale, xywh_scale, class_scale, 
					**kwargs):
		# make the model settings persistent
		self.ignore_thresh  = ignore_thresh
		self.warmup_batches = warmup_batches
		self.anchors        = tf.constant(anchors, dtype='float', shape=[1,1,1,3,2])
		self.grid_scale     = grid_scale
		self.obj_scale      = obj_scale
		self.noobj_scale    = noobj_scale
		self.xywh_scale     = xywh_scale
		self.class_scale    = class_scale        

		# make a persistent mesh grid
		max_grid_h, max_grid_w = max_grid
		
		"""""" 
                ARI EDIT
		https://github.com/tensorflow/tensor2tensor/issues/1736
                """"""
		cell_x = tf.cast(tf.reshape(tf.tile(tf.range(max_grid_w), [max_grid_h]), (1, max_grid_h, max_grid_w, 1, 1)), dtype = tf.float32)
		cell_y = tf.transpose(cell_x, (0,2,1,3,4))
		self.cell_grid = tf.tile(tf.concat([cell_x,cell_y],-1), [batch_size, 1, 1, 3, 1])

		super(YoloLayer, self).__init__(**kwargs)

	def build(self, input_shape):
		super(YoloLayer, self).build(input_shape)  # Be sure to call this somewhere!


         """"""
	ARI MOD
	https://stackoverflow.com/questions/59811781/tf-function-valueerror-creating-variables-on-a-non-first-call-to-a-function-de
""""""
	def call(self, x):
		input_image, y_pred, y_true, true_boxes = x

		# adjust the shape of the y_predict [batch, grid_h, grid_w, 3, 4+1+nb_class]
		y_pred = tf.reshape(y_pred, tf.concat([tf.shape(y_pred)[:3], tf.constant([3, -1])], axis=0))
		
		# initialize the masks
		object_mask     = tf.expand_dims(y_true[..., 4], 4)

		# the variable to keep track of number of batches processed
		global batch_seen
		if batch_seen is None:
			batch_seen = tf.Variable(0.)
			
		# compute grid factor and net factor
		grid_h      = tf.shape(y_true)[1]
		grid_w      = tf.shape(y_true)[2]
		grid_factor = tf.reshape(tf.cast([grid_w, grid_h], tf.float32), [1,1,1,1,2])

		net_h       = tf.shape(input_image)[1]
		net_w       = tf.shape(input_image)[2]            
		net_factor  = tf.reshape(tf.cast([net_w, net_h], tf.float32), [1,1,1,1,2])
		
		""""""
		Adjust prediction
		""""""
		pred_box_xy    = (self.cell_grid[:,:grid_h,:grid_w,:,:] + tf.sigmoid(y_pred[..., :2]))  # sigma(t_xy) + c_xy
		pred_box_wh    = y_pred[..., 2:4]                                                       # t_wh
		pred_box_conf  = tf.expand_dims(tf.sigmoid(y_pred[..., 4]), 4)                          # adjust confidence
		pred_box_class = y_pred[..., 5:]                                                        # adjust class probabilities      

		""""""
		Adjust ground truth
		""""""
		true_box_xy    = y_true[..., 0:2] # (sigma(t_xy) + c_xy)
		true_box_wh    = y_true[..., 2:4] # t_wh
		true_box_conf  = tf.expand_dims(y_true[..., 4], 4)
		true_box_class = tf.argmax(y_true[..., 5:], -1)         

		""""""
		Compare each predicted box to all true boxes
		""""""        
		# initially, drag all objectness of all boxes to 0
		conf_delta  = pred_box_conf - 0 

		# then, ignore the boxes which have good overlap with some true box
		true_xy = true_boxes[..., 0:2] / grid_factor
		true_wh = true_boxes[..., 2:4] / net_factor
		
		true_wh_half = true_wh / 2.
		true_mins    = true_xy - true_wh_half
		true_maxes   = true_xy + true_wh_half
		
		pred_xy = tf.expand_dims(pred_box_xy / grid_factor, 4)
		pred_wh = tf.expand_dims(tf.exp(pred_box_wh) * self.anchors / net_factor, 4)
		
		pred_wh_half = pred_wh / 2.
		pred_mins    = pred_xy - pred_wh_half
		pred_maxes   = pred_xy + pred_wh_half    

		intersect_mins  = tf.maximum(pred_mins,  true_mins)
		intersect_maxes = tf.minimum(pred_maxes, true_maxes)

		intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)
		intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]
		
		true_areas = true_wh[..., 0] * true_wh[..., 1]
		pred_areas = pred_wh[..., 0] * pred_wh[..., 1]

		union_areas = pred_areas + true_areas - intersect_areas
		iou_scores  = tf.truediv(intersect_areas, union_areas)

		best_ious   = tf.reduce_max(iou_scores, axis=4) 
		# ARI EDIT
		#https://github.com/tensorflow/tensor2tensor/issues/1736       
		conf_delta *= tf.expand_dims(tf.cast(best_ious < self.ignore_thresh, dtype=tf.float32), 4)

		""""""
		Compute some online statistics
		""""""            
		true_xy = true_box_xy / grid_factor
		true_wh = tf.exp(true_box_wh) * self.anchors / net_factor

		true_wh_half = true_wh / 2.
		true_mins    = true_xy - true_wh_half
		true_maxes   = true_xy + true_wh_half

		pred_xy = pred_box_xy / grid_factor
		pred_wh = tf.exp(pred_box_wh) * self.anchors / net_factor 
		
		pred_wh_half = pred_wh / 2.
		pred_mins    = pred_xy - pred_wh_half
		pred_maxes   = pred_xy + pred_wh_half      

		intersect_mins  = tf.maximum(pred_mins,  true_mins)
		intersect_maxes = tf.minimum(pred_maxes, true_maxes)
		intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)
		intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]
		
		true_areas = true_wh[..., 0] * true_wh[..., 1]
		pred_areas = pred_wh[..., 0] * pred_wh[..., 1]

		union_areas = pred_areas + true_areas - intersect_areas
		iou_scores  = tf.truediv(intersect_areas, union_areas)
		iou_scores  = object_mask * tf.expand_dims(iou_scores, 4)
		
		count       = tf.reduce_sum(object_mask)
		count_noobj = tf.reduce_sum(1 - object_mask)
		"""""" ARI MOD """"""
		detect_mask = tf.cast((pred_box_conf*object_mask) >= 0.5,dtype=tf.float32)
		class_mask  = tf.expand_dims(tf.cast(tf.equal(tf.argmax(pred_box_class, -1), true_box_class),dtype=tf.float32), 4)
		recall50    = tf.reduce_sum(tf.cast(iou_scores >= 0.5 ,dtype=tf.float32) * detect_mask  * class_mask) / (count + 1e-3)
		recall75    = tf.reduce_sum(tf.cast(iou_scores >= 0.75 ,dtype=tf.float32) * detect_mask  * class_mask) / (count + 1e-3)    
		avg_iou     = tf.reduce_sum(iou_scores) / (count + 1e-3)
		avg_obj     = tf.reduce_sum(pred_box_conf  * object_mask)  / (count + 1e-3)
		avg_noobj   = tf.reduce_sum(pred_box_conf  * (1-object_mask))  / (count_noobj + 1e-3)
		avg_cat     = tf.reduce_sum(object_mask * class_mask) / (count + 1e-3) 

		""""""
		Warm-up training
		""""""
                """"""
		ARI MOD
		https://github.com/tensorflow/tensorflow/issues/30468
                 """"""
		batch_seen = tf.compat.v1.assign_add(batch_seen, 1.)
	
		true_box_xy, true_box_wh, xywh_mask = tf.cond(tf.less(batch_seen, self.warmup_batches+1), 
							  lambda: [true_box_xy + (0.5 + self.cell_grid[:,:grid_h,:grid_w,:,:]) * (1-object_mask), 
									   true_box_wh + tf.zeros_like(true_box_wh) * (1-object_mask), 
									   tf.ones_like(object_mask)],
							  lambda: [true_box_xy, 
									   true_box_wh,
									   object_mask])

		""""""
		Compare each true box to all anchor boxes
		""""""      
		wh_scale = tf.exp(true_box_wh) * self.anchors / net_factor
		wh_scale = tf.expand_dims(2 - wh_scale[..., 0] * wh_scale[..., 1], axis=4) # the smaller the box, the bigger the scale

		xy_delta    = xywh_mask   * (pred_box_xy-true_box_xy) * wh_scale * self.xywh_scale
		wh_delta    = xywh_mask   * (pred_box_wh-true_box_wh) * wh_scale * self.xywh_scale
		conf_delta  = object_mask * (pred_box_conf-true_box_conf) * self.obj_scale + (1-object_mask) * conf_delta * self.noobj_scale
		class_delta = object_mask * \
					  tf.expand_dims(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class), 4) * \
					  self.class_scale

		loss_xy    = tf.reduce_sum(tf.square(xy_delta),       list(range(1,5)))
		loss_wh    = tf.reduce_sum(tf.square(wh_delta),       list(range(1,5)))
		loss_conf  = tf.reduce_sum(tf.square(conf_delta),     list(range(1,5)))
		loss_class = tf.reduce_sum(class_delta,               list(range(1,5)))

		loss = loss_xy + loss_wh + loss_conf + loss_class


		return loss*self.grid_scale

	def compute_output_shape(self, input_shape):
		return [(None, 1)]

def _conv_block(inp, convs, do_skip=True):
	x = inp
	count = 0
	
	for conv in convs:
		if count == (len(convs) - 2) and do_skip:
			skip_connection = x
		count += 1
		
		if conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # unlike tensorflow darknet prefer left and top paddings
		x = Conv2D(conv['filter'], 
				   conv['kernel'], 
				   strides=conv['stride'], 
				   padding='valid' if conv['stride'] > 1 else 'same', # unlike tensorflow darknet prefer left and top paddings
				   name='conv_' + str(conv['layer_idx']), 
				   use_bias=False if conv['bnorm'] else True)(x)
		if conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)
		if conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)

	return add([skip_connection, x]) if do_skip else x        

def create_yolov3_model(
	nb_class, 
	anchors, 
	max_box_per_image, 
	max_grid, 
	batch_size, 
	warmup_batches,
	ignore_thresh,
	grid_scales,
	obj_scale,
	noobj_scale,
	xywh_scale,
	class_scale
):
	input_image = Input(shape=(None, None, 3)) # net_h, net_w, 3
	true_boxes  = Input(shape=(1, 1, 1, max_box_per_image, 4))
	true_yolo_1 = Input(shape=(None, None, len(anchors)//6, 4+1+nb_class)) # grid_h, grid_w, nb_anchor, 5+nb_class
	true_yolo_2 = Input(shape=(None, None, len(anchors)//6, 4+1+nb_class)) # grid_h, grid_w, nb_anchor, 5+nb_class
	true_yolo_3 = Input(shape=(None, None, len(anchors)//6, 4+1+nb_class)) # grid_h, grid_w, nb_anchor, 5+nb_class

	# Layer  0 => 4
	x = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},
								  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},
								  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},
								  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])

	# Layer  5 => 8
	x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},
						{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},
						{'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])

	# Layer  9 => 11
	x = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},
						{'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])

	# Layer 12 => 15
	x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},
						{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},
						{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])

	# Layer 16 => 36
	for i in range(7):
		x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},
							{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])
		
	skip_36 = x
		
	# Layer 37 => 40
	x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},
						{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},
						{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])

	# Layer 41 => 61
	for i in range(7):
		x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},
							{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])
		
	skip_61 = x
		
	# Layer 62 => 65
	x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},
						{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},
						{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])

	# Layer 66 => 74
	for i in range(3):
		x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},
							{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])
		
	# Layer 75 => 79
	x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},
						{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},
						{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},
						{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},
						{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], do_skip=False)

	# Layer 80 => 82
	pred_yolo_1 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},
							 {'filter': (3*(5+nb_class)), 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], do_skip=False)
	loss_yolo_1 = YoloLayer(anchors[12:], 
							[1*num for num in max_grid], 
							batch_size, 
							warmup_batches, 
							ignore_thresh, 
							grid_scales[0],
							obj_scale,
							noobj_scale,
							xywh_scale,
							class_scale)([input_image, pred_yolo_1, true_yolo_1, true_boxes])

	# Layer 83 => 86
	x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], do_skip=False)
	x = UpSampling2D(2)(x)
	x = concatenate([x, skip_61])

	# Layer 87 => 91
	x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},
						{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},
						{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},
						{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},
						{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], do_skip=False)

	# Layer 92 => 94
	pred_yolo_2 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},
							 {'filter': (3*(5+nb_class)), 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], do_skip=False)
	loss_yolo_2 = YoloLayer(anchors[6:12], 
							[2*num for num in max_grid], 
							batch_size, 
							warmup_batches, 
							ignore_thresh, 
							grid_scales[1],
							obj_scale,
							noobj_scale,
							xywh_scale,
							class_scale)([input_image, pred_yolo_2, true_yolo_2, true_boxes])

	# Layer 95 => 98
	x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], do_skip=False)
	x = UpSampling2D(2)(x)
	x = concatenate([x, skip_36])

	# Layer 99 => 106
	pred_yolo_3 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},
							 {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},
							 {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},
							 {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},
							 {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},
							 {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},
							 {'filter': (3*(5+nb_class)), 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], do_skip=False)
	loss_yolo_3 = YoloLayer(anchors[:6], 
							[4*num for num in max_grid], 
							batch_size, 
							warmup_batches, 
							ignore_thresh, 
							grid_scales[2],
							obj_scale,
							noobj_scale,
							xywh_scale,
							class_scale)([input_image, pred_yolo_3, true_yolo_3, true_boxes]) 

	train_model = Model([input_image, true_boxes, true_yolo_1, true_yolo_2, true_yolo_3], [loss_yolo_1, loss_yolo_2, loss_yolo_3])
	infer_model = Model(input_image, [pred_yolo_1, pred_yolo_2, pred_yolo_3])

	return [train_model, infer_model]

def dummy_loss(y_true, y_pred):
	return tf.sqrt(tf.reduce_sum(y_pred))
--------------------------------------------------------------------------------------------------------------------------------------------
**__init__.py**
import os
import re
import numpy as np
import json
from imageai.Detection.Custom.voc import parse_voc_annotation
from imageai.Detection.Custom.yolo import create_yolov3_model, dummy_loss
from imageai.Detection.YOLOv3.models import yolo_main
from imageai.Detection.Custom.generator import BatchGenerator
from imageai.Detection.Custom.utils.utils import normalize, evaluate, makedirs
from keras.callbacks import ReduceLROnPlateau
from keras.optimizers import Adam
from imageai.Detection.Custom.callbacks import CustomModelCheckpoint, CustomTensorBoard
from imageai.Detection.Custom.utils.multi_gpu_model import multi_gpu_model
from imageai.Detection.Custom.gen_anchors import generateAnchors
import tensorflow as tf
from keras.models import load_model, Input
from keras.callbacks import TensorBoard
import keras.backend as K
import cv2

os.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""


class DetectionModelTrainer:

    """"""
    This is the Detection Model training class, which allows you to train object detection models
    on image datasets that are in Pascal VOC annotation format, using the YOLOv3.
    """"""

    def __init__(self):
        self.__model_type = """"
        self.__training_mode = True

        self.__model_min_input_size = 288
        self.__model_max_input_size = 448
        self.__model_anchors = []
        self.__inference_anchors = []
        self.__json_directory = """"
        self.__model_labels = []
        self.__num_objects = 0
        self.__pre_trained_model = """"

        self.__train_images_folder = """"
        self.__train_annotations_folder = """"
        self.__train_cache_file = """"
        self.__train_times = 8
        self.__train_batch_size = 4
        self.__train_learning_rate = 1e-4
        self.__train_epochs = 100
        self.__train_warmup_epochs = 3
        self.__train_ignore_treshold = 0.5
        self.__train_gpus = ""0""
        self.__train_grid_scales = [1, 1, 1]
        self.__train_obj_scale = 5
        self.__train_noobj_scale = 1
        self.__train_xywh_scale = 1
        self.__train_class_scale = 1
        self.__model_directory = """"
        self.__train_weights_name = """"
        self.__train_debug = True
        self.__logs_directory = """"

        self.__validation_images_folder = """"
        self.__validation_annotations_folder = """"
        self.__validation_cache_file = """"
        self.__validation_times = 1

    def setModelTypeAsYOLOv3(self):
        """"""
        'setModelTypeAsYOLOv3()' is used to set the model type to the YOLOv3 model
        for the training instance object .
        :return:
        """"""
        self.__model_type = ""yolov3""

    def setDataDirectory(self, data_directory):

        """"""

        'setDataDirectory()' is required to set the path to which the data/dataset to be used for
                 training is kept. The directory can have any name, but it must have 'train' and 'validation'
                 sub-directory. In the 'train' and 'validation' sub-directories, there must be 'images' and 'annotations'
                 sub-directories respectively. The 'images' folder will contain the pictures for the dataset and the
                 'annotations' folder will contain the XML files with details of the annotations for each image in the
                 'images folder'.

                 N.B: Strictly take note that the filenames (without the extension) of the pictures in the 'images folder'
                  must be the same as the filenames (without the extension) of their corresponding annotation XML files in
                  the 'annotations' folder.

                 The structure of the 'train' and 'validation' folder must be as follows:

                >> train    >> images       >> img_1.jpg
                            >> images       >> img_2.jpg
                            >> images       >> img_3.jpg
                            >> annotations  >> img_1.xml
                            >> annotations  >> img_2.xml
                            >> annotations  >> img_3.xml


                >> validation   >> images       >> img_151.jpg
                                >> images       >> img_152.jpg
                                >> images       >> img_153.jpg
                                >> annotations  >> img_151.xml
                                >> annotations  >> img_152.xml
                                >> annotations  >> img_153.xml

        :param data_directory:
        :return:
        """"""

        self.__train_images_folder = os.path.join(data_directory, ""train"", ""images"")
        self.__train_annotations_folder = os.path.join(data_directory, ""train"", ""annotations"")
        self.__validation_images_folder = os.path.join(data_directory, ""validation"", ""images"")
        self.__validation_annotations_folder = os.path.join(data_directory, ""validation"", ""annotations"")

        os.makedirs(os.path.join(data_directory, ""cache""), exist_ok=True)
        self.__train_cache_file = os.path.join(data_directory, ""cache"", ""detection_train_data.pkl"")
        self.__validation_cache_file = os.path.join(data_directory, ""cache"", ""detection_test_data.pkl"")

        os.makedirs(os.path.join(data_directory, ""models""), exist_ok=True)

        os.makedirs(os.path.join(data_directory, ""json""), exist_ok=True)

        os.makedirs(os.path.join(data_directory, ""logs""), exist_ok=True)

        self.__model_directory = os.path.join(data_directory, ""models"")
        self.__train_weights_name = os.path.join(self.__model_directory, ""detection_model-"")
        self.__json_directory = os.path.join(data_directory, ""json"")
        self.__logs_directory = os.path.join(data_directory, ""logs"")

    def setGpuUsage(self, train_gpus):
        """"""
        'setGpuUsage' function allows you to set the GPUs to be used while training
        train_gpu can be:
        - an integer, indicating the number of GPUs to use
        - a list of integers, indicating the id of the GPUs to be used
        - a string, indicating the it og the id of the GPUs to be used, separated by commas
        :param train_gpus: gpus where to run
        :return:
        """"""
        # train_gpus, could be a string separated by comma, or a list of int or the number of GPUs to be used
        if type(train_gpus) == str:
            train_gpus = train_gpus.split(',')
        if type(train_gpus) == int:
            train_gpus = range(train_gpus)
        # let it as a string separated by commas
        self.__train_gpus = ','.join([str(gpu) for gpu in train_gpus])

  #  def setTrainConfig(self,  object_names_array, batch_size=4, num_experiments=100, train_from_pretrained_model=""""):

        """"""

        'setTrainConfig()' function allows you to set the properties for the training instances. It accepts the following values:

        - object_names_array , this is an array of the names of the different objects in your dataset
        - batch_size (optional),  this is the batch size for the training instance
        - num_experiments (optional),   also known as epochs, it is the number of times the network will train on all the training dataset
        - train_from_pretrained_model (optional), this is used to perform transfer learning by specifying the path to a pre-trained YOLOv3 model

        :param object_names_array:
        :param batch_size:
        :param num_experiments:
        :param train_from_pretrained_model:
        :return:
        """"""

        self.__model_anchors, self.__inference_anchors = generateAnchors(self.__train_annotations_folder,
                                                                         self.__train_images_folder,
                                                                         self.__train_cache_file, self.__model_labels)

        self.__model_labels = sorted(object_names_array)
        self.__num_objects = len(object_names_array)

        self.__train_batch_size = batch_size
        self.__train_epochs = num_experiments
        self.__pre_trained_model = train_from_pretrained_model

        json_data = dict()
        json_data[""labels""] = self.__model_labels
        json_data[""anchors""] = self.__inference_anchors

       ** with open(os.path.join(self.__json_directory, ""detection_config.json""), ""w+"") as json_file:
            json.dump(json_data, json_file, indent=4, separators=("","", "" : ""),
                      ensure_ascii=True)

        print(""Detection configuration saved in "", os.path.join(self.__json_directory, ""detection_config.json"")) **

    def trainModel(self):

        """"""
        'trainModel()' function starts the actual model training. Once the training starts, the training instance
        creates 3 sub-folders in your dataset folder which are:

        - json,  where the JSON configuration file for using your trained model is stored
        - models, where your trained models are stored once they are generated after each improved experiments
        - cache , where temporary traing configuraton files are stored

        :return:
        """"""

        train_ints, valid_ints, labels, max_box_per_image = self._create_training_instances(
            self.__train_annotations_folder,
            self.__train_images_folder,
            self.__train_cache_file,
            self.__validation_annotations_folder,
            self.__validation_images_folder,
            self.__validation_cache_file,
            self.__model_labels

        )
        if self.__training_mode:
            print('Training on: \t' + str(labels) + '')
            print(""Training with Batch Size: "", self.__train_batch_size)
            print(""Number of Training Samples: "", len(train_ints))
            print(""Number of Validation Samples: "", len(valid_ints))
            print(""Number of Experiments: "", self.__train_epochs)

        ###############################
        #   Create the generators
        ###############################
        train_generator = BatchGenerator(
            instances=train_ints,
            anchors=self.__model_anchors,
            labels=labels,
            downsample=32,  # ratio between network input's size and network output's size, 32 for YOLOv3
            max_box_per_image=max_box_per_image,
            batch_size=self.__train_batch_size,
            min_net_size=self.__model_min_input_size,
            max_net_size=self.__model_max_input_size,
            shuffle=True,
            jitter=0.3,
            norm=normalize
        )

        valid_generator = BatchGenerator(
            instances=valid_ints,
            anchors=self.__model_anchors,
            labels=labels,
            downsample=32,  # ratio between network input's size and network output's size, 32 for YOLOv3
            max_box_per_image=max_box_per_image,
            batch_size=self.__train_batch_size,
            min_net_size=self.__model_min_input_size,
            max_net_size=self.__model_max_input_size,
            shuffle=True,
            jitter=0.0,
            norm=normalize
        )

        ###############################
        #   Create the model
        ###############################
        if os.path.exists(self.__pre_trained_model):
            self.__train_warmup_epochs = 0
        warmup_batches = self.__train_warmup_epochs * (self.__train_times * len(train_generator))

        os.environ['CUDA_VISIBLE_DEVICES'] = self.__train_gpus
        multi_gpu = [int(gpu) for gpu in self.__train_gpus.split(',')]

        train_model, infer_model = self._create_model(
            nb_class=len(labels),
            anchors=self.__model_anchors,
            max_box_per_image=max_box_per_image,
            max_grid=[self.__model_max_input_size, self.__model_max_input_size],
            batch_size=self.__train_batch_size,
            warmup_batches=warmup_batches,
            ignore_thresh=self.__train_ignore_treshold,
            multi_gpu=multi_gpu,
            lr=self.__train_learning_rate,
            grid_scales=self.__train_grid_scales,
            obj_scale=self.__train_obj_scale,
            noobj_scale=self.__train_noobj_scale,
            xywh_scale=self.__train_xywh_scale,
            class_scale=self.__train_class_scale,
        )

        ###############################
        #   Kick off the training
        ###############################
        callbacks = self._create_callbacks(self.__train_weights_name, infer_model)
		
        train_model.fit_generator(
            generator=train_generator,
            steps_per_epoch=len(train_generator) * self.__train_times,
            validation_data=valid_generator,
            validation_steps=len(valid_generator) * self.__train_times,
            epochs=self.__train_epochs + self.__train_warmup_epochs,
            verbose=1,
            callbacks=callbacks,
            workers=4,
            max_queue_size=8
        )

    def evaluateModel(self, model_path, json_path, batch_size=4, iou_threshold=0.5, object_threshold=0.2, nms_threshold=0.45):
        """"""

        'evaluateModel()' is used to obtain the mAP metrics for your model(s). It accepts the following values:

        - model_path ( model file or folder), this value can be the part to your model file or the path to the folder containing all your saved model files
        - json_path ,   this is the path the the 'detection_config.json' file saved for the dataset during the training
        - iou_threshold , this value is used to set the desired 'IoU' to obtain the mAP metrics for your model(s)
        - object_threshold , this is used to set your desired minimum 'class score' to obtain the mAP metrics for your model(s)
        - nms_threshold , this is used to set your desired 'Non-maximum suppresion' to obtain the mAP metrics for your model(s)

        :param model_path:
        :param json_path:
        :param batch_size:
        :param iou_threshold:
        :param object_threshold:
        :param nms_threshold:
        :return: list of dictionaries, containing one dict per evaluated model.
            Each dict contains exactly the same metrics that are printed on standard output
        """"""

        self.__training_mode = False

        with open(json_path, 'r') as json_file:
            detection_model_json = json.load(json_file)

        temp_anchor_array = []
        new_anchor_array = []

        temp_anchor_array.append(detection_model_json[""anchors""][2])
        temp_anchor_array.append(detection_model_json[""anchors""][1])
        temp_anchor_array.append(detection_model_json[""anchors""][0])

        for aa in temp_anchor_array:
            for aaa in aa:
                new_anchor_array.append(aaa)

        self.__model_anchors = new_anchor_array
        self.__model_labels = detection_model_json[""labels""]
        self.__num_objects = len(self.__model_labels)

        self.__train_batch_size = batch_size
        self.__train_epochs = 100

        print(""Starting Model evaluation...."")

        _, valid_ints, labels, max_box_per_image = self._create_training_instances(
            self.__train_annotations_folder,
            self.__train_images_folder,
            self.__train_cache_file,
            self.__validation_annotations_folder,
            self.__validation_images_folder,
            self.__validation_cache_file,
            self.__model_labels

        )

        if len(valid_ints) == 0:
            print('Validation samples were not provided.')
            print('Please, check your validation samples are correctly provided:')
            print('\tAnnotations: {}\n\tImages: {}'.format(self.__validation_annotations_folder,
                                                           self.__validation_images_folder))

        valid_generator = BatchGenerator(
            instances=valid_ints,
            anchors=self.__model_anchors,
            labels=labels,
            downsample=32,  # ratio between network input's size and network output's size, 32 for YOLOv3
            max_box_per_image=max_box_per_image,
            batch_size=self.__train_batch_size,
            min_net_size=self.__model_min_input_size,
            max_net_size=self.__model_max_input_size,
            shuffle=True,
            jitter=0.0,
            norm=normalize
        )

        results = list()

        if os.path.isfile(model_path):
            # model_files must be a list containing the complete path to the files,
            # if a file is given, then the list contains just this file
            model_files = [model_path]
        elif os.path.isdir(model_path):
            # model_files must be a list containing the complete path to the files,
            # if a folder is given, then the list contains the complete path to each file on that folder
            model_files = sorted([os.path.join(model_path, file_name) for file_name in os.listdir(model_path)])
            # sort the files to make sure we're always evaluating them on same order
        else:
            print('model_path must be the path to a .h5 file or a directory. Found {}'.format(model_path))
            return results

        for model_file in model_files:
            if str(model_file).endswith("".h5""):
                try:
                    infer_model = load_model(model_file)

                    ###############################
                    #   Run the evaluation
                    ###############################
                    # compute mAP for all the classes
                    average_precisions = evaluate(infer_model, valid_generator, iou_threshold=iou_threshold,
                                                  obj_thresh=object_threshold, nms_thresh=nms_threshold)

                    result_dict = {
                        'model_file': model_file,
                        'using_iou': iou_threshold,
                        'using_object_threshold': object_threshold,
                        'using_non_maximum_suppression': nms_threshold,
                        'average_precision': dict(),
                        'evaluation_samples': len(valid_ints)
                    }
                    # print the score
                    print(""Model File: "", model_file, '\n')
                    print(""Evaluation samples: "", len(valid_ints))
                    print(""Using IoU: "", iou_threshold)
                    print(""Using Object Threshold: "", object_threshold)
                    print(""Using Non-Maximum Suppression: "", nms_threshold)

                    for label, average_precision in average_precisions.items():
                        print(labels[label] + ': {:.4f}'.format(average_precision))
                        result_dict['average_precision'][labels[label]] = average_precision

                    print('mAP: {:.4f}'.format(sum(average_precisions.values()) / len(average_precisions)))
                    result_dict['map'] = sum(average_precisions.values()) / len(average_precisions)
                    print(""==============================="")

                    results.append(result_dict)
                except Exception as e:
                    print('skipping the evaluation of {} because following exception occurred: {}'.format(model_file, e))
                    continue
            else:
                print('skipping the evaluation of {} since it\'s not a .h5 file'.format(model_file))

        return results

    def _create_training_instances(self,
            train_annot_folder,
            train_image_folder,
            train_cache,
            valid_annot_folder,
            valid_image_folder,
            valid_cache,
            labels,
    ):

        # parse annotations of the training set
        train_ints, train_labels = parse_voc_annotation(train_annot_folder, train_image_folder, train_cache, labels)

        # parse annotations of the validation set, if any, otherwise split the training set

        if os.path.exists(valid_annot_folder):
            valid_ints, valid_labels = parse_voc_annotation(valid_annot_folder, valid_image_folder, valid_cache, labels)
            print('Evaluating over {} samples taken from {}'.format(len(valid_ints),
                                                                    os.path.dirname(valid_annot_folder)))
        else:

            train_portion = 0.8  # use 80% to train and the remaining 20% to evaluate
            train_valid_split = int(round(train_portion * len(train_ints)))
            np.random.seed(0)
            np.random.shuffle(train_ints)

            valid_ints = train_ints[train_valid_split:]
            train_ints = train_ints[:train_valid_split]
            print('Evaluating over {} samples taken as {:5.2f}% of the training set '
                  'given at {}'.format(len(valid_ints),
                                       (1 - train_portion)*100,
                                       os.path.dirname(train_annot_folder)))

        print('Training over {} samples  given at {}'.format(len(train_ints), os.path.dirname(train_annot_folder)))

        # compare the seen labels with the given labels in config.json
        if len(labels) > 0:
            overlap_labels = set(labels).intersection(set(train_labels.keys()))

            # return None, None, None if some given label is not in the dataset
            if len(overlap_labels) < len(labels):
                if self.__training_mode:
                    print('Some labels have no annotations! Please revise the list of labels in your configuration.')
                return None, None, None, None
        else:
            if self.__training_mode:
                print('No labels are provided. Train on all seen labels.')
                print(train_labels)

            labels = train_labels.keys()

        max_box_per_image = max([len(inst['object']) for inst in (train_ints + valid_ints)])

        return train_ints, valid_ints, sorted(labels), max_box_per_image

    def _create_callbacks(self, saved_weights_name, model_to_save):

        checkpoint = CustomModelCheckpoint(
            model_to_save=model_to_save,
            filepath=saved_weights_name + 'ex-{epoch:03d}--loss-{loss:08.3f}.h5',
            monitor='loss',
            verbose=0,
            save_best_only=True,
            mode='min',
            period=1
        )
        reduce_on_plateau = ReduceLROnPlateau(
            monitor='loss',
            factor=0.1,
            patience=2,
            verbose=0,
            mode='min',
            epsilon=0.01,
            cooldown=0,
            min_lr=0
        )
        tensor_board = TensorBoard(
            log_dir=self.__logs_directory
        )
        return [checkpoint, reduce_on_plateau, tensor_board]

    def _create_model(
            self,
            nb_class,
            anchors,
            max_box_per_image,
            max_grid, batch_size,
            warmup_batches,
            ignore_thresh,
            multi_gpu,
            lr,
            grid_scales,
            obj_scale,
            noobj_scale,
            xywh_scale,
            class_scale
    ):
        if len(multi_gpu) > 1:
            with tf.device('/cpu:0'):
                template_model, infer_model = create_yolov3_model(
                    nb_class=nb_class,
                    anchors=anchors,
                    max_box_per_image=max_box_per_image,
                    max_grid=max_grid,
                    batch_size=batch_size // len(multi_gpu),
                    warmup_batches=warmup_batches,
                    ignore_thresh=ignore_thresh,
                    grid_scales=grid_scales,
                    obj_scale=obj_scale,
                    noobj_scale=noobj_scale,
                    xywh_scale=xywh_scale,
                    class_scale=class_scale
                )
        else:
            template_model, infer_model = create_yolov3_model(
                nb_class=nb_class,
                anchors=anchors,
                max_box_per_image=max_box_per_image,
                max_grid=max_grid,
                batch_size=batch_size,
                warmup_batches=warmup_batches,
                ignore_thresh=ignore_thresh,
                grid_scales=grid_scales,
                obj_scale=obj_scale,
                noobj_scale=noobj_scale,
                xywh_scale=xywh_scale,
                class_scale=class_scale
            )

            # load the pretrained weight if exists, otherwise load the backend weight only

        if len(self.__pre_trained_model) > 3:
            if self.__training_mode:
                print(""Training with transfer learning from pretrained Model"")
            template_model.load_weights(self.__pre_trained_model, by_name=True)
        else:
            if self.__training_mode:
                print(""Pre-trained Model not provided. Transfer learning not in use."")
                print(""Training will start with 3 warmup experiments"")

        if len(multi_gpu) > 1:
            train_model = multi_gpu_model(template_model, gpus=multi_gpu)
        else:
            train_model = template_model

        optimizer = Adam(lr=lr, clipnorm=0.001)
        train_model.compile(loss=dummy_loss, optimizer=optimizer)

        return train_model, infer_model


class CustomObjectDetection:

    """"""
    This is the object detection class for using your custom trained models. It supports your custom trained YOLOv3 model and allows to you to perform object detection in images.
    """"""

    def __init__(self):
        self.__model_type = """"
        self.__model_path = """"
        self.__model_labels = []
        self.__model_anchors = []
        self.__detection_config_json_path = """"
        self.__input_size = 416
        self.__object_threshold = 0.4
        self.__nms_threshold = 0.4
        self.__model = None
        self.__detection_utils = CustomDetectionUtils(labels=[])

    def setModelTypeAsYOLOv3(self):
        """"""
        'setModelTypeAsYOLOv3' is used to set your custom detection model as YOLOv3
        :return:
        """"""
        self.__model_type = ""yolov3""

    def setModelPath(self, detection_model_path):
        """"""
        'setModelPath' is used to specify the filepath to your custom detection model
        :param detection_model_path: path to the .h5 model file.
            Usually is one of those under <data_directory>/models/detection_model-ex-ddd--loss-dddd.ddd.h5
        :return: None
        """"""
        self.__model_path = detection_model_path

    def setJsonPath(self, configuration_json):
        """"""
        'setJsonPath' is used to set the filepath to the configuration JSON file for your custom detection model
        :param configuration_json: path to the .json file. Usually it is <data_directory>/json/detection_config.json
        :return: None
        """"""
        self.__detection_config_json_path = configuration_json

    def loadModel(self):

        """"""
        'loadModel' is used to load the model into the CustomObjectDetection class
        :return: None
        """"""

        if self.__model_type == ""yolov3"":
            detection_model_json = json.load(open(self.__detection_config_json_path))

            self.__model_labels = detection_model_json[""labels""]
            self.__model_anchors = detection_model_json[""anchors""]

            self.__detection_utils = CustomDetectionUtils(labels=self.__model_labels)

            self.__model = yolo_main(Input(shape=(None, None, 3)), 3, len(self.__model_labels))

            self.__model.load_weights(self.__model_path)

    def detectObjectsFromImage(self, input_image="""", output_image_path="""", input_type=""file"", output_type=""file"",
                               extract_detected_objects=False, minimum_percentage_probability=50, nms_treshold=0.4,
                               display_percentage_probability=True, display_object_name=True, thread_safe=False):

        """"""

        'detectObjectsFromImage()' function is used to detect objects observable in the given image:
                    * input_image , which can be a filepath or image numpy array in BGR
                    * output_image_path (only if output_type = file) , file path to the output image that will contain the detection boxes and label, if output_type=""file""
                    * input_type (optional) , filepath/numpy array of the image. Acceptable values are ""file"" and ""array""
                    * output_type (optional) , file path/numpy array/image file stream of the image. Acceptable values are ""file"" and ""array""
                    * extract_detected_objects (optional) , option to save each object detected individually as an image and return an array of the objects' image path.
                    * minimum_percentage_probability (optional, 30 by default) , option to set the minimum percentage probability for nominating a detected object for output.
                    * nms_threshold (optional, o.45 by default) , option to set the Non-maximum suppression for the detection
                    * display_percentage_probability (optional, True by default), option to show or hide the percentage probability of each object in the saved/returned detected image
                    * display_display_object_name (optional, True by default), option to show or hide the name of each object in the saved/returned detected image
                    * thread_safe (optional, False by default), enforce the loaded detection model works across all threads if set to true, made possible by forcing all Keras inference to run on the default graph


            The values returned by this function depends on the parameters parsed. The possible values returnable
            are stated as below
            - If extract_detected_objects = False or at its default value and output_type = 'file' or
                at its default value, you must parse in the 'output_image_path' as a string to the path you want
                the detected image to be saved. Then the function will return:
                1. an array of dictionaries, with each dictionary corresponding to the objects
                    detected in the image. Each dictionary contains the following property:
                    * name (string)
                    * percentage_probability (float)
                    * box_points (list of x1,y1,x2 and y2 coordinates)

            - If extract_detected_objects = False or at its default value and output_type = 'array' ,
              Then the function will return:

                1. a numpy array of the detected image
                2. an array of dictionaries, with each dictionary corresponding to the objects
                    detected in the image. Each dictionary contains the following property:
                    * name (string)
                    * percentage_probability (float)
                    * box_points (list of x1,y1,x2 and y2 coordinates)

            - If extract_detected_objects = True and output_type = 'file' or
                at its default value, you must parse in the 'output_image_path' as a string to the path you want
                the detected image to be saved. Then the function will return:
                1. an array of dictionaries, with each dictionary corresponding to the objects
                    detected in the image. Each dictionary contains the following property:
                    * name (string)
                    * percentage_probability (float)
                    * box_points (list of x1,y1,x2 and y2 coordinates)
                2. an array of string paths to the image of each object extracted from the image

            - If extract_detected_objects = True and output_type = 'array', the the function will return:
                1. a numpy array of the detected image
                2. an array of dictionaries, with each dictionary corresponding to the objects
                    detected in the image. Each dictionary contains the following property:
                    * name (string)
                    * percentage_probability (float)
                    * box_points (list of x1,y1,x2 and y2 coordinates)
                3. an array of numpy arrays of each object detected in the image

        :param input_image:
        :param output_image_path:
        :param input_type:
        :param output_type:
        :param extract_detected_objects:
        :param minimum_percentage_probability:
        :param nms_treshold:
        :param display_percentage_probability:
        :param display_object_name:
        :param thread_safe:
        :return image_frame:
        :return output_objects_array:
        :return detected_objects_image_array:
        """"""

        if self.__model is None:
            raise ValueError(""You must call the loadModel() function before making object detection."")
        else:
            if output_type == ""file"":
                # from the image file, lets keep the directory and the filename, but remove its  format
                # if output_image_path is path/to/the/output/image.png
                # then output_image_folder is  path/to/the/output/image
                # let's check if it is in the appropriated format soon to fail early
                output_image_folder, n_subs = re.subn(r'\.(?:jpe?g|png|tif|webp|PPM|PGM)$', '', output_image_path, flags=re.I)
                if n_subs == 0:
                    # if no substitution was done, the given output_image_path is not in a supported format,
                    # raise an error
                    raise ValueError(""output_image_path must be the path where to write the image. ""
                                     ""Therefore it must end as one the following: ""
                                     ""'.jpg', '.png', '.tif', '.webp', '.PPM', '.PGM'. {} found"".format(output_image_path))
                elif extract_detected_objects:
                    # Results must be written as files and need to extract detected objects as images,
                    # let's create a folder to store the object's images
                    objects_dir = output_image_folder + ""-objects""

                    os.makedirs(objects_dir, exist_ok=True)

            self.__object_threshold = minimum_percentage_probability / 100
            self.__nms_threshold = nms_treshold

            output_objects_array = []
            detected_objects_image_array = []

            if input_type == ""file"":
                image = cv2.imread(input_image)
            elif input_type == ""array"":
                image = input_image
            else:
                raise ValueError(""input_type must be 'file' or 'array'. {} found"".format(input_type))

            image_frame = image.copy()

            height, width, channels = image.shape

            image = cv2.resize(image, (self.__input_size, self.__input_size))

            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            image = image.astype(""float32"") / 255.

            # expand the image to batch
            image = np.expand_dims(image, 0)

            if self.__model_type == ""yolov3"":
                if thread_safe == True:
                    with K.get_session().graph.as_default():
                        yolo_results = self.__model.predict(image)
                else:
                    yolo_results = self.__model.predict(image)

                boxes = list()

                for idx, result in enumerate(yolo_results):
                    box_set = self.__detection_utils.decode_netout(result[0], self.__model_anchors[idx],
                                                                   self.__object_threshold, self.__input_size,
                                                                   self.__input_size)
                    boxes += box_set

                self.__detection_utils.correct_yolo_boxes(boxes, height, width, self.__input_size, self.__input_size)

                self.__detection_utils.do_nms(boxes, self.__nms_threshold)

                all_boxes, all_labels, all_scores = self.__detection_utils.get_boxes(boxes, self.__model_labels,
                                                                                     self.__object_threshold)

                for object_box, object_label, object_score in zip(all_boxes, all_labels, all_scores):
                    each_object_details = dict()
                    each_object_details[""name""] = object_label
                    each_object_details[""percentage_probability""] = object_score

                    if object_box.xmin < 0:
                        object_box.xmin = 0
                    if object_box.ymin < 0:
                        object_box.ymin = 0

                    each_object_details[""box_points""] = [object_box.xmin, object_box.ymin, object_box.xmax, object_box.ymax]
                    output_objects_array.append(each_object_details)

                drawn_image = self.__detection_utils.draw_boxes_and_caption(image_frame.copy(), all_boxes, all_labels,
                                                                            all_scores, show_names=display_object_name,
                                                                            show_percentage=display_percentage_probability)

                if extract_detected_objects:

                    for cnt, each_object in enumerate(output_objects_array):

                        splitted_image = image_frame[each_object[""box_points""][1]:each_object[""box_points""][3],
                                                     each_object[""box_points""][0]:each_object[""box_points""][2]]
                        if output_type == ""file"":
                            splitted_image_path = os.path.join(objects_dir, ""{}-{:05d}.jpg"".format(each_object[""name""],
                                                                                                   cnt))

                            cv2.imwrite(splitted_image_path, splitted_image)
                            detected_objects_image_array.append(splitted_image_path)
                        elif output_type == ""array"":
                            detected_objects_image_array.append(splitted_image.copy())

                if output_type == ""file"":
                    # we already validated that the output_image_path is a supported by OpenCV one
                    cv2.imwrite(output_image_path, drawn_image)

                if extract_detected_objects:
                    if output_type == ""file"":
                        return output_objects_array, detected_objects_image_array
                    elif output_type == ""array"":
                        return drawn_image, output_objects_array, detected_objects_image_array

                else:
                    if output_type == ""file"":
                        return output_objects_array
                    elif output_type == ""array"":
                        return drawn_image, output_objects_array


class CustomVideoObjectDetection:


    """"""

    This is the object detection class for videos and camera live stream inputs using your custom trained detection models. It provides support for your custom YOLOv3 models.

    """"""

    def __init__(self):
        self.__model_type = """"
        self.__model_path = """"
        self.__model_labels = []
        self.__model_anchors = []
        self.__detection_config_json_path = """"
        self.__model_loaded = False
        self.__input_size = 416
        self.__object_threshold = 0.4
        self.__nms_threshold = 0.4
        self.__detector = []
        self.__detection_utils = CustomDetectionUtils(labels=[])

    def setModelTypeAsYOLOv3(self):

        """"""
        'setModelTypeAsYOLOv3' is used to set your custom detection model as YOLOv3
        :return:
        """"""

        self.__model_type = ""yolov3""


    def setModelPath(self, detection_model_path):
        """"""
        'setModelPath' is used to specify the filepath to your custom detection model

        :param detection_model_path:
        :return:
        """"""
        self.__model_path = detection_model_path


    def setJsonPath(self, configuration_json):
        """"""
        'setJsonPath' is used to set the filepath to the configuration JSON file for your custom detection model

        :param configuration_json:
        :return:
        """"""
        self.__detection_config_json_path = configuration_json

    def loadModel(self):
        """"""
        'loadModel' is used to load the model into the CustomVideoObjectDetection class

        :return:
        """"""

        if (self.__model_loaded == False):
            if(self.__model_type == ""yolov3""):
                detector = CustomObjectDetection()
                detector.setModelTypeAsYOLOv3()
                detector.setModelPath(self.__model_path)
                detector.setJsonPath(self.__detection_config_json_path)
                detector.loadModel()

                self.__detector = detector
                self.__model_loaded = True


    def detectObjectsFromVideo(self, input_file_path="""", camera_input=None, output_file_path="""", frames_per_second=20,
                               frame_detection_interval=1, minimum_percentage_probability=50, log_progress=False,
                               display_percentage_probability=True, display_object_name=True, save_detected_video=True,
                               per_frame_function=None, per_second_function=None, per_minute_function=None,
                               video_complete_function=None, return_detected_frame=False, detection_timeout = None):




        """"""

        'detectObjectsFromVideo()' function is used to detect objects observable in the given video path or a camera input:
            * input_file_path , which is the file path to the input video. It is required only if 'camera_input' is not set
            * camera_input , allows you to parse in camera input for live video detections
            * output_file_path , which is the path to the output video. It is required only if 'save_detected_video' is not set to False
            * frames_per_second , which is the number of frames to be used in the output video
            * frame_detection_interval (optional, 1 by default)  , which is the intervals of frames that will be detected.
            * minimum_percentage_probability (optional, 50 by default) , option to set the minimum percentage probability for nominating a detected object for output.
            * log_progress (optional) , which states if the progress of the frame processed is to be logged to console
            * display_percentage_probability (optional), can be used to hide or show probability scores on the detected video frames
            * display_object_name (optional), can be used to show or hide object names on the detected video frames
            * save_save_detected_video (optional, True by default), can be set to or not to save the detected video
            * per_frame_function (optional), this parameter allows you to parse in a function you will want to execute after each frame of the video is detected. If this parameter is set to a function, after every video  frame is detected, the function will be executed with the following values parsed into it:
                -- position number of the frame
                -- an array of dictinaries, with each dictinary corresponding to each object detected. Each dictionary contains 'name', 'percentage_probability' and 'box_points'
                -- a dictionary with with keys being the name of each unique objects and value are the number of instances of the object present
                -- If return_detected_frame is set to True, the numpy array of the detected frame will be parsed as the fourth value into the function

            * per_second_function (optional), this parameter allows you to parse in a function you will want to execute after each second of the video is detected. If this parameter is set to a function, after every second of a video is detected, the function will be executed with the following values parsed into it:
                -- position number of the second
                -- an array of dictionaries whose keys are position number of each frame present in the last second , and the value for each key is the array for each frame that contains the dictionaries for each object detected in the frame
                -- an array of dictionaries, with each dictionary corresponding to each frame in the past second, and the keys of each dictionary are the name of the number of unique objects detected in each frame, and the key values are the number of instances of the objects found in the frame
                -- a dictionary with its keys being the name of each unique object detected throughout the past second, and the key values are the average number of instances of the object found in all the frames contained in the past second
                -- If return_detected_frame is set to True, the numpy array of the detected frame will be parsed
                                                                    as the fifth value into the function

            * per_minute_function (optional), this parameter allows you to parse in a function you will want to execute after each minute of the video is detected. If this parameter is set to a function, after every minute of a video is detected, the function will be executed with the following values parsed into it:
                -- position number of the minute
                -- an array of dictionaries whose keys are position number of each frame present in the last minute , and the value for each key is the array for each frame that contains the dictionaries for each object detected in the frame

                -- an array of dictionaries, with each dictionary corresponding to each frame in the past minute, and the keys of each dictionary are the name of the number of unique objects detected in each frame, and the key values are the number of instances of the objects found in the frame

                -- a dictionary with its keys being the name of each unique object detected throughout the past minute, and the key values are the average number of instances of the object found in all the frames contained in the past minute

                -- If return_detected_frame is set to True, the numpy array of the detected frame will be parsed as the fifth value into the function

            * video_complete_function (optional), this parameter allows you to parse in a function you will want to execute after all of the video frames have been detected. If this parameter is set to a function, after all of frames of a video is detected, the function will be executed with the following values parsed into it:
                -- an array of dictionaries whose keys are position number of each frame present in the entire video , and the value for each key is the array for each frame that contains the dictionaries for each object detected in the frame
                -- an array of dictionaries, with each dictionary corresponding to each frame in the entire video, and the keys of each dictionary are the name of the number of unique objects detected in each frame, and the key values are the number of instances of the objects found in the frame
                -- a dictionary with its keys being the name of each unique object detected throughout the entire video, and the key values are the average number of instances of the object found in all the frames contained in the entire video

            * return_detected_frame (optionally, False by default), option to obtain the return the last detected video frame into the per_per_frame_function, per_per_second_function or per_per_minute_function

            * detection_timeout (optionally, None by default), option to state the number of seconds of a video that should be detected after which the detection function stop processing the video

        :param input_file_path:
        :param camera_input:
        :param output_file_path:
        :param frames_per_second:
        :param frame_detection_interval:
        :param minimum_percentage_probability:
        :param log_progress:
        :param display_percentage_probability:
        :param display_object_name:
        :param save_detected_video:
        :param per_frame_function:
        :param per_second_function:
        :param per_minute_function:
        :param video_complete_function:
        :param return_detected_frame:
        :param detection_timeout:
        :return output_video_filepath:
        :return counting:
        :return output_objects_array:
        :return output_objects_count:
        :return detected_copy:
        :return this_second_output_object_array:
        :return this_second_counting_array:
        :return this_second_counting:
        :return this_minute_output_object_array:
        :return this_minute_counting_array:
        :return this_minute_counting:
        :return this_video_output_object_array:
        :return this_video_counting_array:
        :return this_video_counting:
        """"""

        output_frames_dict = {}
        output_frames_count_dict = {}

        input_video = cv2.VideoCapture(input_file_path)
        if (camera_input != None):
            input_video = camera_input

        output_video_filepath = output_file_path + '.avi'

        frame_width = int(input_video.get(3))
        frame_height = int(input_video.get(4))
        output_video = cv2.VideoWriter(output_video_filepath, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'),
                                       frames_per_second,
                                       (frame_width, frame_height))

        counting = 0
        predicted_numbers = None
        scores = None
        detections = None


        detection_timeout_count = 0
        video_frames_count = 0


        if(self.__model_type == ""yolov3""):



            while (input_video.isOpened()):
                ret, frame = input_video.read()

                if (ret == True):

                    detected_frame = frame.copy()

                    video_frames_count += 1
                    if (detection_timeout != None):
                        if ((video_frames_count % frames_per_second) == 0):
                            detection_timeout_count += 1

                        if (detection_timeout_count >= detection_timeout):
                            break

                    output_objects_array = []

                    counting += 1

                    if (log_progress == True):
                        print(""Processing Frame : "", str(counting))



                    check_frame_interval = counting % frame_detection_interval

                    if (counting == 1 or check_frame_interval == 0):
                        try:
                            detected_frame, output_objects_array = self.__detector.detectObjectsFromImage(
                                input_image=frame, input_type=""array"", output_type=""array"",
                                minimum_percentage_probability=minimum_percentage_probability,
                                display_percentage_probability=display_percentage_probability,
                                display_object_name=display_object_name)
                        except:
                            None


                    output_frames_dict[counting] = output_objects_array

                    output_objects_count = {}
                    for eachItem in output_objects_array:
                        eachItemName = eachItem[""name""]
                        try:
                            output_objects_count[eachItemName] = output_objects_count[eachItemName] + 1
                        except:
                            output_objects_count[eachItemName] = 1

                    output_frames_count_dict[counting] = output_objects_count


                    if (save_detected_video == True):
                        output_video.write(detected_frame)

                    if (counting == 1 or check_frame_interval == 0):
                        if (per_frame_function != None):
                            if (return_detected_frame == True):
                                per_frame_function(counting, output_objects_array, output_objects_count,
                                                   detected_frame)
                            elif (return_detected_frame == False):
                                per_frame_function(counting, output_objects_array, output_objects_count)

                    if (per_second_function != None):
                        if (counting != 1 and (counting % frames_per_second) == 0):

                            this_second_output_object_array = []
                            this_second_counting_array = []
                            this_second_counting = {}

                            for aa in range(counting):
                                if (aa >= (counting - frames_per_second)):
                                    this_second_output_object_array.append(output_frames_dict[aa + 1])
                                    this_second_counting_array.append(output_frames_count_dict[aa + 1])

                            for eachCountingDict in this_second_counting_array:
                                for eachItem in eachCountingDict:
                                    try:
                                        this_second_counting[eachItem] = this_second_counting[eachItem] + \
                                                                         eachCountingDict[eachItem]
                                    except:
                                        this_second_counting[eachItem] = eachCountingDict[eachItem]

                            for eachCountingItem in this_second_counting:
                                this_second_counting[eachCountingItem] = int(this_second_counting[eachCountingItem] / frames_per_second)

                            if (return_detected_frame == True):
                                per_second_function(int(counting / frames_per_second),
                                                    this_second_output_object_array, this_second_counting_array,
                                                    this_second_counting, detected_frame)

                            elif (return_detected_frame == False):
                                per_second_function(int(counting / frames_per_second),
                                                    this_second_output_object_array, this_second_counting_array,
                                                    this_second_counting)

                    if (per_minute_function != None):

                        if (counting != 1 and (counting % (frames_per_second * 60)) == 0):

                            this_minute_output_object_array = []
                            this_minute_counting_array = []
                            this_minute_counting = {}

                            for aa in range(counting):
                                if (aa >= (counting - (frames_per_second * 60))):
                                    this_minute_output_object_array.append(output_frames_dict[aa + 1])
                                    this_minute_counting_array.append(output_frames_count_dict[aa + 1])

                            for eachCountingDict in this_minute_counting_array:
                                for eachItem in eachCountingDict:
                                    try:
                                        this_minute_counting[eachItem] = this_minute_counting[eachItem] + \
                                                                         eachCountingDict[eachItem]
                                    except:
                                        this_minute_counting[eachItem] = eachCountingDict[eachItem]

                            for eachCountingItem in this_minute_counting:
                                this_minute_counting[eachCountingItem] = int(this_minute_counting[eachCountingItem] / (frames_per_second * 60))

                            if (return_detected_frame == True):
                                per_minute_function(int(counting / (frames_per_second * 60)),
                                                    this_minute_output_object_array, this_minute_counting_array,
                                                    this_minute_counting, detected_frame)

                            elif (return_detected_frame == False):
                                per_minute_function(int(counting / (frames_per_second * 60)),
                                                    this_minute_output_object_array, this_minute_counting_array,
                                                    this_minute_counting)


                else:
                    break

            if (video_complete_function != None):

                this_video_output_object_array = []
                this_video_counting_array = []
                this_video_counting = {}

                for aa in range(counting):
                    this_video_output_object_array.append(output_frames_dict[aa + 1])
                    this_video_counting_array.append(output_frames_count_dict[aa + 1])

                for eachCountingDict in this_video_counting_array:
                    for eachItem in eachCountingDict:
                        try:
                            this_video_counting[eachItem] = this_video_counting[eachItem] + \
                                                            eachCountingDict[eachItem]
                        except:
                            this_video_counting[eachItem] = eachCountingDict[eachItem]

                for eachCountingItem in this_video_counting:
                    this_video_counting[eachCountingItem] = this_video_counting[
                                                                eachCountingItem] / counting

                video_complete_function(this_video_output_object_array, this_video_counting_array,
                                        this_video_counting)

            input_video.release()
            output_video.release()

            if (save_detected_video == True):
                return output_video_filepath


class BoundBox:
    def __init__(self, xmin, ymin, xmax, ymax, objness=None, classes=None):
        self.xmin = xmin
        self.ymin = ymin
        self.xmax = xmax
        self.ymax = ymax
        self.objness = objness
        self.classes = classes
        self.label = -1
        self.score = -1

    def get_label(self):
        if self.label == -1:
            self.label = np.argmax(self.classes)

        return self.label

    def get_score(self):
        if self.score == -1:
            self.score = self.classes[self.get_label()]

        return self.score


class CustomDetectionUtils:
    def __init__(self, labels):
        self.__labels = labels
        self.__colors = []

        for i in range(len(labels)):
            color_space_values = np.random.randint(50, 255, size=(3,))
            red, green, blue = color_space_values
            red, green, blue = int(red), int(green), int(blue)
            self.__colors.append([red, green, blue])

    @staticmethod
    def _sigmoid(x):
        return 1. / (1. + np.exp(-x))

    def decode_netout(self, netout, anchors, obj_thresh, net_h, net_w):
        grid_h, grid_w = netout.shape[:2]
        nb_box = 3
        netout = netout.reshape((grid_h, grid_w, nb_box, -1))
        nb_class = netout.shape[-1] - 5
        boxes = []
        netout[..., :2] = self._sigmoid(netout[..., :2])
        netout[..., 4:] = self._sigmoid(netout[..., 4:])
        netout[..., 5:] = netout[..., 4][..., np.newaxis] * netout[..., 5:]
        netout[..., 5:] *= netout[..., 5:] > obj_thresh

        for row in range(grid_h):
            for col in range(grid_w):
                for b in range(nb_box):
                    # 4th element is objectness score
                    objectness = netout[row, col, b, 4]

                    if objectness <= obj_thresh:
                        continue

                    # first 4 elements are x, y, w, and h
                    x, y, w, h = netout[row, col, b, :4]
                    x = (col + x) / grid_w  # center position, unit: image width
                    y = (row + y) / grid_h  # center position, unit: image height
                    w = anchors[2 * b + 0] * np.exp(w) / net_w  # unit: image width
                    h = anchors[2 * b + 1] * np.exp(h) / net_h  # unit: image height
                    # last elements are class probabilities
                    classes = netout[row, col, b, 5:]
                    box = BoundBox(x - w / 2, y - h / 2, x + w / 2, y + h / 2, objectness, classes)
                    boxes.append(box)

        return boxes

    @staticmethod
    def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):
        new_w, new_h = net_w, net_h
        for i in range(len(boxes)):
            x_offset, x_scale = (net_w - new_w) / 2. / net_w, float(new_w) / net_w
            y_offset, y_scale = (net_h - new_h) / 2. / net_h, float(new_h) / net_h
            boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)
            boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)
            boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)
            boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)

    def _interval_overlap(self, interval_a, interval_b):
        x1, x2 = interval_a
        x3, x4 = interval_b
        if x3 < x1:
            if x4 < x1:
                return 0
            else:
                return min(x2, x4) - x1
        else:
            if x2 < x3:
                return 0
            else:
                return min(x2, x4) - x3

    def bbox_iou(self, box1, box2):
        intersect_w = self._interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])
        intersect_h = self._interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])
        intersect = intersect_w * intersect_h
        w1, h1 = box1.xmax - box1.xmin, box1.ymax - box1.ymin
        w2, h2 = box2.xmax - box2.xmin, box2.ymax - box2.ymin
        union = w1 * h1 + w2 * h2 - intersect

        try:
            result = float(intersect) / float(union)
            return result
        except:
            return 0.0

    def do_nms(self, boxes, nms_thresh):
        if len(boxes) > 0:
            nb_class = len(boxes[0].classes)
        else:
            return

        for c in range(nb_class):
            sorted_indices = np.argsort([-box.classes[c] for box in boxes])

            for i in range(len(sorted_indices)):
                index_i = sorted_indices[i]

                if boxes[index_i].classes[c] == 0: continue

                for j in range(i + 1, len(sorted_indices)):
                    index_j = sorted_indices[j]

                    if self.bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:
                        boxes[index_j].classes[c] = 0

    def get_boxes(self, boxes, labels, thresh):
        v_boxes, v_labels, v_scores = list(), list(), list()
        # enumerate all boxes
        for box in boxes:
            # enumerate all possible labels
            for i in range(len(labels)):
                # check if the threshold for this label is high enough
                if box.classes[i] > thresh:
                    v_boxes.append(box)
                    v_labels.append(labels[i])
                    v_scores.append(box.classes[i] * 100)
                # don't break, many labels may trigger for one box
        return v_boxes, v_labels, v_scores

    def label_color(self, label):
        """""" Return a color from a set of predefined colors. Contains 80 colors in total.

        Args
            label: The label to get the color for.

        Returns
            A list of three values representing a RGB color.

            If no color is defined for a certain label, the color green is returned and a warning is printed.
        """"""
        if label < len(self.__colors):
            return self.__colors[label]
        else:
            return 0, 255, 0

    def draw_boxes_and_caption(self, image_frame, v_boxes, v_labels, v_scores, show_names=False, show_percentage=False):

        for i in range(len(v_boxes)):
            box = v_boxes[i]
            y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax
            width, height = x2 - x1, y2 - y1
            class_color = self.label_color(self.__labels.index(v_labels[i]))

            image_frame = cv2.rectangle(image_frame, (x1, y1), (x2, y2), class_color, 2)

            label = """"
            if show_names and show_percentage:
                label = ""%s : %.3f"" % (v_labels[i], v_scores[i])
            elif show_names:
                label = ""%s"" % (v_labels[i])
            elif show_percentage:
                label = ""%.3f"" % (v_scores[i])

            if show_names or show_percentage:
                b = np.array([x1, y1, x2, y2]).astype(int)
                cv2.putText(image_frame, label, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (200, 0, 0), 3)
                cv2.putText(image_frame, label, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 2)

        return image_frame
--------------------------------------------------------------------------------------------------------------------------------------------
**installed packages in the env**
Package                Version
---------------------- ---------
absl-py                0.10.0
astunparse             1.6.3
cachetools             4.1.1
certifi                2020.6.20
chardet                3.0.4
cycler                 0.10.0
gast                   0.3.3
google-auth            1.22.1
google-auth-oauthlib   0.4.1
google-pasta           0.2.0
grpcio                 1.32.0
h5py                   2.10.0
idna                   2.10
imageai                2.1.5
importlib-metadata     2.0.0
Keras                  2.4.3
Keras-Preprocessing    1.1.2
kiwisolver             1.2.0
Markdown               3.3.1
matplotlib             3.3.2
numpy                  1.18.5
oauthlib               3.1.0
opencv-python          4.4.0.44
opt-einsum             3.3.0
Pillow                 8.0.0
pip                    20.2.4
protobuf               3.13.0
pyasn1                 0.4.8
pyasn1-modules         0.2.8
pyparsing              2.4.7
python-dateutil        2.8.1
PyYAML                 5.3.1
requests               2.24.0
requests-oauthlib      1.3.0
rsa                    4.6
scipy                  1.4.1
setuptools             50.3.2
six                    1.15.0
tensorboard            2.2.2
tensorboard-plugin-wit 1.7.0
tensorflow-estimator   2.2.0
tensorflow-gpu         2.2.0
termcolor              1.1.0
urllib3                1.25.11
Werkzeug               1.0.1
wheel                  0.35.1
wrapt                  1.12.1
zipp                   3.3.1
"
44153,bool disable_per_channel: per-channel vs. per-layer quantization,"I want to switch the quantization scheme to either per-tensor or per-channel quantization e.g. [disable_per_channel](https://github.com/tensorflow/tensorflow/blob/762df3454fa85fa0b842fcf1a08429d093835bc5/tensorflow/compiler/mlir/lite/quantization/quantization_config.h#L53) set as true (no quantization per-channel) within from [quantization_config.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/quantization/quantization_config.h)
```
  // When set to true, quantization will be done per-tensor. Currently, this
  // option is only valid when the quantization parameters need to be created by
  // scanning the constant content (post-training quantization or QAT without
  // weight FakeQuant).
  bool disable_per_channel = false;

```

Do you have any example to switch between either per-channel or per-layer quantization with [Python API](https://www.tensorflow.org/lite/convert#python_api) or [Command Line Tool](https://www.tensorflow.org/lite/convert#cmdline) call?

Is it possible somehow to use [`mlir_quantize`](https://github.com/tensorflow/tensorflow/blob/2396803aa12cff02dbc0aaad05c7614a4976843f/tensorflow/lite/python/convert.py#L126) function, if so, how?
"
44152,Issue running test_hello_world_test on MAC,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC OS Catalina
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:na
- TensorFlow installed from (source or binary): source
- TensorFlow version:


**Describe the problem**
After cloning tensorflow.git , cd in tensorflow, I'm trying to run the hello_world_test on Mac and get the following error following the gmake command :

tensorflow/lite/micro/tools/make/Makefile:413: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
tensorflow/lite/micro/tools/make/Makefile:413: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/hello_world_test tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/examples/hello_world/hello_world_test.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/examples/hello_world/model.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/lib/libtensorflow-microlite.a -Wl,--fatal-warnings -Wl,--gc-sections -lm -framework Foundation -framework AudioToolbox
ld: unknown option: --fatal-warnings
clang: error: linker command failed with exit code 1 (use -v to see invocation)
gmake: *** [tensorflow/lite/micro/examples//hello_world/Makefile.inc:34: tensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/hello_world_test] Error 1


**Provide the exact sequence of commands / steps that you executed before running into the problem**

gmake -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test


**Any other info / logs**
gmake --version
GNU Make 4.3
Built for x86_64-apple-darwin19.2.0

using gmake as my make version is 3.81"
44151, model = GDO.minimize(totalLoss) TypeError: minimize() missing 1 required positional argument: 'var_list',"*i am trying to use Gradient Descent Optimizer in this function ""wx+b"".*


**the code is:**

    import os
    
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    
    import tensorflow as tf
    
    w = tf.Variable([5.0], tf.float32)
    b = tf.Variable([7.0], tf.float32)
    
    def linearRegressionModel(x):
        return w * x + b
    
    def totalLoss(x, y):
        return tf.reduce_sum(tf.square(linearRegressionModel(x) - y))
    
    GDO = tf.keras.optimizers.SGD(0.01)
    model = GDO.minimize(totalLoss)
    for i in range(10000):
        model(x=[0, 1, 2, 3, 4, 5], y=[9, 7, 5, 3, 1, -1])
    print(w, b)

the result is:

**line 20, in <module>
    model = GDO.minimize(totalLoss)
TypeError: minimize() missing 1 required positional argument: 'var_list'
Process finished with exit code 1**

"
44147,[Q] Graph Transformer API,When will TFv2 support python graph transforms API so that we can use `from tensorflow.tools.graph_transforms import TransformGraph`
44146,"Report: AutoGraph could not transform, module 'gast' has no attribute 'Index'","
**System information**
- source: **https://www.tensorflow.org/tutorials/text/transformer**
- OS Platform: **ArchLinux 5.8.14 x86_64**
- TensorFlow installed from (source or binary): **binary** : https://www.archlinux.org/packages/community/x86_64/tensorflow-cuda/
- TensorFlow version (use command below): **2.3.1** -2
- Python version: **3.8.6**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **11.1.0** -2 https://www.archlinux.org/packages/community/x86_64/cuda/
- GPU model and memory: GeForce GTX 1660 SUPER computeCapability: 7.5,  22 deviceMemorySize: 5.80GiB


**Describe the current behavior**
```
WARNING:tensorflow:AutoGraph could not transform 
<function train_step at 0x7f15c0790ee0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, 
set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
```

**Describe the expected behavior**

**Standalone code to reproduce the issue**
python transformer.py

**Other info / logs** Include any logs or source code that would be helpful to
[transformer.zip](https://github.com/tensorflow/tensorflow/files/5402845/transformer.zip)


"
44143,"import Error. used on ubuntu,apache","## env
- ubuntu:16 or 18
- apache:2.4
- python:3.6.9
- tensorflow:2.x.x
- django:2.x

I don't have a clear reason.
This issue error message is not found in `/var/log/apache2/error.log`.

## Reproduction method.
1. `conda create -n django python==3.6.9` or `pyenv install python3.6.9`
2. `conda install tensorflow` or `pip install tensorflow`
3. make django simple project
```python:views.py
from django.shortcuts import render, redirect
import tensorflow as tf

def func(request):
    return redirect(""https://www.google.com/"")
```
4. deploy by Apache2.4 used with mod_wsgi
``` 
~/etc/apache2/sites-available/django.conf~
LoadModule wsgi_module /home/ubuntu/anaconda3/envs/django/lib/python3.6/site-packages/mod_wsgi/server/mod_wsgi-py36.cpython-36m-x86_64-linux-gnu.so
WSGIPythonHome /home/ubuntu/anaconda3/envs/django
WSGIPythonPath /home/ubuntu/django_project:/home/ubuntu/anaconda3/envs/django/lib/python3.6/site-packages
WSGIScriptAlias / /home/ubuntu/django_project/project/wsgi.py
<Directory /home/ubuntu/django_project/project>
  <Files wsgi.py>
    Require all granted
  </Files>
</Directory>
Alias /static/ /home/ubuntu/django_project/static/
<Directory /home/ubuntu/django_project/static>
  Require all granted
</Directory>
```
5. Access 
6. Maybe you will be kept waiting forever ."
44142,"erfp1, erfm1 for more accurate results with single/half precision.","**System information**
- TensorFlow version (you are using): 2.4.0
- Are you willing to contribute it (Yes/No): Yes.

**Describe the feature and the current behavior/state.**

TF contains an implementation of `erf` function. Sometimes it is desirable to compute a probability of a small interval in a Gaussian, i.e. `P(x - eps < X < x)`. This can be done by using erf: `0.5 * (tf.math.erf(x) - tf.math.erf(x-eps))`. Unfortunately, in case of dealing with smaller precision of floating point numbers (single or half) this computation will often lead to zero, i.e:

```
Input: tf.math.erf(-4)
Output:  <tf.Tensor: shape=(), dtype=float32, numpy=-1.0>
Input: tf.math.erf(-4.1)
Output:  <tf.Tensor: shape=(), dtype=float32, numpy=-1.0>
```
One of the reasons why this happens is probably because of inability of floating point numbers to represent numbers around 1 with a sufficient resolution. I believe the behaviour could be improved if there were two additional functions provided:

`erfp1(x)` - that computes `erf(x) + 1` using an approximation that makes it numerically more accurate for negative x
`erfm1(x)`- that computes `erf(x) - 1` using an approximation that makes it numerically more accurate for positive x

**Will this change the current api? How?**

Just two new functions are added.

**Who will benefit with this feature?**

The developers who work with probabilistic models and need to create differentiable models with `log(P(x - eps < X < x))`

**Any Other info.**
One of the questions that I have is how the approximation is done currently, i.e. what kind of formula is used?
"
44141,ModuleNotFoundError: No module named 'tensorflow.python.tools',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04.1 LTS (Focal Fossa)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.3.1
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: pip using a virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

```
$ . venv/bin/activate
$ python
Python 3.8.5 (default, Jul 28 2020, 12:59:40) 
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ubuntu/code/tfr-transforms/venv/lib/python3.8/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
ModuleNotFoundError: No module named 'tensorflow.python.tools'
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
$ pip freeze
absl-py==0.10.0
appdirs==1.4.4
astunparse==1.6.3
attrs==20.2.0
autopep8==1.5.4
boto3==1.14.33
boto3-stubs==1.14.33.0
botocore==1.17.63
cachetools==4.1.1
certifi==2020.6.20
cffi==1.14.3
cfgv==3.2.0
chardet==3.0.4
cycler==0.10.0
Cython==0.29.21
dataclasses==0.6
dill==0.3.2
distlib==0.3.1
dm-tree==0.1.5
docutils==0.15.2
filelock==3.0.12
findspark==1.4.2
flake8==3.8.4
flake8-annotations==2.4.1
flake8-colors==0.1.6
future==0.18.2
gast==0.3.3
gin-config==0.3.0
google-api-core==1.22.4
google-api-python-client==1.12.3
google-auth==1.22.1
google-auth-httplib2==0.0.4
google-auth-oauthlib==0.4.1
google-cloud-bigquery==2.1.0
google-cloud-core==1.4.3
google-crc32c==1.0.0
google-pasta==0.2.0
google-resumable-media==1.1.0
googleapis-common-protos==1.52.0
grpcio==1.32.0
h5py==2.10.0
httplib2==0.18.1
identify==1.5.6
idna==2.10
importlib-metadata==1.7.0
importlib-resources==3.0.0
iniconfig==1.1.1
isort==5.5.4
jmespath==0.10.0
kaggle==1.5.8
Keras-Preprocessing==1.1.2
kiwisolver==1.2.0
linecache2==1.0.0
Markdown==3.3.1
matplotlib==3.3.2
mccabe==0.6.1
mypy-boto3==1.14.33.0
mypy-boto3-ec2==1.14.33.0
mypy-boto3-emr==1.14.33.0
mypy-boto3-s3==1.14.33.0
nodeenv==1.5.0
numpy==1.18.5
oauthlib==3.1.0
opencv-python-headless==4.4.0.44
opt-einsum==3.3.0
packaging==20.4
pandas==1.1.3
Pillow==8.0.0
pluggy==0.13.1
pre-commit==2.7.1
promise==2.3
proto-plus==1.10.2
protobuf==3.13.0
psutil==5.7.2
py==1.9.0
py-cpuinfo==7.0.0
py4j==0.10.9
pyasn1==0.4.8
pyasn1-modules==0.2.8
pycodestyle==2.6.0
pycparser==2.20
pyflakes==2.2.0
pyparsing==2.4.7
pyspark==2.4.5
pyspark-stubs==3.0.0.post1
pytest==6.1.1
python-dateutil==2.8.1
python-slugify==4.0.1
pytz==2020.1
PyYAML==5.3.1
requests==2.24.0
requests-oauthlib==1.3.0
rsa==4.6
s3transfer==0.3.3
scipy==1.5.3
sentencepiece==0.1.91
six==1.15.0
slugify==0.0.1
spark-data-testing==0.1.1
spark-testing-base==0.10.0
tensorboard==2.3.0
tensorboard-plugin-wit==1.7.0
tensorflow==2.3.1
tensorflow-addons==0.11.2
tensorflow-datasets==4.0.1
tensorflow-estimator==2.3.0
tensorflow-hub==0.9.0
tensorflow-metadata==0.24.0
tensorflow-model-optimization==0.5.0
tensorflow-ranking==0.3.2
tensorflow-serving-api==2.3.0
termcolor==1.1.0
text-unidecode==1.3
tf-models-official==2.3.0
tf-slim==1.1.0
toml==0.10.1
tqdm==4.50.2
traceback2==1.4.0
typeguard==2.10.0
unittest2==1.1.0
uritemplate==3.0.1
urllib3==1.25.10
virtualenv==20.0.35
Werkzeug==1.0.1
wrapt==1.12.1
zipp==3.3.1
```"
44139,Image Recognition Experimental main.cc resolver name error,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 2.4.0
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): mbed Discovery STM32 F746NG

**Describe the problem**
The example project ""Image Recognition Experimental"" does not build correctly due to a name error of the resolver argument in the statement ""tflite::MicroInterpreter interpreter(model, **resolver**, tensor_arena, tensor_arena_size, error_reporter);"" of the main.cc source. The argument name **resolver** should be replaced by **micro_op_resolver**. 

**Please provide the exact sequence of commands/steps when you ran into the problem**
Making/building the project:
 -) make -f tensorflow/lite/micro/tools/make/Makefile TAGS=disco_f746ng generate_image_recognition_mbed_project
 -) cd tensorflow/lite/micro/tools/make/gen/linux_x86_64/prj/image_recognition/mbed/
 -) mbed config root . 
 -) mbed deploy
 -) mbed compile -m DISCO_F746NG -t GCC_ARM --profile release

Pull request: #44136"
44138,TypeError: 'InputLayer' object is not iterable,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.6
- TensorFlow installed from (source or binary): using pip3 install
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.8

**Describe the current behavior**
When I try to convert tensorflow model to CoreML model using coremltools I get this error: TypeError: 'InputLayer' object is not iterable

**Describe the expected behavior**
Tensorflow model should convert to CoreML model without any errors.

**Standalone code to reproduce the issue**
model.save(modelFolder)

coreMLModel = convert(modelFolder, input_names = ['input'], output_names = ['output'])
coreMLModel.save(folderManager.modelsFolder)
"
44137,CUDA_ERROR_NOT_INITIALIZED when using multiprocessing with ImageDataGenerator + random hue preprocessing function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Red Hat 7.4
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): GCC 7.3.0
- CUDA/cuDNN version: CUDA 10.1/cuDNN 7.6.5 

**Describe the current behavior**

Currently using ImageDataGenerator to load my data and apply random augmentations during training. This works fine with multiprocessing (setting `use_multiprocessing=True` in `model.fit`), until I add a preprocessing function into my data generator to perform random hue augmentations (using `tf.image.random_hue`). Doing this gives me the following error: 
`2020-10-19 18:33:13.420481: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error
`

I've also tried using `tf.image.stateless_random_hue` from `tf-nightly`, since that adjusts the hue deterministically, however I run into the same issue regardless. 

It works as expected when I set `use_multiprocessing=False`, however since I'm training on a large dataset this would create a significant bottleneck, so any possible fixes or workarounds would be greatly appreciated.

**Standalone code to reproduce the issue**
(example code assuming there are _train_ and _test_ directories contain 2 classes of images to be read in using `flow_from_directory`)

```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Conv2D, BatchNormalization,
                                     MaxPool2D, Flatten, Dense)
from tensorflow.keras.optimizers import Adam
from tensorflow.image import random_hue
import numpy as np

# random hue augmentations - PROBLEMATIC WITH MULTIPROCESSING
def color_augmentation(image):
    return random_hue(image, 0.1)

# get the data generators
# apply random augmentations during training including hue augmentations
train_datagen = ImageDataGenerator(
    rescale=1./255,
    vertical_flip = True,
    horizontal_flip = True,
    preprocessing_function = color_augmentation
)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    'train/',
    target_size = (96, 96),
    batch_size = 32,
)

test_generator = test_datagen.flow_from_directory(
    'test/',
    target_size = (96, 96),
    batch_size = 32,
)

# create model
model = Sequential()
model.add(Conv2D(filters=32, strides=1,input_shape=(96,96,3),
                 activation='relu', kernel_size=3, padding='same'))
model.add(BatchNormalization())
model.add(MaxPool2D())
model.add(Conv2D(filters=64, strides=1, activation='relu',
                 kernel_size=3, padding='same'))
model.add(BatchNormalization())
model.add(MaxPool2D())
model.add(Flatten())
model.add(Dense(units=2, activation='softmax'))

# compile model
model.compile(
    optimizer=Adam(learning_rate=0.01),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# fit the model, using multiprocessing
history = model.fit(
    x = train_generator,
    steps_per_epoch = len(train_generator),
    epochs = 5,
    verbose = 1,
    validation_data = test_generator,
    validation_steps = len(test_generator),
    workers = 4,
    use_multiprocessing = True,
    max_queue_size = 8
)

```

**Other info / logs**
```
2020-10-19 19:34:54.590677: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-19 19:34:57.529118: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-19 19:34:57.547866: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz
2020-10-19 19:34:57.552945: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555558e11bd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-19 19:34:57.552983: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-19 19:34:57.556307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-10-19 19:34:57.784089: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555558ea46c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-10-19 19:34:57.784175: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2020-10-19 19:34:57.786709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:25:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2020-10-19 19:34:57.786764: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-19 19:34:57.791365: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-19 19:34:57.795504: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-19 19:34:57.797375: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-19 19:34:57.801231: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-19 19:34:57.803387: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-19 19:34:57.811940: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-19 19:34:57.815365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-10-19 19:34:57.815402: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-19 19:34:58.501529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-19 19:34:58.501575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-10-19 19:34:58.501581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-10-19 19:34:58.503865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/device:GPU:0 with 14951 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:25:00.0, compute capability: 6.0)
2020-10-19 19:34:58.862187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:25:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s

...

WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.
Found GPU at: /device:GPU:0
Found 1200 images belonging to 2 classes.
Found 200 images belonging to 2 classes.
Epoch 1/5
2020-10-19 19:34:59.947779: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-10-19 19:34:59.949834: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-10-19 19:34:59.952581: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-10-19 19:34:59.956651: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-10-19 19:35:00.090631: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-10-19 19:35:00.135802: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-10-19 19:35:00.171513: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-10-19 19:35:00.221924: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-10-19 19:35:00.358671: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error
2020-10-19 19:35:00.403905: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error
```"
44134,Invoke returns NaN values when performing on floating points,"@tensorflow/micro

**System information**
- Host Platform: Ubuntu 18.04.4 LTS
- TensorFlow installed from source,
- Tensorflow version: 86726adc39a1
- Target platform: STM32H745 (ARM M7)

I have Keras model trained on host platform. I want to use this model on my target device, after conversion to .tflite format. Once I perform conversion with quantization and obtain INT8 weights values, model works fine and gives correct predictions.
But when I try to convert model without quantization and try to run it with floating points values, inout vector and output vector contain NaN values after calling Invoke(). 

Input vector after calling invoke on INT8 weights:
![int](https://user-images.githubusercontent.com/58625554/96422788-c5060600-11f8-11eb-8d23-6e7a12bd7c27.PNG)

Input vector after calling invoke on Floating Points weights:
![float](https://user-images.githubusercontent.com/58625554/96422840-d5b67c00-11f8-11eb-9cf6-13be10c915a2.PNG)

Code that I use to convert with quantization:

```python
import tensorflow as tf
import tensorflow.keras as keras
import numpy as np
import pickle

model = keras.models.load_model('data/models/best_model.hdf5')

X_train = pickle.load(open(""dataset_from_target.pickle"", ""rb""))['x_train']

converter = tf.lite.TFLiteConverter.from_keras_model(model)
model_quant_tflite = converter.convert()


def representative_dataset():
  for i in range(1000):
    yield([np.expand_dims(X_train[i].astype(np.float32), axis=0)])


converter.optimizations = [tf.lite.Optimize.DEFAULT]

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]

converter.representative_dataset = representative_dataset
model_tflite = converter.convert()

open(""model_target.tflite"", ""wb"").write(model_tflite)
```



Code that I use to convert with no quantization:

```python
import tensorflow as tf
import tensorflow.keras as keras


model = keras.models.load_model('data/models/best_model.hdf5')

converter = tf.lite.TFLiteConverter.from_keras_model(model)
model_no_quant_tflite = converter.convert()

# Set the optimization flag.
converter.optimizations = [tf.lite.Optimize.DEFAULT]
model_tflite = converter.convert()

open(""model_target_new_data_no_quant.tflite"", ""wb"").write(model_tflite)
```


Problem is not with RAM memory, which I have already checked. NaN values in 'input->data.f' results in all NaN values in 'output->data.f' which makes predictions impossible. Before calling invoke, input vector contains correct values. 


"
44133,Being able to register custom DataAdapter that inherit from the base class,"**System information**
- TensorFlow version (you are using): 2.3
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Currently the list of available data adapter are registered here:
https://github.com/tensorflow/tensorflow/blob/6f440e215d0b4dfa2952729d8592af28976e48ef/tensorflow/python/keras/engine/data_adapter.py#L946

To support new use case such as supporting Py arrow array or Listarray (that will be converted to ragged). It could be nice to be able to register a custom data adapter to call keras directly with a dict of List Array or even with a Pandas Dataframe. 

**Will this change the current api? How?**

no.
**Who will benefit with this feature?**
Users with custom data need. 

**Any Other info.**
"
44130,Have installed tensorflow but still unbale to access it in Jupyter Error: ModuleNotFoundError: No module named 'tensorflow',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44129,Tensorflow Lite with Nvidia GPU on Ubuntu happen coredump when creating delegate.,"Hi Supporters,

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master branch (2.3.1)
- Python version: 3.8
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce GTX 1050 Ti, 4Gb

**Describe the current behavior**
I tried to inference TF lite model with GPU, everything is normal with TF using CPU, but when create delagate, coredump happens immediately.

**Standalone code to reproduce the issue**
```
#include <iostream>
#include <memory>

#include <tensorflow/lite/interpreter.h>
#include <tensorflow/lite/kernels/register.h>
#include <tensorflow/lite/string_util.h>
#include <tensorflow/lite/model.h>
#include <tensorflow/lite/delegates/gpu/delegate.h>
#include <tensorflow/lite/delegates/gpu/gl_delegate.h>

int main()
{
    std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(""superpoint_640x480.tflite"");
    if (!model)
    {
        std::cerr << ""Failed to mmap tflite model"" << std::endl;
        return -1;
    }
    tflite::ops::builtin::BuiltinOpResolver resolver;
    std::unique_ptr<tflite::Interpreter> interpreter;
    if (tflite::InterpreterBuilder(*model.get(), resolver)(&interpreter) != kTfLiteOk)
    {
        std::cerr << ""Failed to interpreter tflite model"" << std::endl;
        return -1;
    }

    // auto* delegate = TfLiteGpuDelegateCreate(nullptr);
    // if (interpreter->ModifyGraphWithDelegate(delegate) == kTfLiteOk)
    // {
    //     std::cerr << ""Failed to enable GPU."" << std::endl;
    //     return -1;
    // }

    //  NEW: Prepare custom options with feature enabled.
    TfLiteGpuDelegateOptionsV2 options = TfLiteGpuDelegateOptionsV2Default();
    options.experimental_flags = TFLITE_GPU_EXPERIMENTAL_FLAGS_NONE;

    auto* delegate = TfLiteGpuDelegateV2Create(&options);
    if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk){
        std::cerr << ""Failed to register delegate"" << std::endl;
        return -1;
    }

    return 0;
}
```
TF Lite model file : 
[superpoint_640x480.zip](https://github.com/tensorflow/tensorflow/files/5397657/superpoint_640x480.zip)


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
[backtrace.txt](https://github.com/tensorflow/tensorflow/files/5397656/backtrace.txt)

As I know so far, TF Lite on GPU supported PAD ops, but don't know why the coredump happened here.
Have you seen this ? Could you help me overcome this kind of issue ?
Many thanks in advance."
44128,cuda_11.1.0_456.43_win10 + cudnn-11.1-windows-x64-v8.0.4.30 + master branch= some errors,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 20H1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): build from master branch
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda_11.1.0_456.43_win10 + cudnn-11.1-windows-x64-v8.0.4.30
- GPU model and memory: nvidia 1070 ti / 8Gb

I successfully built Tensorflow from master brach with cuda optimization.
cuda_11.1.0_456.43_win10 + cudnn-11.1-windows-x64-v8.0.4.30

when I try to train a model I get errors that I don't understand.
tensorflow 2.3 the model was built without errors.
Log:

coreClock: 1.683GHz coreCount: 19 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s
2020-10-18 17:32:44.007532: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-10-18 17:32:44.386490: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-10-18 17:32:44.604977: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-10-18 17:32:44.632185: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-10-18 17:32:44.866696: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_11.dll
2020-10-18 17:32:45.041032: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-10-18 17:32:45.055357: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-10-18 17:32:45.055986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-10-18 17:32:45.059177: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Instructions AVX2
2020-10-18 17:47:53.948993: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-10-18 17:47:55.715569: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-10-18 17:47:55.716176: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2020-10-18 17:47:55.729870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1070 Ti computeCapability: 6.1
coreClock: 1.683GHz coreCount: 19 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s
2020-10-18 17:47:55.729946: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-10-18 17:47:55.735471: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-10-18 17:47:55.738640: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-10-18 17:47:55.739592: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-10-18 17:47:55.747533: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_11.dll
2020-10-18 17:47:55.750147: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-10-18 17:47:55.750569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-10-18 17:47:55.750671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-10-18 17:47:55.751166: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-18 17:47:55.751629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1070 Ti computeCapability: 6.1
coreClock: 1.683GHz coreCount: 19 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s
2020-10-18 17:47:55.751780: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-10-18 17:47:55.752125: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-10-18 17:47:55.752364: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-10-18 17:47:55.752607: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-10-18 17:47:55.752854: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_11.dll
2020-10-18 17:47:55.753097: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-10-18 17:47:55.753343: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-10-18 17:47:55.753635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-10-18 17:47:56.173367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-18 17:47:56.173485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2020-10-18 17:47:56.173812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2020-10-18 17:47:56.174571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6696 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-10-18 17:47:56.175262: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding (Embedding)        (None, 131, 64)           18151104
_________________________________________________________________
lstm (LSTM)                  (None, 131, 64)           33024
_________________________________________________________________
lstm_1 (LSTM)                (None, 64)                33024
_________________________________________________________________
dense (Dense)                (None, 5000)              325000
_________________________________________________________________
dense_1 (Dense)              (None, 5000)              25005000
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 5001
_________________________________________________________________
activation (Activation)      (None, 1)                 0
=================================================================
Total params: 43,552,153
Trainable params: 43,552,153
Non-trainable params: 0
_________________________________________________________________
None
2020-10-18 17:47:56.892579: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 1)
Epoch 1/5
2020-10-18 17:47:58.976202: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:596] layout failed: Invalid argument: Size of values 2 does not match size of permutation 4 @ fanin shape insequential/dense/BiasAdd-0-TransposeNHWCToNCHW-LayoutOptimizer
2020-10-18 17:47:59.278004: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-10-18 17:47:59.600803: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
199/200 [============================>.] - ETA: 0s - loss: 0.6893 - accuracy: 0.50622020-10-18 17:48:18.028039: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:596] layout failed: Invalid argument: Size of values 2 does not match size of permutation 4 @ fanin shape insequential/dense/BiasAdd-0-TransposeNHWCToNCHW-LayoutOptimizer
200/200 [==============================] - 22s 93ms/step - loss: 0.6892 - accuracy: 0.5062 - val_loss: 0.5702 - val_accuracy: 0.5098
Epoch 2/5
122/200 [=================>............] - ETA: 6s - loss: 0.4413 - accuracy: 0.50562020-10-18 17:48:29.581544: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2020-10-18 17:48:29.581677: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:220] Unexpected Event status: 1

Errors:
tensorflow/core/grappler/optimizers/meta_optimizer.cc:596] layout failed: Invalid argument: Size of values 2 does not match size of permutation 4 @ fanin shape insequential/dense/BiasAdd-0-TransposeNHWCToNCHW-LayoutOptimizer

tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2020-10-18 17:48:29.581677: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:220] Unexpected Event status: 1


what do these errors mean?

"
44127,Optionally return attention weights in tf.keras.layers.BaseDenseAttention to allow us to visualize them,"**Describe the feature and the current behavior/state.**

Currently, when using attention in Keras, the weights are calculated in tf.keras.layers.BaseDenseAttention._apply_scores, and then discarded with only the score returned. It would be nice if the weights could be returned as they do in the ""Neural machine translation with attention"" TensorFlow Core tutorial (you can find the tutorial [here](https://www.tensorflow.org/tutorials/text/nmt_with_attention#write_the_encoder_and_decoder_model), see in the class BahdanauAttention in the tutorial how the attention_weights are calculated and then returned). This would be nice as it would allow us to visualize the scores like they do in the tutorial, which would help us interpret our models.

**Will this change the current api? How?**

Yes. As well as returning the regular output, it should also return the attention weights. 

**Who will benefit with this feature?**

Everyone who wants to better understand their model that uses attention. Being able to visualize the attention weights helps understand what the model is paying attention to in the input when predicting the output. 

**Any Other info.**

I'm surprised this is not already implemented in TensorFlow, so if there's any way I can achieve this without this change, please say, as I'm sure it will help others in the future too."
44126,keras.models.load_model() checking for keras configuration mismatch,"**System information**
- TensorFlow version (you are using): 2.3.1
- Are you willing to contribute it (Yes/No): No (depending on how time-consuming it ends up being)



**Describe the feature and the current behavior/state.**
Currently, if the model is saved with the `""image_data_format"": ""channels_first""` option configured in keras.json it produces a shapes mismatch when loaded in an environment with `""image_data_format"": ""channels_last""` option. It's really difficult to debug since all issues found in Google point to mistakes in how the network is put together and results in hours wasted in looking for possible culprits.

I suggest that relevant parameters from keras.json configuration can be saved together with the model's metadata and keras.models.load_model() function could check for mismatch that will prevent the model from being loaded or running correctly in the environment it is being loaded in.

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
The world

**Any Other info.**
"
44125,No gradients provided for any variable when doing binarization,"I have written a Lambda layer that converts an input variable of range `(0, 1)` to either 0 or 1, i.e. the layer binarizes the inputs. I did this by using `K.random_uniform(shape=K.shape(x)) <= x`. However, when wantint to train the model, I'm getting the error message, that no gradients are provided for any variable. How do I have to change the code such that my idea works? TF version is 2.2.0.

    import numpy as np

    from keras import backend as K
    from keras.layers import *
    from keras.models import Model


    def binarize(d):
        x1, x2 = d
        return x1 * K.cast(K.random_uniform(shape=K.shape(x2)) <= x2, 'float32')


    inp = Input((1,))
    var = Dense(1, activation='sigmoid')(inp)
    out = Lambda(binarize)([inp, var])

    model = Model(inp, out)
    model.compile(loss='mse', optimizer='sgd')

    x = np.random.normal(size=(128, 1))
    y = x >= 0
    model.train_on_batch(x, y)"
44124,Running TFlite on mobile device GPU fails  - Ops not supported,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source):
nightly

.
.
INFO: Created TensorFlow Lite delegate for GPU.
ERROR: Following operations are not supported by GPU delegate:
DELEGATE TfLiteFlexDelegate: Operation is not supported.
EQUAL: Operation is not supported.
SPLIT: Operation is not supported.
WHILE: Operation is not supported.
62 operations will run on the GPU, and the remaining 907 operations will run on the CPU.
ERROR: TfLiteGpuDelegate Prepare: No shader implementation for transpose
ERROR: Node number 1001 (TfLiteGpuDelegate) failed to prepare.

ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
ERROR: Node number 813 (FlexTensorScatterUpdate) failed to prepare.
.
.
unfortunately can't send the code.
"
44123,Tensorflow model inference different than loading weights from checkpoint into equivalent model in keras,"Python version 3.8.3
Tensorflow 1.14 and 2.0 used
Windows 10
CUDA 10, Cudnn 7.6.5


I can send the variable or checkpoint file if this would be helpful.


I am trying to convert the following tensorflow model to keras so I can upload the weights directly, but I am not getting the correct predictions after I do this can someone tell me if I am making an obvious mistake? 


    class LinearModel(object):
      """""" A simple Linear+RELU model """"""
    
      def __init__(self,
                   linear_size,
                   num_layers,
                   residual,
                   batch_norm,
                   max_norm,
                   batch_size,
                   learning_rate,
                   summaries_dir,
                   predict_14=False,
                   dtype=tf.float32):
        
   
        self.HUMAN_2D_SIZE = 16 * 2
    
        self.HUMAN_3D_SIZE = 14 * 3 if predict_14 else 16 * 3
    
        self.input_size  = self.HUMAN_2D_SIZE
        self.output_size = self.HUMAN_3D_SIZE
    
        self.isTraining = tf.placeholder(tf.bool,name=""isTrainingflag"")
        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")
    
        # Summary writers for train and test runs
        self.train_writer = tf.summary.FileWriter( os.path.join(summaries_dir, 'train' ))
        self.test_writer  = tf.summary.FileWriter( os.path.join(summaries_dir, 'test' ))
    
        self.linear_size   = linear_size
        self.batch_size    = batch_size
        self.learning_rate = tf.Variable( float(learning_rate), trainable=False, dtype=dtype, name=""learning_rate"")
        self.global_step   = tf.Variable(0, trainable=False, name=""global_step"")
        decay_steps = 100000  # empirical
        decay_rate = 0.96     # empirical
        self.learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, decay_steps, decay_rate)
    
        # === Transform the inputs ===
        with vs.variable_scope(""inputs""):
    
          # in=2d poses, out=3d poses
          enc_in  = tf.placeholder(dtype, shape=[None, self.input_size], name=""enc_in"")
          dec_out = tf.placeholder(dtype, shape=[None, self.output_size], name=""dec_out"")
    
          self.encoder_inputs  = enc_in
          self.decoder_outputs = dec_out
    
        # === Create the linear + relu combos ===
        with vs.variable_scope( ""linear_model"" ):
    
          # === First layer, brings dimensionality up to linear_size ===
          w1 = tf.get_variable( name=""w1"", initializer=kaiming, shape=[self.HUMAN_2D_SIZE, linear_size], dtype=dtype )
          b1 = tf.get_variable( name=""b1"", initializer=kaiming, shape=[linear_size], dtype=dtype )
          w1 = tf.clip_by_norm(w1,1) if max_norm else w1
          y3 = tf.matmul( enc_in, w1 ) + b1
    
          if batch_norm:
            y3 = tf.layers.batch_normalization(y3,training=self.isTraining, name=""batch_normalization"")
          y3 = tf.nn.relu( y3 )
          y3 = tf.nn.dropout( y3, self.dropout_keep_prob )
    
          # === Create multiple bi-linear layers ===
          for idx in range( num_layers ):
            y3 = self.two_linear( y3, linear_size, residual, self.dropout_keep_prob, max_norm, batch_norm, dtype, idx )
    
          # === Last linear layer has HUMAN_3D_SIZE in output ===
          w4 = tf.get_variable( name=""w4"", initializer=kaiming, shape=[linear_size, self.HUMAN_3D_SIZE], dtype=dtype )
          b4 = tf.get_variable( name=""b4"", initializer=kaiming, shape=[self.HUMAN_3D_SIZE], dtype=dtype )
          w4 = tf.clip_by_norm(w4,1) if max_norm else w4
          y = tf.matmul(y3, w4) + b4
          # === End linear model ===
    
        # Store the outputs here
        self.outputs = y
        self.loss = tf.reduce_mean(tf.square(y - dec_out))
        self.loss_summary = tf.summary.scalar('loss/loss', self.loss)
    
        # To keep track of the loss in mm
        self.err_mm = tf.placeholder( tf.float32, name=""error_mm"" )
        self.err_mm_summary = tf.summary.scalar( ""loss/error_mm"", self.err_mm )
    
        # Gradients and update operation for training the model.
        opt = tf.train.AdamOptimizer( self.learning_rate )
        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    
        with tf.control_dependencies(update_ops):
    
          # Update all the trainable parameters
          gradients = opt.compute_gradients(self.loss)
          self.gradients = [[] if i==None else i for i in gradients]
          self.updates = opt.apply_gradients(gradients, global_step=self.global_step)
    
        # Keep track of the learning rate
        self.learning_rate_summary = tf.summary.scalar('learning_rate/learning_rate', self.learning_rate)
    
        # To save the model
        self.saver = tf.train.Saver( tf.global_variables(), max_to_keep=10 )
    
    
      def two_linear( self, xin, linear_size, residual, dropout_keep_prob, max_norm, batch_norm, dtype, idx ):
        """"""
        Make a bi-linear block with optional residual connection
    
        Returns
          y: the batch after it leaves the block
        """"""
    
        with vs.variable_scope( ""two_linear_""+str(idx) ) as scope:
    
          input_size = int(xin.get_shape()[1])
    
          # Linear 1
          w2 = tf.get_variable( name=""w2_""+str(idx), initializer=kaiming, shape=[input_size, linear_size], dtype=dtype)
          b2 = tf.get_variable( name=""b2_""+str(idx), initializer=kaiming, shape=[linear_size], dtype=dtype)
          w2 = tf.clip_by_norm(w2,1) if max_norm else w2
          y = tf.matmul(xin, w2) + b2
          if  batch_norm:
            y = tf.layers.batch_normalization(y,training=self.isTraining,name=""batch_normalization1""+str(idx))
    
          y = tf.nn.relu( y )
          y = tf.nn.dropout( y, dropout_keep_prob )
    
          # Linear 2
          w3 = tf.get_variable( name=""w3_""+str(idx), initializer=kaiming, shape=[linear_size, linear_size], dtype=dtype)
          b3 = tf.get_variable( name=""b3_""+str(idx), initializer=kaiming, shape=[linear_size], dtype=dtype)
          w3 = tf.clip_by_norm(w3,1) if max_norm else w3
          y = tf.matmul(y, w3) + b3
    
          if  batch_norm:
            y = tf.layers.batch_normalization(y,training=self.isTraining,name=""batch_normalization2""+str(idx))
    
          y = tf.nn.relu( y )
          y = tf.nn.dropout( y, dropout_keep_prob )
    
          # Residual every 2 blocks
          y = (xin + y) if residual else y
    
        return y

The pretrained weights for this model are stored in a checkpoint file which I extract using the following code:

    PATH_REL_META= #insert file path to checkpoint
    
    with tf.Session() as sess:
            
        #import graph
        saver=tf.train.import_meta_graph(PATH_REL_META)
        
        #load weights for graph
        saver.restore(sess,PATH_REL_META[:-5])
        
        #get all global variables 
        vars_global=tf.global_variables()
        
        #get their name and value and puth them into a dictionary
        sess.as_default()
        model_vars={}
        for var in vars_global:
            try:
                model_vars[var.name]=var.eval()
            except:
                print(""For var={}, an exception occurred"".format.name)

Then I define the keras model with the following code and load the weights from the dictionary:

    def two_linear(xin,linear_size,dropout_keep_prob,idx):
       # Args
       # xin: the batch that enters the block
       # linear_size: integer. The size of the linear units
       # residual: boolean. Whether to add a residual connection
       # dropout_keep_prob: float [0,1]. Probability of dropping something out
       # max_norm: boolean. Whether to clip weights to 1-norm
       # batch_norm: boolean. Whether to do batch normalization
       # Returns
       # y: batch after it leaves the block
       
       # Linear 1
       # he_uniform is the Kainming initilization in Keras
       
       
    
       y=tf.keras.layers.Dense(linear_size,kernel_initializer=""he_uniform"",
                               bias_initializer=""he_uniform"",
                               kernel_constraint=max_norm(1),
                               name=""two_linear/w2_""+str(idx))(xin)
       y=tf.keras.layers.BatchNormalization(name=""two_linear/batch_normalization1""+str(idx))(y,training=False)
       y=tf.keras.layers.Activation('relu')(y)
       y=tf.keras.layers.Dropout(dropout_keep_prob)(y,training=False)
       
       
       y=tf.keras.layers.Dense(linear_size,kernel_initializer=""he_uniform"",
                               bias_initializer=""he_uniform"",
                               kernel_constraint=max_norm(1),
                               name=""two_linear/w3_""+str(idx))(y)
       y=tf.keras.layers.BatchNormalization(name=""two_linear/batch_normalization2""+str(idx))(y,training=False)
       y=tf.keras.layers.Activation('relu')(y)
       y=tf.keras.layers.Dropout(dropout_keep_prob)(y,training=False)  
       
       
       y=tf.keras.layers.Add()([xin,y])
       
       
       return y
      
    def ModelDef(linear_size,dropout_keep_prob,num_layers):
        input_shape=(32,)
        xin=tf.keras.layers.Input(input_shape)
        #y_1=tf.keras.layers.Reshape((32,))(xin)
        
        y_1=tf.keras.layers.Dense(linear_size,kernel_initializer='he_uniform',
                                bias_initializer='he_uniform',
                                kernel_constraint=max_norm(1),
                                name=""linear_model/w1"")(xin)
        y_1=tf.keras.layers.BatchNormalization(name=""batch_norm/0"")(y_1,training=False)
        y_1=tf.keras.layers.Activation('relu')(y_1)
        y=tf.keras.layers.Dropout(dropout_keep_prob)(y_1,training=False)
        
        for idx in range(num_layers):
            y=two_linear(y,linear_size,dropout_keep_prob,idx)
            
        
        y=tf.keras.layers.Dense(16*3,kernel_initializer='he_uniform',
                            bias_initializer='he_uniform',
                            kernel_constraint=max_norm(1),
                            name=""linear_model/w4"")(y)
        
        model=tf.keras.Model(inputs=xin,outputs=y)
        
        return model
    
    #Create model and load weights
    
    weight_path=''
    with open(weight_path,""rb"") as pf:
        weights=pickle.load(pf)
        
    model=ModelDef(1024,.1,2)
    
    print(model.weights[0])
        model.get_layer(""linear_model/w1"").set_weights([tf.clip_by_norm(weights[""linear_model/w1:0""],1),weights[""linear_model/b1:0""]])
    model.get_layer(""batch_norm/0"").set_weights([weights['linear_model/batch_normalization/gamma:0']
                                                  ,weights['linear_model/batch_normalization/beta:0']
                                                ,weights['linear_model/batch_normalization/moving_mean:0'] 
                                                ,weights['linear_model/batch_normalization/moving_variance:0']])
    
    model.get_layer(""two_linear/w2_0"").set_weights([tf.clip_by_norm(weights[""linear_model/two_linear_0/w2_0:0""],1),
                                                    weights[""linear_model/two_linear_0/b2_0:0""]])
    model.get_layer(""two_linear/batch_normalization10"").set_weights([weights[""linear_model/two_linear_0/batch_normalization10/gamma:0""],
                                                            weights[""linear_model/two_linear_0/batch_normalization10/beta:0""],
                                                            weights[""linear_model/two_linear_0/batch_normalization10/moving_mean:0""],
                                                            weights[""linear_model/two_linear_0/batch_normalization10/moving_variance:0""]])
    
    model.get_layer(""two_linear/w3_0"").set_weights([tf.clip_by_norm(weights[""linear_model/two_linear_0/w3_0:0""],1),
                                                   weights[""linear_model/two_linear_0/b3_0:0""]])
    
    model.get_layer(""two_linear/batch_normalization20"").set_weights([weights[""linear_model/two_linear_0/batch_normalization20/gamma:0""],
                                                            weights[""linear_model/two_linear_0/batch_normalization20/beta:0""],
                                                            weights[""linear_model/two_linear_0/batch_normalization20/moving_mean:0""],
                                                            weights[""linear_model/two_linear_0/batch_normalization20/moving_variance:0""]])
    
    
    model.get_layer(""two_linear/w2_1"").set_weights([tf.clip_by_norm(weights[""linear_model/two_linear_1/w2_1:0""],1),
                                                    weights[""linear_model/two_linear_1/b2_1:0""]])
    model.get_layer(""two_linear/batch_normalization11"").set_weights([weights[""linear_model/two_linear_1/batch_normalization11/gamma:0""],
                                                            weights[""linear_model/two_linear_1/batch_normalization11/beta:0""],
                                                            weights[""linear_model/two_linear_1/batch_normalization11/moving_mean:0""],
                                                            weights[""linear_model/two_linear_1/batch_normalization11/moving_variance:0""]])
    
    model.get_layer(""two_linear/w3_1"").set_weights([tf.clip_by_norm(weights[""linear_model/two_linear_1/w3_1:0""],1),
                                                   weights[""linear_model/two_linear_1/b3_1:0""]])
    
    model.get_layer(""two_linear/batch_normalization21"").set_weights([weights[""linear_model/two_linear_1/batch_normalization21/gamma:0""],
                                                            weights[""linear_model/two_linear_1/batch_normalization21/beta:0""],
                                                            weights[""linear_model/two_linear_1/batch_normalization21/moving_mean:0""],
                                                            weights[""linear_model/two_linear_1/batch_normalization21/moving_variance:0""]])    
    
    model.get_layer(""linear_model/w4"").set_weights([tf.clip_by_norm(weights[""linear_model/w4:0""],1)
                                                    ,weights[""linear_model/b4:0""]])    

 


weights contains the dictionary created from the middle block of code. Inside the dictionary I have the following keys: 

    learning_rate:0
    global_step:0
    linear_model/w1:0
    linear_model/b1:0
    linear_model/batch_normalization/beta:0
    linear_model/batch_normalization/gamma:0
    linear_model/batch_normalization/moving_mean:0
    linear_model/batch_normalization/moving_variance:0
    linear_model/two_linear_0/w2_0:0
    linear_model/two_linear_0/b2_0:0
    linear_model/two_linear_0/batch_normalization10/beta:0
    linear_model/two_linear_0/batch_normalization10/gamma:0
    linear_model/two_linear_0/batch_normalization10/moving_mean:0
    linear_model/two_linear_0/batch_normalization10/moving_variance:0
    linear_model/two_linear_0/w3_0:0
    linear_model/two_linear_0/b3_0:0
    linear_model/two_linear_0/batch_normalization20/beta:0
    linear_model/two_linear_0/batch_normalization20/gamma:0
    linear_model/two_linear_0/batch_normalization20/moving_mean:0
    linear_model/two_linear_0/batch_normalization20/moving_variance:0
    linear_model/two_linear_1/w2_1:0
    linear_model/two_linear_1/b2_1:0
    linear_model/two_linear_1/batch_normalization11/beta:0
    linear_model/two_linear_1/batch_normalization11/gamma:0
    linear_model/two_linear_1/batch_normalization11/moving_mean:0
    linear_model/two_linear_1/batch_normalization11/moving_variance:0
    linear_model/two_linear_1/w3_1:0
    linear_model/two_linear_1/b3_1:0
    linear_model/two_linear_1/batch_normalization21/beta:0
    linear_model/two_linear_1/batch_normalization21/gamma:0
    linear_model/two_linear_1/batch_normalization21/moving_mean:0
    linear_model/two_linear_1/batch_normalization21/moving_variance:0
    linear_model/w4:0
    linear_model/b4:0
    beta1_power:0
    beta2_power:0
    linear_model/w1/Adam:0
    linear_model/w1/Adam_1:0
    linear_model/b1/Adam:0
    linear_model/b1/Adam_1:0
    linear_model/batch_normalization/beta/Adam:0
    linear_model/batch_normalization/beta/Adam_1:0
    linear_model/batch_normalization/gamma/Adam:0
    linear_model/batch_normalization/gamma/Adam_1:0
    linear_model/two_linear_0/w2_0/Adam:0
    linear_model/two_linear_0/w2_0/Adam_1:0
    linear_model/two_linear_0/b2_0/Adam:0
    linear_model/two_linear_0/b2_0/Adam_1:0
    linear_model/two_linear_0/batch_normalization10/beta/Adam:0
    linear_model/two_linear_0/batch_normalization10/beta/Adam_1:0
    linear_model/two_linear_0/batch_normalization10/gamma/Adam:0
    linear_model/two_linear_0/batch_normalization10/gamma/Adam_1:0
    linear_model/two_linear_0/w3_0/Adam:0
    linear_model/two_linear_0/w3_0/Adam_1:0
    linear_model/two_linear_0/b3_0/Adam:0
    linear_model/two_linear_0/b3_0/Adam_1:0
    linear_model/two_linear_0/batch_normalization20/beta/Adam:0
    linear_model/two_linear_0/batch_normalization20/beta/Adam_1:0
    linear_model/two_linear_0/batch_normalization20/gamma/Adam:0
    linear_model/two_linear_0/batch_normalization20/gamma/Adam_1:0
    linear_model/two_linear_1/w2_1/Adam:0
    linear_model/two_linear_1/w2_1/Adam_1:0
    linear_model/two_linear_1/b2_1/Adam:0
    linear_model/two_linear_1/b2_1/Adam_1:0
    linear_model/two_linear_1/batch_normalization11/beta/Adam:0
    linear_model/two_linear_1/batch_normalization11/beta/Adam_1:0
    linear_model/two_linear_1/batch_normalization11/gamma/Adam:0
    linear_model/two_linear_1/batch_normalization11/gamma/Adam_1:0
    linear_model/two_linear_1/w3_1/Adam:0
    linear_model/two_linear_1/w3_1/Adam_1:0
    linear_model/two_linear_1/b3_1/Adam:0
    linear_model/two_linear_1/b3_1/Adam_1:0
    linear_model/two_linear_1/batch_normalization21/beta/Adam:0
    linear_model/two_linear_1/batch_normalization21/beta/Adam_1:0
    linear_model/two_linear_1/batch_normalization21/gamma/Adam:0
    linear_model/two_linear_1/batch_normalization21/gamma/Adam_1:0
    linear_model/w4/Adam:0
    linear_model/w4/Adam_1:0
    linear_model/b4/Adam:0
    linear_model/b4/Adam_1:0

The predictions I am getting from the Keras model are way off from directly loading the tensorflow checkpoint data and making predictions. Is there something obvious I am missing that someone can point me in the right direction to correct?

Thanks in advance and sorry for the long post!
"
44122,Hello World Ubuntu 16.04 with nvidia 1660 super,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version:0.4.0
- Python version:2.7.12
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.12)
- CUDA/cuDNN version:V7.5.17
- GPU model and memory: NVIDIA 1660 SUPER GIGABYTE 6GB



**Describe the problem**

I do the command make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test to run the hello_world_test and print me this :

tensorflow/lite/micro/tools/make/Makefile:413: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
tensorflow/lite/micro/tools/make/Makefile:413: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/fully_connected.cc -o tensorflow/lite/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/micro/kernels/fully_connected.o
tensorflow/lite/micro/kernels/fully_connected.cc: In function TfLiteStatus tflite::{anonymous}::EvalQuantized(TfLiteContext*, TfLiteNode*, const tflite::{anonymous}::OpData&, const TfLiteEvalTensor*, const TfLiteEvalTensor*, const TfLiteEvalTensor*, TfLiteEvalTensor*):
tensorflow/lite/micro/kernels/fully_connected.cc:135:14: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]
 TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,
              ^
tensorflow/lite/micro/kernels/fully_connected.cc:135:14: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]
tensorflow/lite/micro/kernels/fully_connected.cc:135:14: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]
tensorflow/lite/micro/kernels/fully_connected.cc:135:14: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]
In file included from ./tensorflow/lite/kernels/internal/common.h:29:0,
                 from tensorflow/lite/micro/kernels/fully_connected.cc:20:
./tensorflow/lite/kernels/internal/types.h:197:31: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]
     TFLITE_DCHECK_LT(i, size_);
                               ^
./tensorflow/lite/kernels/internal/types.h:197:31: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]
     TFLITE_DCHECK_LT(i, size_);
                               ^
tensorflow/lite/micro/kernels/fully_connected.cc: In function TfLiteStatus tflite::{anonymous}::Eval(TfLiteContext*, TfLiteNode*):
tensorflow/lite/micro/kernels/fully_connected.cc:203:14: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]
 TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
              ^
tensorflow/lite/micro/kernels/fully_connected.cc:203:14: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]
tensorflow/lite/micro/kernels/fully_connected.cc:203:14: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]
In file included from ./tensorflow/lite/kernels/internal/common.h:29:0,
                 from tensorflow/lite/micro/kernels/fully_connected.cc:20:
./tensorflow/lite/kernels/internal/types.h:197:31: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]
     TFLITE_DCHECK_LT(i, size_);
                               ^
./tensorflow/lite/kernels/internal/types.h:197:31: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]
     TFLITE_DCHECK_LT(i, size_);
                               ^
cc1plus: all warnings being treated as errors
tensorflow/lite/micro/tools/make/Makefile:424: recipe for target 'tensorflow/lite/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/micro/kernels/fully_connected.o' failed
make: *** [tensorflow/lite/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/micro/kernels/fully_connected.o] Error 1
 
How can i fix it and run ?



**Any other info / logs**
I didn't change anything i run it like i get the source code from github.
"
44121,trt_convert error,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.2, 8.0
- GPU model and memory: Nvidia Xavier


I have keras model that I'm trying to convert to tensorrt using TrtGraphConverterV2. The model takes image and classifies it. 

    def my_input_fn():
		img = cv2.imread('messi5.jpg',0)
		img = np.expand_dims(img, axis=0)
		return img

    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS
    conversion_params = conversion_params._replace(
        max_workspace_size_bytes=(1<<32))
    conversion_params = conversion_params._replace(precision_mode=""FP16"")
    conversion_params = conversion_params._replace(
        maximum_cached_engines=100)

    converter = trt.TrtGraphConverterV2(
        input_saved_model_dir=""input_model/"", conversion_params=conversion_params)
    converter.convert()
    converter.build(input_fn=my_input_fn)
    converter.save(""tensorrt_model"")


With this code I'm getting:
2020-10-17 21:47:52.620064: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:588] Running native segment forTRTEngineOp_0_0 due to failure in verifying input shapes: Input shapes do not match input partial shapes stored in graph, for TRTEngineOp_0_0: [[480,768,3]] != [[1,480,768,3]]
2020-10-17 21:47:52.677691: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at transpose_op.cc:157 : Invalid argument: transpose expects a vector of size 3. But input(1) is a vector of size 4
2020-10-17 21:47:52.677916: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at trt_engine_op.cc:401 : Invalid argument: transpose expects a vector of size 3. But input(1) is a vector of size 4
         [[{{node StatefulPartitionedCall/model_1/conv2d_1/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]
Traceback (most recent call last):
  File ""convert_to_tensorrt.py"", line 118, in <module>
    converter.build(input_fn=my_input_fn)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py"", line 1174, in build
    func(*map(ops.convert_to_tensor, inp))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 1657, in __call__
    return self._call_impl(args, kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py"", line 247, in _call_impl
    args, kwargs, cancellation_manager)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 1675, in _call_impl
    return self._call_with_flat_signature(args, kwargs, cancellation_manager)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 1724, in _call_with_flat_signature
    return self._call_flat(args, self.captured_inputs, cancellation_manager)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 1926, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 550, in call
    ctx=ctx)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  transpose expects a vector of size 3. But input(1) is a vector of size 4
         [[{{node StatefulPartitionedCall/model_1/conv2d_1/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]
         [[TRTEngineOp_0_0]] [Op:__inference_pruned_16623]
  <br/>
<br/>
If I remove expand_dims I'm getting:
Traceback (most recent call last):
  File ""convert_to_tensorrt.py"", line 118, in <module>
    converter.build(input_fn=my_input_fn)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py"", line 1174, in build
    func(*map(ops.convert_to_tensor, inp))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 1657, in __call__
    return self._call_impl(args, kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py"", line 247, in _call_impl
    args, kwargs, cancellation_manager)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 1675, in _call_impl
    return self._call_with_flat_signature(args, kwargs, cancellation_manager)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 1697, in _call_with_flat_signature
    len(args)))
TypeError: pruned(x) takes 1 positional arguments but 480 were given"
44120,Disable internal mkldnn_sgemm calls,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version:latest 
- Python version:3.7.3
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source):3.3.0
- GCC/Compiler version (if compiling from source): gcc 8.3
- I did **NOT** use the --config=mkl flag while building
- The image is a call graph of 1024X1024 Matmul done 100 times, if you look carefully mkldnn_sgemm is called internally by Eigen, this is what I want to disable.
 
![internal_eigen](https://user-images.githubusercontent.com/44555985/96214609-7444a200-0f99-11eb-988b-c4f9b32c6d2d.jpg)


After some amount of reading I found out that MKL/mkl_dnn can be called internally by Eigen and after reading the Bazel documentation and seeing how TensorFlow is strucured.I want to disable mkl/mkl_dnn completely if eigen is to be used.
The Eigen[ build files loads the ifmkl symbol](https://github.com/tensorflow/tensorflow/blob/bd35e19edbf55bf3f6b5d10545725d41a99aa9ba/third_party/eigen3/BUILD#L5) from the mkl directory.
My next step was to look at the [if_mkl function inside the build_defs.bzl](https://github.com/tensorflow/tensorflow/blob/c63b59ce648b525c4bbb2d85f04f5b71f1849ff2/third_party/mkl/build_defs.bzl#L17)   

I changed the [includes attribute in the cclibrary rule in the BUILD file of eigen](https://github.com/tensorflow/tensorflow/blob/44aace846df438bf55e88dad1e047b7c81f9efb9/third_party/eigen3/BUILD#L36) to [""//conditions:default""]

which game me an error saying `ModuleNotFoundError: No module named 'portpicker' `

So pip installed portpicker `pip install portpicker`
And the build completed sucessfully, but the profile literally shows no difference mkldnn_sgemm is still bieng called the same number of time by eigen internally

NOTE: **The Ultimate aim is to disable mkldnn which is bieng called from inside eigen**"
44119,Latency Tensorflow serving with docker for video analysis,"I would like to ask a question regarding the response time of a tensorflow model in docker. I transformed a Yolo model from .h5 into a model API with the function.

I use a Ubuntu machine with an i9 CPU and an RTX 2080 TI graphics card

```
Model.save(
    filepath,
    overwrite=True,
    include_optimizer=True,
    save_format=None,
    signatures=None,
    options=None,
)
```
I took a docker image from tensorflow/serving:latest-gpu which I copied my models as follows:

```
FROM tensorflow/serving:latest-gpu
COPY models /apps/
EXPOSE 8500
EXPOSE 8501
```
I wrote the following script to analyze a video

```
img = img.astype('float32')
    img = img / 255.0

    data = json.dumps({""signature_name"": ""serving_default"", ""instances"": img.tolist()})


        }

    headers = {""content-type"": ""application/json""}
    start_time = time.time()
    json_response = requests.post('http://172.27.240.5:8501/v1/models/vetements:predict', data=dat   a, headers=headers)
    print(""--- %s seconds ---"" % (time.time() - start_time))

    predictions = json.loads(json_response.text)['predictions']
```
my question is as follows the rest time to analyze a single image is 0.5 seconds, please does it have a way to speed up the execution of analysis

thank you"
44118,Optimize GPU memory consumption: Decrease heap usage at the beginning of the training and allow GPU to use 100% fragmentation.,"**System information**
- Are you willing to contribute it (Yes/No): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux lz 5.4.0-48-generic #52~18.04.1-Ubuntu
- TensorFlow installed from (source or binary): (in conda env) pip install tensorflow-gpu -i https://pypi.tuna.tsinghua.edu.cn/simple
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0


**Describe the feature and the current behavior/state.**

I just use Tensorflow 2.3 to implement one paper, which has an official Pytorch version. Everything is fine except for the Batch_size of the training data, where the official pytorch could use `batch_size` of `32` with 8GPU, while I could only deploy `16` with the same GPUs and same settings of neural network. I tried to use tensorboard Profiler to check and optimize my training loop.

Here is the Profiler output of 10 steps of my model training.
![image](https://user-images.githubusercontent.com/28552583/96349804-8aab4480-10e4-11eb-8fdb-808798b5b999.png)
![image](https://user-images.githubusercontent.com/28552583/96349883-0ad1aa00-10e5-11eb-894d-bb49763c15e8.png)


You can see that the image points out the peak heap usage occurs when the GradientTape of the last layer of the network asked for it. However, after this allocation of the GPU memory, the peak memory usage goes down from `7.41 GiBs` to around only `6 GiBs`, so I am wondering why TF2 will allocate so much heap usage at the beginning of each training step and will not use this part during the loop, and the difference between the heap and memory usage. Is there any way that I could optimize the heap allocation of the heap so that I could fit my `32 batch_size` to the graphic memory?

I also noticed that the memory capacity of what TF Profiler shows is `10.96 GiBs`, which is only `90%` of Fragmentation. My GPU memory has got `12196 MiB` memory to use, which means that there are still some available space for the training. Some blogs' workarounds don't work, such as `tf.config.experimental.set_memory_growth(gpu, True)`. Thus, I am looking for a way to permit TF to use this part of the graphic memory, say `100%` fragmentation.

I tried to use the following code as the documentation says, but I still failed:
``` python
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
```

I am sure that if I solve the above two problems, I could increase my `batch_size` from `16` to `32`, since a lot of GPU memory was wasted before. Appreciate your help sincerely.

I am using multi-gpu. So error callback is provided:
```
Traceback (most recent call last):
  File ""main.py"", line 29, in <module>
    main()
  File ""main.py"", line 25, in main
    trainer.train()
  File ""/home/lz/potter/EDVR/trainers/train.py"", line 238, in train
    loss, acc = self.train_epoch(epoch)
  File ""/home/lz/potter/EDVR/trainers/train.py"", line 191, in train_epoch
    loss, psnr = self.multi_train_step(batch_x, batch_y)
  File ""/home/lz/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/home/lz/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 846, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File ""/home/lz/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1843, in _filtered_call
    return self._call_flat(
  File ""/home/lz/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1923, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/home/lz/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 545, in call
    outputs = execute.execute(
  File ""/home/lz/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 3 root error(s) found.
  (0) Resource exhausted:  OOM when allocating tensor with shape[4,256,360,640] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[{{node StatefulPartitionedCall/conv2d_61/Conv2D}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[Identity_2/_190]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted:  OOM when allocating tensor with shape[4,256,360,640] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[{{node StatefulPartitionedCall/conv2d_61/Conv2D}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (2) Resource exhausted:  OOM when allocating tensor with shape[4,256,360,640] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[{{node StatefulPartitionedCall/conv2d_61/Conv2D}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[GroupCrossDeviceControlEdges_0/StatefulPartitionedCall/Adam/Adam/update_1_1/Const/_155]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

0 successful operations.
0 derived errors ignored. [Op:__inference_multi_train_step_36442]

Function call stack:
multi_train_step -> multi_train_step -> multi_train_step
```

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Everyone who wants to optimize the consumption of GPU memory."
44116,3080 & 3090 coumpute capability 86 degraded performance after some updates,"This issue is apparent from the difference in performance in NGC containers https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow . For 20.08 se-resnext101 example training performance
```
python nvidia-examples/resnet50v1.5/main.py --arch=se-resnext101-32x4d --batch_size=64  --warmup_steps 200 --data_dir=/hdd/datasets/imagenet/tf/train/ --gpu_memory_fraction 0.95  --precision fp32  --results_dir=/toy/tmp/results_dir/   --mode=training_benchmark   --use_tf_amp --use_xla
```
(have to adapt directories)
on 3080 is around 370-400 img/sec. While on 20.09 container it's more like 115 img/sec. This is also similar for resnet-50 and most likely all other CNN benchmarks. This is not an issue with my setup, it's the same for other folks - you can view discussion at https://www.pugetsystems.com/labs/hpc/RTX3090-TensorFlow-NAMD-and-HPCG-Performance-on-Linux-Preliminary-1902/"
44114,TFlite RuntimeError: Model resulted in Nan value during calibration. Please make sure model results in all real-values during inference with provided dataset.Node number 37 (CONV_2D) failed to invoke.,"I use tf 2.3 and this error occurs when doing full integer quantization.
`RuntimeError: Model resulted in Nan value during calibration. Please make sure model results in all real-values during inference with provided dataset.Node number 37 (CONV_2D) failed to invoke.`

**Code (just a demo how I do quantize and it can't reproduce error)**
```
def representative_dataset_gen():
    for x in validation_fingerprints:
      x = x[np.newaxis,:]
      yield [x]

converter = tf.lite.TFLiteConverter.from_saved_model(flags.train_dir + '/last_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
converter.allow_custom_ops = True
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.representative_dataset = representative_dataset_gen
last_quant_model = converter.convert()
with open(flags.train_dir + '/quant_last_model.tflite', 'wb') as w:
  w.write(last_quant_model)
```
**Some config**
type(validation_fingerprints): <class 'numpy.ndarray'>
shape(validation_fingerprints): (3093, 16384)
type(x): <class 'numpy.ndarray'>
shape(x): (1,16384)
The model_summary
[model_summary.txt](https://github.com/tensorflow/tensorflow/files/5395787/model_summary.txt)

Can anyone help? Thanks."
44113,Loading checkpoint.data weights into equivalent keras model producing horrible inference results,"Python V 3.8.3
TF 1.14 -- class model built here
TF 2.0 -- functional model built here 
RTX 2060 
CUDA 10.0.13
Cudnn 7.6.5
Windows 10
**Describe the current behavior**



"
44112,Unable to fit against and batch ragged output in Keras.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Reproduced on Arch Linux & macOS Catalina.
- TensorFlow installed from (source or binary): Binary(?) with pip.
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087
- Python version: 3.8.6

**Describe the current behavior**

I'm working on a text autoencoder in tensorflow to reduce the dimensionality of large text files (> 50000 chars, > 100000 vocab). I feel like I've got what should be a working solution, but run into `ValueError: TypeError: object of type 'RaggedTensor' has no len()`. My program also can't function when using a batch size of greater than 1, instead producing `tensorflow.python.framework.errors_impl.InvalidArgumentError: PartialTensorShape: Incompatible shapes during merge`. It seems that this kind of use case has been supported in Tensorflow for a long time, and I'm rather confused as to why I've been running into so much difficulty with ragged tensors. My code is linked in Google Colab below. Am I doing something wrong, is this a bug in Tensorflow, or unimplemented functionality? Thanks in advance.

**Describe the expected behavior**

Tensorflow fits happily against ragged truths and is able to batch ragged outputs.

**Standalone code to reproduce the issue**

https://colab.research.google.com/drive/13yYGVDAsf8sfoPrbYPtEsrJxZz8OSoVU?usp=sharing

**Exception logs**

[batched-log.txt](https://github.com/tensorflow/tensorflow/files/5395543/batched-log.txt)
[unbatched-log.txt](https://github.com/tensorflow/tensorflow/files/5395544/unbatched-log.txt)

"
44111,"ValueError: Shapes (None, 3, 2) and (None, 2) are incompatible","In the following code, I save the label to tfrecord and read it again.
(In reality, I save both images and labels to tfrecord, here is a simple example for illustration purpose).

I got an error `ValueError: Shapes (None, 3, 2) and (None, 2) are incompatible`, it looks like a bug? I am using Tensorflow 2.3. 


    import contextlib2
    import numpy as np
    import tensorflow as tf
    from tensorflow.keras import Model
    from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
    
    
    def process_image():
    
        dic={
                ""image/label"": tf.train.Feature(int64_list=tf.train.Int64List(value=[0,1]))
        }
        return tf.train.Example(features=tf.train.Features(feature=dic))
    
    
    with contextlib2.ExitStack() as tf_record_close_stack:
        output_tfrecords = [tf_record_close_stack.enter_context(tf.io.TFRecordWriter(file_name)) for file_name in
                            [f""data_train.tfrecord""]]
        output_tfrecords[0].write(process_image().SerializeToString())
    
    def parse_examples(examples):
        parsed_examples = tf.io.parse_example(examples, features={
            ""image/label"": tf.io.FixedLenFeature(shape=[2], dtype=tf.int64),
        })
        res = np.random.randint(2, size=3072).reshape(32, 32, 3)
        return (res, [parsed_examples[""image/label""],parsed_examples[""image/label""],parsed_examples[""image/label""]])
    
    
    def process_dataset(dataset):
        dataset = dataset.map(parse_examples, num_parallel_calls=tf.data.experimental.AUTOTUNE)
        dataset = dataset.batch(1)
        return dataset
    
    train_data = tf.data.TFRecordDataset(filenames=""data_train.tfrecord"")
    train_data = process_dataset(train_data)
    
    base_model = tf.keras.applications.EfficientNetB7(input_shape=(32,32, 3), weights='imagenet',
                                                      include_top=False)  # or weights='noisy-student'
    
    for layer in base_model.layers[:]:
        layer.trainable = False
    
    x = GlobalAveragePooling2D()(base_model.output)
    dropout_rate = 0.3
    
    
    x = Dense(256, activation='relu')(x)
    x = Dropout(dropout_rate)(x)
    x = Dense(256, activation='relu')(x)
    x = Dropout(dropout_rate)(x)
    
    
    all_target = []
    loss_list = []
    test_metrics = {}
    for name, node in  [(""task1"", 2), (""task2"", 2), (""task3"", 2)]:
        y1 = Dense(128, activation='relu')(x)
        y1 = Dropout(dropout_rate)(y1)
        y1 = Dense(64, activation='relu')(y1)
        y1 = Dropout(dropout_rate)(y1)
        y1 = Dense(node, activation='softmax', name=name)(y1)
        all_target.append(y1)
        loss_list.append('categorical_crossentropy')
        test_metrics[name] = ""accuracy""
    
    #    model = Model(inputs=model_input, outputs=[y1, y2, y3])
    model = Model(inputs=base_model.input, outputs=all_target)
    
    model.compile(loss=loss_list, optimizer='adam', metrics=test_metrics)
    
    history = model.fit(train_data, epochs=1, verbose=1)

"
44110,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.,"**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from: binary
- TensorFlow version: 2.2.0
- Python version: 3.8.6
- Installed using: pip
- CUDA/cuDNN version: 10.1/ 7.6.5
- GPU model and memory: Nvidia Geforce GTX 1050i TI


**Describe the problem**
When i import tensorflow like normal it throw an error, i have done the same thing on my laptop and there it all works fine, ik have the exact same code and followed the exact same steps, but on my computer it just fails. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
- open a python file;
- import tensorflow as tf
- error occurs

Traceback (most recent call last):
  File ""C:\Users\TastDesktop\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\TastDesktop\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 29, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\TastDesktop\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 25, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\TastDesktop\AppData\Local\Programs\Python\Python38\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\TastDesktop\AppData\Local\Programs\Python\Python38\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""model_main_tf2.py"", line 31, in <module>
    import tensorflow.compat.v2 as tf
  File ""C:\Users\TastDesktop\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\TastDesktop\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\__init__.py"", line 52, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\TastDesktop\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\TastDesktop\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\TastDesktop\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 29, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\TastDesktop\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 25, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\TastDesktop\AppData\Local\Programs\Python\Python38\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\TastDesktop\AppData\Local\Programs\Python\Python38\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
"
44108,TFLite Interpreter returns different results when in Python on desktop and when in Java on Android mobile device,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04, Android 9 PKQ1.190118.0 01
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Xiaomi MIX 2
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.8.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Loading the same TFLite model on my desktop using the following command:

```interpreter = tf.lite.Interpreter(model_path='./model.tflite') 
interpreter.allocate_tensors() 
input_details = interpreter.get_input_details() 
output_details = interpreter.get_output_details() 
interpreter.set_tensor(input_details[0]['index'], img_data.reshape(1, 28, 28, 1).astype(np.float32)) 
interpreter.invoke() 
output_data = interpreter.get_tensor(output_details[0]['index'])
```

Returns the correct results in Float32 in output_data, something like [0.345, 0.655]. However, if I load the same model on my device using:

```MappedByteBuffer tfliteModel = FileUtil.loadMappedFile(this, ""model.tflite"");
Interpreter tflite = new Interpreter(tfliteModel);
TensorBuffer probabilityBuffer = TensorBuffer.createFixedSize(new int[]{1, 2}, DataType.FLOAT32);
tflite.run(img_data, probabilityBuffer.getBuffer());
float[] results = probabilityBuffer.getFloatArray();
```

Always returns a very strange result of [1.0, 0.0]

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44107,"WARNING:tensorflow:Can save best model only with val_loss available, skipping.","**System information**
Current Google Colab Pro notebook, Tensorflow version 2.3.0


**Describe the current behavior**
when running .fit() with callback.ModelCheckpoint monitoring both`val_loss` and `val_sparse_categorical_accuracy` the ""WARNING:tensorflow:Can save best model only with val_loss available, skipping."" pops up even if both terms are in hist.history returned from fit

**Describe the expected behavior**


**Standalone code to reproduce the issue**
```
model.compile(optimizer=tf.keras.optimizers.Adam(lr = 0.05),
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

hist = model.fit(X_train,
                 y_train,
                 batch_size = BATCH_SIZE,
                 epochs=EPOCH,
                 validation_data=(X_val, y_val),
                 shuffle = True,
                 callbacks = [ModelCheckpoint(""model.hdf5"",
                              monitor = 'val_loss',
                              save_best_only = True,
                              save_weights_only = False,
                              save_freq= 1,
                              verbose = 0)],
                 verbose = 0)
```

where data are np.ndarray, and 

```
for key in hist.history:
  print(key)
```
returns the following:

loss
sparse_categorical_accuracy
val_loss
val_sparse_categorical_accuracy
lr
"
44106,cublasComputeType_t has not been declared,"**System information**
- Ubuntu 20.04
- Master Branch and Nightly Branch
- Python version: 3.8
- Bazel version (if compiling from source): 3.1.0
- GCC 8
- CUDA/cuDNN version: CUDA 11.1, cuDNN 8
- GPU model and memory: 3090



**Describe the problem**

Configure successful, with cuda11, cudnn8, tensorrt7

command:
`bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package`


**Any other info / logs**

INFO: Found applicable config definition build:linux in file /home/frank/tffun/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/frank/tffun/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/51ff04567b2f8d06b2062bd3ed72eab2e93e4466.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (261 packages loaded, 29454 targets configured).
INFO: Found 1 target...
ERROR: /home/frank/tffun/tensorflow/tensorflow/stream_executor/cuda/BUILD:254:1: C++ compilation of rule '//tensorflow/stream_executor/cuda:cublas_lt_stub' failed (Exit 1)
In file included from tensorflow/stream_executor/cuda/cublasLt_stub.cc:15:
bazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:576:5: error: 'cublasComputeType_t' has not been declared
     cublasComputeType_t computeType,
     ^~~~~~~~~~~~~~~~~~~
bazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:586:5: error: 'cublasComputeType_t' has not been declared
     cublasComputeType_t computeType,
     ^~~~~~~~~~~~~~~~~~~
bazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:597:60: error: 'cublasComputeType_t' has not been declared
 cublasLtMatmulDescCreate(cublasLtMatmulDesc_t *matmulDesc, cublasComputeType_t computeType, cudaDataType_t scaleType);
                                                            ^~~~~~~~~~~~~~~~~~~
In file included from tensorflow/stream_executor/cuda/cublasLt_stub.cc:15:
bazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:1069:5: error: 'cublasComputeType_t' has not been declared
     cublasComputeType_t computeType,
     ^~~~~~~~~~~~~~~~~~~
bazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:1087:26: error: 'cublasComputeType_t' has not been declared
                          cublasComputeType_t computeType,
                          ^~~~~~~~~~~~~~~~~~~
In file included from tensorflow/stream_executor/cuda/cublasLt_stub.cc:58:
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:135:5: error: 'cublasComputeType_t' has not been declared
     cublasComputeType_t computeType, cudaDataType_t scaleType) {
     ^~~~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: In function 'cublasStatus_t cublasLtMatmulDescInit_internal(cublasLtMatmulDesc_t, size_t, int, cudaDataType_t)':
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:137:37: error: 'cublasComputeType_t' has not been declared
       cublasLtMatmulDesc_t, size_t, cublasComputeType_t, cudaDataType_t);
                                     ^~~~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: At global scope:
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:144:39: error: 'cublasComputeType_t' has not been declared
     cublasLtMatmulDesc_t *matmulDesc, cublasComputeType_t computeType,
                                       ^~~~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: In function 'cublasStatus_t cublasLtMatmulDescCreate(cublasLtMatmulDescOpaque_t**, int, cudaDataType_t)':
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:147:31: error: 'cublasComputeType_t' has not been declared
       cublasLtMatmulDesc_t *, cublasComputeType_t, cudaDataType_t);
                               ^~~~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: At global scope:
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:309:35: error: 'cublasComputeType_t' has not been declared
     cublasLtHandle_t lightHandle, cublasComputeType_t computeType,
                                   ^~~~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: In function 'cublasStatus_t cublasLtMatmulAlgoGetIds(cublasLtHandle_t, int, cudaDataType_t, cudaDataType_t, cudaDataType_t, cudaDataType_t, cudaDataType_t, int, int*, int*)':
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:314:25: error: 'cublasComputeType_t' has not been declared
       cublasLtHandle_t, cublasComputeType_t, cudaDataType_t, cudaDataType_t,
                         ^~~~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: At global scope:
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:323:35: error: 'cublasComputeType_t' has not been declared
     cublasLtHandle_t lightHandle, cublasComputeType_t computeType,
                                   ^~~~~~~~~~~~~~~~~~~
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: In function 'cublasStatus_t cublasLtMatmulAlgoInit(cublasLtHandle_t, int, cudaDataType_t, cudaDataType_t, cudaDataType_t, cudaDataType_t, cudaDataType_t, int, cublasLtMatmulAlgo_t*)':
./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:328:25: error: 'cublasComputeType_t' has not been declared
       cublasLtHandle_t, cublasComputeType_t, cudaDataType_t, cudaDataType_t,
                         ^~~~~~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 492.578s, Critical Path: 86.39s
INFO: 4024 processes: 4024 local.
FAILED: Build did NOT complete successfully
"
44103,Dealing with `from tensorflow.contrib import ...` in  tf_upgrade_v2 ,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):2.3.0
- Are you willing to contribute it (Yes/No):No



**Describe the feature and the current behavior/state.**
I ran into cases not covered by the `tf_upgrade_v2` tool yet, namely certain TF1 models in the zoo (https://github.com/tensorflow/models/tree/master/research)  that import from `tensorflow.contrib` - which was deprecated in TF2

I created a small example to illustrate the current behaviour of  the `tf_upgrade_v2` tool on these:

```
%%bash

cat > /tmp/example.py << EOL
try:
    from tensorflow.contrib import quantize as contrib_quantize
    from tensorflow.contrib import framework as contrib_framework
    from tensorflow.contrib import slim as contrib_slim
    from tensorflow.contrib.slim.nets import resnet_utils
    from tensorflow.contrib import layers as contrib_layers
    from tensorflow.contrib import tfprof as contrib_tfprof
    from tensorflow.contrib import training as contrib_training
    from tensorflow.contrib import metrics as contrib_metrics
except Exception as e:
    print(type(e).__name__, "":"", e)

from tensorflow_addons import metrics as addons_metrics

def demo_deprecated():
    print(dir(contrib_metrics))

def demo_ok():
    print(dir(addons_metrics))


if __name__ == '__main__':
    demo_ok()
    print(""==========================="")
    demo_deprecated()
EOL
```

Then, according to https://www.tensorflow.org/guide/upgrade, we do:

```
!tf_upgrade_v2 \
  --infile /tmp/example.py \
  --outfile /tmp/example_v2.py
```

The report file doesn't contain any warning about the deprecated modules:

```!cat report.txt```


```
TensorFlow 2.0 Upgrade Script
-----------------------------
Converted 1 files
Detected 0 issues that require attention
--------------------------------------------------------------------------------
================================================================================
Detailed log follows:

================================================================================
--------------------------------------------------------------------------------
Processing file '/tmp/example.py'
 outputting to '/tmp/example_v2.py'
--------------------------------------------------------------------------------


--------------------------------------------------------------------------------
```

The two files are identical:

```!diff -u /tmp/example.py /tmp/example_v2.py```

However, unsurprisingly, the ModuleNotFoundError is thrown when we execute:

```!python /tmp/example_v2.py```


```
2020-10-16 19:52:37.022043: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
ModuleNotFoundError : No module named 'tensorflow.contrib'
['CohenKappa', 'F1Score', 'FBetaScore', 'HammingLoss', 'MatthewsCorrelationCoefficient',  ... , 'utils']
===========================
Traceback (most recent call last):
  File ""/tmp/example_v2.py"", line 25, in <module>
    demo_deprecated()
  File ""/tmp/example_v2.py"", line 16, in demo_deprecated
    print(dir(contrib_metrics))
NameError: name 'contrib_metrics' is not defined
```


It would be nice to see the `tf_upgrade_v2` tool (more precisely: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/compatibility/tf_upgrade_v2_main.py) updated to treat these cases `from tensorflow.contrib import ...` .  

Moreover, it would be great to have the `tf_upgrade_v2` tool extended so that it incorporates in the migration process (or at least as guidance in the report.txt) the successors of the modules in TF1 `tensorflow.contrib` , such as the `tensorflow_addons` (https://github.com/tensorflow/addons), `tf_slim` (https://github.com/tensorflow/models/tree/master/research/slim), etc.  

**Will this change the current api? How?**
No API changes are necessary.

**Who will benefit with this feature?**
Everyone who tries to migrate TF1 models to TF2, when the files contain imports from `tensorflow.contrib`

**Any Other info.**
-

Many thanks in advance,
Camelia
"
44100,Add tflite_runtime 2.3.1 wheels to Python quickstart,"## URL(s) with the issue: 
https://www.tensorflow.org/lite/guide/python

## Description of issue (what needs changing): 
The [TensorFlow Lite Python quickstart guide](https://www.tensorflow.org/lite/guide/python) currently only contains links to prebuilt wheels for tflite_runtime 2.1.0. TensorFlow 2.1.0 is missing several new features and key [security fixes](https://github.com/tensorflow/tensorflow/releases/tag/v2.3.1) that would be valuable for developers. As such, would it be possible to update this page with download links for a fresh (>= 2.3.1) version of TensorFlow? Updating the wheels to a newer version should not require any other changes to the quickstart guide.


The source for this page is located at https://github.com/tensorflow/tensorflow/blob/1e87951747af11b61206dd19f9363b309fc8e6f5/tensorflow/lite/g3doc/guide/python.md .

### Submit a pull request?

I don't have the ability to host wheels at https://dl.google.com, so I believe a TensorFlower will need to build all the wheels and then upload them. Public instructions for building the wheels are hosted at https://github.com/tensorflow/tensorflow/blob/f1a3160b04dda71d50130d6a03727486bed2f5e1/tensorflow/lite/tools/pip_package/README.md
"
44099,Feature request: ResNet34,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.1
- Are you willing to contribute it (Yes/No): no



**Describe the feature and the current behavior/state.**

create ResNet34 similar to https://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet50

**Will this change the current api? How?**

I don't think so. Hopefully it's an easy adaption of ResNet50 code.

**Who will benefit with this feature?**

People like me who are new to DL and Transfer Learning. With ResNet34 being smaller that ResNet50 I can iterate and learn quicker.

**Any Other info.**

I believe there are a few implementations outside of tf:
https://github.com/calmisential/TensorFlow2.0_ResNet
https://github.com/taki0112/ResNet-Tensorflow
https://github.com/qubvel/classification_models"
44098,Custom function adding before augmentation in ImageDataGenerator.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests, and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.1
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behaviour/state.**
tensorflow.keras.preprocessing.image.ImageDataGenerator class takes an argument to implement custom function after augmentation, which it is 'preproccessing_function'. Allowing to use a custom function before augmentation or rescale/resizing will be a great advantage for images that need to be cropped into bounding boxes.

**Will this change the current API? How?**
I cannot comment on this due to my inexperience.

**Who will benefit with this feature?**
Who has bounding box coordinates in *.csv file. But not applied yet.
 
**Any Other info.**
No."
44095,installation for Torch,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
44093,tf.distribute.MirroredStrategy using TF_CONFIG and non-eager execution assigns ops to nonexistent device names,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google AI Platform
- TensorFlow installed from (source or binary): binary
- TensorFlow version: reproducible on 2.2 or 2.3
- Python version: 3.7
- CUDA/cuDNN version: Unknown
- GPU model and memory: 2x NVIDIA Tesla K80

---

When running [the example code from _Distributed training with Keras_](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/distribute/keras.ipynb) on AI Platform with multiple GPUs, **eager execution disabled**, and using `tf.keras.MirroredStrategy` to utilize all GPUs, TensorFlow raises an `InvalidArgumentError`:

```
tensorflow.python.framework.errors_impl.InvalidArgumentError:
    Cannot assign a device for operation conv2d/kernel/Initializer/random_uniform/RandomUniform:
    Could not satisfy explicit device specification '' because the node
    {{colocation_node conv2d/kernel/Initializer/random_uniform/RandomUniform}}
    was colocated with a group of nodes that required incompatible device 
    '/job:chief/replica:0/task:0/device:GPU:0'. All available devices [
        /job:localhost/replica:0/task:0/device:CPU:0,
        /job:localhost/replica:0/task:0/device:XLA_CPU:0,
        /job:localhost/replica:0/task:0/device:XLA_GPU:0,
        /job:localhost/replica:0/task:0/device:XLA_GPU:1,
        /job:localhost/replica:0/task:0/device:GPU:0,
        /job:localhost/replica:0/task:0/device:GPU:1
    ]. 
    
    Colocation Debug Info:
    Colocation group had the following types and supported devices: 
        Root Member(
            assigned_device_name_index_=-1
            requested_device_name_='/job:chief/replica:0/task:0/device:GPU:0'
            assigned_device_name_=''
            resource_device_name_='/job:chief/replica:0/task:0/device:GPU:0'
            supported_device_types_=[GPU, CPU]
            possible_devices_=[]
        AssignVariableOp: GPU CPU XLA_CPU XLA_GPU 
        RandomUniform: GPU CPU XLA_CPU XLA_GPU 
        VarIsInitializedOp: GPU CPU XLA_CPU XLA_GPU 
        Const: GPU CPU XLA_CPU XLA_GPU 
        Mul: GPU CPU XLA_CPU XLA_GPU 
        ReadVariableOp: GPU CPU XLA_CPU XLA_GPU 
        Sub: GPU CPU XLA_CPU XLA_GPU 
        VarHandleOp: GPU CPU XLA_CPU XLA_GPU 
        Add: GPU CPU XLA_CPU XLA_GPU 

```

Oddly, the list of available devices includes the GPUs I want to use, but their names include `job:localhost` rather than `job:chief`.


The logs used to launch the job show that only one worker is being used, and it's being assigned the type `chief`:
```json
Running task with arguments:
  --cluster={""chief"": [""127.0.0.1:2222""]}
  --task={""type"": ""chief"", ""index"": 0}
  --job={
    ""scale_tier"": ""CUSTOM"", 
    ""master_type"": ""n1-standard-16"",
    ""package_uris"": [...snip...], 
    ""python_module"": ""psobot.mirrored_strategy_test"", 
    ""region"": ""europe-west1"", 
    ""runtime_version"": ""2.2"", 
    ""run_on_raw_vm"": true,
    ""python_version"": ""3.7"", 
    ""master_config"": {
      ""accelerator_config"": {
        ""count"": ""2"",
        ""type"": ""NVIDIA_TESLA_K80""
      }
}}
```

...but also show that the device naming mismatch seems to happen earlier:
```
Created TensorFlow device (/device:GPU:0 with 10634 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)""
successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero""
Created TensorFlow device (/device:GPU:1 with 10634 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:00:05.0, compute capability: 3.7)""
Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:chief/replica:0/task:0/device:GPU:0,/job:chief/replica:0/task:0/device:GPU:1""
Using MirroredStrategy with devices ('/job:chief/replica:0/task:0/device:GPU:0', '/job:chief/replica:0/task:0/device:GPU:1')""
```

**Describe the expected behavior**

TensorFlow should identify that `/job:chief` and `/job:localhost` refer to the same machine (the current machine) and should be able to place ops there, and it should be possible to train on multiple GPUs.

**Standalone code to reproduce the issue**
```python
import tensorflow_datasets as tfds
import tensorflow as tf

# This is the only functional change from the example code.
tf.compat.v1.disable_eager_execution()

# Copied from the example code at:
# https://github.com/tensorflow/docs/blob/master/site/en/tutorials/distribute/keras.ipynb
datasets, info = tfds.load(name=""mnist"", with_info=True, as_supervised=True)
mnist_train, mnist_test = datasets[""train""], datasets[""test""]
strategy = tf.distribute.MirroredStrategy()
print(""Number of devices: {}"".format(strategy.num_replicas_in_sync))

train_dataset = mnist_train.map(lambda im, l: ((tf.cast(im, tf.float32) / 255), l)).batch(64)

with strategy.scope():
    model = tf.keras.Sequential(
        [
            tf.keras.layers.Conv2D(32, 3, activation=""relu"", input_shape=(28, 28, 1)),
            tf.keras.layers.MaxPooling2D(),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(64, activation=""relu""),
            tf.keras.layers.Dense(10),
        ]
    )

    model.compile(
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        optimizer=tf.keras.optimizers.Adam(),
        metrics=[""accuracy""],
    )

model.fit(train_dataset, epochs=12)
```

Running the above code on AI Platform with the following:
```bash
gcloud ai-platform jobs submit training psobot_mirrored_dummy_$(date +%s) \
  --runtime-version 2.2
  --python-version 3.7
  --project [YOUR GCP PROJECT HERE]
  --region europe-west1
  --module-name YOUR_MODULE_NAME_HERE.mirrored_strategy_test
  --package-path YOUR_PACKAGE_NAME_HERE
  --scale-tier custom
  --master-machine-type n1-standard-16
  --master-accelerator count=2,type=nvidia-tesla-k80 
  --staging-bucket YOUR_STAGING_BUCKET_HERE
```
"
44092,Loss of shape information when using dilation_rate != 1 in Conv layers,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Running TF 2.3.0 in colab I have come across an issue very similar to the one fixed in #29542 . The difference however being that I have an extra dimension that is undefined at layer construction time. With a dilation rate = 1 the Conv2D layer in the following code produces an output shape as expected (None, None, 512, 64). However, for layers with a dilation rate > 1, the ""width"" dimension shape information is lost and the result is (None, None, None, 64).

```
import tensorflow as tf

class DenseBlock(tf.keras.Model):
  def __init__(self, input_size, depth=5, in_channels=64):
    super(DenseBlock, self).__init__(name='')
    self.depth = depth
    self.in_channels = in_channels
    self.pad = tf.keras.layers.ZeroPadding2D(((1, 0), (1, 1)))
    self.twidth = 2
    self.kernel_size = (self.twidth, 3)
    for i in range(self.depth):
      dil = 2**i
      pad_length = self.twidth + (dil-1)*(self.twidth-1)-1
      setattr(self, 'pad{}'.format(i+1), tf.keras.layers.ZeroPadding2D(((pad_length, 0), (1, 1))))
      setattr(self, 'conv{}'.format(i+1), tf.keras.layers.Conv2D(filters=self.in_channels, kernel_size=self.kernel_size, dilation_rate=(dil, 1)))
      setattr(self, 'norm{}'.format(i+1), tf.keras.layers.LayerNormalization())
      setattr(self, 'prelu{}'.format(i+1), tf.keras.layers.PReLU(shared_axes=[1, 2]))

  def call(self, input_tensor):
    skip = input_tensor
    for i in range(self.depth):
      print('Dilation rate', 2**i)
      x = getattr(self, 'pad{}'.format(i+1))(skip)
      print(x.shape)
      x = getattr(self, 'conv{}'.format(i+1))(x)
      print(x.shape)
      x = getattr(self, 'norm{}'.format(i+1))(x)
      x = getattr(self, 'prelu{}'.format(i+1))(x)
      skip = tf.concat((x, skip), axis=3)
    return x

input = tf.keras.layers.Input(shape=(None, 512, 64))
x = DenseBlock(512, 5, 64)(input)
```

**Describe the expected behavior**
I believe that the behaviour for dilation rate > 1 should match that of the case where dilation rate = 1.

**Standalone code to reproduce the issue**
Here is a [link](https://colab.research.google.com/drive/11wATlQbeahUHQE63RFsMFl6EV598j1sm?usp=sharing) to a colab notebook that reproduces the issue 

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

The cell output is:
```
2.3.0
Dilation rate 1
(None, None, 514, 64)
(None, None, 512, 64)
Dilation rate 2
(None, None, 514, 128)
(None, None, None, 64)
Dilation rate 4
(None, None, 514, 192)
(None, None, None, 64)
Dilation rate 8
(None, None, 514, 256)
(None, None, None, 64)
Dilation rate 16
(None, None, 514, 320)
(None, None, None, 64)
```
"
44091,ValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr,"### System information
-   **OS Platform and Distribution  - Linux Ubuntu 20.04 LTS, 64bit**:
-   **TensorFlow installed from conda**:
-   **TensorFlow version 2.2.0**:
-   **Python version 3.8.5**:
-   **GCC/Compiler version 9.3.0**:
-   **CUDA/cuDNN version 11.0**:
-   **GPU model and memory GTX 1080, 12GB**:


**Command to export model**
```
python export_tflite_graph_tf2.py --pipeline_config_path trening_demo/pre-trained-models/ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8/pipeline.config \ 
--trained_checkpoint_dir trening_demo/models/my_ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8/ \
--output_directory trening_demo/exported-models-tflite/ex_my_ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8
```

**Command used to run the converter**

```
import tensorflow as tf
import importlib.util
import multiprocessing

def representative_data_gen():
  dataset_list = tf.data.Dataset.list_files ('/content/model_temp/ed/*')
  for i in range(100):
    image = next(iter(dataset_list))
    image = tf.io.read_file(image)
    image = tf.io.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, [1024, 1024])
    image = tf.cast(image / 255., tf.float32)
    image = tf.expand_dims(image, 0)
    yield [image]

converter = tf.lite.TFLiteConverter.from_saved_model('/content/model_temp/exported-models-tflite/ex_my_ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8/saved_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.representative_dataset = representative_data_gen


tflite_model = converter.convert()
with open('/content/model_temp/c-tflite/model.tflite', 'wb') as f:
  f.write(tflite_model)

len(tflite_model)
```
In the above function I am using **decode_jpeg** however, my photos have the extension *** .jpg.** I assume it doesn't matter


**The output from the converter invocation**

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/calibrator.py in __init__(self, model_content)
     51       self._calibrator = (
---> 52           _calibration_wrapper.CalibrationWrapper(model_content))
     53     except Exception as e:

TypeError: pybind11::init(): factory function returned nullptr

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
4 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/calibrator.py in __init__(self, model_content)
     52           _calibration_wrapper.CalibrationWrapper(model_content))
     53     except Exception as e:
---> 54       raise ValueError(""Failed to parse the model: %s."" % e)
     55     if not self._calibrator:
     56       raise ValueError(""Failed to parse the model."")

ValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.
```

**Link to the saved model** -- ex_my_ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8
[https://drive.google.com/drive/folders/1Hz4BdMw2xg5t1rRMhl0smiz17GnL-Pz5?usp=sharing](url)

I read that this may be a problem with the TensorRT version. Right? I use google colab.
"
44090,How to remove unused operators from tensorflowlite_c.so to reduce it's size?,"As in the title. How can one remove unused operators (automatically or manually) in the build of C API library? There's nice process regarding Select Ops, where one has to pass models during the build, but there isn't anything similar present for .so. There is section about .aar (https://www.tensorflow.org/lite/guide/reduce_binary_size), but it's not my use case."
44087,<tf.Operation 'PrintV2_2' type=PrintV2>,"i can NOT run this code!
what is the solution?
```
@tf.function
def squaredDiff(x,y):
    linearRegressionModel(x)
    return tf.square(LinearRegressionModel -y )

@tf.function
def totalLoss(x,y):
    return tf.reduce_sum(squaredDiff(x,y))

tf.print(totalLoss([0,1,2,3,4,5] , [9,7,5,3,1,-1]))
```
the output:
`<tf.Operation 'PrintV2_2' type=PrintV2>`
"
44085,ImportError: libcuda.so.1: cannot open shared object file: No such file or directory,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Scientific Linux 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary):  pip install tensorflow-gpu==1.13.1
- TensorFlow version:Desktop
- Python version:3.6.2
- Installed using virtualenv? pip? conda?: virtualenv and pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0
- GPU model and memory:



**Describe the problem**
I have set up a virtual environment, to run ImageAI on a custom detection task (https://github.com/OlafenwaMoses/ImageAI/blob/master/imageai/Detection/Custom/CUSTOMDETECTIONTRAINING.md) . ImageAI requires now outdated packages, that i have installed in the virtualenv.

pip install tensorflow-gpu==1.13.1 for gpu
 or pip install tensorflow==1.13.1 for cpu
pip install opencv-python
pip install keras==2.2.4
pip install numpy==1.16.1
pip install imageai --upgrade

module load cuda/9.0

**Provide the exact sequence of commands / steps that you executed before running into the problem**
i have installed the packages with the version recommended.

the code itself its just the one from the webpage to retrain yolov3 over a test dataset

from imageai.Detection.Custom import DetectionModelTrainer
import os

INPUT_DIR = ""/zhome/94/5/101974/data/hololens/""

trainer = DetectionModelTrainer()
trainer.setModelTypeAsYOLOv3()
trainer.setDataDirectory(data_directory=INPUT_DIR)
trainer.setTrainConfig(object_names_array=[""hololens""], batch_size=4, num_experiments=10, train_from_pretrained_model=""pretrained-yolov3.h5"", show_network_summary=True)
trainer.trainModel()

""""""## evaluate performance""""""
trainer = DetectionModelTrainer()
trainer.setModelTypeAsYOLOv3()
trainer.setDataDirectory(data_directory=INPUT_DIR)
metrics = trainer.evaluateModel(model_path=INPUT_DIR+""models"", json_path=""hololens/json/detection_config.json"", iou_threshold=0.5, object_threshold=0.3, nms_threshold=0.5)
print(metrics)

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

sing TensorFlow backend.
Traceback (most recent call last):
  File ""/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib64/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib64/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcuda.so.1: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""retraining_YOLO_new_data.py"", line 22, in <module>
    from imageai.Detection.Custom import DetectionModelTrainer
  File ""/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/imageai/Detection/__init__.py"", line 2, in <module>
    from imageai.Detection.keras_retinanet.models.resnet import resnet50_retinanet
  File ""/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/imageai/Detection/keras_retinanet/models/resnet.py"", line 19, in <module>
    import keras
  File ""/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/keras/__init__.py"", line 3, in <module>
    from . import utils
  File ""/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/keras/utils/__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/keras/utils/conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/keras/backend/__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/zhome/94/5/101974/Desktop/Env/nn_train/lib64/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib64/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib64/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcuda.so.1: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
44083,"Training BERT model gives Error :  ""[_Derived_]RecvAsync is cancelled. [[{{node loss_3/mul}}]]""","**Describe the current behaviour**
Error: 
```
 Epoch 1/10
 ---------------------------------------------------------------------------
 CancelledError                            Traceback (most recent call last)
<ipython-input-16-402ab8549e42> in <module>
----> 1 model.fit(trainGen, epochs = 10, steps_per_epoch = trainGen.getLen(),verbose = 1)

D:\Anaconda\envs\Tensorflow2\lib\site-packages\tensorflow_core\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--> 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

D:\Anaconda\envs\Tensorflow2\lib\site-packages\tensorflow_core\python\keras\engine\training_generator.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)
    601         shuffle=shuffle,
    602         initial_epoch=initial_epoch,
--> 603         steps_name='steps_per_epoch')
    604 
    605   def evaluate(self,

D:\Anaconda\envs\Tensorflow2\lib\site-packages\tensorflow_core\python\keras\engine\training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)
    263 
    264       is_deferred = not model._is_compiled
--> 265       batch_outs = batch_function(*batch_data)
    266       if not isinstance(batch_outs, list):
    267         batch_outs = [batch_outs]

D:\Anaconda\envs\Tensorflow2\lib\site-packages\tensorflow_core\python\keras\engine\training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
   1016       self._update_sample_weight_modes(sample_weights=sample_weights)
   1017       self._make_train_function()
-> 1018       outputs = self.train_function(ins)  # pylint: disable=not-callable
   1019 
   1020     if reset_metrics:

D:\Anaconda\envs\Tensorflow2\lib\site-packages\tensorflow_core\python\keras\backend.py in __call__(self, inputs)
   3578 
   3579     fetched = self._callable_fn(*array_vals,
-> 3580                                 run_metadata=self.run_metadata)
   3581     self._call_fetch_callbacks(fetched[-len(self._fetches):])
   3582     output_structure = nest.pack_sequence_as(

D:\Anaconda\envs\Tensorflow2\lib\site-packages\tensorflow_core\python\client\session.py in __call__(self, *args, **kwargs)
   1470         ret = tf_session.TF_SessionRunCallable(self._session._session,
   1471                                                self._handle, args,
-> 1472                                                run_metadata_ptr)
   1473         if run_metadata:
   1474           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

CancelledError: [_Derived_]RecvAsync is cancelled.
	 [[{{node loss_3/mul}}]]

```

**Describe the expected behavior**
I am new to NLP with BERT but I need to train this model using BERT. I have used the pre-trained BERT using tensorflow hub but getting errors.

**Standalone code to reproduce the issue**
Model 
`   

    def createModel_YesNo(vocab_size, batchSize, maxlen):
    bLayer = hub.KerasLayer(""https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2"", trainable=True)
    
    #Document
    input_ids_Document = tf.keras.layers.Input(shape = (maxlen, ), dtype = tf.int32)
    token_type_ids_Document = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)
    attention_mask_Document = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)
    
    bertInputs_Document = [input_ids_Document, token_type_ids_Document, attention_mask_Document]
    
    #bertOutput_Document = bLayer(n_fine_tune_layers = 3)(bertInputs_Document)
    bertOutput_Document, _ = bLayer(bertInputs_Document)
    
    #Question
    input_ids_Question = tf.keras.layers.Input(shape = (maxlen, ), dtype = tf.int32)
    token_type_ids_Question = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)
    attention_mask_Question = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)
    
    bertInputs_Question = [input_ids_Question, token_type_ids_Question, attention_mask_Question]
    
    #bertInputs_Question = bLayer(n_fine_tune_layers = 3)(bertInputs_Question)
    bertInputs_Question, _ = bLayer(bertInputs_Question)
    
    #Concat Layer
    concat = tf.keras.layers.Concatenate()([bertOutput_Document, bertInputs_Question])
    
    denseLayer = tf.keras.layers.Dense(128, activation = 'relu')(concat)
    denseLayer = tf.keras.layers.Dense(256, activation = 'relu')(denseLayer)
    denseLayer = tf.keras.layers.Dense(256, activation = 'relu')(denseLayer)
    denseLayer = tf.keras.layers.Dense(128, activation = 'relu')(denseLayer)
    denseLayer = tf.keras.layers.Flatten()(denseLayer)
    denseLayer = tf.keras.layers.Dense(2)(denseLayer)
    
    model = tf.keras.Model(inputs = [input_ids_Document, token_type_ids_Document, attention_mask_Document, input_ids_Question, token_type_ids_Question, attention_mask_Question],
                           outputs = [denseLayer])
    return model
    `
Generator

```

    class trainGenSeq_short_YesNo(tf.keras.utils.Sequence, ):
    def __init__(self, batchSize, sentenceLength):
        self.batchSize = batchSize
        self.trainFiles = os.listdir('D:/Python/Datasets/v1.0/train/')
        self.trainingSamples = 307372 * 2
        self.sentenceLength = sentenceLength
        
        #Load Vocab
        slow_tokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"")
        save_path = ""bert_base_uncased/""
        if not os.path.exists(save_path):
            os.makedirs(save_path)
        slow_tokenizer.save_pretrained(save_path)
        self.tokenizer = BertTokenizer('vocab.txt', lowercase = True)
        self.vocabSize = len(self.tokenizer.vocab)
        
    
    def __len__(self):
        return int(self.trainingSamples // self.batchSize)
    
    def getLen(self):
        return int(self.trainingSamples // self.batchSize)
    
    def attentionMasks(self,input_dims):
        return [int(id > 0) for id in input_dims]
        
    def inputDims(self, dims):
        return pad_sequences([dims], maxlen = self.sentenceLength, dtype=""long"", value=0, truncating=""post"", padding=""post"")[0]
    
    def encode_sentence(self, sentence):
        sentence = sent_tokenize(sentence)
        ans = []
        for i in range(len(sentence)):
            encode_sent = self.tokenizer.encode(sentence[i],add_special_tokens = True)
            ans += encode_sent

        ans = pad_sequences([ans], maxlen = self.sentenceLength, dtype = ""long"", value = 0, truncating = ""post"", padding = ""post"")
        return ans[0]
    
    def __getitem__(self, _):
        documentStack = np.array([])
        questionStack = np.array([])
        answerStack = np.array([])
        
        document_AttStack = np.array([])
        question_AttStack = np.array([])

        document_SegStack = np.array([])
        question_SegStack = np.array([])

        First = True
        
        for file in self.trainFiles:
            for line in open('D:/Python/Datasets/v1.0/train/' + file):
                file = json.loads(line)
                #annotations
                if file.get('annotations')[0].get('short_answers'):
                    s_Start = file.get('annotations')[0].get('short_answers')[0].get('start_token')
                    s_End = file.get('annotations')[0].get('short_answers')[0].get('end_token')
                    l_Start = file.get('annotations')[0].get('long_answer').get('start_token')
                    l_End = file.get('annotations')[0].get('long_answer').get('end_token')

                    #Question and Title
                    question = file.get('question_text')

                    #document
                    document = []
                    for indexs in file.get('document_tokens')[l_Start:l_End]:
                        if indexs.get('html_token') == False:
                            document.append(indexs.get('token'))
                    
                    #Fake Document OR No document
                    fake = []
                    randomNumber = random.randint(7500, 9000)
                    front = random.choice([True, False])
                    
                    if front:
                        try:
                            for indexs in range(max(0, l_Start - randomNumber), min(len(file.get('document_tokens')),l_End - randomNumber)):
                                if file.get('document_tokens')[indexs].get('html_token') == False:
                                    fake.append(file.get('document_tokens')[indexs].get('token'))
                                else:
                                    indexs -= 1
                        except:
                            for indexs in range(max(0, l_Start + randomNumber), min(len(file.get('document_tokens')),l_End + randomNumber)):
                                if file.get('document_tokens')[indexs].get('html_token') == False:
                                    fake.append(file.get('document_tokens')[indexs].get('token'))
                                else:
                                    indexs -= 1
                    else:
                        try:
                            for indexs in range(max(0, l_Start + randomNumber), min(len(file.get('document_tokens')),l_End + randomNumber)):
                                if file.get('document_tokens')[indexs].get('html_token') == False:
                                    fake.append(file.get('document_tokens')[indexs].get('token'))
                                else:
                                    indexs -= 1
                        except:
                            for indexs in range(max(0, l_Start - randomNumber), min(len(file.get('document_tokens')),l_End - randomNumber)):
                                if file.get('document_tokens')[indexs].get('html_token') == False:
                                    fake.append(file.get('document_tokens')[indexs].get('token'))
                                else:
                                    indexs -= 1
                    
                    document = ' '.join(document)
                    fake = ' '.join(document)

                    document = self.encode_sentence(document)
                    fake = self.encode_sentence(fake)
                    question = self.encode_sentence(question)
                    
                    fake_AttentionMask = self.attentionMasks(fake)
                    document_AttentionMask = self.attentionMasks(document)
                    question_AttentionMask = self.attentionMasks(question)
                    
                    fake_SegID = [0 for _ in range(len(fake))]
                    document_SegID = [0 for _ in range(len(document))]
                    question_SegID = [0 for _ in range(len(question))]

                    if First:
                        #Document
                        documentStack = np.array([document])
                        documentStack = np.append(documentStack, np.array([fake]), axis = 0)
                        document_AttStack = np.array([document_AttentionMask])
                        document_AttStack = np.append(document_AttStack, np.array([fake_AttentionMask]), axis = 0)
                        document_SegStack = np.array([document_SegID])
                        document_SegStack = np.append(document_SegStack, np.array([fake_AttentionMask]), axis = 0)
                        
                        #Add Question Again
                        questionStack = np.array([question])
                        questionStack = np.append(questionStack, np.array([question]), axis = 0)
                        
                        question_AttStack = np.array([question_AttentionMask])
                        question_AttStack = np.append(question_AttStack, np.array([question_AttentionMask]), axis = 0)
                        question_SegStack = np.array([question_SegID])
                        question_SegStack = np.append(question_SegStack, np.array([question_SegID]), axis = 0)
                        
                        #Add Answer
                        answerStack = np.array([np.array([1,0])])
                        answerStack = np.append(answerStack, np.array([np.array([0,1])]), axis = 0)
                        
                        First = False
                    else:
                        documentStack = np.append(documentStack, np.array([document]), axis = 0)
                        documentStack = np.append(documentStack, np.array([fake]), axis = 0)
                        questionStack = np.append(questionStack, np.array([question]), axis = 0)
                        questionStack = np.append(questionStack, np.array([question]), axis = 0)
                        answerStack = np.append(answerStack, np.array([np.array([1,0])]), axis = 0)
                        answerStack = np.append(answerStack, np.array([np.array([0,1])]), axis = 0)
                        
                        #Attention Mask
                        document_AttStack = np.append(document_AttStack, np.array([document_AttentionMask]), axis = 0)
                        document_AttStack = np.append(document_AttStack, np.array([fake_AttentionMask]), axis = 0)
                        question_AttStack = np.append(question_AttStack, np.array([question_AttentionMask]), axis = 0)
                        question_AttStack = np.append(question_AttStack, np.array([question_AttentionMask]), axis = 0)
                        
                        #SegmentIDs
                        document_SegStack = np.append(document_SegStack, np.array([document_AttentionMask]), axis = 0)
                        document_SegStack = np.append(document_SegStack, np.array([fake_AttentionMask]), axis = 0)
                        question_SegStack = np.append(question_SegStack, np.array([question_SegID]), axis = 0)
                        question_SegStack = np.append(question_SegStack, np.array([question_SegID]), axis = 0)
                
                if documentStack.shape[0] == self.batchSize:
                    documentStack = np.reshape(documentStack, (documentStack.shape[0], 1, documentStack.shape[1]))
                    questionStack = np.reshape(questionStack, (questionStack.shape[0], 1, questionStack.shape[1]))
                    answerStack = np.reshape(answerStack, (answerStack.shape[0], 1, answerStack.shape[1]))
                    First = True

                    #print(type(documentStack), type(questionStack), type(answerStack))
                    return [np.squeeze(documentStack), np.squeeze(document_AttStack), np.squeeze(document_SegStack), 
                            np.squeeze(questionStack), np.squeeze(question_AttStack), np.squeeze(question_SegStack)], np.squeeze(answerStack)
                    
                    documentStack = None
                    titleStack = None
                    questionStack = None
                    answerStack = None

trainGen = trainGenSeq_short_YesNo(BatchSize, SeqLength)
```



Fitting and Compiling


```
model = createModel_YesNo(trainGen.vocabSize, BatchSize, SeqLength)
model.summary()
model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')
model.fit(trainGen, epochs = 10, steps_per_epoch = trainGen.getLen(),verbose = 1)
```



TF Version = 2.0.0
CUDA = 10.0
CUDDN = 7.6.5
GPU = GTX 1060
Operating System: Windows 10

**Other info / logs** 
I have tried turning on GPU growth that did not help.

"
44082,Input shape for convolution1D ,"PI have sparse coefficients array of shape 1D. Each value in the 1D array corresponds to one class. For example, I just have an array
a = [1,2,3,4,4,5,6,78,8543,35,878,.............]
class_label = [1,1,1,2,2,3,3,1,2,......]

How should I input this to Convolution1D? It always gives me an error of input dimensions."
44079,Getting errors when converting a simple convs model to INT8 TFLite,"**System information**
- OS Platform and Distribution: **Ubuntu 18.04**
- TensorFlow installed from binary
- TensorFlow version: **tf-nightly (2.4.0-dev20201015)** and **tf-2.3.0**
- TensorFlow Model Optimization version: **0.5.0**

**Command used to run the converter or code if youre using the Python API**

FP32 TFLite is work, but INT8 TFLite is not work with tf-nightly.

Please check the [gist](https://colab.research.google.com/drive/14glMvUN1H2PZHB2VPOwX9sZX9E2RNu0J?usp=sharing) \(tf-nightly\).

1. Can not convert quantization aware model to INT8 TFLite.

2. `converter.inference_input_type = tf.int8` and `converter.inference_output_type = tf.int8` are not work.

3. FP32 TFLite is work.

Item 2 has been fixed in 2.3.0 \(#36024\), but still has another error.

Please check the [gist](https://colab.research.google.com/drive/1ejLyom6qeGJlxkk0jL62OF7ybrzzX3Wu?usp=sharing) \(tf-2.3.0\)

**The output from the converter invocation**

Item 1:

```
RuntimeError: tensorflow/lite/kernels/quantize.cc:113 affine_quantization->scale->size == 1 was not true.Node number 0 (QUANTIZE) failed to prepare.
```

Item 2:

```
ValueError: The inference_input_type and inference_output_type must be tf.float32.
```

**Failure details**

Get errors in tensorflow

Thanks in advance."
44078,Tensorflow lite object detection  andriod examples app run on my phone but nothing detection ,"[GitHub Policy](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android),


**System information**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: xiaomi mi 6x

**Describe the current behavior**
build is OK
run is OK 
but  can not detection erverthing object : 
![WeChat Image_20201016154631](https://user-images.githubusercontent.com/36255328/96235722-b8ea3080-0fcd-11eb-96e6-d8a11b6bf88f.jpg)
"
44075,How do we know the compatibility of flatbuffers version in SchemaGenerated.h ,"After building the flatbuffers latest release and combining with the latest SchemaGenerated.h, there's an error as below

> Error	1	error LNK2001: unresolved external symbol ""private: static class flatbuffers::ClassicLocale flatbuffers::ClassicLocale::instance_"" (?instance_@ClassicLocale@flatbuffers@@0V12@A)

This error is mostly due to incompatibility between flatbuffers version in my system and version used for SchemaGenerated.h
FLatbuffers version used: 1.12.0


"
44074,"NaN occurs when building with GaussianNoise, GaussianNoise and Dense","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):tensorflow2.1.0-GPU
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I got the NaN as loss when I tried to train my model with GaussianNoise, GaussianNoise and Dense
**Describe the expected behavior**
The loss should be a number
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
batch_size = 110
epochs = 128
num_classes = 10
import os
model_name = 'model.h5'
import tensorflow.keras as keras
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
img_rows, img_cols = x_train.shape[1], x_train.shape[2]

x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
input_shape = (img_rows, img_cols, 1)
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
model = keras.models.Sequential()
model.add(keras.layers.GaussianNoise(stddev=0.7498748441096037))

model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(num_classes, activation='relu'))
model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))
model.save(model_name)
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
Epoch 1/128

  110/60000 [..............................] - ETA: 43:48 - loss: 9.9702 - accuracy: 0.0636
  660/60000 [..............................] - ETA: 7:18 - loss: 9.6680 - accuracy: 0.0848 
 1100/60000 [..............................] - ETA: 4:24 - loss: 9.4182 - accuracy: 0.0991
 1540/60000 [..............................] - ETA: 3:10 - loss: nan - accuracy: 0.1019   
 1760/60000 [..............................] - ETA: 2:47 - loss: nan - accuracy: 0.1000
 2200/60000 [>.............................] - ETA: 2:14 - loss: nan - accuracy: 0.0995
 2530/60000 [>.............................] - ETA: 1:58 - loss: nan - accuracy: 0.0992
 3080/60000 [>.............................] - ETA: 1:37 - loss: nan - accuracy: 0.1042
 3630/60000 [>.............................] - ETA: 1:23 - loss: nan - accuracy: 0.1047
```"
44073,`size` argument of TensorArray works only when specified by pythonic int but tf.Tensor doesn't,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux lz 5.4.0-48-generic #52~18.04.1-Ubuntu`
- TensorFlow installed from (source or binary):  `(in conda env) pip install tensorflow-gpu -i https://pypi.tuna.tsinghua.edu.cn/simple`
- TensorFlow version (use command below): `v2.3.0-rc2-23-gb36436b087 2.3.0`
- Python version: `3.8`
- CUDA/cuDNN version: `conda install cudatoolkit=10.1 cudnn=7.6.5`
- GPU model and memory:  ` 4 x Titan xp 12GB`

**Describe the current behavior**

Reference issue: #43698 
Thanks for @saxenasaurabh 's answer. I could initialized the `TensorArray` with a fixed size. However, when I tried to specify the size of the `TensorArray` with tf.Tensor from `tf.shape`, the length of the tensor after stacked together will be None:
```
emb.shape: TensorShape([2, 3, 180, 320, 32])
cor_l.shape: TensorShape([None, 2, 180, 320])
cor_prob1.shape: TensorShape([2, None, 180, 320])
cor_prob2.shape: TensorShape([2, None, 180, 320, 1])
cor_prob3.shape: TensorShape([2, None, 180, 320, 32])
cor_prob4.shape: TensorShape([2, 180, 320, None, 32])
cor_prob5.shape: TensorShape([2, 180, 320, None])
aligned_fea.shape: TensorShape([2, 180, 320, 3, 32])
aligned_fea.shape: TensorShape([2, 180, 320, 96])
```
Parts of the code are as following shows:
``` python
aligned_fea_shape = tf.shape(aligned_fea) # B, N, H, W, C
B = aligned_fea_shape[0]
N = aligned_fea_shape[1]
H = aligned_fea_shape[2]
W = aligned_fea_shape[3]
C = aligned_fea_shape[4]

####
some other codes omit here
####

emb = tf.reshape(emb, [B, N, H, W, -1])
tf.print(""emb.shape:"", emb.shape)

cor_l = tf.TensorArray(dtype=tf.float32, size=N) # TENSORFLOW BUG HERE !
def cond(i, N, input, arr):
    return tf.less(i, N)
def body(i, N, input, arr):
    emb_nbr = input[:, i, :, :, :]
    cor_tmp = tf.reduce_sum(emb_nbr * emb_ref, axis=3) # B, H, W
    arr = arr.write(i, cor_tmp)
    i = tf.add(i, 1)
    return i, N, input, arr
_, _, _, cor_l = tf.while_loop(cond, body, [0, N, emb, cor_l]) # N * (B, H, W)
cor_l = cor_l.stack() # N, B, H, W
tf.print(""cor_l.shape:"", cor_l.shape)

cor_l = tf.transpose(cor_l, [1, 0, 2, 3]) # B, N, H, W

cor_prob = tf.sigmoid(cor_l)  # B, N, H, W
tf.print(""cor_prob1.shape:"", cor_prob.shape)

cor_prob = tf.expand_dims(cor_prob, axis=4) # B, N, H, W, 1
tf.print(""cor_prob2.shape:"", cor_prob.shape)

cor_prob = tf.tile(cor_prob, [1, 1, 1, 1, C]) # B, N, H, W, C

cor_prob = tf.transpose(cor_prob, [0, 2, 3, 1, 4]) # B, H, W, N, C
cor_prob = tf.reshape(cor_prob, [B, H, W, -1]) # B, H, W, NC

aligned_fea = tf.transpose(aligned_fea, [0, 2, 3, 1, 4])  # [B, H, W, N, C]
tf.print(""aligned_fea.shape:"", aligned_fea.shape)

aligned_fea = tf.reshape(aligned_fea, [B, H, W, -1]) * cor_prob
tf.print(""aligned_fea.shape:"", aligned_fea.shape)
```
Look into the comment where `TENSORFLOW BUG` points out:
If `size` is specified by N, which is tf.Tensor, the output will be None. Otherwise, the size is designated by python int, eg. `cor_l = tf.TensorArray(dtype=tf.float32, size=self.nframe)` where self.nframe is pythonic int, everything is fine and the first size of the tensor will be fixed. The output is as follows:
```
emb.shape: TensorShape([2, 3, 180, 320, 32])
cor_l.shape: TensorShape([3, 2, 180, 320])
cor_prob1.shape: TensorShape([2, 3, 180, 320])
cor_prob2.shape: TensorShape([2, 3, 180, 320, 1])
cor_prob3.shape: TensorShape([2, 3, 180, 320, 32])
cor_prob4.shape: TensorShape([2, 180, 320, 3, 32])
cor_prob5.shape: TensorShape([2, 180, 320, 96])
aligned_fea.shape: TensorShape([2, 180, 320, 3, 32])
aligned_fea.shape: TensorShape([2, 180, 320, 96])
```

Thus I am wondering if this is a bug of TensorFlow since the documentation says:
```
size: (optional) int32 scalar `Tensor`: the size of the TensorArray. Required if handle is not provided.
```

Please feel free to contact me if more testing code is required."
44072,CUDNN_STATUS_INTERNAL_ERROR with MirroredStrategy under graph mode when using tf.summary,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):2.3.1
- Python version:3.7.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):6.3.0
- CUDA/cuDNN version: CUDA Version: 10.1 
- GPU model and memory: Tesla T4 * 2


**Describe the current behavior**
**When I remove the tf.summary.write(tag=""prediction_"" + key, tensor=prediction_concat, step=batch), it works well without error.**

**But With tf.summary.write(tag=""prediction_"" + key, tensor=prediction_concat, step=batch).** It has the error: 
tensorflow.python.framework.errors_impl.UnknownError: 3 root error(s) found.
  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[{{node while/body/_17/while/StatefulPartitionedCall/replica_1/conv2d_1/Conv2D}}]]
         [[DeleteIterator/_102]]
  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[{{node while/body/_17/while/StatefulPartitionedCall/replica_1/conv2d_1/Conv2D}}]]
  (2) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[{{node while/body/_17/while/StatefulPartitionedCall/replica_1/conv2d_1/Conv2D}}]]
         [[while/body/_17/while/StatefulPartitionedCall/replica_1/conv2d_1/BiasAdd/_109]]
0 successful operations.


**Describe the expected behavior**
in the infer() function, the result from serving func will be written as tfevents by tf.summary.


**Standalone code to reproduce the issue**

```
import tensorflow as tf  
from tensorflow.keras import layers  

class Evaluation:

    def __init__(self, strategy=None):  
        self.strategy = strategy
        H, W, C = 10, 10, 3
        imgs = tf.zeros([8, H, W, C])
        self.dataset = tf.data.Dataset.from_tensor_slices(imgs)
        self.dataset = self.dataset.batch(4)
        self.dataset = self.strategy.experimental_distribute_dataset(self.dataset)
        with self.strategy.scope():
            self.conv1 = layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same')
            self.conv2 = layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same')

    @tf.function
    def serving(self, img):
        prediction1 = self.conv1(img)
        prediction2 = self.conv2(img)
        return {
            'pre1': prediction1,
            'pre2': prediction2,
        }

    @tf.function
    def infer(self, serve_summary_writer, key_list):
        with serve_summary_writer.as_default():
            batch = tf.cast(0, tf.int64)
            for img in self.dataset:
                prediction_perReplica = strategy.run(self.serving, args=(img,))
                tf.print(""prediction_perReplica:"", prediction_perReplica)
                for key in key_list:
                    prediction_tensor = prediction_perReplica[key].values
                    prediction_concat = tf.concat(prediction_tensor, axis = 0)
                    tf.summary.write(tag=""prediction_"" + key, tensor=prediction_concat, step=batch)
                batch += 1
                
    def eval(self):
        serve_summary_writer = tf.summary.create_file_writer('tmp', max_queue=100000, flush_millis=100000)
        key_list = [""pre1"", ""pre2""]
        self.infer(serve_summary_writer, key_list)
        serve_summary_writer.close()
        tf.io.gfile.rmtree('tmp')  

if __name__ == ""__main__"":

    strategy = tf.distribute.MirroredStrategy()
    e = Evaluation(strategy)   
    e.eval()
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
2020-10-16 04:24:03.428115: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-16 04:24:04.441402: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2020-10-16 04:24:05.457372: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2020-10-16 04:24:05.476782: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2020-10-16 04:24:05.489398: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2020-10-16 04:24:05.491341: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2020-10-16 04:24:05.495273: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2020-10-16 04:24:05.497372: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2020-10-16 04:24:05.501205: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
Traceback (most recent call last):
  File ""test.py"", line 53, in <module>
    e.eval()
  File ""test.py"", line 41, in eval
    self.infer(serve_summary_writer, key_list)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 846, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 550, in call
    ctx=ctx)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.UnknownError: 3 root error(s) found.
  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[{{node while/body/_17/while/StatefulPartitionedCall/replica_1/conv2d_1/Conv2D}}]]
         [[DeleteIterator/_102]]
  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[{{node while/body/_17/while/StatefulPartitionedCall/replica_1/conv2d_1/Conv2D}}]]
  (2) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
         [[{{node while/body/_17/while/StatefulPartitionedCall/replica_1/conv2d_1/Conv2D}}]]
         [[while/body/_17/while/StatefulPartitionedCall/replica_1/conv2d_1/BiasAdd/_109]]
0 successful operations.
0 derived errors ignored. [Op:__inference_infer_436]

Function call stack:
infer -> infer -> infer"
44071,Tensorflow lite application execution on CPU only,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

Hello,
We are trying to execute TFLite application (label_image) on RPi CPU only. We tried cross compilation for aarch64(RaspberryPi). Here are the steps
1. Installed bazel
2. git clone https://github.com/tensorflow/tensorflow.git
    cd tensorflow
3. ./configure

> You have bazel 3.1.0 installed.
> Please specify the location of python. [Default is /usr/bin/python3]: 
> Found possible Python library paths:
>   /usr/local/lib/python3.6/dist-packages
>   /usr/lib/python3/dist-packages
> Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.6/dist-packages]
> 
> Do you wish to build TensorFlow with ROCm support? [y/N]: n
> No ROCm support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with CUDA support? [y/N]: n
> No CUDA support will be enabled for TensorFlow.
> 
> Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
> Clang will not be downloaded.
> 
> Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 
> 
> 
> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
> Not configuring the WORKSPACE for Android builds.
> 
> Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
> 	--config=mkl         	# Build with MKL support.
> 	--config=monolithic  	# Config for mostly static monolithic build.
> 	--config=ngraph      	# Build with Intel nGraph support.
> 	--config=numa        	# Build with NUMA support.
> 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
> 	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
> Preconfigured Bazel build configs to DISABLE default on features:
> 	--config=noaws       	# Disable AWS S3 filesystem support.
> 	--config=nogcp       	# Disable GCP support.
> 	--config=nohdfs      	# Disable HDFS support.
> 	--config=nonccl      	# Disable NVIDIA NCCL support.
> Configuration finished

4.  bazel build --config=elinux_aarch64 //tensorflow/lite/examples/label_image:label_image

Build was successful, but not sure that application executed on CPU only or it is using GPU as well.
1. Can you please help us with running tensorflow_lite application on CPU only(disable GPU)?
2. How to check whether application using both CPU & GPU or only CPU?

Thanks

"
44070,The weights data in tflite model is still in float32 format,"**System information**
- Windows 10
- wheel
- 1.14.0 CPU


 **I convert the tflite from a saved_model as the follow API, and got a tflite model, then I use Netron to open it, why the weights
data in the tflite model is still float32, shouldn't it be int8?**

```
# tf.lite.TFLiteConverter.from_saved_model(model_path)
```
"
44069,tf.keras.layers.experimental.preprocessing.TextVectorization doesn't raise error when  len(vocabulary_list)<=max_tokens<len(vocabulary_list) + 2,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): tf2.3
- Python version: 3.7

**Describe the current behavior**
It doesn't raise error :
```
vocabulary_list = [""a"", ""b"", ""c""]
text_dataset = tf.data.Dataset.from_tensor_slices(vocabulary_list)
max_features = 3 # Maximum vocab size.

# Create the layer.
vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(
 max_tokens=max_features)

# Now that the vocab layer has been created, call `adapt` on the text-only
# dataset to create the vocabulary. You don't have to batch, but for large
# datasets this means we're not keeping spare copies of the dataset.
vectorize_layer.adapt(text_dataset.batch(64))
```
This will lead to wrong result : 
```
vectorize_layer([""a b c d e""])
# output :
# <tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[1, 1, 2, 1, 1]])>
```

**Describe the expected behavior**
Raise error :
```
ValueError: Attempted to set a vocabulary larger than the maximum vocab size. Passed vocab size is 5, max vocab size is 3.
```
"
44068,tf dataset sample_weight is not passed to custom metrics (or any metrics).,"**System information**
- v2.3.0-0-gb36436b087 2.3.0
- code is tested on google colab

**Describe the current behavior**
sample_weight as defined in tf.data.Dataset (not passed explicitly in model.fit(...)) is not being called with 
any metrics' update_state(self, y_true, y_pred, sample_weight). 

**Describe the expected behavior**
sample_weight should be passed to update_state of metrics.

**Standalone code to reproduce the issue**
Colab Notebook
https://colab.research.google.com/drive/1eSVeRuUv0q_KbSHQ2C4AFSIep_HIt1mn?usp=sharing

```
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

x = np.random.randn(10, 1).astype('float32')
y = np.random.randn(10, 1).astype('float32')
w = np.random.randint(0, 2, (10, 1)).astype('float32')   # sample weight

ds = tf.data.Dataset.from_tensor_slices((x, y, w)).batch(2)

model = Sequential()   # simple linear regression
model.add(Dense(1))

# Custom metrics, we will just sum up the sample_weight

class TestSampleWeight(tf.keras.metrics.Metric):
  def __init__(self, name=""test_sample_weight"", **kwargs):
    super(TestSampleWeight, self).__init__(name=name, **kwargs)
    self.sample_weight_sum = self.add_weight(name=""sws"", initializer=""zeros"")

  def update_state(self, y_true, y_pred, sample_weight=None):
    y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))
    values = tf.cast(y_true, ""int32"") == tf.cast(y_pred, ""int32"")
    values = tf.cast(values, ""float32"")
    
    # self.sample_weight_sum.assign_add(1)    # Uncomment to simple sanity test for metrics, comment out next 2 lines.

    sample_weight = tf.cast(sample_weight, ""float32"")
    self.sample_weight_sum.assign_add(sample_weight)

  def result(self):
    return self.sample_weight_sum

  def reset_states(self):
    self.sample_weight_sum.assign(0.0)

model.compile(optimizer='rmsprop', loss='mse', metrics=TestSampleWeight())

model.fit(ds)  # hit ValueError: None values not supported. sample_weight is None?? 
```

**Other info / logs**



"
44067,Expensive decompression in JPEG GetImageInfo,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.3
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
ExtractJpegShape is as expensive to evaluate as full JPEG decompression on non-baseline JPEGs.
**Describe the expected behavior**
ExtractJpegShape is much cheaper than full JPEG decompression.

**Fix and Code To Reproduce**
https://github.com/tensorflow/tensorflow/pull/44066
"
44065,Input shape for convolution1D,"I have sparse coefficients array of shape 1D. Each value in the 1D array corresponds to one class. For example, I just have an array 
a = [1,2,3,4,4,5,6,78,8543,35,878,.............]
class_label = [1,1,1,2,2,3,3,1,2,......]

How should I input this to Convolution1D? It always gives me an error of input dimensions."
44062,"tflite model maker export error, attribute ModelMetadataT not found","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Following exactly the stock example https://www.tensorflow.org/lite/tutorials/model_maker_image_classification
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):2.3.1
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Trying to run the example, everything goes fine and the model trains successfully, however when trying to finally export
the model
model.export(export_dir='.')
I get this error
AttributeError: module 'tensorflow_lite_support.metadata.metadata_schema_py_generated' has no attribute 'ModelMetadataT'

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
AttributeError                            Traceback (most recent call last)
<ipython-input-3-4c3d25dac6cc> in <module>
----> 1 model.export(export_dir='.')

~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\tensorflow_examples\lite\model_maker\core\task\image_classifier.py in export(self, export_dir, tflite_filename, label_filename, saved_model_filename, export_format, **kwargs)
    305       **kwargs: Other parameters like `quantized` for TFLITE model.
    306     """"""
--> 307     super(ImageClassifier, self).export(
    308         export_dir,
    309         tflite_filename=tflite_filename,

~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\tensorflow_examples\lite\model_maker\core\task\custom_model.py in export(self, export_dir, tflite_filename, label_filename, vocab_filename, saved_model_filename, export_format, **kwargs)
    161       with_metadata = kwargs.get('with_metadata', True)
    162       tflite_filepath = os.path.join(export_dir, tflite_filename)
--> 163       self._export_tflite(tflite_filepath, **kwargs)
    164     else:
    165       with_metadata = False

~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\tensorflow_examples\lite\model_maker\core\task\image_classifier.py in _export_tflite(self, tflite_filepath, quantization_config, with_metadata, export_metadata_json_file)
    344         populator = metadata_writer.MetadataPopulatorForImageClassifier(
    345             tflite_filepath, model_info, label_filepath)
--> 346         populator.populate()
    347 
    348       # Validate the output model file by reading the metadata and produce

~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\tensorflow_examples\lite\model_maker\core\task\metadata_writer_for_image_classifier.py in populate(self)
     87   def populate(self):
     88     """"""Creates metadata and then populates it for an image classifier.""""""
---> 89     self._create_metadata()
     90     self._populate_metadata()
     91 

~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\tensorflow_examples\lite\model_maker\core\task\metadata_writer_for_image_classifier.py in _create_metadata(self)
     94 
     95     # Creates model info.
---> 96     model_meta = _metadata_fb.ModelMetadataT()
     97     model_meta.name = self.model_info.name
     98     model_meta.description = (""Identify the most prominent object in the ""

AttributeError: module 'tensorflow_lite_support.metadata.metadata_schema_py_generated' has no attribute 'ModelMetadataT'"
44061,"TensorFlow C API vs. TensorRT: ""Op type not registered 'TRTEngineOp' in binary running on ...""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): docker image (binary)
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0
- GPU model and memory: GTX 1660 Ti, mobile, 6GB

**Describe the current behavior**
- I have a SavedModel that I convert to a TensorRT-optimized model using trt.TrtGraphConverterV2
- I try to load the resulting model via the TensorFlow C API
- My C++ program displays the following error and exits:
  Op type not registered 'TRTEngineOp' in binary running on [docker container]. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) 'tf.contrib.resampler' should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.'

**Describe the expected behavior**
I expect the model to be loaded and to be able to use it for inference, as I can with a non-TensorRT-optimized model.

**Standalone code to reproduce the issue**
I'm happy to post code but it seems all you have to do is load a model with TF_LoadSessionFromSavedModel (which doesn't exit) and then, in my case, when I call TF_GraphOperationByName, it prints the error and exits.

**Other info / logs**
If you google this error message, a lot of people have encountered this problem over the last 2 years.
Some people were able to solve it with much earlier versions of TensorFlow by calling:
  TF_LoadLibrary(""_trt_engine_op.so"", status);
But this does not work for me, the docker image doesn't even seem to have this .so file anywhere, and other people online claim the call is unnecessary.
It seems that TensorRT has been folded into TensorFlow version 2, but maybe somebody forgot to put it into the C API libraries?
"
44060,"[TfLite] gpu gl delegate: reduce_mean([1,2]) returns average of first row only ","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nexus 5X
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.2.0
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

Given a simple model that computes the average intensity of a square, grayscale image:

```python
input = Input(shape=(16,16), batch_size=1, dtype=tf.float32)
output = tf.math.reduce_mean(input, axis = [1, 2])
model = Model(inputs=input, outputs=output, name='mean')
```

when no delegate is used, the average is correct, but when the GPU delegate is used with a SSBO,
the returned value is the average of the first row only, not that of the whole image as it should be. 
I have tested thoroughly that the SSBO values are correct by reading them with glMapBufferRange.
The SSBO array has exactly 16x16 values and the result is always the average of pixels [0, 16, 32, ...].

I can't see issues with lite/delegates/gpu/gl/kernels/mean.h , so I think the wrong dimension is being picked for some reason.

**Describe the expected behavior**

`reduce_mean` should return the average along the two dimensions.

**Other info / logs** 

To enable SSBOs I am using a slightly modified version of TFLite 2.2.0 (https://github.com/natario1/tensorflow/commit/7401fbb4fa0c94004865c089d8c89bdd566ad747) and passing in this object def:

```cpp
tflite::gpu::ObjectDef get_gpu_object_def() {
    tflite::gpu::ObjectDef object_def;
    object_def.data_type = tflite::gpu::DataType::FLOAT32;
    object_def.data_layout = tflite::gpu::DataLayout::BHWC;
    object_def.object_type = tflite::gpu::ObjectType::OPENGL_SSBO;
    object_def.user_provided = true;
    return object_def;
}
```

Note that I use this same logic to run much more complex models (like MediaPipe models) and it works well, so I'm tempted to say that this is an issue with reduce_mean or with my test model.

[simple_model_16x16b.tflite.zip](https://github.com/tensorflow/tensorflow/files/5387188/simple_model_16x16b.tflite.zip)

**More info after investigation**

I found out that the issue is fixed if I explicitly add the channel dimension to the model, so 1x16x16x1 instead of 1x16x16.
This is not consistent with the CPU behavior (delegate off, no SSBOs, only CPU data), where a 1x16x16 model works just fine.

Also it's not clear why, if the last 1 is absent, the dimensions are misinterpreted in such a way that leads `reduce_mean` to only average the first row. I could understand this if we were *adding* an extra dimension equal to 1 in the wrong place (which could be taken as width or height), but in this case we are actually *removing* it."
44059,NotImplementedError: Cannot convert a symbolic Tensor (args_0:0) to a numpy array.,"**System information**
- This is custom code
- Running Google Colab on Mac
- TensorFlow version 2.3.0
- Python version 3.6.9
- XLA_GPU hosted by Colab; memory_limit = 15695549568

**Describe the current behavior**
I have a custom augmentation function written in Colab that worked normally until today. The last time I ran the entire code through was 09/08/2020 and the augmentation functioned performed the operations normally. Now, I receive an error that pertains to symbolic tensors, which I have never seen before.

**Describe the expected behavior**
Augmentation function is meant to map over a batch of images and masks, taking in samples one at a time, converting them to NumPy arrays, performing some sort of augmentation, then returning them back as tensors.

**Standalone code to reproduce the issue**
A link to the Colab notebook: https://colab.research.google.com/drive/1ujShezPjcveG2kg019c7zqweLtu1ESuW?usp=sharing
A link to the training data I use in the notebook: https://drive.google.com/drive/folders/1ZS1wKjo692Lg7Vuhtr3akz0sYSOmZBKl

The particular section of code where the error stems from:
```
# Function used to perform ""on-the-fly"" augmentation during training.
# UPDATED ON 09/21/2020.

def augmentation(img, msk):

  # Call in skimage package, which will be used for transformations.
  from skimage.transform import rotate, AffineTransform, warp
  
  # Create some random floats, which will be used in augmentation steps.
  tilt = tf.random.uniform(shape = [], minval = -90, maxval = 90, dtype = tf.float32)
  dx = tf.random.uniform(shape = [], minval = -20, maxval = 20, dtype = tf.float32)
  dy = tf.random.uniform(shape = [], minval = -20, maxval = 20, dtype = tf.float32)
  
  # Cast image and mask to numpy arrays.
  img = np.array(img)
  msk = np.array(msk)

  # Use TensforFlow-style if conditionals, used to flip image and mask.
  img = tf.cond(tilt > 0, lambda: np.fliplr(img), lambda: np.flipud(img))
  msk = tf.cond(tilt > 0, lambda: np.fliplr(msk), lambda: np.flipud(msk))

  # Rotate the image and mask to some degree.
  img = rotate(img, angle = tilt, mode = 'reflect')
  msk = rotate(msk, angle = tilt, mode = 'reflect')

  # Write the conditions for an affine transformation.
  transform = AffineTransform(translation = (dx,dy))

  # Perform the affine transformation.
  img = warp(img, inverse_map = transform, mode = 'reflect')
  msk = warp(msk, inverse_map = transform, mode = 'reflect')
 
  # Convert the inputs back into tensors, put back into a tuple.
  finalTuple = (tf.convert_to_tensor(img), tf.convert_to_tensor(msk))

  return finalTuple

# Callback for data augmentation.
class aug(tf.keras.callbacks.Callback):
  def on_training_batch_begin(self, batch, logs = None):
    batch.map(augmentation, num_parallel_calls = 5)
    batch.shuffle(10)
    
# Callback for CSV logger (used for charting).
csv = tf.keras.callbacks.CSVLogger(f'/content/gdrive/My Drive/{today}_metrics.csv', separator=',', append=False)

# Callback for saving the model.
save_model_path = f'/content/gdrive/My Drive/{today}_wellpad_model_.h5'
cp = tf.keras.callbacks.ModelCheckpoint(filepath=save_model_path,
                                        monitor='val_loss',
                                        mode='min',
                                        save_best_only=True)
```
```
# Test the augmentation function on a batch of images.
test_batch = evaluation.take(1)

test_batch = test_batch.map(augmentation,num_parallel_calls=5) # ERROR OCCURS HERE.

# For plotting below.
test_batch = [(image,mask) for image, mask in test_batch]

plt.figure(figsize=(10,10))
plt.subplot(1,2,1)
# Show the original image.
plt.imshow(test_batch[0][0][0])
# Show the masked image.
plt.subplot(1,2,2)
plt.imshow(tf.squeeze(test_batch[0][1][0]))
```

**Other info / logs** 
Here is the full error I receive:
```
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-17-81c1086f76f0> in <module>()
      2 test_batch = evaluation.take(1)
      3 
----> 4 test_batch = test_batch.map(augmentation,num_parallel_calls=5)
      5 
      6 test_batch = [(image,mask) for image, mask in test_batch]

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls, deterministic)
   1700           num_parallel_calls,
   1701           deterministic,
-> 1702           preserve_cardinality=True)
   1703 
   1704   def flat_map(self, map_func):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)
   4082         self._transformation_name(),
   4083         dataset=input_dataset,
-> 4084         use_legacy_function=use_legacy_function)
   4085     if deterministic is None:
   4086       self._deterministic = ""default""

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)
   3369       with tracking.resource_tracker_scope(resource_tracker):
   3370         # TODO(b/141462134): Switch to using garbage collection.
-> 3371         self._function = wrapper_fn.get_concrete_function()
   3372         if add_to_graph:
   3373           self._function.add_to_graph(ops.get_default_graph())

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in get_concrete_function(self, *args, **kwargs)
   2937     """"""
   2938     graph_function = self._get_concrete_function_garbage_collected(
-> 2939         *args, **kwargs)
   2940     graph_function._garbage_collector.release()  # pylint: disable=protected-access
   2941     return graph_function

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)
   2904       args, kwargs = None, None
   2905     with self._lock:
-> 2906       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
   2907       seen_names = set()
   2908       captured = object_identity.ObjectIdentitySet(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-> 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3073             arg_names=arg_names,
   3074             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3075             capture_by_value=self._capture_by_value),
   3076         self._function_attributes,
   3077         function_spec=self.function_spec,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--> 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in wrapper_fn(*args)
   3362           attributes=defun_kwargs)
   3363       def wrapper_fn(*args):  # pylint: disable=missing-docstring
-> 3364         ret = _wrapper_helper(*args)
   3365         ret = structure.to_tensor_list(self._output_structure, ret)
   3366         return [ops.convert_to_tensor(t) for t in ret]

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in _wrapper_helper(*args)
   3297         nested_args = (nested_args,)
   3298 
-> 3299       ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
   3300       # If `func` returns a list of tensors, `nest.flatten()` and
   3301       # `ops.convert_to_tensor()` would conspire to attempt to stack

/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    256       except Exception as e:  # pylint:disable=broad-except
    257         if hasattr(e, 'ag_error_metadata'):
--> 258           raise e.ag_error_metadata.to_exception(e)
    259         else:
    260           raise

NotImplementedError: in user code:

    <ipython-input-16-3d72fc8870c2>:15 augmentation  *
        img = np.array(img)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:848 __array__  **
        "" a NumPy call, which is not supported"".format(self.name))

    NotImplementedError: Cannot convert a symbolic Tensor (args_0:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported
```

"
44058, go get github.com/tensorflow/tensorflow/tensorflow/go throwing an error,"Getting below error while installing tensorflow go module

```
-bash-4.2$ export LIBRARY_PATH=$LIBRARY_PATH:/home/gouser/lib/
-bash-4.2$ go get github.com/tensorflow/tensorflow/tensorflow/go
# github.com/tensorflow/tensorflow/tensorflow/go
/home/gouser/lib/libtensorflow.so: undefined reference to `std::length_error::length_error(char const*)@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::__throw_out_of_range_fmt(char const*, ...)@GLIBCXX_3.4.20'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::logic_error::logic_error(char const*)@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::domain_error::domain_error(char const*)@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `typeinfo for std::_V2::error_category@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::__atomic_futex_unsigned_base::_M_futex_notify_all(unsigned int*)@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `lgammaf@GLIBC_2.23'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::_V2::error_category::default_error_condition(int) const@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::_V2::error_category::equivalent(int, std::error_condition const&) const@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::_V2::error_category::~error_category()@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::_V2::generic_category()@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `__cxa_throw_bad_array_new_length@CXXABI_1.3.8'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::invalid_argument::invalid_argument(char const*)@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::_V2::error_category::equivalent(std::error_code const&, int) const@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::runtime_error::runtime_error(char const*)@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::_V2::system_category()@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::out_of_range::out_of_range(char const*)@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::__atomic_futex_unsigned_base::_M_futex_wait_until(unsigned int*, unsigned int, bool, std::chrono::duration<long, std::ratio<1l, 1l> >, std::chrono::duration<long, std::ratio<1l, 1000000000l> >)@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::underflow_error::underflow_error(char const*)@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::thread::_M_start_thread(std::shared_ptr<std::thread::_Impl_base>, void (*)())@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::range_error::range_error(char const*)@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::overflow_error::overflow_error(char const*)@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `operator delete(void*, unsigned long)@CXXABI_1.3.9'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::__future_base::_State_baseV2::_Make_ready::_M_set()@GLIBCXX_3.4.21'
/home/gouser/lib/libtensorflow.so: undefined reference to `lgamma@GLIBC_2.23'
/home/gouser/lib/libtensorflow.so: undefined reference to `operator delete[](void*, unsigned long)@CXXABI_1.3.9'
/home/gouser/lib/libtensorflow.so: undefined reference to `std::_V2::error_category::_M_message(int) const@GLIBCXX_3.4.21'
```

**System information**
- CentOS Linux release 7.6.1810 (Core):
- Python version:
-bash-4.2$ python --version
Python 2.7.6
-bash-4.2$ python3 --version
Python 3.6.8



"
44057,Embedding metadata is not read or displayed in TensorBoard 2.3.0,"Hello, I have trained a model (with TensorFlow 2.3.1) including embeddings via `keras.layers.Embedding` and prepared label files per embedding dictionary, which I point to via the `embeddings_metadata` parameter of `keras.callbacks.TensorBoard`.  TensorBoard executes and displays the various embedding vectors in the Projector without issue, but the embeddings are not labeled per the label files.  The label files are successfully created and stored in the log directory, and I can use them manually just fine via the `Load` button in the TensorBoard interface.

I debugged the `projector_plugin.py` code and determined that `_get_metadata_file_for_tensor()` is never called (verified via `prints`, debugging with `pdb`, etc.).  This method looks to be the only place in the code used to retrieve the `metadata_path` from which the labels would be read, and is apparently only accessed via the `/metadata` HTTP route.

Incidentally when I add the various `Embedding` layers in Keras, the first receives the `tensor_name` `layer_with_weights-0/embeddings/.ATTRIBUTES/VARIABLE_VALUE` but the subsequent ones are `layer_with_weights-1`, `2`, etc.  I see the `keras.callbacks.TensorBoard` hard-codes them to `layer_with_weights-0` but I think this is a separate issue as I have manually corrected the `projector_config.pbtxt` file and, as I mentioned above, the `_get_metadata_file_for_tensor()` method is never called anyway so they aren't even trying to be parsed and used to label tensors.

Any thoughts?  Am I missing something?

Thanks!


Todd"
44056,Segmentation fault (core dumped) error when using representative_dataset_gen(),"I used tf 2.3 and met segmentation fault (core dumped) error when using `representative_dataset_gen()`.
**Code**
```
    def representative_dataset_gen():
      for audio in validation_fingerprints:
        yield [audio]
    converter = tf.lite.TFLiteConverter.from_saved_model(flags.train_dir + '/last_model')
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
    converter.allow_custom_ops = True
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8
    # converter.representative_dataset = representative_dataset_gen
    quant_model = converter.convert()
    with open(flags.train_dir + '/quant_last_model.tflite', 'wb') as w:
      w.write(quant_model)
```
Above code can run without error but if I use `converter.representative_dataset = representative_dataset_gen`, it fail.

The type and shape of data are at below. The input layer size is `[Batch, 16384]`
```
type(validation_fingerprints): <class 'numpy.ndarray'>
shape(validation_fingerprints): (3093, 16384)
```

Any advice? Thanks!"
44055,what is y_train in tensorflow,"what is the meaning of y_train in this line of code?
`(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()`"
44053,TFDS: deep_weeds down,"This has been ongoing for several days:


tfds.load('deep_weeds', shuffle_files=True, as_supervised=True, try_gcs=True)

ConnectionError: HTTPSConnectionPool(host='nextcloud.qriscloud.org.au', port=443): Max retries exceeded with url: /index.php/s/a3KxPawpqkiorST/download (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7ff54e566470>: Failed to establish a new connection: [Errno 111] Connection refused',))

"
44052,full integer quantization error,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (or github SHA if from source):
Nightly

**Command used to run the converter or code if youre using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
The exact command
tflite_model = converter.convert()

**The output from the converter invocation**
Exception has occurred: ValueError
Failed to parse the model: pybind11::init(): factory function returned nullptr.

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
runtime error
Failed to parse the model: pybind11::init(): factory function returned nullptr.

the conversion works w/o quantization AND with dynamic range quantization. BUT, when I add representive dataset for converting full integer quantization with float fallback it fails with the above error.

I also use these flags:
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
since some of the operators are not yet implemented in tflite so maybe it's related to the root cause.

Would be happy to get your advice.

"
44051,There is no defined version as defined in Tensorflow website,"
![Screenshot (130)_LI](https://user-images.githubusercontent.com/53373012/96131702-28e9af80-0f17-11eb-8791-d915f4c3d1c1.jpg)
![Screenshot (131)](https://user-images.githubusercontent.com/53373012/96131712-2b4c0980-0f17-11eb-83ae-33c159b83228.png)

Ohk, So as defined in the tensorflow gpu documentation, the tensorflow 2.3 can have a cuda version of 10.1 and cuDNN version of 7.4. But there isn't any cuDNN v7.4 for CUDA 10.1, according to the nvidia cuDNN archives. So, please update the version of cuDNN in your documentation, to the specified CUDA and Tensorflow version.

## URL(s) with the issue:

Link to the tensorflow gpu section:
https://www.tensorflow.org/install/source_windows#gpu

"
44050,Training pipeline significantly slower for epoch > 1 with caching to disk,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Cloud AI platform
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
From PyPi
- TensorFlow version (use command below):
2.2
- Python version:
3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: 
NVIDIA P100 

**Describe the current behavior**
My training pipeline is significantly slower for epoch > 1

```
Epoch 1/100
3198/3198 - 2287s - loss: 2.7595 - binary_accuracy: 0.8080
Epoch 2/100
3198/3198 - 3974s - loss: 0.9275 - binary_accuracy: 0.8086
Epoch 3/100
3198/3198 - 3562s - loss: 0.5781 - binary_accuracy: 0.8124
Epoch 4/100
3198/3198 - 3572s - loss: 0.5227 - binary_accuracy: 0.8064
```

**Describe the expected behavior**
The TFRecords originate in GCS. I am using caching to disk, so I expect epoch > 1 to be faster than epoch=1

**Standalone code to reproduce the issue**
```
  train_data = tf.data.TFRecordDataset(train_files)
  train_data = train_data.cache(os.path.join(FLAGS.cache_dir, 'train'))
  train_data = train_data.map(training_parser)
  train_data = train_data.prefetch(4)

  val_data = None
  if val_files:
    val_data = tf.data.TFRecordDataset(val_files)
    val_data = val_data.cache(os.path.join(FLAGS.cache_dir, 'val'))
    val_data = val_data.map(validation_parser)
    val_data = val_data.prefetch(4)

  model.fit(
      x=train_data,
      validation_data=val_data,
      epochs=FLAGS.epochs,
      callbacks=[tensorboard_callback,
                 _HyperparmeterTuningCallback()],
      verbose=2)

```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
44048,Incompatible Layer Error in TF Sequential When Adding a Layer Stored in a Variable,"**System information**
- I write my own custom code 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): from pip
- TensorFlow version: 2.3.0
- Python version: 3.6.9

**How to Reproduce the Error**
>>> model = tf.keras.Sequential()
>>> model.add(tf.keras.layers.InputLayer(input_shape=[7]))
>>> layer = tf.keras.layers.Dense(units=20, activation='relu')
>>> model.add(layer)
>>> model.add(layer)

Attempting the second ""model.add(layer)"" will yield an error as follows:
``
ValueError: Input 0 of layer dense_7 is incompatible with the layer: expected axis -1 of input shape to have value 7 but received input with shape [None, 20]
``

It looks like this is a bug. 

The code will work if I do model.add(tf.keras.layers.Dense(units=20, activation='relu')) instead of model.add(layer). However, I can't do this since due to some project requirements, I need to assign tf.keras.layers.Dense(units=20, activation='relu') to a variable. 
"
44047,cuDNN in invalid state after OOM,"**System information**
- Have I written custom code
- OS Platform and Distribution:  Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.13.1-0-g6612da8951
- Python version: 3.6.9
- CUDA/cuDNN version: CUDA 10.0.130-1, cuDNN 7.6.5
- GPU model and memory: GeForce RTX 2070 super, 8 GiB

**Describe the current behavior**
After getting ResourceExhaustedError during an operation using cuDNN (for instance due to out of memory) it seems like cuDNN is left in a broken state and further calls using cuDNN fail even if they don't exceed the resources.

**Describe the expected behavior**
I would expect further calls to cuDNN to be ok. This is important since it seems that the only way to know if a computation will use too much resources (for instance when determining optimal batch size) is to actually try and fail.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np
from tensorflow.python.framework.errors_impl import ResourceExhaustedError

x = tf.placeholder(tf.float32, shape=[None, None, None, None, 32])
fil = tf.zeros((3, 3, 3, 32, 32), dtype=tf.float32)
conv = tf.nn.conv3d(x, filter=fil, strides=(1, 1, 1, 1, 1), padding='VALID', name=None)

with tf.Session() as sess:
    try:
        c = np.zeros((1, 370, 370, 370, 32), np.float32)
        sess.run(conv, feed_dict={x: c})
        r1 = sess.run(conv)
    except ResourceExhaustedError:
        print('Resource Error')

with tf.Session() as sess:
    c = np.zeros((1, 100, 100, 100, 32), np.float32)
    sess.run(conv, feed_dict={x: c})
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
2020-10-15 13:41:46.910748: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-10-15 13:41:47.076574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-15 13:41:47.077067: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1414860 executing computations on platform CUDA. Devices:
2020-10-15 13:41:47.077083: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2070 SUPER, Compute Capability 7.5
2020-10-15 13:41:47.096784: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3498230000 Hz
2020-10-15 13:41:47.097210: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x158f0a0 executing computations on platform Host. Devices:
2020-10-15 13:41:47.097229: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2020-10-15 13:41:47.097351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce RTX 2070 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.815
pciBusID: 0000:01:00.0
totalMemory: 7.79GiB freeMemory: 7.31GiB
2020-10-15 13:41:47.097368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2020-10-15 13:41:47.098194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-15 13:41:47.098208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2020-10-15 13:41:47.098215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2020-10-15 13:41:47.098286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7110 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-10-15 13:41:47.100978: W tensorflow/core/framework/allocator.cc:124] Allocation of 6483584000 exceeds 10% of system memory.
2020-10-15 13:42:01.641512: W tensorflow/core/common_runtime/bfc_allocator.cc:267] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.94GiB.  Current allocation summary follows.
2020-10-15 13:42:01.641594: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (256): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641616: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641638: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1024): 	Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2020-10-15 13:42:01.641656: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641672: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641688: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8192): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641704: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641720: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641742: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (65536): 	Total Chunks: 1, Chunks in use: 1. 108.0KiB allocated for chunks. 108.0KiB in use in bin. 108.0KiB client-requested in use in bin.
2020-10-15 13:42:01.641759: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641775: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641791: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641807: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641823: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641839: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641856: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641872: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641888: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641904: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641924: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-10-15 13:42:01.641942: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (268435456): 	Total Chunks: 2, Chunks in use: 1. 6.94GiB allocated for chunks. 6.04GiB in use in bin. 6.04GiB client-requested in use in bin.
2020-10-15 13:42:01.641960: I tensorflow/core/common_runtime/bfc_allocator.cc:613] Bin for 5.94GiB was 256.00MiB, Chunk State: 
2020-10-15 13:42:01.641986: I tensorflow/core/common_runtime/bfc_allocator.cc:619]   Size: 926.76MiB | Requested Size: 0B | in_use: 0, prev:   Size: 6.04GiB | Requested Size: 6.04GiB | in_use: 1
2020-10-15 13:42:01.642004: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7f86a6000000 of size 110592
2020-10-15 13:42:01.642017: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7f86a601b000 of size 1280
2020-10-15 13:42:01.642031: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7f86a601b500 of size 6483584000
2020-10-15 13:42:01.642043: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7f8828755900 of size 971780864
2020-10-15 13:42:01.642055: I tensorflow/core/common_runtime/bfc_allocator.cc:638]      Summary of in-use Chunks by size: 
2020-10-15 13:42:01.642071: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 1280 totalling 1.2KiB
2020-10-15 13:42:01.642087: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 110592 totalling 108.0KiB
2020-10-15 13:42:01.642101: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 6483584000 totalling 6.04GiB
2020-10-15 13:42:01.642116: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Sum Total of in-use chunks: 6.04GiB
2020-10-15 13:42:01.642136: I tensorflow/core/common_runtime/bfc_allocator.cc:647] Stats: 
Limit:                  7455476941
InUse:                  6483695872
MaxInUse:               6483695872
NumAllocs:                       3
MaxAllocSize:           6483584000

2020-10-15 13:42:01.642153: W tensorflow/core/common_runtime/bfc_allocator.cc:271] ***************************************************************************************_____________
2020-10-15 13:42:01.642223: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at conv_ops_3d.cc:161 : Resource exhausted: OOM when allocating tensor with shape[1,368,368,368,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Resource Error
2020-10-15 13:42:01.801362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2020-10-15 13:42:01.801391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-15 13:42:01.801396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2020-10-15 13:42:01.801400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2020-10-15 13:42:01.801462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7110 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-10-15 13:42:02.948064: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2020-10-15 13:42:02.967863: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
Traceback (most recent call last):
  File ""/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node Conv3D}}]]
	 [[{{node Conv3D}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py"", line 19, in <module>
    sess.run(conv, feed_dict={x: c})
  File ""/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node Conv3D (defined at /home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py:7) ]]
	 [[node Conv3D (defined at /home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py:7) ]]

Caused by op 'Conv3D', defined at:
  File ""/home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py"", line 7, in <module>
    conv = tf.nn.conv3d(x, filter=fil, strides=(1, 1, 1, 1, 1), padding='VALID', name=None)
  File ""/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1440, in conv3d
    dilations=dilations, name=name)
  File ""/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

UnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[node Conv3D (defined at /home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py:7) ]]
	 [[node Conv3D (defined at /home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py:7) ]]


Process finished with exit code 1
```
"
44046,dataset windows don't work in batches,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes (see below)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 64 bit
- TensorFlow installed from (source or binary): pip install tensorflow, pip install tensorflow-nightly
- TensorFlow version (use command below):Currently on nightly - v1.12.1-43485-g0af213f96c 2.4.0-dev20201012 , but also happens on 2.3 release.
- Python version: Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: Tried 10.1 and 11.0
- GPU model and memory:  GeForce RTX 2080 with Max-Q Design computeCapability: 7.5
coreClock: 1.23GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 357.69GiB/s

**Describe the current behavior**

If I create a dataset of windowed data, then batch it up using `tf.data.Dataset.window(5,1,1,True).batch(10,True)` the output of iterating the resulting dataset doesn't evaluate as a tensor and I can't run `model.fit()` on it. It throws an exception:

`TypeError: Inputs to a layer should be tensors. Got: <tensorflow.python.data.ops.dataset_ops._NestedVariant object at 0x000001BC82925250>
`

**Describe the expected behavior**

This not working basically breaks the tf.data loading mechanism for windowed data for me. I want to be able to create a preprocessing pipeline that does the following:
1) load some data
2) apply a sliding window to the data
3) batch the data
4) prefetch the data
5) run model.fit

**Standalone code to reproduce the issue**

```
import tensorflow as tf

# make a source dataset
source_data=tf.data.Dataset.range(100)
# take two windows of different size onto the source dataset as
# our inputs and outputs
# and batch them up into 20 data points
inputs=source_data.window(10,1,1,True).batch(20,True)
targets=source_data.window(5,1,1,True).batch(20,True)
# zip this into a single training data set
training_set=tf.data.Dataset.zip((inputs,targets))
# a very simple network that takes input of [batch_size,10] and outputs [batch_size,5]
inLayer=tf.keras.layers.Input(shape=(10,),batch_size=20)
outLayer=tf.keras.layers.Dense(5)(inLayer)
model=tf.keras.Model(inputs=inLayer,outputs=outLayer)
model.compile(optimizer='adam',loss='mse')
# this fit call fails
model.fit(training_set)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
Traceback (most recent call last):
  File ""d:/machinelearning/predictiveaudio/hellogoodbye/testtf.py"", line 20, in <module>
    model.fit(training_set)
  File ""C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1081, in fit
    tmp_logs = self.train_function(iterator)
  File ""C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\eager\def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\eager\def_function.py"", line 871, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\eager\def_function.py"", line 725, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\eager\function.py"", line 2976, in _get_concrete_function_internal_garbage_collected  
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\eager\function.py"", line 3371, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\eager\function.py"", line 3206, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 990, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\eager\def_function.py"", line 634, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 977, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:

    C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\keras\engine\training.py:790 train_function  *
        return step_function(self, iterator)
    C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\keras\engine\training.py:780 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:1268 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:2734 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:3355 _call_for_each_replica
        return fn(*args, **kwargs)
    C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\keras\engine\training.py:773 run_step  **
        outputs = model.train_step(data)
    C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\keras\engine\training.py:739 train_step
        y_pred = self(x, training=True)
    C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\keras\engine\base_layer.py:989 __call__
        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)
    C:\Users\micro\miniconda3\envs\tf23\lib\site-packages\tensorflow\python\keras\engine\input_spec.py:201 assert_input_compatibility
        raise TypeError('Inputs to a layer should be tensors. Got: %s' % (x,))

    TypeError: Inputs to a layer should be tensors. Got: <tensorflow.python.data.ops.dataset_ops._NestedVariant object at 0x000001BC82925250>
```
"
44045,tflite_convert fails from keras model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.15
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.3


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CAST, CONCATENATION, CONV_2D, ELU, FILL, FULLY_CONNECTED, GATHER, LOGISTIC, MAX_POOL_2D, PACK, RESHAPE, REVERSE_V2, SHAPE, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: StatelessWhile, TensorListFromTensor, TensorListReserve, TensorListStack.
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Standard text classification with Keras sequential model using embeddings, stacked CONV1D and stacked Bidirectional GRUs.
Switching to experimental converter does not help, --allow_custom_ops neither.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
