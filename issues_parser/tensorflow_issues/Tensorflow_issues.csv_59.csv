Issue Number,Issue Title,Issue Body
14021,tf.train.start_queue_runners cannot run under GPU？,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

python 3.5
cuda 8.0 / cudnn 5.1
tensorflow 1.2.1
gpu memory Usage > 95%
gpu Load < 30%
cpu load > 85%

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

When using tf.train.start_queue_runners in trainning,  the trainning process automatically 
goes into cpu mode, when use tf.device('/gpu:0') an error accur thats could not find the data in queue;
so, how should i use queue_runners+tfrecords in GPU?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Main Trainning Code:

with tf.Session() as sess:
    #with tf.device('/gpu:0')
    sess.run(init)
    saver = tf.train.Saver()

    coord = tf.train.Coordinator() 
    threads = tf.train.start_queue_runners(sess=sess, coord = coord)
    
    global_step = 1
   
    if isTrain: 
        tf.train.write_graph(sess.graph_def, './MaxoutModel/', ""model.pb"", as_text=True)
        ckpt = tf.train.get_checkpoint_state('./MaxoutModel/')
        
        if ckpt and ckpt.model_checkpoint_path:
            saver.restore(sess, ckpt.model_checkpoint_path)
            print('Load Model OK!')
        else:
            print('Load Model Error!!')
            pass
    
        for i in range(100):
            
            # learning rate decay
            max_learning_rate = 0.02
            min_learning_rate = 0.0001
            decay_speed = 16000
            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)

            tra_images, tra_labels = sess.run([img_batch, label_batch])
            sess.run(train_step, {X: batch_X, Y_: batch_Y, lr: learning_rate,  keep_prob: 0.75})

            if i%20 == 0:
                cost = sess.run(cross_entropy, 
                        {X: tra_images, Y_: tra_labels , keep_prob: 1.0})
            if i%20 == 0 and i !=0 :    
                saver.save(sess,save_path='./BNModel/model.ckpt',global_step=global_step+1)
        saver.save(sess,save_path='./BNModel/model.ckpt',global_step=global_step+1)

"
14018,Tutorial request for hybrid model (word+character) ,"The implementation done in the paper:
http://aclweb.org/anthology/P/P16/P16-1100.pdf

is a hybrid seq2seq model with advancements where the encoder is fed with inputs based on following two cases:

1.Normal vector representation of a word (Embedding vector) - when the word input is present in the vocabulary

2.Output of another LSTM network - when the word is **out of vocabulary** and a separate character based LSTM is used to **generate an embedding on the fly**

Consider the following example sentence:
""The brown fox jumped over the lazy dog""

Assume these are the words present in the vocabulary: _The, brown, jumped, over, dog_ - These words are fed to the seq2seq encoder as such

out of vocabulary(OOV) words are: _fox, lazy_ - These words are passed to a character LSTM and the output of the same is passed to the seq2seq model along with the above words

These both word level and character level encoder needs to be trained end to end simultaneously. 

Since the implementation is a bit different from the normal seq2seq can a tutorial or example of such case be added to the examples section?"
14017,image_ops_test failing due to math_ops.sin() behaving differently,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.2.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.4.5 
- **CUDA/cuDNN version**: No GPU
- **GPU model and memory**: No GPU
- **Exact command to reproduce**: bazel test -c opt //tensorflow/contrib/image:image_ops_test

### Describe the problem
The above test fails with the output array mismatch in [test_rotate_even](https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/contrib/image/python/kernel_tests/image_ops_test.py#L48).

The exact cause of failure seems to be at [math_ops.sin(angles)](https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/contrib/image/python/ops/image_ops.py#L115), which only fails for `angle=(np.pi / 4.0) = 0.78539819` (45 deg) and works fine for 0 and 90 deg.

The output of `math_ops.sin(0.78539819).eval()` differs on Intel and s390x as below:
1. Intel(test passes) : 0.70710**6769**
2.  s390x(test fails) : 0.70710**6829**

I verified `np.sin(0.78539819)` gives same output `0.7071067999` on both Intel and s390x.

Why is the difference seen in math_ops.sin()? Any pointers would be helpful.

### Source logs
----------------------------------------------------------------------
FAIL: test_rotate_even (__main__.ImageOpsTest)
----------------------------------------------------------------------
```
Traceback (most recent call last):
  File ""/home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/image/image_ops_test.runfiles/org_tensorflow/tensorflow/contrib/image/python/kernel_tests/image_ops_test.py"", line 75, in test_rotate_even
    [1, 7, 13, 19, 25, 31], [0, 6, 12, 18, 24, 30]]])
  File ""/home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/image/image_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 699, in assertAllEqual
    np.testing.assert_array_equal(a, b)
  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 807, in assert_array_equal
    verbose=verbose, header='Arrays are not equal')
  File ""/usr/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 733, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Arrays are not equal

(mismatch 1.85185185185%)
 x: array([[[ 0,  1,  2,  3,  4,  5],
        [ 6,  7,  8,  9, 10, 11],
        [12, 13, 14, 15, 16, 17],...
 y: array([[[ 0,  1,  2,  3,  4,  5],
        [ 6,  7,  8,  9, 10, 11],
        [12, 13, 14, 15, 16, 17],...

not equal where =  (array([1, 1]), array([3, 4]), array([3, 4]))
not equal lhs =  [20 32]
not equal rhs =  [21 33]

```




"
14014,Proposal: Compiling TF 1.4.0 GPU w/ CMAKE on Linux x64,"Currently, I have been playing with the cmake scripts of TensorFlow-GPU to make it CMAKE-buildable in a bare-metal x64 Linux. (isolated environment, without the internet, w/ OBS)
(I'm trying CMAKE because I don't want to port Java)

It's almost done (need code clean) and I'll probably send Pull-Request next week. However, there are a few things I want to check before I write additional features besides simply make it able to build with CMAKE for TF-GPU in Linux.

1. Tensorflow is statically linking to pre-installed devel packages. I think in some cases, it might be better to do dynamic linking (use .so) to other libraries (to save some memory and storage size). May I simply add an option like ""tensorflow_USE_SHARED_LIBS_CUDA""? (e.g., cuda, nccl, ...)

2. Tensorflow is downloading a lot of external packages and use them ""statically"", which is pretty awful for some people. I'd like to make it use shared libraries as well (maybe along with some version restrictions). May I simply add an option like ""tensorflow_USE_SHARD_LIBS_JSONCPP"", which disables downloading ""JSONCPP"" as well?

Some additional benefit with these might be reduced memory requirement for building tensorflow; memory consumption of tensorflow-build gets dangerous at the last step with ```ld```. Probably, for now, I may simply need to be satisfied with letting ```make``` reduce ```-j#``` only for ```ld``` steps.


CC: @leemgs
STATUS: failing at the last step. (without ```-Dtensorflow_BUILD_SHARED_LIB```, it works anyway)
```
...
[ 2457s] /usr/lib64/gcc/x86_64-tizen-linux-gnu/6.2.1/../../../../x86_64-tizen-linux-gnu/bin/ld: libtf_core_gpu_kernels.a(tf_core_gpu_kernels_generated_beam_search_ops_gpu.cu.cc.o): relocation R_X86_64_32 against `.data' can not be used when making a shared object; recompile with -fPIC
[ 2457s] /usr/lib64/gcc/x86_64-tizen-linux-gnu/6.2.1/../../../../x86_64-tizen-linux-gnu/bin/ld: libtf_core_gpu_kernels.a(tf_core_gpu_kernels_generated_resampler_ops_gpu.cu.cc.o): relocation R_X86_64_32 against `.data' can not be used when making a shared object; recompile with -fPIC
[ 2457s] /usr/lib64/gcc/x86_64-tizen-linux-gnu/6.2.1/../../../../x86_64-tizen-linux-gnu/bin/ld: final link failed: Nonrepresentable section on output
[ 2457s] collect2: error: ld returned 1 exit status
[ 2457s] CMakeFiles/tensorflow.dir/build.make:2235: recipe for target 'libtensorflow.so' failed
[ 2457s] make[2]: *** [libtensorflow.so] Error 1
[ 2457s] CMakeFiles/Makefile2:82: recipe for target 'CMakeFiles/tensorflow.dir/all' failed
[ 2457s] make[1]: *** [CMakeFiles/tensorflow.dir/all] Error 2
[ 2457s] make[1]: *** Waiting for unfinished jobs....
[ 2458s] [100%] Linking CXX shared library libpywrap_tensorflow_internal.so
[ 2474s] [100%] Built target grpc_tensorflow_server
[ 2474s] [100%] Built target transform_graph
```

-- STATUS UPDATE: build successful w/ ```-Dtensorflow_BUILD_SHARED_LIB=ON``` as well."
14013,#tensorflow##PYNQ# Could I install tensorflow in my PYNQ-Z1 ? ,"I wanna build some Classifier with it. I'm used to build Classifiers with TensorFlow. 
So I want to know whether it is technically possible.THX!!
[http://www.pynq.io/](url)"
14012,tfdbg ps -b command does not work on Windows,"Issue:

The tfdbg ps -b command does not annotate the source file source file beginning at the given line i.e. the output of ps -b 10 source.py is the same as ps source.py.

Steps to reproduce:

1. Open the command prompt
2. Run python -m tensorflow.python.debug.examples.debug_mnist --debug
3. Press r
4. Type ps -b 10 path/to/debug_mnist

System information:

* Tensorflow 1.3.0 (installed using Anaconda)
* OS: Windows-10-10.0.15063-SP0
* Python: 3.6.1
"
14008,tf.python.keras can't import np_utils,"Typically i'm able to replace any ""from keras.X import Y"" with ""from tensorflow.python.keras.X import Y""

This works with most things, but it does not translates to keras.utils. I can run:
from keras.utils import np_utils

But the following fails:
from tensorflow.python.keras.utils import np_utils

The workaround is just to use: 
from tensorflow.python.keras._impl.keras.utils import np_utils

Unsure if this is intended or not. I guess I'd consider this either a bug fix or a feature request. "
14007,BatchNorm not working in a _FuncGraph,"I am working on macOS 10.12.6 with TensorFlow 1.3.0, Python version: 3.6.2. CPU only.

I found that if `tf.contrib.layers.batch_norm` is called inside a `Defun`, a `TypeError` will be thrown. To clarify, the function here is a `map_func` that will be used in `Dataset.map` invoke, instead of a normal python function.

To regenerate the scenario, try this piece of code:

```
Dataset.range(27).batch(27) \
  .map(lambda x: tf.cast(tf.reshape(x, (3, 3, 3)), tf.float32) / 32.0) \
  .map(lambda img: tf.contrib.layers.batch_norm(img)) \
  .make_one_shot_iterator().get_next().eval()
```

You will get an error like this:

```
TypeError: In op 'BatchNorm/AssignMovingAvg', input types ([tf.float32, tf.float32]) are not compatible with expected types ([tf.float32_ref, tf.float32])
```

I did some investigation. One thing that I found is that inside the `batch_norm`, we will assign a new value to the `mean` variable by calling `assign_sub`, which accept a variable with **ref** type and a value with a **basic type**. But the variable has been created some where in the `_FuncGraph` with a **basic type**, instead of a **ref** type, which make the compatibility check failed.

I will keep digging into this issue, but I think this seems a bug."
14004,[Windows] Speech commands tutorial does not work,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, I'm running `python tensorflow/examples/speech_commands/train.py`
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 x64
- **TensorFlow installed from (source or binary)**: `pip install tf-nightly`  (tf_nightly-1.5.0.dev20171026-cp35-cp35m-win_amd64.whl)
- **TensorFlow version (use command below)**: b'unknown' 1.5.0-dev20171026
- **Python version**: Python 3.5.4
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 8.0, cuDNN 6.1 x64
- **GPU model and memory**: 2x Nvidia GTX 670 2GB
- **Exact command to reproduce**: `python tensorflow/examples/speech_commands/train.py`

### Describe the problem
It appears that the currently nightlies (or 1.4.0rc1) do not contain the gen_audio_ops module. However the documentation for r1.4 or master (https://www.tensorflow.org/versions/r1.4/tutorials/audio_recognition or https://www.tensorflow.org/versions/master/tutorials/audio_recognition) appears to indicate that the speech_commands demo should work.

Related: #13031

### Source code / logs
```
Traceback (most recent call last):
  File ""tensorflow/examples/speech_commands/train.py"", line 81, in <module>
    import input_data
  File ""E:\Tom\Documents\GIT\tensorflow\tensorflow\examples\speech_commands\input_data.py"", line 35, in <module>
    from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio
  File ""C:\Users\Tom\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\contrib\framework\python\ops\audio_ops.py"", line 31, in <module>
    from tensorflow.python.ops.gen_audio_ops import *
ImportError: No module named 'tensorflow.python.ops.gen_audio_ops'
```
"
14003,..,
14002,Bazel build for CUDA failed on Ubuntu w/ lastest tip + CUDNN 6.0 + CUDA 8.0,"Hi,

I am trying to build latest TF with CUDNN 6.0 + CUDA 8.0 on Ubuntu 14 but it failed with the following error message. I found some similar issue (https://github.com/tensorflow/tensorflow/issues/469) reported in the past, not sure if the latest tip has fixed it?

Any suggestion would be appreciated.

Thanks

**Build Command**
- build command for CUDA that failed
`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package `

- build for CPU works well
`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`

**System Info**
- bazel version : Build label: 0.5.4
- CUDA: 8.0
- CUDNN 6.0
- TF origin/master latest sync as 10/26/17 (cb7cb40 Merge pull request #13972 from taehoonlee/fix_typos)

**Error message:**
`ERROR: $PROJECT_ROOT/tensorflow/tensorflow/stream_executor/BUILD:52:1: undeclared inclusion(s) in rule '//tensorflow/stream_executor:cuda_platform':
this rule is missing dependency declarations for the following files included by 'tensorflow/stream_executor/cuda/cuda_blas.cc':
  '/usr/local/cuda/include/cublas_api.h'
  '/usr/local/cuda/include/driver_types.h'
  '/usr/local/cuda/include/host_defines.h'
  '/usr/local/cuda/include/cuComplex.h'
  '/usr/local/cuda/include/vector_types.h'
  '/usr/local/cuda/include/builtin_types.h'
  '/usr/local/cuda/include/device_types.h'
  '/usr/local/cuda/include/surface_types.h'
  '/usr/local/cuda/include/texture_types.h'
  '/usr/local/cuda/include/cuda_fp16.h'
  '/usr/local/cuda/include/library_types.h'
tensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cudaDataType_t perftools::gputools::cuda::{anonymous}::CUDAComputationType(perftools::gputools::blas::ComputationType)':
tensorflow/stream_executor/cuda/cuda_blas.cc:527:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
`"
14001,layers-based cuDNN RNN functionality not working,"I'm trying to use the newly added layers-style cuDNN RNN functionality. I'm running TF 1.4.0-rc0 on Ubuntu with Pascal GPUs, compiled from source with CUDA 8 and cuDNN 7. When trying to import the relevant library:

`from tensorflow.contrib.cudnn_rnn.python.layers import cudnn_rnn`

I get the following error:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/tensorflow-1.4.0-rc0/local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py"", line 33, in <module>
    resource_loader.get_path_to_datafile(""_cudnn_rnn_ops.so""))
  File ""/usr/local/tensorflow-1.4.0-rc0/local/lib/python2.7/site-packages/tensorflow/contrib/util/loader.py"", line 55, in load_op_library
    ret = load_library.load_op_library(path)
  File ""/usr/local/tensorflow-1.4.0-rc0/local/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py"", line 56, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)
  File ""/usr/local/tensorflow-1.4.0-rc0/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/tensorflow-1.4.0-rc0/local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/_cudnn_rnn_ops.so: cannot open shared object file: No such file or directory
```

Note that the non-layer based cuDNN RNN functionality works fine. I.e. I can run this:

`from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops`

and run cuDNN-based RNNs with no problem otherwise."
14000,Model trained on GPU does not restore properly when ran on CPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
```
== cat /etc/issue ===============================================
Linux  3.16.0-4-amd64 #1 SMP Debian 3.16.43-2+deb8u5 (2017-09-19) x86_64 GNU/Linux
VERSION_ID=""8""
VERSION=""8 (jessie)""

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 4.8.5
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux  3.16.0-4-amd64 #1 SMP Debian 3.16.43-2+deb8u5 (2017-09-19) x86_64 GNU/Linux

== check pips ===================================================
numpy (1.12.1)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-gpu (1.4.0rc0)
tensorflow-tensorboard (0.1.5)

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.4.0-rc0
tf.GIT_VERSION = v1.3.0-rc1-3112-g65b6a75
tf.COMPILER_VERSION = v1.3.0-rc1-3112-g65b6a75
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /cuda/cuda-8.0/lib64:/cuda/cudnn-8.0-linux-x64-v6.0/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Thu Oct 26 16:47:55 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GT 610      Off  | 0000:01:00.0     N/A |                  N/A |
| N/A   42C    P0    N/A /  N/A |      0MiB /   963MiB |     N/A      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0                  Not Supported                                         |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
```
- **TensorFlow installed from (source or binary)**:
Installed binary with pip.
- **TensorFlow version (use command below)**:
('v1.3.0-rc1-3112-g65b6a75', '1.4.0-rc0')
The same bug happens on Tensorflow 1.3.0 too.
- **Python version**: 
2.7
- **CUDA/cuDNN version**:
CUDA 8.0
cuDNN 6.0
- **GPU model and memory**:
I am testing on two different computers. On one PC I have this GPU:
name: GeForce GTX TITAN Black major: 3 minor: 5 memoryClockRate(GHz): 0.98
pciBusID: 0000:02:00.0
totalMemory: 5.94GiB freeMemory: 5.87GiB
On my PC I do not have a GPU.

- **Exact command to reproduce**:

This (latest now) checkout of tensorflow models:
https://github.com/tensorflow/models/tree/edcd29f2dbb4b3eaed387fe17cb5270f867aec42/official/mnist

```sh
$ python convert_to_records.py --directory ~/tmp/mnist_data
Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Extracting /home/amir/tmp/mnist_data/train-images-idx3-ubyte.gz
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Extracting /home/amir/tmp/mnist_data/train-labels-idx1-ubyte.gz
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Extracting /home/amir/tmp/mnist_data/t10k-images-idx3-ubyte.gz
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting /home/amir/tmp/mnist_data/t10k-labels-idx1-ubyte.gz
Writing /home/amir/tmp/mnist_data/train.tfrecords
Writing /home/amir/tmp/mnist_data/validation.tfrecords
Writing /home/amir/tmp/mnist_data/test.tfrecords
$ python mnist.py --data_dir ~/tmp/mnist_data --model_dir ~/tmp/mnist_model --steps 2000
INFO:tensorflow:Using default config.
INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_clu
ster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff6e918d910>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000
, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': 1, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_di
r': '/home/amir/tmp/mnist_model', '_save_summary_steps': 100}
WARNING:tensorflow:From mnist.py:84: __init__ (from tensorflow.contrib.data.python.ops.readers) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.TFRecordDataset`.
WARNING:tensorflow:From mnist.py:92: calling map (from tensorflow.contrib.data.python.ops.dataset_ops) with num_threads is deprecated and will be removed in a fut
ure version.
Instructions for updating:
Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.
WARNING:tensorflow:From mnist.py:92: calling map (from tensorflow.contrib.data.python.ops.dataset_ops) with output_buffer_size is deprecated and will be removed i
n a future version.
Instructions for updating:
Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.
INFO:tensorflow:Create CheckpointSaverHook.
2017-10-26 15:52:17.067286: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to us
e: SSE4.1 SSE4.2 AVX
2017-10-26 15:52:18.075205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there mu
st be at least one NUMA node, so returning NUMA node zero
2017-10-26 15:52:18.075593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:
name: GeForce GTX TITAN Black major: 3 minor: 5 memoryClockRate(GHz): 0.98
pciBusID: 0000:02:00.0
totalMemory: 5.94GiB freeMemory: 5.87GiB
2017-10-26 15:52:18.075610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX T
ITAN Black, pci bus id: 0000:02:00.0, compute capability: 3.5)
INFO:tensorflow:Saving checkpoints for 1 into /home/amir/tmp/mnist_model/model.ckpt.
INFO:tensorflow:train_accuracy = 0.13
INFO:tensorflow:loss = 2.28967, step = 1
INFO:tensorflow:global_step/sec: 86.9528
INFO:tensorflow:train_accuracy = 0.485 (1.150 sec)
INFO:tensorflow:loss = 0.519769, step = 101 (1.150 sec)
INFO:tensorflow:global_step/sec: 91.0098
INFO:tensorflow:train_accuracy = 0.61 (1.099 sec)
INFO:tensorflow:loss = 0.464828, step = 201 (1.099 sec)
INFO:tensorflow:global_step/sec: 95.5818
INFO:tensorflow:train_accuracy = 0.6825 (1.046 sec)
INFO:tensorflow:loss = 0.299279, step = 301 (1.046 sec)
...
Evaluation results:
    {'loss': 0.027056012, 'global_step': 20000, 'accuracy': 0.99309999}
$ # comment out the .train line (lines 232-235) in mnist.py
$ # somehow disable GPU
$ python mnist.py --data_dir ~/tmp/mnist_data --model_dir ~/tmp/mnist_model --steps 2000 --data_format channels_last
INFO:tensorflow:Using default config.
INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_clu
ster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4a9afcfd10>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000
, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': 1, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_di
r': '/home/amir/tmp/mnist_model', '_save_summary_steps': 100}
WARNING:tensorflow:From mnist.py:84: __init__ (from tensorflow.contrib.data.python.ops.readers) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.TFRecordDataset`.
WARNING:tensorflow:From mnist.py:92: calling map (from tensorflow.contrib.data.python.ops.dataset_ops) with num_threads is deprecated and will be removed in a fut
ure version.
Instructions for updating:
Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.
WARNING:tensorflow:From mnist.py:92: calling map (from tensorflow.contrib.data.python.ops.dataset_ops) with output_buffer_size is deprecated and will be removed i
n a future version.
Instructions for updating:
Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.
INFO:tensorflow:Starting evaluation at 2017-10-26-14:33:53
2017-10-26 16:33:53.385155: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to us
e: SSE4.1 SSE4.2 AVX AVX2 FMA
2017-10-26 16:33:53.907636: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE
2017-10-26 16:33:53.907712: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: italix14
2017-10-26 16:33:53.907722: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: italix14
2017-10-26 16:33:53.907800: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Invalid argument: expected %d.%d or %d.%d.%d f
orm for driver version; got ""1""
2017-10-26 16:33:53.907837: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: """"""NVRM version: NVIDIA UNIX x86_64 Kernel Mo
dule  375.26  Thu Dec  8 18:36:43 PST 2016
GCC version:  gcc version 4.8.4 (Debian 4.8.4-1)
""""""
2017-10-26 16:33:53.907871: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 375.26.0
INFO:tensorflow:Restoring parameters from /home/amir/tmp/mnist_model/model.ckpt-20000
INFO:tensorflow:Finished evaluation at 2017-10-26-14:34:20
INFO:tensorflow:Saving dict for global step 20000: accuracy = 0.0962, global_step = 20000, loss = 6.32954

Evaluation results:
    {'loss': 6.3295369, 'global_step': 20000, 'accuracy': 0.096199997}
```


### Describe the problem
I have trained the official mnist model on GPU (`channels_first`,  `NCHW`) but when I test it on CPU (`channels_last`,  `NHWC`) I get different results. On GPU, I get an accuracy of `0.99309999` but the same model (when tested on CPU) gives `0.096199997`. I believe when the model is restored from the checkpoint the kernel weights are not restored properly to accommodate for the new channel format.

The problem I have shown here is using the official mnist model but in reality I have a model that I have already trained using the `channels_first` format on GPU which took several days to train. But, now I cannot evaluate this model on CPU.

"
13999,Memory Leak While Reading from TFRecord,"### Problem
As I mentioned in [my previous issue](https://github.com/tensorflow/tensorflow/issues/6599), I have memory leak in my code. Finally I can write a sample code that can reproduce the problem.

### Source code
```
import os
import random

import psutil
import tensorflow as tf


def get_tf_example(inputs, outputs):
    example = tf.train.Example(features=tf.train.Features(feature={
        'inputs': tf.train.Feature(int64_list=tf.train.Int64List(value=inputs)),
        'outputs': tf.train.Feature(int64_list=tf.train.Int64List(value=outputs))
        }))
    return example.SerializeToString()


def memory():
    pid = os.getpid()
    py = psutil.Process(pid)
    memory_use = py.memory_info()[0]/2.**30
    print('memory use:', memory_use)


def main():
    tfrecord_file = tf.python_io.TFRecordWriter('data.tfrecord')
    for i in range(10000):
        random_numbers = [random.randint(0, 100) for _ in range(10)]
        without_an_element = filter(lambda e: e != 100, random_numbers)
        tf_example = get_tf_example(random_numbers, without_an_element)
        tfrecord_file.write(tf_example)
    tfrecord_file.close()

    filename_queue = tf.train.string_input_producer(['data.tfrecord'])
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)
    features = tf.parse_single_example(
        serialized_example,
        features={
            'inputs': tf.FixedLenFeature(10, dtype=tf.int64),
            'outputs': tf.VarLenFeature(dtype=tf.int64)
        })
    images, labels = tf.train.shuffle_batch([features['inputs'], features['outputs']], batch_size=100, capacity=200,
                                            min_after_dequeue=100)
    sess = tf.Session()

    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess, coord=coord)

    for i in range(10000):
        _ = sess.run(images)
        if (i+1) % 100 == 0:
            memory()

    coord.request_stop()
    coord.join(threads)


if __name__ == '__main__':
    main()
```

### logs
```
('memory use:', 0.1137542724609375)
('memory use:', 0.11678314208984375)
('memory use:', 0.11930084228515625)
('memory use:', 0.12792205810546875)
('memory use:', 0.13238525390625)
('memory use:', 0.135406494140625)
('memory use:', 0.13817596435546875)
('memory use:', 0.14484786987304688)
('memory use:', 0.1512603759765625)
('memory use:', 0.15402984619140625)
('memory use:', 0.15654754638671875)
('memory use:', 0.16101455688476562)
('memory use:', 0.17060470581054688)
('memory use:', 0.17287063598632812)
('memory use:', 0.17589187622070312)
('memory use:', 0.18036270141601562)
('memory use:', 0.18872833251953125)
('memory use:', 0.19124221801757812)
('memory use:', 0.19401168823242188)
('memory use:', 0.19848251342773438)
('memory use:', 0.20489883422851562)
('memory use:', 0.20936203002929688)
('memory use:', 0.21213150024414062)
('memory use:', 0.21490097045898438)
('memory use:', 0.22327041625976562)
('memory use:', 0.22988128662109375)
('memory use:', 0.232147216796875)
('memory use:', 0.23491668701171875)
('memory use:', 0.23967361450195312)
('memory use:', 0.2477874755859375)
('memory use:', 0.2508087158203125)
('memory use:', 0.2528228759765625)
('memory use:', 0.2577934265136719)
('memory use:', 0.2661628723144531)
('memory use:', 0.2686767578125)
('memory use:', 0.27144622802734375)
('memory use:', 0.2759132385253906)
('memory use:', 0.2823295593261719)
('memory use:', 0.28704833984375)
('memory use:', 0.2895660400390625)
('memory use:', 0.2940330505371094)
('memory use:', 0.30095672607421875)
('memory use:', 0.30516815185546875)
('memory use:', 0.30768585205078125)
('memory use:', 0.3126564025878906)
('memory use:', 0.31687164306640625)
('memory use:', 0.3235435485839844)
('memory use:', 0.32605743408203125)
('memory use:', 0.3305244445800781)
('memory use:', 0.3332939147949219)
('memory use:', 0.3397102355957031)
('memory use:', 0.34417724609375)
('memory use:', 0.3529624938964844)
('memory use:', 0.3552284240722656)
('memory use:', 0.36199188232421875)
('memory use:', 0.3664588928222656)
('memory use:', 0.37117767333984375)
('memory use:', 0.373443603515625)
('memory use:', 0.3801116943359375)
('memory use:', 0.38458251953125)
('memory use:', 0.3892974853515625)
('memory use:', 0.3923187255859375)
('memory use:', 0.3982391357421875)
('memory use:', 0.40125274658203125)
('memory use:', 0.4074211120605469)
('memory use:', 0.410186767578125)
('memory use:', 0.4146575927734375)
('memory use:', 0.41912078857421875)
('memory use:', 0.4257926940917969)
('memory use:', 0.4280548095703125)
('memory use:', 0.4310760498046875)
('memory use:', 0.4372406005859375)
('memory use:', 0.44196319580078125)
('memory use:', 0.446929931640625)
('memory use:', 0.44919586181640625)
('memory use:', 0.45586395263671875)
('memory use:', 0.4581298828125)
('memory use:', 0.46504974365234375)
('memory use:', 0.467315673828125)
('memory use:', 0.4739875793457031)
('memory use:', 0.47650146484375)
('memory use:', 0.48291778564453125)
('memory use:', 0.485687255859375)
('memory use:', 0.4921112060546875)
('memory use:', 0.49462127685546875)
('memory use:', 0.5015411376953125)
('memory use:', 0.5038070678710938)
('memory use:', 0.5085296630859375)
('memory use:', 0.5129928588867188)
('memory use:', 0.5174598693847656)
('memory use:', 0.522430419921875)
('memory use:', 0.5246963500976562)
('memory use:', 0.5313644409179688)
('memory use:', 0.5355796813964844)
('memory use:', 0.5404815673828125)
('memory use:', 0.5429954528808594)
('memory use:', 0.5494194030761719)
('memory use:', 0.5541419982910156)
('memory use:', 0.5586051940917969)
('memory use:', 0.5611228942871094)
```

I can't find the reason. Many thanks for your consideration.

### System information
- **OS Platform and Distribution**: Linux Ubuntu 14.04
- **TensorFlow installed from**: binary
- **TensorFlow version**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **Python version**: 2.7.6
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
"
13996,Proper way to handle csv input for cpu training?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
pip
- **TensorFlow version (use command below)**: 
('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **Python version**: 
Python 2.7.12 from Anaconda

### Describe the problem
I am training some classification model with an 32-cpu Ubuntu machine and one of the problem is to feed data fast enough to the training process. 

I am trying to read data from some csv file but the default tf.csv or tf.data module seems to be slow.
A speed test for reading 1000000 row * 17 column csv file shows a speed like : 
* tf.decode_csv with queue and theads  :   ~192 seconds
* tf.data :  ~164 seconds
* hand write cpp reading op :  ~25 seconds
* pure python code with help from pandas : ~23 seconds

It is fast enough to use pandas for one single file, but it might face the GIL problem if try to speed up with more threads. 

Codes can be found below. I am not sure if I use it the right way, is there any official benchmarks or guidelines for this? 

### Source code / logs
https://github.com/littleDing/mini_csv_reader
The speed test is run through speed_test.py "
13995,How to output each class accuracy ?,"Using slim I can get the evaluation output Top-1 and Top-5. But how to calculate the accuracy for each class and output them?
Thanks!"
13994,How to output each class accuracy through modifing,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13993,"Compiling from source, ./configure, issue finding cudnn","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.3
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.7.0
- **CUDA/cuDNN version**: 7
- **GPU model and memory**: Nvidia 1070 8GB
- **Exact command to reproduce**: On ./configure

When following the building from source installation instructions, the first step involves running a ./configure.

During this script, you are prompted for the versions and locations of features you want support for in the build.

When getting down to the CUDA SKD version, you will get something like:
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 9.0

I'm using 9.0...
Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 7.0

I'm using 7.0...
However, if you type 7.0, you get the following:
Invalid path to cuDNN  toolkit. Neither of the following two files can be found:
/usr/local/cuda-9.0/lib64/libcudnn.so.7.0
/usr/local/cuda-9.0/libcudnn.so.7.0
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0

But if you answer the following as:
Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 7
It works.

The actual name is libcudnn.so.7, can can't find libcudnn.so.7.0. Otherwise it won't find it, and it looks like you have installed incorrectly.

Can we get a fix for this?
"
13992,tensorflow/contrib/slim/python/slim/nets/resnet_v1_test.py test is failing with array mismatch error,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
      Ubuntu 16.04 (ppc64le)
- **TensorFlow installed from (source or binary)**:
      Installed from source
- **TensorFlow version (use command below)**:
      TF 1.3.1
- **Python version**: 
     Python 2.7.5
- **Bazel version (if compiling from source)**:
     bazel-0.5.4
- **CUDA/cuDNN version**:
     NA
- **GPU model and memory**:
      NA
- **Exact command to reproduce**:
 `bazel test --config=opt //tensorflow/contrib/slim/python/slim/nets:resnet_v1_test`

### Describe the problem
This test is failing due to array mismatch error – a single value in an array of 100+ elements differing i.e.  `0.69775391 VS expected 0.69799805` (minor mismatch), I feel this is not ""critical"".

Hence, I tried running this test by changing minor tolerance `(atol=1e-4 to atol=2e-4)`, and test is passing -
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/resnet_v1_test.py#L389
Original : `output.eval(), expected.eval(), atol=1e-4, rtol=1e-4) `
Update to : ` output.eval(), expected.eval(), atol=2e-4, rtol=1e-4)`

Is it OK to raise a PR with this changes ?. Please provide your comments on this.Thanks!

### Source code / logs
 `bazel test --config=opt //tensorflow/contrib/slim/python/slim/nets:resnet_v1_test`

```
-----------------------------------------------------------------------------
..F..............
======================================================================
FAIL: testAtrousFullyConvolutionalValues (__main__.ResnetCompleteNetworkTest)
Verify dense feature extraction with atrous convolution.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/3429064d1bfc0bd7253170e3e9255ca6/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/slim/python/slim/nets/resnet_v1_test.runfiles/org_tensorflow/tensorflow/contrib/slim/python/slim/nets/resnet_v1_test.py"", line 389, in testAtrousFullyConvolutionalValues
    output.eval(), expected.eval(), atol=1e-4, rtol=1e-4)
  File ""/root/.cache/bazel/_bazel_root/3429064d1bfc0bd7253170e3e9255ca6/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/slim/python/slim/nets/resnet_v1_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 687, in assertAllClose
    self._assertArrayLikeAllClose(a, b, rtol=rtol, atol=atol)
  File ""/root/.cache/bazel/_bazel_root/3429064d1bfc0bd7253170e3e9255ca6/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/slim/python/slim/nets/resnet_v1_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 657, in _assertArrayLikeAllClose
    np.testing.assert_allclose(b, a, rtol=rtol, atol=atol, err_msg=msg)
  File ""/usr/lib64/python2.7/site-packages/numpy/testing/utils.py"", line 1411, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File ""/usr/lib64/python2.7/site-packages/numpy/testing/utils.py"", line 796, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Not equal to tolerance rtol=0.0001, atol=0.0001
None
(mismatch 0.173611111111%)
 x: array([[[[  0.000000e+00,   3.163050e+02,   1.810576e+02,   0.000000e+00,
            0.000000e+00,   7.204424e+01,   2.393519e+02,   2.917337e+02,
            0.000000e+00,   0.000000e+00,   4.375999e+02,   6.552710e+01,...
 y: array([[[[  0.000000e+00,   3.163050e+02,   1.810576e+02,   0.000000e+00,
            0.000000e+00,   7.204424e+01,   2.393519e+02,   2.917337e+02,
            0.000000e+00,   0.000000e+00,   4.375999e+02,   6.552710e+01,...

----------------------------------------------------------------------
Ran 17 tests in 21.369s

FAILED (failures=1)
not close where =  (array([1]), array([2]), array([1]), array([7]))
not close lhs =  [ 0.69799805]
not close rhs =  [ 0.69775391]
not close dif =  [ 0.00024414]
not close tol =  [ 0.00016978]
dtype = float32, shape = (2, 3, 3, 32)
```
"
13989,Fused batch norm can be folded with atrous conv2d ,"### Describe the problem
As we can fold batch norm with convolution, we should also fold batch norm with atrous convolution, which has not been implemented.
"
13986,tensorflow building graph very slow — for more than two hours when looping,"### Describe the problem
Tensorflow is really slow when using loops. A very simple code can even last for hours and this is strange.
Can anyone help me with issue?
### Source code / logs
```
import numpy as np
import tensorflow as tf
import time

# define a guassian function so that it picks a value of 1 when the required value is met and otherwise a small neglegible value
def gaussian(x_box, x_tensor):
    x_box=tf.convert_to_tensor(x_box,dtype=tf.float32)
    return tf.exp(tf.divide(tf.square(x_box-x_tensor),-0.02))

a=time.time()
# the input tensor with shape=[1,1000]
length = 1000
input_= tf.placeholder(dtype= tf.float32, shape= [1, length])


# the box length is 100
box=np.zeros(100, dtype='O')
#zero_tensor=tf.convert_to_tensor(0,dtype=tf.float32)
#box[:]=zero_tensor


# in order to put the element in the correct box, I use a gaussian function, so that when the element
# in input_ is  near the box , it contributes a large number
for i in range(length):
    for j in range(len(box)):
        box[j] += gaussian(j, input_[0][i])
b=time.time()
print(b-a)

##########
# really slow when length=1000 and box len 100, but if I set length =10 and box len also 10
# it get the correct ansewr and faster as expected
# for example
input_value = [[0,0,0,0,0,1,1,1,1,1]]
with tf.Session() as sess:
    for i in range(len(box)):
        box[i] = int(sess.run(box[i], {input_: input_value}))
print(box)

```"
13984,Issues importing external tf_library from project ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14
- **TensorFlow installed from (source or binary)**: Source (bazel)
- **TensorFlow version (use command below)**:  1.2
- **Python version**: 
- **Bazel version (if compiling from source)**: 
bazel version
Build label: 0.7.0
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed Oct 18 14:27:19 2017 (1508336839)
Build timestamp: 1508336839
Build timestamp as int: 1508336839

- **CUDA/cuDNN version**:
- **GPU model and memory**:
2x1080ti
- **Exact command to reproduce**:


I import tensorflow with Bazel into my workspace with:
```
#WORKSPACE
patched_http_archive(
    name = ""org_tensorflow"",
    urls = [
        ""https://github.com/tensorflow/tensorflow/archive/v1.2.0.tar.gz"",
    ],
    sha256 = ""03dbf7548d1fc1c11ed58da5fa68616f795c819f868f43478cbcaa26abed374f"",
    strip_prefix = ""tensorflow-1.2.0"",
    patch_file = ""//tools/third_party:tensorflow_bazel054.patch"",
)

load('@org_tensorflow//tensorflow:workspace.bzl', 'tf_workspace')

tf_workspace(path_prefix = """", tf_repo_name = ""org_tensorflow"")
```

I am able to correctly reference the tf_library from my BUILD file but it fails to resolve the //tensorflow/... targets because its relative to MY workspace which expects tensorflow targets to be under @org_tensorflow//tensorflow/...
```

load(""@org_tensorflow//tensorflow/compiler/aot:tfcompile.bzl"", ""tf_library"")

tf_library(...)
```

Get error:
` no such package 'tensorflow/compiler/tf2xla/kernels': BUILD file not found on package path and referenced by '//my/local/bazel/target:test_graph_tfmatmul'
`
See the output of my bazel query:
```
cc_library(
  name = ""test_graph_tfmatmul"",
  generator_name = ""test_graph_tfmatmul"",
  generator_function = ""tf_library"",
  generator_location = ""ros/src/visual_orientation/bench/BUILD:13"",
  deps = [""//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int32"", ""//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int64"", ""//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_1d"", ""//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_2d"", ""//tensorflow/compiler/aot:runtime"", ""//tensorflow/compiler/tf2xla:xla_local_runtime_context"", ""//tensorflow/compiler/xla/service/cpu:runtime_conv2d"", ""//tensorflow/compiler/xla/service/cpu:runtime_matmul"", ""//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_conv2d"", ""//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_matmul"", ""//tensorflow/compiler/xla:executable_run_options"", ""//third_party/eigen3:eigen3"", ""//tensorflow/core:framework_lite""],
  srcs = [""//ros/src/visual_orientation/bench:test_graph_tfmatmul.o""],
  hdrs = [""//ros/src/visual_orientation/bench:test_graph_tfmatmul.h""],
)
```
The deps reference //tensorflow/... which I do not have in my WORKSPACE. 
bazel build @org_tensorflow//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int32 succeeds
bazel build //tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int32 fails

Can the tf_library reference targets more inteligently or am I making a mistake? Example targets:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/tfcompile.bzl#L313

"
13983,gradient not working with aggregation on TensorArray in tf.while_loop,"### Problem description
I am trying to compute gradient of an aggregation on the currently available elements in `tf.TensorArray` in a `tf.while_loop`, but got an `InvalidArgumentError: TensorArray TensorArray_4_21@while_63/gradients: Could not write to TensorArray index 0 because it has already been read.`

### Minimum code to reproduce the error
```python
def make_loop_test():

    def _cond(i, *_):
        return i < 3

    def _body(i, var, var_hist):
        # write current element
        var_hist = var_hist.write(i, var)

        # retrieve all current previous elements as well as the one appended just now, and compute the sum
        util = tf.reduce_sum(var_hist.gather(tf.range(0, i+1))) * 2.0 + 1.0

        # take gradient, where I think the problem comes from
        grad = tf.gradients(util, [var])[0]

        return i + 1, var - grad * 0.1, var_hist

    _init_state = (0, 2e3, tf.TensorArray(dtype=tf.float32, size=3, clear_after_read=False))

    loop_i, loop_var, loop_var_hist = tf.while_loop(_cond, _body, _init_state, parallel_iterations=1)

    return loop_i, loop_var, loop_var_hist.stack()

with tf.Session() as sess:
    print(sess.run(make_loop_test()))
```

I used to add a bunch of tf.Print statements and found the error was coming from gradient statement in the second iteration. The error message looks weird to me since I am not writing to index 0 at that time.


### Complete logs
```
File ""<ipython-input-132-3da3dc3dd8f4>"", line 18, in <module>
  print(sess.run(make_loop_test()))
File ""<ipython-input-132-3da3dc3dd8f4>"", line 13, in make_loop_test
  loop_i, loop_var, loop_var_hist = tf.while_loop(_cond, _body, _init_state, parallel_iterations=1)
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2775, in while_loop
  result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2604, in BuildLoop
  pred, body, original_loop_vars, loop_vars, shape_invariants)
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2554, in _BuildLoop
  body_result = body(*packed_vars_for_body)
File ""<ipython-input-132-3da3dc3dd8f4>"", line 8, in _body
  grad = tf.gradients(util, [var])[0]
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 542, in gradients
  grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 348, in _MaybeCompile
  return grad_fn()  # Exit early
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 542, in <lambda>
  grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_grad.py"", line 162, in _TensorArrayGatherGrad
  u_g = g.scatter(indices, grad)
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py"", line 175, in wrapped
  return _add_should_use_warning(fn(*args, **kwargs))
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 441, in scatter
  name=name)
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 2649, in _tensor_array_scatter_v3
  name=name)
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
  op_def=op_def)
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
  original_op=self._default_original_op, op_def=op_def)
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
  self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
...which was originally created as op 'while_62/TensorArrayGatherV3', defined at:
File ""/home/ecsark/envs/conda/lib/python3.5/runpy.py"", line 193, in _run_module_as_main
  ""__main__"", mod_spec)
[elided 23 identical lines from previous traceback]
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2554, in _BuildLoop
  body_result = body(*packed_vars_for_body)
File ""<ipython-input-132-3da3dc3dd8f4>"", line 7, in _body
  util = tf.reduce_sum(var_hist.gather(tf.range(0, i+1))) * 2.0 + 1.0
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py"", line 93, in fn
  return method(self, *args, **kwargs)
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 360, in gather
  element_shape=element_shape)
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 2401, in _tensor_array_gather_v3
  element_shape=element_shape, name=name)
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
  op_def=op_def)
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
  original_op=self._default_original_op, op_def=op_def)
File ""/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
  self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
InvalidArgumentError (see above for traceback): TensorArray TensorArray_3_20@while_62/gradients: Could not write to TensorArray index 0 because it has already been read.
 [[Node: while_62/gradients/while_62/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3 = TensorArrayScatterV3[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](while_62/gradients/while_62/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3, while_62/range, while_62/gradients/while_62/Sum_grad/Tile, while_62/gradients/while_62/TensorArrayGatherV3_grad/TensorArrayGrad/gradient_flow)]]
```

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 7.11 (wheezy)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.1-0-g48c54ee 1.3.1 
- **Python version**:  3.5.4
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA"
13982,Feature Request: C++ gradient for Floor,"Implement the gradient for Floor in c++ so that it is available for TF_AddGradients

@suharshs I will implement this one with some guidance from @bpiel 

I believe this is the python code I'll be porting over:
https://github.com/tensorflow/tensorflow/blob/962ed613cf1087637848d3e2b23f5b01d93c7eda/tensorflow/python/ops/math_grad.py#L1004-L1006
"
13980,NadamOptimizer throws InvalidArgumentError (incompatible shapes),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, but this issue can be replicated by changing one line in the example script `word2vec_basic.py`
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7, MacOS High Sierra 
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.4.0-rc0-21-g1e25994', '1.4.0-rc1'). I also know someone who encountered this in the 1.3 release.
- **Python version**: 2.7.10, 2.7.14
- **Bazel version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: 8.0 / 6.0
- **GPU model and memory**: Titan X 12gb
- **Exact command to reproduce**:
Change line 190 in `tensorflow/examples/tutorials/word2vec/word2vec_basic.py` to:
`optimizer = tf.contrib.opt.NadamOptimizer(1.0).minimize(loss)`. 
Run.

### Describe the problem
When I attempt to train a model with `tf.controb.opt.NadamOptimizer`, TensorFlow crashes with an InvalidArgumentError (Incompatible shapes: [105] vs. [50000]), which is thrown from the optimizer (in my code it's when trying to apply updates to the word embedding table.) It looks like it's trying to apply sparse updates to the embeddings using dense operations, causing a shape mismatch, or something along those lines. `AdamOptimizer` and `LazyAdamOptimizer` work fine.

### Source code / logs
Traceback can be found [here](https://gist.github.com/strubell/37897084888252989750b58260f76ff5).
The error is easily replicable in the example script `word2vec_basic.py`.
"
13977,Feature request: adding spectral functions for preserving phase information,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (attempting to)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.5.4
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA v8.0.60, cuDNN 6.0
- **GPU model and memory**: GTX 1060 6 GB
- **Exact command to reproduce**: N/A

### Describe the problem
I am working on a data processing pipeline directly in my tensorflow graph. In that case, I would like to use continuous wavelet transforms (CWT) in order to transform time-series data into scalograms instead of spectrograms in order to increase time-frequency resolution. I can only find a single API for performing the CWT that is compatible directly with TF, but unfortunately, it is not 'enough' since the tf.conv2 operator does not work with inputs of different types (real, complex). I therefore am trying to fix my own op using TF's standard ops, but have hit a snag. The spectral ops rfft and irfft only return and accept positive frequency components, and for my purpose, I would have to apply phase transformations of the spectrum, resulting in non-Hermitian symmetry (in which case I cannot use the tf.spectral.irfft to inverse transform!). So what I request, are spectral ops that return and accept the full complex spectra of input signals. 

### Source code / logs
N/A"
13975,begin_shift_axis not defined in tf.contrib.layers.layer_norm,"The live version of tensorflow's api docs here:
https://www.tensorflow.org/api_docs/python/tf/contrib/layers/layer_norm

Refer to begin_shift_axis, which isn't defined.  I assume this is a typo and is meant to be begin_params_axis

Apologies if documentation bugs are supposed to be tracked elsewhere, it's not clear how those should be reported."
13973,"import error :cudnnConvolutionBiasActivationForward because tensorflow cpu and tensorflow gpu, both were installed","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:Tensorflow_gpu_1.3.0
- **Python version**: on both 2.7 and 3.5
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:CUDA 8/Cudnn 5
- **GPU model and memory**:Nvidia GeForce GTX 1080 Ti, 11 GB
- **Exact command to reproduce**:1)pip install tensorflow
2)pip install tensorflow-gpu
3)pin unistall tensorflow
4)pip uninstall tensorflow-gpu
5)pip install tensorflow-gpu

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I installed tensorflow cpu and tensorflow gpu by mistake .I uninstalled both the versions after  realising my mistake .I then got afresh install with tensorflow gpu.Then i get an import error as below
`ImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: cudnnConvolutionBiasActivationForward`

I tried restarting and force reinstall.but both did not work.I also tried with a virtual environment and failed.
When i downgraded to tensorflow-gpu-1_2-1 it works.However i need 1.3

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13971," is there a way to restore and predict without doing ""dummy training""  ","We are trying **text_classification.py** example of tensorflow  and separated the training and prediction parts of the code and trained the model using model_dir and tried to predict using the same model_dir.

Model is being saved but when we comment the training code and try to predict something, we get the following error:

**InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [10,50] rhs shape= [5386,50]
         [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[""loc:@EmbedSequence/embeddings""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](EmbedSequence/embeddings, save/RestoreV2)]]**

**Where:** 5386 and 10 are words count in training and prediction data.


But when we uncomment the preparation of train_input_fn without using it anywhere in the code , without training the model but using the same model_dir then prediction works fine.

train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={WORDS_FEATURE: x_train},
        y=y_train,
        batch_size=len(x_train),
        num_epochs=None,
        shuffle=True)

We are wondering what exactly is being done by this function which actually overcome the error without training the model (using the function at all) ?

We have gone through many posts but following post looks more relevant where someone said that prediction can't be done without doing at least some training (though the model was already trained and restored using model_dir ):  https://github.com/tensorflow/tensorflow/issues/3340

------------------------

### System information
- **Tried both Python 2.7.6 / Python 3.4.3**:
- **TF Version :  1.3.0**:
- **python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"" :  ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')**


"
13970,import tensorflow error: No Module Named '_pywrap_tensorflow_internal',"Dear All,

I am new to the CNNs and Tensorflow. I wanted to start my journey from Keras.
At this moment I facing problems with onstalling Tensorflow.
I am using Windows 10, I installed CUDA 9 with cuDNN 7, and was getting the following errors.
The I unstalled it and installed tensorflow-gpu, which automatically installs CUDA 8 with cuDNN 6 and I am still facing the same issue. I tried installing Microsoft Visual C++ 2015 Redistributable Update 3 but with no results. My Python version is 3.5.4 and I am using Anaconda 3.
Do you have any ideas what else can I check?  

Here is the error:

Traceback (most recent call last):
  File ""C:\Users\Adam\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Adam\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 985, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 968, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 957, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 938, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Adam\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Adam\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Adam\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Adam\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Adam\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Adam\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Adam\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Adam\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Adam\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 985, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 968, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 957, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 938, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Adam\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Adam\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Adam\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Adam\Anaconda3\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
13969,Centered batch padding with tf.data.Dataset API,"Hi,

First of all, kudos for the PaddedBatchDataset op and all the tf.data.Dataset API.

Right now, all the examples in the batch are aligned at coordinate (0, 0, ...). I think it is very convenient in many applications (e.g. image processing) to have the examples ""centered"" in the batch. 

I would like to know if you would accept a pull request in that direction. If so, I am willing to work on this. I suggest to add a flag to the operator so that the user can decide whether she wants the data centered, or aligned at (0, 0, ...), i.e. the current behavior. This changes should not be difficult to make.
"
13968,"A unexpected read op is in my pb, It make convertion to dlc fail","I want to conver the pb to dlc. but fail. 
I find a unexpected read op between the const and the op.
why ? how can I remove the read op ?


conver error message:
/home/nubiaml/snpe-sdk/snpe-1.2.2/lib/python/converters/tensorflow/layers/eltwise.py:105: RuntimeWarning: error_code=1002; error_message=Layer paramter value is invalid. Layer layer2/Mul: at least two inputs required, have 1; error_component=Model Validation; line_no=582; thread_id=140151840044864
output_name)
/home/nubiaml/snpe-sdk/snpe-1.2.2/lib/python/converters/tensorflow/layers/eltwise.py:83: RuntimeWarning: error_code=1002; error_message=Layer paramter value is invalid. Layer output/output: at least two inputs required, have 1; error_component=Model Validation; line_no=582; thread_id=140151840044864
output_name)

the code which save the pb:
`import tensorflow as tf
from tensorflow.python.framework.graph_util import convert_variables_to_constants

with tf.name_scope(""input""):
    X = tf.placeholder(tf.float32, shape=(None, 1), name=""input"");

with tf.name_scope(""layer2""):
    a = tf.Variable(tf.zeros([1, 1], tf.float32), name=""a"");
    ax = X * a;

with tf.name_scope(""output""):
    b = tf.Variable(tf.zeros([1, 1], tf.float32), name=""b"");
    h = tf.add(ax, b, name=""output"");

y = tf.placeholder(tf.float32, shape=(None, 1), name=""y"");
J = tf.reduce_mean(tf.square(h-y))/2;
optimizer = tf.train.GradientDescentOptimizer(0.1);
train = optimizer.minimize(J, var_list=[a, b]);

sess = tf.Session();
sess.run(tf.global_variables_initializer());

for i in range(10000):
    sess.run([train, J], feed_dict={X:[[1], [2]], y:[[1], [2]]})

print(sess.run([a, b]));

graph = convert_variables_to_constants(sess, sess.graph_def, [""output/output""])
tf.train.write_graph(graph, '.', 'graph.pb', as_text=False)

sess.close();`"
13967,"build the libtensorflow-core.a from the tf1.2 source, but get the SIGABRT running the codes, seems TestCPUFeature fail","Hi TF Experts,

I build the libtensorflow-core.a from the  TF1.2 source codes in one machine with gcc 4.8.2. but when I running the example codes, I got one segment fault. below is the debug info in GDB：

#0  0x00007ffff75f3625 in raise () from /lib64/libc.so.6
#1  0x00007ffff75f4e05 in abort () from /lib64/libc.so.6
#2  0x0000000001d8c395 in __gnu_cxx::__verbose_terminate_handler() () at ../../../../libstdc++-v3/libsupc++/vterminate.cc:95
#3  0x0000000001d59806 in __cxxabiv1::__terminate(void (*)()) () at ../../../../libstdc++-v3/libsupc++/eh_terminate.cc:38
#4  0x0000000001d59833 in std::terminate() () at ../../../../libstdc++-v3/libsupc++/eh_terminate.cc:48
#5  0x0000000001d5a58e in __cxa_throw () at ../../../../libstdc++-v3/libsupc++/eh_throw.cc:84
#6  0x0000000001d88c90 in std::__throw_system_error(int) () at ../../../../../libstdc++-v3/src/c++11/functexcept.cc:104
# #7  0x0000000000cd5051 in tensorflow::port::TestCPUFeature(tensorflow::port::CPUFeature) ()
#8  0x0000000000cd4b0e in _GLOBAL__sub_I_cpu_feature_guard.cc ()
#9  0x0000000001db4966 in __do_global_ctors_aux ()
#10 0x0000000000a3954b in _init ()
#11 0x00007fffffffe588 in ?? ()
#12 0x0000000001db48f5 in __libc_csu_init ()
#13 0x00007ffff75dfcf0 in __libc_start_main () from /lib64/libc.so.6
#14 0x0000000000a40c19 in _start ()

if seems that TestCPUFeature failed. I don't why? I build in this machine, and why it will fail running in the same machine?"
13966,ImportError: libmpich.so.12: cannot open shared object file: No such file or directory,"
### System information
- **OS** : Ubuntu 17.10
- **TensorFlow installed from **: source 
- **TensorFlow version (use command below)**: r1.3
- **Python version**: 3.6
- **CUDA/cuDNN version**: 8.0/ 6

When I try to import tensorflow, I get error:
```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/jihao/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/jihao/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/jihao/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/jihao/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/jihao/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/jihao/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/jihao/anaconda3/lib/python3.6/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/jihao/anaconda3/lib/python3.6/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libmpich.so.12: cannot open shared object file: No such file or directory
```
I try to fix it by command
```bash
sudo apt install libmpich-dev
```
It did not work.
"
13965,compile tensorflow-1.4.0-rc1 failed with error: SWIGing tensorflow/python/tensorflow.i failed (Segmentation fault): swig failed: error executing command,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10 beta2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: TensorFlow 1.4.0-rc1
- **Python version**:  Python 2.7.14
- **Bazel version (if compiling from source)**:  bazel 0.6.0
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
_**bazel build --config=mkl --copt=""-g"" --copt=""-DEIGEN_USE_VML"" --copt=""-mavx2"" --copt=""-mfma"" --copt=""-O3"" --verbose_failures --copt=""-L/opt/intel/gcc/lib64"" -s -c opt //tensorflow/tools/pip_package:build_pip_package**_

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Failed to use  bazel 0.6.0 to compile tensorflow 1.4.0-rc1 on Ubuntu 17.10beta2

1. install Bazel
download bazel-0.6.0-installer-linux-x86_64.sh
bash bazel-0.6.0-installer-linux-x86_64.sh to install bazel
source /usr/local/lib/bazel/bin/bazel-complete.bash

2. compile tensorflow
./configure

_**bazel build --config=mkl --copt=""-g"" --copt=""-DEIGEN_USE_VML"" --copt=""-mavx2"" --copt=""-mfma"" --copt=""-O3"" --verbose_failures --copt=""-L/opt/intel/gcc/lib64"" -s -c opt //tensorflow/tools/pip_package:build_pip_package**_

3. compile failed


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

_ERROR: /home/automation/tensorflow-1.4.0-rc1/tensorflow/python/BUILD:2953:1: SWIGing tensorflow/python/tensorflow.i failed (Segmentation fault): swig failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/35d546f7441fd09e73ff30ea3d9aa112/execroot/org_tensorflow && \
  exec env - \
  bazel-out/host/bin/external/swig/swig -c++ -python -module pywrap_tensorflow_internal -o bazel-out/local-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc -outdir bazel-out/local-opt/bin/tensorflow/python -ltensorflow/python/client/device_lib.i -ltensorflow/python/client/events_writer.i -ltensorflow/python/client/tf_session.i -ltensorflow/python/client/tf_sessionrun_wrapper.i -ltensorflow/python/framework/cpp_shape_inference.i -ltensorflow/python/framework/python_op_gen.i -ltensorflow/python/grappler/cluster.i -ltensorflow/python/grappler/cost_analyzer.i -ltensorflow/python/grappler/item.i -ltensorflow/python/grappler/model_analyzer.i -ltensorflow/python/grappler/tf_optimizer.i -ltensorflow/python/lib/core/py_func.i -ltensorflow/python/lib/core/strings.i -ltensorflow/python/lib/io/file_io.i -ltensorflow/python/lib/io/py_record_reader.i -ltensorflow/python/lib/io/py_record_writer.i -ltensorflow/python/platform/base.i -ltensorflow/python/pywrap_tfe.i -ltensorflow/python/training/quantize_training.i -ltensorflow/python/training/server_lib.i -ltensorflow/python/util/kernel_registry.i -ltensorflow/python/util/port.i -ltensorflow/python/util/py_checkpoint_reader.i -ltensorflow/python/util/stat_summarizer.i -ltensorflow/python/util/tfprof.i -ltensorflow/python/util/transform_graph.i -Ibazel-out/local-opt/genfiles -Iexternal/eigen_archive -Iexternal/grpc -Iexternal/protobuf_archive -Iexternal/swig -Iexternal/boringssl -Ibazel-out/local-opt/genfiles/external/local_config_python -Iexternal/nsync -Iexternal/gemmlowp -Iexternal/jpeg -Iexternal/com_googlesource_code_re2 -Iexternal/mkl -Iexternal/jsoncpp_git -Iexternal/zlib_archive -Iexternal/highwayhash -Iexternal/gif_archive -Iexternal/mkl_dnn -Ibazel-out/local-opt/genfiles/external/jpeg -Iexternal/lmdb -Iexternal/png_archive -Iexternal/farmhash_archive -Iexternal/sqlite_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/python/tensorflow.i): swig failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/35d546f7441fd09e73ff30ea3d9aa112/execroot/org_tensorflow && \
  exec env - \
  bazel-out/host/bin/external/swig/swig -c++ -python -module pywrap_tensorflow_internal -o bazel-out/local-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc -outdir bazel-out/local-opt/bin/tensorflow/python -ltensorflow/python/client/device_lib.i -ltensorflow/python/client/events_writer.i -ltensorflow/python/client/tf_session.i -ltensorflow/python/client/tf_sessionrun_wrapper.i -ltensorflow/python/framework/cpp_shape_inference.i -ltensorflow/python/framework/python_op_gen.i -ltensorflow/python/grappler/cluster.i -ltensorflow/python/grappler/cost_analyzer.i -ltensorflow/python/grappler/item.i -ltensorflow/python/grappler/model_analyzer.i -ltensorflow/python/grappler/tf_optimizer.i -ltensorflow/python/lib/core/py_func.i -ltensorflow/python/lib/core/strings.i -ltensorflow/python/lib/io/file_io.i -ltensorflow/python/lib/io/py_record_reader.i -ltensorflow/python/lib/io/py_record_writer.i -ltensorflow/python/platform/base.i -ltensorflow/python/pywrap_tfe.i -ltensorflow/python/training/quantize_training.i -ltensorflow/python/training/server_lib.i -ltensorflow/python/util/kernel_registry.i -ltensorflow/python/util/port.i -ltensorflow/python/util/py_checkpoint_reader.i -ltensorflow/python/util/stat_summarizer.i -ltensorflow/python/util/tfprof.i -ltensorflow/python/util/transform_graph.i -Ibazel-out/local-opt/genfiles -Iexternal/eigen_archive -Iexternal/grpc -Iexternal/protobuf_archive -Iexternal/swig -Iexternal/boringssl -Ibazel-out/local-opt/genfiles/external/local_config_python -Iexternal/nsync -Iexternal/gemmlowp -Iexternal/jpeg -Iexternal/com_googlesource_code_re2 -Iexternal/mkl -Iexternal/jsoncpp_git -Iexternal/zlib_archive -Iexternal/highwayhash -Iexternal/gif_archive -Iexternal/mkl_dnn -Ibazel-out/local-opt/genfiles/external/jpeg -Iexternal/lmdb -Iexternal/png_archive -Iexternal/farmhash_archive -Iexternal/sqlite_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/python/tensorflow.i)._

"
13964,Callback in tf.Print to access raw data ,"### System information
- **Have I written custom code**: No
- **OS**: Win 10
- **TensorFlow installed from**: binary
- **TensorFlow version**: 1.3.0
- **Python version**:  3.5
- **Bazel version (if compiling from source)**: /
- **CUDA/cuDNN version**:  CPU
- **GPU model and memory**: CPU
- **Exact command to reproduce**: /

### Feature request

The `tf.Print` op lets us print tensor values when it gets evaluated. However, it does not provide any way to access the raw data programmatically. AFAIK, there is no way to access this data during computation. Please correct me if I'm wrong

With Theano, I was able to provide a custom callback to the printing facility, to access the tensor values when tensor was evaluated. This is extremely useful for image tensors for instance to be able to plot them at evaluation time. With TF I cannot do that.

Basically, what I am proposing is adding a custom `callback` parameter to the op:
```
Print(
    input_,
    data,
    message=None,
    first_n=None,
    summarize=None,
    callback=None,
    name=None
)
```
If callback is undefined, `tf.Print` behavior remains identical. Otherwise, `tf.Print` will call the callback with `([values], message, name).

"
13963,Tensorflow crashes when importing certain modules before tensorflow,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
binary both from conda-forge and from pip install using the tfBinaryURL

- **TensorFlow version (use command below)**:
1.3.0

- **Python version**:
3.6.2 from Anaconda

- **CUDA/cuDNN version**:
CUDA 8.0 cuDNN 6.0

- **GPU model and memory**:
GTX 1060 6 GB RAM

- **Exact command to reproduce**:
```python
import psi4
import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello))
```

### Describe the problem
Running the above code produces the following error

```
2017-10-24 22:17:35.570620: F tensorflow/core/framework/op.cc:165] Non-OK-status: RegisterAlreadyLocked(deferred_[i]) status: Invalid argument: Could not parse default value '4000' from Attr(""upper_frequency_limit: float = 4000"") for Op Mfcc
Could not parse default value '20' from Attr(""lower_frequency_limit: float = 20"") for Op Mfcc
forrtl: error (76): Abort trap signal
```

Strangely if tensorflow is imported before psi4 then the same error does not occur. Also, the error seems to be related to one of the Ops in tensorflow/tensorflow/core/kernels/mfcc.h which as far as I can tell is not an Op that I'm even using.

This issue occurs with at least two different python modules that I know of, psi4 as I have run into, and lycon which has their own issue open regarding this same error ethereon/lycon#3. In the linked issue it seems that this error occurs for others on python 2 and 3.5 with Ubuntu 16.04 both with and without GPU support for tensorflow, but another user states that this did not occur for him on Ubuntu 14.04 or on macOS 10.13. It seems like this may be an issue specific to at least Ubuntu 16.04 then.

### Source code / logs
Full output from the error

```
2017-10-24 22:17:35.570620: F tensorflow/core/framework/op.cc:165] Non-OK-status: RegisterAlreadyLocked(deferred_[i]) status: Invalid argument: Could not parse default value '4000' from Attr(""upper_frequency_limit: float = 4000"") for Op Mfcc
Could not parse default value '20' from Attr(""lower_frequency_limit: float = 20"") for Op Mfcc
forrtl: error (76): Abort trap signal
Image              PC                Routine            Line        Source             
libpcm.so.1        00007FCA4B14A725  Unknown               Unknown  Unknown
libpcm.so.1        00007FCA4B148347  Unknown               Unknown  Unknown
libpcm.so.1        00007FCA4B05FAA2  Unknown               Unknown  Unknown
libpcm.so.1        00007FCA4B05F8F6  Unknown               Unknown  Unknown
libpcm.so.1        00007FCA4B02DEFD  Unknown               Unknown  Unknown
libpcm.so.1        00007FCA4B031ABF  Unknown               Unknown  Unknown
libpthread.so.0    00007FCA5609F390  Unknown               Unknown  Unknown
libc.so.6          00007FCA55CF9428  Unknown               Unknown  Unknown
libc.so.6          00007FCA55CFB02A  Unknown               Unknown  Unknown
_pywrap_tensorflo  00007FCA0DA71B94  Unknown               Unknown  Unknown
_pywrap_tensorflo  00007FCA0D94AAFE  Unknown               Unknown  Unknown
_pywrap_tensorflo  00007FCA0D94B21A  Unknown               Unknown  Unknown
_pywrap_tensorflo  00007FCA0D924199  Unknown               Unknown  Unknown
_pywrap_tensorflo  00007FCA0D946EFE  Unknown               Unknown  Unknown
_pywrap_tensorflo  00007FCA0D934760  Unknown               Unknown  Unknown
_pywrap_tensorflo  00007FCA0D68F8D7  Unknown               Unknown  Unknown
_pywrap_tensorflo  00007FCA0D62136E  Unknown               Unknown  Unknown
_pywrap_tensorflo  00007FCA0D621488  Unknown               Unknown  Unknown
_pywrap_tensorflo  00007FCA0D621876  Unknown               Unknown  Unknown
_pywrap_tensorflo  00007FCA0B615970  Unknown               Unknown  Unknown
_pywrap_tensorflo  00007FCA0B434177  Unknown               Unknown  Unknown
python             000055BE1E6850C6  Unknown               Unknown  Unknown
python             000055BE1E7181DC  Unknown               Unknown  Unknown
python             000055BE1E7398BA  Unknown               Unknown  Unknown
python             000055BE1E711FEB  Unknown               Unknown  Unknown
python             000055BE1E7182B5  Unknown               Unknown  Unknown
python             000055BE1E7398BA  Unknown               Unknown  Unknown
python             000055BE1E713116  Unknown               Unknown  Unknown
python             000055BE1E713B84  Unknown               Unknown  Unknown
python             000055BE1E684F8E  Unknown               Unknown  Unknown
python             000055BE1E73AF74  Unknown               Unknown  Unknown
python             000055BE1E711334  Unknown               Unknown  Unknown
python             000055BE1E712221  Unknown               Unknown  Unknown
python             000055BE1E7182B5  Unknown               Unknown  Unknown
python             000055BE1E7398BA  Unknown               Unknown  Unknown
python             000055BE1E711908  Unknown               Unknown  Unknown
python             000055BE1E712221  Unknown               Unknown  Unknown
python             000055BE1E7182B5  Unknown               Unknown  Unknown
python             000055BE1E7398BA  Unknown               Unknown  Unknown
python             000055BE1E711FEB  Unknown               Unknown  Unknown
python             000055BE1E7182B5  Unknown               Unknown  Unknown
python             000055BE1E7398BA  Unknown               Unknown  Unknown
python             000055BE1E711334  Unknown               Unknown  Unknown
python             000055BE1E712221  Unknown               Unknown  Unknown
python             000055BE1E7182B5  Unknown               Unknown  Unknown
python             000055BE1E7398BA  Unknown               Unknown  Unknown
python             000055BE1E712D58  Unknown               Unknown  Unknown
python             000055BE1E713B0C  Unknown               Unknown  Unknown
python             000055BE1E78DF04  Unknown               Unknown  Unknown
python             000055BE1E65475A  Unknown               Unknown  Unknown
python             000055BE1E6548FD  Unknown               Unknown  Unknown
python             000055BE1E65495E  Unknown               Unknown  Unknown
python             000055BE1E656B65  Unknown               Unknown  Unknown
python             000055BE1E65951E  Unknown               Unknown  Unknown
libc.so.6          00007FCA55CE4830  Unknown               Unknown  Unknown
python             000055BE1E7403D9  Unknown               Unknown  Unknown
Aborted (core dumped)
```
"
13962,Compiling TensorFlow 1.4.0 GPU on Windows 10 x64,"There doesn't seem to be any detailed documentation for how to compile TensorFlow 1.4.0 GPU on Windows 10 x64.

I need to recompile TF to add missing functionality for a Windows 7 x64 production system.

 I can use Bazel or CMake to compile something but how do I incorporate the Python and NVIDIA dependencies into that build?

How are the Windows wheels at https://pypi.python.org/pypi/tensorflow-gpu compiled?"
13957,TimeDistributed (keras) wrapper broken in 1.4rc1,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Sierra
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4rc1
- **Python version**: 3.6
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See below

### Describe the problem
Two issues I've found when testing the TimeDistributed wrapper in release *1.4rc1*. I figure I'd wrap them (pun intended?) in one issue since they're both occur on the TimeDistributed wrapper. See simple code examples below which both work fine in version *1.3*.

### Source code / logs
```
def td():
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
    model.add(tf.keras.layers.Dense(4))
    model.summary()
    
    frame_input = tf.keras.layers.Input(shape=(10, 16))
    x = tf.keras.layers.TimeDistributed(model)(frame_input)
    x = tf.keras.layers.Flatten()(x)
    
    full_model = tf.keras.models.Model(inputs=frame_input, outputs=x)
    full_model.summary()
```
produces this trace:

```
     x = tf.keras.layers.TimeDistributed(model)(frame_input)
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py"", line 252, in __call__
    output = super(Layer, self).__call__(inputs, **kwargs)
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 575, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/layers/wrappers.py"", line 238, in call
    output_shape = self._compute_output_shape(input_shape).as_list()
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/layers/wrappers.py"", line 193, in _compute_output_shape
    child_input_shape).as_list()
AttributeError: 'NoneType' object has no attribute 'as_list'
```
and...

```
def td2():
    vgg16_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(100, 100, 3))
    
    frame_input = tf.keras.layers.Input(shape=(10, 100, 100, 3))
    
    x = tf.keras.layers.TimeDistributed(vgg16_model)(frame_input)
    model = tf.keras.models.Model(inputs=frame_input, outputs=x)
    model.summary()
```
produces this trace:

```
    x = tf.keras.layers.TimeDistributed(vgg16_model)(frame_input)
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py"", line 252, in __call__
    output = super(Layer, self).__call__(inputs, **kwargs)
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 575, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/layers/wrappers.py"", line 234, in call
    y = self.layer.call(inputs, **kwargs)
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 1920, in call
    output_tensors, _, _ = self._run_internal_graph(inputs, masks)
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 2084, in _run_internal_graph
    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py"", line 171, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 835, in __call__
    return self.conv_op(inp, filter)
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 499, in __call__
    return self.call(inp, filter)
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 187, in __call__
    name=self.name)
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 631, in conv2d
    data_format=data_format, name=name)
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2959, in create_op
    self._add_op(ret)
  File ""/Users/guest/dev/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2599, in _add_op
    ""is already used"" % op.name)
ValueError: cannot add op with name block1_conv1/convolution as that name is already used
```
"
13956,"tf.contrib.layers.flatten has neither ""name"" nor ""reuse"" parameter.","I think `tf.contrib.layers.flatten` is just a special case of `tf.reshape`. 

However, while `tf.reshape` has `name` parameter, `tf.contrib.layers.flatten` doesn't have one. Also neither of them has `reuse` parameter.  

Is `tf.contrib.layers.flatten` a deprecated API?"
13953,Tensorboard not displaying event files,"Hi all
First time posting, please forgive and correct me if i do anything wrong or stupid.

I have an issue where I am training a model, but tensorboard isn't displaying any data from the event files. I've done everything in my knowledge. Yes I read the tensorboard ReadMe FAQ. [https://github.com/tensorflow/tensorboard/blob/master/README.md#frequently-asked-questions](url)

I start the train process with the folowing command (cmd started in folder with the directories ""tf_files"" and ""flower_photos"" int it):
`python retrain.py --bottleneck_dir=tf_files/bottlenecks --model_dir=tf_files/models/""mobilenet_0.50_224"" --summaries_dir=tf_files/training_summaries/""mobilenet_0.50_224"" --output_graph=tf_files/retrained_graph.pb --output_labels=tf_files/retrained_labels.txt --architecture=""mobilenet_0.50_224"" --image_dir=flower_photos`

![retrain_start_1](https://user-images.githubusercontent.com/9570377/31962587-4074134c-b8fe-11e7-95ab-53b565cfa96e.png)
![retrain_start_2](https://user-images.githubusercontent.com/9570377/31962591-4317b784-b8fe-11e7-94f2-aebbd8bd11ef.png)
![retrain_finish](https://user-images.githubusercontent.com/9570377/31962594-459de2da-b8fe-11e7-8511-76a502fc7763.png)

The retraining finishes successfully.

Tensorboard i start with the command:
`tensorboard --logdir i:/temp/learningtf/tf_files/training_summaries`

![start_tensorboard](https://user-images.githubusercontent.com/9570377/31962693-8ddea9c6-b8fe-11e7-8876-b40bda7349b0.png)


The command above with `--inspect` give me the following output:

![tensorboard_inspect_1](https://user-images.githubusercontent.com/9570377/31962725-a472b268-b8fe-11e7-948b-75c0adb866b1.png)
![tensorboard_inspect_2](https://user-images.githubusercontent.com/9570377/31962727-a6f84458-b8fe-11e7-9b3a-0f4e41a8ebe0.png)


As you can see, tensorboard doesn't display anything. Im quite confused.
![tensorboard_webinterface](https://user-images.githubusercontent.com/9570377/31962837-faed81b8-b8fe-11e7-8f14-d7407a4b9130.png)

Here are the event files I'm talking about. If these are not the files you need, post a reply and tell my which files i need to upload, please.
[the_i_hope_right_files.zip](https://github.com/tensorflow/tensorflow/files/1412114/the_i_hope_right_files.zip)

If you need eny further Info on this case, please let me know.
Thanks for any help in advance."
13952,tensorflow + tensorboard on cifs results in PermissionDeniedError,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Using stock example script mnist.py from https://github.com/dandelionmane/tf-dev-summit-tensorboard-tutorial
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **TensorFlow installed from (source or binary)**: using standard tensorflow/tensorflow:1.1.0-gpu image
- **TensorFlow version (use command below)**: using standard tensorflow/tensorflow:1.1.0-gpu image
- **Python version**: using standard tensorflow/tensorflow:1.1.0-gpu image
- **Bazel version (if compiling from source)**: tensorflow/tensorflow:1.1.0-gpu
- **CUDA/cuDNN version**:
- **GPU model and memory**: Tesla K80
- **Exact command to reproduce**:

### Describe the problem
I am running stock  mnist.py from https://github.com/dandelionmane/tf-dev-summit-tensorboard-tutorial on  tensorflow/tensorflow:1.1.0-gpu and running tensorboard in parallel on the same container. Everything works well if logdir is located on local ssd.
If i am configuring logdir to be on samba directory and using tensorboard during job execution (specifically if i am using Embeddings), the job fails at writing temp checkpoint files (see logs below). If I am not using tensorboard embeddings during job's execution, the job finishes successfully.

### Source code / logs
2017-10-24 17:39:31.165783: W tensorflow/core/framework/op_kernel.cc:1152] Permission denied: ./model.ckpt-500.index.tempstate18351127508205389812
Traceback (most recent call last):
  File ""/mnt/batch/tasks/shared/LS_root/mounts/external/tb_sample/mnist.py"", line 164, in <module>
    main()
  File ""/mnt/batch/tasks/shared/LS_root/mounts/external/tb_sample/mnist.py"", line 156, in main
    mnist_model(learning_rate, use_two_fc, use_two_conv, hparam)
  File ""/mnt/batch/tasks/shared/LS_root/mounts/external/tb_sample/mnist.py"", line 136, in mnist_model
    saver.save(sess, os.path.join(LOGDIR, ""model.ckpt""), i)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1391, in save
    {self.saver_def.filename_tensor_name: checkpoint_file})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 778, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 982, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1032, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1052, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.PermissionDeniedError: ./model.ckpt-500.index.tempstate18351127508205389812
         [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, conv1/B/_41, conv1/B/Adam/_43, conv1/B/Adam_1/_45, conv1/W/_47, conv1/W/Adam/_49, conv1/W/Adam_1/_51, conv2/B/_53, conv2/B/Adam/_55, conv2/B/Adam_1/_57, conv2/W/_59, conv2/W/Adam/_61, conv2/W/Adam_1/_63, fc1/B/_65, fc1/B/Adam/_67, fc1/B/Adam_1/_69, fc1/W/_71, fc1/W/Adam/_73, fc1/W/Adam_1/_75, fc2/B/_77, fc2/B/Adam/_79, fc2/B/Adam_1/_81, fc2/W/_83, fc2/W/Adam/_85, fc2/W/Adam_1/_87, test_embedding/_89, train/beta1_power/_91, train/beta2_power/_93)]]

Caused by op u'save/SaveV2', defined at:
  File ""/mnt/batch/tasks/shared/LS_root/mounts/external/tb_sample/mnist.py"", line 164, in <module>
    main()
  File ""/mnt/batch/tasks/shared/LS_root/mounts/external/tb_sample/mnist.py"", line 156, in main
    mnist_model(learning_rate, use_two_fc, use_two_conv, hparam)
  File ""/mnt/batch/tasks/shared/LS_root/mounts/external/tb_sample/mnist.py"", line 114, in mnist_model
    saver = tf.train.Saver()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1056, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1086, in build
    restore_sequentially=self._restore_sequentially)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 689, in build
    save_tensor = self._AddSaveOps(filename_tensor, saveables)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 276, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 219, in save_op
    tensors)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 780, in save_v2
    tensors=tensors, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1228, in __init__
    self._traceback = _extract_stack()

PermissionDeniedError (see above for traceback): ./model.ckpt-500.index.tempstate18351127508205389812
         [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, conv1/B/_41, conv1/B/Adam/_43, conv1/B/Adam_1/_45, conv1/W/_47, conv1/W/Adam/_49, conv1/W/Adam_1/_51, conv2/B/_53, conv2/B/Adam/_55, conv2/B/Adam_1/_57, conv2/W/_59, conv2/W/Adam/_61, conv2/W/Adam_1/_63, fc1/B/_65, fc1/B/Adam/_67, fc1/B/Adam_1/_69, fc1/W/_71, fc1/W/Adam/_73, fc1/W/Adam_1/_75, fc2/B/_77, fc2/B/Adam/_79, fc2/B/Adam_1/_81, fc2/W/_83, fc2/W/Adam/_85, fc2/W/Adam_1/_87, test_embedding/_89, train/beta1_power/_91, train/beta2_power/_93)]]"
13951,help with classifier.predict and predicted_classes,"### System information
- custom code: no, it is the one in https://www.tensorflow.org/get_started/estimator
- system: Apple
- OS: Mac OsX 10.13
- TensorFlow version: 1.3.0
- Python version: 3.6.3
- GPU model: AMD FirePro D700 (actually, two such GPUs)


### Describe the problem
Dear all,
I am running the simple iris program:
https://www.tensorflow.org/get_started/estimator
under python 3.6.3 and tensorflow 1.3.0.
The program executes correctly, apart from the very last part, i.e. the one related to the confusion matrix.
In fact, the result I get for the confusion matrix is:
New Samples, Class Predictions:    [array([b'1'], dtype=object), array([b'2'], dtype=object)]
rather than the expected output:
New Samples, Class Predictions:    [1 2]
Has anything about confusion matrix changed in the latest release?
If so, how should I modify that part of the code?
Thank you very much for your help!
Best regards
Ivan



### Source code / logs
https://www.tensorflow.org/get_started/estimator
"
13950,Error in documentation: programmers_guide/variables.md,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: No.
- **TensorFlow installed from (source or binary)**: binary.
- **TensorFlow version (use command below)**:  r1.3
- **Python version**: Irrelevant
- **Bazel version (if compiling from source)**: Irrelevant
- **CUDA/cuDNN version**: Irrelevant
- **GPU model and memory**: Irrelevant
- **Exact command to reproduce**: Irrelevant

### Indentation is wrong
The example code snippets from the bottom of chapter **PROGRAMMERS' GUIDE** / **Variables**:
``` python
with tf.variable_scope(""model"") as scope:
  output1 = my_image_filter(input1)
with tf.variable_scope(scope, reuse=True):
  output2 = my_image_filter(input2)

```
should be:
``` python
with tf.variable_scope(""model"") as scope:
  output1 = my_image_filter(input1)
  with tf.variable_scope(scope, reuse=True):
    output2 = my_image_filter(input2)

```
"
13947,Tensorflow import fails with Segmentation fault error,"Hello

I have pip3 installed tensorflow CPU only
Python version:  3.5.2
OS:LSB Version:    :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
Distributor ID: RedHatEnterpriseServer
Description:    Red Hat Enterprise Linux Server release 6.8 (Santiago)
Release:        6.8
Codename:       Santiago

Install command: pip3 install tensorflow

echo $LD_LIBRARY_PATH
:/usr/local/lib:/grapps/hadoop/instantclient/instantclient_11_2:/usr/local/lib:/opt/glibc-2.14/lib


Python 3.5.2 (default, Mar 23 2017, 07:51:35)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-17)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>>
>>> import tensorflow
Segmentation fault


Also tried to import scipy,numpy and matplotlib before importing tensorflow as per suggestion seens in some other cases https://github.com/tensorflow/tensorflow/issues/2034. 

But nothing solved the issue. 

Thanks for the advice 

Abraham 



"
13946,Create Model file for Object (face) Recognition in C++,"I am doing research on face Recognition using tensorflow.
Can I please know how how to use the code to create model file for face recognition.

**Task (required in C++)**
I have 5 folders, each folder has 4 images of a particular person.
I would like to train the images and generate a model file from scratch.
I would then like to use the model file for recognizing one of the 5 persons.

**Steps followed**
1. https://www.tensorflow.org/tutorials/image_retraining

**Issues**
> 1. Its in Python , but I wanted in C++
> 2. It is not training the model file from scratch
> 

2.https://github.com/tensorflow/models/tree/master/research/inception/inception

**Issue Resolved**
> Model can be trained from scratch. 
>
**Unresolved Issue**
> Its in Python , but I wanted in C++
> 

Please help me in creating the model file in C++"
13945,ImportError: cannot import name gen_checkpoint_ops,"### System information
- **Have I written custom code**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: via pip
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 2.7.12
- **Exact command to reproduce**: `from tensorflow.contrib.tensorboard.plugins import projector`

### Describe the problem
Been following [this tuturial from TF website](https://www.tensorflow.org/versions/r0.12/how_tos/embedding_viz/) to visualize embeddings using Tensorboard. When running the code to add metadata, the following error log returns:

```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-39-40557054c972> in <module>()
----> 1 from tensorflow.contrib.tensorboard.plugins import projector
      2 
      3 # Use the same LOG_DIR where you stored your checkpoint.
      4 summary_writer = tf.train.SummaryWriter(LOG_DIR)
      5 

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/__init__.py in <module>()
     20 
     21 # Add projects here, they will show up under tf.contrib.
---> 22 from tensorflow.contrib import bayesflow
     23 from tensorflow.contrib import cloud
     24 from tensorflow.contrib import compiler

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/bayesflow/__init__.py in <module>()
     22 
     23 # pylint: disable=unused-import,line-too-long
---> 24 from tensorflow.contrib.bayesflow.python.ops import csiszar_divergence
     25 from tensorflow.contrib.bayesflow.python.ops import entropy
     26 from tensorflow.contrib.bayesflow.python.ops import monte_carlo

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/bayesflow/python/ops/csiszar_divergence.py in <module>()
     24 # go/tf-wildcard-import
     25 # pylint: disable=wildcard-import
---> 26 from tensorflow.contrib.bayesflow.python.ops.csiszar_divergence_impl import *
     27 # pylint: enable=wildcard-import
     28 from tensorflow.python.util.all_util import remove_undocumented

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/bayesflow/python/ops/csiszar_divergence_impl.py in <module>()
     40 import numpy as np
     41 
---> 42 from tensorflow.contrib import framework as contrib_framework
     43 from tensorflow.contrib.bayesflow.python.ops import monte_carlo_impl as monte_carlo
     44 from tensorflow.python.framework import ops

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/__init__.py in <module>()
     87 # pylint: disable=unused-import,wildcard-import
     88 from tensorflow.contrib.framework.python.framework import *
---> 89 from tensorflow.contrib.framework.python.ops import *
     90 # pylint: enable=unused-import,wildcard-import
     91 

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/__init__.py in <module>()
     22 # pylint: disable=wildcard-import
     23 from tensorflow.contrib.framework.python.ops.arg_scope import *
---> 24 from tensorflow.contrib.framework.python.ops.checkpoint_ops import *
     25 from tensorflow.contrib.framework.python.ops.ops import *
     26 from tensorflow.contrib.framework.python.ops.prettyprint_ops import *

/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/checkpoint_ops.py in <module>()
     20 import math
     21 
---> 22 from tensorflow.contrib.framework.python.ops import gen_checkpoint_ops
     23 from tensorflow.contrib.util import loader
     24 from tensorflow.python.framework import dtypes

ImportError: cannot import name gen_checkpoint_ops

```

The files gen_checkpoint_ops.py and gen_checkpoint_ops.pyc exist in the specified directory.

Would appreciate any hint.
"
13944,How can I use all of cpu cores in android ,"Hi, 
     Thank your for your work.
     In my app, I use tensorflow to classify.  But CPU usage are lower, just about 30%. My device has 4 cores.  I want to use all CPUs to reduce cost time. I try to modify intra_op_parallelism_threads and  inter_op_parallelism_threads, when compile the libtensorflow_inference.so.  And use TF_SetConfig to set  TF_SessionOptions.　But it doesn't work. 
     Can you give me some advice?  thank you.
 "
13943,Doubt in definition of state returned by dynamic_rnn for LSTM cell in tensorflow,"### Describe the problem
Can someone tell me whether the state returned by the dynamic_rnn function for a LSTM cell is (h,c) or is (c,h)? c - cell state h - hidden state.
"
13942,Support grayscale .bmp images,"The current implementation only supports 3 (RGB) or 4 (ARGB) channel BMP images. However, grayscale images are also often used for machine learning stuff. Therefore it would be great to be able to read 1 channel BMP images.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/decode_bmp_op.cc#L71"
13940,"Feature request: tfdbg support for ""Session.partial_run_setup"" and ""Session.partial_run""","Hello,
         When I debug tensorflow codes written with ""partial_run_setup"" and ""partial_run"" using tfdbg, it output ""partial_run_setup is not implemented for debug-wrapper sessions"". I want to know how to debug tensorflow codes with ""partial_run_setup"" and ""partial_run"". Thank you. "
13939,Quantization make graph slower during inference.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: using TF source code (GPU build), can provide docker to reproduce environment conditions if necessary
- **TensorFlow version**: using r1.3 branch, version 1.3.1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.6.1
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: GeForce GTX 1080 Ti, 11170 MB
- **Exact command to reproduce**: 
/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph  --in_graph=/quantization/VGG16/frozen_model.pb   --outputs=""Validation_segmentation/Validation/decoder/Softmax"" --out_graph=/quantization/VGG16/optimized_model.pb   --transforms='add_default_attributes strip_unused_nodes(type=float, shape=""384,1248,3"") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes'


### Describe the problem
Hi, I compressed a graph using transform_graph tool but the resulting graph is actually slower during inference. I am compressing a graph similar to the one presented in this article: https://arxiv.org/pdf/1612.07695.pdf, which has VGG16 as an encoder in input and a classification decoder with a Softmax in output. Inference uses same python script for both graph (original and quantized) and make an average of 100 inferences. Original graph takes ~0.1s for inference, quantized graph takes 70s! If I perform quantization without quantize_nodes, inference takes ~0.3s.

I understand that this quantization is still in a work in progress and maybe was more aimed at improving inference on mobile devices, but I'm surprised that it is actually so much slower, so that's why I'm logging it as a bug here. (I posted this on stackoverflow but didn't get any answer...)

The graph takes ~500Mb, let me know if I should attach it to this ticket (or include an external link?)

### Source code / logs
[quantization_logs.txt](https://github.com/tensorflow/tensorflow/files/1409825/quantization_logs.txt)
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1409824/tf_env.txt)
[inference.py.txt](https://github.com/tensorflow/tensorflow/files/1409864/inference.py.txt)

"
13933,gradient registry has no entry for: FloorMod,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
A small reproducible example has been provided.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
From docker gpu image
- **TensorFlow version (use command below)**:
1.3.0
v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 
3.5.2
- **CUDA/cuDNN version**:
 V8.0.61
- **GPU model and memory**:
GeForce GTX 1080, 8GB


### Describe the problem
The mod operation claims to have no gradient defined. When running the below code, I receive these messages:
```
LookupError: gradient registry has no entry for: FloorMod
```
and
```
LookupError: No gradient defined for operation 'mod' (op type: FloorMod)
```

### Source code / logs
A minimal reproducible example is found here:
```
import tensorflow as tf

sess = tf.InteractiveSession()
a = tf.placeholder(dtype=tf.float32, shape=[5, 2])

# b = snt.Linear(output_size=4)(a)
W = tf.Variable(tf.zeros([2, 10]))
b = tf.Variable(tf.zeros([10]))
b = tf.matmul(a, W) + b

loss = tf.reduce_sum(b) % 2
update_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)

sess.run(tf.global_variables_initializer())
sess.run(update_op, {a: [[1, 2], [3, 4]]})
```

This results in the following traceback:
```
---------------------------------------------------------------------------
LookupError                               Traceback (most recent call last)
/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)
    511             try:
--> 512               grad_fn = ops.get_gradient_function(op)
    513             except LookupError:

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in get_gradient_function(op)
   1835     op_type = op.type
-> 1836   return _gradient_registry.lookup(op_type)
   1837 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/registry.py in lookup(self, name)
     92       raise LookupError(
---> 93           ""%s registry has no entry for: %s"" % (self._name, name))

LookupError: gradient registry has no entry for: FloorMod

During handling of the above exception, another exception occurred:

LookupError                               Traceback (most recent call last)
<ipython-input-1-7b9ad04151d6> in <module>()
     10 
     11 loss = tf.reduce_sum(b) % 2
---> 12 update_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)
     13 
     14 sess.run(tf.global_variables_initializer())

/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)
    313         aggregation_method=aggregation_method,
    314         colocate_gradients_with_ops=colocate_gradients_with_ops,
--> 315         grad_loss=grad_loss)
    316 
    317     vars_with_grad = [v for g, v in grads_and_vars if g is not None]

/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)
    384         gate_gradients=(gate_gradients == Optimizer.GATE_OP),
    385         aggregation_method=aggregation_method,
--> 386         colocate_gradients_with_ops=colocate_gradients_with_ops)
    387     if gate_gradients == Optimizer.GATE_GRAPH:
    388       grads = control_flow_ops.tuple(grads)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)
    514               raise LookupError(
    515                   ""No gradient defined for operation '%s' (op type: %s)"" %
--> 516                   (op.name, op.type))
    517         if loop_state:
    518           loop_state.EnterGradWhileContext(op, before=False)

LookupError: No gradient defined for operation 'mod' (op type: FloorMod)
```



Any idea on a work-around? I need to use modulo as part of my loss function.
I use it to convert some coordinates from global space to their relative position with a specific grid-cell (ask me if you want a better explanation - I don't suppose it's particularly relevant though).


__Thank you!__"
13932,Non-determinism from `tf.data.Dataset.map` with random ops,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes -- please see the minimal reproducible example script below.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12, Linux CentOS 7 (4.6.6-300.el7.centos.x86_64)
- **TensorFlow installed from (source or binary)**: `pip3 install tf-nightly` (also happens when built from source)
- **TensorFlow version (use command below)**: v1.3.0-rc1-3690-g9b9cbbe 1.5.0-dev20171023
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: N/A since nightly build reproduces the issue (but when built from source, I use 0.6.1-homebrew)
- **CUDA/cuDNN version**: a GPU is not needed to reproduce the issue (however, it has also been tested with CUDA 8.0.61 / cuDNN 7.0.1)
- **GPU model and memory**: N/A -- a GPU is not needed to reproduce the issue (however, it has also been tested with Tesla K80s)
- **Exact command to reproduce**: See minimal reproducible example below

### Describe the problem
The new `tf.data.Dataset` API contains a `map` function with a `num_parallel_calls` parameter, which allows elements to be processed in parallel by multiple threads.  Although not explicitly mentioned in the API docs, prior discussions (such as a comment from [today](https://github.com/tensorflow/tensorflow/issues/13847#issuecomment-338772693)) have indicated that the `map` function should be deterministic (w.r.t. the graph seed) even if `num_parallel_calls > 1`.  I have observed that if the function being mapped contains only non-random ops, then this determinism is observed (see step 2 below).  However, if the the function being mapped contains a random op, the results become non-deterministic for all values of `num_parallel_calls > 1`.  This is unexpected, and prevents training experiments from being reproducible, unless `num_parallel_calls == 1`.  Also, please note that the example below serves as a minimal example to reproduce the issue.  The real scenario involves running data augmentation during training.

### Source code / logs
1. `pip3 install tf-nightly`
2.  Run the following code to observe that `map` functions with only *non-random* ops are *deterministic* for *all* values of `num_parallel_calls`, which is the *expected* behavior:

```python
import numpy as np
import tensorflow as tf

def test(threads):
  np.random.seed(42)
  tf.set_random_seed(42)
  images = np.random.rand(100, 64, 64, 3).astype(np.float32)

  def get_data():
    dataset = tf.data.Dataset.from_tensor_slices(images)  # some initial dataset
    dataset = dataset.map(lambda x: x * 2, num_parallel_calls=threads)  # this works fine always
    dataset = dataset.batch(32)
    x = dataset.make_one_shot_iterator().get_next()
    return x

  # execution 1
  x = get_data()
  with tf.Session() as sess:
    x_batch1 = sess.run(x)

  # clear out everything
  tf.reset_default_graph()

  # execution 2
  x = get_data()
  with tf.Session() as sess:
    x_batch2 = sess.run(x)

  # results should be equivalent
  assert np.allclose(x_batch1, x_batch2)

test(1)  # works with 1 thread!
test(15)  # works with >1 threads!
```

3. Run the following code to observe that `map` functions with *random* ops are deterministic if `num_parallel_calls == 1`, but are *non-deterministic* for values of `num_parallel_calls > 1`, which seems to me to be an *unexpected* behavior:
```python
import numpy as np
import tensorflow as tf

def test(threads):
  np.random.seed(42)
  tf.set_random_seed(42)
  images = np.random.rand(100, 64, 64, 3).astype(np.float32)

  def get_data():
    dataset = tf.data.Dataset.from_tensor_slices(images)  # some initial dataset
    # ONLY DIFFERENCE IS THE BELOW LINE:
    dataset = dataset.map(lambda image: tf.image.random_hue(image, 0.04, seed=42), num_parallel_calls=threads)
    # ONLY DIFFERENCE IS THE ABOVE LINE ^^^:
    dataset = dataset.batch(32)
    x = dataset.make_one_shot_iterator().get_next()
    return x

  # execution 1
  x = get_data()
  with tf.Session() as sess:
    x_batch1 = sess.run(x)

  # clear out everything
  tf.reset_default_graph()

  # execution 2
  x = get_data()
  with tf.Session() as sess:
    x_batch2 = sess.run(x)

  # results should be equivalent
  assert np.allclose(x_batch1, x_batch2)

test(1)  # works with 1 thread!
test(15)  # fails with >1 threads!
```

4. Observe that swapping out the `map` line above with an entirely different random op such as `dataset = dataset.map(lambda x: x * tf.random_normal([64, 64, 3], seed=42), num_parallel_calls=threads)` is also *non-deterministic* for values of `num_parallel_calls > 1`."
13931,TF 1.4 (CUDA 9/CUDNN7) Chrome Tracing Timeline broken,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow


------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ArchLinux Latest
- **TensorFlow installed from (source or binary)**:Source
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.7
- **CUDA/cuDNN version**: 9 /7
- **GPU model and memory**: 2* Nvidia1080ti (11GB)
- **Exact command to reproduce**: 
run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
run_metadata = tf.RunMetadata()
training_loss_, _ = sess.run(op_list,                                          
      feed_dict=feed_dict,
      options=run_options,run_metadata=run_metadata)
tl=timeline.Timeline(run_metadata.step_stats)
chrome_trace = tl.generate_chrome_trace_format()
with open(filename, 'w') as f:
              f.write(chrome_trace)


### Describe the problem
After switching to TF 1.4 compiled with CUDA 9 /CUDNN7 from TF 1.3 with CUDA 8 and CUDNN 6, chrome trace timeline stopped working, otherwise the code runs as before. See the attached screenshots (TF1.4 / TF1.3).

### Source code / logs
![image](https://user-images.githubusercontent.com/24719485/31917426-7a1b3828-b84f-11e7-8352-b82088b353bb.png)
![image](https://user-images.githubusercontent.com/24719485/31917461-9f9dd3d0-b84f-11e7-874d-c9c6dadf3bc8.png)
"
13930,Build failure CUDA8 / cudNN8-v7,"### System information
- **Fedora 26 x64**:
- **TensorFlow installed from source**:
- **TensorFlow version commit 53e7541cf7efa61ba22c9f042e07031d87c8f145 (oct 23 11:46)**:
- **Python version 3.6**: 
- **Bazel version 0.5.4**:
- **CUDA 8.0**
- **cuDNN 8.0 v7**:
- **GPU model GTX 1060**:
- **bazel build -c opt --copt=-march=native --config=cuda //tensorflow/tools/pip_package:build_pip_package**:

Have tried `bazel clean`.
```
.....
external/protobuf_archive/python/google/protobuf/pyext/message.cc: In instantiation of 'bool google::protobuf::python::CheckAndGetInteger(PyObject*, T*) [with T = long unsigned int; PyObject = _object]':
external/protobuf_archive/python/google/protobuf/pyext/message.cc:698:60:   required from here
external/protobuf_archive/python/google/protobuf/pyext/message.cc:635:20: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]
INFO: From Compiling tensorflow/core/lib/strings/numbers.cc:
tensorflow/core/lib/strings/numbers.cc: In function 'std::__cxx11::string tensorflow::strings::HumanReadableNumBytes(tensorflow::int64)':
tensorflow/core/lib/strings/numbers.cc:424:8: warning: '%lld' directive output may be truncated writing between 1 and 19 bytes into a region of size between 7 and 8 [-Wformat-truncation=]
 string HumanReadableNumBytes(int64 num_bytes) {
        ^~~~~~~~~~~~~~~~~~~~~
tensorflow/core/lib/strings/numbers.cc:424:8: note: directive argument in the range [0, 9223372036854775807]
In file included from /usr/include/stdio.h:939:0,
                 from /usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/cstdio:42,
                 from /usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/ext/string_conversions.h:43,
                 from /usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/basic_string.h:6347,
                 from /usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/string:52,
                 from ./tensorflow/core/lib/strings/numbers.h:19,
                 from tensorflow/core/lib/strings/numbers.cc:15:
/usr/include/bits/stdio2.h:65:44: note: '__builtin_snprintf' output between 3 and 22 bytes into a destination of size 8
        __bos (__s), __fmt, __va_arg_pack ());
                                            ^
INFO: From Compiling external/nccl_archive/src/all_gather.cu.cc:
/usr/local/cuda-8.0/bin/../targets/x86_64-linux/include/math_functions.h(8897): error: cannot overload functions distinguished by return type alone

/usr/local/cuda-8.0/bin/../targets/x86_64-linux/include/math_functions.h(8901): error: cannot overload functions distinguished by return type alone

2 errors detected in the compilation of ""/tmp/tmpxft_00006246_00000000-7_all_gather.cu.cpp1.ii"".
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/nccl_archive/BUILD:33:1: output 'external/nccl_archive/_objs/nccl/external/nccl_archive/src/all_gather.cu.pic.o' was not created.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/nccl_archive/BUILD:33:1: not all outputs were created or valid.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 186.358s, Critical Path: 25.29s
```"
13928,'Numpy dangling symbolic links' when building from source,"### System information
- **Fedora 26 x64**:
- **TensorFlow installed from source**:
- **TensorFlow version commit 53e7541cf7efa61ba22c9f042e07031d87c8f145 (oct 23 11:46)**:
- **Python version 3.6**: 
- **Bazel version 0.5.4**:
- **CUDA 8.0**
- **cuDNN 8.0 v7**:
- **GPU model GTX 1060**:
- **bazel build -c opt --copt=-march=native --config=cuda //tensorflow/tools/pip_package:build_pip_package**:

```
You have bazel 0.5.4- (@non-git) installed.
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3
Found possible Python library paths:
  /usr/lib/python3.6/site-packages
  /usr/lib64/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3.6/site-packages]
Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: Y
jemalloc as malloc support will be enabled for TensorFlow.
Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.
Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n
No Hadoop File System support will be enabled for TensorFlow.
Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n
No Amazon S3 File System support will be enabled for TensorFlow.
Do you wish to build TensorFlow with XLA JIT support? [y/N]: n
No XLA JIT support will be enabled for TensorFlow.
Do you wish to build TensorFlow with GDR support? [y/N]: n
No GDR support will be enabled for TensorFlow.
Do you wish to build TensorFlow with VERBS support? [y/N]: n
No VERBS support will be enabled for TensorFlow.
Do you wish to build TensorFlow with OpenCL support? [y/N]: n
No OpenCL support will be enabled for TensorFlow.
Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 7
Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]
Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:   


Add ""--config=mkl"" to your bazel command to build with MKL support.
Please note that MKL on MacOS or windows is still not supported.
If you would like to use a local MKL instead of downloading, please set the environment variable ""TF_MKL_ROOT"" every time before build.
Configuration finished
 ~  progs  tensorflow  master  $  bazel build -c opt --copt=-march=native --config=cuda //tensorflow/tools/pip_package:build_pip_package
......................................................................................................................................................................................................................................................................................................................
WARNING: /home/torstein/progs/tensorflow/tensorflow/core/BUILD:1786:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/torstein/progs/tensorflow/tensorflow/tensorflow.bzl:1048:30.
WARNING: /home/torstein/progs/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/torstein/progs/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Found 1 target...
INFO: From Compiling external/snappy/snappy-c.cc:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
INFO: From Compiling external/snappy/snappy-sinksource.cc:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
INFO: From Compiling external/snappy/snappy-stubs-internal.cc:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
INFO: From Compiling external/snappy/snappy.cc:
cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++
external/snappy/snappy.cc: In member function 'void snappy::SnappySinkAllocator::Flush(size_t)':
external/snappy/snappy.cc:1403:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     for (int i = 0; i < blocks_.size(); ++i) {
                     ~~^~~~~~~~~~~~~~~~
In file included from external/snappy/snappy-internal.h:34:0,
                 from external/snappy/snappy.cc:30:
external/snappy/snappy.cc: In instantiation of 'bool snappy::SnappyScatteredWriter<Allocator>::AppendFromSelf(size_t, size_t) [with Allocator = snappy::SnappySinkAllocator; size_t = long unsigned int]':
external/snappy/snappy.cc:715:13:   required from 'void snappy::SnappyDecompressor::DecompressAllTags(Writer*) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>]'
external/snappy/snappy.cc:799:3:   required from 'bool snappy::InternalUncompressAllTags(snappy::SnappyDecompressor*, Writer*, snappy::uint32) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>; snappy::uint32 = unsigned int]'
external/snappy/snappy.cc:1460:78:   required from here
external/snappy/snappy.cc:1316:34: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (PREDICT_TRUE(offset - 1u < op_ptr_ - op_base_ && op_end <= op_limit_)) {
                      ~~~~~~~~~~~~^~~~~~~~~~~~~
external/snappy/snappy-stubs-internal.h:80:25: note: in definition of macro 'PREDICT_TRUE'
 #define PREDICT_TRUE(x) x
                         ^
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_3kcompat.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/ufunc_api.txt' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/arrayscalars.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/noprefix.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/utils.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/ufuncobject.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_endian.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/ndarrayobject.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_cpu.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_no_deprecated_api.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/multiarray_api.txt' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/_numpyconfig.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/old_defines.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/__ufunc_api.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/__multiarray_api.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_interrupt.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_1_7_deprecated_api.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/halffloat.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/_neighborhood_iterator_imp.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_common.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/ndarraytypes.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/numpyconfig.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_os.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/npy_math.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/arrayobject.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: declared output 'external/local_config_python/numpy_include/numpy/oldnumeric.h' is a dangling symbolic link.
ERROR: /home/torstein/.cache/bazel/_bazel_torstein/1f82ba256daa7468e9c0a1514e0b9aa5/external/local_config_python/BUILD:143:1: not all outputs were created or valid.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 58.925s, Critical Path: 20.53s
```"
13926,Build from source issue (CUDA 7.5),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4 RC0
- **Python version**: 3.4.3
- **Bazel version (if compiling from source)**: 0.7
- **CUDA/cuDNN version**: CUDA7.5 , cudnn v5.1
- **GPU model and memory**: GeForce GTX TITAN 
- **Exact command to reproduce**: `bazel build --config opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`

### Describe the problem
I am trying to build tensorflow 1.4 RC0 from source, getting compilation error that 'cusolverEigMode_t' has not been declared.

Looks like this is the commit where this code was added:
https://github.com/tensorflow/tensorflow/commit/e3413de529c3f762885efd62932f76445ed22653#diff-e4b1fa736000720d06dab76006540ec4R467

I tried grepping for `cusolverEigMode_t`in my `/usr/local/cuda/` but could not find any reference, is it possible that `cusolverEigMode_t` is not supported in CUDA 7.5? 
In that case, it should be noted that 1.4 is only supported for CUDA 8.0 and above

### Source code / logs
```
ERROR: /home/xxxxx/downloads/tensorflow/tensorflow/core/kernels/BUILD:839:1: C++ compilation of rule '//tensorflow/core/kernels:where_op' failed (Exit 1).
In file included from tensorflow/core/kernels/where_op.cc:42:0:
./tensorflow/core/kernels/cuda_solvers.h:299:16: error: 'cusolverEigMode_t' has not been declared
   Status Heevd(cusolverEigMode_t jobz, cublasFillMode_t uplo, int n,
                ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
"
13925,Examples of GANs using tensorflow estimator,"I found all the estimator examples in the tutorial assumes there is `x`(features) and `y`(labels).

However, in the context of GANs-based method, there is no `y` in the dataset. Would there be any example of GANs using tensorflow estimator?"
13919,tensorflow.python.framework.errors_impl.InvalidArgumentError,"I have run a tensorflow code, which always give following errors:
tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'div_1' has inputs from different frames. The input 'while/Mean_1' is in frame 'while/while/'. The input 'div_1/y' is in frame ''.

And I have searched for a long time, but find no proper solutions. I really wonder that anyone could give me some advice."
13918,build tensorflow for gpu faild ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:r1.3
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:0.7.0
- **CUDA/cuDNN version**:CUDA9.0/cuDNN7
- **GPU model and memory**:GTX 660M / 2G memory


### Describe the problem
I'm building tensorflow for gpu from source according to the official guide https://www.tensorflow.org/install/install_sources , but alway faild, error message show in below:
`ERROR: /home/dangerous/.cache/bazel/_bazel_dangerous/821f9ca421a3e885f021819a154f9a6e/external/nccl_archive/BUILD:33:1: output 'external/nccl_archive/_objs/nccl/external/nccl_archive/src/broadcast.cu.pic.o' was not created
ERROR: /home/dangerous/.cache/bazel/_bazel_dangerous/821f9ca421a3e885f021819a154f9a6e/external/nccl_archive/BUILD:33:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.`

"
13916,can't able to build android_tensorflow_inference_java file in tensorflow,"iam not able to android_tensorflow_inference_java.jar file in tensorflow using bazel.when i build i got the following error how can i solve this:

**$ bazel build //tensorflow/contrib/android:android_tensorflow_inference_java**

____Loading package: @local_config_xcode//
____Loading complete.  Analyzing...
____Loading package: tensorflow/java
____Loading package: @bazel_tools//third_party/java/jarjar
____Loading package: @bazel_tools//third_party/py/six
____Loading package: @bazel_tools//src/main/native/windows
____Loading package: @androidsdk//
ERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:64:1: Traceback (most recent call last):
        File ""C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel"", line 64
                create_system_images_filegroups(system_image_dirs = [""system-ima...""])
        File ""C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/bazel_tools/tools/android/android_sdk_repository_template.bzl"", line 298, in create_system_images_filegroups
                int(apidir.split(""-"")[1])
invalid literal for int() with base 10: ""N"".
ERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.
ERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows_msvc' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.
ERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows_msys' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.
ERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.
ERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows_msvc' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.
ERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows_msys' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.
ERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.
ERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows_msvc' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.
ERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows_msys' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.
ERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.
ERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows_msvc' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.
ERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:windows_msys' contains an error and its package is in error and referenced by '@androidsdk//:sdk-26'.
ERROR: C:/tools/msys64/tmp/_bazel_deemsys/28x4acs5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:sdk-26' contains an error and its package is in error and referenced by '@androidsdk//:sdk'.
ERROR: E:/arisai1/tensor/tensorflow/WORKSPACE:20:1: Target '@androidsdk//:sdk' contains an error and its package is in error and referenced by '//external:android/sdk'.
ERROR: Analysis of target '//tensorflow/contrib/android:android_tensorflow_inference_java' failed; build aborted.
____Elapsed time: 25.324s
"
13915,Error when install tf from source,"### Branch:
master

### Command:
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package \
--verbose_failures

### Error Info:
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/tianhz/project/tensorflow/tensorflow/tools/pip_package/BUILD:139:1 C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /home/tianhz/.cache/bazel/_bazel_FAREAST.tianhz/1aaf3c53d4483e0897f98d9f35329906/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-8.0 \
    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.0 \
    TF_CUDA_VERSION=8.0 \
    TF_CUDNN_VERSION=5.1.10 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++11' '-march=native' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_dnn.pic.d '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_dnn.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DTF_USE_SNAPPY -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/nsync -iquote bazel-out/local_linux-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jemalloc -iquote bazel-out/local_linux-opt/genfiles/external/jemalloc -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local_linux-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local_linux-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local_linux-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/local_linux-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_linux-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_linux-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/local_linux-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/local_linux-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/local_linux-opt/genfiles/external/local_config_cuda -isystem external/nsync/public -isystem bazel-out/local_linux-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jemalloc/include -isystem bazel-out/local_linux-opt/genfiles/external/jemalloc/include -isystem external/eigen_archive -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local_linux-opt/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/local_linux-opt/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/local_linux-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local_linux-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_linux-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda/cuda/include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c tensorflow/stream_executor/cuda/cuda_dnn.cc -o bazel-out/local_linux-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_dnn.pic.o)^M
INFO: Elapsed time: 654.834s, Critical Path: 213.07s^M
FAILED: Build did NOT complete successfully^M

## Other info:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master(r1.4)
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.7.0
- **CUDA/cuDNN version**:CUDA 8.0, cnDNN 5.1.10"
13914,s390x Tensorflow CI build failure ,"Hello, 
Tensorflow CI build fails with an error: 
```
java.lang.NoClassDefFoundError: Could not initialize class jenkins.model.Jenkins$MasterComputer
	at org.jenkinsci.plugins.gitclient.AbstractGitAPIImpl.withRepository(AbstractGitAPIImpl.java:29)
	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.withRepository(CliGitAPIImpl.java:71)
	at sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)

```"
13912,tensorflow is not importing even after successful installation ,"in Centos 6.9 i am unable to import tensorflow-1.3.0 in any anaconda (2,3) and getting this error please resolve this issue ..




![screenshot-7](https://user-images.githubusercontent.com/24870531/31872238-d008d862-b7d7-11e7-8f4a-0f2ad9b159a1.png)
![screenshot-6](https://user-images.githubusercontent.com/24870531/31872240-d04142c4-b7d7-11e7-8cab-9bd5b2f931c7.png)
"
13910,Feature Request: Getting a collection of variable from specific Graph,"Hi,

For debug purposes, I need to create a collection variables from a specific graph.
I have defined my model in a predefined graph using something like:

pg=tf.Graph()
with pg.as_default():
    ...

I define my session as:

sess = tf.Session(config=config, graph=pg)
If I need to create a collection of trainable variable in graph pg, I try to use:
tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) or simply tf.trainable_variables(), It doesn't return anything .

If I do the same experiment, using the default 'default graph', and without using 'with', constructs, I am able to get the collection of variables.

I suspect, tf.get_variables is not looking in pg, I believe a method for extracting collection of variables from a given graph, or device would be useful for everybody.

If the feature already exist, please accept my apology for wasting your precious time.
I did check on stack overflow and other avenues suggested by Google.


-Regards"
13906,Conditional input to a sequence to sequence model (word+character hybrid network),"I need to create a sequence to sequence model where in the encoder and decoder are both LSTM networks but the encoder takes the inputs from either of the following cases

1.Normal vector representation of a word (Embedding vector) - when the word input is present in the vocabulary

2.Output of another LSTM network - when the word is out of vocabulary and a separate character based LSTM is used to generate an embedding on the fly

Consider the following example sentence:
""The brown fox jumped over the lazy dog""

Assume these are the words present in the vocabulary: The, brown, jumped, over, dog - These words are fed to the seq2seq encoder as such

out of vocabulary(OOV) words are: fox, lazy - These words are passed to a character LSTM and the output of the same is passed to the seq2seq model along with the above words

These both word level and character level encoder needs to be trained end to end simultaneously. How do we model the input layer of the seq2seq model to achieve this scenario?

Reference paper:
http://aclweb.org/anthology/P/P16/P16-1100.pdf
"
13903,Feature request: make `smart_cond` public API,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.3.0-24-g658866597
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: None

### Describe the problem

Currently `tf.cond` does not work if predicate is a Python boolean. As a result, people frequently have to write conditional statements twice, one with `if` statement, and one with `tf.cond` call. There is a `smart_cond` in `tensorflow/python/layers/utils.py`, but it is not in the public API or searchable in documentation. Petition to make it public or just integrate the smartness in `tf.cond` altogether. It is not a big change and will not impact backwards compatibility."
13902,Op type not registered 'GatherTree' in binary running on..,"Hi,

I'm trying to use the GTT (Graph Transform Tool) on NMT model (From the NMT sample project),
https://github.com/tensorflow/nmt

I get the following error :
bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=/tmp/NMT_frozenGraphPB.pb --out_graph=Quant_NMT.pb --inputs='' --outputs='attention' --transforms='add_default_attributes quantize weights quantize_nodes'

""Op type not registered 'GatherTree' in binary running on .... Make sure the Op and Kernel are registered in the binary running in this process.""

Any idea how to solve this issue?

Thx
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No code change , using GTT tool example
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
From Source
- **TensorFlow version (use command below)**:
Latest
- **Python version**: 
3.5 , 3.6 and 2.7
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
None
- **GPU model and memory**:
Tried in several systems and installations 
- **Exact command to reproduce**:
bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=/tmp/NMT_frozenGraphPB.pb --out_graph=Quant_NMT.pb --inputs='' --outputs='attention' --transforms='add_default_attributes quantize weights quantize_nodes'
"
13900,tfdbg doesn't work with tensorflow GPU version?,"Could the dear developers confirm that tfdbg works with tensorflow GPU version or not?
At least on my platform it doesn't work.
OS: Red Hat Enterprise Linux Workstation release 7.3 (Maipo)
Python: Python 2.7.5
tensorflow (1.4.0rc0) (compiled from source with CUDA enabled, CUDA 8.0.61, cuDNN8-7.0.1)
nVidia Quadro P5000

the code for reproducing the problem is as follows:

```
import numpy as np
import tensorflow as tf

from tensorflow.python import debug as tf_debug

t = tf.constant(np.ones((10, 16)), tf.float32)

def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev = 0.1)
    return tf.Variable(initial)

def bias_variable(shape):
    initial = tf.constant(0., shape = shape)
    return tf.Variable(initial)

lin1_output_size = 32
lin2_output_size = 1

with tf.name_scope(""lin1""):
    weight_shape = t.get_shape().as_list()[1:] + [lin1_output_size]
    lin1_output = tf.matmul(t, weight_variable(weight_shape)) + bias_variable([lin1_output_size])

with tf.name_scope(""lin2""):
    weight_shape = [lin1_output_size, lin2_output_size]
    lin2_output = tf.matmul(lin1_output, weight_variable(weight_shape)) + bias_variable([lin2_output_size])

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    sess = tf_debug.LocalCLIDebugWrapperSession(sess)

    print(sess.run(lin2_output))

``` 

The phenomenon is after invoking stepper and steps for several steps, it is reporting segmentation fault and exit.

I just wonder is it due to GPU issue?"
13897,No OpKernel was registered to support Op 'SegmentSum',"### ENV
* OSX 10.12.6
* No GPU
* Tensorflow master. Built from source by `tensorflow/contrib/makefile/build_all_linux.sh`

### Error info
```
Invalid argument: No OpKernel was registered to support Op 'SegmentSum' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[Node: emb_179/embedding_lookup_sparse = SegmentSum[T=DT_FLOAT, Tindices=DT_INT32](emb_179/embedding_lookup_sparse/mul, emb_179/embedding_lookup_sparse/Cast)]]
``` 

### My usage
I use python to train model and freeze the graph with checkpoint. Then I use c++ to load freeze graph protobuf and to predict for some inputs.
#### python core part
```
with tf.variable_scope('input/sparse_field'):
  with tf.variable_scope('index'):
    sparse_index = tf.placeholder(tf.int64)
  with tf.variable_scope('id'):
   sparse_id = tf.placeholder(tf.int64)
   with tf.variable_scope('value'):
    sparse_val = tf.placeholder(tf.float32)
  with tf.variable_scope('shape'):
    sparse_shape = tf.placeholder(tf.int64)
with tf.variable_scope('label'):
  label = tf.placeholder(tf.float32)
sparse_ids = tf.SparseTensor(sparse_index, sparse_id, sparse_shape)
sparse_vals = tf.SparseTensor(sparse_index, sparse_val, sparse_shape)

input_size = 100
embedding_size = 50
with tf.variable_scope(""emb_179""):
  embedding_variable = tf.Variable(tf.truncated_normal([input_size, embedding_size], stddev=0.05), name='emb' + str(field_id))
  embedding = tf.nn.embedding_lookup_sparse(embedding_variable, sparse_ids, sparse_vals, ""mod"", combiner=""sum"")
...
``` 
#### C++ prediction core part
```
auto id_indice_tensor =
test::AsTensor<int64>(indice, {static_cast<int64>(indice.size()/2), 2});
inputs.push_back(std::pair<std::string, Tensor>(""input/sparse_field/index/Placeholder"", id_indice_tensor));
auto id_list_tensor = test::AsTensor<int64>(fid_list);
inputs.push_back(std::pair<std::string, Tensor>(""input/sparse_field/id/Placeholder"", id_list_tensor));
auto val_list_tensor = test::AsTensor<float>(fval_list);
inputs.push_back(std::pair<std::string, Tensor>(""input/sparse_field/value/Placeholder"", val_list_tensor));

std::vector<tensorflow::Tensor> outputs;
Status status = session->Run(inputs, {""predict/add""}, {}, &outputs);
```

### Have checked
I have checked it has registered all real type and complex type in cpu mode. 
https://github.com/tensorflow/tensorflow/blob/d1183ca6a245cd0b498c46fd1079909ebc4abc3a/tensorflow/core/kernels/segment_reduction_ops.cc#L333
```
TF_CALL_REAL_NUMBER_TYPES(REGISTER_REAL_CPU_KERNELS_ALL);
REGISTER_COMPLEX_CPU_KERNELS_ALL(complex64);
REGISTER_COMPLEX_CPU_KERNELS_ALL(complex128);
```"
13896,"Build Error - Unable to clone jsoncpp repo, all others seem to be working","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**:  Source from master branch
- **TensorFlow version (use command below)**:
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**:  cmake 3.9.4
- **CUDA/cuDNN version**: CUDA 9.0/cuDNN 7
- **GPU model and memory**:  GTX 1080
- **Exact command to reproduce**:  `MSBuild /filelogger /m:4 /p:Configuration=Release tf_python_build_pip_package.vcxproj`

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I have been trying to build the TensorFlow pip package.  For some reason, while I have verified that the build is able to clone the other git repos, the `jsoncpp` one seems to be killing my build.

### Source code / logs
`    42>CustomBuild:
         Creating directories for 'jsoncpp'`
`    42>CustomBuild:
         Performing download step (git clone) for 'jsoncpp'`
`    42>CustomBuild:
         fatal: could not create work tree dir 'jsoncpp': Permission denied`
`    42>CustomBuild:
         fatal: could not create work tree dir 'jsoncpp': Permission denied`
`    42>CustomBuild:
         fatal: could not create work tree dir 'jsoncpp': Permission denied`
`    42>CustomBuild:
         -- Had to git clone more than once:
                   3 times.`
`    42>CustomBuild:
         CMake Error at C:/Users/Bryce/Documents/Programming/os_clones/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/tmp/jsoncpp-gitclone.cmake:66 (message):
           Failed to clone repository:
           'https://github.com/open-source-parsers/jsoncpp.git'`

And later on,

`C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\VC\VCTargets\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd.exe"" exited with code 1. [C:\Users\Bryce\Documents\Programming\os_clones\tensorflow\tensorflow\contrib\cmake\build\jsoncpp.vcxproj]`

[msbuild.log](https://github.com/tensorflow/tensorflow/files/1404600/msbuild.log)"
13895,"In the estimator of Tensorflow, how does it work when model_fn is called multiple times?","    def model_fn(features, labels, mode, params):
      """"""Model function for Estimator.""""""
    
      # Connect the first hidden layer to input layer
      # (features[""x""]) with relu activation
      first_hidden_layer = tf.layers.dense(features[""x""], 10, activation=tf.nn.relu)
    
      # Connect the second hidden layer to first hidden layer with relu
      second_hidden_layer = tf.layers.dense(
          first_hidden_layer, 10, activation=tf.nn.relu)
    
      # Connect the output layer to second hidden layer (no activation fn)
      output_layer = tf.layers.dense(second_hidden_layer, 1)
    
      # Reshape output layer to 1-dim Tensor to return predictions
      predictions = tf.reshape(output_layer, [-1])
    
      # Provide an estimator spec for `ModeKeys.PREDICT`.
      if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(
            mode=mode,
            predictions={""ages"": predictions})
    
      # Calculate loss using mean squared error
      loss = tf.losses.mean_squared_error(labels, predictions)
    
      # Calculate root mean squared error as additional eval metric
      eval_metric_ops = {
          ""rmse"": tf.metrics.root_mean_squared_error(
              tf.cast(labels, tf.float64), predictions)
      }
    
      optimizer = tf.train.GradientDescentOptimizer(
          learning_rate=params[""learning_rate""])
      train_op = optimizer.minimize(
          loss=loss, global_step=tf.train.get_global_step())
    
      # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.
      return tf.estimator.EstimatorSpec(
          mode=mode,
          loss=loss,
          train_op=train_op,
          eval_metric_ops=eval_metric_ops)

Above is an example of the model_fn used by Tensorflow's [Estimator][1].

As mentioned in the tutorial, this model_fn could be called in different context (train, predict, evaluate). However, I'm a bit confused, because each time the model_fn is called, **instead of reusing existing graph, it seems to create a new graph.(or create new node in the graph)**

For example, firstly I called model_fn under TRAIN mode, then I called model_fn with PREDICT mode. How can I make sure the PREDICT one is reusing the weight of the trained values?

  [1]: https://www.tensorflow.org/extend/estimators"
13892,Inconsistent Result of SyncReplicaOptimizer,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac Sierra 10.12.6
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: Python 3.5.2 |Anaconda custom (x86_64)
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: Not used/
- **GPU model and memory**: Not used/
- **Exact command to reproduce**: python synchronous_sgd.py (see below)

### Describe the problem
Training a trivial model of 2-layer fully connected MNIST, with one parameter server thread and one worker thread to reproduce this issue.

The file is linked here. We run `python synchronized_sgd.py` and `python async_sgd.py` one after one **in the same terminal so that they receive same random results** to recreate the bug.

The only difference in the two files below is: async comment out 10 trivial lines from sync. (Please diff)

https://github.com/heyucongtom/PGRD/blob/master/synchronized_sgd.py
https://github.com/heyucongtom/PGRD/blob/master/async_sgd.py

I make sure both trainer receive the exactly same data for each batch, and I also fixed the random seed. As a results, both model shall get exactly the same output. However, they don't.

The problem is, after the first step, the two models are in sync. At exactly the **second run of the train_op**, this train_op of the sync replica doesn't update the model, nor does it update the global step, resulting in output:

```
Worker 0: training step 0 done (global step: 0)
On trainer 0, iteration 0 ps it reaches 0.078900 accuracy
Worker 0: training step 1 done (global step: 1)
On trainer 0, iteration 1 ps it reaches 0.319800 accuracy
Worker 0: training step 2 done (global step: 1)
On trainer 0, iteration 1 ps it reaches 0.319800 accuracy
Worker 0: training step 3 done (global step: 2)
On trainer 0, iteration 2 ps it reaches 0.455800 accuracy
Worker 0: training step 4 done (global step: 3)
On trainer 0, iteration 3 ps it reaches 0.477400 accuracy
Worker 0: training step 5 done (global step: 4)
On trainer 0, iteration 4 ps it reaches 0.478100 accuracy
```


**As a comparison, let's take the output of the simple async version. With exactly**

```
Worker 0: training step 0 done (global step: 0)
On trainer 0, iteration 0 ps it reaches 0.078900 accuracy
Worker 0: training step 1 done (global step: 1)
On trainer 0, iteration 1 ps it reaches 0.319800 accuracy <After the first training step, the accuracy is the same, which is expected.>
Worker 0: training step 2 done (global step: 2)
On trainer 0, iteration 2 ps it reaches 0.279100 accuracy <Something different happening.>
Worker 0: training step 3 done (global step: 3)
On trainer 0, iteration 3 ps it reaches 0.427200 accuracy
Worker 0: training step 4 done (global step: 4)
On trainer 0, iteration 4 ps it reaches 0.567100 accuracy
Worker 0: training step 5 done (global step: 5)
On trainer 0, iteration 5 ps it reaches 0.617500 accuracy
Worker 0: training step 6 done (global step: 6)
On trainer 0, iteration 6 ps it reaches 0.561400 accuracy
```

I read through the source code of SyncReplicaOptimizer and find out that the train_op returned by that optimizer is a Assign operation, which could be only executed after the grads were applied and global steps enqueued. So sync and async with only one process should be exactly the same.

This behavior is mysterious to me now. Not sure if I got anything wrong.
"
13890,tf.image.crop_and_resize() return 0 values when assigned to GPU on  the Jetson TX2 ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04.LTS
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.5.2
- **CUDA/cuDNN version**: 8.0/6.0.21
- **GPU model and memory**: Nvidia Tegra X2
- **Exact command to reproduce**:`tf.image.crop_and_resize(raw_sample, boxes, box_ind)`

### Describe the problem
I'm getting completly different results from tensorflow's function `tf.image.crop_and_resize(...)` when assigned it to gpu and cpu.
In other words:
  -when I run this ops on CPU, I get correct results( I mean, the  right crops)
  -when I put it on the GPU device I get crops fulled with 0 values.

### Source code / logs
Here, you can see a simple use case:
```
import tensorflow as tf 
import numpy as np
import cv2 #Just importing cv2 to read  image, you use PIL or anything else to load it

device='gpu' 

def img2batch_crops(input_image):
    raw_sample_tensor_4d=tf.expand_dims(input_image, 0)
    
    #Setting the size to crop and the final size of cropped images
    patches_top=[0,0.5]
    patches_bottom =[0.5,0.5]
    crop_size = [100,100]
    boxes=tf.stack([patches_top, patches_top, patches_bottom, patches_bottom], axis=1)
    
    ##Here is the bug:
        #When device == 'cpu', I got  results 
        #When device == 'gpu', I got  black cropped images( 0 values)
    with tf.device('/'+device+':0'):  
        crops=tf.image.crop_and_resize(raw_sample_tensor_4d, boxes, box_ind=tf.zeros_like(patches_top, dtype=tf.int32), crop_size=crop_size, name=""croper"")

    return crops


def main():

	img_data = cv2.imread('image.jpg') #Just loading the image,

	print(""Shape and type of image input "",img_data.shape, img_data.dtype) #Print the shape and the type of the image, supposed to be a numpy array

	raw_image = tf.placeholder(dtype=tf.float32, shape=img_data.shape, name='input_image')
     
       crops = img2batch_crops(raw_image) # Adding ops to the graph

	with tf.Session() as sess:
	    myBatchedImages = sess.run(crops, feed_dict={raw_image:img_data})
	    cv2.imwrite('result_'+device+'.jpg',myBatchedImages[0])   ## Savej just one cropped image to see how it looks like

main()
```
"
13889,tf.image_crop_and_resize return 0 values when using GPU on Jetson TX2,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13888,InvalidArgumentError: No OpKernel was registered to support Op 'Resampler' with these attrs. ,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: Python 3.6.2 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 8/ CUDNN 6
- **GPU model and memory**: GeForce 940MX
- **Exact command to reproduce**: tf.contrib.resampler.resampler(inp,warp)

### Describe the problem
No registered kernels for the resampler operation. The code is as follows
`import tensorflow as tf`
`inp = tf.ones([1,4,4,3],dtype=tf.float32)`
`warp = tf.zeros([1,4,4,2],dtype=tf.float32)`
`out = tf.contrib.resampler.resampler(inp,warp)`
`print(out)`
`sess = tf.Session()`
`print(sess.run(out))`

### Source code / logs
I get the following output and error, traceback

Tensor(""resampler_1/Resampler:0"", shape=(1, 4, 4, 3), dtype=float32)

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1326     try:
-> 1327       return fn(*args)
   1328     except errors.OpError as e:

F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1296       # Ensure any changes to the graph are reflected in the runtime.
-> 1297       self._extend_graph()
   1298       with errors.raise_exception_on_not_ok_status() as status:

F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py in _extend_graph(self)
   1357           tf_session.TF_ExtendGraph(
-> 1358               self._session, graph_def.SerializeToString(), status)
   1359         self._opened = True

F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\contextlib.py in __exit__(self, type, value, traceback)
     87             try:
---> 88                 next(self.gen)
     89             except StopIteration:

F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\framework\errors_impl.py in raise_exception_on_not_ok_status()
    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 466           pywrap_tensorflow.TF_GetCode(status))
    467   finally:

InvalidArgumentError: No OpKernel was registered to support Op 'Resampler' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
 &lt;no registered kernels&gt;

	 [[Node: resampler_1/Resampler = Resampler[T=DT_FLOAT](ones_2, zeros_2)]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-3-0ff9594126bf> in <module>()
      5 print(out)
      6 sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
----> 7 print(sess.run(out))

F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    893     try:
    894       result = self._run(None, fetches, feed_dict, options_ptr,
--> 895                          run_metadata_ptr)
    896       if run_metadata:
    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1122     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1123       results = self._do_run(handle, final_targets, final_fetches,
-> 1124                              feed_dict_tensor, options, run_metadata)
   1125     else:
   1126       results = []

F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1319     if handle is None:
   1320       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1321                            options, run_metadata)
   1322     else:
   1323       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1338         except KeyError:
   1339           pass
-> 1340       raise type(e)(node_def, op, message)
   1341 
   1342   def _extend_graph(self):

InvalidArgumentError: No OpKernel was registered to support Op 'Resampler' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
 &lt;no registered kernels&gt;

	 [[Node: resampler_1/Resampler = Resampler[T=DT_FLOAT](ones_2, zeros_2)]]

Caused by op 'resampler_1/Resampler', defined at:
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\traitlets\config\application.py"", line 658, in launch_instance
    app.start()
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\ipykernel\kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\zmq\eventloop\ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\tornado\ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\zmq\eventloop\zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\zmq\eventloop\zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\zmq\eventloop\zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\ipykernel\kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\ipykernel\kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\ipykernel\kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\ipykernel\ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\ipykernel\zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\IPython\core\interactiveshell.py"", line 2698, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\IPython\core\interactiveshell.py"", line 2802, in run_ast_nodes
    if self.run_code(code, result):
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\IPython\core\interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-3-0ff9594126bf>"", line 4, in <module>
    out = tf.contrib.resampler.resampler(inp,warp)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\contrib\resampler\python\ops\resampler_ops.py"", line 59, in resampler
    return gen_resampler_ops.resampler(data_tensor, warp_tensor)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\contrib\resampler\ops\gen_resampler_ops.py"", line 28, in resampler
    result = _op_def_lib.apply_op(""Resampler"", data=data, warp=warp, name=name)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""F:\Sharath\Anaconda\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'Resampler' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
  &lt;no registered kernels&gt;

	 [[Node: resampler_1/Resampler = Resampler[T=DT_FLOAT](ones_2, zeros_2)]]
"
13887,"ValueError raised when using AdamOptimizer, but not for GradientDescentOptimizer","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12.1
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.2.0-rc2-21-g12f033d 1.2.0
- **Python version**: 3.5.0

### Source code / logs

Taken from this issue: https://github.com/tensorflow/tensorflow/issues/6220#issuecomment-314688444, the below code reproduces the error.

```
import tensorflow as tf
import numpy as np
class SimpleModel:
    def __init__(self):
        self.loss = self.calc_loss()
        self.train = self.train_model(self.loss)
    def calc_loss(self):
        W = tf.get_variable(""w"", [1])
        b = tf.Variable(tf.zeros([1]))
        y = W * x_data + b
        return tf.reduce_mean(tf.square(y - y_data))
    def train_model(self, loss):
        return tf.train.AdamOptimizer(0.5).minimize(loss)
        #return tf.train.GradientDescentOptimizer(0.5)
x_data = np.random.rand(100).astype(np.float32)
y_data = x_data * 0.1 + 0.3
s1 = SimpleModel()
tf.get_variable_scope().reuse_variables()
s2 = SimpleModel()
```

Running this gives the error `ValueError: Variable w/Adam_2/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?`, but when `train_model` is changed to return the `GradientDescentOptimizer`, the code compiles correctly.

I've tried following all the advice in the issue that relates to this (https://github.com/tensorflow/tensorflow/issues/6220), but haven't had any luck. "
13885,tf.reduce_mean is not compatible with np.mean,"[tf.reduce_mean](https://www.tensorflow.org/api_docs/python/tf/reduce_mean) emphasized that this function is compatible with numpy:

> Equivalent to np.mean

But it doesn't in the output type. Consider the following code for example:

```
import tensorflow as tf
x = tf.Variable([1, 0, 1, 0])
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
print(sess.run(tf.reduce_mean(x)))

```

The output is zero. It seems that tf.reduce_mean infer the output type from the input tensor because casting the input tensor to float values, solve the problem. This attribute is not compatible to np.mean:

```
import numpy as np
print(np.mean([1,0,0,1]))
```


### System information
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.3
- **Python version**: 3.6
"
13883,Why do custom read op only works on test_session,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
NO

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
pip install tensorflow

- **TensorFlow version (use command below)**:
('v1.3.0-rc2-20-g0787eee', '1.3.0')

- **Python version**: 
Python 2.7.12 from anaconda

- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
I'm using a cpu only tensorflow.

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I wrote an custom kernel op in tensorflow for reading csv format data.

It works just fine in the TestCase with the sess object return by test_session() function.

When I turn to normal codes, the reader op returns the same result every time. Then I put some debug printing at the beginning of the MyOp:Compute function. It seems like after the first run, the sess.run(myop) never calls the MyOp:Compute function at all.

Then I return to my test cases, if I replace the session object with an tf.Session() instead of self.test_session(), it failed the same way.

### Source code / logs
to share more details, here's my mini demo codes: https://github.com/littleDing/mini_csv_reader

major codes in test cases
```
def testSimple(self):
  input_data_schema, feas, batch_size = self.get_simple_format()
  iter_op = ops.csv_iter('./sample_data.txt', input_data_schema, feas, batch_size=batch_size, label='label2')
  with self.test_session() as sess:
    label,sign = sess.run(iter_op)
    print label

    self.assertAllEqual(label.shape, [batch_size])
    self.assertAllEqual(sign.shape, [batch_size, len(feas)])
    self.assertAllEqual(sum(label), 2)
    self.assertAllEqual(sign[0,:], [7,0,4,1,1,1,5,9,8])

    label,sign = sess.run(iter_op)
    self.assertAllEqual(label.shape, [batch_size])
    self.assertAllEqual(sign.shape, [batch_size, len(feas)])
    self.assertAllEqual(sum(label), 1)
    self.assertAllEqual(sign[0,:], [9,9,3,1,1,1,5,4,8])
```
major codes in normal session call:
```
def testing_tf():
    path = './sample_data.txt'
    input_data_schema, feas, batch_size = get_simple_format()
    with tf.device('/cpu:0'):
        n_data_op = tf.placeholder(dtype=tf.float32)
        iter_op = ops.csv_iter(path, input_data_schema, feas, batch_size=batch_size, label='label2') 
        init_op = [tf.global_variables_initializer(), tf.local_variables_initializer() ]

    with tf.Session() as sess:
      sess.run(init_op)
      n_data = 0
      for batch_idx in range(3):
        print '>>>>>>>>>>>>>> before run batch', batch_idx
        ## it should be some debug printing here, but nothing come out when batch_idx>0
        label,sign = sess.run(iter_op)
        print '>>>>>>>>>>>>>> after run batch', batch_idx
        ## the content of sign remain the same every time
        print sign
        if len(label) == 0:
          break
```

"
13880,RMSProp fails with InteractiveSession() and embed_sequence on GPU,"Note:
This works fine if any of the following
- without GPU
- if use ""normal"" Session() instead of the interactive one.
- if don't do `embed_sequence`
- user other optimiser, not RMSProp

This is the snippet to get the error:

```
import tensorflow as tf
from tensorflow.contrib import layers, framework
import numpy as np

sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True))
input = tf.placeholder(tf.int64, shape=[None])
optimiser = tf.train.RMSPropOptimizer(0.1)
x = layers.embed_sequence(input, vocab_size=20, embed_dim=5)
loss = tf.reduce_sum(x)
train_op = optimiser.minimize(loss)
sess.run(tf.global_variables_initializer())
sess.run(train_op, feed_dict={input: np.random.randint(10, size=5)})
```

Gives

```
2017-10-21 15:12:38.503957: E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref
   for attr 'tensor_type'
  ; NodeDef: EmbedSequence/embeddings/RMSProp_1/_19 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_36_EmbedSequence/embeddings/RMSProp_1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^RMSProp/learning_rate, ^RMSProp/decay, ^RMSProp/momentum, ^RMSProp/epsilon, ^RMSProp/update_EmbedSequence/embeddings/UnsortedSegmentSum, ^RMSProp/update_EmbedSequence/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
2017-10-21 15:12:38.504081: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref
   for attr 'tensor_type'
  ; NodeDef: EmbedSequence/embeddings/RMSProp_1/_19 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_36_EmbedSequence/embeddings/RMSProp_1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^RMSProp/learning_rate, ^RMSProp/decay, ^RMSProp/momentum, ^RMSProp/epsilon, ^RMSProp/update_EmbedSequence/embeddings/UnsortedSegmentSum, ^RMSProp/update_EmbedSequence/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
   [[Node: EmbedSequence/embeddings/RMSProp_1/_19 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_36_EmbedSequence/embeddings/RMSProp_1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^RMSProp/learning_rate, ^RMSProp/decay, ^RMSProp/momentum, ^RMSProp/epsilon, ^RMSProp/update_EmbedSequence/embeddings/UnsortedSegmentSum, ^RMSProp/update_EmbedSequence/embeddings/Unique)]]
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1306, in _run_fn
    status, run_metadata)
  File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: AttrValue must not have reference type value of float_ref
   for attr 'tensor_type'
  ; NodeDef: EmbedSequence/embeddings/RMSProp_1/_19 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_36_EmbedSequence/embeddings/RMSProp_1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^RMSProp/learning_rate, ^RMSProp/decay, ^RMSProp/momentum, ^RMSProp/epsilon, ^RMSProp/update_EmbedSequence/embeddings/UnsortedSegmentSum, ^RMSProp/update_EmbedSequence/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
   [[Node: EmbedSequence/embeddings/RMSProp_1/_19 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_36_EmbedSequence/embeddings/RMSProp_1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^RMSProp/learning_rate, ^RMSProp/decay, ^RMSProp/momentum, ^RMSProp/epsilon, ^RMSProp/update_EmbedSequence/embeddings/UnsortedSegmentSum, ^RMSProp/update_EmbedSequence/embeddings/Unique)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""rmsprop_fail.py"", line 19, in <module>
    sess.run(train_op, feed_dict={input: np.random.randint(10, size=5)})
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: AttrValue must not have reference type value of float_ref
   for attr 'tensor_type'
  ; NodeDef: EmbedSequence/embeddings/RMSProp_1/_19 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_36_EmbedSequence/embeddings/RMSProp_1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^RMSProp/learning_rate, ^RMSProp/decay, ^RMSProp/momentum, ^RMSProp/epsilon, ^RMSProp/update_EmbedSequence/embeddings/UnsortedSegmentSum, ^RMSProp/update_EmbedSequence/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
   [[Node: EmbedSequence/embeddings/RMSProp_1/_19 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_36_EmbedSequence/embeddings/RMSProp_1"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^RMSProp/learning_rate, ^RMSProp/decay, ^RMSProp/momentum, ^RMSProp/epsilon, ^RMSProp/update_EmbedSequence/embeddings/UnsortedSegmentSum, ^RMSProp/update_EmbedSequence/embeddings/Unique)]]
```

If remove the `allow_soft_placement` then get:

```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1306, in _run_fn
    status, run_metadata)
  File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'gradients/EmbedSequence/embedding_lookup_grad/ToInt32': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'
Colocation Debug Info:
Colocation group had the following types and devices:
SparseApplyRMSProp: CPU
UnsortedSegmentSum: GPU CPU
Gather: GPU CPU
StridedSlice: GPU CPU
Unique: GPU CPU
Shape: GPU CPU
Cast: GPU CPU
Identity: GPU CPU
VariableV2: GPU CPU
Const: GPU CPU
	 [[Node: gradients/EmbedSequence/embedding_lookup_grad/ToInt32 = Cast[DstT=DT_INT32, SrcT=DT_INT64, _class=[""loc:@EmbedSequence/embeddings""]](gradients/EmbedSequence/embedding_lookup_grad/Shape)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""rmsprop_fail.py"", line 19, in <module>
    sess.run(train_op, feed_dict={input: np.random.randint(10, size=5)})
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'gradients/EmbedSequence/embedding_lookup_grad/ToInt32': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'
Colocation Debug Info:
Colocation group had the following types and devices:
SparseApplyRMSProp: CPU
UnsortedSegmentSum: GPU CPU
Gather: GPU CPU
StridedSlice: GPU CPU
Unique: GPU CPU
Shape: GPU CPU
Cast: GPU CPU
Identity: GPU CPU
VariableV2: GPU CPU
Const: GPU CPU
	 [[Node: gradients/EmbedSequence/embedding_lookup_grad/ToInt32 = Cast[DstT=DT_INT32, SrcT=DT_INT64, _class=[""loc:@EmbedSequence/embeddings""]](gradients/EmbedSequence/embedding_lookup_grad/Shape)]]

Caused by op 'gradients/EmbedSequence/embedding_lookup_grad/ToInt32', defined at:
  File ""rmsprop_fail.py"", line 15, in <module>
    train_op = optimiser.minimize(loss)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py"", line 315, in minimize
    grad_loss=grad_loss)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py"", line 386, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 542, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 348, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 542, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_grad.py"", line 365, in _GatherGrad
    params_shape = math_ops.to_int32(params_shape)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py"", line 797, in to_int32
    return cast(x, dtypes.int32, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py"", line 716, in cast
    return gen_math_ops.cast(x, base_type, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 450, in cast
    result = _op_def_lib.apply_op(""Cast"", x=x, DstT=DstT, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'EmbedSequence/embedding_lookup', defined at:
  File ""rmsprop_fail.py"", line 11, in <module>
    x = layers.embed_sequence(input, vocab_size=20, embed_dim=5)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/encoders.py"", line 142, in embed_sequence
    return embedding_ops.embedding_lookup(embeddings, ids)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/embedding_ops.py"", line 294, in embedding_lookup
    transform_fn=None)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/embedding_ops.py"", line 123, in _embedding_lookup_and_transform
    result = _gather_and_clip(params[0], ids, max_norm, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/embedding_ops.py"", line 57, in _gather_and_clip
    embs = array_ops.gather(params, ids, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py"", line 2409, in gather
    validate_indices=validate_indices, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 1219, in gather
    validate_indices=validate_indices, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'gradients/EmbedSequence/embedding_lookup_grad/ToInt32': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'
Colocation Debug Info:
Colocation group had the following types and devices:
SparseApplyRMSProp: CPU
UnsortedSegmentSum: GPU CPU
Gather: GPU CPU
StridedSlice: GPU CPU
Unique: GPU CPU
Shape: GPU CPU
Cast: GPU CPU
Identity: GPU CPU
VariableV2: GPU CPU
Const: GPU CPU
	 [[Node: gradients/EmbedSequence/embedding_lookup_grad/ToInt32 = Cast[DstT=DT_INT32, SrcT=DT_INT64, _class=[""loc:@EmbedSequence/embeddings""]](gradients/EmbedSequence/embedding_lookup_grad/Shape)]]
```

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS 
- **TensorFlow installed from (source or binary)**: docker
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.5
- **CUDA/cuDNN version**:
Cuda compilation tools, release 8.0, V8.0.61
- **GPU model and memory**:
Tesla K80

"
13879,"Variable rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel already exists, disallowed.","Hello,
I wrote this code:

def rnn_inputs(FLAGS, input_data):

    with tf.variable_scope('rnn_inputs', reuse=True):
        W_input = tf.get_variable(""W_input"",
            [FLAGS.en_vocab_size, FLAGS.num_hidden_units])

    # <num_examples, seq_len, num_hidden_units>
    embeddings = tf.nn.embedding_lookup(W_input, input_data)

    return embeddings

def rnn_softmax(FLAGS, outputs):
    with tf.variable_scope('rnn_softmax', reuse=True):
        W_softmax = tf.get_variable(""W_softmax"",
            [FLAGS.num_hidden_units, FLAGS.num_classes])
        b_softmax = tf.get_variable(""b_softmax"", [FLAGS.num_classes])

    logits = tf.matmul(outputs, W_softmax) + b_softmax

    return logits

class model(object):

    def __init__(self, FLAGS):

        # Placeholders
        self.inputs_X = tf.placeholder(tf.int32,
            shape=[None, None], name='inputs_X')
        self.targets_y = tf.placeholder(tf.float32,
            shape=[None, None], name='targets_y')
        self.seq_lens = tf.placeholder(tf.int32,
            shape=[None, ], name='seq_lens')
        self.dropout = tf.placeholder(tf.float32)

        # RNN cell
        stacked_cell = rnn_cell(FLAGS, self.dropout)

        # Inputs to RNN
        with tf.variable_scope('rnn_inputs',reuse=True):
            W_input = tf.get_variable(""W_input"",
                [FLAGS.en_vocab_size, FLAGS.num_hidden_units])

        inputs = rnn_inputs(FLAGS, self.inputs_X)
        #initial_state = stacked_cell.zero_state(FLAGS.batch_size, tf.float32)

        # Outputs from RNN
        all_outputs, state = tf.nn.dynamic_rnn(cell=stacked_cell, inputs=inputs,
            sequence_length=self.seq_lens, dtype=tf.float32)

        # state has the last RELEVANT output automatically since we fed in seq_len
        # [0] because state is a tuple with a tensor inside it
        outputs = state[0]

        # Process RNN outputs
        with tf.variable_scope('rnn_softmax',reuse=True):
            W_softmax = tf.get_variable(""W_softmax"",
                [FLAGS.num_hidden_units, FLAGS.num_classes])
            b_softmax = tf.get_variable(""b_softmax"", [FLAGS.num_classes])

        # Logits
        logits = rnn_softmax(FLAGS, outputs)
        probabilities = tf.nn.softmax(logits)
        self.accuracy = tf.equal(tf.argmax(
            self.targets_y,1), tf.argmax(logits,1))

        # Loss
        self.loss = tf.reduce_mean(
            tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.targets_y))

        # Optimization
        self.lr = tf.Variable(0.0, trainable=False)
        trainable_vars = tf.trainable_variables()
        # clip the gradient to avoid vanishing or blowing up gradients
        grads, _ = tf.clip_by_global_norm(
            tf.gradients(self.loss, trainable_vars), FLAGS.max_gradient_norm)
        optimizer = tf.train.AdamOptimizer(self.lr)
        self.train_optimizer = optimizer.apply_gradients(
            zip(grads, trainable_vars))

        # Below are values we will use for sampling (generating the sentiment
        # after each word.)

        # this is taking all the ouputs for the first input sequence
        # (only 1 input sequence since we are sampling)
        sampling_outputs = all_outputs[0]

        # Logits
        sampling_logits = rnn_softmax(FLAGS, sampling_outputs)
        self.sampling_probabilities = tf.nn.softmax(sampling_logits)

        # Components for model saving
        self.global_step = tf.Variable(0, trainable=False)
        self.saver = tf.train.Saver(tf.all_variables())

    def step(self, sess, batch_X, batch_seq_lens, batch_y=None, dropout=0.0,
        forward_only=True, sampling=False):

        input_feed = {self.inputs_X: batch_X,
                      self.targets_y: batch_y,
                      self.seq_lens: batch_seq_lens,
                      self.dropout: dropout}

        if forward_only:
            if not sampling:
                output_feed = [self.loss,
                               self.accuracy]
            elif sampling:
                input_feed = {self.inputs_X: batch_X,
                              self.seq_lens: batch_seq_lens,
                              self.dropout: dropout}
                output_feed = [self.sampling_probabilities]
        else: # training
            output_feed = [self.train_optimizer,
                           self.loss,
                           self.accuracy]


        outputs = sess.run(output_feed, input_feed)

        if forward_only:
            if not sampling:
                return outputs[0], outputs[1]
            elif sampling:
                return outputs[0]
        else: # training
            return outputs[0], outputs[1], outputs[2]

**But I faced this error while training:**

ValueError                                Traceback (most recent call last)
<ipython-input-19-93fd337a0d5c> in <module>()
----> 1 train()

<ipython-input-18-62be6fa1e73e> in train()
      9 
     10         # Load old model or create new one
---> 11         model = create_model(sess, FLAGS)
     12 
     13         # Train results

<ipython-input-17-0c9c27ad52d3> in create_model(sess, FLAGS)
      1 def create_model(sess, FLAGS):
      2 
----> 3     text_model = model(FLAGS)
      4 
      5     ckpt = tf.train.get_checkpoint_state(FLAGS.ckpt_dir)

<ipython-input-15-bd33cb4f9d34> in __init__(self, FLAGS)
     18         with tf.variable_scope('rnn_inputs',reuse=True):
     19             W_input = tf.get_variable(""W_input"",
---> 20                 [FLAGS.en_vocab_size, FLAGS.num_hidden_units])
     21 
     22         inputs = rnn_inputs(FLAGS, self.inputs_X)

C:\Users\Prof subhasis\Anaconda31\lib\site-packages\tensorflow\python\ops\variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)
   1063       collections=collections, caching_device=caching_device,
   1064       partitioner=partitioner, validate_shape=validate_shape,
-> 1065       use_resource=use_resource, custom_getter=custom_getter)
   1066 get_variable_or_local_docstring = (
   1067     """"""%s

C:\Users\Prof subhasis\Anaconda31\lib\site-packages\tensorflow\python\ops\variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)
    960           collections=collections, caching_device=caching_device,
    961           partitioner=partitioner, validate_shape=validate_shape,
--> 962           use_resource=use_resource, custom_getter=custom_getter)
    963 
    964   def _get_partitioned_variable(self,

C:\Users\Prof subhasis\Anaconda31\lib\site-packages\tensorflow\python\ops\variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)
    365           reuse=reuse, trainable=trainable, collections=collections,
    366           caching_device=caching_device, partitioner=partitioner,
--> 367           validate_shape=validate_shape, use_resource=use_resource)
    368 
    369   def _get_partitioned_variable(

C:\Users\Prof subhasis\Anaconda31\lib\site-packages\tensorflow\python\ops\variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)
    350           trainable=trainable, collections=collections,
    351           caching_device=caching_device, validate_shape=validate_shape,
--> 352           use_resource=use_resource)
    353 
    354     if custom_getter is not None:

C:\Users\Prof subhasis\Anaconda31\lib\site-packages\tensorflow\python\ops\variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)
    680       raise ValueError(""Variable %s does not exist, or was not created with ""
    681                        ""tf.get_variable(). Did you mean to set reuse=None in ""
--> 682                        ""VarScope?"" % name)
    683     if not shape.is_fully_defined() and not initializing_from_value:
    684       raise ValueError(""Shape of a new variable (%s) must be fully defined, ""

**ValueError:** Variable rnn_inputs/W_input does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?

**So,I set reuse=None, but it showed another error:**

**ValueError**: Variable rnn_inputs/W_input already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:

  File ""C:\Users\Prof subhasis\Anaconda31\lib\site-packages\tensorflow\python\framework\ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()
  File ""C:\Users\Prof subhasis\Anaconda31\lib\site-packages\tensorflow\python\framework\ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\Prof subhasis\Anaconda31\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)

**I again set back reuse = True, which should be the case,but this is error this time:**

Variable rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:

  File ""C:\Users\Prof subhasis\Anaconda31\lib\site-packages\tensorflow\python\framework\ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()
  File ""C:\Users\Prof subhasis\Anaconda31\lib\site-packages\tensorflow\python\framework\ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\Prof subhasis\Anaconda31\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)

Can anybody help me with this?"
13876,Should we provide parameters for 'data_dir' or 'untar' to cifar10.load_data?,"https://github.com/tensorflow/tensorflow/blob/d7409d32bba5ffa89141ec5427780f68a3b6942d/tensorflow/python/keras/_impl/keras/datasets/cifar10.py#L30

This is maybe really a trivial issue for such a brilliant framework,
but it would be great more friendly to add this flexibility to load data from somewhere that I already have it and unpacked. Or it will fixed me to dig into the source to find where and in which format should this data be stored.

Hope that make sense.

Thanks a lot for you time."
13875,Missing MPI collectives op symbols in TF build,"Running into a build issue when trying to use MPI collectives. We are able to build a recent version of TF using NVIDIA's changes for CUDA 9/cuDNN 7. However, it appears that the build strips the MPI collectives library ops:

```
% python -c 'import tensorflow.contrib.mpi_collectives as mpi'
Traceback (most recent call last):
 File ""<string>"", line 1, in <module>
 File ""/mnt/home/lib/python3.6/site-packages/tensorflow/contrib/mpi_collectives/__init__.py"", line 126, in <module>
   from tensorflow.contrib.mpi_collectives.mpi_ops import size
 File ""/mnt/home/lib/python3.6/site-packages/tensorflow/contrib/mpi_collectives/mpi_ops.py"", line 59, in <module>
   'MPIAllreduce'])
 File ""/mnt/home/lib/python3.6/site-packages/tensorflow/contrib/mpi_collectives/mpi_ops.py"", line 51, in _load_library
   (expected_op, name))
NameError: Could not find operator MPISize in dynamic library mpi_collectives.so
```

It seems like the issue might be caused by commit 5c7f9e3, which changes linking behavior, but we're unable to bisect due to commit order dependencies.


Anyone have an idea how to fix the issue? We're willing to update the MPI collectives code and submit a PR fix.



### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: We've applied NVIDIA's CUDA 9/cuDNN 7 patches for mixed-precision per this page: http://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html#training_tensorflow
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 14.04
- **TensorFlow version (use command below)**: ea94bbe9fa9f9b3d01fb057c02ef7873d76bf09c
- **Python version**: 3.6
- **Bazel version**: 0.5.4
- **CUDA/cuDNN version**: CUDA 9.0.103_rc/cuDNN 7.0-rc
- **Exact command to reproduce**: `python -c 'import tensorflow.contrib.mpi_collectives as mpi'`




"
13872,OS X: tensorflow java image not found,"i am running with Mac OS
```
<dependencies>
		<dependency>
			<groupId>org.tensorflow</groupId>
			<artifactId>tensorflow</artifactId>
			<version>1.4.0-rc0</version>
		</dependency>
	</dependencies>
```

please note the issue is seen in 1.4.0-rc0 and 1.3.0, however the example works fine with 1.3.0-rc0

```
org.tensorflow.NativeLibrary: tryLoadLibraryFailed: no tensorflow_jni in java.library.path
org.tensorflow.NativeLibrary: jniResourceName: org/tensorflow/native/darwin-x86_64/libtensorflow_jni.dylib
org.tensorflow.NativeLibrary: frameworkResourceName: org/tensorflow/native/darwin-x86_64/libtensorflow_framework.dylib
org.tensorflow.NativeLibrary: org/tensorflow/native/darwin-x86_64/libtensorflow_framework.dylib not found. This is fine assuming org/tensorflow/native/darwin-x86_64/libtensorflow_jni.dylib is not built to depend on it.
org.tensorflow.NativeLibrary: extracting native library to: /var/folders/bj/v1l790113yn16zvhljd6yl8h0000gn/T/tensorflow_native_libraries-1508538938374-0/libtensorflow_jni.dylib
org.tensorflow.NativeLibrary: copied 90774332 bytes to /var/folders/bj/v1l790113yn16zvhljd6yl8h0000gn/T/tensorflow_native_libraries-1508538938374-0/libtensorflow_jni.dylib
Exception in thread ""main"" java.lang.UnsatisfiedLinkError: /private/var/folders/bj/v1l790113yn16zvhljd6yl8h0000gn/T/tensorflow_native_libraries-1508538938374-0/libtensorflow_jni.dylib: dlopen(/private/var/folders/bj/v1l790113yn16zvhljd6yl8h0000gn/T/tensorflow_native_libraries-1508538938374-0/libtensorflow_jni.dylib, 1): Library not loaded: @rpath/libtensorflow_framework.so
  Referenced from: /private/var/folders/bj/v1l790113yn16zvhljd6yl8h0000gn/T/tensorflow_native_libraries-1508538938374-0/libtensorflow_jni.dylib
  Reason: image not found
	at java.lang.ClassLoader$NativeLibrary.load(Native Method)
	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1937)
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1822)
	at java.lang.Runtime.load0(Runtime.java:809)
	at java.lang.System.load(System.java:1086)
	at org.tensorflow.NativeLibrary.load(NativeLibrary.java:96)
	at org.tensorflow.TensorFlow.init(TensorFlow.java:66)
	at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)
	at org.tensorflow.Graph.<clinit>(Graph.java:258)
	at xTensorflow.HelloTF.main(HelloTF.java:13)
```"
13869,"conv2d_transpose crashing, ""NotFoundError: No algorithm worked!"", only with batch size >=2^16","I'm getting a crash running a simple autoencoder network, stack trace below. Interestingly if I trim the batch size of the input (validate_vecs_normed) to 65535, everything is fine. E.g.
`validate_vecs_normed.shape (65536, 75)` crashes, `validate_vecs_normed.shape (65536, 75)` does not. The size of input is less than 20 MB so should be plenty of room with a 12GB card.

```
Traceback (most recent call last):
  [...snip...] 
    recon, batch_cost = sess.run([decoded, cost], feed_dict={x_in_unrav: validate_vecs_normed})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: No algorithm worked!
	 [[Node: conv2d_transpose = Conv2DBackpropInput[T=DT_FLOAT, data_format=""NHWC"", padding=""VALID"", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](stack, W_0/read, enc_output_0)]]
	 [[Node: cost/_25 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_47_cost"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'conv2d_transpose', defined at:
  [...snip...]
    saver = tf.train.import_meta_graph(os.path.join(model_dir, model_meta_format.format(fold_ind)))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1698, in import_meta_graph
    **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py"", line 656, in import_scoped_meta_graph
    producer_op_list=producer_op_list)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 313, in import_graph_def
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): No algorithm worked!
	 [[Node: conv2d_transpose = Conv2DBackpropInput[T=DT_FLOAT, data_format=""NHWC"", padding=""VALID"", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](stack, W_0/read, enc_output_0)]]
	 [[Node: cost/_25 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_47_cost"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
```
Possibly related to #11327 or #9576 but not sure. One other thing, when running same code on tensorflow 1.0.1, there was no crash, but the return value ""decoded"" was all zeros when running a large batch size, and normal with smaller. I'm not sure if it's the same 65535/65536 threshold, I hadn't found it at that point.

Just for fun I ran with `TF_USE_CUDNN=0` but that crashes with `UnimplementedError (see above for traceback): Conv2D for GPU is not currently supported without cudnn`

OS: Ubuntu 16.04. 
Running docker image based on tensorflow/tensorflow:1.3.0-devel-gpu (so python 2.7.12, CUDA v8.0, cuDNN v6.0), with nvidia-docker 17.05.0-ce. 
Docker image tensorflow/tensorflow:1.0.1-gpu returns all zeros instead of crashing.
GPUs: 2x Titan X (Pascal).






"
13868,speech_commands svdf model does not work in android demo app.,"== cat /etc/issue ===============================================
Mac OS X 10.12.6

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.0.0 (clang-900.0.37)
Target: x86_64-apple-darwin16.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================

== check pips ===================================================
numpy (1.13.3)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-tensorboard (0.4.0rc1)

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc1-3261-g934662e7b
tf.COMPILER_VERSION = v1.3.0-rc1-3261-g934662e7b
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
/var/tmp/collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================


### Describe the problem
under tensorflow/examples/speech_commands, there are three models for speech commands. The 
default one(conv) and low-latency-conv works when I install them to android demo app(tensorflow/examples/android). 

low-latency-svdf does not work in the android app, it works on pc though.

If i copy the svdf model into the app, app crashes when startup reporting model file error like this:

`Not a valid TensorFlow Graph serialization: Value for attr 'T' of int64 is not in the list of allowed values: float, int32, qint8, quint8, qint32
; NodeDef: count_nonzero/Sum = Sum[T=DT_INT64, Tidx=DT_INT32, keep_dims=false](count_nonzero/ToInt64, count_nonzero/Const); Op<name=Sum; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_INT32, DT_QINT8, DT_QUINT8, DT_QINT32]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>`

### Source code / logs

> 10-20 17:08:30.889 23598-23598/org.tensorflow.demo.svdf D/AndroidRuntime: Shutting down VM
10-20 17:08:30.894 23598-23598/org.tensorflow.demo.svdf E/AndroidRuntime: FATAL EXCEPTION: main
                                                                          Process: org.tensorflow.demo.svdf, PID: 23598
                                                                          java.lang.RuntimeException: Unable to start activity ComponentInfo{org.tensorflow.demo.svdf/org.tensorflow.demo.SpeechActivity}: java.lang.RuntimeException: Failed to load model from 'file:///android_asset/marvin_sheila_ll_svdf_graph.pb'
                                                                              at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2817)
                                                                              at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2892)
                                                                              at android.app.ActivityThread.-wrap11(Unknown Source:0)
                                                                              at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1593)
                                                                              at android.os.Handler.dispatchMessage(Handler.java:105)
                                                                              at android.os.Looper.loop(Looper.java:164)
                                                                              at android.app.ActivityThread.main(ActivityThread.java:6541)
                                                                              at java.lang.reflect.Method.invoke(Native Method)
                                                                              at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)
                                                                              at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)
                                                                           Caused by: java.lang.RuntimeException: Failed to load model from 'file:///android_asset/marvin_sheila_ll_svdf_graph.pb'
                                                                              at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:113)
                                                                              at org.tensorflow.demo.SpeechActivity.onCreate(SpeechActivity.java:151)
                                                                              at android.app.Activity.performCreate(Activity.java:6975)
                                                                              at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1213)
                                                                              at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2770)
                                                                              at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2892) 
                                                                              at android.app.ActivityThread.-wrap11(Unknown Source:0) 
                                                                              at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1593) 
                                                                              at android.os.Handler.dispatchMessage(Handler.java:105) 
                                                                              at android.os.Looper.loop(Looper.java:164) 
                                                                              at android.app.ActivityThread.main(ActivityThread.java:6541) 
                                                                              at java.lang.reflect.Method.invoke(Native Method) 
                                                                              at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240) 
                                                                              at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767) 
                                                                           Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Value for attr 'T' of int64 is not in the list of allowed values: float, int32, qint8, quint8, qint32
                                                                          	; NodeDef: count_nonzero/Sum = Sum[T=DT_INT64, Tidx=DT_INT32, keep_dims=false](count_nonzero/ToInt64, count_nonzero/Const); Op<name=Sum; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_INT32, DT_QINT8, DT_QUINT8, DT_QINT32]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
                                                                              at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:535)
                                                                              at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:105)
                                                                              	... 13 more
"
13865,Feature Request: Support for None values in tf.contrib.data.Dataset,"It would be very handy if the Dataset API supports `None` types. The idea is to be able to use the same `Iterator` object for the training and the test datasets. As the training dataset contains labels and the test dataset does not, the only workaround I know at the moment is to use some dummy labels in order to make the two datasets compatible with the same `Iterator`. This can waste a lot of memory though and is not a clean solution. Instead, maybe it can be possible to create a `Dataset` from `None`, that behaves in a way such that its `output_types` and `output_shapes` are compatible with any other type and shape, but does not consume so much memory. Here is a quick example:

```
X_train = tf.contrib.data.Dataset.from_tensor_slices(X_train_data)
y_train = tf.contrib.data.Dataset.from_tensor_slices(y_train_data)
data_train = tf.conrib.Dataset.zip((X_train, y_train))

X_test = tf.contrib.data.Dataset.from_tensor_slices(X_test_data)
y_test = tf.contrib.data.Dataset.from_tensor_slices(None)
data_test = tf.conrib.Dataset.zip((X_test, y_test))

assert data_train.output_types == data_test.output_types
assert data_train.output_shapes == data_test.output_shapes

iterator = Iterator.from_structure(data_train.output_types, data_train.output_shapes)

train_init_op = iterator.make_initializer(data_train)
test_init_op = iterator.make_initializer(data_test)

# Build the graph ...

# Train network
with tf.Session() as sess:
  sess.run(train_init_op)
  # Train ...

# Run in prediction mode
with tf.Session() as sess:
  sess.run(test_init_op)
  # Get predictions ...

```
"
13861,Channel number of convolution output is unspecified when atrous rate > 1 ,"Python 2.7
Tensorflow v1.2.0-rc2-21-g12f033d and v1.3.0-rc1-2361-gd1286ab


Easily reproduced by the following code:
``` Python
x = tf.placeholder(tf.float32, [1, 10, None, None])
w = tf.zeros([3, 3, 10, 20])
y = tf.nn.convolution(x, w, 'VALID', [1]*2, [2]*2, data_format='NCHW')
print(y)
```
The input has known channel number but unknown width and height, in NCHW format. The convolution has rate >= 2. The channel number of the output `y` is obviously known, `(1, 20, None, None)` in this example, depending on the shape of `w`. However the execution result gives 
```
Tensor(""convolution/BatchToSpaceND:0"", shape=(1, ?, ?, ?), dtype=float32)
```
I believe this is related to the reshaping operation in the atrous convolution. The issue happens only when the spatial shape is unknown and with NCHW format. Not sure if this happens to the latest version.

"
13860,cudnnGRU is_training placeholder,"When creating a model with batch normalisation I can supply a place-holder for the is_training param like so:

```
training = tf.placeholder(tf.bool)  
sym = create_symbol(training)
# ....
# Training: sess.run(model, feed_dict={X: data, y: label, training: True})
# Inference: sess.run(pred, feed_dict={X: data, training: False})
```

However when I do this for a symbol that contains cudnnGRU (or cudnnLSTM), it doesn't like the place-holder:

```
cudnn_cell = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, 
                                           num_units=NUMHIDDEN, 
                                           input_size=EMBEDSIZE)    # Set params
params_size_t = cudnn_cell.params_size()
params = tf.Variable(tf.random_uniform([params_size_t]), validate_shape=False)   
input_h = tf.Variable(tf.zeros([1, BATCHSIZE, NUMHIDDEN]))
outputs, states = cudnn_cell(is_training=training ,
                             input_data=word_list,
                             input_h=input_h,
                             params=params)
```

Error message:

> 
TypeError: Expected bool for argument 'is_training' not <tf.Tensor 'Placeholder_2:0' shape=<unknown> dtype=bool>."
13857,Trying to import tensorflow but im getting this im using the cuda 9 and cudnn 7,"Traceback (most recent call last):
  File ""/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/mohammad/.conda/envs/my_root/lib/python3.6/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/mohammad/.conda/envs/my_root/lib/python3.6/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/mohammad/.conda/envs/my_root/lib/python3.6/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/mohammad/.conda/envs/my_root/lib/python3.6/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.
"
13856,AttributeError: module 'tensorflow' has no attribute 'estimator',"Hi, for some reason 'estimator' isn't an attribute of 'tensorflow' when I test it on Mac. 

When I try:

`tf.estimator.Estimator(model_fn)`

I get this error 

`AttributeError: module 'tensorflow' has no attribute 'estimator'`

This works fine in Linux though. Upgrading pip/conda/tensorflow did not affect this behavior. Is there a workaround?

-Thanks

### System information
- Mac OS X 10.9.5 (x86_64-apple-darwin13.4.0)
- TensorFlow installed from binary
- TensorFlow version v1.3.0-rc1-3628-g49f9c6f89 1.5.0-dev20171020
- Python 3.6.3

"
13855,compile source code fail on mac,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
macOS Sierra 10.12.6
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**: 
1.3.0
- **Python version**: 
Python 2.7.10
- **Bazel version (if compiling from source)**:
macbookpro:tensorflow fredlee$ bazel version
Build label: 0.7.0-homebrew
Build target: bazel-out/darwin_x86_64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Oct 19 09:12:48 2017 (1508404368)
Build timestamp: 1508404368
Build timestamp as int: 1508404368
- **CUDA/cuDNN version**:
no
- **GPU model and memory**:
no 
- **Exact command to reproduce**:


step by step according to this guide
   step1:
       git clone https://github.com/tensorflow/tensorflow
  step2:
      cd tensorflow
      ./configure
  step3:
     create an example as (https://tensorflow.google.cn/api_guides/cc/guide)
  step4:
    bazel run -c opt //tensorflow/cc/example:example

  it return
      

> ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/cc/example:example failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 3083.675s, Critical Path: 306.19s
ERROR: Build failed. Not running target.

compile log:
[compile.log.zip](https://github.com/tensorflow/tensorflow/files/1401503/compile.log.zip)


****can you tell me how to make it work?** thanks**

**best wishes.**
"
13854,tf.train.SyncReplicasOptimizer training does not start,"System information
- **Have I written custom code **: Yes
- **OS Platform and Distribution **: Linux Ubuntu 16.04
- **TensorFlow installed from **: binary
- **TensorFlow version (use command below)**: tensorflow-gpu==1.3.0
- **Python version**: Python 3.6.2
- **Bazel version (if compiling from source)**: not installed
- **CUDA/cuDNN version**: Cuda compilation tools, release 8.0, V8.0.61
- **GPU model and memory**: NVIDIA Quadro P5000 16GB
- **Exact command to reproduce**:
write actual IP addresses instead of  ip_address1 and ip_address2 

( on machine 1 )
$  python trainer.py \
    --replicas_num=1 \
    --ps_hosts=ip_address1:2222 \
    --worker_hosts=ip_address2:2223 \
    --job_name=ps --task_index=0

( on machine 2 )
$  python trainer.py \
    --replicas_num=1 \
    --ps_hosts=ip_address1:2222 \
    --worker_hosts=ip_address2:2223 \
    --job_name=worker --task_index=0

### Describe the problem
I'm trying to train an below model with distributed synchronized training.
I tried, 1 ps and 1 worker, 1 ps and 2 workers, 1 ps and 3 workers.
It is a sample code similar to https://www.tensorflow.org/deploy/distributed#putting_it_all_together_example_trainer_program

If I comment out line 71 shown below training became asynchronized trainig and there is no problem.
opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=FLAGS.replicas_num, total_num_replicas=FLAGS.replicas_num)

But if I run it with line 71 not commented out training will not start.
(I followed https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer#usage to make it synchronized.)

There is no error. But trainig will not continue. sess.run() (line 96) never ends.
Is there any suggestions what might be the problem ?

### Source code / logs
#### Source code
```python
import tensorflow as tf
from tensorflow.contrib import slim
from tensorflow.examples.tutorials.mnist import input_data

flags = tf.flags
flags.DEFINE_string(""ps_hosts"", """", ""Comma-separated list of hostname:port pairs"")
flags.DEFINE_string(""worker_hosts"", """", ""Comma-separated list of hostname:port pairs"")
flags.DEFINE_string(""job_name"", """", ""One of 'ps', 'worker'"")
flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")
flags.DEFINE_integer(""replicas_num"", 3, ""Number of replicas"")
FLAGS = flags.FLAGS

def main(_):
    # config
    BATCH_SIZE = 10
    TRAINING_STEPS = 5000
    PRINT_EVERY = 100
    LOG_DIR = ""/tmp/""

    ps_hosts = FLAGS.ps_hosts.split("","")
    worker_hosts = FLAGS.worker_hosts.split("","")

    # cluster specification
    cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})

    # start a server for a specific task
    server = tf.train.Server(cluster,
                             job_name=FLAGS.job_name,
                             task_index=FLAGS.task_index)

    mnist = input_data.read_data_sets('/tmp/MNIST_data', one_hot=True)

    def net(x):
        x_image = tf.reshape(x, [-1, 28, 28, 1])
        net = slim.layers.conv2d(x_image, 32, [5, 5], scope='conv1')
        net = slim.layers.max_pool2d(net, [2, 2], scope='pool1')
        net = slim.layers.conv2d(net, 64, [5, 5], scope='conv2')
        net = slim.layers.max_pool2d(net, [2, 2], scope='pool2')
        net = slim.layers.flatten(net, scope='flatten')
        net = slim.layers.fully_connected(net, 500, scope='fully_connected')
        net = slim.layers.fully_connected(net, 10, activation_fn=None, scope='pred')
        return net

    print(""job_name     = %s"" % FLAGS.job_name)
    print(""task_index   = %d"" % FLAGS.task_index)
    print(""replicas_num = %d"" % FLAGS.replicas_num)

    if FLAGS.job_name == ""ps"":
        server.join()
    elif FLAGS.job_name == ""worker"":
        # Between-graph replication
        with tf.device(tf.train.replica_device_setter(
                worker_device=""/job:worker/task:%d"" % FLAGS.task_index,
                cluster=cluster)):
            # count the number of updates
            global_step = tf.get_variable('global_step', [],
                                          initializer=tf.constant_initializer(0),
                                          trainable=False)

            x = tf.placeholder(tf.float32, shape=[None, 784], name=""x-input"")
            # target 10 output classes
            y_ = tf.placeholder(tf.float32, shape=[None, 10], name=""y-input"")
            y = net(x)

            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))
            #cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))

            opt = tf.train.AdamOptimizer(1e-4)

            # for synchronous training
            opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=FLAGS.replicas_num, total_num_replicas=FLAGS.replicas_num)

            train_step = opt.minimize(cross_entropy, global_step=global_step)
            #sync_replicas_hook = opt.make_session_run_hook(is_chief=(FLAGS.task_index == 0))

            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

            init_op = tf.global_variables_initializer()

        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),
                                 logdir=LOG_DIR,
                                 global_step=global_step,
                                 init_op=init_op)

        with sv.managed_session(server.target) as sess:
            step = 0

            test = 0
            while not sv.should_stop() and step <= TRAINING_STEPS:
                batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)

                print(""--------- run_step: test %d ----------"" % test)
                test += 1

                _, acc, step = sess.run([train_step, accuracy, global_step],
                                        feed_dict={x: batch_x, y_: batch_y})
                print(""--------- run_step: test %d ----------"" % test)
                test += 1

                if step % PRINT_EVERY == 0:
                    print(""Worker : {}, Step: {}, Accuracy (batch): {}"".\
                          format(FLAGS.task_index, step, acc))

            test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})
            print(""Test-Accuracy: {}"".format(test_acc))

        sv.stop()


if __name__ == ""__main__"":
    tf.app.run(main=main)

```
#### Logs
I got logs shown below from the parameter server and the worker 

from the parameter server:
```bash
...
job_name     = ps
task_index   = 0
replicas_num = 1
```
from the worker:
```bash
...
job_name     = worker
task_index   = 0
replicas_num = 1
2017-10-20 17:54:43.891111: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session 10bf51b4613ac80d with config:
--------- run_step: test 0 ----------

```
"
13853,"how can i use ""TF_SetTarget or TF_SetConfig"" to set the number of threads in c code","### Source code / logs
this is my c code:

	TF_SessionOptions* sess_opts = TF_NewSessionOptions();
	std::string str(""intra_op_parallelism_threads = 4"");
	TF_SetConfig(sess_opts,(void *)str.c_str(),str.size(),status);
	if (TF_GetCode(status) != TF_OK) {
		  printf(""ERROR: %s\n"", TF_Message(status));
	}

this is the debug output:

       Successfully imported graph
       ERROR: Unparseable ConfigProto
      "
13851,Fail in configuration due to different CUDA libraries path,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 49f9c6f890c938955fa2d448ac5b556b9a6d9aa0
- **Python version**: Python 3.5.3
- **Bazel version (if compiling from source)**: bazel release 0.6.1
- **CUDA/cuDNN version**: CUDA 8, cuDNN 7
- **GPU model and memory**: GeForce GTX 1080 

### Describe the problem
As for now, TF assumes that CUDA liibs are located at `CUDA_PATH/lib64/`, however Ubuntu installs CUDA to `/usr/lib/x86_64-linux-gnu/`, which makes configuration impossible: I can't specify cuda path such that it'll find `/usr/lib/x86_64-linux-gnu/libcudart.so.8.0`
"
13849,Feature request: only master is allowed to export for `tf.contrib.learn.Experiment`,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.11.6
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.0-nightly, 1.3.0
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

When using `tf.contrib.learn.Experiment` for distributed training, it seems that all workers, ps, and master try to export model when finished. However, this will cause write conflict when `model_dir` is on hdfs, because all write the same file to the same location.

Hence, I propose that only master is allowed to export model.
I can work on it if tensorflowers're agreed.

### Source code / logs

```python
INFO:tensorflow:Saving dict for global step 30: accuracy = 0.339453, global_step = 30, loss = 70112.8
Traceback (most recent call last):
  File ""main.py"", line 60, in <module>
    tf.app.run()
  File ""/Users/facai/Library/anaconda3/envs/py27_test/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""main.py"", line 56, in main
    sess.run()
  File ""/Users/facai/Workshop/sina/Prometheus/prometheus/python/session.py"", line 172, in run
    ex.train_and_evaluate()
  File ""/Users/facai/Library/anaconda3/envs/py27_test/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 641, in train_and_evaluate
    export_results = self._maybe_export(eval_result)
  File ""/Users/facai/Library/anaconda3/envs/py27_test/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 744, in _maybe_export
    eval_result=eval_result))
  File ""/Users/facai/Library/anaconda3/envs/py27_test/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/export_strategy.py"", line 87, in export
    return self.export_fn(estimator, export_path, **kwargs)
  File ""/Users/facai/Library/anaconda3/envs/py27_test/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py"", line 454, in export_fn
    checkpoint_path=checkpoint_path)
  File ""/Users/facai/Library/anaconda3/envs/py27_test/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1307, in export_savedmodel
    builder = saved_model_builder.SavedModelBuilder(temp_export_dir)
  File ""/Users/facai/Library/anaconda3/envs/py27_test/lib/python2.7/site-packages/tensorflow/python/saved_model/builder_impl.py"", line 88, in __init__
    ""directory: %s"" % export_dir)
AssertionError: Export directory already exists. Please specify a different export directory: file:///tmp/facai/prome/model/new/export/Servo/temp-1508482645
```
"
13848,can not compile android demo for x86,"I tried to use the following commands to build android demo for x86
bazel build //tensorflow/examples/android:tensorflow_demo --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain  --force_pic --cpu=x86_64 --config=android_x86

I always get the tensorflow_demo.apk for ARM device.
Is there anything wrong?"
13847,tf.train.batch and shuffle_batch undetermined for multi queue input with multiple threads,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Custom code, see below.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
OSX

- **TensorFlow installed from (source or binary)**:
pip install tensorflow == 1.2

- **TensorFlow version (use command below)**:
('v1.2.0-rc2-21-g12f033d', '1.2.0')

- **Python version**: 
Python 2.7.10

- **Bazel version (if compiling from source)**:
n/a

- **CUDA/cuDNN version**:
n/a

- **GPU model and memory**:
n/a

- **Exact command to reproduce**:


```
a_fifo = tf.train.slice_input_producer([np.arange(30)],
                                        shuffle=False,
                                        name='a')[0]
b_fifo = tf.train.slice_input_producer([np.arange(30)],
                                        shuffle=False,
                                        name='b')[0]
train_batch = tf.train.batch([a_fifo, b_fifo],
                            batch_size=5,
                            capacity=18,
                            num_threads=2,
                            name='batch')
at, bt = train_batch
with tf.Session() as sess:
    threads = tf.train.start_queue_runners(sess=sess)
    sess.run(tf.global_variables_initializer())
    try:
        for i in range(150):
            av, bv = sess.run([at, bt])
            assert list(av) == list(bv), ""queues derailed!""
    except Exception as e:
        print((""Exception in training: {}"").format(e))
        print(""__"",i,""__"")
        print(av)
        print(bv)
```

Output:

> Exception in training: queues derailed!
> __ 11 __
> [26 25 27 28 29]
> [25 26 27 28 29]




### Describe the problem
Both queues should be dequeued synchronously, as it does when num_threads =1 in the batch statement. With num_threads > 1 the two input queue's will derail, as shown in the example above. Same issue appears also in train.shuffle_batch.

### Source code / logs
See code above."
13844,Saving large graphs to S3 fails with InternalError: : Unable to connect to endpoint,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes; modified Tensorflow [save/load example code](https://www.tensorflow.org/programmers_guide/saved_model) to save a few large tensors to S3
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version**:
Git version: `v1.3.0-rc1-3504-g27767d8`
Tensorflow version: `1.4.0-rc1`
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: Nvidia Tesla K80 (12 GiB RAM)
- **Exact command to reproduce**: Run the code in [this gist](https://gist.github.com/smurching/8766a9d91c148ef7d89292b6dd4da5b8) in a Python shell. You'll need access to an S3 bucket for which you have write permissions.

### Describe the problem
I'm trying to save a large (~380 MB) graph to S3, but my call to `tf.Saver.save()` crashes after ~1 min with what appears to be an AWS SDK error (`InternalError: : Unable to connect to endpoint`).

If the error is indeed AWS related, it'd be helpful to wrap it in something to indicate that the error isn't coming from tensorflow. Here's the stacktrace:

```
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<command-5036518> in <module>()
----> 1 test_save(num_tensors=10, tensor_size=10000000, save_path=""s3://<redacted_s3_bucket_name>/model.ckpt"")

<command-5036204> in test_save(num_tensors, tensor_size, save_path)
     18     sess.run(init_op)
     19     # Save the variables to disk.
---> 20     save_path = saver.save(sess, save_path)
     21     print(""Model saved in file: %s"" % save_path)

/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc in save(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)
   1571           model_checkpoint_path = sess.run(
   1572               self.saver_def.save_tensor_name,
-> 1573               {self.saver_def.filename_tensor_name: checkpoint_file})
   1574         else:
   1575           self._build_eager(

/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    887     try:
    888       result = self._run(None, fetches, feed_dict, options_ptr,
--> 889                          run_metadata_ptr)
    890       if run_metadata:
    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1119       results = self._do_run(handle, final_targets, final_fetches,
-> 1120                              feed_dict_tensor, options, run_metadata)
   1121     else:
   1122       results = []

/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1315     if handle is None:
   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1317                            options, run_metadata)
   1318     else:
   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
   1334         except KeyError:
   1335           pass
-> 1336       raise type(e)(node_def, op, message)
   1337 
   1338   def _extend_graph(self):

InternalError: : Unable to connect to endpoint
	 [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, v0/_1, v1/_3, v2/_5, v3/_7, v4/_9, v5/_11, v6/_13, v7/_15, v8/_17, v9/_19)]]

Caused by op u'save/SaveV2', defined at:
  File ""/tmp/1509404428358-0/PythonShell.py"", line 990, in <module>
    launch_process()
  File ""/tmp/1509404428358-0/PythonShell.py"", line 986, in launch_process
    shell.executor.run()
  File ""/tmp/1509404428358-0/PythonShell.py"", line 263, in run
    self.shell.shell.run_cell(command_id, cmd, store_history=True)
  File ""/tmp/1509404428358-0/PythonShell.py"", line 572, in run_cell
    super(IPythonShell, self).run_cell(raw_cell, store_history, silent, shell_futures)
  File ""/databricks/python/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2741, in run_cell
    interactivity=interactivity, compiler=compiler)
  File ""/databricks/python/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2833, in run_ast_nodes
    if self.run_code(code):
  File ""/databricks/python/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2883, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<command-5036518>"", line 1, in <module>
    test_save(num_tensors=10, tensor_size=10000000, save_path=""s3://databricks-mllib/tmp/s3-save-failure/model.ckpt"")
  File ""<command-5036204>"", line 13, in test_save
    saver = tf.train.Saver()
  File ""/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1218, in __init__
    self.build()
  File ""/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1227, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1263, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 748, in _build_internal
    save_tensor = self._AddSaveOps(filename_tensor, saveables)
  File ""/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 296, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File ""/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 239, in save_op
    tensors)
  File ""/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1163, in save_v2
    shape_and_slices=shape_and_slices, tensors=tensors, name=name)
  File ""/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InternalError (see above for traceback): : Unable to connect to endpoint
	 [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, v0/_1, v1/_3, v2/_5, v3/_7, v4/_9, v5/_11, v6/_13, v7/_15, v8/_17, v9/_19)]]
```

If I run the same code but checkpoint to my local filesystem the save op runs without error.
The error also only seems to occur for large graphs (running the [linked gist](https://gist.github.com/smurching/8766a9d91c148ef7d89292b6dd4da5b8) with smaller/fewer tensors works)



"
13838,optimize_for_inference - KeyError,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yUP.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Elementary OS 0.4 Loki
- **TensorFlow installed from (source or binary)**: ```pip install tensorflow```
- **TensorFlow version (use command below)**: ```('v1.3.0-rc2-20-g0787eee', '1.3.0')```
- **Python version**: 2.7.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.5
- **GPU model and memory**: NVIDIA GTX 1060 - 6GB 
- **Exact command to reproduce**:

### Describe the problem

Experiencing a strange KeyError whilst attempting to optimize graph.

```
Traceback (most recent call last):
  File ""/home/keo7/Projects/AgriDoctorAlpha/model/keras/main.py"", line 126, in <module>
    export_model(tf.train.Saver(), model, [""input_1""], ""dense_3/kernel"")
  File ""/home/keo7/Projects/AgriDoctorAlpha/model/keras/main.py"", line 115, in export_model
    tf.float32.as_datatype_enum)
  File ""/home/keo7/.virtualenvs/deeplearning/local/lib/python2.7/site-packages/tensorflow/python/tools/optimize_for_inference_lib.py"", line 109, in optimize_for_inference
    placeholder_type_enum)
  File ""/home/keo7/.virtualenvs/deeplearning/local/lib/python2.7/site-packages/tensorflow/python/tools/strip_unused_lib.py"", line 83, in strip_unused
    raise KeyError(""The following input nodes were not found: %s\n"" % not_found)
KeyError: ""The following input nodes were not found: set(['input_1'])\n""
```

### Source code / logs

```python
output_graph_def = optimize_for_inference_lib.optimize_for_inference(
            input_graph_def, [""input_1""], [""dense_3/kernel""]
            tf.float32.as_datatype_enum)
```

Graph:

```
node {
  name: ""input_1""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: -1
        }
        dim {
          size: 48
        }
        dim {
          size: 48
        }
        dim {
          size: 3
        }
      }
    }
  }
}
node {
  name: ""block1_conv1/random_uniform/shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\003\000\000\000@\000\000\000""
      }
    }
  }
}
node {
  name: ""block1_conv1/random_uniform/min""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: -0.0997509360313
      }
    }
  }
}
node {
  name: ""block1_conv1/random_uniform/max""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0997509360313
      }
    }
  }
}
node {
  name: ""block1_conv1/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""block1_conv1/random_uniform/shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 4107740
    }
  }
}
node {
  name: ""block1_conv1/random_uniform/sub""
  op: ""Sub""
  input: ""block1_conv1/random_uniform/max""
  input: ""block1_conv1/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block1_conv1/random_uniform/mul""
  op: ""Mul""
  input: ""block1_conv1/random_uniform/RandomUniform""
  input: ""block1_conv1/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block1_conv1/random_uniform""
  op: ""Add""
  input: ""block1_conv1/random_uniform/mul""
  input: ""block1_conv1/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block1_conv1/kernel""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 64
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block1_conv1/kernel/Assign""
  op: ""Assign""
  input: ""block1_conv1/kernel""
  input: ""block1_conv1/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block1_conv1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block1_conv1/kernel/read""
  op: ""Identity""
  input: ""block1_conv1/kernel""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block1_conv1/kernel""
      }
    }
  }
}
node {
  name: ""block1_conv1/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 64
          }
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""block1_conv1/bias""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 64
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block1_conv1/bias/Assign""
  op: ""Assign""
  input: ""block1_conv1/bias""
  input: ""block1_conv1/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block1_conv1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block1_conv1/bias/read""
  op: ""Identity""
  input: ""block1_conv1/bias""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block1_conv1/bias""
      }
    }
  }
}
node {
  name: ""block1_conv1/convolution/Shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\003\000\000\000@\000\000\000""
      }
    }
  }
}
node {
  name: ""block1_conv1/convolution/dilation_rate""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: ""\001\000\000\000\001\000\000\000""
      }
    }
  }
}
node {
  name: ""block1_conv1/convolution""
  op: ""Conv2D""
  input: ""input_1""
  input: ""block1_conv1/kernel/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""SAME""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 1
        i: 1
        i: 1
      }
    }
  }
  attr {
    key: ""use_cudnn_on_gpu""
    value {
      b: true
    }
  }
}
node {
  name: ""block1_conv1/BiasAdd""
  op: ""BiasAdd""
  input: ""block1_conv1/convolution""
  input: ""block1_conv1/bias/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}
node {
  name: ""block1_conv1/Relu""
  op: ""Relu""
  input: ""block1_conv1/BiasAdd""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block1_conv2/random_uniform/shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000@\000\000\000@\000\000\000""
      }
    }
  }
}
node {
  name: ""block1_conv2/random_uniform/min""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: -0.0721687823534
      }
    }
  }
}
node {
  name: ""block1_conv2/random_uniform/max""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0721687823534
      }
    }
  }
}
node {
  name: ""block1_conv2/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""block1_conv2/random_uniform/shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 9992257
    }
  }
}
node {
  name: ""block1_conv2/random_uniform/sub""
  op: ""Sub""
  input: ""block1_conv2/random_uniform/max""
  input: ""block1_conv2/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block1_conv2/random_uniform/mul""
  op: ""Mul""
  input: ""block1_conv2/random_uniform/RandomUniform""
  input: ""block1_conv2/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block1_conv2/random_uniform""
  op: ""Add""
  input: ""block1_conv2/random_uniform/mul""
  input: ""block1_conv2/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block1_conv2/kernel""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 64
        }
        dim {
          size: 64
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block1_conv2/kernel/Assign""
  op: ""Assign""
  input: ""block1_conv2/kernel""
  input: ""block1_conv2/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block1_conv2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block1_conv2/kernel/read""
  op: ""Identity""
  input: ""block1_conv2/kernel""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block1_conv2/kernel""
      }
    }
  }
}
node {
  name: ""block1_conv2/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 64
          }
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""block1_conv2/bias""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 64
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block1_conv2/bias/Assign""
  op: ""Assign""
  input: ""block1_conv2/bias""
  input: ""block1_conv2/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block1_conv2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block1_conv2/bias/read""
  op: ""Identity""
  input: ""block1_conv2/bias""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block1_conv2/bias""
      }
    }
  }
}
node {
  name: ""block1_conv2/convolution/Shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000@\000\000\000@\000\000\000""
      }
    }
  }
}
node {
  name: ""block1_conv2/convolution/dilation_rate""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: ""\001\000\000\000\001\000\000\000""
      }
    }
  }
}
node {
  name: ""block1_conv2/convolution""
  op: ""Conv2D""
  input: ""block1_conv1/Relu""
  input: ""block1_conv2/kernel/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""SAME""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 1
        i: 1
        i: 1
      }
    }
  }
  attr {
    key: ""use_cudnn_on_gpu""
    value {
      b: true
    }
  }
}
node {
  name: ""block1_conv2/BiasAdd""
  op: ""BiasAdd""
  input: ""block1_conv2/convolution""
  input: ""block1_conv2/bias/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}
node {
  name: ""block1_conv2/Relu""
  op: ""Relu""
  input: ""block1_conv2/BiasAdd""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block1_pool/MaxPool""
  op: ""MaxPool""
  input: ""block1_conv2/Relu""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""ksize""
    value {
      list {
        i: 1
        i: 2
        i: 2
        i: 1
      }
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""VALID""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 2
        i: 2
        i: 1
      }
    }
  }
}
node {
  name: ""block2_conv1/random_uniform/shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000@\000\000\000\200\000\000\000""
      }
    }
  }
}
node {
  name: ""block2_conv1/random_uniform/min""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: -0.0589255653322
      }
    }
  }
}
node {
  name: ""block2_conv1/random_uniform/max""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0589255653322
      }
    }
  }
}
node {
  name: ""block2_conv1/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""block2_conv1/random_uniform/shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 6910695
    }
  }
}
node {
  name: ""block2_conv1/random_uniform/sub""
  op: ""Sub""
  input: ""block2_conv1/random_uniform/max""
  input: ""block2_conv1/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block2_conv1/random_uniform/mul""
  op: ""Mul""
  input: ""block2_conv1/random_uniform/RandomUniform""
  input: ""block2_conv1/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block2_conv1/random_uniform""
  op: ""Add""
  input: ""block2_conv1/random_uniform/mul""
  input: ""block2_conv1/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block2_conv1/kernel""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 64
        }
        dim {
          size: 128
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block2_conv1/kernel/Assign""
  op: ""Assign""
  input: ""block2_conv1/kernel""
  input: ""block2_conv1/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block2_conv1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block2_conv1/kernel/read""
  op: ""Identity""
  input: ""block2_conv1/kernel""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block2_conv1/kernel""
      }
    }
  }
}
node {
  name: ""block2_conv1/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 128
          }
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""block2_conv1/bias""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 128
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block2_conv1/bias/Assign""
  op: ""Assign""
  input: ""block2_conv1/bias""
  input: ""block2_conv1/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block2_conv1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block2_conv1/bias/read""
  op: ""Identity""
  input: ""block2_conv1/bias""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block2_conv1/bias""
      }
    }
  }
}
node {
  name: ""block2_conv1/convolution/Shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000@\000\000\000\200\000\000\000""
      }
    }
  }
}
node {
  name: ""block2_conv1/convolution/dilation_rate""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: ""\001\000\000\000\001\000\000\000""
      }
    }
  }
}
node {
  name: ""block2_conv1/convolution""
  op: ""Conv2D""
  input: ""block1_pool/MaxPool""
  input: ""block2_conv1/kernel/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""SAME""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 1
        i: 1
        i: 1
      }
    }
  }
  attr {
    key: ""use_cudnn_on_gpu""
    value {
      b: true
    }
  }
}
node {
  name: ""block2_conv1/BiasAdd""
  op: ""BiasAdd""
  input: ""block2_conv1/convolution""
  input: ""block2_conv1/bias/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}
node {
  name: ""block2_conv1/Relu""
  op: ""Relu""
  input: ""block2_conv1/BiasAdd""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block2_conv2/random_uniform/shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\200\000\000\000\200\000\000\000""
      }
    }
  }
}
node {
  name: ""block2_conv2/random_uniform/min""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: -0.0510310381651
      }
    }
  }
}
node {
  name: ""block2_conv2/random_uniform/max""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0510310381651
      }
    }
  }
}
node {
  name: ""block2_conv2/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""block2_conv2/random_uniform/shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 7708517
    }
  }
}
node {
  name: ""block2_conv2/random_uniform/sub""
  op: ""Sub""
  input: ""block2_conv2/random_uniform/max""
  input: ""block2_conv2/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block2_conv2/random_uniform/mul""
  op: ""Mul""
  input: ""block2_conv2/random_uniform/RandomUniform""
  input: ""block2_conv2/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block2_conv2/random_uniform""
  op: ""Add""
  input: ""block2_conv2/random_uniform/mul""
  input: ""block2_conv2/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block2_conv2/kernel""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 128
        }
        dim {
          size: 128
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block2_conv2/kernel/Assign""
  op: ""Assign""
  input: ""block2_conv2/kernel""
  input: ""block2_conv2/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block2_conv2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block2_conv2/kernel/read""
  op: ""Identity""
  input: ""block2_conv2/kernel""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block2_conv2/kernel""
      }
    }
  }
}
node {
  name: ""block2_conv2/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 128
          }
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""block2_conv2/bias""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 128
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block2_conv2/bias/Assign""
  op: ""Assign""
  input: ""block2_conv2/bias""
  input: ""block2_conv2/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block2_conv2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block2_conv2/bias/read""
  op: ""Identity""
  input: ""block2_conv2/bias""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block2_conv2/bias""
      }
    }
  }
}
node {
  name: ""block2_conv2/convolution/Shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\200\000\000\000\200\000\000\000""
      }
    }
  }
}
node {
  name: ""block2_conv2/convolution/dilation_rate""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: ""\001\000\000\000\001\000\000\000""
      }
    }
  }
}
node {
  name: ""block2_conv2/convolution""
  op: ""Conv2D""
  input: ""block2_conv1/Relu""
  input: ""block2_conv2/kernel/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""SAME""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 1
        i: 1
        i: 1
      }
    }
  }
  attr {
    key: ""use_cudnn_on_gpu""
    value {
      b: true
    }
  }
}
node {
  name: ""block2_conv2/BiasAdd""
  op: ""BiasAdd""
  input: ""block2_conv2/convolution""
  input: ""block2_conv2/bias/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}
node {
  name: ""block2_conv2/Relu""
  op: ""Relu""
  input: ""block2_conv2/BiasAdd""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block2_pool/MaxPool""
  op: ""MaxPool""
  input: ""block2_conv2/Relu""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""ksize""
    value {
      list {
        i: 1
        i: 2
        i: 2
        i: 1
      }
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""VALID""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 2
        i: 2
        i: 1
      }
    }
  }
}
node {
  name: ""block3_conv1/random_uniform/shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\200\000\000\000\000\001\000\000""
      }
    }
  }
}
node {
  name: ""block3_conv1/random_uniform/min""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: -0.0416666679084
      }
    }
  }
}
node {
  name: ""block3_conv1/random_uniform/max""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0416666679084
      }
    }
  }
}
node {
  name: ""block3_conv1/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""block3_conv1/random_uniform/shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 8232814
    }
  }
}
node {
  name: ""block3_conv1/random_uniform/sub""
  op: ""Sub""
  input: ""block3_conv1/random_uniform/max""
  input: ""block3_conv1/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block3_conv1/random_uniform/mul""
  op: ""Mul""
  input: ""block3_conv1/random_uniform/RandomUniform""
  input: ""block3_conv1/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block3_conv1/random_uniform""
  op: ""Add""
  input: ""block3_conv1/random_uniform/mul""
  input: ""block3_conv1/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block3_conv1/kernel""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 128
        }
        dim {
          size: 256
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block3_conv1/kernel/Assign""
  op: ""Assign""
  input: ""block3_conv1/kernel""
  input: ""block3_conv1/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block3_conv1/kernel/read""
  op: ""Identity""
  input: ""block3_conv1/kernel""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv1/kernel""
      }
    }
  }
}
node {
  name: ""block3_conv1/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 256
          }
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""block3_conv1/bias""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 256
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block3_conv1/bias/Assign""
  op: ""Assign""
  input: ""block3_conv1/bias""
  input: ""block3_conv1/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block3_conv1/bias/read""
  op: ""Identity""
  input: ""block3_conv1/bias""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv1/bias""
      }
    }
  }
}
node {
  name: ""block3_conv1/convolution/Shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\200\000\000\000\000\001\000\000""
      }
    }
  }
}
node {
  name: ""block3_conv1/convolution/dilation_rate""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: ""\001\000\000\000\001\000\000\000""
      }
    }
  }
}
node {
  name: ""block3_conv1/convolution""
  op: ""Conv2D""
  input: ""block2_pool/MaxPool""
  input: ""block3_conv1/kernel/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""SAME""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 1
        i: 1
        i: 1
      }
    }
  }
  attr {
    key: ""use_cudnn_on_gpu""
    value {
      b: true
    }
  }
}
node {
  name: ""block3_conv1/BiasAdd""
  op: ""BiasAdd""
  input: ""block3_conv1/convolution""
  input: ""block3_conv1/bias/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}
node {
  name: ""block3_conv1/Relu""
  op: ""Relu""
  input: ""block3_conv1/BiasAdd""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block3_conv2/random_uniform/shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\000\001\000\000\000\001\000\000""
      }
    }
  }
}
node {
  name: ""block3_conv2/random_uniform/min""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: -0.0360843911767
      }
    }
  }
}
node {
  name: ""block3_conv2/random_uniform/max""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0360843911767
      }
    }
  }
}
node {
  name: ""block3_conv2/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""block3_conv2/random_uniform/shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 7964615
    }
  }
}
node {
  name: ""block3_conv2/random_uniform/sub""
  op: ""Sub""
  input: ""block3_conv2/random_uniform/max""
  input: ""block3_conv2/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block3_conv2/random_uniform/mul""
  op: ""Mul""
  input: ""block3_conv2/random_uniform/RandomUniform""
  input: ""block3_conv2/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block3_conv2/random_uniform""
  op: ""Add""
  input: ""block3_conv2/random_uniform/mul""
  input: ""block3_conv2/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block3_conv2/kernel""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 256
        }
        dim {
          size: 256
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block3_conv2/kernel/Assign""
  op: ""Assign""
  input: ""block3_conv2/kernel""
  input: ""block3_conv2/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block3_conv2/kernel/read""
  op: ""Identity""
  input: ""block3_conv2/kernel""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv2/kernel""
      }
    }
  }
}
node {
  name: ""block3_conv2/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 256
          }
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""block3_conv2/bias""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 256
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block3_conv2/bias/Assign""
  op: ""Assign""
  input: ""block3_conv2/bias""
  input: ""block3_conv2/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block3_conv2/bias/read""
  op: ""Identity""
  input: ""block3_conv2/bias""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv2/bias""
      }
    }
  }
}
node {
  name: ""block3_conv2/convolution/Shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\000\001\000\000\000\001\000\000""
      }
    }
  }
}
node {
  name: ""block3_conv2/convolution/dilation_rate""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: ""\001\000\000\000\001\000\000\000""
      }
    }
  }
}
node {
  name: ""block3_conv2/convolution""
  op: ""Conv2D""
  input: ""block3_conv1/Relu""
  input: ""block3_conv2/kernel/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""SAME""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 1
        i: 1
        i: 1
      }
    }
  }
  attr {
    key: ""use_cudnn_on_gpu""
    value {
      b: true
    }
  }
}
node {
  name: ""block3_conv2/BiasAdd""
  op: ""BiasAdd""
  input: ""block3_conv2/convolution""
  input: ""block3_conv2/bias/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}
node {
  name: ""block3_conv2/Relu""
  op: ""Relu""
  input: ""block3_conv2/BiasAdd""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block3_conv3/random_uniform/shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\000\001\000\000\000\001\000\000""
      }
    }
  }
}
node {
  name: ""block3_conv3/random_uniform/min""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: -0.0360843911767
      }
    }
  }
}
node {
  name: ""block3_conv3/random_uniform/max""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0360843911767
      }
    }
  }
}
node {
  name: ""block3_conv3/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""block3_conv3/random_uniform/shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 6435287
    }
  }
}
node {
  name: ""block3_conv3/random_uniform/sub""
  op: ""Sub""
  input: ""block3_conv3/random_uniform/max""
  input: ""block3_conv3/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block3_conv3/random_uniform/mul""
  op: ""Mul""
  input: ""block3_conv3/random_uniform/RandomUniform""
  input: ""block3_conv3/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block3_conv3/random_uniform""
  op: ""Add""
  input: ""block3_conv3/random_uniform/mul""
  input: ""block3_conv3/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block3_conv3/kernel""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 256
        }
        dim {
          size: 256
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block3_conv3/kernel/Assign""
  op: ""Assign""
  input: ""block3_conv3/kernel""
  input: ""block3_conv3/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv3/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block3_conv3/kernel/read""
  op: ""Identity""
  input: ""block3_conv3/kernel""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv3/kernel""
      }
    }
  }
}
node {
  name: ""block3_conv3/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 256
          }
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""block3_conv3/bias""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 256
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block3_conv3/bias/Assign""
  op: ""Assign""
  input: ""block3_conv3/bias""
  input: ""block3_conv3/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv3/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block3_conv3/bias/read""
  op: ""Identity""
  input: ""block3_conv3/bias""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv3/bias""
      }
    }
  }
}
node {
  name: ""block3_conv3/convolution/Shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\000\001\000\000\000\001\000\000""
      }
    }
  }
}
node {
  name: ""block3_conv3/convolution/dilation_rate""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: ""\001\000\000\000\001\000\000\000""
      }
    }
  }
}
node {
  name: ""block3_conv3/convolution""
  op: ""Conv2D""
  input: ""block3_conv2/Relu""
  input: ""block3_conv3/kernel/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""SAME""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 1
        i: 1
        i: 1
      }
    }
  }
  attr {
    key: ""use_cudnn_on_gpu""
    value {
      b: true
    }
  }
}
node {
  name: ""block3_conv3/BiasAdd""
  op: ""BiasAdd""
  input: ""block3_conv3/convolution""
  input: ""block3_conv3/bias/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}
node {
  name: ""block3_conv3/Relu""
  op: ""Relu""
  input: ""block3_conv3/BiasAdd""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block3_pool/MaxPool""
  op: ""MaxPool""
  input: ""block3_conv3/Relu""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""ksize""
    value {
      list {
        i: 1
        i: 2
        i: 2
        i: 1
      }
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""VALID""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 2
        i: 2
        i: 1
      }
    }
  }
}
node {
  name: ""block4_conv1/random_uniform/shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\000\001\000\000\000\002\000\000""
      }
    }
  }
}
node {
  name: ""block4_conv1/random_uniform/min""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: -0.0294627826661
      }
    }
  }
}
node {
  name: ""block4_conv1/random_uniform/max""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0294627826661
      }
    }
  }
}
node {
  name: ""block4_conv1/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""block4_conv1/random_uniform/shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 215528
    }
  }
}
node {
  name: ""block4_conv1/random_uniform/sub""
  op: ""Sub""
  input: ""block4_conv1/random_uniform/max""
  input: ""block4_conv1/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block4_conv1/random_uniform/mul""
  op: ""Mul""
  input: ""block4_conv1/random_uniform/RandomUniform""
  input: ""block4_conv1/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block4_conv1/random_uniform""
  op: ""Add""
  input: ""block4_conv1/random_uniform/mul""
  input: ""block4_conv1/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block4_conv1/kernel""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 256
        }
        dim {
          size: 512
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block4_conv1/kernel/Assign""
  op: ""Assign""
  input: ""block4_conv1/kernel""
  input: ""block4_conv1/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block4_conv1/kernel/read""
  op: ""Identity""
  input: ""block4_conv1/kernel""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv1/kernel""
      }
    }
  }
}
node {
  name: ""block4_conv1/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 512
          }
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""block4_conv1/bias""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 512
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block4_conv1/bias/Assign""
  op: ""Assign""
  input: ""block4_conv1/bias""
  input: ""block4_conv1/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block4_conv1/bias/read""
  op: ""Identity""
  input: ""block4_conv1/bias""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv1/bias""
      }
    }
  }
}
node {
  name: ""block4_conv1/convolution/Shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\000\001\000\000\000\002\000\000""
      }
    }
  }
}
node {
  name: ""block4_conv1/convolution/dilation_rate""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: ""\001\000\000\000\001\000\000\000""
      }
    }
  }
}
node {
  name: ""block4_conv1/convolution""
  op: ""Conv2D""
  input: ""block3_pool/MaxPool""
  input: ""block4_conv1/kernel/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""SAME""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 1
        i: 1
        i: 1
      }
    }
  }
  attr {
    key: ""use_cudnn_on_gpu""
    value {
      b: true
    }
  }
}
node {
  name: ""block4_conv1/BiasAdd""
  op: ""BiasAdd""
  input: ""block4_conv1/convolution""
  input: ""block4_conv1/bias/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}
node {
  name: ""block4_conv1/Relu""
  op: ""Relu""
  input: ""block4_conv1/BiasAdd""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block4_conv2/random_uniform/shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\000\002\000\000\000\002\000\000""
      }
    }
  }
}
node {
  name: ""block4_conv2/random_uniform/min""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: -0.0255155190825
      }
    }
  }
}
node {
  name: ""block4_conv2/random_uniform/max""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0255155190825
      }
    }
  }
}
node {
  name: ""block4_conv2/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""block4_conv2/random_uniform/shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 5791903
    }
  }
}
node {
  name: ""block4_conv2/random_uniform/sub""
  op: ""Sub""
  input: ""block4_conv2/random_uniform/max""
  input: ""block4_conv2/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block4_conv2/random_uniform/mul""
  op: ""Mul""
  input: ""block4_conv2/random_uniform/RandomUniform""
  input: ""block4_conv2/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block4_conv2/random_uniform""
  op: ""Add""
  input: ""block4_conv2/random_uniform/mul""
  input: ""block4_conv2/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block4_conv2/kernel""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 512
        }
        dim {
          size: 512
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block4_conv2/kernel/Assign""
  op: ""Assign""
  input: ""block4_conv2/kernel""
  input: ""block4_conv2/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block4_conv2/kernel/read""
  op: ""Identity""
  input: ""block4_conv2/kernel""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv2/kernel""
      }
    }
  }
}
node {
  name: ""block4_conv2/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 512
          }
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""block4_conv2/bias""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 512
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block4_conv2/bias/Assign""
  op: ""Assign""
  input: ""block4_conv2/bias""
  input: ""block4_conv2/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block4_conv2/bias/read""
  op: ""Identity""
  input: ""block4_conv2/bias""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv2/bias""
      }
    }
  }
}
node {
  name: ""block4_conv2/convolution/Shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\000\002\000\000\000\002\000\000""
      }
    }
  }
}
node {
  name: ""block4_conv2/convolution/dilation_rate""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: ""\001\000\000\000\001\000\000\000""
      }
    }
  }
}
node {
  name: ""block4_conv2/convolution""
  op: ""Conv2D""
  input: ""block4_conv1/Relu""
  input: ""block4_conv2/kernel/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""SAME""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 1
        i: 1
        i: 1
      }
    }
  }
  attr {
    key: ""use_cudnn_on_gpu""
    value {
      b: true
    }
  }
}
node {
  name: ""block4_conv2/BiasAdd""
  op: ""BiasAdd""
  input: ""block4_conv2/convolution""
  input: ""block4_conv2/bias/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}
node {
  name: ""block4_conv2/Relu""
  op: ""Relu""
  input: ""block4_conv2/BiasAdd""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block4_conv3/random_uniform/shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\000\002\000\000\000\002\000\000""
      }
    }
  }
}
node {
  name: ""block4_conv3/random_uniform/min""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: -0.0255155190825
      }
    }
  }
}
node {
  name: ""block4_conv3/random_uniform/max""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0255155190825
      }
    }
  }
}
node {
  name: ""block4_conv3/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""block4_conv3/random_uniform/shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 6240709
    }
  }
}
node {
  name: ""block4_conv3/random_uniform/sub""
  op: ""Sub""
  input: ""block4_conv3/random_uniform/max""
  input: ""block4_conv3/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block4_conv3/random_uniform/mul""
  op: ""Mul""
  input: ""block4_conv3/random_uniform/RandomUniform""
  input: ""block4_conv3/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block4_conv3/random_uniform""
  op: ""Add""
  input: ""block4_conv3/random_uniform/mul""
  input: ""block4_conv3/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block4_conv3/kernel""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 512
        }
        dim {
          size: 512
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block4_conv3/kernel/Assign""
  op: ""Assign""
  input: ""block4_conv3/kernel""
  input: ""block4_conv3/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv3/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block4_conv3/kernel/read""
  op: ""Identity""
  input: ""block4_conv3/kernel""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv3/kernel""
      }
    }
  }
}
node {
  name: ""block4_conv3/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 512
          }
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""block4_conv3/bias""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 512
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block4_conv3/bias/Assign""
  op: ""Assign""
  input: ""block4_conv3/bias""
  input: ""block4_conv3/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv3/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block4_conv3/bias/read""
  op: ""Identity""
  input: ""block4_conv3/bias""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv3/bias""
      }
    }
  }
}
node {
  name: ""block4_conv3/convolution/Shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\000\002\000\000\000\002\000\000""
      }
    }
  }
}
node {
  name: ""block4_conv3/convolution/dilation_rate""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: ""\001\000\000\000\001\000\000\000""
      }
    }
  }
}
node {
  name: ""block4_conv3/convolution""
  op: ""Conv2D""
  input: ""block4_conv2/Relu""
  input: ""block4_conv3/kernel/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""SAME""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 1
        i: 1
        i: 1
      }
    }
  }
  attr {
    key: ""use_cudnn_on_gpu""
    value {
      b: true
    }
  }
}
node {
  name: ""block4_conv3/BiasAdd""
  op: ""BiasAdd""
  input: ""block4_conv3/convolution""
  input: ""block4_conv3/bias/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}
node {
  name: ""block4_conv3/Relu""
  op: ""Relu""
  input: ""block4_conv3/BiasAdd""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block4_pool/MaxPool""
  op: ""MaxPool""
  input: ""block4_conv3/Relu""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""ksize""
    value {
      list {
        i: 1
        i: 2
        i: 2
        i: 1
      }
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""VALID""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 2
        i: 2
        i: 1
      }
    }
  }
}
node {
  name: ""block5_conv1/random_uniform/shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\000\002\000\000\000\002\000\000""
      }
    }
  }
}
node {
  name: ""block5_conv1/random_uniform/min""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: -0.0255155190825
      }
    }
  }
}
node {
  name: ""block5_conv1/random_uniform/max""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0255155190825
      }
    }
  }
}
node {
  name: ""block5_conv1/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""block5_conv1/random_uniform/shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 7151531
    }
  }
}
node {
  name: ""block5_conv1/random_uniform/sub""
  op: ""Sub""
  input: ""block5_conv1/random_uniform/max""
  input: ""block5_conv1/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block5_conv1/random_uniform/mul""
  op: ""Mul""
  input: ""block5_conv1/random_uniform/RandomUniform""
  input: ""block5_conv1/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block5_conv1/random_uniform""
  op: ""Add""
  input: ""block5_conv1/random_uniform/mul""
  input: ""block5_conv1/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block5_conv1/kernel""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 512
        }
        dim {
          size: 512
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block5_conv1/kernel/Assign""
  op: ""Assign""
  input: ""block5_conv1/kernel""
  input: ""block5_conv1/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block5_conv1/kernel/read""
  op: ""Identity""
  input: ""block5_conv1/kernel""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv1/kernel""
      }
    }
  }
}
node {
  name: ""block5_conv1/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 512
          }
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""block5_conv1/bias""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 512
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block5_conv1/bias/Assign""
  op: ""Assign""
  input: ""block5_conv1/bias""
  input: ""block5_conv1/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block5_conv1/bias/read""
  op: ""Identity""
  input: ""block5_conv1/bias""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv1/bias""
      }
    }
  }
}
node {
  name: ""block5_conv1/convolution/Shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\000\002\000\000\000\002\000\000""
      }
    }
  }
}
node {
  name: ""block5_conv1/convolution/dilation_rate""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: ""\001\000\000\000\001\000\000\000""
      }
    }
  }
}
node {
  name: ""block5_conv1/convolution""
  op: ""Conv2D""
  input: ""block4_pool/MaxPool""
  input: ""block5_conv1/kernel/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""SAME""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 1
        i: 1
        i: 1
      }
    }
  }
  attr {
    key: ""use_cudnn_on_gpu""
    value {
      b: true
    }
  }
}
node {
  name: ""block5_conv1/BiasAdd""
  op: ""BiasAdd""
  input: ""block5_conv1/convolution""
  input: ""block5_conv1/bias/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}
node {
  name: ""block5_conv1/Relu""
  op: ""Relu""
  input: ""block5_conv1/BiasAdd""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block5_conv2/random_uniform/shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\000\002\000\000\000\002\000\000""
      }
    }
  }
}
node {
  name: ""block5_conv2/random_uniform/min""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: -0.0255155190825
      }
    }
  }
}
node {
  name: ""block5_conv2/random_uniform/max""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0255155190825
      }
    }
  }
}
node {
  name: ""block5_conv2/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""block5_conv2/random_uniform/shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 4344407
    }
  }
}
node {
  name: ""block5_conv2/random_uniform/sub""
  op: ""Sub""
  input: ""block5_conv2/random_uniform/max""
  input: ""block5_conv2/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block5_conv2/random_uniform/mul""
  op: ""Mul""
  input: ""block5_conv2/random_uniform/RandomUniform""
  input: ""block5_conv2/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block5_conv2/random_uniform""
  op: ""Add""
  input: ""block5_conv2/random_uniform/mul""
  input: ""block5_conv2/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block5_conv2/kernel""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 512
        }
        dim {
          size: 512
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block5_conv2/kernel/Assign""
  op: ""Assign""
  input: ""block5_conv2/kernel""
  input: ""block5_conv2/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block5_conv2/kernel/read""
  op: ""Identity""
  input: ""block5_conv2/kernel""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv2/kernel""
      }
    }
  }
}
node {
  name: ""block5_conv2/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 512
          }
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""block5_conv2/bias""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 512
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block5_conv2/bias/Assign""
  op: ""Assign""
  input: ""block5_conv2/bias""
  input: ""block5_conv2/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block5_conv2/bias/read""
  op: ""Identity""
  input: ""block5_conv2/bias""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv2/bias""
      }
    }
  }
}
node {
  name: ""block5_conv2/convolution/Shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\000\002\000\000\000\002\000\000""
      }
    }
  }
}
node {
  name: ""block5_conv2/convolution/dilation_rate""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: ""\001\000\000\000\001\000\000\000""
      }
    }
  }
}
node {
  name: ""block5_conv2/convolution""
  op: ""Conv2D""
  input: ""block5_conv1/Relu""
  input: ""block5_conv2/kernel/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""SAME""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 1
        i: 1
        i: 1
      }
    }
  }
  attr {
    key: ""use_cudnn_on_gpu""
    value {
      b: true
    }
  }
}
node {
  name: ""block5_conv2/BiasAdd""
  op: ""BiasAdd""
  input: ""block5_conv2/convolution""
  input: ""block5_conv2/bias/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}
node {
  name: ""block5_conv2/Relu""
  op: ""Relu""
  input: ""block5_conv2/BiasAdd""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block5_conv3/random_uniform/shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\000\002\000\000\000\002\000\000""
      }
    }
  }
}
node {
  name: ""block5_conv3/random_uniform/min""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: -0.0255155190825
      }
    }
  }
}
node {
  name: ""block5_conv3/random_uniform/max""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0255155190825
      }
    }
  }
}
node {
  name: ""block5_conv3/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""block5_conv3/random_uniform/shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 3686380
    }
  }
}
node {
  name: ""block5_conv3/random_uniform/sub""
  op: ""Sub""
  input: ""block5_conv3/random_uniform/max""
  input: ""block5_conv3/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block5_conv3/random_uniform/mul""
  op: ""Mul""
  input: ""block5_conv3/random_uniform/RandomUniform""
  input: ""block5_conv3/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block5_conv3/random_uniform""
  op: ""Add""
  input: ""block5_conv3/random_uniform/mul""
  input: ""block5_conv3/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block5_conv3/kernel""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 512
        }
        dim {
          size: 512
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block5_conv3/kernel/Assign""
  op: ""Assign""
  input: ""block5_conv3/kernel""
  input: ""block5_conv3/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv3/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block5_conv3/kernel/read""
  op: ""Identity""
  input: ""block5_conv3/kernel""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv3/kernel""
      }
    }
  }
}
node {
  name: ""block5_conv3/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 512
          }
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""block5_conv3/bias""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 512
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""block5_conv3/bias/Assign""
  op: ""Assign""
  input: ""block5_conv3/bias""
  input: ""block5_conv3/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv3/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""block5_conv3/bias/read""
  op: ""Identity""
  input: ""block5_conv3/bias""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv3/bias""
      }
    }
  }
}
node {
  name: ""block5_conv3/convolution/Shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: ""\003\000\000\000\003\000\000\000\000\002\000\000\000\002\000\000""
      }
    }
  }
}
node {
  name: ""block5_conv3/convolution/dilation_rate""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: ""\001\000\000\000\001\000\000\000""
      }
    }
  }
}
node {
  name: ""block5_conv3/convolution""
  op: ""Conv2D""
  input: ""block5_conv2/Relu""
  input: ""block5_conv3/kernel/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""SAME""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 1
        i: 1
        i: 1
      }
    }
  }
  attr {
    key: ""use_cudnn_on_gpu""
    value {
      b: true
    }
  }
}
node {
  name: ""block5_conv3/BiasAdd""
  op: ""BiasAdd""
  input: ""block5_conv3/convolution""
  input: ""block5_conv3/bias/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}
node {
  name: ""block5_conv3/Relu""
  op: ""Relu""
  input: ""block5_conv3/BiasAdd""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""block5_pool/MaxPool""
  op: ""MaxPool""
  input: ""block5_conv3/Relu""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
  attr {
    key: ""ksize""
    value {
      list {
        i: 1
        i: 2
        i: 2
        i: 1
      }
    }
  }
  attr {
    key: ""padding""
    value {
      s: ""VALID""
    }
  }
  attr {
    key: ""strides""
    value {
      list {
        i: 1
        i: 2
        i: 2
        i: 1
      }
    }
  }
}
node {
  name: ""Placeholder""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 64
        }
      }
    }
  }
}
node {
  name: ""Assign""
  op: ""Assign""
  input: ""block1_conv1/kernel""
  input: ""Placeholder""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block1_conv1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_1""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 64
        }
      }
    }
  }
}
node {
  name: ""Assign_1""
  op: ""Assign""
  input: ""block1_conv1/bias""
  input: ""Placeholder_1""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block1_conv1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_2""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 64
        }
        dim {
          size: 64
        }
      }
    }
  }
}
node {
  name: ""Assign_2""
  op: ""Assign""
  input: ""block1_conv2/kernel""
  input: ""Placeholder_2""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block1_conv2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_3""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 64
        }
      }
    }
  }
}
node {
  name: ""Assign_3""
  op: ""Assign""
  input: ""block1_conv2/bias""
  input: ""Placeholder_3""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block1_conv2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_4""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 64
        }
        dim {
          size: 128
        }
      }
    }
  }
}
node {
  name: ""Assign_4""
  op: ""Assign""
  input: ""block2_conv1/kernel""
  input: ""Placeholder_4""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block2_conv1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_5""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 128
        }
      }
    }
  }
}
node {
  name: ""Assign_5""
  op: ""Assign""
  input: ""block2_conv1/bias""
  input: ""Placeholder_5""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block2_conv1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_6""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 128
        }
        dim {
          size: 128
        }
      }
    }
  }
}
node {
  name: ""Assign_6""
  op: ""Assign""
  input: ""block2_conv2/kernel""
  input: ""Placeholder_6""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block2_conv2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_7""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 128
        }
      }
    }
  }
}
node {
  name: ""Assign_7""
  op: ""Assign""
  input: ""block2_conv2/bias""
  input: ""Placeholder_7""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block2_conv2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_8""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 128
        }
        dim {
          size: 256
        }
      }
    }
  }
}
node {
  name: ""Assign_8""
  op: ""Assign""
  input: ""block3_conv1/kernel""
  input: ""Placeholder_8""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_9""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 256
        }
      }
    }
  }
}
node {
  name: ""Assign_9""
  op: ""Assign""
  input: ""block3_conv1/bias""
  input: ""Placeholder_9""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_10""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 256
        }
        dim {
          size: 256
        }
      }
    }
  }
}
node {
  name: ""Assign_10""
  op: ""Assign""
  input: ""block3_conv2/kernel""
  input: ""Placeholder_10""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_11""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 256
        }
      }
    }
  }
}
node {
  name: ""Assign_11""
  op: ""Assign""
  input: ""block3_conv2/bias""
  input: ""Placeholder_11""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_12""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 256
        }
        dim {
          size: 256
        }
      }
    }
  }
}
node {
  name: ""Assign_12""
  op: ""Assign""
  input: ""block3_conv3/kernel""
  input: ""Placeholder_12""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv3/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_13""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 256
        }
      }
    }
  }
}
node {
  name: ""Assign_13""
  op: ""Assign""
  input: ""block3_conv3/bias""
  input: ""Placeholder_13""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv3/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_14""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 256
        }
        dim {
          size: 512
        }
      }
    }
  }
}
node {
  name: ""Assign_14""
  op: ""Assign""
  input: ""block4_conv1/kernel""
  input: ""Placeholder_14""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_15""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 512
        }
      }
    }
  }
}
node {
  name: ""Assign_15""
  op: ""Assign""
  input: ""block4_conv1/bias""
  input: ""Placeholder_15""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_16""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 512
        }
        dim {
          size: 512
        }
      }
    }
  }
}
node {
  name: ""Assign_16""
  op: ""Assign""
  input: ""block4_conv2/kernel""
  input: ""Placeholder_16""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_17""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 512
        }
      }
    }
  }
}
node {
  name: ""Assign_17""
  op: ""Assign""
  input: ""block4_conv2/bias""
  input: ""Placeholder_17""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_18""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 512
        }
        dim {
          size: 512
        }
      }
    }
  }
}
node {
  name: ""Assign_18""
  op: ""Assign""
  input: ""block4_conv3/kernel""
  input: ""Placeholder_18""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv3/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_19""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 512
        }
      }
    }
  }
}
node {
  name: ""Assign_19""
  op: ""Assign""
  input: ""block4_conv3/bias""
  input: ""Placeholder_19""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv3/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_20""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 512
        }
        dim {
          size: 512
        }
      }
    }
  }
}
node {
  name: ""Assign_20""
  op: ""Assign""
  input: ""block5_conv1/kernel""
  input: ""Placeholder_20""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_21""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 512
        }
      }
    }
  }
}
node {
  name: ""Assign_21""
  op: ""Assign""
  input: ""block5_conv1/bias""
  input: ""Placeholder_21""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_22""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 512
        }
        dim {
          size: 512
        }
      }
    }
  }
}
node {
  name: ""Assign_22""
  op: ""Assign""
  input: ""block5_conv2/kernel""
  input: ""Placeholder_22""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_23""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 512
        }
      }
    }
  }
}
node {
  name: ""Assign_23""
  op: ""Assign""
  input: ""block5_conv2/bias""
  input: ""Placeholder_23""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_24""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 3
        }
        dim {
          size: 3
        }
        dim {
          size: 512
        }
        dim {
          size: 512
        }
      }
    }
  }
}
node {
  name: ""Assign_24""
  op: ""Assign""
  input: ""block5_conv3/kernel""
  input: ""Placeholder_24""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv3/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""Placeholder_25""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 512
        }
      }
    }
  }
}
node {
  name: ""Assign_25""
  op: ""Assign""
  input: ""block5_conv3/bias""
  input: ""Placeholder_25""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv3/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: false
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""init""
  op: ""NoOp""
  input: ""^block1_conv1/kernel/Assign""
  input: ""^block1_conv1/bias/Assign""
  input: ""^block1_conv2/kernel/Assign""
  input: ""^block1_conv2/bias/Assign""
  input: ""^block2_conv1/kernel/Assign""
  input: ""^block2_conv1/bias/Assign""
  input: ""^block2_conv2/kernel/Assign""
  input: ""^block2_conv2/bias/Assign""
  input: ""^block3_conv1/kernel/Assign""
  input: ""^block3_conv1/bias/Assign""
  input: ""^block3_conv2/kernel/Assign""
  input: ""^block3_conv2/bias/Assign""
  input: ""^block3_conv3/kernel/Assign""
  input: ""^block3_conv3/bias/Assign""
  input: ""^block4_conv1/kernel/Assign""
  input: ""^block4_conv1/bias/Assign""
  input: ""^block4_conv2/kernel/Assign""
  input: ""^block4_conv2/bias/Assign""
  input: ""^block4_conv3/kernel/Assign""
  input: ""^block4_conv3/bias/Assign""
  input: ""^block5_conv1/kernel/Assign""
  input: ""^block5_conv1/bias/Assign""
  input: ""^block5_conv2/kernel/Assign""
  input: ""^block5_conv2/bias/Assign""
  input: ""^block5_conv3/kernel/Assign""
  input: ""^block5_conv3/bias/Assign""
}
node {
  name: ""flatten_1/Shape""
  op: ""Shape""
  input: ""block5_pool/MaxPool""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""out_type""
    value {
      type: DT_INT32
    }
  }
}
node {
  name: ""flatten_1/strided_slice/stack""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 1
          }
        }
        int_val: 1
      }
    }
  }
}
node {
  name: ""flatten_1/strided_slice/stack_1""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 1
          }
        }
        int_val: 0
      }
    }
  }
}
node {
  name: ""flatten_1/strided_slice/stack_2""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 1
          }
        }
        int_val: 1
      }
    }
  }
}
node {
  name: ""flatten_1/strided_slice""
  op: ""StridedSlice""
  input: ""flatten_1/Shape""
  input: ""flatten_1/strided_slice/stack""
  input: ""flatten_1/strided_slice/stack_1""
  input: ""flatten_1/strided_slice/stack_2""
  attr {
    key: ""Index""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""begin_mask""
    value {
      i: 0
    }
  }
  attr {
    key: ""ellipsis_mask""
    value {
      i: 0
    }
  }
  attr {
    key: ""end_mask""
    value {
      i: 1
    }
  }
  attr {
    key: ""new_axis_mask""
    value {
      i: 0
    }
  }
  attr {
    key: ""shrink_axis_mask""
    value {
      i: 0
    }
  }
}
node {
  name: ""flatten_1/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 1
          }
        }
        int_val: 0
      }
    }
  }
}
node {
  name: ""flatten_1/Prod""
  op: ""Prod""
  input: ""flatten_1/strided_slice""
  input: ""flatten_1/Const""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""Tidx""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""keep_dims""
    value {
      b: false
    }
  }
}
node {
  name: ""flatten_1/stack/0""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
        }
        int_val: -1
      }
    }
  }
}
node {
  name: ""flatten_1/stack""
  op: ""Pack""
  input: ""flatten_1/stack/0""
  input: ""flatten_1/Prod""
  attr {
    key: ""N""
    value {
      i: 2
    }
  }
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""axis""
    value {
      i: 0
    }
  }
}
node {
  name: ""flatten_1/Reshape""
  op: ""Reshape""
  input: ""block5_pool/MaxPool""
  input: ""flatten_1/stack""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""Tshape""
    value {
      type: DT_INT32
    }
  }
}
node {
  name: ""dense_1/random_uniform/shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: ""\000\002\000\000\000\004\000\000""
      }
    }
  }
}
node {
  name: ""dense_1/random_uniform/min""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: -0.0625
      }
    }
  }
}
node {
  name: ""dense_1/random_uniform/max""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0625
      }
    }
  }
}
node {
  name: ""dense_1/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""dense_1/random_uniform/shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 9805962
    }
  }
}
node {
  name: ""dense_1/random_uniform/sub""
  op: ""Sub""
  input: ""dense_1/random_uniform/max""
  input: ""dense_1/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dense_1/random_uniform/mul""
  op: ""Mul""
  input: ""dense_1/random_uniform/RandomUniform""
  input: ""dense_1/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dense_1/random_uniform""
  op: ""Add""
  input: ""dense_1/random_uniform/mul""
  input: ""dense_1/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dense_1/kernel""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 512
        }
        dim {
          size: 1024
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""dense_1/kernel/Assign""
  op: ""Assign""
  input: ""dense_1/kernel""
  input: ""dense_1/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""dense_1/kernel/read""
  op: ""Identity""
  input: ""dense_1/kernel""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_1/kernel""
      }
    }
  }
}
node {
  name: ""dense_1/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 1024
          }
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""dense_1/bias""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 1024
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""dense_1/bias/Assign""
  op: ""Assign""
  input: ""dense_1/bias""
  input: ""dense_1/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""dense_1/bias/read""
  op: ""Identity""
  input: ""dense_1/bias""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_1/bias""
      }
    }
  }
}
node {
  name: ""dense_1/MatMul""
  op: ""MatMul""
  input: ""flatten_1/Reshape""
  input: ""dense_1/kernel/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""transpose_a""
    value {
      b: false
    }
  }
  attr {
    key: ""transpose_b""
    value {
      b: false
    }
  }
}
node {
  name: ""dense_1/BiasAdd""
  op: ""BiasAdd""
  input: ""dense_1/MatMul""
  input: ""dense_1/bias/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}
node {
  name: ""dense_1/Relu""
  op: ""Relu""
  input: ""dense_1/BiasAdd""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dropout_1/keras_learning_phase""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_BOOL
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        unknown_rank: true
      }
    }
  }
}
node {
  name: ""dropout_1/cond/Switch""
  op: ""Switch""
  input: ""dropout_1/keras_learning_phase""
  input: ""dropout_1/keras_learning_phase""
  attr {
    key: ""T""
    value {
      type: DT_BOOL
    }
  }
}
node {
  name: ""dropout_1/cond/switch_t""
  op: ""Identity""
  input: ""dropout_1/cond/Switch:1""
  attr {
    key: ""T""
    value {
      type: DT_BOOL
    }
  }
}
node {
  name: ""dropout_1/cond/switch_f""
  op: ""Identity""
  input: ""dropout_1/cond/Switch""
  attr {
    key: ""T""
    value {
      type: DT_BOOL
    }
  }
}
node {
  name: ""dropout_1/cond/pred_id""
  op: ""Identity""
  input: ""dropout_1/keras_learning_phase""
  attr {
    key: ""T""
    value {
      type: DT_BOOL
    }
  }
}
node {
  name: ""dropout_1/cond/mul/y""
  op: ""Const""
  input: ""^dropout_1/cond/switch_t""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 1.0
      }
    }
  }
}
node {
  name: ""dropout_1/cond/mul/Switch""
  op: ""Switch""
  input: ""dense_1/Relu""
  input: ""dropout_1/cond/pred_id""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_1/Relu""
      }
    }
  }
}
node {
  name: ""dropout_1/cond/mul""
  op: ""Mul""
  input: ""dropout_1/cond/mul/Switch:1""
  input: ""dropout_1/cond/mul/y""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dropout_1/cond/dropout/keep_prob""
  op: ""Const""
  input: ""^dropout_1/cond/switch_t""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.5
      }
    }
  }
}
node {
  name: ""dropout_1/cond/dropout/Shape""
  op: ""Shape""
  input: ""dropout_1/cond/mul""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""out_type""
    value {
      type: DT_INT32
    }
  }
}
node {
  name: ""dropout_1/cond/dropout/random_uniform/min""
  op: ""Const""
  input: ""^dropout_1/cond/switch_t""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""dropout_1/cond/dropout/random_uniform/max""
  op: ""Const""
  input: ""^dropout_1/cond/switch_t""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 1.0
      }
    }
  }
}
node {
  name: ""dropout_1/cond/dropout/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""dropout_1/cond/dropout/Shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 4883238
    }
  }
}
node {
  name: ""dropout_1/cond/dropout/random_uniform/sub""
  op: ""Sub""
  input: ""dropout_1/cond/dropout/random_uniform/max""
  input: ""dropout_1/cond/dropout/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dropout_1/cond/dropout/random_uniform/mul""
  op: ""Mul""
  input: ""dropout_1/cond/dropout/random_uniform/RandomUniform""
  input: ""dropout_1/cond/dropout/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dropout_1/cond/dropout/random_uniform""
  op: ""Add""
  input: ""dropout_1/cond/dropout/random_uniform/mul""
  input: ""dropout_1/cond/dropout/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dropout_1/cond/dropout/add""
  op: ""Add""
  input: ""dropout_1/cond/dropout/keep_prob""
  input: ""dropout_1/cond/dropout/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dropout_1/cond/dropout/Floor""
  op: ""Floor""
  input: ""dropout_1/cond/dropout/add""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dropout_1/cond/dropout/div""
  op: ""RealDiv""
  input: ""dropout_1/cond/mul""
  input: ""dropout_1/cond/dropout/keep_prob""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dropout_1/cond/dropout/mul""
  op: ""Mul""
  input: ""dropout_1/cond/dropout/div""
  input: ""dropout_1/cond/dropout/Floor""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dropout_1/cond/Switch_1""
  op: ""Switch""
  input: ""dense_1/Relu""
  input: ""dropout_1/cond/pred_id""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_1/Relu""
      }
    }
  }
}
node {
  name: ""dropout_1/cond/Merge""
  op: ""Merge""
  input: ""dropout_1/cond/Switch_1""
  input: ""dropout_1/cond/dropout/mul""
  attr {
    key: ""N""
    value {
      i: 2
    }
  }
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dense_2/random_uniform/shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: ""\000\004\000\000\000\004\000\000""
      }
    }
  }
}
node {
  name: ""dense_2/random_uniform/min""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: -0.0541265867651
      }
    }
  }
}
node {
  name: ""dense_2/random_uniform/max""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0541265867651
      }
    }
  }
}
node {
  name: ""dense_2/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""dense_2/random_uniform/shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 3506239
    }
  }
}
node {
  name: ""dense_2/random_uniform/sub""
  op: ""Sub""
  input: ""dense_2/random_uniform/max""
  input: ""dense_2/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dense_2/random_uniform/mul""
  op: ""Mul""
  input: ""dense_2/random_uniform/RandomUniform""
  input: ""dense_2/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dense_2/random_uniform""
  op: ""Add""
  input: ""dense_2/random_uniform/mul""
  input: ""dense_2/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dense_2/kernel""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 1024
        }
        dim {
          size: 1024
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""dense_2/kernel/Assign""
  op: ""Assign""
  input: ""dense_2/kernel""
  input: ""dense_2/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""dense_2/kernel/read""
  op: ""Identity""
  input: ""dense_2/kernel""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_2/kernel""
      }
    }
  }
}
node {
  name: ""dense_2/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 1024
          }
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""dense_2/bias""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 1024
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""dense_2/bias/Assign""
  op: ""Assign""
  input: ""dense_2/bias""
  input: ""dense_2/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""dense_2/bias/read""
  op: ""Identity""
  input: ""dense_2/bias""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_2/bias""
      }
    }
  }
}
node {
  name: ""dense_2/MatMul""
  op: ""MatMul""
  input: ""dropout_1/cond/Merge""
  input: ""dense_2/kernel/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""transpose_a""
    value {
      b: false
    }
  }
  attr {
    key: ""transpose_b""
    value {
      b: false
    }
  }
}
node {
  name: ""dense_2/BiasAdd""
  op: ""BiasAdd""
  input: ""dense_2/MatMul""
  input: ""dense_2/bias/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}
node {
  name: ""dense_2/Relu""
  op: ""Relu""
  input: ""dense_2/BiasAdd""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dense_3/random_uniform/shape""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: ""\000\004\000\000\n\000\000\000""
      }
    }
  }
}
node {
  name: ""dense_3/random_uniform/min""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: -0.0761755108833
      }
    }
  }
}
node {
  name: ""dense_3/random_uniform/max""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0761755108833
      }
    }
  }
}
node {
  name: ""dense_3/random_uniform/RandomUniform""
  op: ""RandomUniform""
  input: ""dense_3/random_uniform/shape""
  attr {
    key: ""T""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""seed""
    value {
      i: 87654321
    }
  }
  attr {
    key: ""seed2""
    value {
      i: 1428162
    }
  }
}
node {
  name: ""dense_3/random_uniform/sub""
  op: ""Sub""
  input: ""dense_3/random_uniform/max""
  input: ""dense_3/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dense_3/random_uniform/mul""
  op: ""Mul""
  input: ""dense_3/random_uniform/RandomUniform""
  input: ""dense_3/random_uniform/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dense_3/random_uniform""
  op: ""Add""
  input: ""dense_3/random_uniform/mul""
  input: ""dense_3/random_uniform/min""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""dense_3/kernel""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 1024
        }
        dim {
          size: 10
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""dense_3/kernel/Assign""
  op: ""Assign""
  input: ""dense_3/kernel""
  input: ""dense_3/random_uniform""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_3/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""dense_3/kernel/read""
  op: ""Identity""
  input: ""dense_3/kernel""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_3/kernel""
      }
    }
  }
}
node {
  name: ""dense_3/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 10
          }
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""dense_3/bias""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: 10
        }
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""dense_3/bias/Assign""
  op: ""Assign""
  input: ""dense_3/bias""
  input: ""dense_3/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_3/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""dense_3/bias/read""
  op: ""Identity""
  input: ""dense_3/bias""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_3/bias""
      }
    }
  }
}
node {
  name: ""dense_3/MatMul""
  op: ""MatMul""
  input: ""dense_2/Relu""
  input: ""dense_3/kernel/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""transpose_a""
    value {
      b: false
    }
  }
  attr {
    key: ""transpose_b""
    value {
      b: false
    }
  }
}
node {
  name: ""dense_3/BiasAdd""
  op: ""BiasAdd""
  input: ""dense_3/MatMul""
  input: ""dense_3/bias/read""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""data_format""
    value {
      s: ""NHWC""
    }
  }
}
node {
  name: ""dense_3/Softmax""
  op: ""Softmax""
  input: ""dense_3/BiasAdd""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""SGD/iterations/initial_value""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT64
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT64
        tensor_shape {
        }
        int64_val: 0
      }
    }
  }
}
node {
  name: ""SGD/iterations""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_INT64
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""SGD/iterations/Assign""
  op: ""Assign""
  input: ""SGD/iterations""
  input: ""SGD/iterations/initial_value""
  attr {
    key: ""T""
    value {
      type: DT_INT64
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@SGD/iterations""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""SGD/iterations/read""
  op: ""Identity""
  input: ""SGD/iterations""
  attr {
    key: ""T""
    value {
      type: DT_INT64
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@SGD/iterations""
      }
    }
  }
}
node {
  name: ""SGD/lr/initial_value""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 9.99999974738e-05
      }
    }
  }
}
node {
  name: ""SGD/lr""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""SGD/lr/Assign""
  op: ""Assign""
  input: ""SGD/lr""
  input: ""SGD/lr/initial_value""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@SGD/lr""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""SGD/lr/read""
  op: ""Identity""
  input: ""SGD/lr""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@SGD/lr""
      }
    }
  }
}
node {
  name: ""SGD/momentum/initial_value""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.899999976158
      }
    }
  }
}
node {
  name: ""SGD/momentum""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""SGD/momentum/Assign""
  op: ""Assign""
  input: ""SGD/momentum""
  input: ""SGD/momentum/initial_value""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@SGD/momentum""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""SGD/momentum/read""
  op: ""Identity""
  input: ""SGD/momentum""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@SGD/momentum""
      }
    }
  }
}
node {
  name: ""SGD/decay/initial_value""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""SGD/decay""
  op: ""VariableV2""
  attr {
    key: ""container""
    value {
      s: """"
    }
  }
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
      }
    }
  }
  attr {
    key: ""shared_name""
    value {
      s: """"
    }
  }
}
node {
  name: ""SGD/decay/Assign""
  op: ""Assign""
  input: ""SGD/decay""
  input: ""SGD/decay/initial_value""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@SGD/decay""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""SGD/decay/read""
  op: ""Identity""
  input: ""SGD/decay""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@SGD/decay""
      }
    }
  }
}
node {
  name: ""dense_3_target""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: -1
        }
        dim {
          size: -1
        }
      }
    }
  }
}
node {
  name: ""dense_3_sample_weights""
  op: ""Placeholder""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""shape""
    value {
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
node {
  name: ""loss/dense_3_loss/Sum/reduction_indices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
        }
        int_val: 1
      }
    }
  }
}
node {
  name: ""loss/dense_3_loss/Sum""
  op: ""Sum""
  input: ""dense_3/Softmax""
  input: ""loss/dense_3_loss/Sum/reduction_indices""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""Tidx""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""keep_dims""
    value {
      b: true
    }
  }
}
node {
  name: ""loss/dense_3_loss/div""
  op: ""RealDiv""
  input: ""dense_3/Softmax""
  input: ""loss/dense_3_loss/Sum""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""loss/dense_3_loss/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 1.00000001169e-07
      }
    }
  }
}
node {
  name: ""loss/dense_3_loss/sub/x""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 1.0
      }
    }
  }
}
node {
  name: ""loss/dense_3_loss/sub""
  op: ""Sub""
  input: ""loss/dense_3_loss/sub/x""
  input: ""loss/dense_3_loss/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""loss/dense_3_loss/clip_by_value/Minimum""
  op: ""Minimum""
  input: ""loss/dense_3_loss/div""
  input: ""loss/dense_3_loss/sub""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""loss/dense_3_loss/clip_by_value""
  op: ""Maximum""
  input: ""loss/dense_3_loss/clip_by_value/Minimum""
  input: ""loss/dense_3_loss/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""loss/dense_3_loss/Log""
  op: ""Log""
  input: ""loss/dense_3_loss/clip_by_value""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""loss/dense_3_loss/mul""
  op: ""Mul""
  input: ""dense_3_target""
  input: ""loss/dense_3_loss/Log""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""loss/dense_3_loss/Sum_1/reduction_indices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
        }
        int_val: 1
      }
    }
  }
}
node {
  name: ""loss/dense_3_loss/Sum_1""
  op: ""Sum""
  input: ""loss/dense_3_loss/mul""
  input: ""loss/dense_3_loss/Sum_1/reduction_indices""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""Tidx""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""keep_dims""
    value {
      b: false
    }
  }
}
node {
  name: ""loss/dense_3_loss/Neg""
  op: ""Neg""
  input: ""loss/dense_3_loss/Sum_1""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""loss/dense_3_loss/Mean/reduction_indices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
          }
        }
      }
    }
  }
}
node {
  name: ""loss/dense_3_loss/Mean""
  op: ""Mean""
  input: ""loss/dense_3_loss/Neg""
  input: ""loss/dense_3_loss/Mean/reduction_indices""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""Tidx""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""keep_dims""
    value {
      b: false
    }
  }
}
node {
  name: ""loss/dense_3_loss/mul_1""
  op: ""Mul""
  input: ""loss/dense_3_loss/Mean""
  input: ""dense_3_sample_weights""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""loss/dense_3_loss/NotEqual/y""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 0.0
      }
    }
  }
}
node {
  name: ""loss/dense_3_loss/NotEqual""
  op: ""NotEqual""
  input: ""dense_3_sample_weights""
  input: ""loss/dense_3_loss/NotEqual/y""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""loss/dense_3_loss/Cast""
  op: ""Cast""
  input: ""loss/dense_3_loss/NotEqual""
  attr {
    key: ""DstT""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""SrcT""
    value {
      type: DT_BOOL
    }
  }
}
node {
  name: ""loss/dense_3_loss/Const_1""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 1
          }
        }
        int_val: 0
      }
    }
  }
}
node {
  name: ""loss/dense_3_loss/Mean_1""
  op: ""Mean""
  input: ""loss/dense_3_loss/Cast""
  input: ""loss/dense_3_loss/Const_1""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""Tidx""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""keep_dims""
    value {
      b: false
    }
  }
}
node {
  name: ""loss/dense_3_loss/div_1""
  op: ""RealDiv""
  input: ""loss/dense_3_loss/mul_1""
  input: ""loss/dense_3_loss/Mean_1""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""loss/dense_3_loss/Const_2""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 1
          }
        }
        int_val: 0
      }
    }
  }
}
node {
  name: ""loss/dense_3_loss/Mean_2""
  op: ""Mean""
  input: ""loss/dense_3_loss/div_1""
  input: ""loss/dense_3_loss/Const_2""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""Tidx""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""keep_dims""
    value {
      b: false
    }
  }
}
node {
  name: ""loss/mul/x""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
        }
        float_val: 1.0
      }
    }
  }
}
node {
  name: ""loss/mul""
  op: ""Mul""
  input: ""loss/mul/x""
  input: ""loss/dense_3_loss/Mean_2""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""metrics/acc/ArgMax/dimension""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
        }
        int_val: -1
      }
    }
  }
}
node {
  name: ""metrics/acc/ArgMax""
  op: ""ArgMax""
  input: ""dense_3_target""
  input: ""metrics/acc/ArgMax/dimension""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""Tidx""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""output_type""
    value {
      type: DT_INT64
    }
  }
}
node {
  name: ""metrics/acc/ArgMax_1/dimension""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
        }
        int_val: -1
      }
    }
  }
}
node {
  name: ""metrics/acc/ArgMax_1""
  op: ""ArgMax""
  input: ""dense_3/Softmax""
  input: ""metrics/acc/ArgMax_1/dimension""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""Tidx""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""output_type""
    value {
      type: DT_INT64
    }
  }
}
node {
  name: ""metrics/acc/Equal""
  op: ""Equal""
  input: ""metrics/acc/ArgMax""
  input: ""metrics/acc/ArgMax_1""
  attr {
    key: ""T""
    value {
      type: DT_INT64
    }
  }
}
node {
  name: ""metrics/acc/Cast""
  op: ""Cast""
  input: ""metrics/acc/Equal""
  attr {
    key: ""DstT""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""SrcT""
    value {
      type: DT_BOOL
    }
  }
}
node {
  name: ""metrics/acc/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 1
          }
        }
        int_val: 0
      }
    }
  }
}
node {
  name: ""metrics/acc/Mean""
  op: ""Mean""
  input: ""metrics/acc/Cast""
  input: ""metrics/acc/Const""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""Tidx""
    value {
      type: DT_INT32
    }
  }
  attr {
    key: ""keep_dims""
    value {
      b: false
    }
  }
}
node {
  name: ""save/Const""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
        }
        string_val: ""model""
      }
    }
  }
}
node {
  name: ""save/SaveV2/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 36
          }
        }
        string_val: ""SGD/decay""
        string_val: ""SGD/iterations""
        string_val: ""SGD/lr""
        string_val: ""SGD/momentum""
        string_val: ""block1_conv1/bias""
        string_val: ""block1_conv1/kernel""
        string_val: ""block1_conv2/bias""
        string_val: ""block1_conv2/kernel""
        string_val: ""block2_conv1/bias""
        string_val: ""block2_conv1/kernel""
        string_val: ""block2_conv2/bias""
        string_val: ""block2_conv2/kernel""
        string_val: ""block3_conv1/bias""
        string_val: ""block3_conv1/kernel""
        string_val: ""block3_conv2/bias""
        string_val: ""block3_conv2/kernel""
        string_val: ""block3_conv3/bias""
        string_val: ""block3_conv3/kernel""
        string_val: ""block4_conv1/bias""
        string_val: ""block4_conv1/kernel""
        string_val: ""block4_conv2/bias""
        string_val: ""block4_conv2/kernel""
        string_val: ""block4_conv3/bias""
        string_val: ""block4_conv3/kernel""
        string_val: ""block5_conv1/bias""
        string_val: ""block5_conv1/kernel""
        string_val: ""block5_conv2/bias""
        string_val: ""block5_conv2/kernel""
        string_val: ""block5_conv3/bias""
        string_val: ""block5_conv3/kernel""
        string_val: ""dense_1/bias""
        string_val: ""dense_1/kernel""
        string_val: ""dense_2/bias""
        string_val: ""dense_2/kernel""
        string_val: ""dense_3/bias""
        string_val: ""dense_3/kernel""
      }
    }
  }
}
node {
  name: ""save/SaveV2/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 36
          }
        }
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/SaveV2""
  op: ""SaveV2""
  input: ""save/Const""
  input: ""save/SaveV2/tensor_names""
  input: ""save/SaveV2/shape_and_slices""
  input: ""SGD/decay""
  input: ""SGD/iterations""
  input: ""SGD/lr""
  input: ""SGD/momentum""
  input: ""block1_conv1/bias""
  input: ""block1_conv1/kernel""
  input: ""block1_conv2/bias""
  input: ""block1_conv2/kernel""
  input: ""block2_conv1/bias""
  input: ""block2_conv1/kernel""
  input: ""block2_conv2/bias""
  input: ""block2_conv2/kernel""
  input: ""block3_conv1/bias""
  input: ""block3_conv1/kernel""
  input: ""block3_conv2/bias""
  input: ""block3_conv2/kernel""
  input: ""block3_conv3/bias""
  input: ""block3_conv3/kernel""
  input: ""block4_conv1/bias""
  input: ""block4_conv1/kernel""
  input: ""block4_conv2/bias""
  input: ""block4_conv2/kernel""
  input: ""block4_conv3/bias""
  input: ""block4_conv3/kernel""
  input: ""block5_conv1/bias""
  input: ""block5_conv1/kernel""
  input: ""block5_conv2/bias""
  input: ""block5_conv2/kernel""
  input: ""block5_conv3/bias""
  input: ""block5_conv3/kernel""
  input: ""dense_1/bias""
  input: ""dense_1/kernel""
  input: ""dense_2/bias""
  input: ""dense_2/kernel""
  input: ""dense_3/bias""
  input: ""dense_3/kernel""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
        type: DT_INT64
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/control_dependency""
  op: ""Identity""
  input: ""save/Const""
  input: ""^save/SaveV2""
  attr {
    key: ""T""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@save/Const""
      }
    }
  }
}
node {
  name: ""save/RestoreV2/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""SGD/decay""
      }
    }
  }
}
node {
  name: ""save/RestoreV2/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2/tensor_names""
  input: ""save/RestoreV2/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign""
  op: ""Assign""
  input: ""SGD/decay""
  input: ""save/RestoreV2""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@SGD/decay""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_1/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""SGD/iterations""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_1/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_1""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_1/tensor_names""
  input: ""save/RestoreV2_1/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_INT64
      }
    }
  }
}
node {
  name: ""save/Assign_1""
  op: ""Assign""
  input: ""SGD/iterations""
  input: ""save/RestoreV2_1""
  attr {
    key: ""T""
    value {
      type: DT_INT64
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@SGD/iterations""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_2/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""SGD/lr""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_2/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_2""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_2/tensor_names""
  input: ""save/RestoreV2_2/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_2""
  op: ""Assign""
  input: ""SGD/lr""
  input: ""save/RestoreV2_2""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@SGD/lr""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_3/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""SGD/momentum""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_3/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_3""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_3/tensor_names""
  input: ""save/RestoreV2_3/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_3""
  op: ""Assign""
  input: ""SGD/momentum""
  input: ""save/RestoreV2_3""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@SGD/momentum""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_4/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block1_conv1/bias""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_4/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_4""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_4/tensor_names""
  input: ""save/RestoreV2_4/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_4""
  op: ""Assign""
  input: ""block1_conv1/bias""
  input: ""save/RestoreV2_4""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block1_conv1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_5/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block1_conv1/kernel""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_5/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_5""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_5/tensor_names""
  input: ""save/RestoreV2_5/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_5""
  op: ""Assign""
  input: ""block1_conv1/kernel""
  input: ""save/RestoreV2_5""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block1_conv1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_6/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block1_conv2/bias""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_6/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_6""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_6/tensor_names""
  input: ""save/RestoreV2_6/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_6""
  op: ""Assign""
  input: ""block1_conv2/bias""
  input: ""save/RestoreV2_6""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block1_conv2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_7/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block1_conv2/kernel""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_7/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_7""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_7/tensor_names""
  input: ""save/RestoreV2_7/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_7""
  op: ""Assign""
  input: ""block1_conv2/kernel""
  input: ""save/RestoreV2_7""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block1_conv2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_8/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block2_conv1/bias""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_8/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_8""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_8/tensor_names""
  input: ""save/RestoreV2_8/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_8""
  op: ""Assign""
  input: ""block2_conv1/bias""
  input: ""save/RestoreV2_8""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block2_conv1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_9/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block2_conv1/kernel""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_9/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_9""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_9/tensor_names""
  input: ""save/RestoreV2_9/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_9""
  op: ""Assign""
  input: ""block2_conv1/kernel""
  input: ""save/RestoreV2_9""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block2_conv1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_10/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block2_conv2/bias""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_10/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_10""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_10/tensor_names""
  input: ""save/RestoreV2_10/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_10""
  op: ""Assign""
  input: ""block2_conv2/bias""
  input: ""save/RestoreV2_10""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block2_conv2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_11/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block2_conv2/kernel""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_11/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_11""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_11/tensor_names""
  input: ""save/RestoreV2_11/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_11""
  op: ""Assign""
  input: ""block2_conv2/kernel""
  input: ""save/RestoreV2_11""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block2_conv2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_12/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block3_conv1/bias""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_12/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_12""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_12/tensor_names""
  input: ""save/RestoreV2_12/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_12""
  op: ""Assign""
  input: ""block3_conv1/bias""
  input: ""save/RestoreV2_12""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_13/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block3_conv1/kernel""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_13/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_13""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_13/tensor_names""
  input: ""save/RestoreV2_13/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_13""
  op: ""Assign""
  input: ""block3_conv1/kernel""
  input: ""save/RestoreV2_13""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_14/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block3_conv2/bias""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_14/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_14""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_14/tensor_names""
  input: ""save/RestoreV2_14/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_14""
  op: ""Assign""
  input: ""block3_conv2/bias""
  input: ""save/RestoreV2_14""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_15/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block3_conv2/kernel""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_15/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_15""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_15/tensor_names""
  input: ""save/RestoreV2_15/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_15""
  op: ""Assign""
  input: ""block3_conv2/kernel""
  input: ""save/RestoreV2_15""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_16/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block3_conv3/bias""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_16/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_16""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_16/tensor_names""
  input: ""save/RestoreV2_16/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_16""
  op: ""Assign""
  input: ""block3_conv3/bias""
  input: ""save/RestoreV2_16""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv3/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_17/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block3_conv3/kernel""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_17/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_17""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_17/tensor_names""
  input: ""save/RestoreV2_17/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_17""
  op: ""Assign""
  input: ""block3_conv3/kernel""
  input: ""save/RestoreV2_17""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block3_conv3/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_18/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block4_conv1/bias""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_18/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_18""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_18/tensor_names""
  input: ""save/RestoreV2_18/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_18""
  op: ""Assign""
  input: ""block4_conv1/bias""
  input: ""save/RestoreV2_18""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_19/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block4_conv1/kernel""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_19/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_19""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_19/tensor_names""
  input: ""save/RestoreV2_19/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_19""
  op: ""Assign""
  input: ""block4_conv1/kernel""
  input: ""save/RestoreV2_19""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_20/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block4_conv2/bias""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_20/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_20""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_20/tensor_names""
  input: ""save/RestoreV2_20/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_20""
  op: ""Assign""
  input: ""block4_conv2/bias""
  input: ""save/RestoreV2_20""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_21/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block4_conv2/kernel""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_21/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_21""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_21/tensor_names""
  input: ""save/RestoreV2_21/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_21""
  op: ""Assign""
  input: ""block4_conv2/kernel""
  input: ""save/RestoreV2_21""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_22/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block4_conv3/bias""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_22/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_22""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_22/tensor_names""
  input: ""save/RestoreV2_22/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_22""
  op: ""Assign""
  input: ""block4_conv3/bias""
  input: ""save/RestoreV2_22""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv3/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_23/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block4_conv3/kernel""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_23/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_23""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_23/tensor_names""
  input: ""save/RestoreV2_23/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_23""
  op: ""Assign""
  input: ""block4_conv3/kernel""
  input: ""save/RestoreV2_23""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block4_conv3/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_24/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block5_conv1/bias""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_24/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_24""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_24/tensor_names""
  input: ""save/RestoreV2_24/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_24""
  op: ""Assign""
  input: ""block5_conv1/bias""
  input: ""save/RestoreV2_24""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_25/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block5_conv1/kernel""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_25/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_25""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_25/tensor_names""
  input: ""save/RestoreV2_25/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_25""
  op: ""Assign""
  input: ""block5_conv1/kernel""
  input: ""save/RestoreV2_25""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_26/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block5_conv2/bias""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_26/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_26""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_26/tensor_names""
  input: ""save/RestoreV2_26/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_26""
  op: ""Assign""
  input: ""block5_conv2/bias""
  input: ""save/RestoreV2_26""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_27/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block5_conv2/kernel""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_27/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_27""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_27/tensor_names""
  input: ""save/RestoreV2_27/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_27""
  op: ""Assign""
  input: ""block5_conv2/kernel""
  input: ""save/RestoreV2_27""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_28/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block5_conv3/bias""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_28/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_28""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_28/tensor_names""
  input: ""save/RestoreV2_28/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_28""
  op: ""Assign""
  input: ""block5_conv3/bias""
  input: ""save/RestoreV2_28""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv3/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_29/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""block5_conv3/kernel""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_29/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_29""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_29/tensor_names""
  input: ""save/RestoreV2_29/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_29""
  op: ""Assign""
  input: ""block5_conv3/kernel""
  input: ""save/RestoreV2_29""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@block5_conv3/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_30/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""dense_1/bias""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_30/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_30""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_30/tensor_names""
  input: ""save/RestoreV2_30/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_30""
  op: ""Assign""
  input: ""dense_1/bias""
  input: ""save/RestoreV2_30""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_1/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_31/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""dense_1/kernel""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_31/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_31""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_31/tensor_names""
  input: ""save/RestoreV2_31/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_31""
  op: ""Assign""
  input: ""dense_1/kernel""
  input: ""save/RestoreV2_31""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_1/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_32/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""dense_2/bias""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_32/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_32""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_32/tensor_names""
  input: ""save/RestoreV2_32/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_32""
  op: ""Assign""
  input: ""dense_2/bias""
  input: ""save/RestoreV2_32""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_2/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_33/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""dense_2/kernel""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_33/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_33""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_33/tensor_names""
  input: ""save/RestoreV2_33/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_33""
  op: ""Assign""
  input: ""dense_2/kernel""
  input: ""save/RestoreV2_33""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_2/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_34/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""dense_3/bias""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_34/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_34""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_34/tensor_names""
  input: ""save/RestoreV2_34/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_34""
  op: ""Assign""
  input: ""dense_3/bias""
  input: ""save/RestoreV2_34""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_3/bias""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/RestoreV2_35/tensor_names""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: ""dense_3/kernel""
      }
    }
  }
}
node {
  name: ""save/RestoreV2_35/shape_and_slices""
  op: ""Const""
  attr {
    key: ""dtype""
    value {
      type: DT_STRING
    }
  }
  attr {
    key: ""value""
    value {
      tensor {
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: 1
          }
        }
        string_val: """"
      }
    }
  }
}
node {
  name: ""save/RestoreV2_35""
  op: ""RestoreV2""
  input: ""save/Const""
  input: ""save/RestoreV2_35/tensor_names""
  input: ""save/RestoreV2_35/shape_and_slices""
  attr {
    key: ""dtypes""
    value {
      list {
        type: DT_FLOAT
      }
    }
  }
}
node {
  name: ""save/Assign_35""
  op: ""Assign""
  input: ""dense_3/kernel""
  input: ""save/RestoreV2_35""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""_class""
    value {
      list {
        s: ""loc:@dense_3/kernel""
      }
    }
  }
  attr {
    key: ""use_locking""
    value {
      b: true
    }
  }
  attr {
    key: ""validate_shape""
    value {
      b: true
    }
  }
}
node {
  name: ""save/restore_all""
  op: ""NoOp""
  input: ""^save/Assign""
  input: ""^save/Assign_1""
  input: ""^save/Assign_2""
  input: ""^save/Assign_3""
  input: ""^save/Assign_4""
  input: ""^save/Assign_5""
  input: ""^save/Assign_6""
  input: ""^save/Assign_7""
  input: ""^save/Assign_8""
  input: ""^save/Assign_9""
  input: ""^save/Assign_10""
  input: ""^save/Assign_11""
  input: ""^save/Assign_12""
  input: ""^save/Assign_13""
  input: ""^save/Assign_14""
  input: ""^save/Assign_15""
  input: ""^save/Assign_16""
  input: ""^save/Assign_17""
  input: ""^save/Assign_18""
  input: ""^save/Assign_19""
  input: ""^save/Assign_20""
  input: ""^save/Assign_21""
  input: ""^save/Assign_22""
  input: ""^save/Assign_23""
  input: ""^save/Assign_24""
  input: ""^save/Assign_25""
  input: ""^save/Assign_26""
  input: ""^save/Assign_27""
  input: ""^save/Assign_28""
  input: ""^save/Assign_29""
  input: ""^save/Assign_30""
  input: ""^save/Assign_31""
  input: ""^save/Assign_32""
  input: ""^save/Assign_33""
  input: ""^save/Assign_34""
  input: ""^save/Assign_35""
}
node {
  name: ""init_1""
  op: ""NoOp""
  input: ""^dense_1/kernel/Assign""
  input: ""^dense_1/bias/Assign""
  input: ""^dense_2/kernel/Assign""
  input: ""^dense_2/bias/Assign""
  input: ""^dense_3/kernel/Assign""
  input: ""^dense_3/bias/Assign""
  input: ""^SGD/iterations/Assign""
  input: ""^SGD/lr/Assign""
  input: ""^SGD/momentum/Assign""
  input: ""^SGD/decay/Assign""
}
versions {
  producer: 24
}

```"
13837,issue when installing Tensorflow,"Hello, I have tried to install tensorflow gpu version with ""pip3 install --upgrade tensorflow-gpu"" but I have got this error message:
```
C:\Users\Martin>python
Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\Martin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Martin\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Le module spécifié est introuvable.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Martin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Martin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Martin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Martin\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Martin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Martin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Martin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Martin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Martin\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 906, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: Le module spécifié est introuvable.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Martin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Martin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Martin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Martin\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

can someone help me ?"
13835,Feature query: How to load the graph(model) only once and then give series of inputs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No. Running the Simple Audio recognition network tutorial
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04 
-**TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.3
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.6.0

### Describe the problem
I am a tensorflow and neural network novice. I was able to run the [Simple audio recognition](https://www.tensorflow.org/versions/master/tutorials/audio_recognition) network. For a given input wavefile the model obtained gives the result in approx. 5 seconds. I want reduce this time to 100s of milliseconds. From the issue https://github.com/tensorflow/tensorflow/issues/11618, there seems to be a way to do this using the graph. 

I used the graph optimisation tool but there wasn't a considerable reduction in time.I heard that loading the graph is what takes time and hence I need to figure out how to load the graph only once and then feed the inputs without feeding all the parameters every time a new input needs to be tested.

The command that is used for decoding is as follows: 

python tensorflow/examples/speech_commands/label_wav.py \
--graph=/tmp/my_frozen_graph.pb \
--labels=/tmp/speech_commands_train/conv_labels.txt \
--wav=/tmp/speech_dataset/left/a5d485dc_nohash_0.wav

Also does anyone know if there is official data on how long alexa/google home take to respond to trigger words?"
13827,"Tensorflow 1.3: tf.constant with dtype=[float32, float64, float16] may have inconsistent behavior.","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04 with docker running `gcr.io/tensorflow/tensorflow:latest` 

- **TensorFlow installed from (source or binary)**: NA
- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:

```
// works
test = numpy.array([1,2,3,4,5,6, None], dtype=numpy.float32)
sess = tf.Session()
print(sess.run(tf.constant(test, dtype=tf.float32)))
```

```
// works
sess = tf.Session()
print(sess.run(tf.constant([1, 2, 3, 4, 5, 6, None], dtype=tf.float16)))
```
```
// returns error
sess = tf.Session()
print(sess.run(tf.constant([1, 2, 3, 4, 5, 6, None], dtype=tf.float32)))
// TypeError: Expected float32, got None of type '_Message' instead.
```

### Describe the problem
A tensorflow constant with None in array with dtype float32, float64 seem to throw an error. However, if they are first wrapped by a numpy array, none is accepted and turned into NaN. This behavior seems inconsistent."
13826,while_loop gradient failure with ints,"This is puzzling:

```python
g=tf.Graph()
with g.as_default():
    i=tf.constant(1, name=""i"")
    inc=tf.constant(1, name=""inc"")
    w=tf.while_loop(lambda i,inc: i<=5, lambda i,inc: [i+inc, inc], [i,inc])
    grad=tf.gradients(w[0],i)
```
results in `grad[0]==None`

But changing the constants to floats:

```python
g=tf.Graph()
with g.as_default():
    i=tf.constant(1.0, name=""i"")
    inc=tf.constant(1.0, name=""inc"")
    w=tf.while_loop(lambda i,inc: i<=5.0, lambda i,inc: [i+inc, inc], [i,inc])
    grad=tf.gradients(w[0],i)
```

gives the correct gradient (ie, grad[0] is <tf.Tensor 'gradients/while/Enter_grad/Exit:0' shape=() dtype=float32>).

This is on `tensorflow.VERSION == 1.3.0`.

"
13825,"GRU cell implementation different from Reference paper ""Cho 14""","Hello, 

I've been looking through GRUCell implementation on Tensorflow at ""tensorflow/tensorflow/python/ops/rnn_cell_impl.py"", and got a bit confused.

The output of GRUCell is implemented as
`new_h = u * state + (1 - u) * c`

However, from the reference paper ""Cho 14"" at https://arxiv.org/pdf/1412.3555.pdf
I think it should be like
`new_h = (1 -  u) * state + u * c`

I am not sure if this is a bug or an intended variation from the reference.

Thank you"
13824,GPU Question: dtype supports ,"According to https://github.com/tensorflow/tensorflow/issues/9506, gpu currently only supports floats. In NLP tasks however there's no way around looking up the embedding matrix with int-typed indices. 

So I wonder if it on your shortlist to extend to int support? If not, how to get around the index-lookup issue? Thanks."
13823,feature request: more efficient tf.eye,"tf.eye(n) creates a constant of size n*4
A better implementation would use `Fill` instead of constant

This is similar to issue https://github.com/tensorflow/tensorflow/issues/13221#issuecomment-333893186 , where zeros is getting refactored to use Fill instead of constant"
13822,Convolutional layers cannot be used multiple times,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source (branch 1.4)
- **TensorFlow version (use command below)**: 1.4.0-dev
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: nVidia 1080Ti 11G
- **Exact command to reproduce**: run the script below

### Describe the problem
Keras convolutional layers cannot be used multiple times without creating a name conflict. This is especially bad when trying to copy layers from one model to another (See the second example below). This was working a few weeks ago. Here is a simple test case that used to work:

```python
#!/usr/bin/env python

import tensorflow as tf

from tensorflow.contrib.keras.api.keras.layers import Conv2D
from tensorflow.contrib.keras.api.keras.layers import Input

a = Input(shape=(None, 32, 3))
c = Conv2D(32, (3, 3))
c(a)
c(a)
```

I added some print statement to different versions of the code. It looks like the convolutional ops are created with the following names in the old code:

1. conv2d/convolution/
2. conv2d/convolution_1/

In the new code they are:

1. conv2d/convolution/
2. conv2d/convolution/

It looks like the name scope code was changed recently. I specifically notice that some of the scope handling was moved to __init__ where it used to happen when the function was called. 

This is a slightly more complex version of the code that shows that copying convolutional layers doesn't work:

```python
#!/usr/bin/env python

import tensorflow as tf

from tensorflow.contrib.keras.api.keras.layers import Conv2D
from tensorflow.contrib.keras.api.keras.layers import Input
from tensorflow.contrib.keras.api.keras.models import Model

a = Input(shape=(None, 32, 3))
b = a
b = Conv2D(32, (3, 3))(b)
mod1 = Model(inputs=a, outputs=b)
a = Input(shape=(None, 32, 3))
b = a
for layer in mod1.layers[1:]:
    b = layer(b)
mod2 = Model(inputs=a, outputs=b)
```
This is an example of the traceback:
```
Traceback (most recent call last):
  File ""/root/vish/test.py"", line 16, in <module>
    b = layer(b)
  File ""/opt/rh/rh-python35/root/usr/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py"", line 252, in __call__
    output = super(Layer, self).__call__(inputs, **kwargs)
  File ""/opt/rh/rh-python35/root/usr/lib/python3.5/site-packages/tensorflow/python/layers/base.py"", line 577, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/opt/rh/rh-python35/root/usr/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py"", line 172, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File ""/opt/rh/rh-python35/root/usr/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py"", line 841, in __call__
    return self.conv_op(inp, filter)
  File ""/opt/rh/rh-python35/root/usr/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py"", line 503, in __call__
    return self.call(inp, filter)
  File ""/opt/rh/rh-python35/root/usr/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py"", line 191, in __call__
    name=self.name)
  File ""/opt/rh/rh-python35/root/usr/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 631, in conv2d
    data_format=data_format, name=name)
  File ""/opt/rh/rh-python35/root/usr/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/opt/rh/rh-python35/root/usr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2959, in create_op
    self._add_op(ret)
  File ""/opt/rh/rh-python35/root/usr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2599, in _add_op
    ""is already used"" % op.name)
ValueError: cannot add op with name conv2d/convolution as that name is already used
```"
13821,Feature request: classifier deallocation,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.6
- **TensorFlow installed from (source or binary)**: Binary? (pip)
- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0') (tensorflow)
- **Python version**: 2.7.8
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: https://gist.github.com/alltom/75f3c0e62cd679c5e8af368bb49370c6

### Describe the problem
[DNNClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) doesn't appear to have any methods for freeing its resources, and that doesn't seem to occur automatically if I let the classifier go out of scope.

In [the gist linked above](https://gist.github.com/alltom/75f3c0e62cd679c5e8af368bb49370c6), I train, evaluate, and throw away DNNClassifiers as part of a genetic algorithm for feature selection, but after training a little more than a hundred models, it fails:

`InvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on ./model_2017-10-18-21:41:26/125/model.ckpt-4000: Resource exhausted: ./model_2017-10-18-21:41:26/125`

### Source code / logs
https://gist.github.com/alltom/75f3c0e62cd679c5e8af368bb49370c6"
13820,Custom Reader Op : Undeclared Inclusions,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
Custom Code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
Linux Ubuntu 16.04 
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
0.6.1

- **CUDA/cuDNN version**:
Laptop CPU
- **GPU model and memory**:
Laptop CPU
- **Exact command to reproduce**:
 bazel build -c opt //tensorflow/core/user_ops:rest_api.so --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""

### Describe the problem
I am getting undeclared inclusions for files in the //tensorflow/core/framework directory. I am not sure why this is happening since the user_ops_op_lib in the //tensorflow/core/BUILD has "":framework"" as its dependency. Is there a missing dependency that needs to be added? 

### Source code / logs

BUILD File In /tensorflow/core/user_ops
```
load(""//tensorflow:tensorflow.bzl"", ""tf_custom_op_library"")

cc_library(
        name = ""rest_api_dependencies"",
        srcs = glob([""rest_api.cc"", ""*.c"", ""*.cpp""]),
        hdrs = glob([""rest_api.h"", ""*.h"", ""*.hpp"", ""dict.c""]),
	linkopts = [""-pthread"", ""-lev"", ""-fexceptions""],
)

tf_custom_op_library(
        name = ""rest_api.so"",
        srcs = [""rest_api.cc""],
        deps = ["":rest_api_dependencies""],
)
```

undeclared inclusion(s) in rule '//tensorflow/core/user_ops:rest_api_dependencies':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/user_ops/rest_api.cc':
  'bazel-out/local-opt/genfiles/tensorflow/core/framework/op_def.pb.h'
  'bazel-out/local-opt/genfiles/tensorflow/core/framework/attr_value.pb.h'
  'bazel-out/local-opt/genfiles/tensorflow/core/framework/tensor.pb.h'
  'bazel-out/local-opt/genfiles/tensorflow/core/framework/resource_handle.pb.h'
  'bazel-out/local-opt/genfiles/tensorflow/core/framework/tensor_shape.pb.h'
  'bazel-out/local-opt/genfiles/tensorflow/core/framework/types.pb.h'
  'bazel-out/local-opt/genfiles/tensorflow/core/lib/core/error_codes.pb.h'
  'bazel-out/local-opt/genfiles/tensorflow/core/framework/step_stats.pb.h'
  'bazel-out/local-opt/genfiles/tensorflow/core/framework/allocation_description.pb.h'
  'bazel-out/local-opt/genfiles/tensorflow/core/framework/tensor_description.pb.h'
"
13818,"Feature request: C API ""TF_FinishWhile"" should create a ""WhileContextDef""","Thanks @skye et al for getting while loops into the C API!

I'd like to request that `TF_FinishWhile` generate a `WhileContextDef` protobuf object. That way, non-Python TensorFlow clients can create metagraphs involving while loops which the Python client can then import and take gradients of. Ideally, you could gradients of while loops directly in the C API, but I think just creating the while context should be much easier and is a good stop gap on the way there. 
"
13814,Kernel/Bias created for only one LSTM when using BasicLSTMCell with MultiRNNCell,"I am using the example code ptb_word_lm.py that creates 2 layers of lstm's. The code clearly creates 2 (c,h) tuples, each tuple corresponding to one LSTM. Each c and h should depend on matrices i,j,f,o (input_gate, new_input, foget_gate, outpu_gate) that are found in BasicLSTMCell  in rnn_cell_impl.py. With tf.global_variables() you should be able to get these matrices. In fact, it returns a kernel matrix and a bias matrix. The kernel matrix is of size 400x800 which corresponds to only one LSTM parameters. (the i,j,f,o matrices are concatenated horizontally. The matrices for the input and h_t-1 are concatenated vertically). However, whether I am using 1 layer LSTM or 2 layer LSTM, the number of parameters stays the same (400x800), when it should be 400x1600 or a tuple of some sort or an additional kernel and bias. It seems that tf.global_variables() does not expose all trainable parameters in this case. Is that true or am I missing something?

EDIT:
I am now fairly certain that this is a problem, because when I use the ""block"" option in the script, multirnncell works correctly and generates 2 kernel matrices and 2 bias matrices. with BasicLSTMCell, it only generates one kernel matrix and 1 bias matrix.
"
13810,tf.nn.max_pool memory leak?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 3.6.1
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 5.
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I'm using a tensorflow graph for offline preprocessing, and I believe I've stumbled onto a memory leak problem with tf.nn.maxpool. After certain amount of feed-forward operations (only inference, no training), I get a `Allocator (cuda_host_bfc) ran out of memory trying to allocate ...` error. This happens consistently after a number of data*points* fed through the graph, independtly on how those datapoints are spread over batches. e.g. with batchsize 4 I get this after 64 iterations, with batchsize 2 I get this error after 128 iterations.
The following rough code should reproduce this issue on a Titan XP with 12gb memory, didn't test this due to time constraints.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import tensorflow as tf
from keras import Input
from keras.layers import AvgPool2D, MaxPool2D, Lambda, Concatenate
from keras.engine import Model
import numpy as np

thumb_dim = 8192
strides = 1
pooling = 22
inputs, outputs = [], []
slide = Input([thumb_dim, thumb_dim, 3])
mask = Input([thumb_dim, thumb_dim, 1])
inputs.append(slide)
inputs.append(mask)

hsv_slide = Lambda(lambda x: tf.image.rgb_to_hsv(x))(slide)
heatmap = Lambda(
    lambda x: x[..., 1:2] * tf.cast(x[..., 2:3] > 0.3, 'float32'))(
    hsv_slide)
# heatmap = AvgPool2D(4, strides=1, padding='same')(heatmap)

to_pool = Concatenate(axis=-1)([heatmap, mask])
concat = Lambda(lambda x: tf.stop_gradient(tf.nn.max_pool(x, ksize=(1, pooling, pooling, 1),
                                                          strides=(1, strides, strides, 1), padding='VALID')))(to_pool)

outputs.append(concat)
rpn = Model(inputs, outputs)
rpn.predict_generator(([
    np.random.random((2, thumb_dim, thumb_dim, 3)),
    np.random.random((2, thumb_dim, thumb_dim, 1))] for _ in range(200)))
```"
13807,tensorflow-android library 1.4.0-rc0 seems to contain tensorflow 1.3.0-rc2,"For my work I require the new String Tensor feature introduced for the Java API in tensorflow 1.4. I tested my network with my Java code on my Desktop which works fine (using org.tensorflow:tensorflow:1.4.0-rc0). 

However, when I use the same code in an Android project (using org.tensorflow:tensorflow-android:1.4.0-rc0), I get the following error leading me to the guess that the wrong version was packaged:

```
10-18 16:33:33.987 21288-22070/test.android.fisheye W/System.err: java.util.concurrent.ExecutionException: java.lang.UnsupportedOperationException: non-scalar DataType.STRING tensors are not supported yet (version 1.3.0-rc2). Please file a feature request at https://github.com/tensorflow/tensorflow/issues/new
10-18 16:33:33.988 21288-22070/test.android.fisheye W/System.err:     at java.util.concurrent.FutureTask.report(FutureTask.java:94)
10-18 16:33:33.988 21288-22070/test.android.fisheye W/System.err:     at java.util.concurrent.FutureTask.get(FutureTask.java:164)
10-18 16:33:33.988 21288-22070/test.android.fisheye W/System.err:     at test.neuronalnetwork.service.TaskWorkerLoop$Loop.run(TaskWorkerLoop.java:71)
10-18 16:33:33.988 21288-22070/test.android.fisheye W/System.err:     at java.lang.Thread.run(Thread.java:762)
```

I also downloaded the org.tensorflow:tensorflow-android:1.4.0-rc0 from maven central and had a look at the class files. They look like the class files from the 1.3.0 version. 

Furthermore, when looking at the Tensor and Tensorflow classes, you can see that the version displayed in the error message above, is actually read from the native library. 

Do you agree, or did I miss something? If you do agree, can you please upload a real 1.4.0-rc0 to maven central?"
13806,What will happen if one of the worker becomes dead in distribute tensorflow?,"Hi, I deployed a **distribute tensorflow** cluster to train a Deep Neural Network system, but in the training process, one of the worker broke down for some unknown reason(maybe the bad network), while the other workers were still training data, their training progresses didn't stop and continued going on. And after I restarted the broken worker, the ps node could send data to the broken worker, the broken worker also could train data with other workers, but the amazing thing happened: **the good workers' training progresses were set to zero, and they began to retrain the data with the broken worker node. So my previous training progresses were all gone...** That's to say, the tarining process restarted. is there anybody know how to solve this problem?
My tensorflow version: 0.10.0

Thanks in advance!"
13805,Tensorflow serving API does not support python3.5 ,Tensorflow serving API only supports python2.7.   It is not work for python3 .5
13804,Figure out CUDA and cuDNN versions,"So I'm setting up an image for a group to be used in the cloud. 
Given that there certain cloud related things which we don't have premissions to we want to be able to verify that Tensorflow is using indeed what we intended. However, there does not seem to be any comprehensive way of understanding at the moment what exactly is it using. The output after creating a session is:
```
2017-10-18 10:49:51.880192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-10-18 10:49:51.880752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:1e.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-10-18 10:49:51.880818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2017-10-18 10:49:51.880849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2017-10-18 10:49:51.880883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)
```
Does this means the cuda fails since I don't see any ""succesffully loaded libcuda..."" or anthing like that. Does it even load cuDNN? Is there any way to directly check it (once is enough don't need to be printed every time. 
I did not find any documentations and there was not too much here in the issues except people not beeing able to do it. "
13803,Adding a variable of learning rate for each layer,"I am wondering that is it easy to add a variable in tf.get_variable function, which allows us to initialize the learning rate for each layer easily. Since I found out that it's not easy to do in tensorflow but easy to do in other toolboxes."
13802,deadlock in fork() because of OpenBlas,"TensorFlow: 1.3.0 (v1.3.0-rc2-20-g0787eee)

I'm not exactly sure whether this is a TF specific problem, or OpenBlas specific, or at what place this should be fixed.

At some part in my code, I want to start a subprocess, via `Popen`, and it uses `fork()` internally. Before that, I already initialized the TF session and thus have initialized the thread pools.

The `fork()` will cause a deadlock because OpenBlas has used `pthread_atfork()` to register `blas_thread_shutdown_()` to do some cleanup, which will wait for a lock, which probably was acquired by some of the other threads at that time. Stacktrace:

```
#0  0x00002b289b1702ad in __lll_lock_wait () from /u/zeyer/tools/glibc217/libpthread.so.0
#1  0x00002b289b16dabd in pthread_cond_signal@@GLIBC_2.3.2 () from /u/zeyer/tools/glibc217/libpthread.so.0
#2  0x00002b294d9fb68e in blas_thread_shutdown_ () from /usr/lib/libopenblas.so.0
#3  0x00002b289b44b965 in fork () from /u/zeyer/tools/glibc217/libc.so.6
#4  0x00002b28ae1f1c47 in subprocess_fork_exec (self=<optimized out>, args=<optimized out>)
    at /tmp/python3-20170710-4344-zt9hmb/Python-3.6.1/Modules/_posixsubprocess.c:672
#5  0x00002b289acf79c9 in _PyCFunction_FastCallDict (func_obj=0x2b28adfa34c8, args=0x1653030, nargs=<optimized out>, 
    kwargs=kwargs@entry=0x0) at Objects/methodobject.c:234
#6  0x00002b289acf7c97 in _PyCFunction_FastCallKeywords (func=func@entry=0x2b28adfa34c8, stack=stack@entry=0x1653030, 
    nargs=<optimized out>, kwnames=kwnames@entry=0x0) at Objects/methodobject.c:295
#7  0x00002b289ad8b171 in call_function (pp_stack=pp_stack@entry=0x7ffe97057f30, oparg=oparg@entry=17, 
    kwnames=kwnames@entry=0x0) at Python/ceval.c:4798
#8  0x00002b289ad8eeb7 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
...
```

[This OpenBLAS issue](https://github.com/xianyi/OpenBLAS/issues/240) and [this Sage issue](https://trac.sagemath.org/ticket/22021) might be related. Basically the Sage solution is to disable the multi-threading support of OpenBLAS by setting `OMP_NUM_THREADS=1` but I actually want to use the multi-threading support if possible.
"
13801,Feature Request: Scripts for build tensorflow with FORTIFY,"FORTIFY is an important security feature that can help us discover hidden bugs. 
This is success story from Android:
https://android-developers.googleblog.com/2017/04/fortify-in-android.html

Please do the same thing for tensorflow, provide a checked build(with FORTIFY turned on) publicly. 

"
13799,Feature Request: need to support dynamically RDMA gid setting in tensorflow/tensorflow/contrib/verbs/rdma.cc,"In tensorflow/tensorflow/contrib/verbs/rdma.cc when calling  ibv_query_gid() the gid_index field is hard-coded as 0, which could not work well in real world.
To fix this, it is better to add a user-specified option. 
"
13796,Feature Request: use S3 for checkpoint loading/saving,EFS is not available in most AWS regions. For distributed TensorFlow in those regions one would have to rely on S3 to save/restore checkpoints
13794,error,"

---------
Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem."
13793,Tensorflow not detecting GPU even when CUDA is installed,"Hi there,
I am running the following on Ubuntu 16.04 LTS and I have GPU 1070 and tensorflow version 1.0.1 (installed and checked through conda -list

I am trying to run a simple piece of code in tf
with tf.device('/gpu:0'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
    c = tf.matmul(a, b)

with tf.Session() as sess:
    print (sess.run(c))

And I am getting the following error:
InvalidArgumentError (see above for traceback): Cannot assign a device to node 'MatMul_1': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0

Regarding the CUDA installed, I have the following information obtained by running nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Tue_Jan_10_13:22:03_CST_2017
Cuda compilation tools, release 8.0, V8.0.61

Theano is able to detect and use GPU
Using gpu device 0: GeForce GTX 1070 (CNMeM is disabled, cuDNN 5110)


How to get TF to use the GPU. Or what CUDA version should I install to make it work with GPU.
I understand this is not the platform to ask questions related to CUDA but if someone can point me in the right direction then it will be helpful.

Thank You."
13790,compile failure with --config=mkl,"Building from git 27767d8e9c1325979cf32ff5b81c10df9006fd57 with `TF_NEED_MKL=1` and `TF_DOWNLOAD_MKL=1`. It seems d835d677ade78a41e0e097f67c87b6ab8588a90a introduced a compile failure:

```
ERROR: /build/tensorflow-git/src/tensorflow-mkl/tensorflow/core/kernels/BUILD:819:1: C++ compilation of rule '//tensorflow/core/kernels:transpose_op' failed (Exit 1).
tensorflow/core/kernels/mkl_transpose_op.cc:53:10: error: template-id 'MKLTranspose2D<float>' for 'tensorflow::Status tensorflow::MKLTranspose2D(char, const tensorflow::Tensor&, tensorflow::Tensor*)' does not match any template declaration
   Status MKLTranspose2D<T>(const char trans, const Tensor& in, Tensor* out) { \
          ^~~~~~~~~~~~~~~~~
tensorflow/core/kernels/mkl_transpose_op.cc:53:10: note: in definition of macro 'INSTANTIATE'
   Status MKLTranspose2D<T>(const char trans, const Tensor& in, Tensor* out) { \
          ^~~~~~~~~~~~~~
tensorflow/core/kernels/mkl_transpose_op.cc:44:6: note: candidate is: template<class T> void tensorflow::{anonymous}::MKLTranspose2D(char, const tensorflow::Tensor&, tensorflow::Tensor*)
 void MKLTranspose2D(const char trans, const Tensor& in, Tensor* out) {}
      ^~~~~~~~~~~~~~
tensorflow/core/kernels/mkl_transpose_op.cc:122:1: error: expected '}' at end of input
 }  // namespace tensorflow
 ^
```

There are multiple problems in there. This patch gets it closer to compiling (correcting return type of `MKLTranspose2D` base template, adding missing line-continuation escape):

```diff
diff --git a/tensorflow/core/kernels/mkl_transpose_op.cc b/tensorflow/core/kernels/mkl_transpose_op.cc
index 89a1d5e8a..93da2a6ea 100644
--- a/tensorflow/core/kernels/mkl_transpose_op.cc
+++ b/tensorflow/core/kernels/mkl_transpose_op.cc
@@ -41,7 +41,7 @@ namespace tensorflow {
 
 namespace {
 template <typename T>
-void MKLTranspose2D(const char trans, const Tensor& in, Tensor* out) {}
+Status MKLTranspose2D(const char trans, const Tensor& in, Tensor* out);
 
 // Documentation here: https://software.intel.com/en-us/node/520863
 // Parameters: (ordering:row-major, operation:transpose, num_rows, num_cols,
@@ -54,7 +54,7 @@ void MKLTranspose2D(const char trans, const Tensor& in, Tensor* out) {}
     mkl_##PREFIX##omatcopy('R', trans, in.dim_size(0), in.dim_size(1), 1,     \
                            in.flat<T>().data(), in.dim_size(1),               \
                            out->flat<T>().data(), in.dim_size(0));            \
-    return Status::OK();
+    return Status::OK();                                                      \
   }
 
   INSTANTIATE(float, s)
@@ -66,7 +66,7 @@ void MKLTranspose2D(const char trans, const Tensor& in, Tensor* out) {}
   static const char kMKLTranspose = 'T';
   static const char kMKLConjugateTranspose = 'C';
 
-  }  // namespace tensorflow
+  }  // anonymous namespace
 
   Status MklTransposeCpuOp::DoTranspose(OpKernelContext* ctx, const Tensor& in,
                                         gtl::ArraySlice<int32> perm,
```

But I'm not sure how to handle this issue (conversion of the `alpha` argument into float/double/MKL_Complex8/MKL_Complex16):

```
ERROR: /home/steven/Development/misc-packages/tensorflow-git/src/tensorflow-mkl/tensorflow/core/kernels/BUILD:819:1: C++ compilation of rule '//tensorflow/core/kernels:transpose_op' failed (Exit 1).
tensorflow/core/kernels/mkl_transpose_op.cc: In function 'tensorflow::Status tensorflow::{anonymous}::MKLTranspose2D(char, const tensorflow::Tensor&, tensorflow::Tensor*) [with T = std::complex<float>]':
tensorflow/core/kernels/mkl_transpose_op.cc:57:69: error: could not convert '1' from 'int' to 'MKL_Complex8 {aka _MKL_Complex8}'
                                out->flat<T>().data(), in.dim_size(0));            \
                                                                     ^
tensorflow/core/kernels/mkl_transpose_op.cc:63:5: note: in expansion of macro 'INSTANTIATE'
     INSTANTIATE(complex64, c)
     ^
tensorflow/core/kernels/mkl_transpose_op.cc: In function 'tensorflow::Status tensorflow::{anonymous}::MKLTranspose2D(char, const tensorflow::Tensor&, tensorflow::Tensor*) [with T = std::complex<double>]':
tensorflow/core/kernels/mkl_transpose_op.cc:57:69: error: could not convert '1' from 'int' to 'MKL_Complex16 {aka _MKL_Complex16}'
                                out->flat<T>().data(), in.dim_size(0));            \
                                                                     ^
tensorflow/core/kernels/mkl_transpose_op.cc:64:5: note: in expansion of macro 'INSTANTIATE'
     INSTANTIATE(complex128, z)
     ^
tensorflow/core/kernels/mkl_transpose_op.cc: At global scope:
tensorflow/core/kernels/mkl_transpose_op.cc:95:10: error: 'template<bool conjugate> class tensorflow::MklConjugateTransposeCpuOp' used without template parameters
   Status MklConjugateTransposeCpuOp::DoTranspose(OpKernelContext* ctx,
          ^~~~~~~~~~~~~~~~~~~~~~~~~~
```"
13789,Feature Request: C++ gradient for LRN (Local Response Normalization),"Implement the gradient for LRN in c++ so that it is available for TF_AddGradients

This is the python code that I think would need to be ported:
https://github.com/tensorflow/tensorflow/blob/48be6a56d5c49d019ca049f8c48b2df597594343/tensorflow/python/ops/nn_grad.py#L516

I'm using this issue to call dibs on this gradient port per @bpiel's suggestion
I was also advised to mention @suharshs "
13788,Feature request: Prevent predict_scores() in tf.contrib.learn.estimators from reloading graph variables from checkpoint file everytime,"Hi there,

I am using tf.contrib.learn.DNNLinearCombinedRegressor with wide and deep feature columns in Tesnorflow 1.3. I use an input_fn to parse pandas dataframes to this regressor for training. I have both real and categorical features, and so my deep feature columns are made of sparse columns with embedding as well as real valued columns. After training, I want to use the trained model for prediction in another application. I can call estimator.predict_scores() function to do this, but it seems very slow. Mainly because, it seems to be reloading the graph variables from last checkpoint file created during training everytime it is called. Can we prevent this for faster predictions, so that it reloads the variables only the first time it is called ? My application is simple, and I would like to not use Tensorflow serving.

Thanks."
13787,no module named models.rnn.translate,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13786,no module,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13785,TypeError in freeze_graph tool while trying to freeze a graph in Tensorflow 1.3,"I train a tf.contrib.learn estimator (specifically, DNNLinearCombineRegressor) in Python and saved the model’s parameters and graph by specifying model_dir when defining the estimator. My estimator uses embedding and real valued feature columns, and a custom input_fn to parse the data. After training is done, I try to freeze the graph using the CLI as mentioned in this [post](https://stackoverflow.com/questions/46223252/error-while-freezing-the-model-freeze-graph), and get this error --` TypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(""dnn/hiddenlayer_0/biases:0"", shape=(10,), dtype=float32)` in saver.py in the function _ValidateAndSliceInputs.  Is this because I am using an input_fn with pandas dataframe, or is it because of the tf.feature_columns that I am using? 


I want to freeze the graph so that predict_scores() doesnt reload the graph variables from checkpoint file everytime for prediction. I am using Tensorflow 1.3 with Anaconda Python 3.5 on Windows 10. 

Thanks in advance. 

"
13784,Docker image file generated by parameterized_docker_build.sh fails (3),"**Issue**

Fixed previous docker install issue [#13379](https://github.com/tensorflow/tensorflow/issues/13379)

Executing parameterized_docker_build.sh generates new docker file but image fails to turn into an docker image, throws error message.

**System information**

- I have used a stock example script provided in TensorFlow
- Windows 10 professional
- TensorFlow install as docker image tensorflow/tensorflow:1.3.0-devel-py3
- TensorFlow version 1.3
- Python version 3
- Bazel version 0.5.0
- CUDA/cuDNN version not relevant (CPU install)
- GPU model and memory not relevant (CPU install)
- Docker CE 17.09.0-ce

`$which docker`
> /usr/bin/docker

**Command triggering issue**

`$/tensorflow/tensorflow/tools/docker/parameterized_docker_build.sh`

**Error messages**

> Required build parameters:
>   TF_DOCKER_BUILD_TYPE=cpu
>   TF_DOCKER_BUILD_IS_DEVEL=no
>   TF_DOCKER_BUILD_DEVEL_BRANCH=
> 
> Optional build parameters:
>   TF_DOCKER_BUILD_CENTRAL_PIP=http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl
>   TF_DOCKER_BUILD_IMAGE_NAME=
>   TF_DOCKER_BUILD_VERSION=
>   TF_DOCKER_BUILD_PORT=
>   TF_DOCKER_BUILD_PUSH_CMD=
> 
> FINAL_IMAGE_NAME: tensorflow/tensorflow
> FINAL_TAG: latest
> Original Dockerfile: /tensorflow/tensorflow/tools/docker/Dockerfile
> 
> 
> Docker build will occur in temporary directory: /tmp/tmp.nBIOwyaShf
> Downloading pip wheel from: http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl
> 
> Modified Dockerfile at: /tmp/tmp.nBIOwyaShf/Dockerfile
> 
> Building docker image with image name and tag: /tensorflow:latest
> invalid argument ""/tensorflow:latest"" for t: invalid reference format
> See 'docker build --help'.
> FAIL: docker build of /tensorflow:latest with Dockerfile /tmp/tmp.nBIOwyaShf/Dockerfile failed

**Inspecting the resulting docker file for further introspection**

`$cat /tmp/tmp.nBIOwyaShf/Dockerfile`

> FROM ubuntu:16.04
> 
> MAINTAINER Craig Citro <craigcitro@google.com>
> 
> RUN apt-get update && apt-get install -y --no-install-recommends \
>         build-essential \
>         curl \
>         libfreetype6-dev \
>         libpng12-dev \
>         libzmq3-dev \
>         pkg-config \
>         python \
>         python-dev \
>         rsync \
>         software-properties-common \
>         unzip \
>         && \
>     apt-get clean && \
>     rm -rf /var/lib/apt/lists/*
> 
> RUN curl -O https://bootstrap.pypa.io/get-pip.py && \
>     python get-pip.py && \
>     rm get-pip.py
> 
> RUN pip --no-cache-dir install \
>         Pillow \
>         h5py \
>         ipykernel \
>         jupyter \
>         matplotlib \
>         numpy \
>         pandas \
>         scipy \
>         sklearn \
>         && \
>     python -m ipykernel.kernelspec
> 
> RUN pip --no-cache-dir install http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl
> 
> 
> COPY jupyter_notebook_config.py /root/.jupyter/
> 
> COPY notebooks /notebooks
> 
> COPY run_jupyter.sh /
> 
> EXPOSE 6006
> EXPOSE 8888
> 
> WORKDIR ""/notebooks""
> 
> CMD [""/run_jupyter.sh"", ""--allow-root""]

**Additional introspection with docker image file**

Copied and pasted the parameterized_docker_build.sh output docker file content and tried to execute on docker hub which fails.

Docker hub log reports yields the following additional error information:

> Step 6/9 : RUN pip --no-cache-dir install http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl
> 
>  ---> Running in fd0d49a714e4
> 
> [91mtensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl is not a supported wheel on this platform.
> [0m
> Removing intermediate container fd0d49a714e4
> 
> The command '/bin/sh -c pip --no-cache-dir install http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl' returned a non-zero code: 1"
13783,How can I do multples input Api c?,"Hi guys, 

I am trying to make build my project in c++. The model use CTC and I don't have idea with re-writte code for main.cc. So,  It has input four values: `Tensor(""the_input:0"", shape=(?, 128, 64, 1), dtype=float32) Tensor(""the_labels:0"", shape=(?, 16), dtype=float32) Tensor(""input_length:0"", shape=(?, 1), dtype=int64) Tensor(""label_length:0"", shape=(?, 1), dtype=int64)`

I am use [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc](url) as model base for new code, however, the input is erro. 


Can someone help me? 


Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- ** Linux Ubuntu 16.04**:
- **TensorFlow installed by git alfer compiling**:
- **TensorFlow 1.3.0**:
- **Python version 3.5**: 
- **Bazel version 0.6.1**:
- **CUDA/cuDNN version**:
- **GPU Titan X pascal 12GB**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

I a m triyng create a image ocr mobilie as tensorflow CTC.  

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13782,QuantConv2D,"Hi, I've followed the documentation and TF Github code and couldn't find the relation between QuantizedConv2D to GEMMlowp. Does QuantizedConv2D use, implicitly, in a way the QuantizedMatMul(quantized_matmul_op.cc), under the hood, where the latter calls GemmlowpMultiply explicitly? Otherwise how the QuantizedConv2D using the great benefit of google GEMM HW platform specific implementation?
"
13780,I don't understand why I get these errors when i used dict () instead of { }. and how could I solve it?,"This is the code. is very similar to tensorflow tutorial but uses estimator instead of classifier .                                  
# 6.  Definition del model
def model_fn1(features, labels, mode=None, params=None, config=None):
    # 6.1. Connect the first hidden layer to the # (features[""x""]) with relu activation
    hidden = tf.layers.dense(features[""x""], 10,
                             activation=tf.nn.relu)

    # 6.2. Connect the second hidden layer to the first hidden with elu activation
    hidden1 = tf.layers.dense(hidden, 10, activation=tf.nn.relu)

    # 6.3. Connect output to the the second hidden layer without activation
    out_y = tf.layers.dense(hidden1, 22)
    out_y = tf.reshape(out_y, [-1, 22])

    # 6.4. Provide an estimator spec for `ModeKeys.PREDICT`.
    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(
            mode=mode,
            predictions={'': out_y})
            #predictions = dict(out_y))

    # 6.5. Calculate loss using mean squared error and another approach
    loss = tf.losses.mean_squared_error(labels, out_y)

    # 6.6. Training sub-graph
    optimizer = tf.train.GradientDescentOptimizer(
        learning_rate=0.01)
    train_op = optimizer.minimize(
        loss=loss, global_step=tf.train.get_global_step())

    # 6.7 Calculate root mean squared error as additional eval metric
    eval_metric_ops = {
        ""rmse"": tf.metrics.root_mean_squared_error(
            tf.cast(labels, tf.float64), out_y)
    }

    # 6.8. Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.
    return tf.estimator.EstimatorSpec(
        mode=mode,
        loss=loss,
        train_op=train_op,
        eval_metric_ops=eval_metric_ops)


# 7. Creation of an estimator
estimator1 = tf.estimator.Estimator(model_fn=model_fn1, params=None,
                                    model_dir='/home/jennydariska/targetDirectory/project_1/project1/test/')

# 8. Running of our model
with tf.Session() as session:

    input_fn = tf.estimator.inputs.numpy_input_fn(x={""x"": x_train}, y=y_train,
                                                  shuffle=True, num_epochs=None)

    train_input_fn = tf.estimator.inputs.numpy_input_fn(x={""x"": x_train}, y=y_train,
                                                        num_epochs=None, shuffle=False)

    eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={""x"": x_eval}, y=y_eval,
                                                       num_epochs=1, shuffle=False)
    # 8.1 Training of the estimator
    estimator1.train(input_fn=input_fn, steps=5000)

    # 8.2 Evaluation of how well our model did.
    train_metrics = estimator1.evaluate(input_fn=train_input_fn, steps=500)
    eval_metrics = estimator1.evaluate(input_fn=eval_input_fn)
    print(""train metrics: %r"" % train_metrics)
    print(""eval metrics: %r"" % eval_metrics)
    print(""Loss: %s"" % eval_metrics[""loss""])

    # 8.3 Prediction for the news samples
    predict_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""x"": new_samples}, num_epochs=1,
        shuffle=False)

    predictions = estimator1.predict(input_fn=predict_input_fn)
    print(predictions)

    for i in enumerate(predictions):
        print(""Prediction %s: "" % i )
`                                                                                                                                                                             when i use {}, i get this error: 
<ERROR:tensorflow:==================================
Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):
<tf.Tensor 'report_uninitialized_variables_1/boolean_mask/Gather:0' shape=(?,) dtype=string>
If you want to mark it as used call its ""mark_used()"" method.
>  
 when i use dict(), i get this 
<TypeError: 'Tensor' object is not iterable.>.I can not figure out where the problem lies

> "
13779,MonitoredSession.close() doesn't stop a thread when using SyncReplicasOptimizer hook,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3
- **Python version**: 3.5.2

### Describe the problem
If using SyncReplicasOptimizer, the MonitoredSession is not able to stop a thread when calling session.close(). It will wait the whole ""stop_grace_period_secs"" and then close, reporting that a thread could not be stopped:  `INFO:tensorflow:Coordinator stopped with threads still running: Thread-5` (the thread number may vary).

Here you can find the piece of code (reduced as much as possible) to reproduce it:

### Source code
```
import tensorflow as tf
import logging
logging.getLogger().setLevel(logging.INFO) #To see the message ""Coordinator stopped with threads still running""

opt = tf.train.RMSPropOptimizer(1.0)
opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=1, total_num_replicas=1)
sync_hook = opt.make_session_run_hook(is_chief=True, num_tokens=0)

global_step = tf.train.create_global_step() #global_step is necessary for SyncReplicasOptimizer
var = tf.Variable(0.0) #dummy variable and gradient
grad = tf.constant(1.0)

opt.apply_gradients([(grad, var)], global_step = global_step)

print(""CREATING SESSION"", flush=True)
sess = tf.train.MonitoredSession(hooks = [sync_hook],
                                 stop_grace_period_secs=10) #increasing this number will just make you wait more ;)

print(""CLOSING... here's the problem!"", flush=True)
sess.close()

print(""DONE"", flush=True)
```

### Output
It hangs at sess.close() for ""stop_grace_period_secs"" (showing the message ""CLOSING..."") 
```
INFO:tensorflow:SyncReplicasV2: replicas_to_aggregate=1; total_num_replicas=1
CREATING SESSION
CLOSING... here's the problem!
INFO:tensorflow:Coordinator stopped with threads still running: Thread-1
DONE
```

Sometimes, I additionally get the following traceback (when calling it from ipython I always get it):
```
Traceback (most recent call last):
  File ""/usr/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py"", line 254, in _run
    coord.request_stop(e)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py"", line 211, in request_stop
    six.reraise(*sys.exc_info())
  File ""/usr/local/lib/python3.5/dist-packages/six.py"", line 693, in reraise
    raise value
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py"", line 238, in _run
    enqueue_callable()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1235, in _single_operation_run
    target_list_as_strings, status, None)
  File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.CancelledError: TakeGrad operation was cancelled
         [[Node: sync_replicas/AccumulatorTakeGradient = AccumulatorTakeGradient[_class=[""loc:@sync_replicas/conditional_accumulator""], dtype=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](sync_replicas/conditional_accumulator, sync_replicas/AccumulatorTakeGradient/num_required)]]
```
After several runs of this same script I also got a segmentation fault, but only once."
13778,program could not run in docker but could run on local machine,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
CentOS7 in local machine
Ubuntu 16.04.2 LTS in docker image
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
v1.3.0-rc0-33-g6f0d70e 1.3.0-rc1
- **Python version**: 
3.5.3
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
8.0 6.0
- **GPU model and memory**:
M40 24G



### Describe the problem
I could run my program on local machine, but can not run on docker(ubuntu image).

the stacktrace of the program in nvidia-docker:

```
2017-10-17 09:02:50.757982: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758004: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758020: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758052: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758073: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758089: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758134: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758154: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758178: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758209: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758229: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758246: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758289: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758323: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758337: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758464: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758500: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758532: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758562: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758593: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758624: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758851: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758876: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758891: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758922: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758942: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758957: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.758986: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.759006: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.759021: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.759065: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.759085: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.759100: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.759129: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.759149: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.759170: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.759203: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.759223: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
2017-10-17 09:02:50.759283: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
Traceback (most recent call last):
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1306, in _run_fn
    status, run_metadata)
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
	 [[Node: grads_0/gradients/AddN_2519/_461 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_289171_grads_0/gradients/AddN_2519"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/nfsdata/guotong_data/bi-att-flow-dev/basic/cli.py"", line 113, in <module>
    tf.app.run()
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/nfsdata/guotong_data/bi-att-flow-dev/basic/cli.py"", line 110, in main
    m(config)
  File ""/nfsdata/guotong_data/bi-att-flow-dev/basic/main.py"", line 25, in main
    _train(config)
  File ""/nfsdata/guotong_data/bi-att-flow-dev/basic/main.py"", line 102, in _train
    loss, summary, train_op = trainer.step(sess, batches, get_summary=get_summary)
  File ""/nfsdata/guotong_data/bi-att-flow-dev/basic/trainer.py"", line 71, in step
    loss, train_op = sess.run([self.loss, self.train_op], feed_dict=feed_dict)
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
	 [[Node: grads_0/gradients/AddN_2519/_461 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_289171_grads_0/gradients/AddN_2519"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op 'grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs', defined at:
  File ""/nfsdata/guotong_data/bi-att-flow-dev/basic/cli.py"", line 113, in <module>
    tf.app.run()
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/nfsdata/guotong_data/bi-att-flow-dev/basic/cli.py"", line 110, in main
    m(config)
  File ""/nfsdata/guotong_data/bi-att-flow-dev/basic/main.py"", line 25, in main
    _train(config)
  File ""/nfsdata/guotong_data/bi-att-flow-dev/basic/main.py"", line 87, in _train
    trainer = MultiGPUTrainer(config, models)
  File ""/nfsdata/guotong_data/bi-att-flow-dev/basic/trainer.py"", line 52, in __init__
    grads = self.opt.compute_gradients(loss, var_list=self.var_list)
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 386, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 542, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 348, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 542, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py"", line 689, in _AddGrad
    rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 395, in _broadcast_gradient_args
    name=name)
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'model_0/main/logits2/exp_mask', defined at:
  File ""/nfsdata/guotong_data/bi-att-flow-dev/basic/cli.py"", line 113, in <module>
    tf.app.run()
[elided 2 identical lines from previous traceback]
  File ""/nfsdata/guotong_data/bi-att-flow-dev/basic/main.py"", line 25, in main
    _train(config)
  File ""/nfsdata/guotong_data/bi-att-flow-dev/basic/main.py"", line 84, in _train
    models = get_multi_gpu_models_new(config)
  File ""/nfsdata/guotong_data/bi-att-flow-dev/basic/model_fusion.py"", line 23, in get_multi_gpu_models_new
    model = Model(config, scope, rep=gpu_idx == 0)
  File ""/nfsdata/guotong_data/bi-att-flow-dev/basic/model_fusion.py"", line 75, in __init__
    self._build_forward()
  File ""/nfsdata/guotong_data/bi-att-flow-dev/basic/model_fusion.py"", line 223, in _build_forward
    is_train=self.is_train, func=config.answer_func, scope='logits2')
  File ""/nfsdata/guotong_data/bi-att-flow-dev/my/tensorflow/nn.py"", line 107, in get_logits
    is_train=is_train)
  File ""/nfsdata/guotong_data/bi-att-flow-dev/my/tensorflow/nn.py"", line 83, in linear_logits
    logits = exp_mask(logits, mask)
  File ""/nfsdata/guotong_data/bi-att-flow-dev/my/tensorflow/general.py"", line 138, in exp_mask
    return tf.add(val, (1 - tf.cast(mask, 'float')) * VERY_NEGATIVE_NUMBER, name=name)
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 82, in add
    result = _op_def_lib.apply_op(""Add"", x=x, y=y, name=name)
  File ""/nfsdata/guotong_data/Py35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)

InvalidArgumentError (see above for traceback): Incompatible shapes: [32,1,400] vs. [32,1,213]
	 [[Node: grads_0/gradients/model_0/main/logits2/exp_mask_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape, grads_0/gradients/model_0/main/logits2/exp_mask_grad/Shape_1)]]
	 [[Node: grads_0/gradients/AddN_2519/_461 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_289171_grads_0/gradients/AddN_2519"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]


Process finished with exit code 1
```
the 213 of `[32,1,213]` in stacktrace may change to 160 or other number as I change the GPU type (M40-11G , P100-15G in nvidia-docker)

In local machine it could run without the stacktrace above.
"
13777,s390x support for google/nsync,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: building from source
- **TensorFlow version (use command below)**: master 
- **Python version**:  Python 2.7.12
- **Bazel version (if compiling from source)**: 0.6.1
- **Exact command to reproduce**:  bazel build -c opt //tensorflow/tools/pip_package:build_pip_package

Tensorflow master build is failing on s390x platform with an error below: 
```
$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package
.................................................
ERROR: /home/test/.cache/bazel/_bazel_test/dece280ae0e9dc772a9ff752f1374540/external/nsync/BUILD:401:13: Configurable attribute ""copts"" doesn't match this configuration (would a default condition help?).
Conditions checked:
@nsync//:android_arm
@nsync//:android_arm64
@nsync//:android_armeabi
@nsync//:android_x86_32
@nsync//:android_x86_64
@nsync//:clang_macos_x86_64
@nsync//:gcc_linux_aarch64
@nsync//:gcc_linux_ppc64
@nsync//:gcc_linux_x86_64_1
@nsync//:gcc_linux_x86_64_2
@nsync//:ios_x86_64
@nsync//:msvc_windows_x86_64.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.
```

Recently,  [google/nsync](https://github.com/google/nsync) is added to Tensorflow master as an [external dependency](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl).  
So, s390x support needs to be added to nsync module which includes code changes in BUILD file as well as adding source code in nsync package for s390x.   

We could create platform specific subdirectory inside [nysnc/builds](https://github.com/google/nsync) for s390x by executing  tools/mkmakefile.sh script. Also, could build nsync separately on s390x by executing command make depend test.
However, Tensorflow also uses code from platform-specific sub directories available inside  [nsync/platform/ ](https://github.com/google/nsync/tree/master/platform)  which has source code containing assembly instructions. 

Need information on how to generate similar code structure for s390x (we could see ppc, windows , arm etc platforms are supported).  
@gunan As we are not able to create an issue on nsync repository, could you please let me know whom to contact for this? 

Note: In Tensorflow, nsync dependency is added through [this ](https://github.com/tensorflow/tensorflow/commit/b48cfaea2aea3707a33e60c10385a87e37101b95)commit. 





"
13775,Installed Tensorflow-gpu but i cant import it it gives me this error,"Traceback (most recent call last):
  File ""C:\Users\Mohammad Reza\AppData\Local\conda\conda\envs\my_root\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Mohammad Reza\AppData\Local\conda\conda\envs\my_root\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Mohammad Reza\AppData\Local\conda\conda\envs\my_root\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Mohammad Reza\AppData\Local\conda\conda\envs\my_root\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Mohammad Reza\AppData\Local\conda\conda\envs\my_root\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Mohammad Reza\AppData\Local\conda\conda\envs\my_root\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Mohammad Reza\AppData\Local\conda\conda\envs\my_root\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Mohammad Reza\AppData\Local\conda\conda\envs\my_root\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Mohammad Reza\AppData\Local\conda\conda\envs\my_root\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Mohammad Reza\AppData\Local\conda\conda\envs\my_root\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Mohammad Reza\AppData\Local\conda\conda\envs\my_root\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Mohammad Reza\AppData\Local\conda\conda\envs\my_root\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Mohammad Reza\AppData\Local\conda\conda\envs\my_root\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Mohammad Reza\AppData\Local\conda\conda\envs\my_root\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Mohammad Reza\AppData\Local\conda\conda\envs\my_root\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
13774,Cannot install Tensorflow with specific GCC version,"I'm trying to install Tensorflow 1.3 without success 

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Gentoo
- **TensorFlow installed from (source or binary)**: Trying from source
- **TensorFlow version (use command below)**: 1.3
- **Python version**: 3.4.5
- **Bazel version (if compiling from source)**: 0.5.2
- **CUDA/cuDNN version**: 8.0 / 5.1.5
- **GPU model and memory**: GTX 960
- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

### Describe the problem

I'm trying to install Tensorflow 1.3 (branch r1.3) from sources on my Gentoo machine. 

The problem with the default compiler is that it it's a version 6.3 that is not supported by Tensorflow: I've tried and got an error indicating that GCC later than 5 are not supported. 
Therefore, I need to specify a specific GCC version. I'm setting it via $CC/$CXX and in the installation for CUDA:

`build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc-5.4.0""`

But when I try to run bazel, I got the following error: 

> gcc-5.4.0: error trying to exec 'cc1': execvp: No such file or directory

What can I do to install Tensorflow using /usr/bin/gcc-5.4.0 and /usr/bin/g++-5.4.0 rather than /usr/bin/gcc ? 

I've seen this f0faf5139819e2c6d7b0437d9e03ffce71c7d6e5 that seems releated, but in my case, this is already done. And it seems to be used for clang compilation anyway. 

### Source code / logs

> ____Loading complete.  Analyzing...
> WARNING: /home/wichtounet/dev/tensorflow13/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately..
> WARNING: /home/wichtounet/dev/tensorflow13/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately..
> ____Found 1 target...
> ____Building...
> ____[3 / 255] Writing file external/snappy/libsnappy.a-2.params [for host]
> ERROR: /home/wichtounet/.cache/bazel/_bazel_wichtounet/bb5d0ce0cffd837b9339296f34d5478c/external/nasm/BUILD.bazel:8:1: C++ compilation of rule '@nasm//:nasm' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 34 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
> gcc-5.4.0: error trying to exec 'cc1': execvp: No such file or directory
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> ____Elapsed time: 0.285s, Critical Path: 0.08s"
13773,ERROR:tensorflow:Exception in QueueRunner: truncated record at 935047,"```
###########################small data train, train_step code:
def run_train():
    with tf.Graph().as_default():
        global_step = tf.Variable(0, trainable=False)
        images,labels=read_and_decode('./dog_train')
        images_batch,labels_batch = add_batch(images,labels,5,5)
        softmax_linear = cnn_model(images_batch)
        loss = loss(softmax_linear, labels_batch)
        train_op = train(loss, global_step)
        saver = tf.train.Saver(tf.global_variables())
        summary_op = tf.summary.merge_all()
        init = tf.global_variables_initializer()
        sess = tf.Session(config=tf.ConfigProto(log_device_placement=FLAGS.log_device_placement))
        sess.run(init)
        tf.train.start_queue_runners(sess=sess)
        summary_writer = tf.summary.FileWriter(FLAGS.train_dir,graph=sess.graph)
        for step in xrange(FLAGS.max_steps):
            start_time = time.time()
            _, loss_value = sess.run([train_op, loss])
            duration = time.time() - start_time
            assert not np.isnan(loss_value), 'Model diverged with loss = NaN'
            if step % 10 == 0:
                num_examples_per_step = FLAGS.batch_size
                examples_per_sec = num_examples_per_step / duration
                sec_per_batch = float(duration)

                format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f ''sec/batch)')
                print(format_str % (datetime.now(), step, loss_value,examples_per_sec, sec_per_batch))
            if step % 100 == 0:
                summary_str = sess.run(summary_op)
                summary_writer.add_summary(summary_str, step)
            if step % 1000 == 0 or (step + 1) == FLAGS.max_steps:
                checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')
                saver.save(sess,checkpoint_path, global_step=step)
#######################################and ERROR:
2017-10-17 15:08:31.510128: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x46f9230
ERROR:tensorflow:Exception in QueueRunner: truncated record at 935047
	 [[Node: ReaderReadV2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](TFRecordReaderV2, input_producer)]]
	 [[Node: DecodeRaw/_9 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_17_DecodeRaw"", tensor_type=DT_UINT8, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
Exception in thread Thread-3:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/usr/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py"", line 237, in _run
    enqueue_callable()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1060, in _single_operation_run
    target_list_as_strings, status, None)
  File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
    self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
DataLossError: truncated record at 935047
	 [[Node: ReaderReadV2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](TFRecordReaderV2, input_producer)]]
	 [[Node: DecodeRaw/_9 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_17_DecodeRaw"", tensor_type=DT_UINT8, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

ERROR:tensorflow:Exception in QueueRunner: truncated record at 935047
	 [[Node: ReaderReadV2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](TFRecordReaderV2, input_producer)]]
Exception in thread Thread-2:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/usr/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py"", line 237, in _run
    enqueue_callable()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1060, in _single_operation_run
    target_list_as_strings, status, None)
  File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
    self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
DataLossError: truncated record at 935047
	 [[Node: ReaderReadV2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](TFRecordReaderV2, input_producer)]]

2017-10-17 15:08:49.270117: step 0, loss = 21.79 (0.6 examples/sec; 8.205 sec/batch)
 ############ here stop 
```
so,why this errors ?


"
13772,No OpKernel was registered to support Op 'StridedSlice',"## Wrong usage. Fixed.

I trained a model via python and load the freeze model for prediction using c++. When running prediction binary, some errors occur.

OS: macOS 10.12.6
Tensorflow: master. built from source via `makefile/build_all_ios.sh`

Deadly error:
```
Invalid argument: No OpKernel was registered to support Op 'StridedSlice' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_BOOL]
  device='CPU'; T in [DT_BFLOAT16]

	 [[Node: emb_179/embedding_lookup_sparse/strided_slice = StridedSlice[Index=DT_INT32, T=DT_INT64, begin_mask=1, ellipsis_mask=0, end_mask=1, new_axis_mask=0, shrink_axis_mask=2](input/linear_179/index/Placeholder, emb_179/embedding_lookup_sparse/strided_slice/stack, emb_179/embedding_lookup_sparse/strided_slice/stack_1, emb_179/embedding_lookup_sparse/strided_slice/stack_2)]]
```

Full error message:
```
Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
Session created successfully
Load graph protobuf successfully
2017-10-17 12:51:52.708004: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""PopulationCount"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT32 } } }') for unknown op: PopulationCount
2017-10-17 12:51:52.708091: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""BitwiseAnd"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseAnd
2017-10-17 12:51:52.708111: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""BitwiseXor"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseXor
2017-10-17 12:51:52.708138: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""EncodeWav"" device_type: ""CPU""') for unknown op: EncodeWav
2017-10-17 12:51:52.708147: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""DecodeWav"" device_type: ""CPU""') for unknown op: DecodeWav
2017-10-17 12:51:52.708188: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""Mfcc"" device_type: ""CPU""') for unknown op: Mfcc
2017-10-17 12:51:52.708205: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""Invert"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT8 } } }') for unknown op: Invert
2017-10-17 12:51:52.708277: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""BitwiseOr"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseOr
2017-10-17 12:51:52.708346: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""AudioSpectrogram"" device_type: ""CPU""') for unknown op: AudioSpectrogram
Invalid argument: No OpKernel was registered to support Op 'StridedSlice' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_BOOL]
  device='CPU'; T in [DT_BFLOAT16]

	 [[Node: emb_179/embedding_lookup_sparse/strided_slice = StridedSlice[Index=DT_INT32, T=DT_INT64, begin_mask=1, ellipsis_mask=0, end_mask=1, new_axis_mask=0, shrink_axis_mask=2](input/linear_179/index/Placeholder, emb_179/embedding_lookup_sparse/strided_slice/stack, emb_179/embedding_lookup_sparse/strided_slice/stack_1, emb_179/embedding_lookup_sparse/strided_slice/stack_2)]]
```"
13771,[Error] unknown op in C++,"I load a simple graph `c=a*b` in C++ and do prediction for `c`. It gives right result but some errors `unknown op` occur. What's the problem?

OS: macOS 10.12.6
Tensorflow: master. build from source using `makefile/build_all_ios.sh`

prediction cpp code:
```
GraphDef graph_def;
std::string graph_path = argv[1];
status = ReadBinaryProto(Env::Default(), graph_path, &graph_def);
if (!status.ok()) {
    std::cout << status.ToString() << std::endl;
    return 1;
} else {
    std::cout << ""Load graph protobuf successfully"" << std::endl;
}
status = session->Create(graph_def);
if (!status.ok()) {
    std::cout << status.ToString() << std::endl;
    return 1;
} else {
    std::cout << ""Add graph to session successfully"" << std::endl;
}

// Setup inputs and outputs:
Tensor a(DT_FLOAT, TensorShape());
a.scalar<float>()() = 3.0;
Tensor b(DT_FLOAT, TensorShape());
b.scalar<float>()() = 2.0;
std::vector<std::pair<string, tensorflow::Tensor>> inputs = {
    { ""a"", a },
    { ""b"", b },
};

std::vector<tensorflow::Tensor> outputs;

// Run the session, evaluating our ""c"" operation from the graph
status = session->Run(inputs, {""c""}, {}, &outputs);
if (!status.ok()) {
    std::cout << status.ToString() << std::endl;
    return 1;
} else {
    std::cout << ""Run session successfully"" << std::endl;
}

auto output_c = outputs[0].scalar<float>();
std::cout << outputs[0].DebugString() << std::endl; 
std::cout << ""output value: "" << output_c() << std::endl; 

session->Close();
```

Full error message:
```
2017-10-17 12:32:11.796090: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
Session created successfully
Load graph protobuf successfully
2017-10-17 12:32:11.808591: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""PopulationCount"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT32 } } }') for unknown op: PopulationCount
2017-10-17 12:32:11.808658: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""BitwiseAnd"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseAnd
2017-10-17 12:32:11.808669: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""BitwiseXor"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseXor
2017-10-17 12:32:11.808691: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""EncodeWav"" device_type: ""CPU""') for unknown op: EncodeWav
2017-10-17 12:32:11.808699: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""DecodeWav"" device_type: ""CPU""') for unknown op: DecodeWav
2017-10-17 12:32:11.808732: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""Mfcc"" device_type: ""CPU""') for unknown op: Mfcc
2017-10-17 12:32:11.808747: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""Invert"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT8 } } }') for unknown op: Invert
2017-10-17 12:32:11.808806: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""BitwiseOr"" device_type: ""CPU"" constraint { name: ""T"" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseOr
2017-10-17 12:32:11.808857: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: ""AudioSpectrogram"" device_type: ""CPU""') for unknown op: AudioSpectrogram
Add graph to session successfully
Run session successfully
Tensor<type: float shape: [] values: 6>
output value: 6
```"
13769,How periodicaly evaluate the Performance of Models in TF-Slim?,"I am trying to use [DensNet][1] for regression problem with TF-Slim. My data contains 60000 jpeg images with 37 float labels for each image. I divided my data into three different tfrecords files of a train set (60%), a validation set (20%) and a test set (20%). 

I need to evaluate validation set during training loop and make a plot like [image][2]. 
In TF-Slim documentation they just explain train loop and evaluation loop separately. I can just evaluate validation or test set after training loop finished. While as I said I need to evaluate during training.

I tried to use slim.evaluation.evaluation_loop function instead of slim.evaluation.evaluate_once. But it doesn't help.

    slim.evaluation.evaluation_loop(
        master=FLAGS.master,
        checkpoint_dir=checkpoint_path,
        logdir=FLAGS.eval_dir,
        num_evals=num_batches,
        eval_op=list(names_to_updates.values()) + print_ops,
        variables_to_restore=variables_to_restore,
        summary_op = tf.summary.merge(summary_ops),
        eval_interval_secs = eval_interval_secs )

I tried evaluation.evaluate_repeatedly as well.

    from tensorflow.contrib.training.python.training import evaluation

    evaluation.evaluate_repeatedly(
        master=FLAGS.master,
        checkpoint_dir=checkpoint_path,
        eval_ops=list(names_to_updates.values()) + print_ops,
        eval_interval_secs = eval_interval_secs )

In both of these functions, they just read the latest available checkpoint from checkpoint_dir and apparently waiting for the next one, however when the new checkpoints are generated, they don't perform at all.

I use Python 2.7.13 and Tensorflow 1.3.0 on CPU.

Any help will be highly appreciated.

  [1]: https://github.com/pudae/tensorflow-densenet
  [2]: https://i.stack.imgur.com/HzLPq.jpg"
13768,Packages missing in current channels: - tensorflow,"I typed the following in the Anaconda Prompt
""
conda create --name=IntroToTensorFlow python=3 anaconda
source activate IntroToTensorFlow
conda install -c conda-forge tensorflow""

Then it shows

Packages missing in current channels: 
      -  tensorflow
- http://conda.anaconda.org/conda-forge/win-32
- http://conda.anaconda.org/conda-forge/noarch
- http://repo.continuum.io/pkgs/main/win-32
- http://repo.continuum.io/pkgs/main/noarch
- http://repo.continuum.io/pkgs/free/win-32
- http://repo.continuum.io/pkgs/free/noarch
- http://repo.continuum.io/pkgs/r/win-32
- http://repo.continuum.io/pkgs/r/noarch
- http://repo.continuum.io/pkgs/pro/win-32
- http://repo.continuum.io/pkgs/pro/noarch
- http://repo.continuum.io/pkgs/msys2/win-32
- http://repo.continuum.io/pkgs/msys2/noarch
"
13766,Add sparse tensor support to `tf.data.Dataset`,"The `tf.data.Dataset` class does not currently recognize a `tf.SparseTensor` object as a component of a dataset element. This makes it difficult to use the full capabilities of `SparseTensor`-producing ops such as `tf.parse_single_example()` in `Dataset.map()` transformations, and then manipulating those elements using operations like `Dataset.batch()`. The existing `tf.train.batch()` and related functions for queue-based pipelines support `tf.SparseTensor`, and we should add support to `Dataset` for parity.

This [Stack Overflow answer](https://stackoverflow.com/a/46732695/3574081) suggests some possible workarounds in the meantime."
13765,"Import ""google/protobuf/any.proto"" was not found or had errors."," make -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI OPTFLAGS=""-Os"" CXX=g++-4.8
PROTOC = ""protoc""
CC_PREFIX = """"
protoc  tensorflow/contrib/boosted_trees/proto/learner.proto --cpp_out /home/a_name/tensorflow/tensorflow/contrib/makefile/gen/proto/
protoc  tensorflow/contrib/boosted_trees/proto/quantiles.proto --cpp_out /home/a_name/tensorflow/tensorflow/contrib/makefile/gen/proto/
protoc  tensorflow/contrib/boosted_trees/proto/split_info.proto --cpp_out /home/a_name/tensorflow/tensorflow/contrib/makefile/gen/proto/
protoc  tensorflow/contrib/boosted_trees/proto/tree_config.proto --cpp_out /home/a_name/tensorflow/tensorflow/contrib/makefile/gen/proto/
protoc  tensorflow/core/util/test_log.proto --cpp_out /home/a_name/tensorflow/tensorflow/contrib/makefile/gen/proto/
google/protobuf/any.proto: File not found.
tensorflow/core/util/test_log.proto: Import ""google/protobuf/any.proto"" was not found or had errors.
tensorflow/core/util/test_log.proto:132:12: ""google.protobuf.Any"" is not defined.
make: *** [/home/a_name/tensorflow/tensorflow/contrib/makefile/gen/proto/tensorflow/core/util/test_log.pb.cc] Error 1"
13764,Failure in TestNewTensor when running go test,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source (branch 1.4)
- **TensorFlow version (use command below)**: 1.4.0-dev
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: nVidia 1080Ti 11G
- **Exact command to reproduce**: go test -v github.com/tensorflow/tensorflow/tensorflow/go

### Describe the problem

I'm trying to use the go bindings to the tensorflow c library. When I run the tests, I get a nil pointer dereference and a segfault. The details are below. Note that I've built the c library from source using the following options:

`bazel build -c opt --config=cuda --config=mkl -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 -c opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow:libtensorflow.so`

### Source code / logs
When I run go test -v github.com/tensorflow/tensorflow/tensorflow/go I get the following error:
```
2017-10-16 17:12:30.568054: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
2017-10-16 17:12:30.568065: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
--- FAIL: TestNewTensor (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
        panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x536098]

goroutine 168 [running]:
testing.tRunner.func1(0xc42059c4e0)
        /usr/lib/go-1.8/src/testing/testing.go:622 +0x29d
panic(0x6a0b80, 0xa18e80)
        /usr/lib/go-1.8/src/runtime/panic.go:489 +0x2cf
github.com/tensorflow/tensorflow/tensorflow/go.tensorData(0x7fa8f40195b0, 0xc420595900, 0x688a80, 0x6ffb90)
        /home/vishvananda/go/src/github.com/tensorflow/tensorflow/tensorflow/go/tensor.go:209 +0x48
github.com/tensorflow/tensorflow/tensorflow/go.NewTensor(0x683d20, 0xc4205945e0, 0xc42004d9a0, 0x2, 0x2)
        /home/vishvananda/go/src/github.com/tensorflow/tensorflow/tensorflow/go/tensor.go:92 +0x221
github.com/tensorflow/tensorflow/tensorflow/go.TestNewTensor(0xc42059c4e0)
        /home/vishvananda/go/src/github.com/tensorflow/tensorflow/tensorflow/go/tensor_test.go:92 +0x2526
testing.tRunner(0xc42059c4e0, 0x6ffbd0)
        /usr/lib/go-1.8/src/testing/testing.go:657 +0x96
created by testing.(*T).Run
        /usr/lib/go-1.8/src/testing/testing.go:697 +0x2ca
exit status 2
FAIL    github.com/tensorflow/tensorflow/tensorflow/go  0.443s
```
Adding some debugging, it turns out that the TestNewTensor test fails when attempting to create the following tensor `{[]int64{2, 0}, [][]int64{{}, {}}}`. If I comment out that line, the tests pass."
13763,Modify the TensorFlow Scheduler and Runtime to Change the Operations Priority,"Hi,

I am trying to modify the TensorFlow scheduler and runtime to change the operation priorities. As my understanding, TensorFlow has inter-operation and intra-operation thread pool with a scheduler scheduling operations for different threads and there is also a FIFO queue of operations for operations waiting. The workflow between them is that operations are sent to the inter-op thread pool from the executor, and then that work is running through XLA compiler to eventually be executed on the intra-op thread pool. Schedule() is in inside a specific ThreadPool and is to schedule function for the threads in the pool. The thread scheduler selects a subset of threads to run at any given moment. When the tasks are passed to the ThreadPool, they are added to one of the scheduler thread’s FIFO queues and then the scheduler will pick up the tasks distributed to the available worker threads. I have all the control and data dependencies got from the Graph in TensorFlow by using TensorBoard. Now I am not sure whether my understanding for the TensorFlow scheduler and runtime is correct or not.

The problem is that I still have no clue how and where to modify the threadpool/scheduler/ready_queue or others to change the operations priority/sequence for different threads at the TensorFlow runtime by modifying the source code. Does anyone have any ideas?"
13762,How are threads bound to physical cores in TensorFlow runtime and inter/intra threadpools?,"Hi all,

If one CPU has 8 cores and each core has two threads, does TF runtime use the OS threads scheduler to match the logical cores and threads to the physical cores and threads? Or does it has its own policy to match the threads and cores? Does TF only recognize cores and but not the whole device(CPU) like the system does? (The OS only knows the number/id of cores and sockets and doesn't know how many CPU devices) Does the ""device"" in TF correspond to the physical core, e.g., TF recognizes 8 devices in the above CPU?

I found out that there are inter/intra global and local threadpools in /tensorflow/core/common_runtime/local_device.cc and /tensorflow/core/common_runtime/dirrect_session.cc. Does that mean each device (or core?) have its own local inter/intra threadpool and there is only one inter/intra global threadpool in the whole TF runtime?

Thank you for your time."
13761,tf.reduce_sum gives value error when given int64 as input.,"Passing a tensor of dtype=int64 into tf.reduce sum I receive the following error:

Tensor(""loss/diff:0"", shape=(50,), dtype=int64)
ValueError: Invalid type tf.int64 for loss/Sum:0, expected: [tf.float32, tf.float64, tf.float16].

According to the documents from https://www.tensorflow.org/api_docs/python/tf/reduce_sum:

input_tensor: The tensor to reduce. Should have numeric type.

As int64 is a numeric type I am not sure what's wrong. 

To create the diff tensor I do:

```
        self.predictions = tf.argmax(self.logits, 1, name='predictions')

         # Loss
        with tf.name_scope(""loss""):
            self.diff = tf.subtract(self.predictions, self.targets)
            self.diff = tf.multiply(self.diff,self.diff, name='diff')
            print self.diff
            self.diff = tf.reduce_sum(self.diff)
```

Where predictions is of type int64 and my targets placeholder is also of type int64.

Is this a tensorflow error or an error on my end?
"
13756,Does this version support the CUDA 9.0?,"Does this version support the CUDA 9.0?

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13754,graph_editor uses deprecated API,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: b'v1.3.0-21-gc701d19b2' 1.3.0
- **Python version**: 3.6.0
- **Bazel version (if compiling from source)**: 0.5.1
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See below

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
When I use graph_editor, my console fills up with dozens of repetitions of the following error message:

```
WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.
```
The number of repetitions of the message scales with the number of tensors being copied and can get very large.  To reproduce the problem, run the script below.

### Source code / logs
```python
import tensorflow as tf

x = tf.Variable(1.0)
y = tf.Variable(2.0)
a = x+1
replace = {tf.convert_to_tensor(x):tf.convert_to_tensor(y)}
b = tf.contrib.graph_editor.graph_replace(a, replace)
```
"
13751,Parsing TFRecords bug in TensorFlow v1.2,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.2.0-rc2-21-g12f033d', '1.2.0')
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See below

### Describe the problem
I believe there's a bug in TensorFlow v1.2. The code below runs fine in v1.4, while erroring out in v1.2. Here's the code:
```python
filenames = [""gs://bucket/file.tfrecords""]
dataset = tf.contrib.data.TFRecordDataset(filenames)
parse_fn = lambda r: tf.parse_single_example(r, {""f1"": tf.VarLenFeature(tf.int64)})
dataset.map(parse_fn)
```

Here's the stacktrace that I get:
```
TypeError                                 Traceback (most recent call last)
<ipython-input-42-0473d32d7931> in <module>()
----> 1 dataset.map(parser)

/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.pyc in map(self, map_func, num_threads, output_buffer_size)
    811       A `Dataset`.
    812     """"""
--> 813     return MapDataset(self, map_func, num_threads, output_buffer_size)
    814
    815   def flat_map(self, map_func):

/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.pyc in __init__(self, input_dataset, map_func, num_threads, output_buffer_size)
   1434
   1435     self._map_func = tf_map_func
-> 1436     self._map_func.add_to_graph(ops.get_default_graph())
   1437     if num_threads is not None:
   1438       self._num_threads = ops.convert_to_tensor(

/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/python/framework/function.pyc in add_to_graph(self, g)
    617   def add_to_graph(self, g):
    618     """"""Adds this function into the graph g.""""""
--> 619     self._create_definition_if_needed()
    620
    621     # pylint: disable=protected-access

/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/contrib/data/python/framework/function.pyc in _create_definition_if_needed(self)
    165       # Call func and gather the output tensors.
    166       with vs.variable_scope("""", custom_getter=temp_graph.getvar):
--> 167         outputs = self._func(*inputs)
    168       # If func only returned one value, make it a tuple.
    169       if not isinstance(outputs, (list, tuple)):

/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.pyc in tf_map_func(*args)
   1425
   1426       # Extract shape information from the returned values.
-> 1427       flattened_ret = [ops.convert_to_tensor(t) for t in nest.flatten(ret)]
   1428       self._output_shapes = nest.pack_sequence_as(
   1429           ret, [t.get_shape() for t in flattened_ret])

/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in convert_to_tensor(value, dtype, name, preferred_dtype)
    674       name=name,
    675       preferred_dtype=preferred_dtype,
--> 676       as_ref=False)
    677
    678

/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)
    739
    740         if ret is None:
--> 741           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    742
    743         if ret is NotImplemented:

/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    111                                          as_ref=False):
    112   _ = as_ref
--> 113   return constant(v, dtype=dtype, name=name)
    114
    115

/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in constant(value, dtype, shape, name, verify_shape)
    100   tensor_value = attr_value_pb2.AttrValue()
    101   tensor_value.tensor.CopyFrom(
--> 102       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
    103   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
    104   const_tensor = g.create_op(

/Users/stypka/env/tf-1.2/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.pyc in make_tensor_proto(values, dtype, shape, verify_shape)
    460       raise TypeError(""Failed to convert object of type %s to Tensor. ""
    461                       ""Contents: %s. Consider casting elements to a ""
--> 462                       ""supported type."" % (type(values), values))
    463     tensor_proto.string_val.extend(str_values)
    464     return tensor_proto

TypeError: Failed to convert object of type <type 'dict'> to Tensor. Contents: {'f1': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x117918410>}. Consider casting elements to a supported type.
```

And again, this works fine in TensorFlow v1.4.0rc0 that I tested.

Let me know if I should provide any more info!"
13746,GraphDef ParseFromString error,"I load a `graph.pb` saved by `tf.train.write_graph(sess.graph.as_graph_def(), FLAGS.model_dir, 'graph.pb', as_text=False)` via blow code. But an error occur. Why?

```
graph_def = graph_pb2.GraphDef()
with tf.gfile.GFile('./graph.pb', 'rb') as f:
    graph_def.ParseFromString(f.read())
```

```
File ""../../python/graph_test.py"", line 53, in freeze_graph
    graph_def.ParseFromString(f.read())
  File ""/Users/anaconda/lib/python3.6/site-packages/google/protobuf/message.py"", line 185, in ParseFromString
    self.MergeFromString(serialized)
  File ""/Users/anaconda/lib/python3.6/site-packages/google/protobuf/internal/python_message.py"", line 1069, in MergeFromString
    if self._InternalParse(serialized, 0, length) != length:
  File ""/Users/anaconda/lib/python3.6/site-packages/google/protobuf/internal/python_message.py"", line 1095, in InternalParse
    new_pos = local_SkipField(buffer, new_pos, end, tag_bytes)
  File ""/Users/anaconda/lib/python3.6/site-packages/google/protobuf/internal/decoder.py"", line 850, in SkipField
    return WIRETYPE_TO_SKIPPER[wire_type](buffer, pos, end)
  File ""/Users/anaconda/lib/python3.6/site-packages/google/protobuf/internal/decoder.py"", line 820, in _RaiseInvalidWireType
    raise _DecodeError('Tag had invalid wire type.')
google.protobuf.message.DecodeError: Tag had invalid wire type.
```"
13745,tf.contrib.data.Dataset  does not handle well with last  elements with is fewer than batch size,"tf.contrib.data.Dataset  does not handle well with last  elements with is fewer than batch size.

Maybe batch_size = 10, but last batch has 9 elements."
13744,TensorFlow: lack of repeatability in tf.estimator.DNNClassifier,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

custom code: no, it is the one in https://www.tensorflow.org/get_started/estimator
system: Apple
OS: Mac OsX 10.13
TensorFlow version: 1.3.0
Python version: 3.6.3
GPU model: AMD FirePro D700 (actually, two such GPUs)


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Dear all,

I wish to use TensorFlow to perform classification on medical datasets.
To do this, I am using exactly the same code as that proposed in https://www.tensorflow.org/get_started/estimator, the only difference being that I face medical datasets rather than the Iris database. So, it is not custom code.

I do need help about the following problem: if I run the same code on the same data with the same parameters for the network configuration (number of layers, number of neurons in each layer, and so on) , the results are different. I mean, I run the same code ten times, and I obtain ten different values for the accuracy. These values are even largely different, as they range from 73% up to 83%.
This means that the subjects considered suffering from a given disease vary from a run to another. Differently said, once set a network structure, there are several subjects who are considered either healthy or sick depending on the run only.
As you can imagine, this lack of repeatability makes that code useless from both scientific and medical viewpoints: another user, running the same configuration over the same data set, would find a different model and different results, so would cure different subjects. 
I have noticed that for the Iris database the problem seems not to take place, and accuracy is always 0.9666. This depends on the problem being very easy (linearly separable for all but one items, and very small data set). 

I have carried out a search on the internet, and I have found several other people who have noted the same problem. As for the possible solutions, I have read several as well, I have implemented them all, with no result.

Here I add a short list of some suggested remedies that failed in my case:

os.environ['PYTHONHASHSEED'] = '0'
np.random.seed(0)
tf.set_random_seed(0)
rn.seed(0)
tf.reset_default_graph()

session_conf = tf.ConfigProto(
    intra_op_parallelism_threads=1,
    inter_op_parallelism_threads=1
)
sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)

Is there any chance to fix this problem? It is a pity that such an excellent tool, as TensorFlow is, cannot guarantee repeatibility.

I am using TensorFlow 1.3.0, Python 3.6.3, and an Apple with Mac OsX 10.13.

Thank you very much!

Best regards

Ivanoe De Falco




### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Source code is in:
https://www.tensorflow.org/get_started/estimator
"
13742,libtensorflow_cc.so need to work on single (but not specific) core. (feature request),"I use self builded libtensorflow_cc.so shared library in my c++ model ""player"" program. I builded *.so for cpu-only usage. But by default in runtime it uses all cores. But I want to use only one core.
If i run my program through taskset, for example, I can use specific mask:
$taskset -c 0 ./my_program param1 param2
And it will perfectly work, but only on 0-th core of my cpu. And if I run several parallel programs with theese parameters all of them will work on 0-th core, while another cores are free.
I need that for my service, for several parallel program works on different cores.
How can I build libtensorflow_cc.so WITHOUT multi-threading? Or maybe how can I configure it for single-core (but I repeat, not specific core)?
Thank you."
13741,Decoding and resizing image is giving unknown tensor shape,"I'm trying to load two images, one is .png and another is .jpg, to tensorflow and resize them to 100x100 pixel size using `tf.image.pad_to_bounding_box`, so that they will be of same size and can be used for training. Here's my code:

```
import os
import tensorflow as tf

def decode(image_data):
    return tf.image.decode_image(image_data, channels=3)

def adjust_paddig(image_tensor):
    return tf.image.pad_to_bounding_box(image_tensor, offset_height=0, offset_width=0, target_height=100, target_width=100)

def load(images_paths):
    filename_queue = tf.train.string_input_producer(images_paths)
    reader = tf.WholeFileReader()
    _, image_file = reader.read(filename_queue)
    image_tensor = decode(image_file)
    padded_image_tensor = adjust_paddig(image_tensor)
    return padded_image_tensor

if __name__ == '__main__':
    IMAGES_PATH = [""images/1.png"",""images/2.jpg""] # Both image are of different shape
    class_images_tensor = load(IMAGES_PATH)
    print(class_images_tensor.shape)
```

But some how the resized image size is not proper. It's displaying height and width but not depth(I mean channels).

`Output: (100,100,?) #height, width are 100, but depth is '?'`

and Surprisingly, It's giving same output for invalid paths also.

`Eg: IMAGES_PATH = ['images/']  `

What Am I doing wrong? Please help."
13740,AV in nvcuda on Win10 amd64,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: example script startup
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win10 amd64 10.0.16291.0
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: b'unknown' 1.3.0
- **Python version**:  3.6.3
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0.61.2/6.0
- **GPU model and memory**: nVidia 1080Ti
- **Exact command to reproduce**:

```
import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
```

### Describe the problem

Access violation in nvcuda

### Source code / logs

```
2017-10-15 22:51:24.306411: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-10-15 22:51:24.306463: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
```

```
0:000> kn
 # Child-SP          RetAddr           Call Site
00 000000c2`28be6150 00007fff`ace93028 nvcuda!cuTexRefSetAddress+0x309622
01 000000c2`28be6180 00007fff`ace92ac2 nvcuda!cuTexRefSetAddress+0x1759ea
02 000000c2`28be61b0 00007fff`ad02abf2 nvcuda!cuTexRefSetAddress+0x175484
03 000000c2`28be6250 00007fff`ace950d6 nvcuda!cuTexRefSetAddress+0x30d5b4
04 000000c2`28beddd0 00007fff`ace5c2e0 nvcuda!cuTexRefSetAddress+0x177a98
05 000000c2`28bedee0 00007fff`ace5acfe nvcuda!cuTexRefSetAddress+0x13eca2
06 000000c2`28bedf10 00007fff`ace5aa77 nvcuda!cuTexRefSetAddress+0x13d6c0
07 000000c2`28bedf50 00007fff`acec6617 nvcuda!cuTexRefSetAddress+0x13d439
*** WARNING: Unable to verify checksum for C:\Python36\lib\site-packages\tensorflow\python\_pywrap_tensorflow_internal.pyd
*** ERROR: Symbol file could not be found.  Defaulted to export symbols for C:\Python36\lib\site-packages\tensorflow\python\_pywrap_tensorflow_internal.pyd - 
08 000000c2`28bedf80 00007fff`8d0e1b50 nvcuda!cuTexRefSetAddress+0x1a8fd9
09 000000c2`28bee040 00007fff`8d0e185e _pywrap_tensorflow_internal!perftools::gputools::port::InternalError+0xe0
0a 000000c2`28bee250 00007fff`8d0eebd4 _pywrap_tensorflow_internal!perftools::gputools::cuda::CUDADriver::Init+0x10e
0b 000000c2`28bee2a0 00007fff`8b6225d0 _pywrap_tensorflow_internal!perftools::gputools::cuda::CudaPlatform::VisibleDeviceCount+0x14
0c 000000c2`28bee2d0 00007fff`8b62043b _pywrap_tensorflow_internal!tensorflow::BaseGPUDeviceFactory::GetValidDeviceIds+0x120
0d 000000c2`28bee5f0 00007fff`8b51dd9c _pywrap_tensorflow_internal!tensorflow::BaseGPUDeviceFactory::CreateDevices+0x25b
0e 000000c2`28bee7a0 00007fff`8b8e96ee _pywrap_tensorflow_internal!tensorflow::DeviceFactory::AddDevices+0x24c
0f 000000c2`28bee880 00007fff`8b71742c _pywrap_tensorflow_internal!tensorflow::DirectSessionFactory::NewSession+0xae
10 000000c2`28beea30 00007fff`8b4c6d9f _pywrap_tensorflow_internal!tensorflow::NewSession+0x12c
11 000000c2`28beeb90 00007fff`8b4aa72b _pywrap_tensorflow_internal!TF_NewDeprecatedSession+0x1f
*** ERROR: Symbol file could not be found.  Defaulted to export symbols for C:\Python36\python36.dll - 
12 000000c2`28beebc0 00000000`604aad02 _pywrap_tensorflow_internal!tensorflow::Scope::status+0x138ab
13 000000c2`28beec10 00000000`604aa413 python36!PyCFunction_FastCallDict+0x182
14 000000c2`28beec90 00000000`604888d8 python36!PyObject_CallFunctionObjArgs+0x383
15 000000c2`28beed70 00000000`604ab284 python36!PyEval_EvalFrameDefault+0x3c8
16 000000c2`28beef00 00000000`604aa87f python36!Py_CheckFunctionResult+0x314
17 000000c2`28beefb0 00000000`6048a03c python36!PyObject_CallFunctionObjArgs+0x7ef
18 000000c2`28bef090 00000000`604ab284 python36!PyEval_EvalFrameDefault+0x1b2c
19 000000c2`28bef220 00000000`604a9f98 python36!Py_CheckFunctionResult+0x314
1a 000000c2`28bef2d0 00000000`604a9d65 python36!PyFunction_FastCallDict+0x1b8
1b 000000c2`28bef3a0 00000000`604ad33c python36!PyUnicode_Partition+0x745
1c 000000c2`28bef450 00000000`604ab067 python36!PyType_GenericAlloc+0x72c
1d 000000c2`28bef4d0 00000000`604aa76f python36!Py_CheckFunctionResult+0xf7
1e 000000c2`28bef500 00000000`604888d8 python36!PyObject_CallFunctionObjArgs+0x6df
1f 000000c2`28bef5e0 00000000`604ab284 python36!PyEval_EvalFrameDefault+0x3c8
20 000000c2`28bef770 00000000`604b5ee3 python36!Py_CheckFunctionResult+0x314
21 000000c2`28bef820 00000000`604b5e41 python36!PyEval_EvalCodeEx+0x9b
22 000000c2`28bef8b0 00000000`604b5deb python36!PyEval_EvalCode+0x2d
23 000000c2`28bef920 00000000`6060a864 python36!PyArena_Free+0xa7
24 000000c2`28bef960 00000000`6060a514 python36!PyRun_InteractiveOneObject+0x2b8
25 000000c2`28befa00 00000000`6060a279 python36!PyRun_InteractiveLoopFlags+0xe8
26 000000c2`28befa30 00000000`6055b0b0 python36!PyRun_AnyFileExFlags+0x45
27 000000c2`28befa60 00000000`604f9ea8 python36!Py_hashtable_size+0x5140
*** ERROR: Module load completed but symbols could not be loaded for C:\Python36\python.exe
28 000000c2`28befaa0 00000000`1cf8126d python36!Py_FatalError+0x2cb48
29 000000c2`28befba0 00007fff`fab71fe4 python+0x126d
2a 000000c2`28befbe0 00007fff`fc451eb1 KERNEL32!BaseThreadInitThunk+0x14
2b 000000c2`28befc10 00000000`00000000 ntdll!RtlUserThreadStart+0x21
```

```
0:000> r
rax=0000000000000000 rbx=00000223d1788910 rcx=00000223d1788910
rdx=0000000000000064 rsi=000000c228be61e8 rdi=00000000000003e7
rip=00007fffad026c60 rsp=000000c228be6150 rbp=0000000000000000
 r8=00000223be8a0f00  r9=0000000000008000 r10=00000223d04c0030
r11=0000000000000246 r12=0000000000000001 r13=000000005c000001
r14=000000c228be62e0 r15=0000000000000000
iopl=0         nv up ei pl nz na po nc
cs=0033  ss=002b  ds=002b  es=002b  fs=0053  gs=002b             efl=00010206
nvcuda!cuTexRefSetAddress+0x309622:
00007fff`ad026c60 488b5010        mov     rdx,qword ptr [rax+10h] ds:00000000`00000010=????????????????
0:000> ub
nvcuda!cuTexRefSetAddress+0x30960e:
00007fff`ad026c4c cc              int     3
00007fff`ad026c4d cc              int     3
00007fff`ad026c4e cc              int     3
00007fff`ad026c4f cc              int     3
00007fff`ad026c50 4053            push    rbx
00007fff`ad026c52 4883ec20        sub     rsp,20h
00007fff`ad026c56 488b81d81b0000  mov     rax,qword ptr [rcx+1BD8h]
00007fff`ad026c5d 488bd9          mov     rbx,rcx
0:000> u
nvcuda!cuTexRefSetAddress+0x309622:
00007fff`ad026c60 488b5010        mov     rdx,qword ptr [rax+10h]
00007fff`ad026c64 817a04d0070000  cmp     dword ptr [rdx+4],7D0h
00007fff`ad026c6b 7c11            jl      nvcuda!cuTexRefSetAddress+0x309640 (00007fff`ad026c7e)
00007fff`ad026c6d e89ec5ceff      call    nvcuda!cuTexRefDestroy+0x113 (00007fff`acd13210)
00007fff`ad026c72 84c0            test    al,al
00007fff`ad026c74 7408            je      nvcuda!cuTexRefSetAddress+0x309640 (00007fff`ad026c7e)
00007fff`ad026c76 488bcb          mov     rcx,rbx
00007fff`ad026c79 e837dfceff      call    nvcuda!cuTexRefSetFlags+0xc8 (00007fff`acd14bb5)
0:000> u
nvcuda!cuTexRefSetAddress+0x309640:
00007fff`ad026c7e 33c0            xor     eax,eax
00007fff`ad026c80 4883c420        add     rsp,20h
00007fff`ad026c84 5b              pop     rbx
00007fff`ad026c85 c3              ret
00007fff`ad026c86 cc              int     3
00007fff`ad026c87 cc              int     3
00007fff`ad026c88 cc              int     3
00007fff`ad026c89 cc              int     3
```"
13738,.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13733,Build error with boringssl ,"With the most recent master bazel build fails with `Ubuntu 16.04`, `gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4)`, and `bazel` `0.6.1`.

```sh
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' -MD -MF bazel-out/local-opt/bin/external/curl/_objs/curl/external/curl/lib/curl_multibyte.pic.d -fPIC -iquote external/curl -iquote bazel-out/local-opt/genfiles/external/curl -iquote external/zlib_archive -iquote bazel-out/local-opt/genfiles/external/zlib_archive -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/boringssl -iquote bazel-out/local-opt/genfiles/external/boringssl -isystem external/curl/include -isystem bazel-out/local-opt/genfiles/external/curl/include -isystem external/zlib_archive -isystem bazel-out/local-opt/genfiles/external/zlib_archive -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/boringssl/src/include -isystem bazel-out/local-opt/genfiles/external/boringssl/src/include -Iexternal/curl/lib -D_GNU_SOURCE -DHAVE_CONFIG_H -DCURL_DISABLE_FTP -DCURL_DISABLE_NTLM -DHAVE_LIBZ -DHAVE_ZLIB_H -Wno-string-plus-int '-DCURL_MAX_WRITE_SIZE=65536' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/curl/lib/curl_multibyte.c -o bazel-out/local-opt/bin/external/curl/_objs/curl/external/curl/lib/curl_multibyte.pic.o)
ERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/boringssl/BUILD:128:1: C++ compilation of rule '@boringssl//:ssl' failed (Exit 1)
external/boringssl/src/ssl/t1_lib.cc: In function 'int bssl::ssl_ext_key_share_parse_clienthello(bssl::SSL_HANDSHAKE*, bool*, bssl::Array<unsigned char>*, uint8_t*, CBS*)':
external/boringssl/src/ssl/t1_lib.cc:2189:7: error: 'peer_key.cbs_st::len' may be used uninitialized in this function [-Werror=maybe-uninitialized]
   CBS peer_key;
       ^
external/boringssl/src/ssl/t1_lib.cc:2189:7: error: 'peer_key.cbs_st::data' may be used uninitialized in this function [-Werror=maybe-uninitialized]
cc1plus: all warnings being treated as errors
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 107.966s, Critical Path: 18.12s
FAILED: Build did NOT complete successfully
```

I think the issue is caused by the recent changes of the patch in boringssl with in PR #13638."
13730,freeze  model graph failure,"I can generate the pbtxt,but freeze  model graph failure
I use tensorflow1.2

File ""/Users/gouwei/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph"", line 178, in <module>
    Main()
  File ""/Users/gouwei/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph"", line 117, in Main
    module_space = FindModuleSpace()
  File ""/Users/gouwei/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph"", line 91, in FindModuleSpace
    sys.argv[0])
AssertionError: Cannot find .runfiles directory for /Users/gouwei/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph
Traceback (most recent call last):
  File ""/Users/gouwei/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph"", line 178, in <module>
    Main()
  File ""/Users/gouwei/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph"", line 117, in Main
    module_space = FindModuleSpace()
  File ""/Users/gouwei/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph"", line 91, in FindModuleSpace
    sys.argv[0])
AssertionError: Cannot find .runfiles directory for /Users/gouwei/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph"
13729,android:tensorflow_demo build error - external/fft2d/fft/fftsg.c:641:10: fatal error: 'math.h' file not found,"Hi

I succeeded build and install TensorFlow from sources with the link below.
https://www.tensorflow.org/install/install_sources

But, I could not build 'android: tensorflow_demo' under the guidance of the link below.
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android
> bazel build -c opt //tensorflow/examples/android:tensorflow_demo

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: none
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.11.6
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:
tf.VERSION = 1.4.0-rc0
tf.GIT_VERSION = b'v1.3.0-rc1-3399-g7cdd26f'
tf.COMPILER_VERSION = b'v1.3.0-rc1-3399-g7cdd26f'
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: Build label: 0.6.1-homebrew
- **CUDA/cuDNN version**: none
- **GPU model and memory**: none
- **Exact command to reproduce**: 
bazel build -c opt //tensorflow/examples/android:tensorflow_demo

### Describe the problem
I could not build 'android: tensorflow_demo' under the guidance of the link below.
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android
bazel build -c opt //tensorflow/examples/android:tensorflow_demo

My System OSX system has 'math.h' header in /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.12.sdk/usr/include/math.h folder.

And I can check fftsg.o, fftsg.d file below path.
bazel-out/host/bin/external/fft2d/_objs/fft2d/external/fft2d/fft/

And fftsg.d have math.h path.

vi fftsg.d
bazel-out/host/bin/external/fft2d/_objs/fft2d/external/fft2d/fft/fftsg.o: \
  external/fft2d/fft/fftsg.c \
  /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.12.sdk/usr/include/math.h \
  /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.12.sdk/usr/include/sys/cdefs.h \
  /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.12.sdk/usr/include/sys/_symbol_aliasing.h \
  /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.12.sdk/usr/include/sys/_posix_availability.h \
  /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.12.sdk/usr/include/Availability.h \
  /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.12.sdk/usr/include/AvailabilityInternal.h

By the way, why do errors occur when building tensorflow_demo?

### Source code / logs
ERROR: /private/var/tmp/_bazel_leebongjun/3584e4473c72a5166d05587429923d11/external/fft2d/BUILD.bazel:21:1: C++ compilation of rule '@fft2d//:fft2d' failed (Exit 1).
external/fft2d/fft/fftsg.c:641:10: fatal error: 'math.h' file not found
#include <math.h>
         ^~~~~~~~
1 error generated.
Target //tensorflow/examples/android:tensorflow_demo failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2.271s, Critical Path: 0.39s"
13726,Error while installing Tensorflow on Mac via terminal,"I am trying to install Tensorflow on my mac via the terminal using the tutorial and commands that are provided by the official website linked below.

https://www.tensorflow.org/versions/r0.12/get_started/os_setup#pip_installation

However when running the pip install tensorflow & the pip install tensorflow-gpu command i get the same error (picture is attached).


<img width=""820"" alt=""screen shot 2017-10-14 at 11 24 00 pm"" src=""https://user-images.githubusercontent.com/5986856/31581317-ca2a4126-b136-11e7-81a8-462276e4bff3.png"">
<img width=""1182"" alt=""screen shot 2017-10-14 at 11 23 41 pm"" src=""https://user-images.githubusercontent.com/5986856/31581318-ca348d52-b136-11e7-884b-9fa95fe7a25a.png"">

I am not sure why this error is occurring. Any help that can be provided would be appreciated. 
"
13725, Build tensorflow 1.0.1 for bug on jetson TX2,"Hi,
Build tensorflow 1.0.1 for bug on jetson TX2
$ ./cloneTensorFlow.sh$ 
$./setTensorFlowEV.sh
$ ./buildTensorFlow.sh


ERROR: /home/nvidia/tensorflow/tensorflow/core/kernels/BUILD:685:1: output 'tensorflow/core/kernels/_objs/tile_ops_gpu/tensorflow/core/kernels/tile_ops_gpu.cu.pic.o' was not created.
ERROR: /home/nvidia/tensorflow/tensorflow/core/kernels/BUILD:685:1: not all outputs were created or valid.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 3022.881s, Critical Path: 2684.15s

Thank you for your help"
13723,Feature request: bitwise operations on boolean tensors,"Currently, bitwise operations such as OR, AND only take integers.
It would be nice to be able to feed boolean tensors, e.g. `tf.zeros(4, dtype=tf.bool)` or to be able to bitcast an axis of a boolean tensor to an int.  "
13718,fatal error: third_party/eigen3/Eigen/Core: No such file or directory,"Hi,
I have followed below mentioned steps on my raspberry PI device to enable tensorflow support, but stuck with this issue when executed ""**make -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI OPTFLAGS=""-Os"" CXX=g++-4.8** "" command -

Error description - 

```
 fatal error: third_party/eigen3/Eigen/Core: No such file or directory
 #include ""third_party/eigen3/Eigen/Core""                                         ^
compilation terminated.
tensorflow/contrib/makefile/Makefile:617: recipe for target '/home/pi/tensorflow                                                                                                             /contrib/makefile/gen/host_obj/tensorflow/core/platform/denormal.o' failed
make: *** [/home/pi/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/pla                                                                                                             tform/denormal.o] Error 1
```

Steps executed on raspberry pi; steps a to k are successful, getting error at step l- 
a) tensorflow/contrib/makefile/download_dependencies.sh
b) sudo apt-get install -y autoconf automake libtool gcc-4.8 g++-4.8
c ) cd tensorflow/contrib/makefile/downloads/protobuf/
d) ./autogen.sh
e) ./configure
f) make
g) sudo make install
h) sudo ldconfig  # refresh shared library cache
i) cd ../../../../..
j) export HOST_NSYNC_LIB=`tensorflow/contrib/makefile/compile_nsync.sh`
k) export TARGET_NSYNC_LIB=""$HOST_NSYNC_LIB""
**l) make -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI OPTFLAGS=""-Os"" CXX=g++-4.8**

Not sure, what am i missing? Please provide suggestions to resolve this issue. Thanks!

Thanks
Amit Srivastava
"
13717,`tf.reverse_sequence` does not support bool sequence,It seems that `tf.reverse_sequence` can not reverse bool sequence. 
13716,Tensorflow hangs during run() call on some machines,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 9.1
- **TensorFlow installed from (source or binary)**: Both
- **TensorFlow version (use command below)**:  v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 3.5.3
- **Bazel version (if compiling from source)**: 0.6.1
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
```python
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

x_ph = x = tf.placeholder(tf.float32, [None, 784], name='x')
y = tf.placeholder(tf.float32, [None, 10], name='y')
n_layers = 100
batch_size = 256
for _ in range(n_layers):
    x = tf.layers.dense(x, 500, kernel_initializer=tf.contrib.layers.xavier_initializer())
logits = tf.layers.dense(x, 10)
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y), name=""loss"")
train_op = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
with tf.Session().as_default():
    tf.global_variables_initializer().run()
    for i in range(10000):
        batch = mnist.train.next_batch(batch_size)
        train_op.run({x_ph: batch[0], y: batch[1]})
        print(i)
```

### Describe the problem
When executing the above code (minimal example), the program hangs inside the run() call after several iterations have been successfully executed. The process and some of its threads are still busy while it hangs. Also it cannot be killed with ^C but ```kill``` works. The number of iterations before it hangs varies a lot (50-1000) and it seems to be dependent on the amount of computation being done. E.g. with smaller batch size, it would take longer before it freezes, and with a vgg-like neural net it would freeze after ~20 iterations. If I attach ```gdb``` to the frozen process, and run ```cont```, the process starts running before it hangs again after several iterations.

This must be system specific since the above example of course runs without any problems on most machines. I have tried it with tensorflow installed both from source and from binary, and with both python 3.5 and 3.6 but the issue is always there. Probably the same issue has been discussed in #2788 and #7573 but it wasn't resolved there.

```uname -r```: 4.9.0-3-amd64
CPU: 4 x Xeon(R) CPU E5-4627 v2


### Source code / logs
This is output from ```gdb``` when the process hangs.
```
(gdb) thread apply all bt

Thread 65 (Thread 0x2aaacf2b3700 (LWP 15633)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab2855273 in Eigen::Barrier::Wait() ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab34597eb in void Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>::evalProduct<true, false, false, 0>(float*) const () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab3464ddd in tensorflow::LaunchMatMulBase<Eigen::ThreadPoolDevice, float>::launch(tensorflow::OpKernelContext*, tensorflow::OpKernel*, tensorflow::Tensor const&, tensorflow::Tensor const&, Eigen::array<Eigen::IndexPair<long>, 1ul> const&, tensorflow::Tensor*) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab3465211 in tensorflow::MatMulOp<Eigen::ThreadPoolDevice, float, false>::Compute(tensorflow::OpKernelContext*) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00002aaab3fbefdc in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00002aaab3f8f3e8 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00002aaab3f7ee10 in std::_Function_handler<void (), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> (tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> >::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00002aaab4308742 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#12 0x00002aaaaacd8494 in start_thread (arg=0x2aaacf2b3700) at pthread_create.c:333
#13 0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 64 (Thread 0x2aaacf0b2700 (LWP 15632)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaacf0b2700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 63 (Thread 0x2aaaceeb1700 (LWP 15631)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaaceeb1700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 62 (Thread 0x2aaacecb0700 (LWP 15630)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaacecb0700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 61 (Thread 0x2aaaceaaf700 (LWP 15629)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaaceaaf700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 60 (Thread 0x2aaace8ae700 (LWP 15628)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaace8ae700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 59 (Thread 0x2aaace6ad700 (LWP 15627)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaace6ad700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 58 (Thread 0x2aaace4ac700 (LWP 15626)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaace4ac700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 57 (Thread 0x2aaace2ab700 (LWP 15625)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaace2ab700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 56 (Thread 0x2aaace0aa700 (LWP 15624)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaace0aa700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 55 (Thread 0x2aaacdea9700 (LWP 15623)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaacdea9700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 54 (Thread 0x2aaacdca8700 (LWP 15622)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaacdca8700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 53 (Thread 0x2aaacdaa7700 (LWP 15621)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaacdaa7700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 52 (Thread 0x2aaacd8a6700 (LWP 15620)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaacd8a6700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 51 (Thread 0x2aaacd6a5700 (LWP 15619)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaacd6a5700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 50 (Thread 0x2aaacd4a4700 (LWP 15618)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaacd4a4700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 49 (Thread 0x2aaacd2a3700 (LWP 15617)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaacd2a3700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 48 (Thread 0x2aaacb0de700 (LWP 15616)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaacb0de700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 47 (Thread 0x2aaacaedd700 (LWP 15615)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaacaedd700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 46 (Thread 0x2aaacacdc700 (LWP 15614)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaacacdc700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 45 (Thread 0x2aaacaadb700 (LWP 15613)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaacaadb700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 44 (Thread 0x2aaaca8da700 (LWP 15612)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaaca8da700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 43 (Thread 0x2aaaca6d9700 (LWP 15611)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaaca6d9700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 42 (Thread 0x2aaaca4d8700 (LWP 15610)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaaca4d8700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 41 (Thread 0x2aaaca2d7700 (LWP 15609)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaaca2d7700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 40 (Thread 0x2aaaca0d6700 (LWP 15608)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaaca0d6700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 39 (Thread 0x2aaac9ed5700 (LWP 15607)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaac9ed5700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 38 (Thread 0x2aaac9cd4700 (LWP 15606)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaac9cd4700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 37 (Thread 0x2aaac9ad3700 (LWP 15605)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaac9ad3700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 36 (Thread 0x2aaac98d2700 (LWP 15604)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaac98d2700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 35 (Thread 0x2aaac96d1700 (LWP 15603)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaac96d1700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 34 (Thread 0x2aaac8576700 (LWP 15602)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaac8576700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 33 (Thread 0x2aaac8375700 (LWP 15601)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac8375700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 32 (Thread 0x2aaac8174700 (LWP 15600)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac8174700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 31 (Thread 0x2aaac7f73700 (LWP 15599)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac7f73700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 30 (Thread 0x2aaac7d72700 (LWP 15598)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac7d72700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 29 (Thread 0x2aaac7b71700 (LWP 15597)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaac7b71700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 28 (Thread 0x2aaac7970700 (LWP 15596)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac7970700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 27 (Thread 0x2aaac776f700 (LWP 15595)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaac776f700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 26 (Thread 0x2aaac756e700 (LWP 15594)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaac756e700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 25 (Thread 0x2aaac736d700 (LWP 15593)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac736d700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 24 (Thread 0x2aaac716c700 (LWP 15592)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac716c700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 23 (Thread 0x2aaac6a10700 (LWP 15591)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac6a10700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 22 (Thread 0x2aaac680f700 (LWP 15590)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac680f700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 21 (Thread 0x2aaac660e700 (LWP 15589)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac660e700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 20 (Thread 0x2aaac640d700 (LWP 15588)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac640d700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 19 (Thread 0x2aaac620c700 (LWP 15587)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac620c700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 18 (Thread 0x2aaac600b700 (LWP 15586)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac600b700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 17 (Thread 0x2aaac5e0a700 (LWP 15585)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac5e0a700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 16 (Thread 0x2aaac5c09700 (LWP 15584)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaac5c09700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 15 (Thread 0x2aaac5a08700 (LWP 15583)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac5a08700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 14 (Thread 0x2aaac5807700 (LWP 15582)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaac5807700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 13 (Thread 0x2aaac5606700 (LWP 15581)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac5606700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 12 (Thread 0x2aaac5405700 (LWP 15580)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac5405700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 11 (Thread 0x2aaac5204700 (LWP 15579)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaac5204700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 10 (Thread 0x2aaac5003700 (LWP 15578)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac5003700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 9 (Thread 0x2aaac4e02700 (LWP 15577)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac4e02700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 8 (Thread 0x2aaac4c01700 (LWP 15576)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac4c01700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 7 (Thread 0x2aaac4a00700 (LWP 15575)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaac4a00700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 6 (Thread 0x2aaac47ff700 (LWP 15574)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaac47ff700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 5 (Thread 0x2aaac45fe700 (LWP 15573)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac45fe700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 4 (Thread 0x2aaac43fd700 (LWP 15572)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4308047 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6  0x00002aaaaacd8494 in start_thread (arg=0x2aaac43fd700) at pthread_create.c:333
#7  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 3 (Thread 0x2aaac41fc700 (LWP 15571)):
#0  pthread_cond_signal@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_signal.S:87
#1  0x00002aaab5427529 in std::condition_variable::notify_one() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab4307dbb in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::Schedule(std::function<void ()>) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab4309681 in tensorflow::thread::ThreadPool::Schedule(std::function<void ()>) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab3f9f8d6 in tensorflow::EigenThreadPoolWrapper::Schedule(std::function<void ()>) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab3458371 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 4, true, false, 0, Eigen::MakePointer>, 8, 4, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 4, false, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 8, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 4, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 4, false, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >::enqueue_packing_helper(long, long, long, bool) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00002aaab4308742 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#9  0x00002aaaaacd8494 in start_thread (arg=0x2aaac41fc700) at pthread_create.c:333
#10 0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 2 (Thread 0x2aaac3ffb700 (LWP 15570)):
#0  0x00002aaaabb10ca7 in sched_yield () at ../sysdeps/unix/syscall-template.S:84
#1  0x00002aaab4308090 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00002aaab4308abe in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab43079d2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab542d200 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5  0x00002aaaaacd8494 in start_thread (arg=0x2aaac3ffb700) at pthread_create.c:333
#6  0x00002aaaabb27aff in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97

Thread 1 (Thread 0x2aaaaab10240 (LWP 15558)):
#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x00002aaab542750c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00002aaab3f46acb in tensorflow::DirectSession::WaitForNotification(tensorflow::Notification*, long long) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00002aaab3f46b5d in tensorflow::DirectSession::WaitForNotification(tensorflow::DirectSession::RunState*, tensorflow::CancellationManager*, long long) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00002aaab3f52e1d in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00002aaab25b4c97 in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) () from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00002aaab25b4fc4 in TF_Run ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00002aaab241dcd2 in tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00002aaab241e0d1 in tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00002aaab23e4e83 in _wrap_TF_Run ()
   from /nfs/scistore12/clustersw/debian/stretch/tensorflow-1.3.0-python3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00005555557146df in PyCFunction_Call ()
#11 0x00005555556d46e9 in PyEval_EvalFrameEx ()
#12 0x00005555556da817 in PyEval_EvalCodeEx ()
#13 0x00005555557165d3 in ?? ()
#14 0x000055555575d647 in PyObject_Call ()
#15 0x00005555556d2180 in PyEval_EvalFrameEx ()
#16 0x00005555556d9286 in ?? ()
#17 0x00005555556d54c9 in PyEval_EvalFrameEx ()
#18 0x00005555556d97bf in ?? ()
#19 0x00005555556d54c9 in PyEval_EvalFrameEx ()
#20 0x00005555556d493f in PyEval_EvalFrameEx ()
#21 0x00005555556d9286 in ?? ()
#22 0x00005555556d50a9 in PyEval_EvalFrameEx ()
#23 0x00005555556d9286 in ?? ()
#24 0x00005555556d50a9 in PyEval_EvalFrameEx ()
#25 0x00005555556d9286 in ?? ()
#26 0x00005555556d50a9 in PyEval_EvalFrameEx ()
#27 0x00005555556d9286 in ?? ()
#28 0x00005555556d9f9f in PyEval_EvalCode ()
#29 0x00005555557a78f2 in ?? ()
#30 0x00005555557a9e1d in PyRun_FileExFlags ()
#31 0x00005555557aa5be in PyRun_SimpleFileExFlags ()
#32 0x00005555557d84d7 in Py_Main ()
#33 0x0000555555668c01 in main ()

```"
13715,Failed to load the native TensorFlow runtime.,"my python version is 3.5.2
cuda version is  8.0
and cudnn version is 6.14

but when I import tensorflow , I have get the follow error message, how I can solve it!

```
C:\Users\SXY004>python3
Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\SXY004\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\SXY004\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 35, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\SXY004\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 30, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\SXY004\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\SXY004\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\SXY004\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\SXY004\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 51, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\SXY004\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\SXY004\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\SXY004\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 35, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\SXY004\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 30, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\SXY004\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\SXY004\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```
"
13713,Unable to change reuse from True to False when using a variable scope to create new one.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.04.
- **TensorFlow installed from (source or binary)**:
From source.
- **TensorFlow version (use command below)**:
('v1.3.0-rc1-633-gcf375f0', '1.2.1-rc1').
- **Python version**: 
2.7.6.
- **Bazel version (if compiling from source)**:
0.5.3.
- **CUDA/cuDNN version**:
CUDA 8.0.
- **GPU model and memory**:
N/A.
- **Exact command to reproduce**:
    ```python
    with tf.variable_scope(""test_scope"", reuse=True) as scope:
        pass
    with tf.variable_scope(scope, reuse=False) as scope2:
        print scope2.reuse
    ```

### Describe the problem
When users using a variable scope to create another variable scope, the `reuse` can not be changed from `True` to `False`. But it's OK if users change `reuse` from `False` to `True`. You can use the code above to reproduce the issue.

### Source code / logs
I debugged the problem and found the root cause is at https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/ops/variable_scope.py#L1322.
```python
default_varscope[0] = VariableScope(
    name_or_scope.reuse if reuse is None else reuse, # the line of code causing the bug
    name=new_name,
    initializer=name_or_scope.initializer,
    regularizer=name_or_scope.regularizer,
    caching_device=name_or_scope.caching_device,
    partitioner=name_or_scope.partitioner,
    dtype=name_or_scope.dtype,
    custom_getter=name_or_scope.custom_getter,
    name_scope=name_scope,
    use_resource=name_or_scope.use_resource)
```
The line of code means that if `reuse` is `None`, it will use `name_or_scope.reuse`, preventing users from changing `reuse` to `None or False`. Note that  `None` is the same as `False` for `reuse` and tensorflow replaces `False` with `None` beforehand at https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/ops/variable_scope.py#L1540.

I think it should be changed to `reuse`.

Thank you.
"
13711,--config=mkl leads to libmklml_intel.so: cannot open shared object file: No such file or directory,"I compiled passing --config=mkl to bazel, it compiles fine and i get the .whl file, i install it with pip correctly, but when i launch a python session and type : import tensorflow as tf i get:
ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory

Any ideas?
"
13710,Wrong syntax in bibtex (DOCS),"### System information

Irrelevant

### Describe the problem

The [bibtex entry for citing the Tensorflow white paper](https://www.tensorflow.org/about/bib#in_bibtex_format) has incorrect syntax: `\'{\i}` should be `\'{i}` in the first author name.

### Source code / logs

See also: https://tex.stackexchange.com/questions/23817/biber-writes-incorrect-unicode-encoding-of-i

"
13707,cd /,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13706,/tensorflow/core/lib/io/record_reader.h:83:19: error: use of undeclared identifier  'InputStreamInterface'   std::unique_ptr<InputStreamInterface> input_stream_;,"When try to build iOS dependencies got this error. I simply run:
```
./tensorflow/contrib/makefile/build_all_ios_ssd.sh
```
And here is my env:

* macOS 10.13
* XCode 9

Any help would be every appreciated!
More detail errors:

```
In file included from tensorflow/core/lib/db/sqlite.cc:17:
./tensorflow/core/lib/io/record_reader.h:83:19: error: use of undeclared identifier
      'InputStreamInterface'
  std::unique_ptr<InputStreamInterface> input_stream_;
                  ^
1 error generated.
make: *** [/Volumes/xs/CodeSpace/ng/ai/frameworks/tensorflow/tensorflow/contrib/makefile/gen/obj/ios_ARM64/tensorflow/core/lib/db/sqlite.o] Error 1
make: *** Waiting for unfinished jobs....
+ '[' 2 -ne 0 ']'
+ echo 'arm64 compilation failed.'
arm64 compilation failed.
+ exit 1
```"
13705,While compiling external app -> fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: It's a C++ code from [this tutorial](https://tebesu.github.io/posts/Training-a-TensorFlow-graph-in-C++-API)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: Release 1.4.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.6.1
- **CUDA/cuDNN version**: no CUDA
- **GPU model and memory**: no GPU
- **Exact command to reproduce**: `g++ -I /opt/tensorflow -I /opt/tensorflow/bazel-genfiles loader.cpp`

### Describe the problem
I have a problem while trying to use tensorflow in external app. I took the code from the tutorial above, built tensorflow with following command: `bazel build //tensorflow:libtensorflow_cc.so`. Now, I want to build my external application with tensorflow. While compiling with given command, I receive an error. I also tried compiling with `cmake` and proper `include_directories` directive, but to no avail.

### Source code / logs

The problematic line of code is:
`#include ""tensorflow/core/public/session.h""`<br>
Compiling with command: `g++ -I /opt/tensorflow -I /opt/tensorflow/bazel-genfiles loader.cpp` generates error:

```
In file included from /opt/tensorflow/tensorflow/core/framework/tensor.h:19:0,
                 from /opt/tensorflow/tensorflow/core/public/session.h:24,
                 from loader.cpp:1:
/opt/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory
compilation terminated.
```
Whole code snippet can be seen in the tutorial link above

### Remarks
A similar issue is #4680 but:
1. It is closed without specific information, if it's resolved or not.
2. There is a comment, which states, that if similar issue happens in future, it should be opened as new issue
3. The use case there was not precisely using external app on Ubuntu, but on RaspberryPi instead.
Thus, I'm submitting new issue for this case."
13702,NET::ERR_CERT_COMMON_NAME_INVALID visiting https://download.tensorflow.org,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Chrome, mac os latest

- **TensorFlow installed from (source or binary)**:
N/A

- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I'm following a tensorflow tutorial: https://www.tensorflow.org/versions/master/tutorials/audio_recognition

The tutorial references 'Speech Commands dataset', which points to https://download.tensorflow.org/data/speech_commands_v0.01.tar.gz

I follow that link in chrome and get:

""Your connection is not private

Attackers might be trying to steal your information from download.tensorflow.org (for example, passwords, messages, or credit cards). Learn more
NET::ERR_CERT_COMMON_NAME_INVALID
Subject: *.storage.googleapis.com
Issuer: Google Internet Authority G2
Expires on: Dec 26, 2017
Current date: Oct 13, 2017""

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


"
13696,Problem with CuDNN RNNs on Windows,"OS: Windows10
Keras version: master (as of today: 2.08+)
Tensorflow backend version: master (as of today ~1.4rc0)

GPU: Geforce GTX 1080Ti (11GB)
Cuda version: v8.0
cuDNN version: cudnn-8.0-windows10-x64-v6.0

First opened as a Keras GitHub issue here (includes full code and error output):
https://github.com/fchollet/keras/issues/8135

Francois Chollet suggested that this may be a Windows TF problem.

Hi, I tried keras.layers.CuDNNLSTM after seeing fchollet's tweet the other day. I have the latest Keras and Tensorflow, but there is a tensorflow problem with the Op 'CudnnRNN' not being registered.

Have I missed something?

Thanks



"
13694,Module not found: tensorflow  Running Docker Jupyter OSX,"

 After installing docker, I attempted to run tensorflow in Jupyter. My run command was
`LewIss-MacBook-Pro:MyTensorFlow` lewleib$  docker run -it -p 8888:8888 -p 6006:6006 -v ~/Users/lewleib/MyTensorFlow:/notebooks tensorflow/tensorflow`

In the Jupyter notebook I ran  
 ```
import tensorflow as tf     
hello = tf.constant('Hello, TensorFlow!')     
sess = tf.Session()     
print(sess.run(hello) 
```

This resulted in
`ModuleNotFoundError: No module named 'tensorflow'`

I have tried starting in different directories and added prefix gcr.io with the same results. Thanks
-- | --



"
13693,tf.image.resize_bicubic is 2x slower than opencv.resize(interpolation=INTER_CUBIC),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: --
- **CUDA/cuDNN version**: 8/6
- **GPU model and memory**: GCP Tesla P100 16Gb
- **Exact command to reproduce**: https://gist.github.com/ftokarev/43adbf1afacaa4bd612c55354c17b29b


tf.image.resize_bicubic is 2x slower than opencv.resize when used on input data with lots of channels.

40 channels:
 - opencv: 152.23ms
 - tf: 297.71ms

With three channels they perform about the same:
 - opencv: 11.02ms
 - tf: 11.76ms
"
13692,TensorFlow 1.3.0 GPU for Windows doesn't work for 2 NVIDIA P4s,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 (factory production machine)
- **TensorFlow installed from (source or binary)**:
https://pypi.python.org/pypi/tensorflow-gpu
tensorflow_gpu-1.3.0-cp35-cp35m-win_amd64.whl

- **TensorFlow version (use command below)**: 1.3.0
- **Python version**:  3.5
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: Cuda release 8.0, V8.0.60.  cuDNN 6.
- **GPU model and memory**:  2 NVIDIA Tesla P4s (same as specs)
- **Exact command to reproduce**: https://github.com/tkuanlun350/Tensorflow-SegNet

### Describe the problem
We're currently evaluating Tensorflow as a possible replacement for Caffe but we seem to have run into a problem with running multiple P4s on a production system.
 
Our production systems are offline so I had to install a Python wheel of the GPU version TF framework which works fine until I try to use 2 GPUs for training where I get the error message:
 
`2017-10-05 13:53:42.989423: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:847] Peer access not supported between device ordinals 1 and 0`

P4s are installed and working correctly.  I don't understand why peer-to-peer GPU access is required for using GPUs.  We can run multiple GPUs on our Caffe implementation.

```
nvidia-smi
Tue Oct 10 10:53:57 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 385.54                 Driver Version: 385.54                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P4            TCC  | 00000000:04:00.0 Off |                    0 |
| N/A   31C    P8     6W /  75W |      0MiB /  7590MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P4            TCC  | 00000000:81:00.0 Off |                    0 |
| N/A   30C    P8     6W /  75W |      0MiB /  7590MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

```
  "
13691,Compatibility with Bazel 0.8,"Starting from Bazel 0.8 support for `set` constructors will be removed completely. Currently one of the dependencies, grpc, is not compatible with this: tensorflow uses an old version of it. grpc has already been fixed, please update your dependencies to use grpc with the following commit included: https://github.com/grpc/grpc/pull/12571/commits/759c7029b18f8bbcf6a6399d512abed8b6e12436

grpc is loaded as a `patched_http_archive`, you can also update the file `third_party/grpc/grpc.patch` to include the one-line patch from the commit above if it's more convenient.

Just updating grpc to head or to to the commit that has the fix is not trivial because `third_party/grpc/grpc.patch` then has to be updated as well. and as I don't know why those changes were necessary I don't know if I should keep, update or delete them when I update grpc."
13687,scatter_nd doesn't check indices for out-of-bounds on GPU when using float32,"I noticed that the `scatter_nd` command behaves inconsistently with respect to checking the given indices. In most cases the command raises an error when trying to set an index that is out of bound for the given shape. However, on the GPU when `updates` is of dtype `tf.float32` this check is not performed and invalid indices are just ignored. In the case of tf.int32 or always when on the CPU, and exception is raised. See below for detailed information and a minimal example that has been adapted from the `scatter_nd` documentation.

Additionally I noticed that the `scatter_nd` documentation doesn't state whether a bound check is performed or not. That might be worth adding.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
  yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
installed from binary via pip (GPU version)
- **TensorFlow version (use command below)**:
v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 
3.5.2
- **CUDA/cuDNN version**:
cuda8.0/cuDNN6 (via docker container `nvidia/cuda:8.0-cudnn6-devel`)
- **GPU model and memory**:
GeForce GTX 1080 (8112 MB)
- **Exact command to reproduce**:
```python

# doesn't fail with invalid index on GPU (but it should)
with tf.Graph().as_default() as g:
    indices = tf.constant([[0, 4], [0, 3], [0, 1], [2, 7]])
    updates = tf.constant([9, 10, 11, 12], dtype=tf.float32)
    scatter = tf.scatter_nd(indices, updates, (1, 10))
    with tf.Session(graph=g) as session:
        print(session.run(scatter))

# fails with invalid index error
with tf.Graph().as_default() as g:
    indices = tf.constant([[0, 4], [0, 3], [0, 1], [2, 7]])
    updates = tf.constant([9, 10, 11, 12], dtype=tf.int32)
    scatter = tf.scatter_nd(indices, updates, (1, 10))
    with tf.Session(graph=g) as session:
        print(session.run(scatter))
```"
13686,Documentation truncated_normal does not match implementation,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Nope
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7.3
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: Python 3.6.1
- **Bazel version (if compiling from source)**: N.A.
- **CUDA/cuDNN version**: 8.0 / 6.0
- **GPU model and memory**: Nvidia Tesla 12 GB
- **Exact command to reproduce**: ?

### Describe the problem
The documentation of `tf.truncated_normal`[https://www.tensorflow.org/api_docs/python/tf/truncated_normal] contains the following line for its parameter `stddev`: 

> The standard deviation of the truncated normal distribution.

However, the example below shows that the truncated normal does not have the given standard deviation. This means that the documentation would be correct if it said

> The standard deviation of the normal distribution, before truncation.

or if the standard deviation of the samples would be indeed the given standard deviation.

### Source code / logs

    import numpy as np
    import tensorflow as tf

    # standard deviation not 1
    with tf.Session():
        print(np.std(tf.truncated_normal([10000, ], stddev=1).eval()))

    # compared to scipy
    from scipy.stats import truncnorm
    print(truncnorm(-2, 2, loc=0, scale=1).std())
"
13685,Linux compile v1.1.0 failed,"> ERROR: /home/fesun/.cache/bazel/_bazel_fesun/c44abb322eef8ca1d3dd1c34fcda8c3a/external/io_bazel_rules_closure/closure/private/defs.bzl:27:16: The `set` constructor for depsets is deprecated and will be removed. Please use the `depset` constructor instead. You can temporarily enable the deprecated `set` constructor by passing the flag --incompatible_disallow_set_constructor=false
> ERROR: error loading package '': Extension file 'closure/private/defs.bzl' has errors
> ERROR: error loading package '': Extension file 'closure/private/defs.bzl' has errors

I added the flag --incompatible_disallow_set_constructor=false for all bazel command call, then:

> DEBUG: /home/fesun/.cache/bazel/_bazel_fesun/c44abb322eef8ca1d3dd1c34fcda8c3a/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:197:9: @//tensorflow/tools/lib_package:libtensorflow_jni: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
> DEBUG: /home/fesun/.cache/bazel/_bazel_fesun/c44abb322eef8ca1d3dd1c34fcda8c3a/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:197:9: @//tensorflow/tools/lib_package:cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
> DEBUG: /home/fesun/.cache/bazel/_bazel_fesun/c44abb322eef8ca1d3dd1c34fcda8c3a/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:197:9: @//tensorflow/tools/lib_package:clib: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
> DEBUG: /home/fesun/.cache/bazel/_bazel_fesun/c44abb322eef8ca1d3dd1c34fcda8c3a/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:197:9: @//tensorflow/tools/lib_package:clicenses: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
> ERROR: /home/fesun/tensorflow/tensorflow/core/kernels/BUILD:59:14: Traceback (most recent call last):
>         File ""/home/fesun/tensorflow/tensorflow/core/kernels/BUILD"", line 54
>                 config_setting(name = ""xsmm_backward"", values = {...""})
>         File ""/home/fesun/tensorflow/tensorflow/core/kernels/BUILD"", line 59, in config_setting
>                 {""define"": ""tensorflow_xsmm=1"", ""define"": ""tensorflow_xsmm_backward=1""}
> Duplicated key ""define"" when creating dictionary
> ERROR: package contains errors: tensorflow/core/kernels
> ERROR: error loading package 'tensorflow/core/kernels': Package 'tensorflow/core/kernels' contains errors
> 

Any solution for this?
"
13683,How to feed SparseTensor in C++ API?,"I trained model via python. After training done, I want to load the model and its checkpoint for prediction via C++. But I can't find any solutions about how to feed SparseTensor or Example in C++ API.

python train code:
```python
with tf.name_scope('input'):
    filename_queue = tf.train.string_input_producer(
    tf.train.match_filenames_once(file_name), num_epochs=max_epoch)
    serialized_example = self.Decode(filename_queue)
    capacity = thread_num * batch_size + min_after_dequeue
    batch_serialized_example = tf.train.shuffle_batch(
                                    [serialized_example],
                                    batch_size=batch_size,
                                    num_threads=thread_num,
                                    capacity=capacity,
                                    min_after_dequeue=min_after_dequeue)
    features = {}
    features['label'] = tf.FixedLenFeature([], tf.float32)
    features['continuous_feature'] = tf.FixedLenFeature([], tf.float32)
    features['sparse_id'] = tf.VarLenFeature(tf.int64)
    features['sparse_val'] = tf.VarLenFeature(tf.float32)
    instance = tf.parse_example(batch_serialized_example, features)

    label_tensor = instance['label']
    continuous_feature_tensor = instance['continuous_val']
    sparse_id_tensor = instance['sparse_id']
    sparse_val_tensor = instance['sparse_val']

with tf.variable_scope(""emb"") as scope:
    embedding_variable = tf.Variable(tf.truncated_normal([1000000, 50], stddev=0.05), name='emb_matrix')
    embedding = tf.nn.embedding_lookup_sparse(embedding_variable, sparse_id_tensor, sparse_val_tensor, ""mod"", combiner=""sum"")
net = tf.concat([embedding, continuous_feature_tensor], 1, name='concat')
dim = net.get_shape().as_list()[1]
weight = tf.Variable(tf.truncated_normal([dim, 100], stddev=0.05), name='fully_weight')
bias = tf.Variable(tf.truncated_normal([100], stddev=0.05), name='fully_bias')
net = tf.matmul(net, weight) + bias
cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=net, labels=label_tensor, name='cross_entropy')
```

In C++, all solutions I have found is to feed `vector<pair<string, Tensor>>` into `session.Run()`. However, in my model, I don't know which function in C++ I should use. 

Thanks.

@martinwicke "
13682,can any one reproduce this problew with tf.gather?,"x = tf.constant([[0.22, 0.3,0.1,0.11],[0.4,0.5, 0.6,0.99],[0.8, 0.9,0.43,0.21]])
indices = tf.constant([[1,2],[0,1],[2,3]])
b=tf.gather_nd(x, indices）
sess = tf.InteractiveSession()
cc=sess.run([b], feed_dict={})
print cc

with tensorflow 1.0.1 it gets [array([ 0., 0., 0.], dtype=float32)]
is it a bug in 1.0.1, but with higher version,it gets right"
13681,type error in tensorflow document API r1.3 ( tf.truncatediv ),"Cause this issue is not about tensorflow itself, please, excuse my ignoring some conventions. Plus, as you know, the postage can cost more than the goods once I try to contribute to documentation via committing so as to correct a tiny error. Please, let me use this channel to comment.

""""""problematic document
Truncation designates that negative numbers will round fractional quantities toward zero. **I.e. -7 / 5 = 1**. This matches C semantics but it is different than Python semantics. See FloorDiv for a division function that matches Python Semantics.
"""""" 
**I.e. -7 / 5 = 1** should be **I.e -7 / 5 = -1**



"
13680,Feature request: Profiling with multiple workers in distributed settings and visualizing them individually,"Hi,
I am running Tensorflow in distributed settings only using CPUs with multiple workers and parameter servers. I would like to find a way to generate timeline information of individual workers/parameter servers and visualize them together to check on potential scaling issues. 
Currently, I can generate timeline traces containing the profiles of all the parameter servers and a single worker.  But, visualizing all the workers in the same timeline trace would be beneficial. 

Thank you."
13675,Import Error on Windows 10: No module named '_pywrap_tensorflow_internal',"**System information**
OS: Windows 10 64
Tensorflow version 1.3 GPU version
Python 3.5.4
CUDA 8.0 -> cudart64_80.dll
CUDNN 6.0 -> cudnn64_6.dll
GPU: GeForce GTX 1070

**Installation Process & Issue:**
I followed the guide provided on https://www.tensorflow.org/install/install_windows for the installation via Anaconda. The installation seems to be successful, but upon running the following command in the console:

>>> import tensorflow as tf

I am getting the following error:

#BEGIN ERROR MESSAGE

Traceback (most recent call last):
  File ""C:\Users\Eric\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Eric\AppData\Local\conda\conda\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 985, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 968, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 957, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 938, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Eric\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Eric\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Eric\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 23, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Eric\AppData\Local\conda\conda\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Eric\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Eric\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Eric\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Eric\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Eric\AppData\Local\conda\conda\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 985, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 968, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 957, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 938, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Eric\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Eric\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Eric\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 23, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Eric\AppData\Local\conda\conda\envs\tensorflow\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

#END ERROR MESSAGE

**Attempted Fixes:**
Ensuring that cudart64_80.dll and cudnn64_6.dll locations are appended into %PATH%.
Appending .DLL to %PATHEXT%.
Ensuring that the following dependencies are available and their location in %PATH%. This list is sourced from user mrry's reply in https://github.com/tensorflow/tensorflow/issues/5949.
KERNEL32.dll
WSOCK32.dll
WS2_32.dll
SHLWAPI.dll
python35.dll
MSVCP140.dll
VCRUNTIME140.dll
api-ms-win-crt-runtime-l1-1-0.dll
api-ms-win-crt-heap-l1-1-0.dll
api-ms-win-crt-utility-l1-1-0.dll
api-ms-win-crt-stdio-l1-1-0.dll
api-ms-win-crt-string-l1-1-0.dll
api-ms-win-crt-math-l1-1-0.dll
api-ms-win-crt-convert-l1-1-0.dll
api-ms-win-crt-environment-l1-1-0.dll
api-ms-win-crt-filesystem-l1-1-0.dll
api-ms-win-crt-time-l1-1-0.dll

Any help would be greatly appreciated!
"
13669,monotonic attention is buggy,"### System information
- **Have I written custom code **: Yes
- **OS Platform and Distribution**: Manjaro Linux, kernel 4.13.5
- **TensorFlow installed from **: binary
- **TensorFlow version (use command below)**: 1.3, 1.4 nightly (11 oct)
- **Python version**:  3.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: cuda8, cudnn 7 & 6
- **GPU model and memory**: gtx 1080

### the problem

The two monotonic attention mechanisms, LuongMonotonicAttention and BahdanauMonotonicAttention, do not seem to work as expected on my task.

In the case of LuongMonotonicAttention, the alignment that I obtain looks like a horizontal line drawn on the first row of the image. However, the alignment has a diagonal shape using BahdanauAttention or LuongAttention, and I am instantiating these classes with the same parameters.

In the case of BahdanauMonotonicAttention, I simply receive an error message:
https://pastebin.com/raw/mPPWnEH5

Is there any preprocessing I should do in addition to the non-monotonic case ?"
13668,"ValueError: Cannot feed value of shape (128,) for Tensor 'Placeholder_142:0', which has shape '(?, 3433)","Hello... after several unsuccesfull tries, I would like to ask your help in solving this error.
I am trying to train a deep autoencoder network, by using a local csv file that is then transformed (by the csv and the numpy libraries) into a numpy array. But this data is never feeding into my placeholder's tensor.

Here's an abstract of the deep autoencoder:

`
class Deep_Autoencoder:
    
    def __init__(self, input_dim, n_nodes_hl = (32, 16, 1), 
                 epochs = 400, batch_size = 128, learning_rate = 0.02, n_examples = 10):
        
        # Hyperparameters
        self.input_dim = input_dim
        self.epochs = epochs
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.n_examples = n_examples
        
        # Input and target placeholders
        X = tf.placeholder('float', [None, self.input_dim])
        Y = tf.placeholder('float', [None, self.input_dim])
		...
		
		self.X = X
        print(""self.X : "", self.X)
        self.Y = Y
        print(""self.Y : "", self.Y)
		...
	        
    def train_neural_network(self, data, targets):
        
        with tf.Session() as sess:
            sess.run(tf.global_variables_initializer())
            for epoch in range(self.epochs):
                epoch_loss = 0
                i = 0

                # Let's train it in batch-mode
                while i < len(data):
                    start = i
                    end = i + self.batch_size
                    
                    batch_x = np.array(data[start:end])
                    print(""type batch_x :"", type(batch_x))
                    print(""len batch_x :"", len(batch_x))
                    batch_y = np.array(targets[start:end])
                    print(""type batch_y :"", type(batch_y))
                    print(""len batch_y :"", len(batch_y))
                    
                    hidden, _, c = sess.run([self.encoded, self.optimizer, self.cost], 
                                            feed_dict={self.X: batch_x, self.Y: batch_y})
                    epoch_loss +=c
                    i += self.batch_size

            self.saver.save(sess, 'selfautoencoder.ckpt')
            print('Accuracy', self.accuracy.eval({self.X: data, self.Y: targets}))`

Here I create the input data and below you can see that I'll printout their main features for your info (note that I am actually interested on column 3 only):

`	features_DeepAE = create_feature_sets(filename)

	Train_x = np.array(features_DeepAE[0])
	Train_y = np.array(features_DeepAE[1])

	print(""type Train_x : "", type(Train_x))
	print(""type Train_x.T[3] : "", type(Train_x.T[3]))
	print(""len Train_x : "", len(Train_x))
	print(""len Train_x.T[3] : "", len(Train_x.T[3]))
	print(""shape Train_x : "", Train_x.shape)
	print(""type Train_y : "", type(Train_y))
	print(""type Train_y.T[3] : "", type(Train_y.T[3]))
	print(""len Train_y : "", len(Train_y))
	print(""len Train_y.T[3] : "", len(Train_y.T[3]))
	print(""shape Train_y : "", Train_y.shape)`

And here I run the code:

`	DAE = Deep_Autoencoder(input_dim = len(Train_x))
	DAE.train_neural_network(Train_x.T[3], Train_y.T[3])
`

------

These are the printouts, fyi:

	type Train_x :  <class 'numpy.ndarray'>
	type Train_x.T[3] :  <class 'numpy.ndarray'>
	len Train_x :  3433
	len Train_x.T[3] :  3433
	shape Train_x :  (3433, 5)
	type Train_y :  <class 'numpy.ndarray'>
	type Train_y.T[3] :  <class 'numpy.ndarray'>
	len Train_y :  3433
	len Train_y.T[3] :  3433
	shape Train_y :  (3433, 5)
	self.X :  Tensor(""Placeholder_142:0"", shape=(?, 3433), dtype=float32)
	self.Y :  Tensor(""Placeholder_143:0"", shape=(?, 3433), dtype=float32)
	type batch_x : <class 'numpy.ndarray'>
	len batch_x : 128
	type batch_y : <class 'numpy.ndarray'>
	len batch_y : 128




------
And finally the error:
**ValueError: Cannot feed value of shape (128,) for Tensor 'Placeholder_142:0', which has shape '(?, 3433)'**

and yes... I'm at placeholder # 143... that meas a lot of failures (reshaping the batch and/or the tensor, transposing one and/or the other, looking for workarounds on internet..) ! 
Do not hesitate to ask for more info if needed."
13664,MaxPoolingOp only supports NHWC. ERROR in my benchmark we would use NCHW dataformat,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.0-rc0
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**: 0.6.1
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
**numactl -m 1 python ./tensorflow/models/image/convnet-benchmark-alexnet/benchmark_alexnet_MKL.py --batch_size 256 --num_batches 100 -forward_backward_only --cpu knl 2>&1|tee tensorflow_alexnet_mkl.txt**

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I have compiled tensorflow with bazel 0.6.1 and install it on my xeon phi TM platform.
Meanwhile I have got a benchmark related with alexnet.

When I run the benchmark I got the error:
_2017-10-12 09:38:14.503463: E tensorflow/core/common_runtime/executor.cc:643] Executor failed to create kernel. Invalid argument: Default MaxPoolingOp only supports NHWC.
         [[Node: pool1 = MaxPool[T=DT_FLOAT, data_format=""NCHW"", ksize=[1, 1, 3, 3], padding=""VALID"", strides=[1, 1, 2, 2], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](conv1)]]_

Also I find that in the benchmark, it has take both **""NCHW""** and **""NHWC""** data format into consideration. So the benchmark should support both data format.
I couldn't figure out why,  is this error relates with out compile process, or might I need to add some other option into the compile process.
Here is my compile command:

_bazel build --config=mkl --copt=""-g"" --copt=""-DEIGEN_USE_VML"" --copt=""-mavx2"" --copt=""-mfma"" --copt=""-O3"" --verbose_failures  --copt=""-L/opt/intel/gcc/lib64"" -s -c opt //tensorflow/tools/pip_package:build_pip_package_

anyone could help!



### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13663,"Failed to run TensorFlow inference with inputs:[image_tensor], outputs:[detection_boxes, detection_scores, detection_classes, num_detections]","Facing this issue when running TFDetect app. Even the app is crashing.
This issue i'm facing is when i'm running the example tensorflow android app."
13658,tensorflow 1.3.0 build with bazel failed on redhat7.4BU1,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: redhat 7.4BU1
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: Python 2.7.5
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
build tensorflow with bazel failed:
**bazel build --config=mkl --copt=""-g"" --copt=""-DEIGEN_USE_VML"" --copt=""-mavx2"" --copt=""-mfma"" --copt=""-O3"" --verbose_failures --copt=""-Ofast"" --copt=""-L/opt/intel/gcc/lib64"" -s -c opt //tensorflow/tools/pip_package:build_pip_package**

failed message:
_WARNING: ignoring http_proxy in environment.
ERROR: /root/.cache/bazel/_bazel_root/6093305914d4a581ed00c0f6c06f975b/external/io_bazel_rules_closure/closure/private/defs.bzl:27:16: The `set` constructor for depsets is deprecated and will be removed. Please use the `depset` constructor instead. You can temporarily enable the deprecated `set` constructor by passing the flag --incompatible_disallow_set_constructor=false.
ERROR: error loading package '': Extension file 'closure/private/defs.bzl' has errors._


what's more see the bazel info:
_[root@unassigned-hostname tensorflow-1.3.0]# bazel info
WARNING: ignoring http_proxy in environment.
Extracting Bazel installation...
..........................................
ERROR: /root/.cache/bazel/_bazel_root/6093305914d4a581ed00c0f6c06f975b/external/io_bazel_rules_closure/closure/private/defs.bzl:27:16: The `set` constructor for depsets is deprecated and will be removed. Please use the `depset` constructor instead. You can temporarily enable the deprecated `set` constructor by passing the flag --incompatible_disallow_set_constructor=false.
ERROR: error loading package '': Extension file 'closure/private/defs.bzl' has errors._


how did I install bazel on redhat:
https://docs.bazel.build/versions/master/install-redhat.html

add the bazel repo
then yum install bazel

Is something wrong with the bazel?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13657,tf.train.import_meta_graph() works strangely compared to restored session with tf.train.Supervisor,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.1.0-13-g8ddd727 1.1.0
- **Python version**: Python 3.4.3 
- **CUDA/cuDNN version**: CUDA
- **GPU model and memory**: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:02:00.0
Total memory: 11.92GiB

### Describe the problem

Working on GANs in tensorflow.

I would like to load a generator from a different saved session and graph, in order to do some new ops on this part of the graph that has been trained. I use tf.train.import_meta_graph() but it works strangely compared to restored session with tf.train.Supervisor.
It seems like the tf.train.Supervisor reassigns the tensors in a better way. I would have guessed that the generator would have been successfully loaded or not, but not in such a strange way. 
I don't know if it is a bug.

### Source code / logs

When I use tf.train.import_meta_graph():
```
train_dir = ""./train_logs/mnist/0"" 
ckpt = tf.train.latest_checkpoint(train_dir)
filename = ""."".join([ckpt, 'meta'])
saver = tf.train.import_meta_graph(filename)

z_optim = tf.get_variable(name='z_optim', shape= [number_ini_z * batch_imgs_number, 100], initializer=tf.truncated_normal_initializer())
gen_z = dcgan.generator(z_optim, is_training=False, reuse=False, name='generator_z')
```
Then I create basic image summary:
```
image_test = tf.cast(((gen_z / 2.0) + 0.5) * 255.0, tf.uint8)
s_test = tf.summary.image('reconstructed_image', image_test, max_outputs=3)
```
In order to reassign all the trained values to the new generator created from scratch, I collect all the variables that were in the scope of the ""generator"" and then create my own list of initializers:
```
gen_variables_to_initialize = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator_z')

gen_tensors_to_restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator')

list_assign_op = []

for variables, tensors in zip(gen_variables_to_initialize, gen_tensors_to_restore):
    list_assign_op.append(tf.assign(variables, tensors))

```
I have to explicitly create another list of variables that have to be initialized:
```
variables_initializer_except_for_generator = []

variables_to_initialize = gen_variables_to_initialize + dis_variables_to_initialize 

for variable in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):
        if variable not in variables_to_initialize:
            variables_initializer_except_for_generator.append(variable.initializer)
```
 so I run the session:
```
with tf.Session() as sess:
    logwriter = tf.summary.FileWriter('./logs', sess.graph)
    saver.restore(sess, ckpt)
    sess.run((list_assign_op, variables_initializer_except_for_generator))
    s = sess.run(s_test)
    logwriter.add_summary(s)
```
This code gives me such digits on tensorboard: 
![yo](https://user-images.githubusercontent.com/23338676/31488738-4d73e66c-af71-11e7-94a2-161745e052d0.jpg)

And then I remembered I used tf.train.Supervisor during the training, At the beginning I thought my GAN was not trained enough, but I instead ran:
```
logdir = ""./train_logs/mnist/0""
sv = tf.train.Supervisor(logdir=logdir,
                           save_summaries_secs=None, save_model_secs=120)

with sv.managed_session() as sess:
    logwriter = tf.summary.FileWriter('./logs2', sess.graph)
    s = sess.run(s_test)
    logwriter.add_summary(s)
```
I had this on tensorboard:

![yo2](https://user-images.githubusercontent.com/23338676/31488901-bf877566-af71-11e7-9f22-419a7b0f425b.jpg)

It seems like the tf.train.Supervisor reassigns the tensors in a better way. I would have guessed that the generator would have been successfully loaded or not, but not in such a strange way.

The problem is that I can't use the second method if I want to do some new stuffs with the generator (such as inverting it through another training, but his time on z_optim) because it does not want to add new nodes to the graph when it has not found it in previous training checkpoint. (Btw I had to change z_optim in tf.random_normal([number_ini_z * batch_imgs_number, 100], mean=0.0, stddev=1.0,name='random_z') for that same reason.

Do you have an any idea of why such a thing occurs? Or any other suggestion in loading the generator?"
13655,AttributeError: 'GFile' object has no attribute 'writelines,"Hi when i try to execute this code:

`from tensorflow.python.platform import gfile
... with gfile.GFile(""%s_%s"" % (target_path, len_d + len_q), mode=""w"") as tokens_file:
        tokens_file.writelines(results)...`

I obtain the following error:

`AttributeError: 'GFile' object has no attribute 'writelines'`

Why GFile haven't writelines attribute in verision 1.2? What method should i use instead? Thanks a lot.
"
13653,"Android: Error:(68, 13) Failed to resolve: org.tensorflow:tensorflow-android:+","Error:(68, 13) Failed to resolve: org.tensorflow:tensorflow-android:+

Are there some problems with remote repository?

```
buildscript {
    repositories {
        jcenter()
        google()
    }
    dependencies {
        classpath 'com.android.tools.build:gradle:3.0.0-beta7'
    }
}

allprojects {
    repositories {
        jcenter()
        maven {
            url 'https://maven.google.com'
        }
        maven { url ""https://jitpack.io"" }
    }
}
```

https://bintray.com/google/tensorflow/tensorflow-android

![screenshot_1](https://user-images.githubusercontent.com/8851301/31483788-91917476-af36-11e7-93cd-d9749967982d.png)
"
13652,No OpKernel was registered to support Op 'Assign' running nightlypi stable build on raspberry pi 3,"I am running the tensorflow backend to keras on a raspberry pi 3 with the lastest Stretch
The tensorflow 1.3 build is http://ci.tensorflow.org/view/Nightly/job/nightly-pi/lastStableBuild/

When running the Adam optimizer I'm seeing the following error:

tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Assign' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_INT32]

         [[Node: Adam_2/iterations/Assign = Assign[T=DT_INT64, _class=[""loc:@Adam_2/iterations""], use_locking=true, validate_shape=true](Adam_2/iterations, Adam_2/iterations/initial_value)]]

What can I do to solve this?"
13651,Android -- No OpKernel was registered to support Op 'SparseToDense' with these attrs,"I am trying to load a graph inside Android that I generated and frozen. I keep getting this error whenever I try to run it:

```
Caused by: java.lang.IllegalArgumentException: No OpKernel was registered to support
Op 'SparseToDense' with these attrs.  Registered devices: [CPU], Registered kernels:
device='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]
device='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]
device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]
device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]
device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]
device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]
                                                                                            
[[Node: output = SparseToDense[T=DT_INT64, Tindices=DT_INT64, validate_indices=true](CTCBeamSearchDecoder, CTCBeamSearchDecoder:2, CTCBeamSearchDecoder:1, output/default_value)]]
```

I also optimized the graph for inference. When I try to load it onto Android, I get this error:

```
java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef expected inputs '' do not match 1 inputs 
specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; 
NodeDef: stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/add/y = Const[dtype=DT_INT32, 
value=Tensor<type: int32 shape: [] values: 1>](stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Switch:1)
```

Steps to get the frozen graph and optimized graph:
1. Clone this [repository](https://github.com/selcouthlyBlue/bi_lstm_ocr):
2. Run dummy_train.py producing the .pbtxt and checkpoint files
3. Run dummy_freeze_and_save.py producing the frozen and optimized graphs frozen_bi_lstm_ctc_ocr.pb and optimized_frozen_bi_lstm_ctc_ocr.pb, respectively.

Files in the mentioned repository relevant to the problem:
- [The Bidirectional LSTM Network](https://github.com/selcouthlyBlue/bi_lstm_ocr/blob/master/main/TFStackedBidirectionalLstmNetwork.py) (contains the network specs and training code)
- [The utilities file](https://github.com/selcouthlyBlue/bi_lstm_ocr/blob/master/main/utils.py) (containing the graph freezing and optimization codes)
- [The dummy configs](https://github.com/selcouthlyBlue/bi_lstm_ocr/tree/master/test_files/configs) in the test files (just to train the model for 1 epoch)

Here is the Java part related to the problem:

   ```
 public String recognizeHandwritingFrom(Bitmap bitmap) {
        bitmap = Bitmap.createScaledBitmap(bitmap, 1024, 128,, true);
        int[] intValues = new int[bitmap.getWidth() * bitmap.getHeight()];
        bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());
        float[] floatValues = new float[bitmap.getWidth() * bitmap.getHeight()];
        for (int i = 0; i < intValues.length; ++i) {
            final int val = intValues[i];
            floatValues[i] = (((val >> 16) & 0xFF));
        }
        float[] result = new float[80];

        long[] INPUT_SIZE = new long[]{1, bitmap.getHeight(), bitmap.getWidth()};
        String[] inputs = new String[]{""input"", ""seq_len_input""};
        inferenceInterface.feed(inputs[0], floatValues, INPUT_SIZE);
        inferenceInterface.feed(inputs[1], new int[]{bitmap.getWidth()}, 1);

        String[] outputs = new String[]{""output""};
        inferenceInterface.run(outputs);
        inferenceInterface.fetch(outputs[0], result);

        return result.toString();
    }
```
I'm using 

- Python 3.5.3 :: Anaconda custom (64-bit)
- The python tensorflow build is downloaded using Anaconda. Tensorflow version is 1.2.1
- The compiled tensorflow for IOS is from this [nightly build](https://ci.tensorflow.org/view/Nightly/job/nightly-android/44/artifact/).

I would be really grateful if anyone has an idea on why Android seems to not be able to find `SparseToDense` as this is the only thing I have to fix to make it work.

If you would like to run the android application as well, you can clone this [repository](https://github.com/selcouthlyBlue/mem2speech). Just get the files from the nightly build and place them inside app/libs folder following this structure:

```
libs
|____arm64-v8a
| |____libtensorflow_inference.so
|____armeabi-v7a
| |____libtensorflow_inference.so
|____libandroid_tensorflow_inference_java.jar
|____x86
| |____libtensorflow_inference.so
|____x86_64
| |____libtensorflow_inference.so
```"
13650,What's the difference between function eval() and sess.run()?,"Codes are as follows:
```python
# encoding: utf-8
import load
import tensorflow as tf
import numpy as np
from sklearn.metrics import confusion_matrix

def get_chunk(samples, labels, chunkSize):
    if len(samples) != len(labels):
        raise Exception('Length of samples and labels must equal')
    stepStart = 0
    i = 0
    while stepStart < len(samples):
        stepEnd = stepStart + chunkSize
        if stepEnd < len(samples):
            yield i, samples[stepStart: stepEnd], labels[stepStart: stepEnd]
            i += 1
        stepStart = stepEnd

class NetWork():
    def __init__(self, num_hidden, batch_size, num_labels, image_size, channel):
        self.num_hidden = num_hidden
        self.batch_size = batch_size
        self.num_labels = num_labels

        self.image_size = image_size
        self.num_channel= channel

        self.tf_train_samples = None
        self.tf_train_labels  = None
        self.tf_test_samples  = None

        self.graph = tf.Graph()      
        self.define_graph()
        self.sess  = tf.Session(graph = self.graph)
        self.writer= tf.summary.FileWriter('./board', self.graph)
    
    def define_graph(self):
        with self.graph.as_default():
            with tf.name_scope('inputs'):
                self.tf_train_samples = tf.placeholder(tf.float32, shape = (self.batch_size, self.image_size, self.image_size, self.num_channel), name = 'train_samples')
                self.tf_train_labels   = tf.placeholder(tf.float32, shape = (self.batch_size, self.num_labels), name = 'train_labels')
                self.tf_test_samples  = tf.placeholder(tf.float32, shape = (self.batch_size, self.image_size, self.image_size, self.num_channel), name = 'test_labels')
            with tf.name_scope('fc1'):
                weight1 = tf.Variable(tf.truncated_normal([self.image_size * self.image_size, self.num_hidden], stddev = 0.1), name = 'weight1')
                bias1   = tf.Variable(tf.truncated_normal([self.num_hidden], stddev = 0.1), name = 'bias1')
                tf.summary.histogram('weight1', weight1)
                tf.summary.histogram('bias1', bias1)
            with tf.name_scope('fc2'):
                weight2 = tf.Variable(tf.truncated_normal([self.num_hidden, self.num_labels], stddev = 0.1), name = 'weight2')
                bias2   = tf.Variable(tf.truncated_normal([self.num_labels], stddev = 0.1), name = 'bias2')
                tf.summary.histogram('weight2', weight2)
                tf.summary.histogram('bias2', bias2)

            def model(data):
                shape = data.get_shape().as_list()
                reshape = tf.reshape(data, [shape[0], shape[1] * shape[2] * shape[3]])
                with tf.name_scope('fc1_model'):
                    hidden  = tf.nn.relu(tf.matmul(reshape, weight1) + bias1)   
                with tf.name_scope('fc2_model'):
                    return tf.matmul(hidden, weight2) + bias2

            logits = model(self.tf_train_samples)
            with tf.name_scope('loss'):
                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = self.tf_train_labels))
                tf.summary.scalar('loss', self.loss)
            with tf.name_scope('optimizer'):
                self.optimizer = tf.train.AdamOptimizer(0.01).minimize(self.loss)
            with tf.name_scope('predictions'):
                self.train_prediction = tf.nn.softmax(logits)
                self.test_prediction  = tf.nn.softmax(model(self.tf_test_samples))
            self.merged = tf.summary.merge_all()

    def run(self):
        ### train
        def print_confusion_matrix(confusionMatrix):
            print('Consusion Matrix: ')
            for i, line in enumerate(confusionMatrix):
                print line, line[i]/np.sum(line)
            
            a = 0
            for i, column in enumerate(np.transpose(confusionMatrix, (1, 0))):
                a += (column[i] / np.sum(column)) * (np.sum(column) / 26000)
                print column[i] / np.sum(column), 
            print '\n', np.sum(confusionMatrix, a)

        with self.sess as sess:
            tf.global_variables_initializer().run()
            print 'Start Training'

            train_samples, train_labels = load.loadmat_data('train_32x32.mat')
            train_samples, train_labels = load.reformat(train_samples, train_labels)
            train_samples = load.normalize(train_samples)

            test_samples, test_labels = load.loadmat_data('test_32x32.mat')
            test_samples, test_labels = load.reformat(test_samples, test_labels)
            test_samples = load.normalize(test_samples)

            for i, samples, labels in get_chunk(train_samples, train_labels, chunkSize = self.batch_size):
                _, l, predictions, summary = sess.run([self.optimizer, self.loss, self.train_prediction, self.merged], feed_dict = {self.tf_train_samples: samples, self.tf_train_labels: labels})
                self.writer.add_summary(summary, i)
                accuracy, _ = self.accuracy(predictions, labels)
                if i % 50 == 0:
                    print 'Minibatch loss at step %d: loss is %f' % (i, l)
                    print 'Minibatch accuracy: %.1f' % accuracy

            accuracies = []
            confusionMatrices = []
            for i, samples, labels in get_chunk(test_samples, test_labels, chunkSize=self.batch_size):
                result = sess.run(self.test_prediction, feed_dict={self.tf_test_samples: samples})
				# result = self.test_prediction.eval(feed_dict={self.tf_test_samples: samples})
                accuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)
                accuracies.append(accuracy)
                confusionMatrices.append(cm)
                print('Test Accuracy: %.1f%%' % accuracy)
            print(' Average  Accuracy:', np.average(accuracies))
            print('Standard Deviation:', np.std(accuracies))
            print_confusion_matrix(np.add.reduce(confusionMatrices))

    def accuracy(self, predictions, labels, need_confusion_matrix = False):
        _predictions = np.argmax(predictions, 1)
        _labels      = np.argmax(labels, 1)
        cm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None
        accuracy = (100 * np.sum(_predictions == _labels) / predictions.shape[0])
        return accuracy, cm

if __name__ == '__main__':
    net = NetWork(num_hidden = 128, batch_size = 100, num_labels = 10, image_size = 32, channel = 1)
    net.run()
```

When I use 'result = self.test_prediction.eval(feed_dict={self.tf_test_samples: samples})', it works fine, while in 'result = sess.run(self.test_prediction, feed_dict={self.tf_test_samples: samples})', it shows error 'ValueError: operands could not be broadcast together with shapes (10,10) (9,9) '. SO, what's difference between function eval() and sess.run()"
13649,LSTMBlockFusedCell  does not support using  DropoutWrapper ,"I am trying to use DropoutWrapper with LSTMBlockFusedCell as follows:

```
cell = tf.contrib.rnn.LSTMBlockFusedCell(num_units,forget_bias) 
cell = tf.contrib.rnn.DropoutWrapper(cell,dropout)
```
 I get an exception that the LSTMBlockFusedCell is not an RNNCell
Message: The parameter cell is not a RNNCell. Which is raised form _like_rnncell during DropoutWrapper initialization. 

It is checking for those proprieties on the cell:
""""Checks that a given object is an RNNCell by using duck typing.""""""
   conditions = [hasattr(cell, ""output_size""), hasattr(cell, ""state_size""),                 hasattr(cell, ""zero_state""), callable(cell)]  LSTMBlockFusedCell does not have output_size , state_size or zero_state properties. 

Should LSTMBlockFusedCell  act like RNNCell to allow using various wrappers?

https://stackoverflow.com/questions/46699985/using-tensorflow-dropoutwrapper-with-lstmblockfusedcell"
13648,tf.string_input_producer() doesn't work with tf.placeholder,"In a scenario it is intended to dynamically change the file names of tensorflow record files, as indicated by the following code:

```
import tensorflow as tf

def test(s):
    filename_queue = tf.train.string_input_producer([s])

    reader = tf.TextLineReader()
    key, value = reader.read(filename_queue)

    record_defaults = [[1.0], [1]]
    col1, col2 = tf.decode_csv(value, record_defaults = record_defaults)

    return col1, col2

s = tf.placeholder(tf.string, None, name = 's')
# s = tf.constant('file0.csv', tf.string)
ss = [""file0.csv"", ""file1.csv""]
inputs, labels = test(s)

with tf.Session() as sess:
    sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])

    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)

    for e in ss:
        inputs_val, labels_val = sess.run([inputs, labels], feed_dict = {s: e})
        print(""input {} - label {}"".format(inputs_val, labels_val))

    coord.request_stop()
    coord.join(threads)

```

it is observed the code above is working with tf.constant which is commented above, however not tf.placeholder, which is believed to be equivalent.

There is no direct error related to the malfunctioning as below:

```
(tensorflow)[yuming@atlas4 working-files]$ python 36.py
2017-10-12 11:13:21.753318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties:
name: Quadro M4000 major: 5 minor: 2 memoryClockRate(GHz): 0.7725
pciBusID: 0000:83:00.0
totalMemory: 7.93GiB freeMemory: 7.87GiB
2017-10-12 11:13:21.858019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 1 with properties:
name: Quadro K2200 major: 5 minor: 0 memoryClockRate(GHz): 1.124
pciBusID: 0000:03:00.0
totalMemory: 3.95GiB freeMemory: 3.54GiB
2017-10-12 11:13:21.858060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:980] Device peer to peer matrix
2017-10-12 11:13:21.858068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] DMA: 0 1
2017-10-12 11:13:21.858072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] 0:   Y N
2017-10-12 11:13:21.858075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] 1:   N Y
2017-10-12 11:13:21.858082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Quadro M4000, pci bus id: 0000:83:00.0, compute capability: 5.2)
2017-10-12 11:13:21.858088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1042] Ignoring gpu device (device: 1, name: Quadro K2200, pci bus id: 0000:03:00.0, compute capability: 5.0) with Cuda multiprocessor count: 5. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
Traceback (most recent call last):
  File ""36.py"", line 26, in <module>
    inputs_val, labels_val = sess.run([inputs, labels], feed_dict = {s: e})
  File ""/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1118, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1315, in _do_run
    options, run_metadata)
  File ""/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_0_input_producer' is closed and has insufficient elements (requested 1, current size 0)
         [[Node: ReaderReadV2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](TextLineReaderV2, input_producer)]]

Caused by op u'ReaderReadV2', defined at:
  File ""36.py"", line 17, in <module>
    inputs, labels = test(s)
  File ""36.py"", line 7, in test
    key, value = reader.read(filename_queue)
  File ""/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py"", line 194, in read
    return gen_io_ops._reader_read_v2(self._reader_ref, queue_ref, name=name)
  File ""/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 654, in _reader_read_v2
    queue_handle=queue_handle, name=name)
  File ""/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 789, in _apply_op_helper
    op_def=op_def)
  File ""/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3052, in create_op
    op_def=op_def)
  File ""/home/yuming/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1610, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): FIFOQueue '_0_input_producer' is closed and has insufficient elements (requested 1, current size 0)
         [[Node: ReaderReadV2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](TextLineReaderV2, input_producer)]]

```

OS version Redhat 7.3, Python version 2.7.5, Tensorflow version 1.3

file0.csv and file1.csv are quite simple csv files just with two lines:
0.1,0
0.9,1"
13647,Extracting weight values from output_graph.pb,"Hi, this isn't a bug more of request for a tutorial or guidance -  I have asked this question multiple times on Stackoverflow  and can't seem to get any responses nor views. My past questions on tensorflow also didn't get much views on Stackoverflow -- I think this is because tensorflow is still relatively new and it's hard to find knowledgeable people.

I'm sorry to post, here but I don't know where else to ask -- I've been at it for a few days and looked 
the tutorials on the official site but I couldn't find any. 

I used inceptionV-3 for transfer learning  and now I have a output_graph.pb.
I want to extract all the weights and biases for each layer, but I can't seem to find a way to do this. 

I've gone over most the tutorials in tensorflow:
https://www.tensorflow.org/programmers_guide/saved_model#models

And most blogs or stackoverflows posts show how to deal with 'meta` and `ckpt` graphs.

but not on `pb` graphs.  

I've attempted this:

https://stackoverflow.com/questions/46696859/tf-graphkeys-trainable-variables-on-output-graph-pb-resulting-in-empty-list


```
def create_graph(modelFullPath):
    """"""Creates a graph from saved GraphDef file and returns a saver.""""""
    # Creates graph from saved graph_def.pb.
    with tf.gfile.FastGFile(modelFullPath, 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        tf.import_graph_def(graph_def, name='')

GRAPH_DIR = r'C:\tmp\output_graph.pb'
create_graph(GRAPH_DIR)


with tf.Session() as sess:
    all_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
    print (len(all_vars))
```

But I'm getting a value of 0 -- so empty return. 


I've also attempted using `freeze_graph.py`  after cloning tensorflow repository but couldn't understand all of the arguments (created a stackoverflow question on this as well)

--input_graph    = path to the graph I want to extract.

I'm assuming the above arugument is all I need to use?

However when running this command:

`python3.6 C:\Users\Moondra\tensorflow\tensorflow\python\tools\freeze_graph.py --input-graph=C:\tmp\output_graph.pb`

I'm getting a import error which is confusing me as well:

```
from tensorflow.python.tools import saved_model_util
     importError: cannot import name 'saved_model_utils'
```


I've cloned the repository properly and the saved_model_utils is within the tools folder, so not sure 
why I""m getting an import error.  

Any help would be appreciated, and once again, sorry for posting here.  :(


"
13644,Add support for Mel Generalized Cepstrum Analysis to tf.signal.,"Requested via discuss@
https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/k6EI-BxbCMg

Please :+1: if you would like to see this feature in tf.contrib.signal."
13643,conv2d_transpose crashes on GPU with zero size batch,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary (installed with `conda install tensorflow-gpu`)
- **TensorFlow version (use command below)**: b'unknown' 1.3.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: GTX 980
- **Exact command to reproduce**: See below

### Describe the problem
Execute the script below.  It works correctly when running on a CPU, but on a GPU it crashes with this error:

```
tensorflow/stream_executor/cuda/cuda_dnn.cc:430] could not convert BatchDescriptor {count: 0 feature_map_count: 1 spatial: 7 7  value_min: 0.000000 value_max: 0.000000 layout: BatchDepthYX} to cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM
```

The error happens when the first dimension of the input array is 0.

### Source code / logs
```python
import tensorflow as tf
import numpy as np

a = tf.placeholder(dtype=tf.float32, shape=(None, 7, 7, 1))
b = tf.contrib.layers.conv2d_transpose(a, num_outputs=16, kernel_size=5, stride=2)
session = tf.Session()
session.run(tf.global_variables_initializer())
print(session.run(b, feed_dict={a: np.zeros((0, 7, 7, 1))}))
```"
13642,[bug?] tf.nn.embedding_lookup returns 0 when ids out of range,"It seems tf.nn.embedding_lookup will simply return tensor of zeros when ids out of range (larger than the embedding table size):
```
import tensorflow as tf
embs = tf.ones([100, 100]) 
idx = tf.cast(tf.ones([1]) * 1000, tf.int32)
with tf.Session() as sess:
  emb = sess.run(tf.nn.embedding_lookup(embs, idx))
```
The emb will be tensor of zeros. I am not sure if this is a bug, or by design for efficiency concern? It would be nice if there is a runtime exception. That will do a big favor in avoiding hidden bugs that lead to performance degeneration.

(I am running tensorflow-gpu 1.3.0 in Ubuntu 16.04)"
13641,Extend SVD gradient to support backpropagating through complex and (strongly) rectangular U and V,"This initial version of SVD gradients has the following restrictions:
  Only supports statically known inner matrix dimensions m and n.

Backpropagating through U and V (i.e. backpropagating through SVD nodes with compute_uv=True) has further restrictions:
  a) Only supports real tensors.
  b) Only supports square and ""almost square"" matrices where the number of rows and columns differ by at most 1.
  c) full_matrices must be true also. This does not currently have severe implications, given the restriction in b).

Support for dynamic shapes and a) (I think) is straightforward to fix.  But b) is probably a deeper issue having to do with the (lack of) uniqueness of the decomposition, and will require some analysis. I think that if we understand b), we can get around the restriction in c) as well.

I'm marking this as contributions welcome, in the hope that somebody with better math skills than myself will help out :-)"
13639,Invoke get_shape() on sparse_tensor leads to feeding error,"If I invoke get_shape method on sparse_tensor, the shape tensor will be added into the _unfeedable_tensors set of the current graph. Then when I feed the sparse tensor, an error occurs.

The codes below show this error

```python
import tensorflow as tf
import numpy as np

shape = np.array([7, 9, 2], dtype=np.int64)
indices = np.array([[3, 2, 0], [4, 5, 1]], dtype=np.int64)
values = np.array([1.0, 2.0], dtype=np.float32)
x = tf.sparse_placeholder(tf.float32, shape=shape)

with tf.Session() as sess:
    x.get_shape() # <-- Troublemaker
    # This line leads to the exception:
    # 	   Tensor Tensor(""Const:0"", shape=(3,), dtype=int64) may not be fed.
    # The side effection of this line is that 
    # it adds the 'shape' tensor into Graph._unfeedable_tensors, 

    print(sess.run(x, feed_dict={
    	x: tf.SparseTensorValue(indices, values, shape)}))
```

The stacktrace
```
ValueError                                Traceback (most recent call last)
<ipython-input-1-baac6f49a954> in <module>()
     10     x.get_shape()
     11     print(sess.run(x, feed_dict={
---> 12     	x: tf.SparseTensorValue(indices, values, shape)}))

/Users/liqimai/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    765     try:
    766       result = self._run(None, fetches, feed_dict, options_ptr,
--> 767                          run_metadata_ptr)
    768       if run_metadata:
    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/Users/liqimai/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
    944                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))
    945           if not self.graph.is_feedable(subfeed_t):
--> 946             raise ValueError('Tensor %s may not be fed.' % subfeed_t)
    947           subfeed_name = compat.as_bytes(subfeed_t.name)
    948           feed_dict_string[subfeed_name] = np_val

ValueError: Tensor Tensor(""Const:0"", shape=(3,), dtype=int64) may not be fed.
```
------------------------

### System information
I do not think this bug is related to my environment.
== cat /etc/issue ===============================================
Darwin liqimaideMacBook-Pro.local 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64
Mac OS X 10.12.6

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.0.0 (clang-900.0.37)
Target: x86_64-apple-darwin16.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin MacBook-Pro.local 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.11.3)
protobuf (3.2.0)
tensorflow (1.0.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.0.0
tf.GIT_VERSION = v1.0.0-rc2-15-g47bba63-dirty
tf.COMPILER_VERSION = v1.0.0-rc2-15-g47bba63-dirty
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ==================================================="
13636,get_shape() does not work for output of tf.image.resize_nearest_neighbor(),"Hello everyone,

Following the issue  #7932, I have also noticed that get_shape() does not work when using
tf.image.resize_nearest_neighbor().

I use linux 16.06 and TF in [1.3.0, 1.3.1, 1.4.0-dev20171008] and I have the same error.

The error is quite easy to understand. The fact that tf.image.resize_nearest_neighbor is unable to compute correctly **get_shape** leads to some problematic behavior.

```python
import numpy as np
import tensorflow as tf

x = tf.placeholder(tf.float32, [None, 32, 32, 1])

print(""x_shape:"", x.get_shape()) # x_shape: (?, 32, 32, 1)

input_shape = tf.shape(x)[1:3]
newsize     = tf.multiply(input_shape, (2, 2)) # gives (64, 64)

####################

resized_data = tf.image.resize_nearest_neighbor(
    images        = x,
    size          = newsize,
    align_corners = None,
    name          = None
)
print(""resized_data:"", resized_data.get_shape()) # resized_data: (?, ?, ?, 1)

#################### NOW THE PROBLEMATIC #################

flatten_tensor = tf.contrib.layers.flatten(inputs = resized_data)
print(""flatten_tensor:"", flatten_tensor.get_shape()) # resized_data: (?, ?)

failing_layer = tf.contrib.layers.fully_connected(
    inputs        = flatten_tensor,
    num_outputs   = 1,
    activation_fn = None
)

##############################################
############# LAUNCH THE SESSION #############
##############################################

with tf.Session() as sess:
    rslt = sess.run(resized_data, feed_dict={x: np.ones((666, 32, 32,1))})
    print(""Rslt Shape:"", rslt.shape)
    
    failing_layer = sess.run(failing_layer, feed_dict={x: np.ones((666, 32, 32,1))})
    print(""failing_layer Shape:"", failing_layer.shape)
```

For information, it is absolutely to launch the session with the FC layer in the script, it completely fails."
13635,w9wang2,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13634,"Failed build tensorflow with bazel, failed with: error SQLite will not work correctly with the -ffast-math option of GCC.","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: **Ubuntu 17.04**
- **TensorFlow installed from (source or binary)**: 
- **TensorFlow version (use command below)**: **commit: 1ad5e692e2fc218ca0b2a9a461c19762fdc9674b master branch**
- **Python version**: **Python 2.7.13**
- **Bazel version (if compiling from source)**:  **0.6.1**
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
**#build tensorflow**
_bazel build --config=mkl --copt=""-g"" --copt=""-DEIGEN_USE_VML"" --copt=""-mavx2"" --copt=""-mfma"" --copt=""-O3"" --verbose_failures  --copt=""-Ofast"" --copt=""-L/opt/intel/gcc/lib64"" -s -c opt //tensorflow/tools/pip_package:build_pip_package_

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
**During the build process, it failed with the message:**

_ERROR: /root/.cache/bazel/_bazel_root/33016bcb7111d180c7dd9b171742c7e7/external/sqlite_archive/BUILD.bazel:9:1: C++ compilation of rule '@sqlite_archive//:sqlite' failed (Exit 1): gcc failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/33016bcb7111d180c7dd9b171742c7e7/execroot/org_tensorflow && \
  exec env - \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL=0 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DEIGEN_USE_VML -g -DEIGEN_USE_VML -mavx2 -mfma -O3 -Ofast -L/opt/intel/gcc/lib64 -MD -MF bazel-out/local-opt/bin/external/sqlite_archive/_objs/sqlite/external/sqlite_archive/sqlite3.pic.d -fPIC -iquote external/sqlite_archive -iquote bazel-out/local-opt/genfiles/external/sqlite_archive -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -isystem external/sqlite_archive -isystem bazel-out/local-opt/genfiles/external/sqlite_archive -isystem external/bazel_tools/tools/cpp/gcc3 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/sqlite_archive/sqlite3.c -o bazel-out/local-opt/bin/external/sqlite_archive/_objs/sqlite/external/sqlite_archive/sqlite3.pic.o)
external/sqlite_archive/sqlite3.c: In function 'sqlite3IsNaN':
external/sqlite_archive/sqlite3.c:28276:3: error: #error SQLite will not work correctly with the -ffast-math option of GCC.
 # error SQLite will not work correctly with the -ffast-math option of GCC.
   ^~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 75.602s, Critical Path: 59.11s
FAILED: Build did NOT complete successfully_


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13632,W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] ,"The source code I try to compile has been changed, and now it requires tf-nightly build(1.4 version). For that reason, I created new conda environment which includes 1.4 version of tensorflow.

However, I still use the same set-up except the version of tensorflow. I've read they anticipate releasing 1.4 version of tensorflow wih cuDNN 7 instead of cuDNN 6. Is this the reason that I get these warnings? Should I upgrade the cuDNN version from 6 to 7. 

I'm also getting ""RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6"" although I loaded the 3.6 version of tf-nightly build.

This is what I get:
```
2017-10-11 15:42:05.897046: W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] 
train 0 4000 0.764138 1.0965 0.604099
2017-10-11 15:42:40.117190: W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] 
2017-10-11 15:42:40.118152: W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] 
2017-10-11 15:42:40.118270: W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] 
train 0 4100 0.778095 1.09562 0.603462
2017-10-11 15:43:14.819378: W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] 
2017-10-11 15:43:14.820271: W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] 
2017-10-11 15:43:14.820375: W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] 
train 0 4200 1.55563 1.09606 0.603783
```

### System information
- **OS Platform and Distribution** : Linux Ubuntu 16.04 LTS
- **TensorFlow installed from (source or binary)**: tf_nightly_gpu-1.4.0.dev20171010-cp36-cp36m-manylinux1_x86_64.whl
- **TensorFlow version** : 1.4.0-dev20171010
- **Python version**: 3.6.1 Anaconda 64bit
- **CUDA/cuDNN version**:  cuda-ga2_8.0_amd64 / cudnn-8.0-linux-x64-v6.0
- **GPU model and memory**: Nvidia GeForce GT 710  1GB memory
- **Exact command to reproduce**: python train_xxx --train_batch_size 16 --val_batch_size 16
"
13631,[Feature Request] Add partial_run and partial_run_setup to MonitoredSession,"Hello, 

as mentioned in the topic, I wonder if there is a chance in the near future for MonitoredSession to support partial graph computation. "
13630," INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, assertion failed: [Unable to decode bytes as JPEG, PNG, GIF, or BMP]","Hi all,
I use Python 2.7.13 and Tensorflow 1.3.0 on CPU.

I want to use DensNet( [https://github.com/pudae/tensorflow-densenet](url) ) for regression problem. My data contains 60000 jpeg images with 37 float labels for each image. 
I saved my data into tfrecords files by: 

`
def Read_Labels(label_path):
    labels_csv = pd.read_csv(label_path)
    labels = np.array(labels_csv)
    return labels[:,1:]

def load_image(addr):
    # read an image and resize to (224, 224)
    img = cv2.imread(addr)
    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = img.astype(np.float32)
    return img

def Shuffle_images_with_labels(shuffle_data, photo_filenames, labels):
    if shuffle_data:
        c = list(zip(photo_filenames, labels))
        shuffle(c)
        addrs, labels = zip(*c)
        return addrs, labels

def image_to_tfexample_mine(image_data, image_format, height, width, label):
  return tf.train.Example(features=tf.train.Features(feature={
      'image/encoded': bytes_feature(image_data),    
      'image/format': bytes_feature(image_format),
      'image/class/label': _float_feature(label),
      'image/height': int64_feature(height),
      'image/width': int64_feature(width),
  }))

def _convert_dataset(split_name, filenames, labels, dataset_dir):
  assert split_name in ['train', 'validation']

  num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))

  with tf.Graph().as_default():
        
        for shard_id in range(_NUM_SHARDS):
          output_filename = _get_dataset_filename(dataset_path, split_name, shard_id)
         
          with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:
              start_ndx = shard_id * num_per_shard
              end_ndx = min((shard_id+1) * num_per_shard, len(filenames))
              for i in range(start_ndx, end_ndx):
                  sys.stdout.write('\r>> Converting image %d/%d shard %d' % (
                          i+1, len(filenames), shard_id))
                  sys.stdout.flush()

                  img = load_image(filenames[i])
                  image_data = tf.compat.as_bytes(img.tostring())
                    
                  label = labels[i]
                    
                  example = image_to_tfexample_mine(image_data, image_format, height, width, label)
                    
                  # Serialize to string and write on the file
                  tfrecord_writer.write(example.SerializeToString())
                
  sys.stdout.write('\n')
  sys.stdout.flush()
  
def run(dataset_dir):

    labels = Read_Labels(dataset_dir + '/training_labels.csv')
    
    photo_filenames = _get_filenames_and_classes(dataset_dir + '/images_training')
    
    shuffle_data = True 
    
    photo_filenames, labels = Shuffle_images_with_labels(
            shuffle_data,photo_filenames, labels)
    
    training_filenames = photo_filenames[_NUM_VALIDATION:]
    training_labels = labels[_NUM_VALIDATION:]
    
    validation_filenames = photo_filenames[:_NUM_VALIDATION]
    validation_labels = labels[:_NUM_VALIDATION]
    
    _convert_dataset('train',
                     training_filenames, training_labels, dataset_path)
    _convert_dataset('validation',
                     validation_filenames, validation_labels, dataset_path)
    
    print('\nFinished converting the Flowers dataset!')` 
________________________________________________________________________________
And I decode it by:

`
with tf.Session() as sess:

    feature = {
      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),
      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpeg'),
      'image/class/label': tf.FixedLenFeature(
          [37,], tf.float32, default_value=tf.zeros([37,], dtype=tf.float32)),
       }

    filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)
    
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)
   
    features = tf.parse_single_example(serialized_example, features=feature)
    
    image = tf.decode_raw(features['image/encoded'], tf.float32)
    print(image.get_shape())
    
    label = tf.cast(features['image/class/label'], tf.float32)

    image = tf.reshape(image, [224, 224, 3])

    images, labels = tf.train.shuffle_batch([image, label], batch_size=10, capacity=30, num_threads=1, min_after_dequeue=10)
    
    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
    sess.run(init_op)

    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)
    
    for batch_index in range(6):
        img, lbl = sess.run([images, labels])
        img = img.astype(np.uint8)
        print(img.shape)
        for j in range(6):
            plt.subplot(2, 3, j+1)
            plt.imshow(img[j, ...])
        plt.show()
    
    coord.request_stop()
    
    coord.join(threads)`
______________________________________________________________________________________________________
It's all fine up to this point. But when I use the bellow commands for decoding TFRecord files:

` 
reader = tf.TFRecordReader

 keys_to_features = {
      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),
      'image/format': tf.FixedLenFeature((), tf.string, default_value='raw'),
      'image/class/label': tf.FixedLenFeature(
          [37,], tf.float32, default_value=tf.zeros([37,], dtype=tf.float32)),
     }

  items_to_handlers = {
      'image': slim.tfexample_decoder.Image('image/encoded'),
      'label': slim.tfexample_decoder.Tensor('image/class/label'),
  }
  
decoder = slim.tfexample_decoder.TFExampleDecoder(
      keys_to_features, items_to_handlers)`
_________________________________________________________________________________________________
 I get the following error.

> INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, assertion failed: [Unable to decode bytes as JPEG, PNG, GIF, or BMP]
         [[Node: case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/Assert_1/Assert = Assert[T=[DT_STRING], summarize=3, _device=""/job:localhost/replica:0/task:0/cpu:0""](case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/is_bmp, case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/Assert_1/Assert/data_0)]]
INFO:tensorflow:Caught OutOfRangeError. Stopping Training.
INFO:sensorflow:Finished training! Saving model to disk.
_______________________________________________________________________________________________
To use Densenet for my problem, I should fix this error first. 
Could anybody please help me out of this problem. This code works perfectly for the datasets like flowers, MNIST and CIFAR10 available at [https://github.com/pudae/tensorflow-densenet/tree/master/datasets](url) but does not work for my data."
13629,"INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, assertion failed: [Unable to decode bytes as JPEG, PNG, GIF, or BMP] [[Node: case/If_0/decode_image/cond_jpeg/cond_png/cond_gif","Hi all,
I use Python 2.7.13 and Tensorflow 1.3.0 on CPU.

I want to use DensNet( [https://github.com/pudae/tensorflow-densenet](url) ) for regression problem. My data contains 60000 jpeg images with 37 float labels for each image. 
I saved my data into tfrecords files by: 

`
def Read_Labels(label_path):
    labels_csv = pd.read_csv(label_path)
    labels = np.array(labels_csv)
    return labels[:,1:]

def load_image(addr):
    # read an image and resize to (224, 224)
    img = cv2.imread(addr)
    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = img.astype(np.float32)
    return img

def Shuffle_images_with_labels(shuffle_data, photo_filenames, labels):
    if shuffle_data:
        c = list(zip(photo_filenames, labels))
        shuffle(c)
        addrs, labels = zip(*c)
        return addrs, labels

def image_to_tfexample_mine(image_data, image_format, height, width, label):
  return tf.train.Example(features=tf.train.Features(feature={
      'image/encoded': bytes_feature(image_data),    
      'image/format': bytes_feature(image_format),
      'image/class/label': _float_feature(label),
      'image/height': int64_feature(height),
      'image/width': int64_feature(width),
  }))

def _convert_dataset(split_name, filenames, labels, dataset_dir):
  assert split_name in ['train', 'validation']

  num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))

  with tf.Graph().as_default():
        
        for shard_id in range(_NUM_SHARDS):
          output_filename = _get_dataset_filename(dataset_path, split_name, shard_id)
         
          with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:
              start_ndx = shard_id * num_per_shard
              end_ndx = min((shard_id+1) * num_per_shard, len(filenames))
              for i in range(start_ndx, end_ndx):
                  sys.stdout.write('\r>> Converting image %d/%d shard %d' % (
                          i+1, len(filenames), shard_id))
                  sys.stdout.flush()

                  img = load_image(filenames[i])
                  image_data = tf.compat.as_bytes(img.tostring())
                    
                  label = labels[i]
                    
                  example = image_to_tfexample_mine(image_data, image_format, height, width, label)
                    
                  # Serialize to string and write on the file
                  tfrecord_writer.write(example.SerializeToString())
                
  sys.stdout.write('\n')
  sys.stdout.flush()
  
def run(dataset_dir):

    labels = Read_Labels(dataset_dir + '/training_labels.csv')
    
    photo_filenames = _get_filenames_and_classes(dataset_dir + '/images_training')
    
    shuffle_data = True 
    
    photo_filenames, labels = Shuffle_images_with_labels(
            shuffle_data,photo_filenames, labels)
    
    training_filenames = photo_filenames[_NUM_VALIDATION:]
    training_labels = labels[_NUM_VALIDATION:]
    
    validation_filenames = photo_filenames[:_NUM_VALIDATION]
    validation_labels = labels[:_NUM_VALIDATION]
    
    _convert_dataset('train',
                     training_filenames, training_labels, dataset_path)
    _convert_dataset('validation',
                     validation_filenames, validation_labels, dataset_path)
    
    print('\nFinished converting the Flowers dataset!')` 
________________________________________________________________________________
And I decode it by:

`
with tf.Session() as sess:

    feature = {
      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),
      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpeg'),
      'image/class/label': tf.FixedLenFeature(
          [37,], tf.float32, default_value=tf.zeros([37,], dtype=tf.float32)),
       }

    filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)
    
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)
   
    features = tf.parse_single_example(serialized_example, features=feature)
    
    image = tf.decode_raw(features['image/encoded'], tf.float32)
    print(image.get_shape())
    
    label = tf.cast(features['image/class/label'], tf.float32)

    image = tf.reshape(image, [224, 224, 3])

    images, labels = tf.train.shuffle_batch([image, label], batch_size=10, capacity=30, num_threads=1, min_after_dequeue=10)
    
    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
    sess.run(init_op)

    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)
    
    for batch_index in range(6):
        img, lbl = sess.run([images, labels])
        img = img.astype(np.uint8)
        print(img.shape)
        for j in range(6):
            plt.subplot(2, 3, j+1)
            plt.imshow(img[j, ...])
        plt.show()
    
    coord.request_stop()
    
    coord.join(threads)`
______________________________________________________________________________________________________
It's all fine up to this point. But when I use the bellow commands for decoding TFRecord files:

` 
reader = tf.TFRecordReader

 keys_to_features = {
      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),
      'image/format': tf.FixedLenFeature((), tf.string, default_value='raw'),
      'image/class/label': tf.FixedLenFeature(
          [37,], tf.float32, default_value=tf.zeros([37,], dtype=tf.float32)),
     }

  items_to_handlers = {
      'image': slim.tfexample_decoder.Image('image/encoded'),
      'label': slim.tfexample_decoder.Tensor('image/class/label'),
  }
  
decoder = slim.tfexample_decoder.TFExampleDecoder(
      keys_to_features, items_to_handlers)`
_________________________________________________________________________________________________
 I get the following error.

> INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, assertion failed: [Unable to decode bytes as JPEG, PNG, GIF, or BMP]
         [[Node: case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/Assert_1/Assert = Assert[T=[DT_STRING], summarize=3, _device=""/job:localhost/replica:0/task:0/cpu:0""](case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/is_bmp, case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/Assert_1/Assert/data_0)]]
INFO:tensorflow:Caught OutOfRangeError. Stopping Training.
INFO:sensorflow:Finished training! Saving model to disk.
_______________________________________________________________________________________________
To use Densenet for my problem, I should fix this error first. 
Could anybody please help me out of this problem. This code works perfectly for the datasets like flowers, MNIST and CIFAR10 available at [https://github.com/pudae/tensorflow-densenet/tree/master/datasets](url) but does not work for my data."
13628,"INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, assertion failed: [Unable to decode bytes as JPEG, PNG, GIF, or BMP]          [[Node: case/If_0/decode_image/cond_jpeg/cond_png/cond_gif","Hi all,
I use Python 2.7.13 and Tensorflow 1.3.0 on CPU.

I want to use DensNet( [https://github.com/pudae/tensorflow-densenet](url) ) for regression problem. My data contains 60000 jpeg images with 37 float labels for each image. 
I saved my data into tfrecords files by: 

`
def Read_Labels(label_path):
    labels_csv = pd.read_csv(label_path)
    labels = np.array(labels_csv)
    return labels[:,1:]

def load_image(addr):
    # read an image and resize to (224, 224)
    img = cv2.imread(addr)
    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = img.astype(np.float32)
    return img

def Shuffle_images_with_labels(shuffle_data, photo_filenames, labels):
    if shuffle_data:
        c = list(zip(photo_filenames, labels))
        shuffle(c)
        addrs, labels = zip(*c)
        return addrs, labels

def image_to_tfexample_mine(image_data, image_format, height, width, label):
  return tf.train.Example(features=tf.train.Features(feature={
      'image/encoded': bytes_feature(image_data),    
      'image/format': bytes_feature(image_format),
      'image/class/label': _float_feature(label),
      'image/height': int64_feature(height),
      'image/width': int64_feature(width),
  }))

def _convert_dataset(split_name, filenames, labels, dataset_dir):
  assert split_name in ['train', 'validation']

  num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))

  with tf.Graph().as_default():
        
        for shard_id in range(_NUM_SHARDS):
          output_filename = _get_dataset_filename(dataset_path, split_name, shard_id)
         
          with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:
              start_ndx = shard_id * num_per_shard
              end_ndx = min((shard_id+1) * num_per_shard, len(filenames))
              for i in range(start_ndx, end_ndx):
                  sys.stdout.write('\r>> Converting image %d/%d shard %d' % (
                          i+1, len(filenames), shard_id))
                  sys.stdout.flush()

                  img = load_image(filenames[i])
                  image_data = tf.compat.as_bytes(img.tostring())
                    
                  label = labels[i]
                    
                  example = image_to_tfexample_mine(image_data, image_format, height, width, label)
                    
                  # Serialize to string and write on the file
                  tfrecord_writer.write(example.SerializeToString())
                
  sys.stdout.write('\n')
  sys.stdout.flush()
  
def run(dataset_dir):

    labels = Read_Labels(dataset_dir + '/training_labels.csv')
    
    photo_filenames = _get_filenames_and_classes(dataset_dir + '/images_training')
    
    shuffle_data = True 
    
    photo_filenames, labels = Shuffle_images_with_labels(
            shuffle_data,photo_filenames, labels)
    
    training_filenames = photo_filenames[_NUM_VALIDATION:]
    training_labels = labels[_NUM_VALIDATION:]
    
    validation_filenames = photo_filenames[:_NUM_VALIDATION]
    validation_labels = labels[:_NUM_VALIDATION]
    
    _convert_dataset('train',
                     training_filenames, training_labels, dataset_path)
    _convert_dataset('validation',
                     validation_filenames, validation_labels, dataset_path)
    
    print('\nFinished converting the Flowers dataset!')` 
________________________________________________________________________________
And I decode it by:

`
with tf.Session() as sess:

    feature = {
      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),
      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpeg'),
      'image/class/label': tf.FixedLenFeature(
          [37,], tf.float32, default_value=tf.zeros([37,], dtype=tf.float32)),
       }

    filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)
    
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)
   
    features = tf.parse_single_example(serialized_example, features=feature)
    
    image = tf.decode_raw(features['image/encoded'], tf.float32)
    print(image.get_shape())
    
    label = tf.cast(features['image/class/label'], tf.float32)

    image = tf.reshape(image, [224, 224, 3])

    images, labels = tf.train.shuffle_batch([image, label], batch_size=10, capacity=30, num_threads=1, min_after_dequeue=10)
    
    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
    sess.run(init_op)

    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)
    
    for batch_index in range(6):
        img, lbl = sess.run([images, labels])
        img = img.astype(np.uint8)
        print(img.shape)
        for j in range(6):
            plt.subplot(2, 3, j+1)
            plt.imshow(img[j, ...])
        plt.show()
    
    coord.request_stop()
    
    coord.join(threads)`
______________________________________________________________________________________________________
It's all fine up to this point. But when I use the bellow commands for decoding TFRecord files:

` 
reader = tf.TFRecordReader

 keys_to_features = {
      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),
      'image/format': tf.FixedLenFeature((), tf.string, default_value='raw'),
      'image/class/label': tf.FixedLenFeature(
          [37,], tf.float32, default_value=tf.zeros([37,], dtype=tf.float32)),
     }

  items_to_handlers = {
      'image': slim.tfexample_decoder.Image('image/encoded'),
      'label': slim.tfexample_decoder.Tensor('image/class/label'),
  }
  
decoder = slim.tfexample_decoder.TFExampleDecoder(
      keys_to_features, items_to_handlers)`
_________________________________________________________________________________________________
 I get the following error.

> INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, assertion failed: [Unable to decode bytes as JPEG, PNG, GIF, or BMP]
         [[Node: case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/Assert_1/Assert = Assert[T=[DT_STRING], summarize=3, _device=""/job:localhost/replica:0/task:0/cpu:0""](case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/is_bmp, case/If_0/decode_image/cond_jpeg/cond_png/cond_gif/Assert_1/Assert/data_0)]]
INFO:tensorflow:Caught OutOfRangeError. Stopping Training.
INFO:sensorflow:Finished training! Saving model to disk.
_______________________________________________________________________________________________
To use Densenet for my problem, I should fix this error first. 
Could anybody please help me out of this problem. This code works perfectly for the datasets like flowers, MNIST and CIFAR10 available at [https://github.com/pudae/tensorflow-densenet/tree/master/datasets](url) but does not work for my data."
13627,HDFS user impersonation,"TensorFlow  does support HDFS filesystem but there is no way to specify as which user to access the filesystem.

The native hdfs library provides a function to set the user name, hdfsBuilderSetUserName() similar to hdfsBuilderSetNameNode(). There is also hdfsConnectAsUser() where you can specify the user, builds the hdfsBuilder struct, set the username among other arguments and return the FileSystem handle.

Currently I don't see any way to provide a username in TensorFlow and more specifically in tensorflow/core/platform/hadoop/hadoop_file_system.cc

I suppose that something like the following would be sufficient.
https://github.com/kouzant/tensorflow/commit/eacef5cb81d09d0490403fde33de8e5526f212ad"
13626,[Import Error] Tensorflow is looking for wrong shared library; libnvidia-fatbinaryloader.so.375.88,"### System information
- **OS Platform and Distribution**:  Ubuntu 16.04
- **TensorFlow installed from**: Binary
- **TensorFlow version**: 1.3.0 
- **Python version**: 2.7
- **CUDA/cuDNN version**: 6
- **GPU model and memory**: GTX 840M, 2GB
- **Exact command to reproduce**: import tensorflow as tf

When ever I try to import tensorflow (1.3.0), I encounter a ImportError where the system is unable to locate the shared library libnvidia-fatbinaryloader.so.375.88. I've looked in /usr/lib/nvidia-375 and found that another version of the .so exists (375.66). 
The issue is resolved if I downgrade to tensorflow version 1.2.1"
13625,Something wrong while using C++ Session.Run api. ,"I use tensorflow C++ api to run my model. I flower this reference: [label_image](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc).
In my model, the outputs' shape is 2d,  I run model like this:

` std::vector<Tensor> outputs;`
`status = session->Run(m_feed_dict, {""model/wav_outputs""},{}, &outputs);`
However, I get this:
`
tensorflow/core/framework/tensor_shape.cc:44] Check failed: NDIMS == dims() (1 vs. 2)Asking for tensor of 1 dimensions from a tensor of 2 dimensions
`
Could anybody tell me how to fix this?
@petewarden.  @vclteam  #
"
13624,speech demo is not working on duration different from 1000ms,"hi

when i try to use a different data-set (different duration) on the speech recognition demo. the training works well.but the freeze part crash with the message

**""Assign requires shapes of both tensors to match ""**

tried almost everything:
- delete the temp directory
- pass the same param from train to freeze
- make sure that all wav file are with the same size
- used the nightly docker
- used the latest python

nothing worked!!!!!

my guess is that the problem is in the loading part

any idea what next? 
"
13623,Why the parameters aren't stored on ps server for distributed tensorflow ?,"Please see more detail on [stackoverflow](https://stackoverflow.com/questions/46667790/distributed-tensorflow-of-between-graph-replication).

I don't start `ps` server, and parameters are stored on `worker` server. "
13622,CudnnLSTM returns all Ones(1) after the 10th sequence ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: v1.3.0-rc1-1486-g752dcb6 1.3.0
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: GTX1050Ti
- **Exact command to reproduce**:


### Describe the problem
I tried to use CudnnLSTM to speed up the training, but found it only returns one after the 10th step, following code generate the output.

### Source code / logs
    import tensorflow as tf
    from tensorflow.contrib.cudnn_rnn import CudnnLSTM
    import numpy as np


    np.set_printoptions(linewidth=240, edgeitems=6)
    # Reset default graph
    tf.reset_default_graph()

    num_layer = 5
    num_unit = 256
    input_size = 400
    seq_lenght = 20

    with tf.device('/gpu:0'):
        x = tf.random_uniform([seq_lenght, input_size], maxval=1, dtype=tf.float32)
        x1 = tf.expand_dims(x, 1)
        lstm = CudnnLSTM(num_layers=num_layer, num_units=num_unit, input_size=input_size,
                     input_mode='linear_input',
                     direction='unidirectional')

        # CudnnLSTM parameter
        lstm_para_size = lstm.params_size()
        lstm_para = tf.Variable(tf.random_uniform([lstm_para_size]), validate_shape=False, name='lstm_para')

        state_c = tf.Variable(tf.zeros(shape=[num_layer, 1, num_unit]), trainable=False)
        state_h = tf.Variable(tf.zeros(shape=[num_layer, 1, num_unit]), trainable=False)

        lstm_output, lstm_h, lstm_c = lstm(input_data=x1, input_h=state_h, input_c=state_c, params=lstm_para)

    # Variable initializing op
    init = tf.global_variables_initializer()

    with tf.Session() as sess:
        sess.run(init)
    cudnn_output = sess.run(lstm_output)
    print(cudnn_output)

###LSTM output
[[[ 0.76159418  0.76159418  0.76159418  0.76159418  0.76159418  0.76159418 ...,  0.76159418  0.76159418  0.76159418  0.76159418  0.76159418  0.76159418]]

 [[ 0.96402758  0.96402758  0.96402758  0.96402758  0.96402758  0.96402758 ...,  0.96402758  0.96402758  0.96402758  0.96402758  0.96402758  0.96402758]]

 [[ 0.99505478  0.99505478  0.99505478  0.99505478  0.99505478  0.99505478 ...,  0.99505478  0.99505478  0.99505478  0.99505478  0.99505478  0.99505478]]

 [[ 0.99932933  0.99932933  0.99932933  0.99932933  0.99932933  0.99932933 ...,  0.99932933  0.99932933  0.99932933  0.99932933  0.99932933  0.99932933]]

 [[ 0.99990922  0.99990922  0.99990922  0.99990922  0.99990922  0.99990922 ...,  0.99990922  0.99990922  0.99990922  0.99990922  0.99990922  0.99990922]]

 [[ 0.99998772  0.99998772  0.99998772  0.99998772  0.99998772  0.99998772 ...,  0.99998772  0.99998772  0.99998772  0.99998772  0.99998772  0.99998772]]

 ..., 
 [[ 1.          1.          1.          1.          1.          1.         ...,  1.          1.          1.          1.          1.          1.        ]]

 [[ 1.          1.          1.          1.          1.          1.         ...,  1.          1.          1.          1.          1.          1.        ]]

 [[ 1.          1.          1.          1.          1.          1.         ...,  1.          1.          1.          1.          1.          1.        ]]

 [[ 1.          1.          1.          1.          1.          1.         ...,  1.          1.          1.          1.          1.          1.        ]]

 [[ 1.          1.          1.          1.          1.          1.         ...,  1.          1.          1.          1.          1.          1.        ]]

 [[ 1.          1.          1.          1.          1.          1.         ...,  1.          1.          1.          1.          1.          1.        ]]]"
13621,“None” value for gradient of Tensorflow variables which is used in the network,"I want to develop a custom Seq2Seq in tensorflow and I've created this part of network

`wflat0 = tf.get_variable(""wflat0"", shape=(1024, 1024), 
initializer=LinearInitializer)
bflat0 = tf.get_variable(""bflat0"", shape=1024, initializer=BiasInitializer)

l0flat = selu(tf.matmul(x, wflat0) + bflat0)
x=tf.reshape(l0flat,[shape[0],shape[1],1024])
x = tf.unstack(x, constant.Lstm_cell, 1)
lstm_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(LSTMHiddenSize, forget_bias=1.0, dropout_keep_prob=0.9)
outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32)

decoder = tf.contrib.rnn.LayerNormBasicLSTMCell(LSTMHiddenSize, forget_bias=1.0, dropout_keep_prob=0.9)

y = tf.unstack(tf.reshape(self.y_,[tf.shape(self.y_)[0],tf.shape(self.y_)[1],1]), constant.Lstm_cell, 1)
decoutputs, decstates = tf.contrib.rnn.static_rnn(decoder, y, dtype=tf.float32)

wflat1 = tf.get_variable(""wflat1"", shape=(LSTMHiddenSize, 128),
                                     initializer=LinearInitializer)
bflat1 = tf.get_variable(""bflat1"", shape=128, initializer=BiasInitializer)
shapes = tf.shape(decoutputs)
decoutputs=tf.transpose(decoutputs, perm=[1, 0, 2])
decoutputs=tf.reshape(decoutputs,(shapes[0]*shapes[1],shapes[2]))
self.sss=decoutputs
l1flat = selu(tf.matmul(decoutputs, wflat1) + bflat1)
wflat2 = tf.get_variable(""wflat2"", shape=(128, 1), initializer=LinearInitializer)
bflat2 = tf.get_variable(""bflat2"", shape=1, initializer=BiasInitializer)
l2flat = tf.matmul(l1flat, wflat2) + bflat2
l2flat = tf.reshape(l2flat,(constant.batchsize , constant.Lstm_cell))
self.sigout=tf.nn.softmax(l2flat)
self.out=l2flat
cost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=self.out, labels=self.y_))`

and this network create NAN output after first update iteration for large sequence and it just works for small sequence so I've decided to add gradient clipping and when I was developing that I've found that most of the variables have a none gradient as you can see in this [picture Imag](https://i.stack.imgur.com/XtkY0.png)e of computed Gradient how could it possible that some variables which take participate in loss function have a none gradient is this a bug?"
13620,Erase Operation on tf.contrib.lookup.MutableHashTable,"### System information
- TensorFlow r1.3

### Describe the problem

The `tf.contrib.lookup` package has `MutableHashTable` class which wraps the C++11 std::unordered_map class. However, the erase method of unordered_map is not exposed. 

Is there some existing way to erase keys from the lookup table? 

If not I can create a patch with a new kernel/op on the MutableHashTable classes, and also add a method to the Python API.

Thanks"
13619,"The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, str","When i feed the value, unfortunately , it is a tensor.
i have already known that i should convert it to a array or other types to feed. But if i use sess.run() or .eval(). The speed makes me crazy, for the shape is [3600,14,25500].

if other ways are available?

"
13618,question in image_ops_impl.py,"I am confused about the code line 827 in image_ops_impl.py .It should be 
variance = (math_ops.reduce_mean(math_ops.square(image-image_mean))

or some reason for the equation?
"
13616,"Force tensor evaluation inside while-loop, scan and others.","Hello everyone,

I had big plans for `tf.while_loop` until I discovered that it is impossible to re-evaluate tensor inside it. Let's dive into the the issue and potential useful feature:

```python
In [13]: import tensorflow as tf
    ...: import numpy as np
    ...:
    ...: def cond(i, _x, _sq):
    ...:       return tf.less(i, 10)
    ...:
    ...: def gen_body(v):
    ...:     def body(i, x, sq):
    ...:         x = tf.Print(x, [x, sq], ""x and sq: "")
    ...:         v_assign = v.assign(x + 1)
    ...:         v_assign = tf.Print(v_assign, [v_assign], ""v_assign: "")
    ...:         with tf.control_dependencies([v_assign]):
    ...:             sq_neg = tf.negative(sq)
    ...:         sq_neg = tf.Print(sq_neg, [i, sq_neg], message='i and sq_neg:')
    ...:         return tf.add(i, 1), sq_neg, sq
    ...:     return body
    ...:

In [14]: sess = tf.InteractiveSession()

In [15]: i = tf.Variable(0)
    ...: v = tf.Variable(2)
    ...: sq = tf.square(v)
    ...: l = tf.while_loop(cond, gen_body(v), (i, v, sq))
    ...: sess.run(tf.global_variables_initializer())
    ...: sess.run(l)
    ...:
2017-10-10 22:59:44.819271: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [2][4]
2017-10-10 22:59:44.819405: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [3]
2017-10-10 22:59:44.819466: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[0][-4]
2017-10-10 22:59:44.819553: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]
2017-10-10 22:59:44.819615: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]
2017-10-10 22:59:44.819680: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[1][-4]
2017-10-10 22:59:44.819827: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]
2017-10-10 22:59:44.819885: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]
2017-10-10 22:59:44.819932: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[2][-4]
2017-10-10 22:59:44.820034: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]
2017-10-10 22:59:44.820094: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]
2017-10-10 22:59:44.820111: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[3][-4]
2017-10-10 22:59:44.820162: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]
2017-10-10 22:59:44.820250: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]
2017-10-10 22:59:44.820265: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[4][-4]
2017-10-10 22:59:44.820315: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]
2017-10-10 22:59:44.820379: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]
2017-10-10 22:59:44.820408: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[5][-4]
2017-10-10 22:59:44.820428: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]
2017-10-10 22:59:44.820438: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]
2017-10-10 22:59:44.820446: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[6][-4]
2017-10-10 22:59:44.820464: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]
2017-10-10 22:59:44.820490: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]
2017-10-10 22:59:44.820500: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[7][-4]
2017-10-10 22:59:44.820519: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]
2017-10-10 22:59:44.820532: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]
2017-10-10 22:59:44.820542: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[8][-4]
2017-10-10 22:59:44.820559: I tensorflow/core/kernels/logging_ops.cc:79] x and sq: [-4][4]
2017-10-10 22:59:44.820580: I tensorflow/core/kernels/logging_ops.cc:79] v_assign: [-3]
2017-10-10 22:59:44.820593: I tensorflow/core/kernels/logging_ops.cc:79] i and sq_neg:[9][-4]
Out[15]: (10, -4, 4)
```

I created `v` variable and tensor `sq` which equals to `v^2`. In fact, we don't have control over them, they are our input as `x` and `y` and we know that `y` depends on `x`. I would like to assign new value inside TensorFlow loop to `x` (equavalent to `v` at example) and evaluate fresh `y` (`sq` inside example) at each iteration of the loop. Meanwhile we can do other evaluations inside `while_loop`, but most important is that I need to update `x` and get updated `y`. Currently, assigning operation doesn't propagate updates down to the dependant nodes and it shouldn't, but when someone calls tensor depending on value which were updated via assign inside `while_loop`, I suppose the tensor node must detect this change and evaluate new tensor value again.

Thanks!"
13615,Importing tensorflow after import pytorch crashes inside tensorflow::port::TestCPUFeature,"TF version:  https://github.com/tensorflow/tensorflow/commit/07bf1d3
PyTorch version: '0.2.0_4' (whatever is installed by default yesterday)

Crashing code
```
import torch
import tensorflow
```
Work-around
```
import tensorflow
import torch
```

Stacktrace

```
#0  0x00007f2d2f5e7577 in void std::__once_call_impl<std::_Bind_simple<void (*())()> >() ()
   from /home/yaroslav/anaconda3/envs/oct12/lib/python3.5/site-packages/torch/lib/libTHC.so.1
#1  0x00007f2d546d5a99 in __pthread_once_slow (
    once_control=0x7f2d0a470830 <tensorflow::port::(anonymous namespace)::cpuid_once_flag>, init_routine=0x7f2d2789e2a0 <__once_proxy>) at pthread_once.c:116
#2  0x00007f2d09ae7faa in void std::call_once<void (&)()>(std::once_flag&, void (&)()) ()
   from /home/yaroslav/anaconda3/envs/oct12/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007f2d09ae7fee in tensorflow::port::TestCPUFeature(tensorflow::port::CPUFeature) ()
   from /home/yaroslav/anaconda3/envs/oct12/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
#4  0x00007f2d09942895 in _GLOBAL__sub_I_cpu_feature_guard.cc ()
   from /home/yaroslav/anaconda3/envs/oct12/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
#5  0x00007f2d54de86ba in call_init (l=<optimized out>, argc=argc@entry=2, 
    argv=argv@entry=0x7ffe69b84308, env=env@entry=0x1ba0d00) at dl-init.c:72
#6  0x00007f2d54de87cb in call_init (env=0x1ba0d00, argv=0x7ffe69b84308, 
    argc=2, l=<optimized out>) at dl-init.c:30
#7  _dl_init (main_map=main_map@entry=0x284e6d0, argc=2, argv=0x7ffe69b84308, 
    env=0x1ba0d00) at dl-init.c:120
#8  0x00007f2d54ded8e2 in dl_open_worker (a=a@entry=0x7ffe69b7e310)
    at dl-open.c:575
#9  0x00007f2d54de8564 in _dl_catch_error (
    objname=objname@entry=0x7ffe69b7e300, 
    errstring=errstring@entry=0x7ffe69b7e308, 
    mallocedp=mallocedp@entry=0x7ffe69b7e2ff, 
    operate=operate@entry=0x7f2d54ded4d0 <dl_open_worker>, 
    args=args@entry=0x7ffe69b7e310) at dl-error.c:187
#10 0x00007f2d54decda9 in _dl_open (
    file=0x7f2d1ce72cc8 ""/home/yaroslav/anaconda3/envs/oct12/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so"", 
    mode=-2147483646, 
    caller_dlopen=0x7f2d54a68553 <_PyImport_FindSharedFuncptr+115>, nsid=-2, 
    argc=<optimized out>, argv=<optimized out>, env=0x1ba0d00)
    at dl-open.c:660
#11 0x00007f2d544c3f09 in dlopen_doit (a=a@entry=0x7ffe69b7e540)
    at dlopen.c:66
#12 0x00007f2d54de8564 in _dl_catch_error (objname=0x1b20de0, 
    errstring=0x1b20de8, mallocedp=0x1b20dd8, 
    operate=0x7f2d544c3eb0 <dlopen_doit>, args=0x7ffe69b7e540)
    at dl-error.c:187
#13 0x00007f2d544c4571 in _dlerror_run (
    operate=operate@entry=0x7f2d544c3eb0 <dlopen_doit>, 
    args=args@entry=0x7ffe69b7e540) at dlerror.c:163
#14 0x00007f2d544c3fa1 in __dlopen (file=<optimized out>, 
    mode=<optimized out>) at dlopen.c:87
#15 0x00007f2d54a68553 in _PyImport_FindSharedFuncptr (
    prefix=0x7f2d54aeceda ""PyInit"", 
    shortname=0x7f2d1c9d04d0 ""_pywrap_tensorflow_internal"", 
---Type <return> to continue, or q <return> to quit--- 
    pathname=0x7f2d1ce72cc8 ""/home/yaroslav/anaconda3/envs/oct12/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so"", fp=0x0)
    at ./Python/dynload_shlib.c:95
#16 0x00007f2d54a43ce7 in _PyImport_LoadDynamicModuleWithSpec (
    spec=0x7f2d1c9b3b00, fp=0x0) at ./Python/importdl.c:124
#17 0x00007f2d54a40aef in _imp_create_dynamic_impl (file=<optimized out>, 
    spec=0x7f2d1c9b3b00, module=<optimized out>) at Python/import.c:2031
#18 _imp_create_dynamic (module=<optimized out>, args=<optimized out>)
    at Python/clinic/import.c.h:282
#19 0x00007f2d549a1209 in PyCFunction_Call (func=0x7f2d54f77ee8, 
    args=0x7f2d1c9b3c88, kwds=<optimized out>) at Objects/methodobject.c:109
#20 0x00007f2d54a274fa in ext_do_call (nk=479935624, na=0, 
    flags=<optimized out>, pp_stack=0x7ffe69b7ea48, func=0x7f2d54f77ee8)
    at Python/ceval.c:5084
#21 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>)
    at Python/ceval.c:3328
#22 0x00007f2d54a2aa49 in _PyEval_EvalCodeWithName (_co=<optimized out>, 
    globals=<optimized out>, locals=<optimized out>, args=<optimized out>, 
    argcount=2, kws=0x7f2d53040760, kwcount=0, defs=0x0, defcount=0, 
    kwdefs=0x0, closure=0x0, name=0x7f2d54f657b0, qualname=0x7f2d54f657b0)
    at Python/ceval.c:4071
#23 0x00007f2d54a2894c in fast_function (nk=<optimized out>, na=2, 
    n=<optimized out>, pp_stack=0x7ffe69b7ec68, func=0x7f2d54f830d0)
    at Python/ceval.c:4866
#24 call_function (oparg=<optimized out>, pp_stack=0x7ffe69b7ec68)
    at Python/ceval.c:4783
#25 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>)
    at Python/ceval.c:3289
#26 0x00007f2d54a28ccc in fast_function (nk=<optimized out>, na=2, 
    n=<optimized out>, pp_stack=0x7ffe69b7ede8, func=0x7f2d54f2a7b8)
    at Python/ceval.c:4856
#27 call_function (oparg=<optimized out>, pp_stack=0x7ffe69b7ede8)
    at Python/ceval.c:4783
#28 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>)
    at Python/ceval.c:3289
#29 0x00007f2d54a28ccc in fast_function (nk=<optimized out>, na=1, 
    n=<optimized out>, pp_stack=0x7ffe69b7ef68, func=0x7f2d54f83b70)
    at Python/ceval.c:4856
#30 call_function (oparg=<optimized out>, pp_stack=0x7ffe69b7ef68)
    at Python/ceval.c:4783
#31 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>)
    at Python/ceval.c:3289
#32 0x00007f2d54a28ccc in fast_function (nk=<optimized out>, na=1, 
    n=<optimized out>, pp_stack=0x7ffe69b7f0e8, func=0x7f2d54f83d90)
    at Python/ceval.c:4856
#33 call_function (oparg=<optimized out>, pp_stack=0x7ffe69b7f0e8)
    at Python/ceval.c:4783
#34 PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>)
    at Python/ceval.c:3289
#35 0x00007f2d54a28ccc in fast_function (nk=<optimized out>, na=1, 
    n=<optimized out>, pp_stack=0x7ffe69b7f268, func=0x7f2d54f83e18)
    at Python/ceval.c:4856
#36 call_function (oparg=<optimized out>, pp_stack=0x7ffe69b7f268)
```"
13614,SSD mobilenet trained model with custom data only recognize images in short distances," System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
     No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
     Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
     binary
- **TensorFlow version (use command below)**:
     'v1.2.0-rc2-21-g12f033d', '1.2.0'
- **Python version**: 
     2.7
- **Bazel version (if compiling from source)**:
     na
- **CUDA/cuDNN version**:
     CUDA Version 8.0.61
     
- **GPU model and memory**:
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:01:00.0
Total memory: 10.91GiB
Free memory: 10.75GiB

- **Exact command to reproduce**:


### Describe the problem

I've trained a model with a custom dataset (Garfield images) with Tensorflow Object Detection API (ssd_mobilenet_v1 model) and referring it in the android sample application available on Tensorflow repository. The application can only detected the images in distances less or equal 20cm approximately.

Do you have any clue about I can improve the model to perform recognitions in longer distances (about 30cm or more) ?

I don't know with this limitation is related with input size I'm using (tested with images with 300x300 and 68x68) or any custom data augmentation is needed to improve that.


### Source code / logs
# SSD with Mobilenet v1, configured for Oxford-IIIT Pets Dataset.
# Users should configure the fine_tune_checkpoint field in the train config as
# well as the label_map_path and input_path fields in the train_input_reader and
# eval_input_reader. Search for ""PATH_TO_BE_CONFIGURED"" to find the fields that
# should be configured.

model {
  ssd {
    num_classes: 1
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
      }
    }
    image_resizer {
      fixed_shape_resizer {
        height: 68
        width: 68
      }
    }
    box_predictor {
      convolutional_box_predictor {
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.8
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false
        conv_hyperparams {
          activation: RELU_6,
          regularizer {
            l2_regularizer {
              weight: 0.00004
            }
          }
          initializer {
            truncated_normal_initializer {
              stddev: 0.03
              mean: 0.0
            }
          }
          batch_norm {
            train: true,
            scale: true,
            center: true,
            decay: 0.9997,
            epsilon: 0.001,
          }
        }
      }
    }
    feature_extractor {
      type: 'ssd_mobilenet_v1'
      min_depth: 16
      depth_multiplier: 1.0
      conv_hyperparams {
        activation: RELU_6,
        regularizer {
          l2_regularizer {
            weight: 0.00004
          }
        }
        initializer {
          truncated_normal_initializer {
            stddev: 0.03
            mean: 0.0
          }
        }
        batch_norm {
          train: true,
          scale: true,
          center: true,
          decay: 0.9997,
          epsilon: 0.001,
        }
      }
    }
    loss {
      classification_loss {
        weighted_sigmoid {
          anchorwise_output: true
        }
      }
      localization_loss {
        weighted_smooth_l1 {
          anchorwise_output: true
        }
      }
      hard_example_miner {
        num_hard_examples: 3000
        iou_threshold: 0.99
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 0
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    normalize_loss_by_num_matches: true
    post_processing {
      batch_non_max_suppression {
        score_threshold: 1e-8
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
  }
}	
train_config: {
  batch_size: 24
  optimizer {
    rms_prop_optimizer: {
      learning_rate: {
        exponential_decay_learning_rate {
          initial_learning_rate: 0.004
          decay_steps: 800720
          decay_factor: 0.95
        }
      }
      momentum_optimizer_value: 0.9
      decay: 0.9
      epsilon: 1.0
    }
  }
  #fine_tune_checkpoint: ""/home/oliveira/tf_oda/checkpoints/ssd_mobilenet_v1_coco_11_06_2017/model.ckpt""
  from_detection_checkpoint: true
  # Note: The below line limits the training process to 200K steps, which we
  # empirically found to be sufficient enough to train the pets dataset. This
  # effectively bypasses the learning rate schedule (the learning rate will
  # never decay). Remove the below line to train indefinitely.
  num_steps: 200000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
}

train_input_reader: {
  tf_record_input_reader {
    input_path: ""/data/tf_oda/garfield/dataset/pascal_train_garfield_68.record""
  }
  label_map_path: ""/data/tf_oda/garfield/data/pascal_label_map_garfield.pbtxt""
}

eval_config: {
  num_examples: 2000
  # Note: The below line limits the evaluation process to 10 evaluations.
  # Remove the below line to evaluate indefinitely.
  max_evals: 10
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""/data/tf_oda/garfield/dataset/pascal_val_garfield_68.record""
  }
  label_map_path: ""/data/tf_oda/garfield/data/pascal_label_map_garfield.pbtxt""
  shuffle: false
  num_readers: 1
}"
13610,Can we run Dataset API on GPU?,"I am running binary TF 1.3.0 on Ubuntu 16.04.

Python Version: 3.5.3

I have Nvidia TITAN Xp installed.

I use Dataset API to build input pipeline. What I want is to extract image features using CNN layers 
in the input pipeline. The code looks like this:

```
    def extract_feats(image):
      with tf.device(""/gpu:0""):
        _, end_points = vgg.vgg_16(tf.expand_dims(image, 0),
                                   is_training=(mode == ModeKeys.TRAIN),
                                   spatial_squeeze=False)
        final_conv_layer = end_points['vgg_16/conv5/conv5_3']
        feats = spatial_pyramid_pooling(final_conv_layer, [bin_size], mode='avg')
      return tf.reshape(feats, shape=(bin_size * bin_size, tf.shape(final_conv_layer)[-1]))

    features = features.map(extract_feats)
```

When running the code, my CPU usage is more than 1000% (I have an 6 cores/12 threads CPU), while the GPU usage is 0%. I suspect that the input pipeline built from Dataset API are forced to run on CPU. I tried to set `log_device_placement=True` and I can see that the operation is placed on GPU.

Since I want to extract vectors with same length from variable-sized images using SPP pooling, I have to process these images one by one using `Dataset.map` before calling `Dataset.batch`. So I hope the operations inside `Dataset` could be run on GPU.
"
13607,Building custom op instructions out of date,"Following instructions here
https://www.tensorflow.org/extend/adding_an_op

To try to rebuild this [op](https://github.com/yaroslavvb/max_align_bytes_op)

First I ran into issue with nsync headers, fixed by following
https://github.com/tensorflow/tensorflow/issues/12482#issuecomment-328829250

Then while trying to load the `.so` file I run into 
tensorflow.python.framework.errors_impl.NotFoundError: ./max_align_bytes_op.so: undefined symbol: _ZTIN10tensorflow8OpKernelE

So the definition for `tensorflow::OpKernel` is missing

tf commit: https://github.com/tensorflow/tensorflow/commit/22a886b
cc @allenlavoie "
13604,Keras + tfdbg error: Dump root directory does not exist,"# Problem
I'm running the tf debugger and specifically am looking for nans and infs.
I'm doing this from keras using the tf backend by setting the keras sess to tf.Session wrapped with the debugger. 

## MVCE
```
    import tensorflow as tf
    from keras import Input
    from keras import backend as keras_backend

    inputs = Input((1,1))
    nan_ = keras_backend.log(inputs * 0)
    model = Model(inputs,nan_)
    model.compile(loss='mse',optimizer='sgd')
   
    from tensorflow.python import debug as tf_debug
    sess = tf_debug.LocalCLIDebugWrapperSession(tf.Session())
    #Add filter for nans and infs
    sess.add_tensor_filter(""has_inf_or_nan"", tf_debug.has_inf_or_nan)
    keras_backend.set_session(sess)

    import numpy as np
    model.fit(np.zeros([1,1,1]), np.zeros([1,1,1])) 
```
Inside the debugger execute `run -f has_inf_or_nan`.

## Traceback 

OSError: Dump root directory /tmp/tfdbg_dm3xvee9 does not exist. 

## Additional info

#7615 references the same error a while ago, and gives similar traceback.

## System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
No.

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
Linux Fedora 4.11.5-200.fc25.x86_64

TensorFlow installed from (source or binary): 
binary

TensorFlow version (use command below): 
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)

Python version:
3.6

CUDA/cuDNN version:
Cuda 8.0, cuDNN v6

GPU model and memory:
Quadro M2000M, 4042MiB


"
13603,SVD on GPU is slower than SVD on CPU,"OS:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS release 7.4.1708
- **TensorFlow installed from (source or binary)**: From source
- **Python version**: 2.7.13
- **Bazel version**: 0.6.1
- **CUDA/cuDNN version**: CUDA 8.0/cuDNN 6.0.21
- **GPU model and memory**: GeForce GTX 950M, memory 4GB

output of `tf_env_collect.sh`
```

== cat /etc/issue ===============================================
Linux zhanghao 3.10.0-693.2.2.el7.x86_64 #1 SMP Tue Sep 12 22:26:13 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7 (Core)""
VERSION_ID=""7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)
Copyright © 2015 Free Software Foundation, Inc.
本程序是自由软件；请参看源代码的版权声明。本软件没有任何担保；
包括没有适销性和某一专用目的下的适用性担保。

== uname -a =====================================================
Linux zhanghao 3.10.0-693.2.2.el7.x86_64 #1 SMP Tue Sep 12 22:26:13 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named tensorflow

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue Oct 10 16:36:08 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 950M    Off  | 00000000:0A:00.0 Off |                  N/A |
| N/A   45C    P0    N/A /  N/A |      0MiB /  4044MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================

== cat /etc/issue ===============================================
Linux zhanghao 3.10.0-693.2.2.el7.x86_64 #1 SMP Tue Sep 12 22:26:13 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7 (Core)""
VERSION_ID=""7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)
Copyright © 2015 Free Software Foundation, Inc.
本程序是自由软件；请参看源代码的版权声明。本软件没有任何担保；
包括没有适销性和某一专用目的下的适用性担保。

== uname -a =====================================================
Linux zhanghao 3.10.0-693.2.2.el7.x86_64 #1 SMP Tue Sep 12 22:26:13 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.12.1)
protobuf (3.4.0)
tensorflow (1.4.0rc0)
tensorflow-tensorboard (0.4.0rc1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.4.0-rc0
tf.GIT_VERSION = v1.3.0-rc1-3111-g4196d6d
tf.COMPILER_VERSION = v1.3.0-rc1-3111-g4196d6d
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/lib64/:/usr/local/cuda/lib64/stubs/:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/cuda/nvvm/lib64/:/usr/lib64/nvidia/:/opt/intel/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64:/opt/intel/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2017.4.196/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2017.4.196/linux/mpi/mic/lib:/opt/intel/compilers_and_libraries_2017.4.196/linux/ipp/lib/intel64:/opt/intel/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2017.4.196/linux/mkl/lib/intel64_lin:/opt/intel/compilers_and_libraries_2017.4.196/linux/tbb/lib/intel64/gcc4.7:/opt/intel/debugger_2017/iga/lib:/opt/intel/debugger_2017/libipt/intel64/lib:/opt/intel/compilers_and_libraries_2017.4.196/linux/daal/lib/intel64_lin:/opt/intel/compilers_and_libraries_2017.4.196/linux/daal/../tbb/lib/intel64_lin/gcc4.4
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue Oct 10 16:36:37 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 950M    Off  | 00000000:0A:00.0 Off |                  N/A |
| N/A   45C    P0    N/A /  N/A |      0MiB /  4044MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
```

output of `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
```
('v1.3.0-rc1-3111-g4196d6d', '1.4.0-rc0')
```

### Describe the problem

SVD on GPU is slower than SVD on CPU

### Source code / logs

file main.py
```
import tensorflow as tf
import numpy as np
import sys

D = 1024
dA = np.random.normal(size=(D,D))

dev = ""/gpu:0"" if len(sys.argv)==1 else ""/cpu:0""

with tf.device(dev):
    A = tf.placeholder(shape=(D,D),dtype=tf.float32)
    S, U, V = tf.svd(A)

config = tf.ConfigProto()
config.log_device_placement = True
config.graph_options.optimizer_options.global_jit_level=tf.OptimizerOptions.ON_1
sess = tf.Session(config=config)

for _ in xrange(10):
    dS, dU, dV = sess.run((S, U, V), feed_dict={A:dA})
```

## run on GPU
`time python main.py`
```
2017-10-10 16:28:49.047703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-10-10 16:28:49.048176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:
name: GeForce GTX 950M major: 5 minor: 0 memoryClockRate(GHz): 1.124
pciBusID: 0000:0a:00.0
totalMemory: 3.95GiB freeMemory: 3.91GiB
2017-10-10 16:28:49.048205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0)
Device mapping:
/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0
2017-10-10 16:28:49.064960: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:
/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0

Svd: (Svd): /job:localhost/replica:0/task:0/device:GPU:0
2017-10-10 16:28:49.067234: I tensorflow/core/common_runtime/placer.cc:874] Svd: (Svd)/job:localhost/replica:0/task:0/device:GPU:0
Placeholder: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0
2017-10-10 16:28:49.067302: I tensorflow/core/common_runtime/placer.cc:874] Placeholder: (Placeholder)/job:localhost/replica:0/task:0/device:GPU:0
2017-10-10 16:28:49.074053: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x488e860
python main.py  27.50s user 2.30s system 100% cpu 29.658 total
```

## run on CPU
`time python main.py -`
```
2017-10-10 16:29:53.252138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-10-10 16:29:53.252572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:
name: GeForce GTX 950M major: 5 minor: 0 memoryClockRate(GHz): 1.124
pciBusID: 0000:0a:00.0
totalMemory: 3.95GiB freeMemory: 3.91GiB
2017-10-10 16:29:53.252600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0)
Device mapping:
/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0
2017-10-10 16:29:53.269242: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:
/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0

Svd: (Svd): /job:localhost/replica:0/task:0/device:CPU:0
2017-10-10 16:29:53.271505: I tensorflow/core/common_runtime/placer.cc:874] Svd: (Svd)/job:localhost/replica:0/task:0/device:CPU:0
Placeholder: (Placeholder): /job:localhost/replica:0/task:0/device:CPU:0
2017-10-10 16:29:53.271544: I tensorflow/core/common_runtime/placer.cc:874] Placeholder: (Placeholder)/job:localhost/replica:0/task:0/device:CPU:0
python main.py -  34.33s user 10.68s system 621% cpu 7.241 total
```"
13601,java API had no bool tensorflow，how to add it to the session in java,"i am  doing transer the facenet to android,the input is the img,and the phase_train is a bool data,but the java api had no bool to be feed to session,as i had down this:

```
    private String inputName;
    private String phaseName;
    private String outputName;
    private int inputSize;

    private int[] intValues;
    private int[] valuedata;
    private float[] outputs;

 bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());
        for(int i = 0; i <intValues.length(); i++ )
        {
            final int val = intValues[i];
            valuedata[i * 3 + 0] = ((val >> 16) & 0xFF);
            valuedata[i * 3 + 1] = ((val >> 8) & 0xFF);
            valuedata[i * 3 + 2] = (val & 0xFF);
        }
inferenceInterface.fillNodeInt(
                inputName, new int[]{1, inputSize, inputSize, 3}, valuedata);
```
and then the c++ write :
m_phase_tensor = tensorflow::Tensor(tensorflow::DT_BOOL, tensorflow::TensorShape());
m_phase_tensor.scalar<bool>()() = false;
how to writen in java,can somebody help me!
"
13600,LayerNormBasicLSTMCell causes `bias key not found in checkpoint` when layer_norm=False,"When initializing `LayerNormBasicLSTMCell`, it has a parameter `layer_norm` which controls whether we want to enable layer norm or not. I assume `layer_norm=True` should be set during training and `layer_norm=False` for evaluation. However, if I use this in an Estimator, due to the following line

https://github.com/tensorflow/tensorflow/blob/a2d9b3bf5f9e96bf459074d079b01e1c74b25afa/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L1331

it will not initialize the `bias` term because `layer_norm=True`, resulting in a `NotFoundError` when loading the saved checkpoint with `layer_norm=False`.

What should be the expected behavior of this? Should this cell applies the bias anyway regardless the `layer_norm`? If we do not use the bias during training, I see no points to use it during inference.

"
13597, module 'tensorflow.contrib.tfprof' has no attribute 'ProfileContext' ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: tensorflow-gpu (1.3.0)
- **Python version**:  3.5
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: not relevant
- **GPU model and memory**: not relevant
- **Exact command to reproduce**: not relevant

### Describe the problem
Production machine not hooked up to Internet. Used pip3 to install [tensorflow_gpu-1.3.0-cp35-cp35m-win_amd64.whl](https://pypi.python.org/pypi/tensorflow-gpu)  It looks the profile code may be missing from the wheel.

When run tensorflow with the following code:
```
import tensorflow as tf
with tf.contrib.tfprof.ProfileContext(FLAGS.log_dir + '\\test', trace_steps=[], dump_steps=[]) as pctx:
```
get error:
`AttributeError: module 'tensorflow.contrib.tfprof' has no attribute 'ProfileContext'`
```
Directory of C:\Python35\Lib\site-packages\tensorflow\contrib\tfprof
08/25/2017  02:55 PM             4,132 model_analyzer.py
08/25/2017  02:55 PM             1,301 tfprof_logger.py
08/25/2017  02:55 PM             1,030 __init__.py
09/11/2017  10:54 AM    <DIR>          __pycache__
```
### Source code / logs
It appears that the examples in the https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler Readme.md  may be obsolete.
"
13595,cifar-10-multi-gpu-train code isn't doing synchronization correctly?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12.6
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3
- **Python version**: 3.5.2
- **CUDA/cuDNN version**:  irrelevant
- **GPU model and memory**: irrelevant
- **Exact command to reproduce**: irrelevant

### Describe the problem
Recently I am trying to implement some new synchronization models, and during the research process I came across the cifar-10-multi-gpu-train code. It seems that on this line: 
https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py#L196
The synchronization point only collects gradients from each worker and do averaging. It doesn't provide a **synchronization barrier** as tf.train.SyncReplicaOptimizer does. In this way there may be some stale gradient exists. E.g on worker_1 it computes to local_step = 104, while on worker_2 it only computes to local_step = 91
Please correct me if I am wrong here.
"
13594,How to display Runtime Statistics in Tensorboard using Estimator API in a distributed environment,"Hello,

I am running in the the same issue than described in this stack overflow question: https://stackoverflow.com/questions/45719176/how-to-display-runtime-statistics-in-tensorboard-using-estimator-api-in-a-distri.. 
I know that GitHub is used for features requests and bugs but this question didn't get an answer and I am not the only one running in the problem.

 This is how the doc illustrates how to add  and save Runtime statistics:
```
run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
run_metadata = tf.RunMetadata()
summary, _ = sess.run([merged, train_step],
                                       feed_dict=feed_dict(True),
                                       options=run_options,
                                       run_metadata=run_metadata)
train_writer.add_run_metadata(run_metadata, 'step%d' % i)
 train_writer.add_summary(summary, i)
```

Given that there is no evident way to call `sess.run` in the training phase with the Estimator API, I am genuinely wondering how to write this kind of summary... is there a workaround?

I was thinking about using a `SessionRunHook` to create something to pass to the `EstimatorSpec` but I am really not familiar with that.."
13591,GPU Allocation and Results Unexpected for Simple Test codes,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: 
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/ 6.0
- **GPU model and memory**:two of Nvidia Quadro M4000
- **Exact command to reproduce**: 

### Describe the problem
I have two Nvidia Quadro M4000 GPUs; each has a 8G Memory. My regular memory has 64G space. 

I tested a simple GPU memory allocation for tensorflow and found that the allocation seems larger than the GPU memory. My test codes are as below:

    GPU_MEMORY_BYTES = 8 * 2**30 # Assuming your GPU has 8GB of memory, adjust accordingly

    # Number of float32 elements (4 bytes) that consume 7/8 of GPU memory
    NUM_ELEMS = int((7 * GPU_MEMORY_BYTES / 8) / 4)

	def Test1():
	  with tf.device(""/gpu:0""):
		t = tf.ones([2, NUM_ELEMS])
	  s = tf.reduce_sum(t)
	  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0'),
							  allow_soft_placement=False)
	  with tf.Session(config=config) as sess:
		print(sess.run(s)) # This should fail since it consumes more memory than exists in the GPU

	def Test2():
	  with tf.device(""/gpu:0""):
		t0 = tf.ones([NUM_ELEMS])  # Tensor that consumes 80% of GPU0's memory
		s0 = tf.reduce_sum(t0)
	  with tf.device(""/gpu:1""):
		t1 = tf.ones([NUM_ELEMS])  # Tensor that consumes 80% of GPU1's memory
		s1 = tf.reduce_sum(t1)
	  s = tf.add(s0, s1)
	  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0,1'),
							  allow_soft_placement=False)
	  with tf.Session(config=config) as sess:
		print(sess.run(s))

I expect the Test2() should work well but Test1() should fail as one GPU only has 8G memory. However, both test functions succeeded but the results are incorrect!

NUM_ELEMS is about 1.879*e+9, and I expected that it should be half of the output of Test2() (Test1() should fail). 

However, I got Test1's output as 1.07374e+09, and Test2's output as 2.14748e+09.

It seems that Both Test1 and Test2 clip the number of elements to 2^30! As NUM_ELEMS is int32, even clipping should be clipped to 2^31 but not 2^30. 

In Test1(), if I change the first dimension in (tf.ones([2, NUM_ELEMS])) from 2 to 15, the outputs are always 1.07374e+09; if the first dimension is no less than 16, it starts to crash showing out of GPU memory.

My questions are:

 1. Why the GPU allocation did not crash for larger than 8G for single GPU?

 2. Why the output results are clipped to 2^30 for single GPU and 2^31 for two GPUs

 3. How could I get the correct outputs? "
13590,Tensorflow Session Connects to Multiple Targets,"I posted a question on [SO](https://stackoverflow.com/questions/46626879/can-one-tensorflow-session-connect-to-two-targets-at-the-same-time), where I wonder if a Tensorflow Session could connect to two (or more than two) targets at the same time. It looks the feature is not supported yet. Can we make a feature request on it?"
13589,tensorflow batch norm used when rank = 4?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: pip install 
- **TensorFlow version (use command below)**: 1.2
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: Titan X Pascal
- **Exact command to reproduce**: See code snippet

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I am using slim.batch_norm from layers and trying to understand the code flow in my use case. It looks to me like the logic that decides whether to use _fused_batch_norm() or the base class will only use the _fused_batch_norm() in my case if the input rank is 2. The code description sounds like it should also be used if rank is 4 and the function itself (_fused_batch_norm()) supports rank of 4, but the logic seems to prevent calling it. 

If my input is rank 4, it looks like the code will use the fused implementation in normalization_layers.BatchNormalization Is my understanding of the logic correct?

Is this the expected and proper behavior? I am wondering if the the condition rank==2 should actually be rank in [2,4]? If the latter is correct, then this would be a potential bug. If the original is correct, then why have rank in [2,4] for determining feature_supported ?

Issue posted to stack overflow and cited as a bug to report: [which tensorflow batch norm..](https://stackoverflow.com/questions/44809342/which-tensorflow-batch-norm-code-is-used-when-input-rank-is-4/46620919#46620919)

### Source code / logs
```python
  # Only use _fused_batch_norm (1) if fused is set True or if it is
  # possible to use (currently it doesn't support batch weights,
  # renorm, and the case when rank is neither 2 nor 4),
  # and (2) if used with zero_debias_moving_mean, or an input shape of rank 2,
  # or non-default updates_collections (not implemented in
  # normalization_layers.BatchNormalization yet); otherwise use the fused
  # implementation in normalization_layers.BatchNormalization.
  inputs = ops.convert_to_tensor(inputs)
  rank = inputs.get_shape().ndims
  feature_supported = batch_weights is None and not renorm and rank in [2, 4]
  possible_to_fuse = fused is None and feature_supported
  if (fused or possible_to_fuse) and (
      zero_debias_moving_mean or rank == 2 or
      updates_collections is not ops.GraphKeys.UPDATE_OPS):
      return _fused_batch_norm(...)
```"
13588,Unable to open table file. Data loss: file is too short to be an sstable,"I am trying simple save and restore operation in tensorflow. Here is link to Jupyter Notebook.[https://github.com/BrazilForever11/tf_error/blob/master/reproducible%20tf%20saving%20restoring%20error.ipynb](url)



The following line generates error:
`new_all_saver.restore(sess, data_path)`

Here is error message:

```
INFO:tensorflow:Restoring parameters from C:\tmp\tmp\.data-00000-of-00001
---------------------------------------------------------------------------
DataLossError                             Traceback (most recent call last)
~\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1326     try:
-> 1327       return fn(*args)
   1328     except errors.OpError as e:

~\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1305                                    feed_dict, fetch_list, target_list,
-> 1306                                    status, run_metadata)
   1307 

~\AppData\Local\conda\conda\envs\tensorflow\lib\contextlib.py in __exit__(self, type, value, traceback)
     65             try:
---> 66                 next(self.gen)
     67             except StopIteration:

~\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\errors_impl.py in raise_exception_on_not_ok_status()
    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--> 466           pywrap_tensorflow.TF_GetCode(status))
    467   finally:

DataLossError: Unable to open table file C:\tmp\tmp\.data-00000-of-00001: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]

During handling of the above exception, another exception occurred:

DataLossError                             Traceback (most recent call last)
<ipython-input-2-4f763d68be26> in <module>()
      6 
      7 sess=tf.Session()
----> 8 new_all_saver.restore(sess, data_path)

~\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\training\saver.py in restore(self, sess, save_path)
   1558     logging.info(""Restoring parameters from %s"", save_path)
   1559     sess.run(self.saver_def.restore_op_name,
-> 1560              {self.saver_def.filename_tensor_name: save_path})
   1561 
   1562   @staticmethod

~\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    893     try:
    894       result = self._run(None, fetches, feed_dict, options_ptr,
--> 895                          run_metadata_ptr)
    896       if run_metadata:
    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1122     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1123       results = self._do_run(handle, final_targets, final_fetches,
-> 1124                              feed_dict_tensor, options, run_metadata)
   1125     else:
   1126       results = []

~\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1319     if handle is None:
   1320       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1321                            options, run_metadata)
   1322     else:
   1323       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

~\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1338         except KeyError:
   1339           pass
-> 1340       raise type(e)(node_def, op, message)
   1341 
   1342   def _extend_graph(self):

DataLossError: Unable to open table file C:\tmp\tmp\.data-00000-of-00001: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]

Caused by op 'save/RestoreV2_1', defined at:
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\traitlets\config\application.py"", line 658, in launch_instance
    app.start()
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\ipykernel\kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\zmq\eventloop\ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tornado\ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\zmq\eventloop\zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\zmq\eventloop\zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\zmq\eventloop\zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\ipykernel\kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\ipykernel\kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\ipykernel\kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\ipykernel\ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\ipykernel\zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2698, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2802, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-4f763d68be26>"", line 5, in <module>
    new_all_saver = tf.train.import_meta_graph(meta_path)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\training\saver.py"", line 1698, in import_meta_graph
    **kwargs)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\meta_graph.py"", line 656, in import_scoped_meta_graph
    producer_op_list=producer_op_list)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\importer.py"", line 313, in import_graph_def
    op_def=op_def)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\some_user\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

DataLossError (see above for traceback): Unable to open table file C:\tmp\tmp\.data-00000-of-00001: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?
	 [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]

```
Here link to data file [https://github.com/BrazilForever11/tf_error/blob/master/.data-00000-of-00001](url)

It is not clear to me what is wrong. Error is reproducible.

Here some details of my setup:
OS: Win 7, 64 bit
Tensorflow installed through anaconda enviroment
Python 3.5
Tensorflow version 1.3.0
No GPU, 
2 cores CPU
"
13587,TF fails to build on PowerPC. Issues with BoringSSL.,"
------------------------
BoringSSL doesn't seem to have their own issue board. So I'm guessing here's the closest alternative.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.2 LTS
- **TensorFlow installed from (source or binary)**: source (github master, 10c871ed92a1d9b36c5e2e3a674d5812c67e82a1)
- **TensorFlow version (use command below)**:  (doesn't compile)
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.6.1
- **CUDA/cuDNN version**: (not relevant)
- **GPU model and memory**: Nvidia K40
- **Exact command to reproduce**: ```bazel --host_jvm_args=-XX:+IgnoreUnrecognizedVMOptions build -c opt --config=cuda  //tensorflow/tools/pip_package:build_pip_package --verbose_failures```


### Describe the problem
Latest TF master branch does not build on PowerPC. 
Problem seems to be with BoringSSL.
This [link](https://github.com/google/boringssl/blob/f21650709a6f76e829ddcc77fe221c9d6a5c12de/crypto/fipsmodule/bcm.c#L91) points to a file in BoringSSL that has the following inclusions:
```
#include ""sha/sha1-altivec.c""
#include ""sha/sha1.c""
```

Both files included seem to have their own definition of `sha1_block_data_order` and both are enabled on PowerPC. This seems to cause GCC 4.8.5 to freak out (did not test other versions of gcc). My temporary fix is to remove the altivec version of sha1 and it works. Hopefully someone working on BoringSSL can take a look.


Below is the exact error message:
```
ERROR: /gsa/yktgsa-h2/05/tjin/.cache/bazel/_bazel_tjin/5ebd35a31f2e08b1acf4a588141b13f1/external/boringssl/BUILD:118:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /gsa/yktgsa-h2/05/tjin/.cache/bazel/_bazel_tjin/5ebd35a31f2e08b1acf4a588141b13f1/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64: \
    PATH=/usr/lib/jvm/java-8-openjdk-ppc64el/bin:/localhd/tjin/tensorflow_latest/bin:/gsa/yktgsa/home/t/j/tjin/anaconda2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda/bin:/localhd/tjin/tensorflow_latest/bazel/output:/usr/local/cuda/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/localhd/tjin/tensorflow_latest/bin/python \
    PYTHON_LIB_PATH=/localhd/tjin/tensorflow_latest/lib/python2.7/site-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2 \
    TF_CUDA_VERSION=8.0 \
    TF_CUDNN_VERSION=5.1.5 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/local_linux-opt/bin/external/boringssl/_objs/crypto/external/boringssl/src/crypto/fipsmodule/bcm.pic.d -fPIC -iquote external/boringssl -iquote bazel-out/local_linux-opt/genfiles/external/boringssl -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem external/boringssl/src/include -isystem bazel-out/local_linux-opt/genfiles/external/boringssl/src/include -isystem external/bazel_tools/tools/cpp/gcc3 -Wa,--noexecstack '-D_XOPEN_SOURCE=700' -Wall -Werror '-Wformat=2' -Wsign-compare -Wmissing-field-initializers -Wwrite-strings -Wshadow -fno-common -DOPENSSL_NO_ASM '-std=c11' -Wmissing-prototypes -Wold-style-definition -Wstrict-prototypes -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c external/boringssl/src/crypto/fipsmodule/bcm.c -o bazel-out/local_linux-opt/bin/external/boringssl/_objs/crypto/external/boringssl/src/crypto/fipsmodule/bcm.pic.o).
In file included from external/boringssl/src/crypto/fipsmodule/bcm.c:92:0:
external/boringssl/src/crypto/fipsmodule/sha/sha1.c:125:6: error: static declaration of 'sha1_block_data_order' follows non-static declaration
 void sha1_block_data_order(uint32_t *state, const uint8_t *data, size_t num);
      ^
In file included from external/boringssl/src/crypto/fipsmodule/bcm.c:91:0:
external/boringssl/src/crypto/fipsmodule/sha/sha1-altivec.c:190:6: note: previous definition of 'sha1_block_data_order' was here
 void sha1_block_data_order(uint32_t *state, const uint8_t *data, size_t num) {
      ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 311.401s, Critical Path: 8.71s
```"
13585,"AttributeError: 'SummaryMetadata' object has no attribute 'display_name' ; windows10 (64bit), install tensorflow (1.3.0rc0) in Anaconda python36  with tensorflow-tensorboard (0.1.7) on win10","My laptop OS is windows10 (64bit), install tensorflow (1.3.0rc0) in Anaconda python36  with tensorflow-tensorboard (0.1.7) in it. when run command "" tensorboard --logdir=""path//to//logs"", met below error.

```
Exception in thread Reloader:
Traceback (most recent call last):
  File ""c:\soft_app\anaconda3\lib\threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""c:\soft_app\anaconda3\lib\threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""c:\soft_app\anaconda3\lib\site-packages\tensorboard\backend\application.py"", line 327, in _reload_forever
    reload_multiplexer(multiplexer, path_to_run)
  File ""c:\soft_app\anaconda3\lib\site-packages\tensorboard\backend\application.py"", line 301, in reload_multiplexer
    multiplexer.Reload()
  File ""c:\soft_app\anaconda3\lib\site-packages\tensorboard\backend\event_processing\plugin_event_multiplexer.py"", line 195, in Reload
    accumulator.Reload()
  File ""c:\soft_app\anaconda3\lib\site-packages\tensorboard\backend\event_processing\plugin_event_accumulator.py"", line 189, in Reload
    self._ProcessEvent(event)
  File ""c:\soft_app\anaconda3\lib\site-packages\tensorboard\backend\event_processing\plugin_event_accumulator.py"", line 335, in _ProcessEvent
    value = data_compat.migrate_value(value)
  File ""c:\soft_app\anaconda3\lib\site-packages\tensorboard\data_compat.py"", line 57, in migrate_value
    return handler(value) if handler else value
  File ""c:\soft_app\anaconda3\lib\site-packages\tensorboard\data_compat.py"", line 69, in _migrate_histogram_value
    display_name=value.metadata.display_name or value.tag,
AttributeError: 'SummaryMetadata' object has no attribute 'display_name'
```"
13584,"AttributeError: 'SummaryMetadata' object has no attribute 'display_name' ( OS:windows, Python36, tensorflow (1.3.0rc0) tensorflow-tensorboard (0.1.7)","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13583,"InvalidArgumentError : Expected image (JPEG, PNG, or GIF), got unknown format starting with '255'","### System information
-  **OS Platform and Distribution**  :Linux Ubuntu 16.04
-  **TensorFlow installed from**  :binary
-  **TensorFlow version**  :1.1.0
-  **Python version** : 2.7.12


### Describe the problem
trying to feed a model an image encoded in string as the model require that as an input string
```
signature_def {
  key: ""serving_default""
  value {
    inputs {
      key: ""image_bytes""
      value {
        name: ""Placeholder:0""
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: -1
          }
        }
      }
    }
    inputs {
      key: ""key""
      value {
        name: ""Placeholder_1:0""
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: -1
          }
        }
      }
    }
    outputs {
      key: ""key""
      value {
        name: ""Identity:0""
        dtype: DT_STRING
        tensor_shape {
          dim {
            size: -1
          }
        }
      }
    }
    outputs {
      key: ""prediction""
      value {
        name: ""ArgMax:0""
        dtype: DT_INT64
        tensor_shape {
          dim {
            size: -1
          }
        }
      }
    }
    outputs {
      key: ""scores""
      value {
        name: ""final_ops/softmax:0""
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: -1
          }
          dim {
            size: 3
          }
        }
      }
    }
    method_name: ""tensorflow/serving/predict""
  }
}
```
Here is the code :-
```
def load_image( infilename ) :
    img = Image.open( infilename )
    img.load()
    data = np.asarray( img, dtype=""string"" )
    return data

export_dir = '.'
with tf.Session(graph=tf.Graph()) as sess:
	model = tf.saved_model.loader.load(sess, ['serve'], export_dir)
	input_dict, output_dict =_signature_def_to_tensors(model.signature_def['serving_default'])
	out = sess.run(output_dict, feed_dict={input_dict['image_bytes']: load_image(""fullsize.jpeg"").flatten()})
	print(input_dict)
```
Error returns  is :
```
InvalidArgumentError (see above for traceback): Expected image (JPEG, PNG, or GIF), got unknown format starting with '255'
	 [[Node: map/while/DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method="""", fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=""/job:localhost/replica:0/task:0/cpu:0""](map/while/TensorArrayReadV3)]]
```

how i can solve that , any help "
13581,Assign requires shapes of both tensors to match,"im trying to run the voice recognition example

for some reason specifying a clip_duration_ms diffren from 1000 generate an error while freezing the model.

so running
python tensorflow/examples/speech_commands/freeze.py 
--wanted_words=yes 
--clip_duration_ms=2800 --sample_rate=16000 --window_size_ms=20 
--start_checkpoint=/notebooks/yesmodel/conv.ckpt-10 
--output_file=/notebooks/yesmodel/conv_frozen.pb

generate the following message
Error: Assign requires shapes of both tensors to match. lhs shape= [320000,3] rhs shape= [62720,3]

any idea what im doing wrong?"
13580,Equeued values to Queue get chopped-off if the Queue isn't instantiated properly.,"### Describe the problem

In QueueBase._check_enqueue_dtypes, the following code is run:
`tensors = []
    for i, (val, dtype) in enumerate(zip(vals, self._dtypes)):
      tensors.append(ops.convert_to_tensor(val, dtype=dtype,
          name=""component_%d"" % i))`

The problem is if the user feeds a list of tensors as `val` (the input) which is longer than the `_dtypes`-construction argument of QueueBase. 

If the user hasn't specified a length of `_dtypes`, e.g if he constructs a Queue like this:
`tf.FIFOQueue(100,dtypes=tf.int64)`
The FIFOQueue will have a default length 1. This means that the zip-function in the code above will essentialy **cut-off** any data that is longer than 1 (or whatever length `_dtypes` is).

I think an exception should be thrown if the user tries to enqueue a list of tensors that is of unexpected length.




### Source code / logs
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/data_flow_ops.py
Line 270, 271, 272 and 273
"
13578,Using an LSTM-CTC Tensorflow Model in Android,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 8.1
- **TensorFlow installed from (source or binary)**:
Anaconda Install. [Prebuilt libraries](https://ci.tensorflow.org/view/Nightly/job/nightly-android/) were used for Tensorflow Android.
- **TensorFlow version (use command below)**:
1.2.1
- **Python version**: 
Python 3.5.3 :: Anaconda custom (64-bit)
- **Bazel version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A

### Describe the problem
I have succeeded in training my bi-lstm-ctc tensorflow model and now I want to use it for my handwriting recognition android application. Here's the part of the code that defines the graph I used:

```
self.inputs = tf.placeholder(tf.float32, [None, None, network_config.num_features], name=""input"")
self.labels = tf.sparse_placeholder(tf.int32, name=""label"")
self.seq_len = tf.placeholder(tf.int32, [None], name=""seq_len_input"")

logits = self._bidirectional_lstm_layers(
   network_config.num_hidden_units,
   network_config.num_layers,
   network_config.num_classes
)

self.global_step = tf.Variable(0, trainable=False)
self.loss = tf.nn.ctc_loss(labels=self.labels, inputs=logits, sequence_length=self.seq_len)
self.cost = tf.reduce_mean(self.loss)

self.optimizer = tf.train.AdamOptimizer(network_config.learning_rate).minimize(self.cost)
self.decoded, self.log_prob = tf.nn.ctc_beam_search_decoder(inputs=logits, sequence_length=self.seq_len, merge_repeated=False)
self.dense_decoded = tf.sparse_tensor_to_dense(self.decoded[0], default_value=-1, name=""output"")

```

I also succeeded in freezing and optimizing the graph using this code:

```
def freeze(input_graph_path, checkpoint_path, output_node_names, input_saver_def_path="""", input_binary=False,
           restore_op_name=""save/restore_all"", filename_tensor_name=""save/Const:0"",
           output_frozen_graph_name=""frozen_output.pb"",
           clear_devices=True):
    freeze_graph.freeze_graph(input_graph_path, input_saver_def_path, input_binary,
                              checkpoint_path, output_node_names, restore_op_name, filename_tensor_name,
                              output_frozen_graph_name, clear_devices, """")


def optimize_graph(graph_path, input_nodes, output_nodes):
    input_graph_def = tf.GraphDef()
    with tf.gfile.Open(graph_path, ""rb"") as f:
        data = f.read()
        input_graph_def.ParseFromString(data)

    output_graph_def = optimize_for_inference_lib.optimize_for_inference(
        input_graph_def,
        input_nodes,
        output_nodes,
        tf.float32.as_datatype_enum
    )

    f = tf.gfile.FastGFile(""optimized_"" + graph_path, ""w"")
    f.write(output_graph_def.SerializeToString())
```

And here's the part of the android code that is supposed to run the model:

```
bitmap = Bitmap.createScaledBitmap(bitmap, 1024, 128, true);
int[] intValues = new int[bitmap.getWidth() * bitmap.getHeight()];
bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());
float[] floatValues = new float[bitmap.getWidth() * bitmap.getHeight()];
for (int i = 0; i < intValues.length; ++i) {
    final int val = intValues[i];
    floatValues[i] = (((val >> 16) & 0xFF));
}
float[] result = new float[80];
long[] INPUT_SIZE = new long[]{1, bitmap.getHeight(), bitmap.getWidth()};
inferenceInterface.feed(config.getInputName(), floatValues, INPUT_SIZE);
inferenceInterface.feed(""seq_len_input"", new int[]{bitmap.getWidth()}, 1);
inferenceInterface.run(config.getOutputNames());
inferenceInterface.fetch(config.getOutputNames()[0], result);

return result.toString();
```

However, I encounter these problems depending on the model I use. If I use the frozen graph, I encounter this error:

```
Caused by: java.lang.IllegalArgumentException: No OpKernel was registered to support
Op 'SparseToDense' with these attrs.  Registered devices: [CPU], Registered kernels:
device='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]
device='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]
device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]
device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]
device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]
device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]
                                                                                            
[[Node: output = SparseToDense[T=DT_INT64, Tindices=DT_INT64, validate_indices=true](CTCBeamSearchDecoder, CTCBeamSearchDecoder:2, CTCBeamSearchDecoder:1, output/default_value)]]
```

If I use the optimized frozen graph, I encounter this error:

```
java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef expected inputs '' do not match 1 inputs 
specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; 
NodeDef: stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/add/y = Const[dtype=DT_INT32, 
value=Tensor<type: int32 shape: [] values: 1>](stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Switch:1)
```

I have no ideas on what these error messages tell me nor how to resolve these."
13577,Size of TFRecord is much more larger than CSV format,"### Testing Data:
 `adult.data` in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py
### Problem:
To use a `tf.contrib.data.TFRecordDataset`, i tried to convert `adult.data` in CSV into a TFRecord, but i just found that the **TFRecord** after converted is about **12MB**, while the original **CSV** is only about **3MB**, oops, **why the storage efficiency for TFRecord is so poor？**

### Source code for converting CSV to TFRecord:
```
def _int64_feature(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))

def _bytes_feature(value):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))

def csv2proto(
        data_source,
        all_cols,
        categorical_cols=[],
        continuous_cols=[],
        multi_values_cols=[],
        inner_delimiter=';'):
    if not os.path.isfile(data_source):
        raise ValueError('data file passed do not exist or not a file')

    file_name = os.path.splitext(data_source)[0] + '.tfrecords'
    writer = tf.python_io.TFRecordWriter(file_name)
    with open(data_source) as f:
        reader = csv.DictReader(f, fieldnames=all_cols)
        for row in reader:
            feature = dict()
            for col in categorical_cols:
                feature.update({col: _bytes_feature([row[col]])})
            for col in continuous_cols:
                feature.update({col: _int64_feature([int(row[col])])})
            for col in multi_values_cols:
                feature.update({col: _bytes_feature(row[col].split(inner_delimiter))})

            example = tf.train.Example(features=tf.train.Features(feature=feature))
            writer.write(example.SerializeToString())

        writer.close()
```

### System information:
- **OS Platform**:  Mac OS X 10.12.5
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None"
13576,sparse_softmax_cross_entropy_with_logits wrong annotation,"https://github.com/tensorflow/tensorflow/blob/107cc777af7880c140d089e44ad898a6ba929286/tensorflow/python/ops/nn_ops.py#L1661

It should be `If logits are scalars (need to have rank >= 1) or if the rank
      of the labels is not equal to the rank of the logits minus one.`"
13578,Using an LSTM-CTC Tensorflow Model in Android,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 8.1
- **TensorFlow installed from (source or binary)**:
Anaconda Install. [Prebuilt libraries](https://ci.tensorflow.org/view/Nightly/job/nightly-android/) were used for Tensorflow Android.
- **TensorFlow version (use command below)**:
1.2.1
- **Python version**: 
Python 3.5.3 :: Anaconda custom (64-bit)
- **Bazel version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A

### Describe the problem
I have succeeded in training my bi-lstm-ctc tensorflow model and now I want to use it for my handwriting recognition android application. Here's the part of the code that defines the graph I used:

```
self.inputs = tf.placeholder(tf.float32, [None, None, network_config.num_features], name=""input"")
self.labels = tf.sparse_placeholder(tf.int32, name=""label"")
self.seq_len = tf.placeholder(tf.int32, [None], name=""seq_len_input"")

logits = self._bidirectional_lstm_layers(
   network_config.num_hidden_units,
   network_config.num_layers,
   network_config.num_classes
)

self.global_step = tf.Variable(0, trainable=False)
self.loss = tf.nn.ctc_loss(labels=self.labels, inputs=logits, sequence_length=self.seq_len)
self.cost = tf.reduce_mean(self.loss)

self.optimizer = tf.train.AdamOptimizer(network_config.learning_rate).minimize(self.cost)
self.decoded, self.log_prob = tf.nn.ctc_beam_search_decoder(inputs=logits, sequence_length=self.seq_len, merge_repeated=False)
self.dense_decoded = tf.sparse_tensor_to_dense(self.decoded[0], default_value=-1, name=""output"")

```

I also succeeded in freezing and optimizing the graph using this code:

```
def freeze(input_graph_path, checkpoint_path, output_node_names, input_saver_def_path="""", input_binary=False,
           restore_op_name=""save/restore_all"", filename_tensor_name=""save/Const:0"",
           output_frozen_graph_name=""frozen_output.pb"",
           clear_devices=True):
    freeze_graph.freeze_graph(input_graph_path, input_saver_def_path, input_binary,
                              checkpoint_path, output_node_names, restore_op_name, filename_tensor_name,
                              output_frozen_graph_name, clear_devices, """")


def optimize_graph(graph_path, input_nodes, output_nodes):
    input_graph_def = tf.GraphDef()
    with tf.gfile.Open(graph_path, ""rb"") as f:
        data = f.read()
        input_graph_def.ParseFromString(data)

    output_graph_def = optimize_for_inference_lib.optimize_for_inference(
        input_graph_def,
        input_nodes,
        output_nodes,
        tf.float32.as_datatype_enum
    )

    f = tf.gfile.FastGFile(""optimized_"" + graph_path, ""w"")
    f.write(output_graph_def.SerializeToString())
```

And here's the part of the android code that is supposed to run the model:

```
bitmap = Bitmap.createScaledBitmap(bitmap, 1024, 128, true);
int[] intValues = new int[bitmap.getWidth() * bitmap.getHeight()];
bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());
float[] floatValues = new float[bitmap.getWidth() * bitmap.getHeight()];
for (int i = 0; i < intValues.length; ++i) {
    final int val = intValues[i];
    floatValues[i] = (((val >> 16) & 0xFF));
}
float[] result = new float[80];
long[] INPUT_SIZE = new long[]{1, bitmap.getHeight(), bitmap.getWidth()};
inferenceInterface.feed(config.getInputName(), floatValues, INPUT_SIZE);
inferenceInterface.feed(""seq_len_input"", new int[]{bitmap.getWidth()}, 1);
inferenceInterface.run(config.getOutputNames());
inferenceInterface.fetch(config.getOutputNames()[0], result);

return result.toString();
```

However, I encounter these problems depending on the model I use. If I use the frozen graph, I encounter this error:

```
Caused by: java.lang.IllegalArgumentException: No OpKernel was registered to support
Op 'SparseToDense' with these attrs.  Registered devices: [CPU], Registered kernels:
device='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]
device='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]
device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]
device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]
device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]
device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]
                                                                                            
[[Node: output = SparseToDense[T=DT_INT64, Tindices=DT_INT64, validate_indices=true](CTCBeamSearchDecoder, CTCBeamSearchDecoder:2, CTCBeamSearchDecoder:1, output/default_value)]]
```

If I use the optimized frozen graph, I encounter this error:

```
java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef expected inputs '' do not match 1 inputs 
specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; 
NodeDef: stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/add/y = Const[dtype=DT_INT32, 
value=Tensor<type: int32 shape: [] values: 1>](stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Switch:1)
```

I have no ideas on what these error messages tell me nor how to resolve these."
13577,Size of TFRecord is much more larger than CSV format,"### Testing Data:
 `adult.data` in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py
### Problem:
To use a `tf.contrib.data.TFRecordDataset`, i tried to convert `adult.data` in CSV into a TFRecord, but i just found that the **TFRecord** after converted is about **12MB**, while the original **CSV** is only about **3MB**, oops, **why the storage efficiency for TFRecord is so poor？**

### Source code for converting CSV to TFRecord:
```
def _int64_feature(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))

def _bytes_feature(value):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))

def csv2proto(
        data_source,
        all_cols,
        categorical_cols=[],
        continuous_cols=[],
        multi_values_cols=[],
        inner_delimiter=';'):
    if not os.path.isfile(data_source):
        raise ValueError('data file passed do not exist or not a file')

    file_name = os.path.splitext(data_source)[0] + '.tfrecords'
    writer = tf.python_io.TFRecordWriter(file_name)
    with open(data_source) as f:
        reader = csv.DictReader(f, fieldnames=all_cols)
        for row in reader:
            feature = dict()
            for col in categorical_cols:
                feature.update({col: _bytes_feature([row[col]])})
            for col in continuous_cols:
                feature.update({col: _int64_feature([int(row[col])])})
            for col in multi_values_cols:
                feature.update({col: _bytes_feature(row[col].split(inner_delimiter))})

            example = tf.train.Example(features=tf.train.Features(feature=feature))
            writer.write(example.SerializeToString())

        writer.close()
```

### System information:
- **OS Platform**:  Mac OS X 10.12.5
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None"
13576,sparse_softmax_cross_entropy_with_logits wrong annotation,"https://github.com/tensorflow/tensorflow/blob/107cc777af7880c140d089e44ad898a6ba929286/tensorflow/python/ops/nn_ops.py#L1661

It should be `If logits are scalars (need to have rank >= 1) or if the rank
      of the labels is not equal to the rank of the logits minus one.`"
13575,Reading .tfrecords files greater than 64mb brings up errors,"I have already read this thread: https://github.com/tensorflow/tensorflow/issues/7311 but I am still encountering this issue with version 1.3.0. Was wondering if anyone knows why?


Tested on 2 systems:
OS: Windows 10, Ubuntu 16.04
Tensorflow installed from source,
Tensorflow version: 1.3.0
One system running tensorflow non gpu and another system running CUDA 8 and cuDNN v6
GPU model: none and GTX Titan Z

The error I get is:

`  File ""C:\Users\benja\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Could not parse example input, value: '
�ם9
ѻ
csvȻ
Ļ
��@CQ�.�������M�B

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\write_to_tfrecords.py"", line 581, in <module>
    main()
  File ""D:\write_to_tfrecords.py"", line 577, in main
    read_dataset_from_tfrecords()
  File ""D:\write_to_tfrecords.py"", line 554, in read_dataset_from_tfrecords
    image_dataset, csv_dataset = sess.run([image_out_reshaped, csv_out_reshaped])
  File ""C:\Users\benja\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 895, in run
    run_metadata_ptr)
  File ""C:\Users\benja\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\benja\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""C:\Users\benja\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Could not parse example input, value: '
�ם9
ѻ
csvȻ
Ļ
��@CQ�.�������M�B
>>> 
`


When I try to read .tfrecords files that are less than 64mb this error does not occur.

My code is here:
`        with tf.Session() as sess:
            try:
                feature = {'images': tf.FixedLenFeature([], tf.string),
                           'csv': tf.FixedLenFeature([], tf.string)
                           }

                filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)

                reader = tf.TFRecordReader()

                _, serialized_example = reader.read(filename_queue)

                features = tf.parse_single_example(serialized_example, features=feature)

                image_out = tf.decode_raw(features['images'], tf.uint8)
                csv_out = tf.decode_raw(features['csv'], tf.float32)

                image_out_reshaped = tf.reshape(image_out, [1000, 200, 200, 3])
                csv_out_reshaped = tf.reshape(csv_out, [1000, 6])

                sess.run(tf.global_variables_initializer())
                sess.run(tf.local_variables_initializer())

                # Create a coordinator and run all QueueRunner objects
                coord = tf.train.Coordinator()
                threads = tf.train.start_queue_runners(coord=coord)

                image_dataset, csv_dataset = sess.run([image_out_reshaped, csv_out_reshaped])

                coord.request_stop()
                coord.join(threads)
`"
13568,batch_flatten gives unpredictable results when batch_size is 1,"## Problem

When I attempt to use `tf.contrib.keras.backend.batch_flatten` or more readily `keras.backend.batch_flatten` on a Tensor of shape (1, a, b) it produces a Tensor with shape (None, None).

## MCVE

Note, I'll use keras tensorflow backend because that is how I noticed it. 

```
t = K.zeros((1,2,2))
print(K.int_shape(K.reshape(t,(-1,2*2))))
print(K.int_shape(K.batch_flatten(t)))
```
prints out `(1,4)` and `(None, None)`.
If you replace the first dimension to 2 or more it works as expected.

## System

tensorflow 1.2.0 
keras 2.0.8 python 3.6 installed from git repo




"
13567,"Hello every one, please i got an error in my code, here is the error, please can someone help me how to fix it...thanks.... ""but saw tensor: %s"" % p) ValueError: prefix tensor must be either a scalar or vector, but saw tensor: Tensor(""batch_size:0"", dtype=int32)","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13566,tf.estimator Quickstart web doc needs syncing with GitHub,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.11.6
- **TensorFlow installed from (source or binary)**: conda-forge
- **TensorFlow version (use command below)**: 1.3
- **Python version**:  3.6
- **Bazel version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: Copy and paste code sample and run on a Jupter Notebook. The code sample on GitHub works (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/get_started/estimator.md). The code sample on Tensorflow documentation doesn't (https://www.tensorflow.org/get_started/estimator). My guess is that the web documentation requires ""syncing"" with Github version? 

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
This is regarding (possibly out-dated) web documentation not ""synced"" with the correct GitHub version.

- The code sample on Tensorflow documentation doesn't run on Python 3.6 / TensorFlow v1.3 (https://www.tensorflow.org/get_started/estimator). 
- The code sample on GitHub works (probably more up to date)
 (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/get_started/estimator.md). 

### Source code / logs
If you copy and paste the code sample from web documentation and run on a Jupter notebook, you get:

```
AttributeError: module 'urllib' has no attribute 'urlopen'
```

(this has been addressed in GitHub repo sample code. Just not the web doc).

Also, the line that reads (for both training and test):

```.py
with open(IRIS_TRAINING, ""w"") as f:
```

should be corrected to:

```.[py
with open(IRIS_TRAINING, ""wb"") as f:
```

(again, this has been addressed in GitHub repo sample code. Just not the web doc).

So possible just need a refresh of the code sample on the web doc?

i.e. replace the [current web doc](https://www.tensorflow.org/get_started/estimator) with the [GitHub doc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/get_started/estimator.md)?

Thanks!"
13565,Bug in Estimator tutorial?,"This is the tutorial for the Estimator-class:

https://www.tensorflow.org/extend/estimators

You have the following code:

    my_nn = tf.estimator.DNNClassifier(feature_columns=[age, height, weight],
                                       hidden_units=[10, 10, 10],
                                       activation_fn=tf.nn.relu,
                                       dropout=0.2,
                                       n_classes=3,
                                       optimizer=""Adam"")

and the following:

    input_layer = tf.feature_column.input_layer(
        features=features, feature_columns=[age, height, weight])

If I understand correctly, the feature-columns use the __variables__ `age`, `height` and `weight`. However, these __variables__ are not defined anywhere in the source-code for the tutorial.

The complete source-code is available in `abalone.py` as well:

https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/tutorials/estimators/abalone.py

But here you no longer have the `input_layer` that uses the feature-columns. Instead you have the following which pulls out `""x""` from the `features`-dict:

    # Connect the first hidden layer to input layer
    # (features[""x""]) with relu activation
    first_hidden_layer = tf.layers.dense(features[""x""], 10, activation=tf.nn.relu)

So I'm a bit confused how this is supposed to work?

In general, why don't you make the tutorials as Jupyter Notebooks instead? It would be immensely more helpful than your current tutorial style which is very confusing.

And please remember, that for each hour you spend polishing the code, you will likely save several hours of head-aches for each person trying to understand your code. Multiplied by the many thousands of TensorFlow users, this is a tremendous amount of work-hours that is freed up for the community!
"
13563,What the wrong with sess.run ? ,"I want to test a single test sample into a graph , but  unfortunately , I got nothing . If i just put a batch size of test sample ,  the result is good, why is it?  Furthermore , if i just copy a single test sample multiple times, the result is also nothing.

x_reconstruction = sess.run(t.x_r, feed_dict={t.z_r: z_batch})
x_reconstruction[0]
Out[42]: 
array([ -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,
        -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,
        -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,
        -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,
        -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,
        -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,
        -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,
        -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,
        -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,

I just take the a test sample from z_batch, running the graph

x_reconstruction_1 = sess.run(t.x_r, feed_dict={t.z_r: z_batch[0].reshape(1,2)})
x_reconstruction_1
Out[44]: 
array([[ -2.21053764e-01,  -2.20187426e-01,  -2.38173127e-01,
         -2.24671751e-01,  -2.32440352e-01,  -2.28797898e-01,
         -2.25955158e-01,  -2.28772879e-01,  -2.28901237e-01,
         -2.22546220e-01,  -2.15402722e-01,  -2.31919050e-01,
         -2.24671602e-01,  -2.24030137e-01,  -2.37917259e-01,
         -2.35338598e-01,  -2.11188301e-01,  -2.30172306e-01,
         -2.26653352e-01,  -2.27616981e-01,  -2.25351438e-01,
         -2.26480648e-01,  -2.29957879e-01,  -2.28425398e-01,

Why is there a big difference between the same implementation?"
13560,The document of tf.nn.dynamic_rnn needs to be re-formatted,"Hi,

Someone take a look at the document page of [tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn). I think there are some formatting bugs in the second half of the web page. "
13558,segfaults in GPU tf.matrix_inverse,"I'm running into segfaults in tf.matrix_inverse
I'm adding identity*0.001 so matrices should be invertible, and same procedure works fine in numpy and in TensorFlow CPU version.

https://github.com/yaroslavvb/stuff/blob/master/inverse_segfault.py
`python inverse_segfault.py`

This non-deterministically crashes after 1-2 seconds with various backtraces.

IE

```
#0  0x0000000000000001 in ?? ()
#1  0x00007fe90ed9c652 in tensorflow::Tensor::TotalBytes() const ()
   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
#2  0x00007fe90ed9c7d6 in tensorflow::Tensor::tensor_data() const ()
   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007fe9137adda3 in bool tensorflow::internal::TransposeUsingTile<unsigned int>(Eigen::GpuDevice const&, tensorflow::Tensor const&, tensorflow::gtl::ArraySlice<int>, tensorflow::Tensor*) ()
   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007fe9137a696c in tensorflow::Status tensorflow::DoTranspose<Eigen::GpuDevice>(Eigen::GpuDevice const&, tensorflow::Tensor const&, tensorflow::gtl::ArraySlice<int>, tensorflow::Tensor*) ()
   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007fe911ada0fd in tensorflow::SvdOpGpu<float>::PerformSVD_MgeqN(tensorflow::OpKernelContext*, std::function<void ()>, long long, long long, long long, tensorflow::gtl::ArraySlice<int> const&, tensorflow::Tensor const&, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*) ()
   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007fe911ade897 in tensorflow::SvdOpGpu<float>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>) ()
   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007fe90f20790b in tensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>) ()
   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
#8  0x00007fe90f23cf37 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)
    ()
   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
```

or this

```
#0  0x00007fa89090686a in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
#1  0x00007fa89091b074 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
#2  0x00007fa890826e2c in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
#3  0x00007fa890978880 in cuLaunchKernel ()
   from /usr/lib/x86_64-linux-gnu/libcuda.so.1
#4  0x00007fa891bf1dc1 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0
#5  0x00007fa891c0f9cd in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0
#6  0x00007fa891aa1132 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0
#7  0x00007fa891aa2b72 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0
#8  0x00007fa891aa32e3 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0
#9  0x00007fa891aa36fa in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0
#10 0x00007fa89190f5f3 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0
#11 0x00007fa891912375 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0
#12 0x00007fa89aa82c50 in tensorflow::Status tensorflow::CudaSolver::Getrf<float>(int, int, float*, int, int*, int*) ()
   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#13 0x00007fa89a55f5d6 in tensorflow::MatrixInverseOpGpu<float>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>) ()
   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#14 0x00007fa897dce90b in tensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>) ()
   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
#15 0x00007fa897e03f37 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)
    ()
   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
#16 0x00007fa897df1ec5 in std::_Function_handler<void (), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> (tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> >::_M_invoke(std::_Any_data const&) ()
   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so

```

TensorFlow commit: https://github.com/tensorflow/tensorflow/commit/22a886b
NVIDIA-SMI 381.09
libcudart.so.8.0.44
libcudnn.so.6.0.21
Nvidia GTX 1080
"
13556,Problem importing Tensorflow in Windows 10 64bits,"Hello everybody. I'm new in Tensorflow. So, forgive me for any mistakes that a make. 

I'm importing Tensorfow-gpu in Windows but I'm getting the message bellow. 

**### System information**
- **OS Platform and Distribution:** Windows 10 64 bits
- **TensorFlow installed from:** pip
- **TensorFlow version:** 1.3
- **Python version:** 3.6.2 - v3.6.2:5fd33b5, Jul  8 2017, 04:57:36
- **CUDA:** 9.0.176
- **cuDNN version:** I've tried with version 5, 6 and 7
- **GPU model and memory**: 1050 4GB
- **Exact command to reproduce**: import tensorflow

**PS:** all the path settings have been set. 

**### The error message**
C:\Windows\system32>python
Python 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\python\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: Não foi possível encontrar o módulo especificado.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\python\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\python\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\python\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\python\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: Não foi possível encontrar o módulo especificado.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\python\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\python\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\python\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>"
13552,how to restore the certain variable_scope Variables into another certain variable_scope?,"Now I have trained a model A and I need two model A instances because one of them just is fixed and untrainable for outputting and another is trainable for next network. I design two variable_scope **A_train** and **A_untrain**, I pre-trained A model in variable_scope **A_untrain** and restore the model also in this scope, code like:
```python
saver_untrain = tf.train.Saver(tf.get_collection(
                                   tf.GraphKeys.GLOBAL_VARIABLES,
                                   'A_untrain'))
saver_path = '~/models/model.ckpt'
# here pre-train model A
saver_untrain.save(sess, saver_path)
```
Now I need to restore the same model A parameters into the same model in scope **A_train**, but I cannot follow the previous code because the ckpt files restore the params like `A_untrain/input_w1` instread of `A_train/input_w1`. I want to know if there is a solution to my problem OR a better solution to make two instances which one is trainable and another is untrainable. Thanks a lot.

**EDIT_1**: I know I can realize my need use code like:

saver_train = tf.train.Saver({'A_untrain/input_w1': A_train.input_w1})
but it will be unpractical when my variables amount is large, so I need to use the variable_scope to restore instead of the specific variables' names."
13550,https://www.tensorflow.org/ extremly sucks: cockie madness,"https://www.tensorflow.org/* totally sucks because of its cookie popup dialog, which jumps into one's face __each__ time one goes to a different page on this site, NO matter whether one _has already clicked clicked_ OK or something else. No way to do any research on such annoying site ..."
13549,const_op.h missing from C++ API documentation,"I could not find the documentation for tensorflow::ops::Const starting from the C++ API documentation. I figured out it is declared in ops/const_op.h, but there is no link from the main C++ docs:
https://www.tensorflow.org/api_docs/cc/

A search returns this, but it looks orphaned:
https://www.tensorflow.org/api_docs/cc/group/const-op

So I guess there is something wrong in the docs."
13546,Feature request: RMSProp without momentum variables,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.3.0-24-g658866597 1.3.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: 0.6.1
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**:

### Describe the problem

The current `RMSPropOptimizer` always allocates momentum variables, even though the default (and probably 99% of people) never make use of it. In fact, if one wishes to combine momentum and adaptive gradient descent, he/she will most likely instantiate an `AdamOptimizer` instead. The extra variables waste precious GPU/CPU memory as well as disk space (when saved as checkpoints) while providing minimal utility. 

Suggestion: introduce a new version of `ApplyRMSProp` operations that doesn't use momentum variables at all, and dynamically choose which implementation to use in `RMSPropOptimizer` constructor depending on whether the `momentum` argument is constant zero.

Alternative solution: change the current `ApplyRMSProp` operations so that it doesn't use momentum variables, and direct the minority users who currently need momentum with RMSProp to Adam instead.

"
13543,how to condition encoder final hidden state on the inputs of RNN dynamic decoder with ScheduledOutputTrainingHelper?,"Hi, I'm trying to use tensorflow to code RDD encoder and decoder and with different length sequence inputs, so hope both encoder and decoder can be dynamic. Additionally, a decoder inputs is conditioned by the encoder final hidden states (context vector), which is similar to the [Related Paper](https://arxiv.org/pdf/1702.05538.pdf) see picture a in page 3. The decoder is trying to fully inference during training with feeding previous outputs and context vector as inputs at each step.

`class RNNEncoder_Decoder(object):
    def __init__(self,input_dim,
                 context_dim,output_dim,hidden_dim,
                 layers_stacked_count,learning_rate):
        
        self.graph = tf.get_default_graph()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.context_dim = context_dim
        self.hidden_dim = hidden_dim
        self.layers_stacked_count = layers_stacked_count
        self.learning_rate = learning_rate
        self.sampling_probability = tf.constant(dtype=tf.float32,value=1.0)
        
        # [batch_size,sequence_length,input_dimension]
        self.enc_inp = tf.placeholder(tf.float32, [None,None,self.input_dim], name='encoder_inputs')
        self.expected_out = tf.placeholder(tf.float32, [None,None,self.output_dim], name='expected_outs')
        # fullly inference during trianing
        self.dec_inp = tf.zeros_like(self.expected_out,dtype=tf.float32,name='decoder_inputs')
                
        seq_length = tf.reduce_sum(tf.sign(tf.reduce_max(tf.abs(self.enc_inp), 2)), 1)
        self.seq_length = tf.cast(seq_length, tf.int32)
        
        with tf.variable_scope('RNNEncoderDecoder'):
            with tf.variable_scope(""Enocder"") as encoder_varscope:
                # create encoder LSTM cell
                encoder_cells = []
                for i in range(self.layers_stacked_count):
                    with tf.variable_scope('EncoderCell_{}'.format(i)):
                        encoder_cells.append(tf.nn.rnn_cell.LSTMCell(self.hidden_dim,
                                                             use_peepholes=True))
                self.encoder_cell = tf.nn.rnn_cell.MultiRNNCell(encoder_cells)

                # ruuning dynamic rnn encoder                
                _, enc_state = tf.nn.dynamic_rnn(cell = self.encoder_cell,
                                                 initial_state=None,
                                                 dtype=tf.float32,
                                                 inputs = self.enc_inp,
                                                 sequence_length = self.seq_length
                                                )
 
                # extract top layer hidden state as feature representation
                self.context_vector = enc_state[-1].h
                
                cell_state0 = tf.zeros_like(enc_state[0].c,dtype=tf.float32)
                hidden_state0 = tf.zeros_like(enc_state[0].h,dtype=tf.float32)

                dec_init_state = (enc_state[1], # pass the top layer state of enocder to the bottom layer of decoder
                                  tf.nn.rnn_cell.LSTMStateTuple(cell_state0, hidden_state0))
                
                # condition extracted features on decoder inputs
                # with a shape that matches decoder inputs in all but (potentially) the final dimension. 
                # tile context vector from [batch_size,context_dim] to [batch_size,decoder_sequence_length,context_dim]
                context_vector_shape = tf.shape(self.context_vector)
                context_vector_reshaped = tf.reshape(self.context_vector, 
                                                     [context_vector_shape[0], 1, context_vector_shape[1]]
                                                    )
                enc_inp_shape = tf.shape(self.enc_inp)
                self.auxiliary_inputs = tf.tile(context_vector_reshaped,
                                           multiples=[1,enc_inp_shape[1],1]
                                          )
                
            with tf.variable_scope(""Deocder"") as decoder_varscope:
                # create decoder LSTM cell
                decoder_cells = []
                for i in range(self.layers_stacked_count):
                    with tf.variable_scope('DecoderCell_{}'.format(i)):
                        decoder_cells.append(tf.nn.rnn_cell.LSTMCell(self.hidden_dim,
                                                             use_peepholes=True))
                self.decoder_cell = tf.nn.rnn_cell.MultiRNNCell(decoder_cells)

                dec_out_dense = Dense(units = self.output_dim,
                                      activation = None,
                                      use_bias = False,
                                      kernel_initializer = tf.truncated_normal_initializer(
                                          dtype=tf.float32,
                                          stddev = 1.0 / math.sqrt(float(self.hidden_dim))
                                      ),
                                      name = 'dec_outp_linear_projection'
                                     )
                
                training_helper = tf.contrib.seq2seq.ScheduledOutputTrainingHelper(
                    inputs = self.dec_inp,
                    sequence_length = self.seq_length,
                    auxiliary_inputs = self.auxiliary_inputs, # condtional on inputs
                    sampling_probability = 1.0, # for fullly inference
                    name = 'feeding_conditional_input'
                )
                
                decoder = tf.contrib.seq2seq.BasicDecoder(
                    cell = self.decoder_cell,
                    helper = training_helper,
                    initial_state = dec_init_state,
                    output_layer = dec_out_dense
                )
                
                outputs, _ , final_seq_lengths = tf.contrib.seq2seq.dynamic_decode(decoder=decoder,
                                                                                   impute_finished = True
                                                                                  )
            self.outputs = outputs
            
    ### optimize loss part
    
    def get_decoder_prediction(self,X,session):
        feed_dict = {
            self.enc_inp:X
        }
        feed_dict.update({self.expected_out:X})
        run = [self.outputs]
        return session.run(run,feed_dict=feed_dict)
RNN_test = RNNEncoder_Decoder(input_dim=1,context_dim=32,output_dim=1,hidden_dim=32,layers_stacked_count=2,learning_rate=0.01)`


Without ""auxiliary_inputs = self.auxiliary_inputs"", it running successfully,
But with auxiliary_inputs = self.auxiliary_inputs I got following error:

`---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-02522a01f0d8> in <module>()
      9                           hidden_dim=hidden_dim,
     10                           layers_stacked_count=layers_stacked_count,
---> 11                           learning_rate=learning_rate
     12                          )

<ipython-input-2-86494b8d99fa> in __init__(self, input_dim, context_dim, output_dim, hidden_dim, layers_stacked_count, learning_rate)
     98 
     99                 outputs, _ , final_seq_lengths = tf.contrib.seq2seq.dynamic_decode(decoder=decoder,
--> 100                                                                                    impute_finished = True
    101                                                                                   )
    102             self.outputs = outputs

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in dynamic_decode(decoder, output_time_major, impute_finished, maximum_iterations, parallel_iterations, swap_memory, scope)
    284         ],
    285         parallel_iterations=parallel_iterations,
--> 286         swap_memory=swap_memory)
    287 
    288     final_outputs_ta = res[1]

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)
   2773     context = WhileContext(parallel_iterations, back_prop, swap_memory, name)
   2774     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, context)
-> 2775     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
   2776     return result
   2777 

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)
   2602       self.Enter()
   2603       original_body_result, exit_vars = self._BuildLoop(
-> 2604           pred, body, original_loop_vars, loop_vars, shape_invariants)
   2605     finally:
   2606       self.Exit()

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)
   2552         structure=original_loop_vars,
   2553         flat_sequence=vars_for_body_with_tensor_arrays)
-> 2554     body_result = body(*packed_vars_for_body)
   2555     if not nest.is_sequence(body_result):
   2556       body_result = [body_result]

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in body(time, outputs_ta, state, inputs, finished, sequence_lengths)
    232       """"""
    233       (next_outputs, decoder_state, next_inputs,
--> 234        decoder_finished) = decoder.step(time, inputs, state)
    235       next_finished = math_ops.logical_or(decoder_finished, finished)
    236       if maximum_iterations is not None:

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py in step(self, time, inputs, state, name)
    137     """"""
    138     with ops.name_scope(name, ""BasicDecoderStep"", (time, inputs, state)):
--> 139       cell_outputs, cell_state = self._cell(inputs, state)
    140       if self._output_layer is not None:
    141         cell_outputs = self._output_layer(cell_outputs)

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)
    178       with vs.variable_scope(vs.get_variable_scope(),
    179                              custom_getter=self._rnn_get_variable):
--> 180         return super(RNNCell, self).__call__(inputs, state)
    181 
    182   def _rnn_get_variable(self, getter, *args, **kwargs):

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)
    448         # Check input assumptions set after layer building, e.g. input shape.
    449         self._assert_input_compatibility(inputs)
--> 450         outputs = self.call(inputs, *args, **kwargs)
    451 
    452         # Apply activity regularization.

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)
    936                                       [-1, cell.state_size])
    937           cur_state_pos += cell.state_size
--> 938         cur_inp, new_state = cell(cur_inp, cur_state)
    939         new_states.append(new_state)
    940 

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)
    178       with vs.variable_scope(vs.get_variable_scope(),
    179                              custom_getter=self._rnn_get_variable):
--> 180         return super(RNNCell, self).__call__(inputs, state)
    181 
    182   def _rnn_get_variable(self, getter, *args, **kwargs):

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)
    448         # Check input assumptions set after layer building, e.g. input shape.
    449         self._assert_input_compatibility(inputs)
--> 450         outputs = self.call(inputs, *args, **kwargs)
    451 
    452         # Apply activity regularization.

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)
    554     input_size = inputs.get_shape().with_rank(2)[1]
    555     if input_size.value is None:
--> 556       raise ValueError(""Could not infer input size from inputs.get_shape()[-1]"")
    557     scope = vs.get_variable_scope()
    558     with vs.variable_scope(scope, initializer=self._initializer) as unit_scope:

ValueError: Could not infer input size from inputs.get_shape()[-1]`

I'm just getting start to use tensforflow, so could anyone help me with:
Is this a correct way to condition the last hidden state of encoder on the inputs of decoder?
and why the inputs of decoder become None after I feed the auxiliary_inputs as the error?
@ebrevdo 
"
13537,Feature Request: tf.assign() support tuples,"I have recently updated to V1.3 of Tensorflow.  I have some code that I use for dynamic_rnn which copies the STATE of the cell so it persists to the next .run(), I can also INIT that value as well.  Since the update, I am getting a ""WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f278c196940>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True."".  I have tried to enable state_is_tuple but then the assign() commands fail as they don't support the tuple structures.

I have an open StackOverflow question with the details:
https://stackoverflow.com/questions/46576194/how-do-i-assign-a-lstmstatetuple-using-tf-assign

Since it seems like the RNN core is moving in the direction of the tuple for the state, it would be nice if the .assign() can handle this transparently.
"
13536,BeamSearchDecoder incorrectly truncates results when used with dynamic_decode,"### System information (irrelevant for this bug)
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04/Any
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: Python 3.5.2 :: Continuum Analytics, Inc.
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: irrelevant
- **GPU model and memory**: irrelevant
- **Exact command to reproduce**: irrelevant

### Describe the problem
tf.contrib.seq2seq.BeamSearchDecoder incorrectly truncates some of the results because the same index was previously used for a beam member that ended at a earlier step.

The root of the problem is that the while_loop body in dynamic_decode assumes that sequences are independent and will finish only once. In the same time BeamSearchDecoder creates a tree-like structure where a beam index can be reused in a later step for a state that originates from a different parent index.  This causes the decoding loop to sometimes record the wrong sequence length for a beam member. Then this wrong sequence length is passed to BeamSearchDecoder.finalize which returns a truncated sequence.


### Source code / logs
I use the following code to workaround the problem. This causes the right sequence to be returned but still the length returned by dynamic_decode is wrong.
```python
class FixedBeamSearchDecoder(seq2seq.BeamSearchDecoder):
    def finalize(self, outputs, final_state, sequence_lengths):
        # BeamSearchDecoder does not follow the correct semantics of the the finished flag
        # which results in taking wrong length here and getting wrong decoded string.
        # We substitute the sequence length recorded by dynamic_decoder (which is wrong because
        # of the wrong finished flag returned by BeamSearchDecoder.step) with the length
        # recorded in BeamSearchState which is correct.
        return super().finalize(outputs, final_state, final_state.lengths)
``` 
"
13533,"KeyError: ""Couldn't find enum google.protobuf.MethodOptions.IdempotencyLevel""","Hello! I am using macOS sierra 10.2.6, with tensorflow=1.3.0. TF was working fine until sometime ago, but as of today I get the error below when trying to import. Has anyone had the same problem?

Here are the logs.

---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-1-a649b509054f> in <module>()
----> 1 import tensorflow

/Users/antoniocampello/anaconda/lib/python3.5/site-packages/tensorflow/__init__.py in <module>()
     22 
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 

/Users/antoniocampello/anaconda/lib/python3.5/site-packages/tensorflow/python/__init__.py in <module>()
     52 
     53 # Protocol buffers
---> 54 from tensorflow.core.framework.graph_pb2 import *
     55 from tensorflow.core.framework.node_def_pb2 import *
     56 from tensorflow.core.framework.summary_pb2 import *

/Users/antoniocampello/anaconda/lib/python3.5/site-packages/tensorflow/core/framework/graph_pb2.py in <module>()
      8 from google.protobuf import reflection as _reflection
      9 from google.protobuf import symbol_database as _symbol_database
---> 10 from google.protobuf import descriptor_pb2
     11 # @@protoc_insertion_point(imports)
     12 

/Users/antoniocampello/anaconda/lib/python3.5/site-packages/google/protobuf/descriptor_pb2.py in <module>()
    237   options=None,
    238   serialized_start=4644,
--> 239   serialized_end=4724,
    240 )
    241 _sym_db.RegisterEnumDescriptor(_METHODOPTIONS_IDEMPOTENCYLEVEL)

/Users/antoniocampello/anaconda/lib/python3.5/site-packages/google/protobuf/descriptor.py in __new__(cls, name, full_name, filename, values, containing_type, options, file, serialized_start, serialized_end)
    597                 serialized_start=None, serialized_end=None):
    598       _message.Message._CheckCalledFromGeneratedFile()
--> 599       return _message.default_pool.FindEnumTypeByName(full_name)
    600 
    601   def __init__(self, name, full_name, filename, values,

KeyError: ""Couldn't find enum google.protobuf.MethodOptions.IdempotencyLevel""


"
13532,tf.contrib.data.Dataset generated by slicing and dicing very large images,"Hi
(writing here as requested by @mrry for further tf.contrib.data feature requests)

I would like to create a Dataset by cutting up and preprocessing very large images. I did this:

```
dataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(img_filelist))
dataset = dataset.flat_map(load_cut_up_and_process)
```

This goes out of memory because my function load_cut_up_and_process creates too many pieces from one image, all in memory. If I try to make a function that returns fewer pieces for an image and then want to call it repeatedly on the same image to get more, how can I achieve that with Dataset, without replicating the huge image in memory? The only thing I can think of is:

```
dataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(img_filelist))
dataset = dataset.flat_map(load_and_replicate_each_image) # Dataset is [im1, im1, im1, im2, im2, im2, im3, im3, im3, ...]
dataset = dataset.flat_map(cut_up_and_process_gently)
```

Now the second step goes out of memory because the implementation of load_and_replicate_each_image necessarily involves a tensor like [im, im, im] and multiple copies of the image will not fit. I also thought of this:

```
dataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(img_filelist))
dataset = dataset.flat_map(replicate_each_filename) # Dataset is [imfname1, imfname1, imfname1, imfname2, imfname2, imfname2, imfname3, imfname3, imfname3, ...]
dataset = dataset.flat_map(load_cut_up_and_process_gently)
```

Which works, does not go out of memory, but now I am loading the same huge image multiple times in a row which is slow.

Any ideas ?"
13530,"Pandas_input_fn slow, starving CPU/GPU","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: It is a customized version of the Deep & Wide example code. Fairly close to original code.
- **OS Platform and Distribution: Windows Server 2012 R2
- **TensorFlow installed from: nightly build WHL through pip (this was tried after numerous other versions, including install through pip)
- **TensorFlow version (use command below)**: b'unknown' 1.4.0-dev20170926
- **Python version**: 3.5 and 3.6
- **Bazel version (if compiling from source)**: Not compiling
- **CUDA/cuDNN version**: CUDA 8, CUDnn 6.1
- **GPU model and memory**: Tesla M60 GPU 8GB
- **Exact command to reproduce**:  See attached Script.
-For the record, the server vm has 8 xeon physical cores and 240 gb ram allocated. The CPU only machine is a new skylake i7 with 32gb ram.

### Describe the problem
To start, I submitted to stack overflow (https://stackoverflow.com/questions/46457476/tensorflow-pandas-input-fn-slow-starving-cpu-gpu) and have not been able to garner assistance after multiple edits to make sure it was framed correctly. I truly believe this is a bug since I am sticking so close to the example code, but if I have made a mistake I am deeply sorry to all of you.

I am working on a wide and deep model following the framework in the Tensorflow Wide and Deep tutorial (https://www.tensorflow.org/tutorials/wide_and_deep). Model works fine when built the old way (load entire dataset from pandas, convert to tensors, feed in input_fn) which is ok for running on a CPU. 

However, to make it work on the GPU the dataset is too large to fit into GPU memory, so batching is necessary. I tried using the pandas_input_fn to batch data to the video card and noticed I get spikes of activity followed by long lulls while the next batch is prepared. The odd thing is, this happens even if I run it on a machine with CPU only. The lulls are almost the exact same length, so it isn't simply the video card crushing through a simple model faster than the proc can deliver it. It seems like it is always waiting to begin loading the next batch until the last one is done training. 

(If this function simply cannot be used in this way, can we get an example of Deep and Wide using the dataset API? or a manual build of deep and wide using layers and queues? At the moment, the example code for the dataset api using make_one_shot_iterator for canned estimators doesn't run.)

I increased the complexity of the model to make sure it wasn't too easy to compute and still have the same issue. I have tried increasing the number of threads allocated to pandas_input_fn, I have tried increasing the queue size to far larger than seems reasonable (10x dataset size) which helps a bit, but not much. I am not sure if the slowdown is when it is queueing or de-queueing, but I have been unable to solve the issue after two weeks of troubleshooting. The data I am working with is 117 columns, 400k rows.

I have created a generic script that generates fake values to simulate the problem. However, there are far fewer fake columns than real ones, so the gap between steps is not nearly as long, but still noticeable. Code attached.


--

### Source code / logs
attached
[pandas_input_example.txt](https://github.com/tensorflow/tensorflow/files/1363335/pandas_input_example.txt)
"
13529,skip some single example when reading tfrecords,"I have created several tfrecords files that containing images, labels, or other information. 
When reading tfrecords, can I skip some samples according to the information after `tf.parse_single_example`. For example, I want to select those samples with certain labels or those sample with the image size larger than the threshold. 

I don't know how to do it in the current version. I hope you will support this feature in the future."
13526,Importing TF in Python yields 'cannot import name 'build_info',"### System information
Fedora 26 x64 (4.13.4-200.fc26.x86_64)
Tensorflow installed from source:
```
tf.VERSION = 1.3.0
tf.GIT_VERSION = b'v1.3.0-rc1-3011-gd86448938'
tf.COMPILER_VERSION = b'v1.3.0-rc1-3011-gd86448938'
```
Python version 3.6.2 (Anaconda)
Bazel installed from their Fedora/COPR repositories, version 0.6.0- (@non-git)
No CUDA (or compatible GPU)
Intel MKL 2018.0.128
c++ (GCC) 7.2.1 20170915 (Red Hat 7.2.1-2)
`bazel build -c opt --config=mkl //tensorflow/tools/pip_package:build_pip_package`
Notice the `mkl` flag in the bazel build

### Describe the problem
Configuration and bazel build finished without error. When attempting to import tensorflow in Python, I get this:

```
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/torstein/progs/tensorflow/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/torstein/progs/tensorflow/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/torstein/progs/tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 25, in <module>
    from tensorflow.python.platform import self_check
  File ""/home/torstein/progs/tensorflow/tensorflow/python/platform/self_check.py"", line 24, in <module>
    from tensorflow.python.platform import build_info
ImportError: cannot import name 'build_info'
```"
13524,Why is gen_ctc_op not visible in the github repository?,"I want to take a look at the core of the ctc_loss implementation, and looking at ctc_ops.py shows it's imported from the gen_ctc_ops module but that module is impossible to find in the github repository. Where can I find it?
Thanks"
13523,Float variable in loop not incremented properly,"Hi,

I am running the following code in tensorflow:

import tensorflow as tf

count = tf.get_variable(""count"", shape=(), dtype=tf.float32, trainable=False)
tf.assign(count, 0)
i = tf.get_variable(""i"", shape=(), dtype=tf.int32, trainable=False)
tf.assign(i, 0)

cond = lambda i, count: tf.less(i, 5)
body = lambda i, count: (tf.add(i,1), tf.add(count, 1))
i, count = tf.while_loop(cond, body, [i, count], shape_invariants=[tf.TensorShape(None), tf.TensorShape(None)])

init = tf.global_variables_initializer()

with tf.Session() as session:
    session.run(init)
    print(""i"", session.run(i), ""count"", session.run(count))

The expected result is that both, count and i have value 5. However, for count I get always different strange results like 6.18427 5.47266 5.81323

However, when I change the datatype of count to tf.int32, it works as expected.

Ubuntu 16.04 64 Bit
TF Version: v1.2.0-5-g435cdfc 1.2.1
Python 3.5.2
"
13522,Cannot register 2 metrics with the same name error.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Swift API for TensorFlow
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
macOS 10.12.6 (16G29), Darwin MacBookPro.local 16.7.0 Darwin Kernel Version 16.7.0
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
master branch [d864489]
- **Python version**: 
Python 2.7.13
- **Bazel version (if compiling from source)**:
bazel release 0.5.4
- **CUDA/cuDNN version**:
no
- **GPU model and memory**:
no
- **Exact command to reproduce**:

Dear TensorFlow contributors, 
 I am working on swift height-level API for TensorFlow. To provide that, I am [using system module](https://github.com/apple/swift-package-manager/blob/master/Documentation/Usage.md#require-system-libraries) to provide access to C and C++ API. C API is clear in swift code, for C++ library I am writing my own wrappers. After update master branch I can't pass tests in my framework. Launch tests leads to error: 
> Cannot register 2 metrics with the same name: /tensorflow/cc/saved_model/load_attempt_count

My guess is:
After commit [a674130] ''Expose C API symbols on OS X"", all C API available on C++ library. 
[more info](https://github.com/tensorflow/tensorflow/pull/12741)
You can see all C interfaces are available at C++ library:
`
$ nm bazel-bin/tensorflow/libtensorflow_cc.so | grep 'TF_New'
000000000000f5f0 T _TF_NewBuffer
000000000000f610 T _TF_NewBufferFromString
000000000000f6b0 T _TF_NewDeprecatedSession
00000000000176f0 T _TF_NewGraph
0000000000017aa0 T _TF_NewImportGraphDefOptions
0000000000013190 T _TF_NewOperation
000000000001a170 T _TF_NewSession
000000000000f500 T _TF_NewSessionOptions
000000000000ea90 T _TF_NewStatus
000000000000ee80 T _TF_NewTensor
0000000000018a70 T _TF_NewWhile
000000000001cbb0 t __ZZ22TF_NewBufferFromStringEN3$_08__invokeEPvm
`

So, my question/request is: 
1) Is it well-considered and final decision to provide C API in C++ library?
2) Will it be default and public configuration to provide C API in C++ library?
3) Is there any way to configure and build tensorflow library without changing 
`tools/tf_env_collect.sh` file?
Thank you for your work. "
13521,complex gradient update in optimization,"Is there a plan to allow complex optimization in Tensorflow in the future?

When you try to do it with version 1.3, you can calculate and evaluate gradients, but you cannot apply them with opt.apply_gradients(grds_and_vars). The error message you get is: 

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'ApplyAdadelta' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No

- **OS Platform and Distribution 
Distributor ID: Debian
Description:    Debian GNU/Linux 8.9 (jessie)
Release:        8.9

- **TensorFlow installed from (source or binary)**:
using pip install in a virtual conda environment

- **TensorFlow version (use command below)**:
1.3

- **Python version**: 
Python 3.5.2 |Anaconda 4.3.0 (64-bit)| (default, Jul  2 2016, 17:53:06) 

- **Bazel version (if compiling from source)**:
not applicable

- **CUDA/cuDNN version**:
8.0/6.0

- **GPU model and memory**:
Tesla K40c, 11439MiB

- **Exact command to reproduce**:
not necessary, since feature request/question

### Describe the problem
problem description above

### Source code / logs
not applicable
"
13520,issue installing Tensorflow on NVIDIA Jetson TX2,"Hello

I am following this tutorial from Jetsonhacks to install Tensorflow on my NVIDIA Jetson TX2 board: http://www.jetsonhacks.com/2017/04/02/tensorflow-on-nvidia-jetson-tx2-development-kit/ 

The situation: 

I ran a couple of provided scripts and seems like I ran into a bug, based on the error message I got. 

    - I haven't set up any swap memory
    - normally CUDNN and CUDA should be properly installed, I installed them remotely via Jetpack
    - df -h returns:


    $ df -h
    Filesystem      Size  Used Avail Use% Mounted on
    /dev/mmcblk0p1   28G   19G  7.6G  71% /
    none            7.0G     0  7.0G   0% /dev
    tmpfs           7.7G  264K  7.7G   1% /dev/shm
    tmpfs           7.7G   14M  7.7G   1% /run
    tmpfs           5.0M  4.0K  5.0M   1% /run/lock
    tmpfs           7.7G     0  7.7G   0% /sys/fs/cgroup
    tmpfs           786M   72K  786M   1% /run/user/1001



The issue:

When running the script provided by jetsonhacks: $ ./buildTensorFlow.sh
I get this error message:

    ERROR: /home/nvidia/tensorflow/tensorflow/core/kernels/BUILD:2183:1: C++ compilation of rule '//tensorflow/core/kernels:svd_op'     failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command 
      (cd /home/nvidia/.cache/bazel/_bazel_nvidia/d2751a49dacf4cb14a513ec663770624/execroot/org_tensorflow && \
      exec env - \
        CUDA_TOOLKIT_PATH=/usr/local/cuda \
        CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu \
        GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
        LD_LIBRARY_PATH=/home/nvidia/torch/install/lib:/home/nvidia/torch/install/lib: \
        PATH=/home/nvidia/torch/install/bin:/home/nvidia/torch/install/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
        PWD=/proc/self/cwd \
        PYTHON_BIN_PATH=/usr/bin/python \
        PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
        TF_CUDA_CLANG=0 \
        TF_CUDA_COMPUTE_CAPABILITIES=6.2 \
        TF_CUDA_VERSION=8.0 \
        TF_CUDNN_VERSION=5.1.10 \
        TF_NEED_CUDA=1 \
        TF_NEED_OPENCL=0 \

      external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE         '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-    frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/local_linux-opt/bin    /tensorflow/core/kernels/_objs/svd_op/tensorflow/core/kernels/svd_op_complex64.pic.d '-frandom-seed=bazel-out/local_linux-    opt/bin/tensorflow/core/kernels/_objs/svd_op/tensorflow/core/kernels/svd_op_complex64.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DSNAPPY -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-    opt/genfiles/external/bazel_tools -iquote external/protobuf -iquote bazel-out/local_linux-opt/genfiles/external/protobuf -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local_linux-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local_linux-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local_linux-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_linux-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_linux-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/local_linux-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/local_linux-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-opt/genfiles/external/zlib_archive -iquote external/snappy -iquote bazel-out/local_linux-opt/genfiles/external/snappy -iquote external/local_config_cuda -iquote bazel-out/local_linux-opt/genfiles/external/local_config_cuda -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/external/protobuf/src -isystem external/eigen_archive -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local_linux-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local_linux-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local_linux-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_linux-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda/cuda/include -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-DGOOGLE_CUDA=1' -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined         '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c     tensorflow/core/kernels/svd_op_complex64.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/svd_op/tensorflow/core/kernels/svd_op_complex64.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.
    gcc: internal compiler error: Killed (program cc1plus)
    Please submit a full bug report,
    with preprocessed source if appropriate.
    See <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.
    Target //tensorflow/tools/pip_package:build_pip_package failed to build
    INFO: Elapsed time: 5924.737s, Critical Path: 813.48s[/code]
    

after this error message I can't run this script: 

    $ ./packageTensorFlow.sh
    ./packageTensorFlow.sh: line 3: cd: /home/nvidia/tensorflow: No such file or directory
    ./packageTensorFlow.sh: line 4: bazel-bin/tensorflow/tools/pip_package/build_pip_package: No such file or directory
    mv: cannot stat '/tmp/tensorflow_pkg/tensorflow-*.whl': No such file or directory


What can I do to solve this issue so I can install Tensorflow as shown in this tutorial?

"
13519,"Documentation mentions FeatureValueToId, but it's not found anywhere on the site.","The latest documentation on tensorflow.org expains that for embedding_lookup* functions ids are obtained typically from FeatureValueTold. The last term is not found anywhere else on the site. Probably some remnant from ancient functions.

It should probably be replaced with index_table_from* or something similar.
"
13516,"When using placeholder in MonitoredTrainingSession, summary called at first and not feed placeholder error","### System information

== cat /etc/issue ===============================================
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== check pips ===================================================
numpy (1.13.3)
protobuf (3.4.0)
tensorflow-gpu (1.3.0)
tensorflow-tensorboard (0.1.5)

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH :/usr/local/cuda-8.0/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri Oct  6 11:16:46 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX TIT...  Off  | 0000:01:00.0     Off |                  N/A |
|  0%   54C    P0    52W / 250W |      0MiB / 12205MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7


### Describe the problem
I am using MonitoredTrainingSession. With both `tf.summary.scalar('test', d)` in graph and `checkpoint_dir='/temp',` as param, the error occurs. I assume it tries to summary all at the first and run `tf.summary.scalar('test', d)`. But I just want to run `a` and `b` to get the value, not activate any things relate to placeholder `h`. For people who will ask why you can't just feed a int, code of real I want to do is at last.
I think this is a bug, or I do it in a wrong way?

### Source code / logs
```
import tensorflow as tf
    
with tf.Graph().as_default():
    global_step = tf.contrib.framework.get_or_create_global_step()

    a = tf.constant(1, dtype=tf.int64)
    b = tf.constant(2, dtype=tf.int64)
    c = tf.add(a, b)

    h = tf.placeholder(tf.int64)
    d = tf.multiply(c, h)
    tf.summary.scalar('test', d)

    with tf.train.MonitoredTrainingSession(
            checkpoint_dir='/temp',
    ) as sess:
        a_val, b_val = sess.run([a, b])
```
```
InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype int64
	 [[Node: Placeholder = Placeholder[dtype=DT_INT64, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
	 [[Node: Const/_35 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_16_Const"", tensor_type=DT_INT64, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

```


**Code I real want to do**
```
import tensorflow as tf

with tf.Graph().as_default():
    global_step = tf.contrib.framework.get_or_create_global_step()
    dataset_train = tf.contrib.data.Dataset.range(10)
    dataset_val = tf.contrib.data.Dataset.range(90, 100)

    iter_train_handle = dataset_train.make_one_shot_iterator().string_handle()
    iter_val_handle = dataset_val.make_one_shot_iterator().string_handle()

    handle = tf.placeholder(tf.string, shape=[])
    iterator = tf.contrib.data.Iterator.from_string_handle(
        handle, dataset_train.output_types, dataset_train.output_shapes)
    next_batch = iterator.get_next()
    tf.summary.scalar('test', next_batch)

    with tf.train.MonitoredTrainingSession(
            checkpoint_dir='/temp',
    ) as sess:
        handle_train, handle_val = sess.run([iter_train_handle, iter_val_handle])
        for step in range(10):
            print('train', sess.run(next_batch, feed_dict={handle: handle_train}))
            if step % 3 == 0:
                print('val', sess.run(next_batch, feed_dict={handle: handle_val}))
```"
13511,TensorForest: feature request - multi output label support,"Looking at the code for TensorForest, it appears that it only allows: multiclass (including binary) classification or regression (scalar and vector). However, **there does not seem to be an obvious way to do a multi output classification setup** (in which there are multiple binary tasks that should all be predicted by a single model). Could this feature be implemented, or would you be able to guide in the easiest hack to convert to a multi output setup? Apologies if this should actually go to Stack Overflow, please let me know. Thank you!!"
13508,Iterator.get_next() returning a tensor of shape (),"### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution**: Windows 10 (also tested on Ubuntu 16.04)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: b'unknown' 1.3.0
- **Python version**: 3.5.3
- **CUDA/cuDNN version**: 8.0/6.0

### Describe the problem
Hello,
I am currently trying to use the Dataset API with TFRecords. Using a (tested) dataset I made and refactoring working code, I wanted to switch from queues to Datasets. With the following code:
 
```Python
def parse_function(serialized_example):

    # Context features
    context_features = {
         ""label"": tf.FixedLenFeature([], dtype=tf.int64),
    }

    # Sequence features
    sequence_features = {
         ""positions"": tf.FixedLenSequenceFeature([], dtype=tf.string),
    }

    # Parsing the TFRecord
    context_parsed, sequence_parsed = tf.parse_single_sequence_example(
        serialized=serialized_example,
        context_features=context_features,
        sequence_features=sequence_features)

    label = tf.cast(context_parsed['label'], tf.int32)
    sequence = tf.decode_raw(sequence_parsed['positions'], tf.float32)

    return tf.reshape(sequence, [height, width, channels]), label

def main(_):
    dataset = tensorflow.contrib.data.TFRecordDataset(""path"")
    dataset.map(parse_function)
    dataset.batch(64)

    iterator = dataset.make_one_shot_iterator()
    sequences, labels = iterator.get_next()
    ...

```
the last line will raise a TypeError(""'Tensor' object is not iterable.""). iterator.get_next() returns in this case a tensor with shape (), but I believe it should return a tuple of batched sequences and labels, as defined in my parse_function. I tried with every iterator, in or out of a session, but no change.

Is it a bug or am I doing something wrong? Examples from https://www.tensorflow.org/programmers_guide/datasets#decoding_image_data_and_resizing_it
really suggest this is the way to go.

Thanks,
Quentin"
13507,AttributeError in distributed training,"Im am running tensorflow gpu '0.12.1' installed in a virtualenv on Debian 9.1 with cuda 8 and cudnn 5.1.

I tried to run the tutorial from https://www.tensorflow.org/versions/r1.2/deploy/distributed

I started 2 servers and 2 workers like in the tutorial. The servers started as expected.

I run this command to start a worker:
```bash
python cluster_trainer.py \
  --ps_hosts=131.188.30.144:2222,131.188.30.142:2222 \
  --worker_hosts=131.188.30.134:2222,131.188.30.135:2222 \
  --job_name=worker --task_index=1
```
The works exited with the error message:
```
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1050 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.392
pciBusID 0000:02:00.0
Total memory: 3.94GiB
Free memory: 3.87GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:02:00.0)
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job ps -> {0 -> 131.188.30.144:2222, 1 -> 131.188.30.142:2222}
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222, 1 -> 131.188.30.135:2222}
I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:211] Started server with target: grpc://localhost:2222
['131.188.30.144:2222', '131.188.30.142:2222'] ['131.188.30.134:2222', '131.188.30.135:2222'] worker 0
Traceback (most recent call last):
  File ""cluster_trainer.py"", line 85, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/cip/2016/ko01jaxu/lib/tensorflow/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""cluster_trainer.py"", line 36, in main
    loss, global_step=global_step)
  File ""/home/cip/2016/ko01jaxu/lib/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 269, in minimize
    grad_loss=grad_loss)
  File ""/home/cip/2016/ko01jaxu/lib/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 320, in compute_gradients
    self._assert_valid_dtypes([loss])
  File ""/home/cip/2016/ko01jaxu/lib/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 460, in _assert_valid_dtypes
    dtype = t.dtype.base_dtype
AttributeError: 'ellipsis' object has no attribute 'dtype'
```
"
13506,tf.image.pad_to_bounding_box crashes when passed bounds with dtype int64,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:from pip in virtualenv
- **TensorFlow version (use command below)**:v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: '3.5.2 (default, Nov 17 2016, 17:05:23) \n[GCC 5.4.0 20160609]'
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

## Description

Passing arguments of type int64 to `tf.image.pad_to_bounding_box` triggers a crash of the python interpreter. This is a bug because the type required by `tf.image.pad_to_bounding_box` not documented anywhere and just causes a crash with a cryptic error message.

## Sources/Logs

The following snippet crashes the whole python interpreter with a core dump.

    import tensorflow as tf
    i = tf.constant([0, 0, 3, 3], dtype=tf.int64)
    img = tf.ones([1,1,1], dtype=tf.float32)
    sess = tf.Session()
    sess.run(tf.image.pad_to_bounding_box(img, i[0], i[1], i[2], i[3]))

And leaves the following 

    2017-10-05 13:51:24.789715: F tensorflow/core/framework/tensor.cc:493] Check failed: dtype() == expected_dtype (9 vs. 3)
"
13501,Feature Request: collections scope,"The is currently no way to assign variables to collections during creation time other than specifying in the explicit function call to `tf.get_variable` or `tf.Variable`.  This is problematic anytime you'd like to use existing functions that create variables as part of their calls and you want those variables to be part of a certain collection.  It's even more problematic when you want variables to not be members of  `GraphKeys.GLOBAL_VARIABLES`, because this is the default for the `tf.get_variable` and `tf.Variable`. 
"
13500,"Tensorflow binary seems compiled to use SIMD instructions like AVX2 and FMA, but actually not?","I found similar issues mentioned as #8037, #7778 etc, but the issue seems not solved: the warnings did disappear after building with the necessary optimization options, but they appeared again when I followed this tutorial (https://www.tensorflow.org/performance/xla/tfcompile) to the last step. So, is the tensorflow binary compiled to use the SIMD instructions or not?

### System information
- **Have I written custom code**: No
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: source
- **TensorFlow version**: v1.3.0-rc1-3000-g840dcae
- **Python version**: Python3
- **Bazel version**: 0.6.0
- **CPU**: Intel Core i7-4770, Haswell architecture, supporting AVX2 and FMA
- **GPU**: No
- **Compiler**: gcc 5.4.0

### Issue reproducing:
1. Building tensorflow from source:

- Configure: only jemalloc and XLA JIT support are ticked. The default optimization flag is `-march=native`, therefore was not specified;

- Build pip package:
```
bazel build --config=opt --copt=-mavx2 --copt=-mfma --config=mkl --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package
``` 
- Install pip package:
```
sudo -H python3 -m pip install /tmp/tensorflow_pkg/tensorflow-1.3.0-cp35-cp35m-linux_x86_64.whl
```
- The installation was validated using the ""Hello, TensorFlow!"" example, and no warnings are generated.

2. Generating `tfcompile` binary:
```
bazel build --config=opt --copt=-mavx2 --copt=-mfma --config=mkl --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/compiler/aot:tfcompile
```

3. Follow the tutorial here https://www.tensorflow.org/performance/xla/tfcompile, in the directory `//tensorflow/compiler/aot/tests`:
- Step 1: The config file already exists as `test_graph_tfmatmul.config.pbtxt`;

- Step 2.1: Generate the graph file `test_graph_tfmatmul.pb`:
```
python3 ./make_test_graphs.py --out_dir=./
```
- Step 2.2: Compile the graph using `tfcompile`:
```
~/tensorFlow_src/tensorflow/bazel-bin/tensorflow/compiler/aot/tfcompile --graph=""./test_graph_tfmatmul.pb"" --config=""./test_graph_tfmatmul.config.pbtxt"" --entry_point=""test_graph_tfmatmul"" --cpp_class=""foo::bar::MatMulComp"" --out_object=""test_graph_tfmatmul.o"" --out_header=""test_graph_tfmatmul.h"" --target_features=""+avx2""
```

- Step 3: Creating a file named `my_code.cc`:
```
#define EIGEN_USE_THREADS
#define EIGEN_USE_CUSTOM_THREAD_POOL

#include <iostream>
#include ""third_party/eigen3/unsupported/Eigen/CXX11/Tensor""
#include ""tensorflow/compiler/aot/tests/test_graph_tfmatmul.h"" // generated

int main(int argc, char** argv) {
    Eigen::ThreadPool tp(2);  // Size the thread pool as appropriate.
    Eigen::ThreadPoolDevice device(&tp, tp.NumThreads());

    foo::bar::MatMulComp matmul;
    matmul.set_thread_pool(&device);

    // Set up args and run the computation.
    const float args[12] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12};
    std::copy(args + 0, args + 6, matmul.arg0_data());
    std::copy(args + 6, args + 12, matmul.arg1_data());
    matmul.Run();

    // Check result
    if (matmul.result0(0, 0) == 58) {
        std::cout << ""Success"" << std::endl;
    } else {
        std::cout << ""Failed. Expected value 58 at 0,0. Got:""
                    << matmul.result0(0, 0) << std::endl;
    }

    return 0;
}
```

- Step 4.1: Create the `BUILD` file:
```
# Example of linking your binary
# Also see //third_party/tensorflow/compiler/aot/tests/BUILD
load(""//tensorflow/compiler/aot:tfcompile.bzl"", ""tf_library"")

# The same tf_library call from step 2 above.
tf_library(
    name = ""test_graph_tfmatmul"",
    cpp_class = ""foo::bar::MatMulComp"",
    graph = ""test_graph_tfmatmul.pb"",
    config = ""test_graph_tfmatmul.config.pbtxt"",
)

# The executable code generated by tf_library can then be linked into your code.
cc_binary(
    name = ""my_binary"",
    srcs = [
        ""my_code.cc"",  # include test_graph_tfmatmul.h to access the generated header
    ],
    deps = [
        "":test_graph_tfmatmul"",  # link in the generated object file
        ""//third_party/eigen3"",
    ],
    linkopts = [
        ""-lpthread"",
    ]
)
```

- Step 4.2: Create the final binary:
```
bazel build --config=opt --copt=-mavx2 --copt=-mfma --config=mkl --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/compiler/aot/tests:my_binary
```

Finally, it will print:
`INFO: From Executing genrule //tensorflow/compiler/aot/tests:gen_test_graph_tfmatmul:
2017-10-05 15:15:29.233159: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA`
(An error will also occur, but that is another issue #13482).

So, is the tensorflow binary compiled to use the SIMD instructions (SSE4.1 SSE4.2 AVX AVX2 FMA) or not? May I have your advice?
"
13499,Using third party library in custom op implementation with GPU memory manipulation,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 14.04.5
- **TensorFlow installed from (source or binary)**:pip
- **TensorFlow version (use command below)**:1.2.0
- **Python version**: 2.7.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:CUDA 8.0
- **GPU model and memory**:GeForce GTX 1080 8G
- **Exact command to reproduce**:


### Describe the problem

To speed up the model's training and evaluating process, and to make the model more flexible, people would like to use some third party libraries for their customized ops. For example, CUDPP, http://cudpp.github.io/, allows people to build a hash table on GPU.

However, I find that TensorFlow does not allow people to manipulate GPU memory by themselves, but only to use allocate_temp in the compute function, which is very inconvenient and inflexible. And these good third party libraries involve many GPU memory manipulations. I cannot find much information about this, but I met some errors in allocating memories.

* What's the reason behind this prohibition?
* Is there some method to allow us manipulate GPU by ourselves? (some switches, some options)
* If we cannot manipulate GPU memory, then how can we use/adapt these third party GPU libraries for the customized ops? What's the good programming practice?

More specifically, the problem I met is that I implemented a customized op on GPU, using the multivalue hash table from the CUDPP library. For very small-scale data, the customized op passed the test. But when I use larger data, and incorporate the customized op into a larger model, then I cannot allocate the memory on CUDA. https://stackoverflow.com/questions/40183189/trouble-compiling-with-custom-tensorflow-gpu-op, this expalins something. I can avoid manipulating GPU memory myself, but the CUDPP library needs to manipulate GPU memory.



### Source code / logs
The cudpp git repo: https://github.com/cudpp/cudpp/compare?expand=1.

My OpKernel:
```C++
void querySquarePointLauncher(int b, int n, int m, float grid_size, int nsample, const float *all_xyz, const float *centroids_xyz, const float *limits, const int *sizes, int *idx, int *pts_cnt, unsigned int *d_keys, unsigned int *d_vals, unsigned int *d_queries, uint2 *d_vals_multivalue);
class QuerySquarePointGpuOp : public OpKernel {
    public:
        explicit QuerySquarePointGpuOp(OpKernelConstruction* context) : OpKernel(context) {
            OP_REQUIRES_OK(context, context->GetAttr(""grid_size"", &grid_size_));
            OP_REQUIRES(context, grid_size_ > 0, errors::InvalidArgument(""QuerySquarePoint expects positive grid size""));

            OP_REQUIRES_OK(context, context->GetAttr(""nsample"", &nsample_));
            OP_REQUIRES(context, nsample_ > 0, errors::InvalidArgument(""QuerySquarePoint expects positive nsample""));
        }

        void Compute(OpKernelContext* context) override {
            const Tensor& all_xyz_tensor = context->input(0);
            OP_REQUIRES(context, all_xyz_tensor.dims()==3 && all_xyz_tensor.shape().dim_size(2)==3, errors::InvalidArgument(""QuerySquarePoint expects (batch_size, ndataset, 3) all_xyz_tensor shape.""));
            int b = all_xyz_tensor.shape().dim_size(0);
            int n = all_xyz_tensor.shape().dim_size(1);

            const Tensor& centroids_xyz_tensor = context->input(1);
            OP_REQUIRES(context, centroids_xyz_tensor.dims()==3 && centroids_xyz_tensor.shape().dim_size(2)==3, errors::InvalidArgument(""QuerySquarePoint expects (batch_size, npoint, 3) centroids_xyz shape.""));
            int m = centroids_xyz_tensor.shape().dim_size(1);
            
            const Tensor& limits_tensor = context->input(2);
            OP_REQUIRES(context, limits_tensor.dims()==1 && limits_tensor.shape().dim_size(0)==6, errors::InvalidArgument(""QuerySquarePoint expects (6) limits shape.""))
            
            const Tensor& sizes_tensor = context->input(3);
            OP_REQUIRES(context, sizes_tensor.dims()==1 && sizes_tensor.shape().dim_size(0) == 3, errors::InvalidArgument(""QuerySquarePoint expects (3) sizes shape.""))

            Tensor *idx_tensor = nullptr;
            OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape{b,m,nsample_}, &idx_tensor));
            Tensor *pts_cnt_tensor = nullptr;
            OP_REQUIRES_OK(context, context->allocate_output(1, TensorShape{b,m}, &pts_cnt_tensor));
            
            Tensor keys_tensor;
            OP_REQUIRES_OK(context, context->allocate_temp(DT_INT32, TensorShape({b*n}), &keys_tensor));
            
            Tensor vals_tensor;
            OP_REQUIRES_OK(context, context->allocate_temp(DT_INT32, TensorShape({b*n}), &vals_tensor));
            
            Tensor vals_multivalue_tensor;
            OP_REQUIRES_OK(context, context->allocate_temp(DT_INT32, TensorShape({b*m*27*2}), &vals_multivalue_tensor));
            
            Tensor queries_tensor;
            OP_REQUIRES_OK(context, context->allocate_temp(DT_INT32, TensorShape({b*m*27}), &queries_tensor));
            

            auto all_xyz_flat = all_xyz_tensor.flat<float>();
            const float *all_xyz = &(all_xyz_flat(0));
            
            auto centroids_xyz_flat = centroids_xyz_tensor.flat<float>();
            const float *centroids_xyz = &(centroids_xyz_flat(0));
            
            auto limits_flat = limits_tensor.flat<float>();
            const float *limits = &(limits_flat(0));
            
            auto sizes_flat = sizes_tensor.flat<int>();
            const int *sizes = &(sizes_flat(0));
            
            auto idx_flat = idx_tensor->flat<int>();
            int *idx = &(idx_flat(0));
            auto pts_cnt_flat = pts_cnt_tensor->flat<int>();
            int *pts_cnt = &(pts_cnt_flat(0));
            
            auto keys_flat = keys_tensor.flat<int>();
            unsigned int *keys = (unsigned int *)&(keys_flat(0));
            
            auto vals_flat = vals_tensor.flat<int>();
            unsigned int *vals = (unsigned int *)&(vals_flat(0));
            
            auto queries_flat = queries_tensor.flat<int>();
            unsigned int *queries = (unsigned int *)&(queries_flat(0));
            
            auto vals_multivalue_flat = vals_multivalue_tensor.flat<int>();
            uint2 *vals_multivalue = reinterpret_cast<uint2*> (&(vals_multivalue_flat(0)));
            
            printf(""Before launcher in cpp\n"");
            
            querySquarePointLauncher(b, n, m, grid_size_, nsample_, all_xyz, centroids_xyz, limits, sizes, idx, pts_cnt, keys, vals, queries, vals_multivalue);         
        }
    private:
        float grid_size_;
        int nsample_;
};
REGISTER_KERNEL_BUILDER(Name(""QuerySquarePoint"").Device(DEVICE_GPU), QuerySquarePointGpuOp);
```

The CUDA implementation:
```C++
__global__ void compose_insert_items(int b, int n, float grid_size, const float *all_xyz, const float *limits, const int *sizes, unsigned int *d_keys, unsigned int *d_vals){
    int index = threadIdx.x;
   
    if(index < n){
        int batch_index = blockIdx.x;
        all_xyz += batch_index * n * 3;
        unsigned int *tmp_d_keys = d_keys + batch_index * n;
        unsigned int *tmp_d_vals = d_vals + batch_index * n;
        int stride = blockDim.x;
        
        for(int point_idx = index; point_idx < n; point_idx += stride){
            unsigned int x_idx = __float2uint_rd((all_xyz[point_idx*3] - limits[0]) / grid_size) + 1;
            unsigned int y_idx = __float2uint_rd((all_xyz[point_idx*3+1] - limits[2]) / grid_size) + 1;
            unsigned int z_idx = __float2uint_rd((all_xyz[point_idx*3+2] - limits[4]) / grid_size) + 1;
            
            tmp_d_keys[point_idx] = z_idx + sizes[2] * (y_idx + sizes[1] * (x_idx + batch_index * sizes[0]));
            tmp_d_vals[point_idx] = point_idx;
        }
    }
}
//compose_queries<<<b,256>>>(b, m, grid_size, centroids_xyz, limits, sizes, d_queries);
__global__ void compose_queries(int b, int m, float grid_size, const float *centroids_xyz, const float *limits, const int *sizes, unsigned int *d_queries){

    int index = threadIdx.x;
    
    if(index < m){
        int stride = blockDim.x;
        int batch_index = blockIdx.x;
        centroids_xyz += batch_index * m * 3;
        unsigned int *tmp_d_queries = d_queries + batch_index * m * 27;
        
        unsigned int x_idx = __float2uint_rd((centroids_xyz[index*3] - limits[0]) / grid_size);
        unsigned int y_idx = __float2uint_rd((centroids_xyz[index*3+1] - limits[2]) / grid_size);
        unsigned int z_idx = __float2uint_rd((centroids_xyz[index*3+2] - limits[4]) / grid_size);
        
        int cnt = 0;
        for(int x_offset = 0; x_offset < 3; x_offset++){
            for(int y_offset = 0; y_offset < 3; y_offset++){
                for(int z_offset = 0; z_offset < 3; z_offset++){
                    tmp_d_queries[index*27+cnt] = z_idx + z_offset + sizes[2] * (y_idx + y_offset + sizes[1] * (x_idx + x_offset + batch_index * sizes[0]));  
                    cnt++;
                }
            }
        }

    }
}
__global__ void hash_square_idx_gpu(int b, int n, int m, int nsample, const uint2 *d_vals_multivalue, const unsigned int * d_all_values, int *idx, int *pts_cnt){
    int index = threadIdx.x;
    if(index < m){
        int stride = blockDim.x;
        int batch_index = blockIdx.x;
        unsigned int sorted_idx[27] = {13, 4,10,12,14,16,22, 1,3,5,7,9,11,15,17,19,21,23,25,  0,2,6,8,18,20,24,26};
                
        idx += batch_index * m * nsample;
        pts_cnt += batch_index * m;
        int query_idx_base = batch_index*m*27+index*27;
        
        int cnt = 0;
        for(int i = 0; i < 27; i++){
            int query_idx = query_idx_base + sorted_idx[i];
            unsigned int num_values = d_vals_multivalue[query_idx].y;
            if(num_values > 0){
                for(unsigned int j = 0; j < num_values && cnt < nsample; j++){
                    idx[index*nsample + cnt] = d_all_values[d_vals_multivalue[query_idx].x + j];
                    cnt++;
                }
            }
        }
        pts_cnt[index] = cnt;
        for(;cnt < nsample;cnt++){
            idx[index*nsample + cnt] = idx[index*nsample];
        }
    }
}

void querySquarePointLauncher(int b, int n, int m, float grid_size, int nsample, const float *all_xyz, const float *centroids_xyz, const float *limits, const int *sizes, int *idx, int *pts_cnt, unsigned int *d_keys, unsigned int *d_vals, unsigned int *d_queries, uint2 *d_vals_multivalue) {
    printf(""Start\n"");    
    unsigned int kInputSize = b * n;
    printf(""b %d, n %d, kInputSize: %u\n"", b, n, kInputSize);
    
    compose_insert_items<<<b,256>>>(b, n, grid_size, all_xyz, limits, sizes, d_keys, d_vals);
    cudaDeviceSynchronize();
    
    CUDPPHandle theCudpp;
    CUDPPResult result = cudppCreate(&theCudpp);
    if (result != CUDPP_SUCCESS){
        fprintf(stderr, ""Error initializing CUDPP Library.\n"");
        exit(-1);
    }

    CUDPPHashTableConfig config;
    config.type = CUDPP_MULTIVALUE_HASH_TABLE;
    config.kInputSize = kInputSize;
    config.space_usage = 2.0f;
    CUDPPHandle hash_table_handle;
    result = cudppHashTable(theCudpp, &hash_table_handle, &config);
    if (result != CUDPP_SUCCESS){
        fprintf(stderr, ""Error in cudppHashTable call in""
                ""testHashTable (make sure your device is at""
                ""least compute version 2.0\n"");
    }
    
    result = cudppHashInsert(hash_table_handle, d_keys,
                                d_vals, kInputSize);
    cudaThreadSynchronize();
    printf(""insert values\n"");
    if (result != CUDPP_SUCCESS){
        fprintf(stderr, ""Error in cudppHashInsert call in""
                ""testHashTable\n"");
    }
    
    unsigned int values_size;
    if (cudppMultivalueHashGetValuesSize(hash_table_handle,
                                    &values_size) !=
                                    CUDPP_SUCCESS){
        fprintf(stderr, ""Error: ""
                ""cudppMultivalueHashGetValuesSize()\n"");
    }
    
    unsigned int * d_all_values = NULL;
    if (cudppMultivalueHashGetAllValues(hash_table_handle,
                                        &d_all_values) !=
                                        CUDPP_SUCCESS){
        fprintf(stderr, ""Error: ""
                ""cudppMultivalueHashGetAllValues()\n"");
    }
    
    compose_queries<<<b,256>>>(b, m, grid_size, centroids_xyz, limits, sizes, d_queries);
    cudaDeviceSynchronize();
    
    result = cudppHashRetrieve(hash_table_handle,
                                d_queries,
                                d_vals_multivalue,
                                b * m * 27);
    cudaThreadSynchronize();
    printf(""retrieved values\n"");
    if (result != CUDPP_SUCCESS){
        fprintf(stderr, ""Error in cudppHashRetrieve call\n"");
    }

    hash_square_idx_gpu<<<b,256>>>(b, n, m, nsample, d_vals_multivalue, d_all_values, idx, pts_cnt);
    cudaDeviceSynchronize();
    printf(""obtain idx\n"");
    
    result = cudppDestroyHashTable(theCudpp, hash_table_handle);
    if (result != CUDPP_SUCCESS){
        fprintf(stderr, ""Error in cudppDestroyHashTable call in""
                ""testHashTable\n"");
    }

    result = cudppDestroy(theCudpp);
    if (result != CUDPP_SUCCESS){
        printf(""Error shutting down CUDPP Library.\n"");
    }
    printf(""Ends\n"");
}
```


"
13498,"Problem with ""AddControlInput"" in python_api.h","@asimshankar @skye I'm observing an issue with `AddControlInput` in `python_api.h`. It seems to not be working for me actually and I'm not sure why. Everything is ok if I add the control dependency during op creation. However, if I add it right after, it's not enforced during execution. A simple example is creating a `switch` op, creating two constant ops and adding one control dependency for each on each switch output, and then feeding these two constant ops into a `merge`op. The result will be whichever constant op was fed first into the merge op (i.e., the control dependencies are not satisfied and both ops are executed). If I add the control dependency during op construction, all is good.

Note that the control input does show up in the `GraphDef` that I generated after the call. It's simply not enforced during execution.

Do you have any idea why that might be happening? I am aware that this is not part of the public API and thus is not stable, but it is still unexpected behavior. I am using that in my implementation of control flow ops and gradients (I ended up re-implementing it based on the Python version because I depend on it and the C++ functionality is currently very limited -- I'll update on this later on, once I release it)."
13492,Can't run new ops in new session after sess.run RuntimeError,"Scenario:
1. runtime error during session.run
2. create new session
3. new session fails to evaluate any tensor, giving same runtime error as in step 1.

The following example fails to evaluate `tf.constant` with error `tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Diag`

Using 12-day old version from this commit: https://github.com/tensorflow/tensorflow/commit/ea94bbe9fa9f9b3d01fb057c02ef7873d76bf09c

```
import tensorflow as tf
def main():
  
  sess = tf.Session()
  with tf.device(""/gpu:0""):
    bad_op = tf.diag([1, 1])
    
  print(""About to run tensor "", bad_op)
  try:
    sess.run(bad_op)
  except:
    print(""First run failed, trying something else"")

  sess = tf.Session()
  good_op = tf.constant(1)
  sess.run(good_op)

main()
```

I thought it's something to do with `TF_ExtendGraph` not being called, but manually adding ExtendGraph before second call results in segmentation fault


```
import tensorflow as tf
def main():
  
  sess = tf.Session()
  with tf.device(""/gpu:0""):
    bad_op = tf.diag([1, 1])
    
  print(""About to run tensor "", bad_op)
  try:
    sess.run(bad_op)
  except:
    print(""First run failed, trying something else"")

  sess = tf.Session()
  good_op = tf.constant(1)
  from tensorflow.python import pywrap_tensorflow as tf_session
  from tensorflow.python.framework import errors
  with errors.raise_exception_on_not_ok_status() as status:
    tf_session.TF_ExtendGraph(sess._session, sess.graph.as_graph_def().SerializeToString(), None)
  sess.run(good_op)

main()

```

  
"
13491,Feature Request: support tf.diag on GPU,"This could be a good project for an external contributor, currently no GPU support for tf.diag so the following fails

```
import tensorflow as tf
with tf.device(""/gpu:0""):
  mat = tf.diag([1,1])
sess = tf.Session()
sess.run(mat)
```"
13490,tf.sparse_add doesn't work for tf.SparseTensor that has tf.Variable as values,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 8.0 / cuDNN7
- **GPU model and memory**: 4x GTX1080ti
- **Exact command to reproduce**: see below


### Describe the problem
`tf.sparse_add` raises `RuntimeError` for `tf.SparseTensor` that has `tf.Variable` as `values`.

Simplest example code to reproduce the problem:

    import numpy as np
    import scipy.sparse as sp
    import tensorflow as tf
    
    A = sp.random(100, 100)
    vals = tf.Variable(tf.ones([A.nnz]))
    A_tf = tf.SparseTensor(np.column_stack([A.row, A.col]), vals, A.shape)
    tf.sparse_add(A_tf, A_tf)  # raises RuntimeError

Running this snippet produces the following error message

	--------------------------------------------------------------
	RuntimeError                 Traceback (most recent call last)
	<ipython-input-177-3d37a994a045> in <module>()
	      6 vals = tf.Variable(tf.ones([A.nnz]))
	      7 A_tf = tf.SparseTensor(np.column_stack([A.row, A.col]), vals, A.shape)
	----> 8 tf.sparse_add(A_tf, A_tf)  # raises RuntimeError

	~/.conda/envs/rml/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py in sparse_add(a, b, thresh)
	    297     b = _convert_to_sparse_tensor(b)
	    298     thresh = ops.convert_to_tensor(
	--> 299         thresh, dtype=a.values.dtype.real_dtype, name=""thresh"")
	    300     output_ind, output_val, output_shape = (gen_sparse_ops._sparse_add(
	    301         a.indices, a.values, a.dense_shape,

	~/.conda/envs/rml/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)
	    609       name=name,
	    610       preferred_dtype=preferred_dtype,
	--> 611       as_ref=False)
	    612 
	    613 

	~/.conda/envs/rml/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)
	    688               ""dtype: requested = %s, actual = %s""
	    689               % (error_prefix, conversion_func, base_type,
	--> 690                  dtype.name, ret.dtype.name))
	    691         return ret
	    692   raise TypeError(""%sCannot convert %r with type %s to Tensor: ""

	RuntimeError: thresh: Conversion function <function _constant_tensor_conversion_function at 0x7fe50a9c0ea0> for type <class 'object'> returned incompatible dtype: requested = float32_ref, actual = float32

I found a workaround, but I strongly believe that this is not the intended behavior

	zero_tensor = tf.SparseTensor(np.column_stack([0, 0]), [0.0], A.shape)
	# A_tf = tf.sparse_add(A_tf, zero_tensor)  # still same RuntimeError
	A_tf = tf.sparse_add(zero_tensor, A_tf)  # doesn't change the value of A_tf
	tf.sparse_add(A_tf, A_tf)  # works

### Source code / logs

#### Output of the log file

	== cat /etc/issue ===============================================
	Linux ************** 4.10.0-33-generic #37~16.04.1-Ubuntu SMP Fri Aug 11 14:07:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

	== cat /etc/issue ===============================================
	Linux ************** 4.10.0-33-generic #37~16.04.1-Ubuntu SMP Fri Aug 11 14:07:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
	VERSION=""16.04.3 LTS (Xenial Xerus)""
	VERSION_ID=""16.04""
	VERSION_CODENAME=xenial

	== are we in docker =============================================
	No

	== compiler =====================================================
	c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
	Copyright (C) 2015 Free Software Foundation, Inc.
	This is free software; see the source for copying conditions.  There is NO
	warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


	== uname -a =====================================================
	Linux ************** 4.10.0-33-generic #37~16.04.1-Ubuntu SMP Fri Aug 11 14:07:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

	== check pips ===================================================
	numpy (1.13.1)
	protobuf (3.4.0)
	tensorflow-gpu (1.3.0)
	tensorflow-tensorboard (0.1.6)

	== check for virtualenv =========================================
	False

	== tensorflow import ============================================
	tf.VERSION = 1.3.0
	tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
	tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
	Sanity check: array([1], dtype=int32)

	== env ==========================================================
	LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64
	DYLD_LIBRARY_PATH is unset

	== nvidia-smi ===================================================
	Wed Oct  4 17:59:42 2017       
	+-----------------------------------------------------------------------------+
	| NVIDIA-SMI 375.82                 Driver Version: 375.82                    |
	|-------------------------------+----------------------+----------------------+
	| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
	| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
	|===============================+======================+======================|
	|   0  GeForce GTX 108...  Off  | 0000:02:00.0     Off |                  N/A |
	| 20%   33C    P2    58W / 250W |    445MiB / 11172MiB |      0%      Default |
	+-------------------------------+----------------------+----------------------+
	|   1  GeForce GTX 108...  Off  | 0000:03:00.0     Off |                  N/A |
	| 20%   32C    P2    57W / 250W |    153MiB / 11172MiB |      0%      Default |
	+-------------------------------+----------------------+----------------------+
	|   2  GeForce GTX 108...  Off  | 0000:83:00.0     Off |                  N/A |
	| 20%   31C    P8    10W / 250W |   8671MiB / 11172MiB |      0%      Default |
	+-------------------------------+----------------------+----------------------+
	|   3  GeForce GTX 108...  Off  | 0000:84:00.0     Off |                  N/A |
	| 20%   37C    P8     9W / 250W |    153MiB / 11172MiB |      0%      Default |
	+-------------------------------+----------------------+----------------------+
	                                                                               
	+-----------------------------------------------------------------------------+
	| Processes:                                                       GPU Memory |
	|  GPU       PID  Type  Process name                               Usage      |
	|=============================================================================|
	|    0     19158    C   ****************************************       443MiB |
	|    1     19158    C   ****************************************       151MiB |
	|    2     20686    C   ********************************************  8669MiB |
	|    3     20686    C   ********************************************   151MiB |
	+-----------------------------------------------------------------------------+

	== cuda libs  ===================================================
	/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
	/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
	/usr/local/cuda-8.0/doc/man/man7/libcudart.7
	/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
"
13488,Having issues with bazel build with respect to patch,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I'm just compiling
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Redhat 7
- **TensorFlow installed from (source or binary)**: Installing from source
- **TensorFlow version (use command below)**: Tried 1.0, 1.3 and latest
- **Python version**:  2.7.10
- **Bazel version (if compiling from source)**: 0.5.3
- **CUDA/cuDNN version**: cuda 8/cudnn 5
- **GPU model and memory**:
- **Exact command to reproduce**: bazel --output_base=./cache build -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-mavx2 --copt=-mfma --config=cuda tensorflow_serving/...

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Trying to compile with and without GPU doesn't seem to work for me because of the patch command. For r0.5.1, I can compile successfully without GPU, but for the other later versions, I am getting the following error:

ERROR: error loading package 'tensorflow_serving/resources': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Traceback (most recent call last):
	File ""/serving-1.0/cache/external/org_tensorflow/tensorflow/workspace.bzl"", line 117
		_apply_patch(repo_ctx, repo_ctx.attr.patch_file)
	File ""/serving-1.0/cache/external/org_tensorflow/tensorflow/workspace.bzl"", line 108, in _apply_patch
		_execute_and_check_ret_code(repo_ctx, cmd)
	File ""/serving-1.0/cache/external/org_tensorflow/tensorflow/workspace.bzl"", line 92, in _execute_and_check_ret_code
		fail(""Non-zero return code({1}) when ...))
Non-zero return code(2) when executing 'patch -p1 -d /serving-1.0/cache/external/protobuf -i /serving-1.0/cache/external/org_tensorflow/third_party/protobuf/add_noinlines.patch':
Stdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Hunk #1 succeeded at 701 with fuzz 1 (offset 144 lines).
Hunk #2 succeeded at 803 (offset 147 lines).
Hunk #3 succeeded at 884 (offset 147 lines).

Stderr: patch: setting attribute security.selinux for security.selinux: Permission denied

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13487,"tweak validate_shape to remove ""Assign requires shapes of both tensors to match. lhs shape= [12] rhs shape= [3]"" error","
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No. I am running the Audio recognition tutorial
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:0.6.0
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:python tensorflow/examples/speech_commands/freeze.py \
--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-18000 \
--output_file=/tmp/my_frozen_graph.pb


I am asking about the error (attached below) here(rather than on stackoverflow) because of this related issue https://github.com/tensorflow/tensorflow/issues/5492 . I want to know if my error can be resolved by tweaking the validate_shape parameter,by setting it to false? . If not, please suggest alternatives.

The error is as follows: 

Traceback (most recent call last):
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_call
    return fn(*args)
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1300, in _run_fn
    status, run_metadata)
  File ""/home/cogknit/anaconda3/lib/python3.6/contextlib.py"", line 89, in __exit__
    next(self.gen)
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 467, in raise_exception_on_not_ok_status
    c_api.TF_GetCode(status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [12] rhs shape= [3]
	 [[Node: save/Assign_5 = Assign[T=DT_FLOAT, _class=[""loc:@Variable_5""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_5, save/RestoreV2_5)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tensorflow/examples/speech_commands/freeze.py"", line 180, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""tensorflow/examples/speech_commands/freeze.py"", line 117, in main
    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)
  File ""/home/cogknit/tensorflow/tensorflow/examples/speech_commands/models.py"", line 123, in load_variables_from_checkpoint
    saver.restore(sess, start_checkpoint)
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1657, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1118, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1315, in _do_run
    options, run_metadata)
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [12] rhs shape= [3]
	 [[Node: save/Assign_5 = Assign[T=DT_FLOAT, _class=[""loc:@Variable_5""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_5, save/RestoreV2_5)]]

Caused by op 'save/Assign_5', defined at:
  File ""tensorflow/examples/speech_commands/freeze.py"", line 180, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""tensorflow/examples/speech_commands/freeze.py"", line 117, in main
    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)
  File ""/home/cogknit/tensorflow/tensorflow/examples/speech_commands/models.py"", line 122, in load_variables_from_checkpoint
    saver = tf.train.Saver(tf.global_variables())
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1214, in __init__
    self.build()
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1223, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1259, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 747, in _build_internal
    restore_sequentially, reshape)
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 435, in _AddRestoreOps
    assign_ops.append(saveable.restore(tensors, shapes))
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 160, in restore
    self.op.get_shape().is_fully_defined())
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py"", line 276, in assign
    validate_shape=validate_shape)
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 56, in assign
    use_locking=use_locking, name=name)
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3082, in create_op
    op_def=op_def)
  File ""/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1632, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [12] rhs shape= [3]
	 [[Node: save/Assign_5 = Assign[T=DT_FLOAT, _class=[""loc:@Variable_5""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](Variable_5, save/RestoreV2_5)]]


"
13486,ImportError: No module named 'tensorflowvisu',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13485,tf.Session.list_devices seems to return an (int64 *) for device.memory_limit_bytes and leaks,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

no

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Linux Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:

binary 1.3.0 GPU

- **TensorFlow version (use command below)**:

('v1.3.0-rc2-20-g0787eee', '1.3.0')

- **Python version**: 

Python 2.7.12

- **Bazel version (if compiling from source)**:

N/A

- **CUDA/cuDNN version**:

CUDA 8.0 + cuDNN 6.0

- **GPU model and memory**:

GeForce GTX 960M

- **Exact command to reproduce**:

```python
import tensorflow as tf

with tf.Session() as S:
    for d in S.list_devices():
        print d.name, d.device_type, d.memory_limit_bytes
```

produces:

```bash
2017-10-04 10:31:04.370386: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-04 10:31:04.370409: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-04 10:31:04.370415: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-10-04 10:31:04.370419: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-04 10:31:04.370423: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-10-04 10:31:04.566504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-10-04 10:31:04.566814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX 960M
major: 5 minor: 0 memoryClockRate (GHz) 1.176
pciBusID 0000:01:00.0
Total memory: 3.95GiB
Free memory: 3.92GiB
2017-10-04 10:31:04.566862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2017-10-04 10:31:04.566868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2017-10-04 10:31:04.566875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0)
/job:localhost/replica:0/task:0/device:CPU:0 CPU <Swig Object of type 'int64_t *' at 0x7f5c22f61a50>
/job:localhost/replica:0/task:0/device:GPU:0 GPU <Swig Object of type 'int64_t *' at 0x7f5c22f613f0>
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
```

### Describe the problem

The `memory_limit_bytes` attribute seems to be wrapping an `int64 *` pointer (as opposed to a int64) and swig seems to think it's leaking this pointer. Also, in the docs its referred to as [`memory_limit`](https://www.tensorflow.org/api_docs/python/tf/Session#list_devices)

### Source code / logs

N/A"
13482,Error in creating the final binary using AOT compilation for CPU backend,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.3.0-rc1-2665-g242b0f1
- **Python version**: Python3
- **Bazel version (if compiling from source)**: 0.6.0
- **CUDA/cuDNN version**: No
- **GPU model and memory**: No
- **Exact command to reproduce**: 
bazel build //tensorflow/compiler/aot/tests:my_binary


### Describe the problem
I simply followed the tutorial here: https://www.tensorflow.org/performance/xla/tfcompile

According to Step 1 and 2, I compiled the subgraph and generated the header (`test_graph_tfmatmul.h`) and object (`test_graph_tfmatmul.o`) files using `tfcompile`;

According to Step 3, I used the example code (named as `my_code.cc`) to invoke the subgraph;

According to Step 4, I added the code snippet `cc_binary` to the existing `BUILD` file (`//tensorflow/compiler/aot/tests/BUILD`), and tried to create the final binary with the command:

`bazel build //tensorflow/compiler/aot/tests:my_binary`

but I got the following error:

`undeclared inclusion(s) in rule '//tensorflow/compiler/aot/tests:my_binary':
this rule is missing dependency declarations for the following files included by 'tensorflow/compiler/aot/tests/my_code.cc':
'/home/tensorFlow_src/tensorflow/tensorflow/compiler/aot/tests/test_graph_tfmatmul.h'`

### Source code / logs
`my_code.cc` (exactly the same as in the tutorial):
```c++
#define EIGEN_USE_THREADS
#define EIGEN_USE_CUSTOM_THREAD_POOL

#include <iostream>
#include ""third_party/eigen3/unsupported/Eigen/CXX11/Tensor""
#include ""tensorflow/compiler/aot/tests/test_graph_tfmatmul.h"" // generated

int main(int argc, char** argv) {
  Eigen::ThreadPool tp(2);  // Size the thread pool as appropriate.
  Eigen::ThreadPoolDevice device(&tp, tp.NumThreads());

  foo::bar::MatMulComp matmul;
  matmul.set_thread_pool(&device);

  // Set up args and run the computation.
  const float args[12] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12};
  std::copy(args + 0, args + 6, matmul.arg0_data());
  std::copy(args + 6, args + 12, matmul.arg1_data());
  matmul.Run();

  // Check result
  if (matmul.result0(0, 0) == 58) {
    std::cout << ""Success"" << std::endl;
  } else {
    std::cout << ""Failed. Expected value 58 at 0,0. Got:""
              << matmul.result0(0, 0) << std::endl;
  }

  return 0;
}
```

`cc_binary` in `BUILD` file:
```
cc_binary(
    name = ""my_binary"",
    srcs = [""my_code.cc""],
    deps = [
        ""//tensorflow/compiler/aot/tests:test_graph_tfmatmul"",
        ""//third_party/eigen3"",
    ],
    linkopts = [""-lpthread"",]
)
```"
13481,crosstool_wrapper_driver_is_not_gcc failed building tensorflow/tools/graph_transforms:transform_graph,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:  1.3.0 @ commit 635196
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: not complied from source but Build label: 0.6.0 
- **CUDA/cuDNN version**: CUDA 8.0, cudnn6.0
- **GPU model and memory**: GTX 1080 Ti
- **Exact command to reproduce**: bazel build tensorflow/tools/graph_transforms:transform_graph  --verbose_failures

### Describe the problem
Build tensorflow from source successful, verified it works, but cannot build the tool ""transform_graph""
~~~
ERROR: /home/local/ANT/luxial/tensorflow/tensorflow/tools/graph_transforms/BUILD:222:1: Linking of rule '//tensorflow/tools/graph_transforms:transform_graph' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command (cd /home/local/ANT/luxial/.cache/bazel/_bazel_luxial/ce09802cfa8c7dbfadcb21edd190af0e/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_VERSION=8.0 \
    TF_CUDNN_VERSION=6 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/local_linux-opt/bin/tensorflow/tools/graph_transforms/transform_graph '-Wl,-rpath,$
ORIGIN/../../../_solib_local/_U_S_Stensorflow_Stools_Sgraph_Utransforms_Ctransform_Ugraph___Utensorflow' '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccu
blas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scu
da_Slib' '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' -Lbazel-out/local_linux-opt/bin/_solib_l
ocal/_U_S_Stensorflow_Stools_Sgraph_Utransforms_Ctransform_Ugraph___Utensorflow -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Sloca
l_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../..' -Wl,-z,muldefs -Wl,-z,muldefs -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -pthread -Wl,-no-as-needed -B/usr/bin/ -fPIC -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections -Wl,@bazel-out/local_linux-opt/bin/tensorflow/tools/graph_transforms/transform_graph-2.params)
~~~
### What I tried
Basically everything in #8790, #4365, #817
1. export LD_LIBRARY_PATH=/usr/local/cuda 8.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} 
2. Upgrade Bazel, current build label 0.6.0
3. bazel clean --expunge
4. Adding -fPIC to the options in ../third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl line 60.
5. Add cpu_compiler_flags.append('-fno-use-linker-plugin') in third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl

None of them solve the issue.
"
13479,Allow creating global tf.FIFOQueue in deep frame contexts,"Let's say I'm building a graph. It's a pretty deep graph with many nested while loops. Let's also say I have some template/abstraction that allows me to create a tf.FIFOQueue on demand deep in my nested while loops as I encounter things I want to send out of the graph. This template allows me to infer the shape automatically from the tensors as they are encountered rather than to have to write a bunch of painful boilerplate to define all the queues and their corresponding shapes up-front.

This is currently not possible with TensorFlow, because this gives this error: 

> tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'fifo_queue_DequeueMany' has inputs from different frames. The input 'while/fifo_queue' is in frame 'while/while/'. The input 'fifo_queue_DequeueMany/n' is in frame ''.

I want to create a FIFOQueue() in the global ""frame"" so I don't get this error. In addition the documentation never explains what a ""frame"" even is. Having undocumented restrictions that suddenly explode without warning is a sign of leaky abstractions and very frustrating for users."
13477,Bug in 1.3 Preventing Export of Canned Estimators,"
------------------------

### System information

TensorFlow v1.3.0-rc2-20-g0787eee 1.3.0
Python version 3.5


### Describe the problem

See below Stack Overflow Page. 
https://stackoverflow.com/questions/46098863/how-to-import-an-saved-tensorflow-model-train-using-tf-estimator-and-predict-on

Use case #2 is what this bug is about. The model appears to export, but when attempting to perform predictions on new data, the following error message is rendered: 
ValueError: Got unexpected keys in input_dict: {'feature1', 'feature2', 'feature3'...}

### Source code / logs
### Code to store the model:

```python
def column_to_dtype(column):
    if column in CATEGORICAL_COLUMNS:
        return tf.string
    else:
        return tf.float32
    
feature_spec = {
    column: tf.FixedLenFeature(shape=[1], dtype=column_to_dtype(column))
        for column in FEATURE_COLUMNS
}
serving_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)
servable_model_dir = ""DLModels""
servable_model_path = m.export_savedmodel(export_dir_base=servable_model_dir,
                            serving_input_receiver_fn=serving_fn)
```
### Code to recall the model for predictions on new data:
```python
from tensorflow.contrib import predictor
predict_fn = predictor.from_saved_model(servable_model_path)
predictions = predict_fn(trainfeatures1)
print(predictions)'''

### Error message:
2017-10-03 21:23:37,175, INFO, Restoring parameters from b'DLModels/1507065657/variables/variables'
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3479-70528b4cfb46> in <module>()
      1 from tensorflow.contrib import predictor
      2 predict_fn = predictor.from_saved_model(servable_model_path)
----> 3 predictions = predict_fn(trainfeatures1)
      4 print(predictions)

/opt/miniconda/lib/python3.5/site-packages/tensorflow/contrib/predictor/predictor.py in __call__(self, input_dict)
     68     if unexpected_keys:
     69       raise ValueError('Got unexpected keys in input_dict: {}'.format(
---> 70           unexpected_keys))
     71 
     72     feed_dict = {}

ValueError: Got unexpected keys in input_dict: {'feature1', 'feature2', 'feature3'}
"
13476,`tf.rnn.contrib` ops missing in Java API,"I am trying to run an LSTM built with the Python API, exported to to protobuf, and evaluated in Java. After I updated my python scripts to use `tf.contrib.rnn.LSTMBlockCell`, `LSTMBlockFusedCell`, etc cell implementations, the Java portion stopped working and started throwing: 

```
org.tensorflow.TensorFlowException: org.tensorflow.TensorFlowException: Op type not registered 'BlockLSTM' in binary running on xubuntu. Make sure the Op and Kernel are registered in the binary running in this process.
```

and

```
org.tensorflow.TensorFlowException: org.tensorflow.TensorFlowException: Op type not registered 'LSTMBlockCell' in binary running on xubuntu. Make sure the Op and Kernel are registered in the binary running in this process.
```

Evaluating the graph in Python works swimmingly. I didn't see anything on stack overflow. There are a couple tickets on github which might be related: #11847 and #12566 

TF Version (Java and python): 1.3.0
Python Version: 2.7
"
13473,StochasticTensor strange behavior,"I've noticed strange behavior of StochasticTensor. Please, look at this peace of code: 

    inputs = tf.placeholder(shape=(1, 10, ), name=""inputs"", dtype=tf.float32)

    outputs = tf.layers.dense(inputs, units=10, use_bias=False, activation=tf.nn.sigmoid)
    outputs = st.StochasticTensor(distributions.Bernoulli(probs=outputs, dtype=tf.int32))
    outputs = tf.reshape(outputs, shape=(-1, ))
    
    init_op = tf.group(
        tf.global_variables_initializer(),
        tf.local_variables_initializer()
    )
    
    with tf.Session() as sess:
        sess.run(init_op)
        x = np.random.rand(1, 10)
        
        tf.set_random_seed(2017)
        z1 = sess.run(outputs, feed_dict={inputs: x})
        
        tf.set_random_seed(2017)
        z2 = sess.run(outputs, feed_dict={inputs: x})
    
    print(z1)
    print(z2)

As the result I get:
`
[1 0 1 0 0 1 1 1 0 0]
[0 1 0 1 0 1 0 1 1 1]
`

But numpy sampling has another behavior:

        np.random.seed(2017)
        x = np.random.randint(0, 10, size=10)

        np.random.seed(2017)
        y = np.random.randint(0, 10, size=10)

        print(x)
        print(y)

And (again) the result:
`
[9 6 8 2 3 7 8 0 8 6]
[9 6 8 2 3 7 8 0 8 6]
`

Is it an issue?

P.S.
Tensorflow v1.3.0-rc2-20-g0787eee"
13471,Issue when the saving the model when the session is made by with statement," i use tensorflow to train LSTM network. The training run well but when i want to save the model, i get error below.
```
Step 1, Minibatch Loss= 0.0146, Training Accuracy= 1.000
Step 1, Minibatch Loss= 0.0129, Training Accuracy= 1.000
Optimization Finished!
Traceback (most recent call last):
  File "".\lstm.py"", line 169, in <module>
    save_path = saver.save(sess, ""modelslstm/"" + str(time.strftime(""%d-%m-%Y-%H-%M-%S"")) + "".ckpt"")
  File ""C:\Python35\lib\site-packages\tensorflow\python\client\session.py"", line 1314, in __exit__
    self._default_graph_context_manager.__exit__(exec_type, exec_value, exec_tb)
  File ""C:\Python35\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""C:\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 3815, in get_controller
    if self.stack[-1] is not default:
IndexError: list index out of range
```

My Code :
```
with tf.Session() as sess:
    sess.run(init)
    saver = tf.train.Saver()
   ....
   save_path = saver.save(sess, ""modelslstm/"" + str(time.strftime(""%d-%m-%Y-%H-%M-%S"")) + "".ckpt"")
```
After i changed the `with` statement with the `sess = tf.Session()` the problem gone"
13468,Issues in training the model on macOS Sierra,"hello, 

i want to create a AI-chatbot using the tensorflow seq2seq model. So, when i try to train the model there are two issues that i want to ask about:

1- why i'm getting this WARNING 
`WARNING:tensorflow:From /Users/emansaad/Desktop/AI-chtabot/Victor/eng-victor/seq2seq_model.py:129 in __init__.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.`

2-the temperature of my MAC book pro (macOS Sierra) is getting very high and the fans are running to cool the device , is that normal?
"
13467,cannot create an object of the class tf.layers.Dense(),"I want to create an object of the class tf.layers.Dense(), but don't want to do the feedforward step immediately (i.e. not tf.layers.dense(input, units)). Because I want to first declare these modules/layers in a class, and then to have several member functions apply1(x, y), apply2(x,y) to use these layers. But when I did in tensorflow tf.layers.Dense(units), it returned:

layer = tf.layers.Dense(100)
 AttributeError: 'module' object has no attribute 'Dense'

How can I fix that?
Thanks"
13466,contrib/verbs only works on GPU,"### System information
- **OS Linux readhat** 7.3
- **TensorFlow** installed from source
- **TensorFlow version** 1.3.0
- **Python version**:  2.7.5
- **Bazel version**:
Build label: 0.5.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Aug 25 10:00:00 2017 (1503655200)
Build timestamp: 1503655200
Build timestamp as int: 1503655200

- **CUDA/cuDNN version**: no CUDA
- **GPU model and memory**: no GPU
- **Exact command to reproduce**: Compilation with the following command fails:

 bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

ERROR: /root/tensorflow/tensorflow/contrib/verbs/BUILD:133:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma' failed (Exit 1).
In file included from ./tensorflow/core/platform/stream_executor.h:26:0,
                 from ./tensorflow/core/common_runtime/gpu/gpu_util.h:23,
                 from tensorflow/contrib/verbs/rdma.cc:24:
./tensorflow/stream_executor/dso_loader.h:32:30: fatal error: cuda/cuda_config.h: No such file or directory
 #include ""cuda/cuda_config.h""
                              ^
compilation terminated.

The ./configure is done with all the defaults except enabling verbs.

Reverting the following commit fixes compilation:
commit 0d864630161d9f3b9eaef0b7c6ce7443654df97a
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Fri Sep 22 13:48:33 2017 -0700

    Move GPU-specific dependencies of core/grappler:devices into
    cuda_deps.

    Fix #includes and deps of contrib/verbs:verbs_util, in particular
    removing an unnecessary #include of gpu_util.h that relied on a
    transitive dependency through :devices.

    PiperOrigin-RevId: 169732234

@junshi15 "
13464,Tensorflow Windows 64bit build for Python 2.7,"Dear All,

I happened to have seemingly successfully built **tensorflow 1.3.1 (CPU only) for Python 2.7 64bit** with Visual Studio 2015 on Windows.

Apart from numerous minor manual tweaks, the main things that I had to do are the following:

-  modify _tensorflow-1.3.1\tensorflow\contrib\cmake\tools\create_def_file.py_
-  modify _tensorflow-1.3.1\tensorflow\tools\git\gen_git_source.py_. 
-  modify some of the automatically generated _*.py_ files.
-  copy some folders like _tensorflow\core\profiler_ to _tensorflow\python\profiler_ for the final distribution package.
- add blank ___init__.py_ files in some folders for the final distribution package.
- translate some Cmake build routines to Matlab (as it was easier for me to debug, :p )

I have tested some example codes and found to be running as expected. I however believe to make thorough tests to claim whether my build is 100% okay or not.

I have found in different forums that many users had struggled to build for Python 2.7 64bit in Windows and even if built, ended up with import errors, missing files, etc. I don't know whether they had finally succeeded or not.

So, for those who are really desperate to have one, I have shared the wheel package [here](http://mohammadulhaque.alotspace.com/download.php?id=56287).

**Please beware that there is a high possibility of ABI mismatch, if used.**

Acknowledgement: _I would like to acknowledge [Wingware](https://wingware.com/) for providing me a professional license for Wing IDE, which I have used, and in general use, extensively for debugging purposes in Python._

Disclaimer:  _The above shared wheel package is for illustrative purposes only which, I hope, will provide some useful information regarding the build process. It is supplied ""AS IS"" without any warranties and support. I assume no responsibility or liability for its use of any kind._



"
13463,DataLossError (see above for traceback): corrupted record at 12,"I have a big problem, I use the  tfrecord file to import data for my tensorflow program. But, when the program run a period of time， it displays the DataLossError:
------------------------

### System information
OS Platform and Distribution : Linux Ubuntu 14.04
TensorFlow installed from : Anaconda
TensorFlow version : 1.3.0
Python version:  2.7.13
CUDA/cuDNN version: 8.0 / 6.0  
GPU model and memory: Pascal TITAN X

### Describe the problem
 2017-10-03 19:45:43.854601: W tensorflow/core/framework/op_kernel.cc:1192] Data loss: corrupted record at 12
Traceback (most recent call last):
  File ""east_quad_train_backup.py"", line 416, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""east_quad_train_backup.py"", line 330, in main
    Training()
  File ""east_quad_train_backup.py"", line 312, in Training
    feed_dict={learning_rate: lr})
  File ""/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 12
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,512,512,3], [?,128,128,9]], output_types=[DT_UINT8, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](Iterator)]]
	 [[Node: gradients/Tile_grad/Shape/_23 = _HostRecv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_442_gradients/Tile_grad/Shape"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

Caused by op u'IteratorGetNext', defined at:
  File ""east_quad_train_backup.py"", line 416, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""east_quad_train_backup.py"", line 330, in main
    Training()
  File ""east_quad_train_backup.py"", line 251, in Training
    batch_image, batch_label = iterator.get_next()
  File ""/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py"", line 304, in get_next
    name=name))
  File ""/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 379, in iterator_get_next
    output_shapes=output_shapes, name=name)
  File ""/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

DataLossError (see above for traceback): corrupted record at 12
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,512,512,3], [?,128,128,9]], output_types=[DT_UINT8, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](Iterator)]]
	 [[Node: gradients/Tile_grad/Shape/_23 = _HostRecv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_442_gradients/Tile_grad/Shape"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]


Thanks anyone to answer this question."
13462,Feature Request: recompute gradient with updated weights within a graph,"Hi,

I wonder could there could be some new features to recompute gradients with updated weights within a graph or if there is any better way to do this. For example, for estimating hessian norm, we need to compute

delta ~ N(0, I)
hessian_norm = 1/M \sum_{1}^{M}  gradient(f(x+delta))- gradient(f(x-delta))/(2delta)

we need to gradient value on x+delta. Currently we will get None type if we use tf.gradient on var+delta directly. 

Thank you very much."
13461,using ExponentialMovingAverage differs in CPU & GPU,"I use BN in cnn,here is the code

```
def batch_norm(x, beta, gamma, train_phase, scope='bn', decay=0.9, eps=1e-5):

    with tf.variable_scope(scope):        
        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')
        ema = tf.train.ExponentialMovingAverage(decay=decay)

        def mean_var_with_update():
            ema_apply_op = ema.apply([batch_mean, batch_var])
            with tf.control_dependencies([ema_apply_op]):
                return tf.identity(batch_mean), tf.identity(batch_var)

        mean, var = tf.cond(train_phase, mean_var_with_update, lambda: (ema.average(batch_mean), ema.average(batch_var)))
        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, eps)
    return normed
```

when i print variables in CPU & GPU, it differs:
In CPU it shows:
`bn_1/bn_1/moments/Squeeze/ExponentialMovingAverage (DT_FLOAT) [32]
bn_1/bn_1/moments/Squeeze_1/ExponentialMovingAverage (DT_FLOAT) [32]
bn_2/bn_2/moments/Squeeze/ExponentialMovingAverage (DT_FLOAT) [64]
bn_2/bn_2/moments/Squeeze_1/ExponentialMovingAverage (DT_FLOAT) [64]
bn_3/bn_3/moments/Squeeze/ExponentialMovingAverage (DT_FLOAT) [64]
bn_3/bn_3/moments/Squeeze_1/ExponentialMovingAverage (DT_FLOAT) [64]
bn_4/bn_4/moments/Squeeze/ExponentialMovingAverage (DT_FLOAT) [64]
bn_4/bn_4/moments/Squeeze_1/ExponentialMovingAverage (DT_FLOAT) [64]`
while in GPU it shows:
`bn_1/bn_1/moments/moments_1/mean/ExponentialMovingAverage (DT_FLOAT) [32]
bn_1/bn_1/moments/moments_1/variance/ExponentialMovingAverage (DT_FLOAT) [32]
bn_2/bn_2/moments/moments_1/mean/ExponentialMovingAverage (DT_FLOAT) [64]
bn_2/bn_2/moments/moments_1/variance/ExponentialMovingAverage (DT_FLOAT) [64]
bn_3/bn_3/moments/moments_1/mean/ExponentialMovingAverage (DT_FLOAT) [64]
bn_3/bn_3/moments/moments_1/variance/ExponentialMovingAverage (DT_FLOAT) [64]
bn_4/bn_4/moments/moments_1/mean/ExponentialMovingAverage (DT_FLOAT) [64]
bn_4/bn_4/moments/moments_1/variance/ExponentialMovingAverage (DT_FLOAT) [64]`

therefore, when i load a CPU-trainned ckeckpoint in GPU, it turns out some errors like ""variables not found in file"" etc. Can someone solve this problem? thanks a lot."
13460,Feature request: segment_argmax,"Currently trying to return argmax from a tensor for selected slices (segments)
slices do not have the same length, so reshaping can't be used.
was looking for a function similar to tf.segment_max, only with indices as the return value.

for e.g
a = [1, 2, 3, 4, 5, 6]
seg = [0, 0, 0, 1, 1, 2]
tf.segment_argmax  return value will be
[2, 4, 5]
"
13458,How to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config),"I want to limit the total memory of each GPU in mnist,
https://www.tensorflow.org/tutorials/using_gpu

config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.4
session = tf.Session(config=config, ...)

and I added the above code to the mnis.py
https://github.com/tensorflow/models/tree/master/official/mnist

here is the modified code in mnis.py :
def main(unused_argv):


  config = tf.ConfigProto()
  config.gpu_options.per_process_gpu_memory_fraction = 0.4

  mnist_classifier = tf.estimator.Estimator(
      model_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)



but I get the below error:




Traceback (most recent call last):
  File ""mnist.py"", line 231, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""mnist.py"", line 206, in main
    model_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 142, in __init__
    config)
ValueError: config must be an instance of RunConfig, but provided gpu_options {
  per_process_gpu_memory_fraction: 0.4
}
.


My question is :
How to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config)"
13457,Two issues on tf.nn.ctc_loss,"Environments: tf version 1.3, CPU version; python 3.5/3.6; Win10/Ubuntu 16.04.

To begin with, we start from code:

import tensorflow as tf
num_classes, batch_size, seq_len = 3, 1, 2
labels = tf.SparseTensor(indices=[[0,0]], values=[0], dense_shape=[1,1])
inputs = tf.zeros([seq_len, batch_size, num_classes])
loss = tf.nn.ctc_loss(labels, inputs, [seq_len])
print(tf.InteractiveSession().run(loss))

tf.nn.ctc_loss behaves as expected, and print the correct answer: 1.09861231

Issue one: How to calculate the ctc loss of a sequence with all blanks? The tf.nn.ctc_loss API requires that values < num_labels, so we have no way to achieve it? If I do change the values in the above example to num_classes - 1 (the reserved blank ID), tf.nn.ctc_loss has no complain, and returns the wrong answer: 0.81093025! The correct answer is 2*log(3). The code to reproduce issue one is as below:

import tensorflow as tf
num_classes, batch_size, seq_len = 3, 1, 2
labels = tf.SparseTensor(indices=[[0,0]], values=[2], dense_shape=[1,1])
inputs = tf.zeros([seq_len, batch_size, num_classes])
loss = tf.nn.ctc_loss(labels, inputs, [seq_len])
print(tf.InteractiveSession().run(loss))

Issue two: Let's change the sequence length to 1 as below

import tensorflow as tf
num_classes, batch_size, seq_len = 3, 1, 1
labels = tf.SparseTensor(indices=[[0,0]], values=[2], dense_shape=[1,1])
inputs = tf.zeros([seq_len, batch_size, num_classes])
loss = tf.nn.ctc_loss(labels, inputs, [seq_len])
print(tf.InteractiveSession().run(loss)) 

and run the code again. This code gives the correct answer, log(3), in Ubuntu, but crashes in Win10 with message: Kernel died, restarting."
13452,Error Raised contradicting to the documentation,"Hi,

This [Line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/inspect_checkpoint.py#L35-L36) says the variable can be left empty, but when done raises this error, `TypeError: print_tensors_in_checkpoint_file() takes exactly 3 arguments (2 given)`"
13449,[FeatureRequest] TFDBG change default limit depth for recursive checking tensors,"Hi,

Recently, I was debugging with TFDBG for a neural network training. It is pretty useful, however, when I want to check some ill-conditioned tensors and their parent, by default TFDBG give me its parents up to a recursive level of 20, i.e. limit depth=20 by default. I could solve the problem by putting a ""-d 1"" to check its immediate parent each time but it is quite annoying to input this every time.

Would you consider adding a feature in this TFDBG allowing us to set a default value for recursive retrieval of a tensor's input nodes?

My TensorFlow version is
```('v1.3.0-rc2-20-g0787eee', '1.3.0')```

Here is an example of tracing a node's inputs when running the TFDBG currently 


``` 
Inputs to node ""node name"" (Depth limit = 20, controlled input included )
...
... some detail information
...
```
for command  ```>tfdbg li -c -r ""node name"" ```

"
13448,Building for tensorflow.dll failed because architectures < sm35 are not supported,"Hi,
I am trying to compile the dynamic library tensorflow.dll with cmake for windows 10 and GPU enabled. However, the library tf_core_gpu_kernels failed because of the following error:
""Unable to compile CudaAtomicAdd for complex64  architectures < sm35 are not supported""

I am building for a device with a card graphic Gforce 1080Ti (compute 61). So my configuration in Visual Studio is:
Cuda C/C++-> Device -> Code generation: compute_61, sm_61. But, I am still having the error.
Is it related with any other configuration?. It would be great if someone could help with this issue.

Thanks in advance.


 "
13446,"Dataset: ""Shuffle"" doesn't work","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:  source
- **TensorFlow version (use command below)**:
- **Python version**: python 3.5
- **Bazel version (if compiling from source)**:
Build label: 0.5.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Aug 25 10:00:00 2017 (1503655200)
Build timestamp: 1503655200
Build timestamp as int: 1503655200
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

== tensorflow import ============================================      
tf.VERSION = 1.3.0                                                                    
tf.GIT_VERSION = b'v1.3.0-rc1-2408-ge9d5ee1'                             
tf.COMPILER_VERSION = b'v1.3.0-rc1-2408-ge9d5ee1'   


### Describe the problem
""Shuffle"" from Dataset doesn't work. 

### Source code / logs
The following files can be used to reproduce problem.

```python
import tensorflow as tf
tf.set_random_seed(123)

def input_pipeline(filenames, batch_size):
    # Define a `tf.contrib.data.Dataset` for iterating over one epoch of the data.
    dataset = (tf.contrib.data.TextLineDataset(filenames)
               .map(lambda line: tf.decode_csv(
                    line, record_defaults=[['1'], ['1'], ['1']], field_delim='-'))
               .shuffle(buffer_size=10)  # Equivalent to min_after_dequeue=10.
               .batch(batch_size))

    # Return an *initializable* iterator over the dataset, which will allow us to
    # re-initialize it at the beginning of each epoch.
    return dataset.make_initializable_iterator()

filenames=['1.txt']
batch_size = 3
num_epochs = 3
iterator = input_pipeline(filenames, batch_size)

# `a1`, `a2`, and `a3` represent the next element to be retrieved from the iterator.
a1, a2, a3 = iterator.get_next()

with tf.Session() as sess:
    for epoch in range(num_epochs):
        # Resets the iterator at the beginning of an epoch.
        sess.run(iterator.initializer)
        print('epoch:%d' % (epoch))
        try:
            while True:
                a, b, c = sess.run([a1, a2, a3])
                print(a, b, c)
        except tf.errors.OutOfRangeError:
            # This will be raised when you reach the end of an epoch (i.e. the
            # iterator has no more elements).
            pass
```

The corresponding file: ""1.txt""

```
1,2-3,4-A
7,8-9,10-B
12,13-14,15-C
17,18-19,20-D
22,23-24,25-E
27,28-29,30-F
32,33-34,35-G
37,38-39,40-H
```

The output:
```
2017-10-02 15:06:43.523320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-10-02 15:06:43.523788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties: 
name: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705
pciBusID: 0000:01:00.0
totalMemory: 5.93GiB freeMemory: 5.44GiB
2017-10-02 15:06:43.523800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)
epoch:0
[b'27,28' b'17,18' b'7,8'] [b'29,30' b'19,20' b'9,10'] [b'F' b'D' b'B']
[b'32,33' b'22,23' b'12,13'] [b'34,35' b'24,25' b'14,15'] [b'G' b'E' b'C']
[b'1,2' b'37,38'] [b'3,4' b'39,40'] [b'A' b'H']
epoch:1
[b'27,28' b'17,18' b'7,8'] [b'29,30' b'19,20' b'9,10'] [b'F' b'D' b'B']
[b'32,33' b'22,23' b'12,13'] [b'34,35' b'24,25' b'14,15'] [b'G' b'E' b'C']
[b'1,2' b'37,38'] [b'3,4' b'39,40'] [b'A' b'H']
epoch:2
[b'27,28' b'17,18' b'7,8'] [b'29,30' b'19,20' b'9,10'] [b'F' b'D' b'B']
[b'32,33' b'22,23' b'12,13'] [b'34,35' b'24,25' b'14,15'] [b'G' b'E' b'C']
[b'1,2' b'37,38'] [b'3,4' b'39,40'] [b'A' b'H']
```
"
13444,Feature request: Allow for custom hooks in Slim's `evaluate_once` to support TFDBG,"### System information
- **TensorFlow version**: `v1.3.0-rc2-20-g0787eee`

### Describe the problem
The evaluation functions provided by `contrib.slim.python.slim` are wrappers around methods from `contrib.training.python.training`. `contrib.slim.python.slim.evaluation_loop` provides argument `hooks` which attaches custom hooks to the evaluation loop. This can be used to hook the TFDBG debugger into evaluation. `contrib.slim.python.slim.evaluation_once` however does not provide the argument, even though the underlying `contrib.training.python.training.evaluate_once` does support the argument. The code to extend the hooks with custom hooks is already there in `contrib.slim.python.slim.evaluation_loop` but that fix somehow was not applied to `contrib.slim.python.slim.evaluation_once`.

The request is to implement the addition of custom hooks to `contrib.slim.python.slim.evaluation_once` so that TFDBG can be used with this method. This is a really easy fix that adds a lot of functionality.
"
13442,Tensorflow-GPU installation Error ,"GPU:  1050Ti
OS: windows 10
--------------------------
Cuda tool kit: version 9.0
cuDNN: version 8.0
tensorflow-gpu: verision 1.3
--------------------------
after installing CUDA tool kit v.90 checked 'bandwith test'
C:\Program Files\NVIDIA Corporation\NVSMI\nvidia-smi.exe 
-------------------------
%Path%
Path variables 
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\libnvvp
C:\Program Files\cuda\bin
--------------------------

created a virtual environment 'tensorflow-gpu' 
conda install tensorflow-gpu 

to check the tensorflow installation
import tensorflow as tf

to check the devices available 
>>from tensorflow.python.client import device_lib
>> device_lib.list_local_devices()

it show only CPU, not the GPU stack 

------------------------------
please help me out where i am getting wrong.

in the virtual environment 'tensorflow-gpu' some times when i load 
>> import tensorflow as tf
it thoughs dll missing error
Please help me out almost spent 3-4 days in it. 

C:\WINDOWS\system32>python
Python 3.6.1 |Anaconda custom (64-bit)| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Program Files\Anaconda 3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Anaconda 3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\Anaconda 3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Anaconda 3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Anaconda 3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Program Files\Anaconda 3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Anaconda 3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Program Files\Anaconda 3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Anaconda 3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Anaconda 3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Anaconda 3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\Anaconda 3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Anaconda 3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Anaconda 3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Program Files\Anaconda 3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.



"
13441,tf.biderectional_dynamic_rnn get stuck when running the graph,"I built a 1-layer bidirectional RNN with 128 hidden nodes using the `output = tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, inputs, seqlens, tf.float32, is_tuple=True, time_major=True)` interface. I run the network with input data size of 57x285x4608 ([time_step x batch_size x num_feature]) but get stuck in the `_outputs = sess.run(outputs, feeds)`. The system does not indicate any resource exhausted. When I reduce the time_step to 31, the network runs successfully. When I only reduce the number of 3rd dimension to 512, it still fails to work. It seems there is some constraints on the input sequence length.

Any idea on this problem?

I run this program on Nvidia DGX Server with 4 Tesla P100 GPUs. The OS is ubuntu 14.04."
13439,Keras has much better gradients calculated than native TF,"Hi,
I am not sure if this is a bug in some TF function or Keras has just some clever ways to pull things off.
I was prototyping a simple logistic regression model with Keras and trying to write the exact same model with TF to reproduce the result. However, there's something unexplainable to me that Keras always has much better gradients calculated than TF does when I use mini-batch SGD.

tensorflow==1.2.1
Keras==2.0.8
GPU: Tesla P40

Keras version:
```python
def custom_objective(y_true, y_pred):
    loss = tf.reduce_mean(-(y_true*tf.log(y_pred)+((1.0-y_true)*tf.log(1.0-y_pred))))
    return loss
model = Sequential()
model.add(Dense(1,input_dim=2440000, activation='sigmoid', bias_initializer='zeros', kernel_initializer='zeros'))
sgd = tf.train.GradientDescentOptimizer(0.5)
model.compile(loss=custom_objective, optimizer=sgd)
model.fit_generator(generator, steps_per_epoch=1, epochs=1, callbacks=[ival], max_queue_size=10, workers=1, use_multiprocessing=False, initial_epoch=0)
```
TF version:
```python
def linear(x, n_input, n_output, name=None):
    with tf.variable_scope(name or 'fc'):
        W = tf.get_variable(
            name = ""W"",
            # shape = [n_input, n_output],
            dtype=tf.float32,
            # initializer=tf.contrib.layers.xavier_initializer())
            initializer=tf.zeros(shape=[n_input,n_output]))
        b = tf.get_variable(
            name='bias',
            shape=[n_output],
            dtype=tf.float32,
            initializer=tf.constant_initializer(0.0))
        if not isinstance(x, tf.SparseTensor):
            h = tf.nn.bias_add(
                tf.matmul(x, W),
                b,
                name='h')
        else:
            h = tf.nn.bias_add(
                tf.sparse_tensor_dense_matmul(x, W),
                b,
                name='h')
    return h, W, b

tf.reset_default_graph()
X_shape = tf.placeholder(tf.int64, shape=[2], name=""X_shape"")
X_indices = tf.placeholder(tf.int64, name=""X_indices"")
X_values = tf.placeholder(tf.float32, shape=[None], name=""X_values"")
y = tf.placeholder(dtype=tf.float32, name=""y"")
H = tf.SparseTensor(indices=X_indices, values=X_values, dense_shape=X_shape)
logit, w, b = linear(H, 2440000, 1, name=""output_layer"")
y_pred = tf.nn.sigmoid(logit)
train_error = -(y*tf.log(y_pred) + ((1.0 - y) * tf.log(1.0-y_pred)))
loss = tf.reduce_mean(train_error)
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)
gvs = optimizer.compute_gradients(loss,[w,b])
train_op = optimizer.apply_gradients(gvs)
sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True), graph=tf.get_default_graph())
sess.run(tf.global_variables_initializer())
```

TL;DR
Keras has better gradients calculated/updates than TF.

Both version implements a vanilla logistic regression, with **same native TF optimizer**, **same user defined cross entropy** and **same data generator**(except for Keras accepts a dense matrix and TF accepts sparse matrix.tocoo()), **same learning rate**, **same zero initializer for both w and b**.
Simple calculus can show that if the first batch contains all NEGATIVE examples, the gradient for b in the first update must be exactly 0.5.

If a batch has very few examples (e.g 1-9), both version produce an exact gradient of 0.5 for b.
When sample size goes above 9, Keras starts to have a way better gradients calculated for both b and w. For example, with sample size 10, Keras calculates 0.50000006 for b and TF gives 0.49999988. With sample size 12, Keras gives 0.49999994 but TF gives 0.50000012. Though both give wrong gradient, Keras is always better, not to mentions the weights gradients. Also trying casting the loss to float16, 32 or 64 won't make the gradient as good as Keras'.

The accumulated differences after 100 batches of training makes TF's model worse than Keras' in terms of AUC.

At this stage I am not sure where I should look for so I resort to the community to help me with this ""unexplainable"" phenomena. Any suggestion will be much appreciated.

Oscar
"
13438,The problem in saving and restoring LSTM models,"I have trained a LSTM model and saved it as a small part of another model. the key code as follows:
```python
       initializer = tf.truncated_normal_initializer()
        with tf.variable_scope('RNN_model', reuse=None, initializer=initializer):
            train_rnn = RNNmodel.LSTMmodel(True, RNNmodel.TRAIN_BATCH_SIZE, RNNmodel.NUM_STEP)
        with tf.variable_scope('RNN_model', reuse=True, initializer=initializer):
            test_rnn = RNNmodel.LSTMmodel(False, RNNmodel.EVAL_BATCH_SIZE, RNNmodel.NUM_STEP)
        init = tf.global_variables_initializer()
        sess.run(init)
        # train here
        saver.save(sess, saver_path)
```

everything looks normal when i restore it `saver.restore(sess, saver_path)`, but when i run the epoch and reach to the code `sess.run(op)`, it throws an error saying that
> FailedPreconditionError (see above for traceback): Attempting to use uninitialized value RNN_model/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/kernel

and i checked out the ckpt files and found that there was not any variables about LSTM cells so i wonder if `Saver().save` could save the variables in LSTM cells.Or i just had a wrong practice.
"
13437,Sin family identities for y=x yield bad gradients,"I'm writing a custom continuous piecewise function. At some point the function becomes an identity of f(x) = x, but while the loss decreases, the accuracy does not improve. Simply swapping in an ""x"" in the below code does cause everything to work smoothly.

Originally suspected tf.where as that has NaN gradient troubles, so I rewrote an equivalent function using boolean_mask. Still the same issue. I also attempted to trim values to prevent NaN propagation. A simplified version of the code is below (the troublesome statement in question being tf.cos(i*tf.acos(x)), which equals x):

    i = 1

    location_value = tf.stack([
        tf.less_equal(tf.abs(x), 1), # between_neg_1_and_1
        tf.greater(x, 1), # greater_than_1
        tf.less(x, -1), # less_than_neg_1
        tf.is_nan(x)] 
    , -1)

    res = tf.stack([
        tf.cos(i*tf.acos(tf.minimum(tf.maximum(x, -1), 1))),
        x,
        x,
        0*x]
    , -1)

    out_shape = x.get_shape().as_list()
    out_shape[0] = batch_size

    res = tf.reshape(tf.boolean_mask(res, location_value), out_shape)



------------------------

== cat /etc/issue ===============================================
Linux tyler-desktop 4.2.0-42-generic #49~14.04.1-Ubuntu SMP Wed Jun 29 20:22:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""14.04.5 LTS, Trusty Tahr""
VERSION_ID=""14.04""

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 4.8.5-2ubuntu1~14.04.1) 4.8.5
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux tyler-desktop 4.2.0-42-generic #49~14.04.1-Ubuntu SMP Wed Jun 29 20:22:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.12.0)
protobuf (3.1.0.post1)
tensorflow-gpu (0.12.1)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 0.12.1
tf.GIT_VERSION = v0.12.0-10-g4d924e7-dirty
tf.COMPILER_VERSION = v0.12.0-10-g4d924e7-dirty
Sanity check: array([1], dtype=int32)
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally

== env ==========================================================
LD_LIBRARY_PATH /root/torch/install/lib:/root/torch/install/lib:/usr/local/cuda-8.0/lib64:/usr/lib:/usr/openwin/lib:/usr/dt/lib:/X11.6/lib:/X11.5/lib:/uva/lib:/gnu/lib:/usr/local/cuda/lib64:/usr/local/cuda:/usr/bin/g++
DYLD_LIBRARY_PATH /root/torch/install/lib:/root/torch/install/lib:

== nvidia-smi ===================================================
Sun Oct  1 18:05:01 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1070    Off  | 0000:01:00.0      On |                  N/A |
|  0%   37C    P0    40W / 230W |    728MiB /  8110MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1137    G   /usr/bin/X                                     370MiB |
|    0      1761    G   compiz                                         243MiB |
|    0      2645    G   /usr/lib/firefox/firefox                         1MiB |
|    0      3024    G   ...ble-features=DocumentWriteEvaluator<Disal   111MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44
/usr/local/cuda-8.0/lib64/libcudart_static.a
"
13436,"""Variable rnn/basic_rnn_cell/kernel already exists, disallowed."" error while defining dynamic_rnn","I was writing a simple code to define an RNN and the code goes thus:

```
n_steps = 28
n_inputs = 28
n_neurons = 150
n_outputs = 10
n_epochs = 100
batch_sz = 150
l_rate = 0.001

X0 = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
Y0 = tf.placeholder(tf.int32, [None])
init_state = tf.zeros([n_steps, n_inputs])

basic_r_cell = rnn.BasicRNNCell(num_units = n_neurons)
ouputs, states = tf.nn.dynamic_rnn(basic_r_cell, X0, initial_state = init_state)

logits = layers.fully_connected(states, n_outputs, activation_fn = None)
```

Executing the above code gave the below error with traceback:

```
> ---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-67-05674d7f7864> in <module>()
     16 
     17 basic_r_cell = rnn.BasicRNNCell(num_units = n_neurons)
---> 18 ouputs, states = tf.nn.dynamic_rnn(basic_r_cell, X0, initial_state = init_state)
     19 
     20 logits = layers.fully_connected(states, n_outputs, activation_fn = None)

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\ops\rnn.py in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)
    572         swap_memory=swap_memory,
    573         sequence_length=sequence_length,
--> 574         dtype=dtype)
    575 
    576     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\ops\rnn.py in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)
    735       loop_vars=(time, output_ta, state),
    736       parallel_iterations=parallel_iterations,
--> 737       swap_memory=swap_memory)
    738 
    739   # Unpack final output if not using output tuples.

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\ops\control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)
   2768     context = WhileContext(parallel_iterations, back_prop, swap_memory, name)
   2769     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, context)
-> 2770     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
   2771     return result
   2772 

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\ops\control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)
   2597       self.Enter()
   2598       original_body_result, exit_vars = self._BuildLoop(
-> 2599           pred, body, original_loop_vars, loop_vars, shape_invariants)
   2600     finally:
   2601       self.Exit()

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\ops\control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)
   2547         structure=original_loop_vars,
   2548         flat_sequence=vars_for_body_with_tensor_arrays)
-> 2549     body_result = body(*packed_vars_for_body)
   2550     if not nest.is_sequence(body_result):
   2551       body_result = [body_result]

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\ops\rnn.py in _time_step(time, output_ta_t, state)
    720           skip_conditionals=True)
    721     else:
--> 722       (output, new_state) = call_cell()
    723 
    724     # Pack state if using state tuples

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\ops\rnn.py in <lambda>()
    706 
    707     input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)
--> 708     call_cell = lambda: cell(input_t, state)
    709 
    710     if sequence_length is not None:

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py in __call__(self, inputs, state, scope)
    178       with vs.variable_scope(vs.get_variable_scope(),
    179                              custom_getter=self._rnn_get_variable):
--> 180         return super(RNNCell, self).__call__(inputs, state)
    181 
    182   def _rnn_get_variable(self, getter, *args, **kwargs):

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\layers\base.py in __call__(self, inputs, *args, **kwargs)
    439         # Check input assumptions set after layer building, e.g. input shape.
    440         self._assert_input_compatibility(inputs)
--> 441         outputs = self.call(inputs, *args, **kwargs)
    442 
    443         # Apply activity regularization.

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py in call(self, inputs, state)
    256   def call(self, inputs, state):
    257     """"""Most basic RNN: output = new_state = act(W * input + U * state + B).""""""
--> 258     output = self._activation(_linear([inputs, state], self._num_units, True))
    259     return output, output
    260 

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py in _linear(args, output_size, bias, bias_initializer, kernel_initializer)
   1015         _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],
   1016         dtype=dtype,
-> 1017         initializer=kernel_initializer)
   1018     if len(args) == 1:
   1019       res = math_ops.matmul(args[0], weights)

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\ops\variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)
   1063       collections=collections, caching_device=caching_device,
   1064       partitioner=partitioner, validate_shape=validate_shape,
-> 1065       use_resource=use_resource, custom_getter=custom_getter)
   1066 get_variable_or_local_docstring = (
   1067     """"""%s

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\ops\variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)
    960           collections=collections, caching_device=caching_device,
    961           partitioner=partitioner, validate_shape=validate_shape,
--> 962           use_resource=use_resource, custom_getter=custom_getter)
    963 
    964   def _get_partitioned_variable(self,

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\ops\variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)
    358           reuse=reuse, trainable=trainable, collections=collections,
    359           caching_device=caching_device, partitioner=partitioner,
--> 360           validate_shape=validate_shape, use_resource=use_resource)
    361     else:
    362       return _true_getter(

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py in _rnn_get_variable(self, getter, *args, **kwargs)
    181 
    182   def _rnn_get_variable(self, getter, *args, **kwargs):
--> 183     variable = getter(*args, **kwargs)
    184     trainable = (variable in tf_variables.trainable_variables() or
    185                  (isinstance(variable, tf_variables.PartitionedVariable) and

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\ops\variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)
    350           trainable=trainable, collections=collections,
    351           caching_device=caching_device, validate_shape=validate_shape,
--> 352           use_resource=use_resource)
    353 
    354     if custom_getter is not None:

c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\ops\variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)
    662                          "" Did you mean to set reuse=True in VarScope? ""
    663                          ""Originally defined at:\n\n%s"" % (
--> 664                              name, """".join(traceback.format_list(tb))))
    665       found_var = self._vars[name]
    666       if not shape.is_compatible_with(found_var.get_shape()):

ValueError: Variable rnn/basic_rnn_cell/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:

  File ""c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()
  File ""c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""c:\users\antunnug\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
```"
13434,Tensorflow does NOT utilize the memory from two GPUs in Windows 10,"Tensorflow Version 1.3.0
OS: Windows 10
GPUs: Nvidia Quadro M4000 * 2 with 8G GPU memory for each
GPU modes: one for WDDM, one for TCC

I tested the official codes at https://github.com/tensorflow/models/blob/master/official/resnet/imagenet_main.py

I just add the GPU constraints in the main function as:

	def main(unused_argv):
	  os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'

          # For this line, visible_divice_list set to only ""0"" and ""0, 1"" can only support the same batch_size
	  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0, 1')) 

	  resnet_classifier = tf.estimator.Estimator(
		  model_fn=imagenet_model_fn, model_dir=FLAGS.model_dir,
		  config=tf.contrib.learn.RunConfig(session_config=config))

	  for cycle in range(FLAGS.train_steps // FLAGS.steps_per_eval):
		tensors_to_log = {
			'learning_rate': 'learning_rate',
			'cross_entropy': 'cross_entropy',
			'train_accuracy': 'train_accuracy'
		}

		logging_hook = tf.train.LoggingTensorHook(
			tensors=tensors_to_log, every_n_iter=100)

		print('Starting a training cycle.')
		resnet_classifier.train(
			input_fn=lambda: input_fn(tf.estimator.ModeKeys.TRAIN),
			steps=FLAGS.first_cycle_steps or FLAGS.steps_per_eval,
			hooks=[logging_hook])
		FLAGS.first_cycle_steps = None

		print('Starting to evaluate.')
		eval_results = resnet_classifier.evaluate(
		  input_fn=lambda: input_fn(tf.estimator.ModeKeys.EVAL))
		print(eval_results)

In the training process, if I set the visible device list to ""0, 1"" or ""0"" only, both can run successfully with batch_size=48, but BOTH failed with batch_size=49! This indicates that the second GPU's memory is not utilized, as batch size could not be bigger when using two GPUs. I have use Nvidia-smi to confirm that only one or two GPUs are used in the above experiments.

My questions are:
1. Is there any way that I can use bigger batch_size when using two GPUs?
2. If the answer for Q1 is No in Windows, is there any way to do it in Linux? I am not familiar with Linux. In Linux, can I set all GPUs to TCC mode? Will the batch size be bigger when two GPUs are both in TCC mode? 

Thank you."
13433,Bug: tf.Variable uses always twice the memory (on the CPU),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: tested on both
- **TensorFlow version (use command below)**: 1.3 for pip / 0cfb16e025b3d20e8c8aca431fc0887814817c44 for self-compiled
- **Python version**: Python 3.4.3 [GCC 4.9.2] on linux
- **Bazel version (if compiling from source)**: 0.5.4
- **CUDA/cuDNN version**: not used
- **GPU model and memory**: not used

### Describe the problem
Every tf.variable occupies always twice the necessary memory: 
Once for the tf.constant vector that is created for the initializer and once as the persistend storage.
Code to reproduce:
```
import os
os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '100' #print all
import tensorflow as tf

runs = 1
N = int(1024 * 1024 * 1.1)
M = int(1024 / 8)
print(""testing allocation of {:.2f} MB"".format(N*M*8. / 1024 / 1024))

#does not matter which version you use:
v = tf.Variable(tf.ones([M, N], tf.float64), name=""var1"")
#v = tf.get_variable(shape=(M,N), initializer=tf.ones_initializer, dtype=tf.float64, name='var1', trainable=False)

init = tf.global_variables_initializer()
for i in range(runs):
      print(""start session"")
      with tf.Session() as sess:
            print(""start init"")
            sess.run(init)
```

In my self-compiled version the output contains the following lines:
```
 tensorflow/core/common_runtime/bfc_allocator.cc:133] Extending allocation by 2.00GiB bytes.
 tensorflow/core/common_runtime/bfc_allocator.cc:137] Total allocated bytes: 4.00GiB
```
For the pip version, the current allocation is not displayed, but observing htop during the execution or using mprof reveals the same.

The issue seems to occur because assign does not reuse the memory of tf.ones but instead allocates additional memory. Related lines: [assign_op.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/assign_op.h#L79-L86)
context->forward_input returns null in this case, because the memory of tf.ones has a ref count of 2.
(I dont know why) see [op_kernel.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/op_kernel.cc#L455)
input->RefCountIsOne() is therefore false.

I tried to comment out the input->RefCountIsOne() check, but then it still doesn't work because of the 
output_attr.IsEqualOrLessRestrictiveThan() check.
If you remove this check too, the memory usage finaly drops to the expected value, the memory of tf.ones is reused.  
But this is not a real solution, because I don't know how this would effect other operations and it seems to break the memory freeing.

I think this bug is quiet serious, because it affects nearly all computations. 
Is this an already known bug?

"
13432,"An exception has occurred, use %tb to see the full traceback. and low accuracy","![tf issue](https://user-images.githubusercontent.com/22562558/31056287-282fb44e-a69d-11e7-800b-af2a8ab41f12.png)

The accuracy I get for sklearn and tensorflow is 0.428571. And after that I get a line that says: An exception has occurred, use %tb to see the full traceback.

What should I do? What is the accuracy that I am supposed to get?
"
13431,Windows nightly build Dataset.from_generator fails with pyfunc error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows-7
- **TensorFlow installed from (source or binary)**:pip
- **TensorFlow version**:1.4.0-dev20170929
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:-
- **CUDA/cuDNN version**:-
- **GPU model and memory**:-
- **Exact command to reproduce**:see below

### Describe the problem
As described in the SO question https://stackoverflow.com/q/46511328/281545 the code:

```python
import tensorflow as tf

Dataset = tf.contrib.data.Dataset
it2 = Dataset.range(5).make_one_shot_iterator()

# Dataset.from_generator need tensorflow > 1.3 !
with tf.Session() as sess:
    print(tf.__version__)
    def _dataset_generator():
        while True:
            try:
                yield sess.run(it2.get_next())
            except tf.errors.OutOfRangeError:
                return
    das_dataset = Dataset.from_generator(_dataset_generator, tf.int64)
    das_dataset_it = das_dataset.make_one_shot_iterator()
    while True:
        try:
            print(sess.run(das_dataset_it.get_next()))
        except tf.errors.OutOfRangeError:
            break
```

fails with:

```
C:\Dropbox\_\PyCharmVirtual\TF-Nigthly-2\Scripts\python.exe C:/Dropbox/eclipse_workspaces/python/zebra/so_46511328_from_generator.py
1.4.0-dev20170929
2017-10-01 16:41:41.978576: W C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows\PY\35\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: 0-th value returned by pyfunc_0 is int32, but expects int64
	 [[Node: PyFunc = PyFunc[Tin=[], Tout=[DT_INT64], token=""pyfunc_0""]()]]
Traceback (most recent call last):
  File ""C:\Dropbox\_\PyCharmVirtual\TF-Nigthly-2\lib\site-packages\tensorflow\python\client\session.py"", line 1323, in _do_call
    return fn(*args)
  File ""C:\Dropbox\_\PyCharmVirtual\TF-Nigthly-2\lib\site-packages\tensorflow\python\client\session.py"", line 1302, in _run_fn
    status, run_metadata)
  File ""C:\_\Python35\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""C:\Dropbox\_\PyCharmVirtual\TF-Nigthly-2\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 467, in raise_exception_on_not_ok_status
    c_api.TF_GetCode(status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: 0-th value returned by pyfunc_0 is int32, but expects int64
	 [[Node: PyFunc = PyFunc[Tin=[], Tout=[DT_INT64], token=""pyfunc_0""]()]]
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[<unknown>], output_types=[DT_INT64], _device=""/job:localhost/replica:0/task:0/cpu:0""](OneShotIterator_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Dropbox/eclipse_workspaces/python/zebra/so_46511328_from_generator.py"", line 19, in <module>
    print(sess.run(das_dataset_it.get_next()))
  File ""C:\Dropbox\_\PyCharmVirtual\TF-Nigthly-2\lib\site-packages\tensorflow\python\client\session.py"", line 889, in run
    run_metadata_ptr)
  File ""C:\Dropbox\_\PyCharmVirtual\TF-Nigthly-2\lib\site-packages\tensorflow\python\client\session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Dropbox\_\PyCharmVirtual\TF-Nigthly-2\lib\site-packages\tensorflow\python\client\session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""C:\Dropbox\_\PyCharmVirtual\TF-Nigthly-2\lib\site-packages\tensorflow\python\client\session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 0-th value returned by pyfunc_0 is int32, but expects int64
	 [[Node: PyFunc = PyFunc[Tin=[], Tout=[DT_INT64], token=""pyfunc_0""]()]]
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[<unknown>], output_types=[DT_INT64], _device=""/job:localhost/replica:0/task:0/cpu:0""](OneShotIterator_1)]]

Process finished with exit code 1
```

That's a problem in windows nightly - installing the nightly on an Ubuntu machine works:

```
$ pipenv run python3 so_46511328_from_generator.py
2017-10-01 13:34:21.840423: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
1.4.0-dev20170929
0
1
2
3
4
2017-10-01 13:34:21.903201: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: StopIteration: Iteration finished.
```

**EDIT:** Maybe related to https://github.com/tensorflow/tensorflow/issues/8196 ?
"
13430,Unstable numerics in MvNormal KL,"https://github.com/tensorflow/tensorflow/blame/0cfb16e025b3d20e8c8aca431fc0887814817c44/tensorflow/contrib/distributions/python/ops/mvn_linear_operator.py#L302

This line should look like this:

```python
def squared_frobenius_norm(x):                                          
    """"""Helper to make KL calculation slightly more readable. And Stable!
    square(sqrt(sqnorm)) = sqnorm                                                 
    """"""                                                                 
    # http://mathworld.wolfram.com/FrobeniusNorm.html                   
    return tf.reduce_sum(x * tf.conj(x), axis=[-2, -1], keep_dims=False)
```

It yields NaN in grad KL(q||p) if q=p "
13429,Name/variable scopes of tensorflow.python.layers.base.Layer,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

This is a tiny code that creates layers and connects them in series.

```python
import tensorflow as tf
from tensorflow.python.layers.base import Layer


class A(Layer):
    def build(self, input_shape):
        self.v = self.add_variable('v', (), tf.float32)
        self.built = True
    
    def call(self, inputs):
        return self.v * inputs
    

# Case 1
with tf.Graph().as_default() as graph:
    x = tf.placeholder(tf.float32, (), 'x')
   
    out = x
    out = A()(out)
    out = A()(out)
    out = A()(out)
    
    tf.summary.FileWriter('/tmp/tensorboard/1', graph=graph).close()

# Case 2
with tf.Graph().as_default() as graph:
    x = tf.placeholder(tf.float32, (), 'x')
   
    out = x
    out = A(name='a')(out)
    out = A(name='a_1')(out)
    out = A(name='a_2')(out)
    
    tf.summary.FileWriter('/tmp/tensorboard/2', graph=graph).close()

# Case 3
with tf.Graph().as_default() as graph:
    x = tf.placeholder(tf.float32, (), 'x')
   
    out = x
    out = A(name='a_1')(out)
    out = A(name='a_2')(out)
    out = A(name='a_3')(out)
    
    tf.summary.FileWriter('/tmp/tensorboard/3', graph=graph).close()
```

#### Results

Other than case 3, an unexpected graph is generated.
Is this a bug?

- Case 1

<img alt=""result"" height=""450"" src=""https://user-images.githubusercontent.com/7009040/31054076-b290a772-a6e5-11e7-8e6a-2d97420b5e0e.png"">

- Case 2

<img alt=""result"" height=""450"" src=""https://user-images.githubusercontent.com/7009040/31054078-b2a7ca7e-a6e5-11e7-9a17-805fd56e6ea7.png"">

- Case 3

<img alt=""result"" height=""450"" src=""https://user-images.githubusercontent.com/7009040/31054077-b291b7c0-a6e5-11e7-9256-be7c986094ec.png"">"
13426,//tensorflow/contrib/android:libtensorflow_inference.so build fails when compiling @protobuf//:protobuf,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Docker image gcr.io/tensorflow/tensorflow:1.3.0-devel
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**: 0.5.0

Output from `tf_env_collect.sh` is at the end of this report.

### Describe the problem
I'm trying to follow the instructions in [print_selective_registration_header.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/print_selective_registration_header.py#L15) to create a smaller TensorFlow binary size.

Part of those instructions involves building ` //tensorflow/contrib/android:libtensorflow_inference.so`, but that build fails every time.

I'm doing this in the `gcr.io/tensorflow/tensorflow:1.3.0-devel` Docker container (not sure if this is appropriate because I can't find documentation explaining what each container is for).  I tried to use the `1.3.0` container, but that doesn't contain the TensorFlow repo or `git`.

### Source code / logs

Here are the steps I took (the first steps succeeded so I have not included their output):
```
$ docker run -it -v $HOME/TF:/TF gcr.io/tensorflow/tensorflow:1.3.0-devel bash

# bazel build tensorflow/python/tools:print_selective_registration_header

# bazel-bin/tensorflow/python/tools/print_selective_registration_header --graphs=/TF/mnist_model_graph.pb > ops_to_register.h

# bazel build -c opt --copt=""-DSELECTIVE_REGISTRATION"" --copt=""-DSUPPORT_SELECTIVE_REGISTRATION"" //tensorflow/contrib/android:libtensorflow_inference.so     --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --verbose_failures
INFO: Reading 'startup' options from /etc/bazel.bazelrc: --batch
WARNING: /tensorflow/tensorflow/core/BUILD:935:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.
<snip repeated warning>
WARNING: /tensorflow/tensorflow/core/BUILD:935:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.
INFO: Found 1 target...
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/protobuf/BUILD:133:1: C++ compilation of rule '@protobuf//:protobuf' failed: false failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL=0 \
  /bin/false -DSELECTIVE_REGISTRATION -DSUPPORT_SELECTIVE_REGISTRATION -MD -MF bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/io/printer.pic.d '-frandom-seed=bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/io/printer.pic.o' -fPIC -iquote external/protobuf -iquote bazel-out/stub_armeabi-v7a-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/stub_armeabi-v7a-opt/genfiles/external/bazel_tools -isystem external/protobuf/src -isystem bazel-out/stub_armeabi-v7a-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -c external/protobuf/src/google/protobuf/io/printer.cc -o bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/io/printer.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.
Target //tensorflow/contrib/android:libtensorflow_inference.so failed to build
INFO: Elapsed time: 202.538s, Critical Path: 11.70s
```
I also tried (with the same result):
 - `bazel clean` before the failing build step.
 - Removing the ops_to_register.h file and the `--copt` parameters.

Here's my full environment info (which shows an error when running pywrap_tensorflow_internal):
```
# cat tf_env.txt

== cat /etc/issue ===============================================
Linux 813a49ffd3e0 4.9.27-moby #1 SMP Thu May 11 04:01:18 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 813a49ffd3e0 4.9.27-moby #1 SMP Thu May 11 04:01:18 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.1)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.2)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
ImportError: No module named pywrap_tensorflow_internal


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tools/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```"
13423,In what cases a OpKernel instance may be accessed concurrently?,"In the documentation for adding new op, it is said that Compute method of OpKernel should be thread safe, because instances of your OpKernel may be accessed concurrently. I want to know in what cases an instance may be accessed concurrently. If the graph has no loop or any other control flow,does this guarantee that every OpKernel instance is accessed only once for each forward/backward pass?

Thank you."
13419,BUG: variables outside won't update in DNNLinearCombinedRegressor,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.11.6
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A


### Describe the problem

Variables outside won't update for `DNNLinearCombinedRegressor`, while everything is OK for `DNNRegressor`.

The bug stems from that only variables in dnn / linear scope will be updated in the code below:

https://github.com/tensorflow/tensorflow/blob/107cc777af7880c140d089e44ad898a6ba929286/tensorflow/python/estimator/canned/dnn_linear_combined.py#L214-L227


### Source code / logs

This is a tiny code to see whether `w` is updated or not.

```python
import numpy as np

import tensorflow as tf
from tensorflow import feature_column as fc
from tensorflow.python.summary import summary

tf.logging.set_verbosity(tf.logging.DEBUG)


BATCH_SIZE = 4


def input_fn():
    x = tf.constant(np.random.randn(BATCH_SIZE, 4), dtype=tf.float32)

    w = tf.Variable(np.array([1, 2, 3, 4]).reshape((4, 1)), dtype=tf.float32, name=""test/w"")
    summary.scalar(""test/w_0_0"", w[0][0])
    summary.scalar(""test/w_1_0"", w[1][0])
    summary.scalar(""test/w_2_0"", w[2][0])
    summary.scalar(""test/w_3_0"", w[3][0])

    y = tf.matmul(x, w)
    label = tf.constant(np.random.randint(0, 1, size=(BATCH_SIZE,)))

    return {""y"": y}, label

f = fc.numeric_column(""y"")

def gen_estimator(cls, model_dir):
    if cls == ""dnn"":
        return tf.estimator.DNNRegressor(
                feature_columns=[f],
                hidden_units=[2],
                model_dir=model_dir)
    else:
        return tf.estimator.DNNLinearCombinedRegressor(
                dnn_feature_columns=[f],
                dnn_hidden_units=[2],
                model_dir=model_dir)

gen_estimator(""dnn"", model_dir=""/tmp/tf/facai/test_dnn"").train(input_fn, steps=1000)
gen_estimator(""deep_and_wide"", model_dir=""/tmp/tf/facai/test_wide_and_deep"").train(input_fn, steps=1000)
```

##### Results

+ For `DNNRegressor`, `w` variables are updated:

<img width=""688"" alt=""dnn"" src=""https://user-images.githubusercontent.com/1112263/31045300-20944e50-a613-11e7-9f5d-611b5fde5de8.png"">

+ For `DNNLinearCombinedRegressor`, `w` keeps constant:

<img width=""690"" alt=""deep_and_wide"" src=""https://user-images.githubusercontent.com/1112263/31045305-5a821192-a613-11e7-8610-7a6d312e2552.png"">


"
13418,BUG: variable won't update in input_fn (or outside Estimator),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.11.6
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:


### Describe the problem

Sometimes we will preprocess our data before feeding them into `Estimator`. For example, text data will be split or truncated at first, and then we might create a shallow convolution layer for it. However, it seems that those variables, if created, won't update in training.

I create a tiny code below by using `input_fn` to clarify my question: variable `w` seems its initial value after training.


### Source code / logs

code:

```python
import numpy as np

import tensorflow as tf
from tensorflow import feature_column as fc
from tensorflow.python.summary import summary

tf.logging.set_verbosity(tf.logging.DEBUG)


BATCH_SIZE=4


def input_fn():
    x = tf.constant(np.random.randn(BATCH_SIZE, 4), dtype=tf.float32)

    w = tf.Variable(np.array([1, 2, 3, 4]).reshape((4, 1)), dtype=tf.float32, name=""test/w"")
    summary.scalar(""test/w[0][0]"", w[0][0])
    summary.scalar(""test/w[1][0]"", w[1][0])
    summary.scalar(""test/w[2][0]"", w[2][0])
    summary.scalar(""test/w[3][0]"", w[3][0])

    y = tf.to_int64(tf.matmul(x, w))
    label = tf.constant(np.random.randint(0, 1, size=(BATCH_SIZE,)))

    return {""y"": y}, label

f = fc.embedding_column(
        fc.categorical_column_with_hash_bucket(""y"", 4, dtype=tf.int64),
        dimension=2)

e = tf.estimator.DNNRegressor(
        feature_columns=[f],
        hidden_units=[2],
        model_dir=""/tmp/tf/facai/test"")

e.train(input_fn, steps=1000)
```

logs:
```
~/Downloads ❯❯❯ python test.py
INFO:tensorflow:Using default config.
INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf/facai/test', '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_tf_random_seed': 1, '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600, '_session_config': None, '_save_checkpoints_steps': None, '_save_summary_steps': 100}
INFO:tensorflow:Summary name test/w[0][0] is illegal; using test/w_0__0_ instead.
INFO:tensorflow:Summary name test/w[1][0] is illegal; using test/w_1__0_ instead.
INFO:tensorflow:Summary name test/w[2][0] is illegal; using test/w_2__0_ instead.
INFO:tensorflow:Summary name test/w[3][0] is illegal; using test/w_3__0_ instead.
DEBUG:tensorflow:Transforming feature_column _HashedCategoricalColumn(key='y', hash_bucket_size=4, dtype=tf.int64).
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Saving checkpoints for 1 into /tmp/tf/facai/test/model.ckpt.
INFO:tensorflow:step = 1, loss = 18.9489
INFO:tensorflow:global_step/sec: 723.222
INFO:tensorflow:step = 101, loss = 0.187707 (0.138 sec)
INFO:tensorflow:global_step/sec: 817.997
INFO:tensorflow:step = 201, loss = 0.0705907 (0.123 sec)
INFO:tensorflow:global_step/sec: 777.309
INFO:tensorflow:step = 301, loss = 0.037501 (0.128 sec)
INFO:tensorflow:global_step/sec: 795.153
INFO:tensorflow:step = 401, loss = 0.0219456 (0.126 sec)
INFO:tensorflow:global_step/sec: 810.99
INFO:tensorflow:step = 501, loss = 0.0132351 (0.123 sec)
INFO:tensorflow:global_step/sec: 744.186
INFO:tensorflow:step = 601, loss = 0.00807175 (0.134 sec)
INFO:tensorflow:global_step/sec: 778.229
INFO:tensorflow:step = 701, loss = 0.00494827 (0.129 sec)
INFO:tensorflow:global_step/sec: 760.138
INFO:tensorflow:step = 801, loss = 0.00304232 (0.131 sec)
INFO:tensorflow:global_step/sec: 830.641
INFO:tensorflow:step = 901, loss = 0.00187403 (0.120 sec)
INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tf/facai/test/model.ckpt.
INFO:tensorflow:Loss for final step: 0.00116151.
```

<img width=""701"" alt=""screen shot 2017-09-30 at 5 40 29 pm"" src=""https://user-images.githubusercontent.com/1112263/31044707-d92a2ba4-a606-11e7-93c5-457ac8658141.png"">


"
13417,Is it wrong design of slim.learning.train?,"Look at this [issue](https://github.com/tensorflow/tensorflow/issues/13342),  see my commit."
13416,Install tensorflow-gpu in Ubuntun16.04 meet some problem,"Successfully installed tensorflow-gpu-1.3.0
t91@ubuntu:~/pengyulong$ python 
Python 3.5.2 (default, Nov 17 2016, 17:05:23) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/t91/pengyulong/tensorflow/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/t91/pengyulong/tensorflow/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/t91/pengyulong/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/t91/pengyulong/tensorflow/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/t91/pengyulong/tensorflow/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.


P.S:run this Machine configure list:GPU:P100*4,cuda9.0,cudnn7.0"
13413,V1.3.1 undefined reference to `clock_gettime' Error,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
ReaHat 6.5 with self compiled gcc 4.8.2 installed locally
- **TensorFlow installed from (source or binary)**:
Compile from source
- **TensorFlow version (use command below)**:
- **Python version**: 
2.7.14 compiled from source
- **Bazel version (if compiling from source)**:
5.4.0 Compiled from source
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
bazel build //tensorflow/examples/label_image:label_image
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I working on Redhat 6.5, with gcc 4.8.2 compiled from source and installed locally, binutiles like ld, as were not updated, glic was 2.12.

I managed to compile tensorflow 1.3.1 shared C++ libs libtensorflow_cc.so, but when Iinking to my projects, it encounted with ""undefined reference to `clock_gettime' error. This error can also be reproduced by compiling label image examples with command 
``` 
bazel build //tensorflow/examples/label_image:label_image
``` 
Here are the error details:

/home/xxxxxxx/opensource/tensorflow/tensorflow/tensorflow/examples/label_image/BUILD:10:1: Linking of rule '//tensorflow/examples/label_image:label_image' failed (Exit 1).
bazel-out/local-opt/bin/tensorflow/core/kernels/libattention_ops.lo(attention_ops.o): In function `void Eigen::(anonymous namespace)::GlimpseExtractionOp<long>::eval<Eigen::TensorLayoutSwapOp<Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const>, Eigen::TensorMap<Eigen::Tensor<float, 4, 0, long>, 0, Eigen::MakePointer>, Eigen::ThreadPoolDevice>(Eigen::TensorLayoutSwapOp<Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const&, Eigen::TensorMap<Eigen::Tensor<float, 4, 0, long>, 0, Eigen::MakePointer>&, Eigen::ThreadPoolDevice const&) const':
attention_ops.cc:(.text.unlikely._ZNK5Eigen12_GLOBAL__N_119GlimpseExtractionOpIlE4evalINS_18TensorLayoutSwapOpIKNS_9TensorMapINS_6TensorIKfLi4ELi1ElEELi16ENS_11MakePointerEEEEENS5_INS6_IfLi4ELi0ElEELi0ES9_EENS_16ThreadPoolDeviceEEEvRKT_RT0_RKT1_+0xbb): undefined reference to `clock_gettime'
attention_ops.cc:(.text.unlikely._ZNK5Eigen12_GLOBAL__N_119GlimpseExtractionOpIlE4evalINS_18TensorLayoutSwapOpIKNS_9TensorMapINS_6TensorIKfLi4ELi1ElEELi16ENS_11MakePointerEEEEENS5_INS6_IfLi4ELi0ElEELi0ES9_EENS_16ThreadPoolDeviceEEEvRKT_RT0_RKT1_+0xdf): undefined reference to `clock_gettime'
collect2: error: ld returned 1 exit status
Target //tensorflow/examples/label_image:label_image failed to build
Use --verbose_failures to see the command lines of failed build steps.


I compiled python lib on the exactly the same environment. I had this error when running "" import tensorflow"". The error was fixed by modifying  ""return []"" to 'return [""-lrt""] ' in tensorflow/tensorflow.bzl line 975.

I tried the same trick for label_image example, but it didn't work out.




### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13405,tensorflow.contrib.data.Dataset,"Issue with tensorflow.contrib.data module 

![screenshot from 2017-09-30 10-25-36](https://user-images.githubusercontent.com/13569817/31042660-9b2323a6-a5ca-11e7-88f9-2960cd7439d2.png)
"
13403,tensorflow multi -GPU lstm :ValueError: None values not supported.,"I am trying to implement a Multi-GPU LSTM Using the [muti-gpu cifar10 ](https://github.com/normanheckscher/mnist-multi-gpu/blob/master/mnist_multi_gpu_batching_train.py)
My code is present [here ](https://github.com/AbdalaDiasse/ancun/blob/master/multi_gpu_lstm.py)
How ever when i run the code I got the following issues:

`Traceback (most recent call last):
  File ""multi_gpu_lstm.py"", line 227, in <module>
    tf.app.run()
  File ""/BIGDATA/app/TensorFlow/python-venv/py2.9-gpu/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 44, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""multi_gpu_lstm.py"", line 224, in main
    training()
  File ""multi_gpu_lstm.py"", line 164, in training
    tower_grads_avg = average_gradients(tower_grads)
  File ""multi_gpu_lstm.py"", line 105, in average_gradients
    expanded_g = tf.expand_dims(g, 0)
  File ""/BIGDATA/app/TensorFlow/python-venv/py2.9-gpu/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 168, in expand_dims
    return gen_array_ops._expand_dims(input, axis, name)
  File ""/BIGDATA/app/TensorFlow/python-venv/py2.9-gpu/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1051, in _expand_dims
    result = _op_def_lib.apply_op(""ExpandDims"", input=input, dim=dim, name=name)
  File ""/BIGDATA/app/TensorFlow/python-venv/py2.9-gpu/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 504, in apply_op
    values, as_ref=input_arg.is_ref).dtype.name
  File ""/BIGDATA/app/TensorFlow/python-venv/py2.9-gpu/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 702, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/BIGDATA/app/TensorFlow/python-venv/py2.9-gpu/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 110, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/BIGDATA/app/TensorFlow/python-venv/py2.9-gpu/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 99, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/BIGDATA/app/TensorFlow/python-venv/py2.9-gpu/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 360, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.
`
The problem is raised at this line:
`expanded_g = tf.expand_dims(g, 0)`
it's because g gets None values . if I modify to:
`if g is not None:
    expanded_g = tf.expand_dims(g, 0)`
everything work very fine. After investigation I have realized grads get None value at this line :
`grads = optimizer.compute_gradients(loss_op)`
When I print grads this is what I got : 
`[(<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/MatMul_grad/tuple/control_dependency_1:0' shape=(28, 1) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac80ff32110>), (<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/MatMul_1_grad/tuple/control_dependency_1:0' shape=(1, 1) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac810ad1f90>), (<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/add_grad/tuple/control_dependency_1:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac810ad19d0>), (<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/add_1_grad/tuple/control_dependency_1:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac810ae9d50>), (<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(2, 4) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac814428c50>), (<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(4,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac814438bd0>), (<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/mul/Enter_grad/b_acc_3:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac814446650>), (<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/mul_2/Enter_grad/b_acc_3:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac814446690>), (<tf.Tensor 'Tower_0/gradients/Tower_0/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/mul_4/Enter_grad/b_acc_3:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac8144466d0>)]
  [(None, <tensorflow.python.ops.variables.Variable object at 0x2ac80ff32110>), (None, <tensorflow.python.ops.variables.Variable object at 0x2ac810ad1f90>), (None, <tensorflow.python.ops.variables.Variable object at 0x2ac810ad19d0>), (None, <tensorflow.python.ops.variables.Variable object at 0x2ac810ae9d50>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(2, 4) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac814428c50>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(4,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac814438bd0>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/mul/Enter_grad/b_acc_3:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac814446650>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/mul_2/Enter_grad/b_acc_3:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac814446690>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/rnn/while/multi_rnn_cell/cell_0/lstm_cell/mul_4/Enter_grad/b_acc_3:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac8144466d0>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/MatMul_grad/tuple/control_dependency_1:0' shape=(28, 1) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac80ff32150>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/MatMul_1_grad/tuple/control_dependency_1:0' shape=(1, 1) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac8152d9b90>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/add_grad/tuple/control_dependency_1:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac8152d9c50>), (<tf.Tensor 'Tower_1/gradients/Tower_1/lstm/add_1_grad/tuple/control_dependency_1:0' shape=(1,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2ac8152fd090>)]`

We can clearly see that some None value from grads

I don't know why but it seems to be very strange to be .I think its probably related to some variable that are wrongly set. PLEASE HELP ME TO FIND OUT 

"
13397,tf.read_file isn't in the informed folder,"the documentation below:

https://www.tensorflow.org/api_docs/python/tf/read_file

Says that this function is here:

https://github.com/tensorflow/tensorflow/tree/r1.3/tensorflow/python/ops

But isn't .

------------------------

Thanks,"
13387,ERROR: Failed to import the TensorFlow module.,"I was trying to install tensorflow-gpu. Using the tensorflow self check, it says all the required DLLs are present, but I still have an import error. I get the same error if I try to import numpy. There seems to be an issue with my conda command as well. Any help is greatly appreciated to resolve this issue. Thanks!

PS C:\Users\andrew\Documents> python tensorflow_self_check.py
ERROR: Failed to import the TensorFlow module.

- Python version is 3.6.

- TensorFlow is installed at: C:\Users\andre\Anaconda3\lib\site-packages\tensorflow

- All required DLLs appear to be present. Please open an issue on the
  TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues

PS C:\Users\andrew\Documents> conda create -n tensorflow_gpu python=3.6
usage: conda [-h]
             {keygen,sign,unsign,verify,unpack,install,install-scripts,convert,version,help}
             ...
conda: error: invalid choice: 'create' (choose from 'keygen', 'sign', 'unsign', 'verify', 'unpack', 'install', 'install-
scripts', 'convert', 'version', 'help')
"
13384,Tensorflow - Error: Cannot convert value dtype('<f4') to a TensorFlow DType,"I receive an error when testing a Neural Style Transformation project with Tensorflow & OpenCV using the ""lion"" test image. The source of test project is:  [ [https://github.com/cysmith/neural-style-tf](https://github.com/cysmith/neural-style-tf) ]. Both Tensorflow and OpenCV packages were installed and compiled from source. The training model used is: imagenet-vgg-verydeep-19.mat and the runtime environment runs on a s390x CPU and does not use any CUDA support. The  current Python environment is 2.7 under Docker Linux Ubuntu 16.

Run command:
`bash stylize_image.sh ./image_input/lion.jpg ./styles/kandinsky.jpg`

The error message at log is:
```
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/dtypes.py"", line 584, in as_dtype
    ""Cannot convert value %r to a TensorFlow DType."" % type_value)
TypeError: Cannot convert value dtype('<f4') to a TensorFlow DType.
```

Versions installed: 

```
python 2.7.12
import tensorflow as tf 
>>> print tf.VERSION
1.3.1
import cv2 
>>> print cv2.__version__ 
3.3.0
>>> print np.__version__
1.13.2

```
Is this a known issue? 

"
13381,tf.train.Saver setting `max_to_keep` parameter to 0 or None unintended behavior,"As the [tf.train.Saver](https://www.tensorflow.org/api_docs/python/tf/train/Saver) documentation says:

""`max_to_keep` indicates the maximum number of recent checkpoint files to keep. As new files are created, older files are deleted. If None or 0, all checkpoint files are kept. Defaults to 5 (that is, the 5 most recent checkpoint files are kept.)""

However, when I use `max_to_keep=0`, the `checkpoint` file attribute `all_model_checkpoint_paths` only records the most recent checkpoint file (and I'm unable to load older checkpoint files). I believe this is due to the block:

```python
if len(self._last_checkpoints) > self.saver_def.max_to_keep:
    self._checkpoints_to_be_deleted.append(self._last_checkpoints.pop(0))
```

It should probably instead read something like:

```python
if len(self._last_checkpoints) > self.saver_def.max_to_keep > 0:
    self._checkpoints_to_be_deleted.append(self._last_checkpoints.pop(0))
```"
13380,"'train_x, train_y = sess.run([train_x, train_y])'  leads machine run slowly","Hi Team,
     My program run without error, but it seems not work, while my GPU shows running. The main code are as follows:
`def main(file_name, batch_size, iter_times):
    x = tf.placeholder('float', (batch_size, 32, 32, 3) )
    y = tf.placeholder('float', shape = [batch_size, 10] )

    predictions, _, _, _ = inference_op(x, keep_prob = 0.5)
    predictions = tf.cast(predictions, tf.float32)

    cost = -tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits (labels = y, logits = predictions))
    correct_prediction = tf.equal(y, predictions)
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))
    train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)
    init = tf.global_variables_initializer()
    sess = tf.Session()
    sess.run(init)
    for i in xrange(iter_times):
        train_x, train_y = read_data.fetch_data(file_name, batch_size) 
        train_x, train_y = sess.run([train_x, train_y]) 
        if i % 10 == 0 :
            train_accuracy = accuracy.eval(feed_dict = {x: train_x, y: train_y})
            print ""%d step accuarcy is %f"" % (i, sess.run(train_accuracy))
        sess.run(train)

main('train.tfrecords', 5, 2000)`

This net is based on  VGG (without the 3rd,4th,5th hidden layers) and the dataset is SVHN. I don't know how to deal with this phenomenon......
    "
13379,parameterized_docker_build.sh fails,"### Issue

Executing parameterized_docker_build.sh fails to generate new docker image, throws error message.

### System information
- I have used a stock example script provided in TensorFlow
- Windows 10 professional
- TensorFlow install as docker image tensorflow/tensorflow:1.3.0-devel-py3
- TensorFlow version 1.3
- Python version 3
- Bazel version 0.5.0
- CUDA/cuDNN version not relevant (CPU install)
- GPU model and memory not relevant (CPU install)

### Set-up

```
>>> docker pull tensorflow/tensorflow:1.3.0-devel-py3
>>> docker run -it tensorflow/tensorflow:1.3.0-devel-py3
$ cd /
$ cd /tensorflow/tensorflow/tools/docker
$ chmod +x  /tensorflow/tensorflow/tools/docker/parameterized_docker_build.sh
$ export TF_DOCKER_BUILD_IS_DEVEL=NO
$ export TF_DOCKER_BUILD_TYPE=CPU
$ export TF_DOCKER_BUILD_PYTHON_VERSION=PYTHON3
$ export NIGHTLY_VERSION=""1.head""
$ export TF_DOCKER_BUILD_CENTRAL_PIP=$(echo ${TF_DOCKER_BUILD_PYTHON_VERSION} | sed s^PYTHON2^http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=${TF_DOCKER_BUILD_PYTHON_VERSION},label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-${NIGHTLY_VERSION}-cp27-cp27mu-manylinux1_x86_64.whl^ | sed s^PYTHON3^http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-${NIGHTLY_VERSION}-cp35-cp35m-manylinux1_x86_64.whl^)
```

### Command triggering issue

`$ /tensorflow/tensorflow/tools/docker/parameterized_docker_build.sh`

### Error messages

> Required build parameters:
>   TF_DOCKER_BUILD_TYPE=cpu
>   TF_DOCKER_BUILD_IS_DEVEL=no
>   TF_DOCKER_BUILD_DEVEL_BRANCH=NO
> 
> Optional build parameters:
>   TF_DOCKER_BUILD_CENTRAL_PIP=http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl
>   TF_DOCKER_BUILD_IMAGE_NAME=
>   TF_DOCKER_BUILD_VERSION=
>   TF_DOCKER_BUILD_PORT=
>   TF_DOCKER_BUILD_PUSH_CMD=
> ERROR: docker is not available on path"
13377,bazel version 0.6.0 unable to build latest stable release (v1.3.1),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.3.1
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.6.0
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: `bazel build //tensorflow/tools/pip_package:build_pip_package`

### Describe the problem

With bazel version 0.6.0, building on the latest stable tagged release (v1.3.1) is not possible. I get the following error:

```
ERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/8ea56cff279f39ec6c0003641e649819/external/io_bazel_rules_closure/closure/private/defs.bzl:27:16: The `set` constructor for depsets is deprecated and will be removed. Please use the `depset` constructor instead. You can temporarily enable the deprecated `set` constructor by passing the flag --incompatible_disallow_set_constructor=false
ERROR: error loading package '': Extension file 'closure/private/defs.bzl' has errors
ERROR: error loading package '': Extension file 'closure/private/defs.bzl' has errors
INFO: Elapsed time: 0.569s
FAILED: Build did NOT complete successfully (0 packages loaded)
```

When I pass the flag mentioned in the error message (`--incompatible_disallow_set_constructor=false`) to the pip package build command, I get a similar message:

```
ERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/8ea56cff279f39ec6c0003641e649819/external/org_python_pypi_backports_weakref/BUILD.bazel:17:1: no such package '@org_python_license//': The `set` constructor for depsets is deprecated and will be removed. Please use the `depset` constructor instead. You can temporarily enable the deprecated `set` constructor by passing the flag --incompatible_disallow_set_constructor=false and referenced by '@org_python_pypi_backports_weakref//:license'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted
INFO: Elapsed time: 0.626s
FAILED: Build did NOT complete successfully (28 packages loaded)
    currently loading: tensorflow/core
```

This problem does not exist on `master`. It would be very useful to have even a tiny stable release (v1.3.2 or something) to include this fix for those of us that like to build against a stable release.
"
13376,MatMul in TensorFlow is slower than dot product in numpy,"I am observing that on my machine tf.matmul in tensorflow is running significantly slower than dot product in numpy. I have GTX 1080 GPU, and expecting tf.matmul to be at least as fast as when running the code using CPU (numpy).

**Environment Info

Operating System**

```
lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 16.10
Release:	16.10
Codename:	yakkety
```
**Installed version of CUDA and cuDNN:**

```
ls -l /usr/local/cuda-8.0/lib64/libcud*
-rw-r--r-- 1 root      root    556000 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudadevrt.a
lrwxrwxrwx 1 root      root        16 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root      root        19 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
-rwxr-xr-x 1 root      root    415432 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
-rw-r--r-- 1 root      root    775162 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudart_static.a
lrwxrwxrwx 1 voldemaro users       13 Nov  6  2016 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5
lrwxrwxrwx 1 voldemaro users       18 Nov  6  2016 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10
-rwxr-xr-x 1 voldemaro users 84163560 Nov  6  2016 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10
-rw-r--r-- 1 voldemaro users 70364814 Nov  6  2016 /usr/local/cuda-8.0/lib64/libcudnn_static.a
```
**TensorFlow Setup**

```
python -c ""import tensorflow; print(tensorflow.__version__)""
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
1.0.0
```
**Code:**

```
'''
Created on Sep 28, 2017

@author: voldemaro

Running on I7/GTX 1080

no MKL
('TF version: ', 'v1.0.0-rc2-15-g47bba63-dirty')
('TF url: ', 'https://github.com/tensorflow/tensorflow/commit/47bba63')
Timing in ms for 2048 x 2048 SVD of type <type 'numpy.float32'> and matmul for 16920 x 2048 of type <type 'numpy.float32'>
numpy default SVD    min:  3956.20, median:  4127.75, mean:  4264.41
TF CPU SVD           min:  5926.43, median:  5951.70, mean:  5961.43
TF GPU SVD           min:  5917.10, median:  6015.87, mean:  6039.63
numpy default .dot product min:  5816.97, median:  5933.43, mean:  5965.22
TF CPU matmul        min: 21939.19, median: 22485.99, mean: 22374.69
TF GPU matmul        min: 22026.52, median: 22109.97, mean: 22199.43
'''

from scipy import linalg;  # for svd
import numpy as np;
import os;
import sys;
import time;

os.environ[""TF_CPP_MIN_LOG_LEVEL""]=""2""  # nospam

import tensorflow as tf;
import gc; gc.disable();

NUM_RUNS = 5;
dtype = np.float32;
N=2048;
M =  16920;


def get_tensorflow_version_url():
    import tensorflow as tf
    version=tf.__version__
    commit = tf.__git_version__
    # commit looks like this
    # 'v1.0.0-65-g4763edf-dirty'
    commit = commit.replace(""'"","""")
    if commit.endswith('-dirty'):
        dirty = True
        commit = commit[:-len('-dirty')]
    commit=commit.rsplit('-g', 1)[1]
    url = 'https://github.com/tensorflow/tensorflow/commit/'+commit
    return url

def get_mkl_version():
    import ctypes
    import numpy as np
    ver = np.zeros(199, dtype=np.uint8)
    mkl = ctypes.cdll.LoadLibrary(""libmkl_rt.so"")
    mkl.MKL_Get_Version_String(ver.ctypes.data_as(ctypes.c_char_p), 198)
    return ver[ver != 0].tostring()

timeline_counter = 0
run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE);


def benchmark(message, func):
    time_list = []
    for i in range(NUM_RUNS):
        start_time = time.time();
        func();
        time_list.append(time.time()-start_time);

    time_list = 1000*np.array(time_list);  # get seconds, convert to ms
    if len(time_list)>0:
        min = np.min(time_list);
        median = np.median(time_list);
        formatted = [""%.2f""%(d,) for d in time_list[:10]];
        result = ""min: %8.2f, median: %8.2f, mean: %8.2f""%(min, median, np.mean(time_list))
    else:
        result = ""empty""
    print(""%-20s %s""%(message, result))
    

if np.__config__.get_info(""lapack_mkl_info""):
    print(""MKL version"", get_mkl_version())
else:
    print(""no MKL"")

print(""TF version: "", tf.__git_version__)
print(""TF url: "", get_tensorflow_version_url())


svd_array = np.random.random_sample((N,N)).astype(dtype);
another_array = np.random.random_sample((M,N)).astype(dtype);

init_OP = tf.global_variables_initializer();


with tf.device(""/gpu:0""):
    init_holder_gpu = tf.placeholder(dtype, shape=(M,M));
    
    specVarGPU = tf.random_uniform((N,N), dtype=dtype);
    S_gpu = tf.random_uniform((M,N), dtype=dtype);
    V_gpu = tf.matmul(tf.matmul(tf.transpose(tf.transpose(tf.conj(S_gpu))), specVarGPU, ), tf.transpose(S_gpu));
    [D2_gpu, E1_gpu,  E2_gpu] = tf.svd(specVarGPU);

with tf.device(""/cpu:0""):
    init_holder_cpu = tf.placeholder(dtype, shape=(M,M));
    specVarCPU = tf.random_uniform((N,N), dtype=dtype);
    S_cpu = tf.random_uniform((M,N), dtype=dtype);
    V_cpu = tf.matmul(tf.matmul(tf.transpose(tf.transpose(tf.conj(S_cpu))), specVarCPU, ), tf.transpose(S_cpu));
    
    
    [D2_cpu, E1_cpu,  E2_cpu] = tf.svd(specVarCPU);
    V_cpu = tf.matmul(tf.matmul(tf.transpose(tf.transpose(tf.conj(S_cpu))), E1_cpu), tf.transpose(S_cpu));

print(""Timing in ms for %d x %d SVD of type %s and matmul for %d x %d of type %s""%(N, N, dtype, M, N, dtype));

def func(): linalg.svd(svd_array)
benchmark(""numpy default SVD"", func)

config = tf.ConfigProto(allow_soft_placement = True, graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)));
sess = tf.Session(config = config);
sess.run(init_OP);

def func2(): sess.run([D2_cpu.op, E1_cpu.op,  E2_cpu.op]);
benchmark(""TF CPU SVD"", func2);

def func3(): sess.run([D2_gpu.op, E1_gpu.op,  E2_gpu.op]);
benchmark(""TF GPU SVD"", func3);

def func1(): np.transpose(np.asmatrix(another_array)).getH().dot(svd_array).dot(np.transpose(another_array));
benchmark(""numpy default .dot product"", func1)

def func4(): sess.run([V_cpu]);
benchmark(""TF CPU matmul"", func4)

def func5(): sess.run([V_gpu])
benchmark(""TF GPU matmul"", func4)

```
"
13375,Add python 3 iterator support for `tf.convert_to_tensor`,"I am working on TensorFlow 1.3.0 and Python 3.6.3.

I found that I have to write many unnecessary code to cast python iterator (such as return value of map, filter, zip and dict_keys) to `list` so that `tf.convert_to_tensor` could work. Supporting convert python iterator to tensor could not only help python 3 user write concise code without many `list(xxx)`, but also memory saving."
13373,How to use the output of the graph as a parameter of the loss function,"Is there any example about using the output of the graph as a parameter of the loss function?
For example:

_,_,summary_str_train, **acc_train** = sess.run([train_step, cross_entropy, merged_summary_op, accuracy], feed_dict={x: batch_xs, y: batch_ys, keep_prob: 0.7})

acc_train is the accuracy of the current minibatch, output of the graph, I want to use the value of the acc_train in the loss function? the cross_entropy_loss is a customized function. 

cross_entropy = tf.reduce_mean(cross_entropy_loss(logits=prediction, one_hot_labels=y, accuracy=acc_train))


Is there any example about this? Thanks "
13370,Error with conditional labelling of summary,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip install tensorflow --upgrade
- **TensorFlow version (use command below)**:1.3.0
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: Cuda compilation tools, release 7.5, V7.5.17
- **GPU model and memory**: NVIDIA 1070 16G
- **Exact command to reproduce**: python model_wgraph2.py

### Describe the problem

I am attempting to vary my labels for the summary conditional on some tf.bool data type.  I get an error when I attempt to use the tf.cond.  

Per: https://github.com/tensorflow/tensor2tensor/issues/159,
this problem should have been fixed in version 1.1.0

""lukaszkaiser:

This was due to summaries not working with tf.conds right. Should be corrected in 1.1.0, please take a look and reopen if you see this again!""
--



### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
	text2 = tf.cast('blah', tf.string)	
	text_summary = tf.summary.text(""NULL"", text2) #works fine without conditional	
	rightness = tf.cast(True, tf.bool)
	text_summary = tf.cond( rightness, lambda: tf.summary.text(""TRUE"", text2), lambda: tf.summary.text(""FALSE"", text2) ) #breaks with error below

###Error
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value
"
13365,Installing Tensorflow 1.3.0 succeeds even when the cuDNN version is not correct,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.4
- **TensorFlow installed from (source or binary)**: pip (binary?)
- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 8.0, cuDNN 5.1
- **GPU model and memory**: TITAN X (Pascal), 12GB
- **Exact command to reproduce**:  

 * Install tensorflow: `pip install tensorflow-gpu`
 * Try to use tensorflow: `python -c ""import tensorflow;""

### Describe the problem
When imported, Tensorflow looks for `libcudnn.so.6` and does not find it, since only 5.1 is installed, and fails with an error message

What I would expect to happen is a) it either compiles against the version of libcudnn.so actually available, or b) it tries to check whether the correct version exists at install time and the install fails with a helpful message if it does not.

This is also not documented in https://www.tensorflow.org/install/install_linux or https://www.tensorflow.org/install/install_sources, in fact all it says for software requirements is ""cuDNN (>= v3). We recommend version 5.1"", which is what I had installed.

Thank you!

### Source code / logs

N/A"
13363,Remove frame_length restrictions in tf.contrib.signal.stft/inverse_stft.,"So far tf.contrib.signal.stft() does not allow a larger fft_length than frame_length.
```
 raise ValueError('frame_length (%d) may not be larger than '
                       'fft_length (%d)' % (frame_length_static,
                                            fft_length_static))
```
 I think it is worth to allow this option by zero-padding the input frames, matching fft_length, since it is the usual proceeding that provides a smooth time-freq representation. "
13362,Trying to run 2_fullyconnected.ipynb but neural net doesn't learn,"I'm trying to run that notebook as it is but model doesn't learn at all. Initially I got right results like in sample but now I'm instantly getting such output when train a multinomial logistic regression using simple gradient descent:

Initialized
Loss at step 0: 24.628622
Training accuracy: 9.7%
Validation accuracy: 10.0%
Loss at step 100: 35.364712
Training accuracy: 9.9%
Validation accuracy: 9.6%
Loss at step 200: 38.703053
Training accuracy: 9.9%
Validation accuracy: 10.0%
Loss at step 300: 30.087294
Training accuracy: 10.1%
Validation accuracy: 10.0%
Loss at step 400: 35.924911
Training accuracy: 9.9%
Validation accuracy: 10.0%
Loss at step 500: 38.568333
Training accuracy: 10.5%
Validation accuracy: 10.0%
Loss at step 600: 31.410255
Training accuracy: 10.8%
Validation accuracy: 10.0%
Loss at step 700: 42.134827
Training accuracy: 10.1%
Validation accuracy: 10.0%
Loss at step 800: 26.819206
Training accuracy: 9.9%
Validation accuracy: 10.0%
Test accuracy: 10.0%

When I switch to switch to stochastic gradient descent training I get simmilar output. Restarting kernel and even VM doesn't work. I also tried to delete preveously generated pickle and generate it from scratch but it also doesn't help.

What might cause my neural net to stop learning at some point?"
13361,'tensorflow.python.ops.nn' has no attribute 'selu',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 8.0
- **GPU model and memory**: GTX 1080
- **Exact command to reproduce**: 
```
import tensorflow as tf
...
y = tf.nn.selu(x)
```

### Describe the problem
TensorFlow 1.3 cannot find selu activation. Relu and elu work fine.
Relevant documentation: [tf/nn/selu](https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/selu)

### Source code / logs
```
Traceback (most recent call last):
  File ""my_file.py"", line 212, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/file/location"", line 35, in model
   y = tf.nn.selu(x)
AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'selu'
```"
13360,Feature request: tf.reduce_median(),"Hi, is there any plan to support a `tf.reduce_median` operator? Now one have to resort to `tf.nn.top_k` for implementing this."
13359,Sess.list_devices() get  swig/python detected a memory leak error.,"Look at the code:

```python
def different_graph_session_test():
	'''
		a_0 is in graph g_0, a_1 is in graph g_1,a_2 is in default graph,
		so a_0 can only run on session of g_0, a_1 can only run on session of g_1,
		a_2 can only run on session of tf.get_default_graph()
	'''
	g_0 = tf.Graph()
	g_1 = tf.Graph()
	with g_0.as_default():
		a_0 = tf.constant(1)
	with g_1.as_default():
		a_1 = tf.constant(1)
	a_2 = tf.constant(1)

	assert not a_0.graph is a_1.graph, 'different_graph_session_test wrong!'
	assert a_2.graph is tf.get_default_graph(), 'different_graph_session_test wrong!'
	
	with tf.Session(graph=g_0) as sess:
		sess.run(a_0)
		devices = sess.list_devices()
		for d in devices:
			print(d.name)
	with tf.Session(graph=g_1) as sess:
		sess.run(a_1)
		devices = sess.list_devices()
		for d in devices:
			print(d.name)
	with tf.Session() as sess:
		sess.run(a_2)
		devices = sess.list_devices()
		for d in devices:
			print(d.name)
```

It will print

```
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
/job:localhost/replica:0/task:0/device:CPU:0
/job:localhost/replica:0/task:0/device:GPU:0
swig/python detected a memory leak of type 'int64_t *', no destructor found.
swig/python detected a memory leak of type 'int64_t *', no destructor found.
```
Is it a bug or something wrong?"
13358,parameterized_docker_build.sh fails,"### Issue

Executing parameterized_docker_build.sh fails to generate new docker image, throws out multiple error messages.

### System information
- **I have used a stock example script provided in TensorFlow**
- **Windows 10 professional**
- **TensorFlow not installed (looking for generating a dockerfile)**
- **TensorFlow version 1.3:latest (eventually looking for this version)**
- **Python version 3 (eventually looking for this version)**
- **Bazel version not yet installed**
- **CUDA/cuDNN version not relevant (CPU install)**
- **GPU model and memory not relevant (CPU install)**

### Set-up

```
>>> Docker run -it Ubuntu:latest /bin/bash
$ apt-get update
$ apt-get install curl
$ cd /opt
$ curl -O https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/docker/parameterized_docker_build.sh
$ curl -O https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/docker/parameterized_docker_build.sh
$ chmod +x opt/parameterized_docker_build.sh
$ export TF_DOCKER_BUILD_IS_DEVEL=NO
$ export TF_DOCKER_BUILD_TYPE=CPU
$ export TF_DOCKER_BUILD_PYTHON_VERSION=PYTHON3
$ export NIGHTLY_VERSION=""1.head""
$ export TF_DOCKER_BUILD_CENTRAL_PIP=$(echo ${TF_DOCKER_BUILD_PYTHON_VERSION} | sed s^PYTHON2^http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=${TF_DOCKER_BUILD_PYTHON_VERSION},label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-${NIGHTLY_VERSION}-cp27-cp27mu-manylinux1_x86_64.whl^ | sed s^PYTHON3^http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-${NIGHTLY_VERSION}-cp35-cp35m-manylinux1_x86_64.whl^)
```

### Command triggering issue

`$ sh opt/parameterized_docker_build.sh`

### Error messages

> opt/parameterized_docker_build.sh: 70: opt/parameterized_docker_build.sh: Bad substitution
> opt/parameterized_docker_build.sh: 71: opt/parameterized_docker_build.sh: source: not found
> opt/parameterized_docker_build.sh: 81: opt/parameterized_docker_build.sh: to_lower: not found
> opt/parameterized_docker_build.sh: 82: opt/parameterized_docker_build.sh: to_lower: not found
> opt/parameterized_docker_build.sh: 83: opt/parameterized_docker_build.sh: to_lower: not found
> opt/parameterized_docker_build.sh: 84: opt/parameterized_docker_build.sh: to_lower: not found
> Required build parameters:
>   TF_DOCKER_BUILD_TYPE=
>   TF_DOCKER_BUILD_IS_DEVEL=
>   TF_DOCKER_BUILD_DEVEL_BRANCH=
> 
> Optional build parameters:
>   TF_DOCKER_BUILD_CENTRAL_PIP=http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl
>   TF_DOCKER_BUILD_IMAGE_NAME=
>   TF_DOCKER_BUILD_VERSION=
>   TF_DOCKER_BUILD_PORT=
>   TF_DOCKER_BUILD_PUSH_CMD=
> opt/parameterized_docker_build.sh: 102: opt/parameterized_docker_build.sh: [[: not found
> opt/parameterized_docker_build.sh: 114: opt/parameterized_docker_build.sh: [[: not found
> opt/parameterized_docker_build.sh: 121: opt/parameterized_docker_build.sh: [[: not found
> opt/parameterized_docker_build.sh: 124: opt/parameterized_docker_build.sh: die: not found
> opt/parameterized_docker_build.sh: 128: opt/parameterized_docker_build.sh: [[: not found
> opt/parameterized_docker_build.sh: 130: opt/parameterized_docker_build.sh: [[: not found
> opt/parameterized_docker_build.sh: 141: opt/parameterized_docker_build.sh: die: not found
> opt/parameterized_docker_build.sh: 145: opt/parameterized_docker_build.sh: [[: not found
> opt/parameterized_docker_build.sh: 147: opt/parameterized_docker_build.sh: [[: not found
> opt/parameterized_docker_build.sh: 150: opt/parameterized_docker_build.sh: die: not found
> opt/parameterized_docker_build.sh: 156: opt/parameterized_docker_build.sh: [[: not found
> 
> FINAL_IMAGE_NAME: tensorflow/tensorflow
> FINAL_TAG: latest
> Original Dockerfile: //Dockerfile
> 
> Docker build will occur in temporary directory: /tmp/tmp.IItoQQJuhx
> cp: error reading '//proc/1/task/1/personality': Operation not permitted
> cp: error reading '//proc/1/task/1/syscall': Operation not permitted
> cp: cannot open '//proc/1/task/1/mem' for reading: Permission denied
> cp: error reading '//proc/1/task/1/clear_refs': Invalid argument"
13357,[XLA] Failure in the OS/X xla/service tests: duplicate symbol,"I am getting the following error when running the XLA service unit tests:

```
duplicate symbol __ZN3xla7PrintToEPKNS_14HloInstructionEPNSt3__113basic_ostreamIcNS3_11char_traitsIcEEEE in:
    bazel-out/local-opt/bin/tensorflow/compiler/xla/service/_objs/user_computation_test/tensorflow/compiler/xla/service/user_computation_test.o
    bazel-out/local-opt/bin/tensorflow/compiler/xla/service/libhlo_matchers.a(hlo_matchers.o)
```

Command line for running the tests is:

```
bazel test --test_env TF_CPP_MIN_VLOG_LEVEL=2 --test_size_filters=small,medium,large --config opt --verbose_failures --test_output=all --nocache_test_results tensorflow/compiler/xla/service/...
```

This is OS/X, head of the master branch, using bazel `0.5.4-homebrew`.

Is this a known issue at the moment?
"
13356,How to save the network graph into a file ?,"I want to save the network graph, not the weights. Is there any tool to export the network to a file like json?"
13355,TensorArray grad bug,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip binary
- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 3.6.1

### Describe the problem

`tf.TensorArray` in some cases does not correctly passes the gradient. See the test case.

### Source code / logs

This fails:

```
def test_tensorarray_grad_simple():
  n_time = 1
  n_dim = 1
  x = [[1.42]]
  dy = [[2.42]]

  x = tf.convert_to_tensor(x)
  x.set_shape(tf.TensorShape((n_time, n_dim)))
  with tf.name_scope(""gradients""):
    # Note that tensor_array_grad._GetGradSource() has this ugly hack
    # which requires that we have the ""gradients"" prefix.
    dy = tf.identity(tf.convert_to_tensor(dy), name=""dy"")
  dy.set_shape(tf.TensorShape((n_time, n_dim)))

  ta = tf.TensorArray(tf.float32, size=n_time, element_shape=tf.TensorShape((n_dim,)))
  for t in range(n_time):
    ta = ta.write(index=t, value=x[t])
  y = ta.stack()
  y.set_shape(tf.TensorShape((n_time, n_dim)))
  # y = y[::1]  -- if you add this, the test passes
  dx, = tf.gradients(ys=[y], grad_ys=[dy], xs=[x])
  vx, vdy, vy, vdx = session.run([x, dy, y, dx])
  print(""x:"", vx)
  print(""y:"", vy)
  print(""dy:"", vdy)
  print(""dx:"", vdx)
  assert_allclose(vx, vy)
  assert_allclose(vdy, vdx)
```

I get the output:
```
x: [[ 1.41999996]]
y: [[ 1.41999996]]
dy: [[ 2.42000008]]
dx: [[ 0.]]
```

Strangely, if you add something like `y = y[::1]` before taking the gradient, it passes.
"
13354,Dynamic loading / freeing GPU devices,"I wonder whether there are any on-going works or plans on dynamic loading / freeing GPUs.

What I mean by dynamic loading is a client-side feature <code>sess.load_device()</code> like below:

```python
# start with /gpu:0 only
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
sess = tf.Session()
with tf.device('/gpu:0'):
  a = tf.constant(0.5)
sess.run(a)

# add new device /gpu:1
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
sess.load_device('/gpu:1')
with tf.device('/gpu:1'):
  b = tf.constant(0.3)
sess.run(a + b)

# free device /gpu:0
sess.free_device('/gpu:0')
```

I'm trying to test whether I can run <code>BaseGPUDeviceFactory::CreateDevices()</code> after the session is created, but if there are any better ways, would you please give me some hints?"
13353,Changing compiler to gcc not works,"In my macOS, I have set `export CC=/usr/local/bin/gcc-6` and `export CXX=/usr/local/bin/g++-6`. Then I built tensorflow from source using `sh tensorflow/contrib/makefile/build_all_ios.sh`. But I saw it also compile using `clang`. So, how to change compiler? @martinwicke "
13351,TensorFlow variable initializers broken,"Three symptoms observed with models after upgrading tensorflow:
1. must feed placeholder error
or
2. things hang with 100% utilization inside python _build_initializer_expr
3. things succeed but sess.run call takes 100x slower than before

I believe this is due to this commit:
https://github.com/tensorflow/tensorflow/commit/07adc2ea910de715d31e16a019fcbcccb575e931

Because the following work-around restores good behavior:
```
from tensorflow.python.ops import variables
def passthrough(obj, value): return value
try:
  variables.Variable._build_initializer_expr=passthrough
except: # older versions of TF don't have this
  pass
```


Here's a self contained repro: https://github.com/yaroslavvb/stuff/blob/master/tf_initializer_bug_report.py

It works fine in tensorflow 1.2, or with the fix, in latest version is throws 

```
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a va
lue for placeholder tensor 'Wf_holder' with dtype float and shape [307328]
         [[Node: Wf_holder = Placeholder[dtype=DT_FLOAT, shape=[307328], _device
=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

```

Sorry I didn't make the repro smaller, I lost motivation after finding the quick fix :)"
13350,Compilation fails with --config=sycl due to missing comma in tensorflow/core/kernels/training_ops.cc,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 17.10
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
master from github
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
bazel release 0.5.4
- **CUDA/cuDNN version**:
No
- **GPU model and memory**:
AMD Radeon (TM) R9 380 Series
- **Exact command to reproduce**:
bazel build --config=opt --config=sycl //tensorflow/tools/pip_package:build_pip_package
### Describe the problem
Bug in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc
in line 2551 resulting from a missing comma. Instead of
                            ctx, 0, use_exclusive_lock_, false & var));
it should read
                            ctx, 0, use_exclusive_lock_, false, &var));

### Source code / logs
When compiling with opencl enabled (--config=sycl) the bazel build command will result in an error."
13349,ImportError: DLL load failed: The specified module could not be found. While attempting to import TensorFlow,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13348,Dataset API does not pass dimensionality information when constructing graph.  [using official ResNet],"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX 10.12.6
- **TensorFlow installed from (source or binary)**: 
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.6.1 


### Describe the problem
When I try to combine the dataset API with the resnet architecture provided at tensorflow/models/official/resnet the graph cannot be constructed because the dimension of the input data is not passed to the model constructing function.  

### Source code / logs

Skeleton code:

```
with tf.Session() as sess:
    print(""initialized"")

    features_placeholder = tf.placeholder(prepared_x.dtype, prepared_x.shape)
    labels_placeholder = tf.placeholder(dtype=tf.float32, shape=prepared_t.shape)

    dataset = tf.contrib.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))
    dataset = dataset.shuffle(buffer_size=10000)
    dataset = dataset.batch(batch_size)
    dataset = dataset.repeat(num_epoch)

    iterator = dataset.make_initializable_iterator()

    (next_x_test, next_t_test) = iterator.get_next()
    next_x_test = tf.to_float(next_x_test, name='ToFloat')


    sess.run(iterator.initializer, feed_dict={features_placeholder: prepared_x,
                                              labels_placeholder: prepared_t})


    print(next_x_test)
    print(next_t_test)

    model = resnet_v2(resnet_size=50, num_classes=num_bins)

    output = model(next_x_test,is_training=True)
```

The last line throws an error on compiling:
> ValueError: The last dimension of the inputs to Dense should be defined. Found None.

this makes reference back to the resent_v2 definition where the final layer is a dense layer.

It appears that the dataset API is not passing dimension information hence the final dense layer does not know how to construct itself.  "
13347,Tensorflow 1.3.0  and Python 3.5.2  Issue in Redhat RH 6.7 x86_64 issue in runtime,"** GLIBC version 2.12 ***
python -c ""import tensorflow""
 File ""/hdpapp/Anaconda3-4.2.0/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /hdpapp/Anaconda3-4.2.0/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)

Upgraded *** GLIB version to 2.14*****
python -c ""import tensorflow"" 
 File ""/hdpapp/Anaconda3-4.2.0/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /opt/glibc-2.14/lib/libc.so.6: version `GLIBC_2.17' not found (required by /hdpapp/Anaconda3-4.2.0/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)

Upgraded *** GLIB version to 2.17*****
python -c ""import tensorflow""
error while loading shared libraries: __vdso_time: invalid mode for dlopen(): Invalid argument

Any idea what configuration is not matching causing the runtime issue
"
13346,Diagnosis after running tensorflow_self_check.py,"Hello

After running tensorflow_self_check.py with Jupyter, I get the following message:

ERROR: Failed to import the TensorFlow module.

- Python version is 3.5.

- TensorFlow is installed at: c:\users\gmagen\appdata\local\programs\python\python35\lib\site-packages\tensorflow

- All required DLLs appear to be present. Please open an issue on the
  TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues
An exception has occurred, use %tb to see the full traceback.

SystemExit: -1


How do I solve it|"
13345,Exposing the TF Server interface through the C API,"@asimshankar Would you be willing to expose the server interface through the C API? It consists of mainly 4 methods (`New`, `Start`, `Stop`, and `Join`) and it's the only functionality missing from the C API to allow easy setup for distributed training."
13344,Using CheckpointReader from another language,@asimshankar Is the checkpoint reader class (`checkpoint_reader.h`) currently exposed through any dynamic library?
13342,slim.learning.train can't restore variables if new variable have created.,"Look at my code:

```python
def slim_train_init_fn_test():
    '''
        In the model_ckpt/slim_train_init_fn_test.ckpt,
        {'x/y': 1.0, 'x/z': 2.0, 'y/z': 3.0}
    '''
    x_y = slim.variable('x/y', initializer=4.0)
    x_z = slim.variable('x/z', initializer=5.0)
    y_z = slim.variable('y/z', initializer=6.0)

    variables_to_restore = slim.get_variables_to_restore(include=['x'])

    init_fn = tf.contrib.framework.assign_from_checkpoint_fn(
        'model_ckpt/slim_train_init_fn_test.ckpt', variables_to_restore)
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)
    loss = slim.variable('loss', initializer=10.0)
    train_op = slim.learning.create_train_op(loss, optimizer)
    slim.learning.train(train_op, 'log', init_fn=init_fn, number_of_steps=1)
```

 In the `model_ckpt/slim_train_init_fn_test.ckpt`,  there is a map  `{'x/y': 1.0, 'x/z': 2.0, 'y/z': 3.0}`, and I use 
`get_variables_to_restore(include=['x'])` to restore variable `x_y` and  `x_z` which are in  `model_ckpt/slim_train_init_fn_test.ckpt`, but I create a new variable `loss` which isn't in `model_ckpt/slim_train_init_fn_test.ckpt`, the code raise an error:  Can't find key `loss` in check point file. 
So I can't understand why check point file should have key `loss`,  does `init_fn` not pass `variables_to_restore` to `tf.train.Saver` ? Is it a bug?


View the [assign_from_checkpoint_fn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/ops/variables.py#L659), it indeed uses `assign_from_checkpoint_fn` to initialize `tf.train.Saver` .  And there is another parameter `saver`  in  [slim.learning.train](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L729), it is  just used to [save](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L774)  parameters of model, not `restore`. 

So what's  wrong?












"
13341,"LSTM RNN ""Variable rnn/basic_lstm_cell/kernel already exists, disallowed"" error ","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13340,Tensor Flow Installation issue: couldnot find a version that satisfies the requirement tensorflow,"I am trying to install Tensorflow through anaconda. Following is the error when I try to install
_""couldnot find a version that satisfies the requirement tensorflow""._

I tried to create two sessions - with Python 3.5 and Python 3.6 - but both of them returns the error identically.

Code used in the activated sessions: pip install tensorflow
 
Screenshot1 - Python 3.6 (Environment name: tenflow)
![image](https://user-images.githubusercontent.com/8396984/30917306-543b3d56-a3b9-11e7-91f8-aa705342e773.png)

Screenshot2 - Python 3.6 (Environment name: tflow)
![image](https://user-images.githubusercontent.com/8396984/30917325-603c716a-a3b9-11e7-9daf-c8e323d55bf2.png) "
13338,Feature request: add tf.layers.Group to group multiple layers under one name,"Example usage (relevant for networks with skip connection i.e. u-net):

```python
encoder1 = tf.layers.Group([
  tf.layers.Conv2d(...),
  tf.layers.BatchNorm(),
  ActivationLayer(),
], name='encoder1')

encoder2 = tf.layers.Group([
  tf.layers.Conv2d(...),
  tf.layers.BatchNorm(),
  ActivationLayer(),
], name='encoder1')

inputs = outputs = tf.layers.Input(tensor=x)

for enc in [encoder1, encoder2]:
  outputs = enc(outputs)

encoders = tf.layers.Network(inputs, outputs)

# same for decoders
for i, dec in enumerate(decoders):
  outputs = dec(tf.concat([encoders.get_layer(f'encoder{i}').output, outputs], 3))
```

Currently you could either
* use `network.get_output_at()` however this requires to track node indices or parse `network.layers` if encoders are not symmetric (i.e. some encoder have no batch norm / dropout)
* inherit from `tf.layers.Layer` however documentation is not clear on how to ""forward"" variables for example"
13337,MonitoredTrainingSession: CreateSession still waiting,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.3.0-rc1-2195-gd86ee219e 1.4.0-dev
- **Python version**: Python 3.6.1 :: Anaconda 4.4.0 (x86_64)
- **Bazel version (if compiling from source)**: 0.5.2-homebrew
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**: `./run.sh`

### Describe the problem

I've already raised this problem on [stackoverflow](https://stackoverflow.com/questions/46429766/distributed-tensorflow-createsession-still-waiting), but haven't got any feedback.

I run the python script four times with the bash script. There are three `worker` tasks and one `master` task. Sometimes all the workers successfully exit. But often one or two of them hang and begin emitting ""CreateSession still waiting for some other task"" messages. Such waiting worker can wait for the chief (worker task 0) or some other task which is already completed.

So, the problem is that workers and even the chief do not wait for some lagging worker. Is it a bug? Chief is responsible for initialisation of variables. But if I put a variable on each worker nothing changes. The chief still do not wait.

I've succeeded in synchronisation of these workers by adding `FIFOQueue` barriers to the beginning of each session.

### Source code / logs

TF script (`train.py`):

```python
import argparse
import tensorflow as tf

parser = argparse.ArgumentParser()
parser.add_argument('--job', type=str)
parser.add_argument('--task', type=int)
args = parser.parse_args()
hosts = {
    ""master"": [
        ""localhost:2222"",
    ],
    ""worker"": [
        ""localhost:2223"",
        ""localhost:2224"",
        ""localhost:2225"",
    ]
}

nworkers = len(hosts['worker'])
cluster = tf.train.ClusterSpec(hosts)
server = tf.train.Server(cluster, job_name=args.job, task_index=args.task)

with tf.device(f'/job:master/task:0'):
    global_step = tf.train.get_or_create_global_step()
    inc_global_step = tf.assign(global_step, global_step + 1)

if args.job == 'worker':
    hooks = [
        tf.train.StopAtStepHook(last_step=4),
    ]
    with tf.train.MonitoredTrainingSession(master=server.target,
                                           is_chief=(args.task == 0),
                                           hooks=hooks) as sess:
        while not sess.should_stop():
            print(args.task, sess.run(inc_global_step))
else:
    server.join()
```
Bash script (`run.sh`):
```bash
#!/bin/bash
python train.py --job master --task 0 &
python train.py --job worker --task 0 &
python train.py --job worker --task 1 &
python train.py --job worker --task 2 &
```
Example of message:
```
2017-09-27 11:52:48.973442: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0
```"
13336,Cannot assign a device for operation 'save/ShardedFilename_1' when exporting custom Estimator,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux ubuntu254 4.2.0-42-generic \#49~14.04.1-Ubuntu SMP Wed Jun 29 20:22:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
- **TensorFlow installed from (source or binary)**:
pip
- **TensorFlow version (use command below)**:
tf.VERSION = 1.3.0
- **Python version**: 
2.7.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
release 8.0, V8.0.44 / 6.0
- **GPU model and memory**:
GTX Titan Black 6 GB, GTX 1060 6 GB
- **Exact command to reproduce**:
run estimator_CNN.py

### Describe the problem
Continued from [this discussion](https://groups.google.com/a/tensorflow.org/forum/#!searchin/discuss/export/discuss/XHABHQG5l2I/jZBvc0-NBgAJ). I want to export a custom Estimator (multiple CNN layer + CTC loss in multi GPU setting derived from cifar-10 multi GPU example) using `export_savemodel()`. But i encountered this error:
`InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'save/ShardedFilename_1': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.`
this should not occured in 1.3.0 (please see discussion)

### Source code / logs
environment [tf_env.txt](https://github.com/tensorflow/tensorflow/files/1336111/tf_env.txt)
full error trace [error.txt](https://github.com/tensorflow/tensorflow/files/1336110/error.txt)
source code [TF_bug.zip](https://github.com/tensorflow/tensorflow/files/1336108/TF_bug.zip)
"
13335,Potential Bug in CUDA implementation of matrix_set_diag on Windows,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution**: Windows 10 (64-bit)
- **TensorFlow installed from**: binary
- **TensorFlow version (use command below)**: b'unknown' 1.3.0
- **Python version**: 3.6.2
- **CUDA/cuDNN version**: CUDA 8.0 /  cuDNN 6.0
- **GPU model and memory**: Nvidia GTX 1060 6GB

### Describe the problem
I ran into this while trying to run the code at https://github.com/deepmind/dnc. It seems that tf.matrix_set_diag can cause tensorflow to crash when run on GPU under Windows. I am able to run other complex models on GPU so it seems as though my environment is configured correctly for CUDA.
### Source code / logs
```
import tensorflow as tf

batch_size = 16
num_writes = 1
memory_size = 16

with tf.device('/gpu:0'):
    link = tf.Variable(tf.ones([batch_size, num_writes, memory_size, memory_size], dtype=tf.float32))
    test_op = tf.matrix_set_diag(link,tf.zeros([batch_size, num_writes, memory_size],dtype=tf.float32))

sess = tf.Session()
sess.run(tf.global_variables_initializer())
print(sess.run(test_op))
```
Error log here for normal execution:
[error_log.txt](https://github.com/tensorflow/tensorflow/files/1336102/error_log.txt)

The log from running with cuda-memcheck shows what looks like an issue with a null pointer buried deep inside Eigen...
[error_log_cuda_memcheck.txt](https://github.com/tensorflow/tensorflow/files/1336103/error_log_cuda_memcheck.txt)

"
13333,TensorFlow buillt with -march option gives illegal instruction when imported,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.2.1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.4.5
- **CUDA/cuDNN version**: No GPU
- **GPU model and memory**: No GPU
- **Exact command to reproduce**: Build and Install TensorFlow wheel and import TensorFlow

### Describe the problem
I am trying to build TensorFlow by explicitly providing `–march option on z13`.   
In the past I could build and install same version (v1.2.1) with default options(-march=native). 

The issue is when I use below commands for configure and build respectively:
* Configure -  `Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: -march=z13   
`   
* Build - `bazel build -c opt --config=opt //tensorflow/tools/pip_package:build_pip_package,
`   

the build succeeds however the TensorFlow wheel when installed gives issue of python crash:
```
root@8a268ac0eaea:~# python
 >>> import tensorflow as tf
Illegal instruction (core dumped)
root@8a268ac0eaea:

```
I have verified the system arch to be z13. However when I use command `gcc –Q –help=target ` I could see `-march= zEC12`.
I am now trying to debug why passing ‘z13’  causes above issue with wheel.
Also is there a way to find out what arch is exactly detected when ‘native’ is passed?

"
13331,Passing CPU value along with GPU tensor,"Hello, i'm trying to pass a CPU value or a block of values from one custom op into another along with a GPU tensor but the framework seems to be converting everything to GPU tensor. There doesn't seem to be a mechanism for passing mixed GPU+CPU op results right now."
13329,Auto-Parallel excludes update operators of sparse tensors,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
r1.3
- **Bazel version (if compiling from source)**:
0.4.5
- **CUDA/cuDNN version**:
cuda 8.0/cudnn 5.1.5
- **GPU model and memory**:
Tesla P40 
- **Exact command to reproduce**:

### Describe the problem
I'm trying to use auto_parallel in grappler, but I found it only controls dense tensors. In the code, only below operators update averaged gradients, but what about 'ScatterSub' for sparse tensors? Do you have a plan to implement it? 

```
const std::set<string> apply_gradients_ops = {""ApplyGradientDescent"",
                                                ""ApplyProximalGradientDescent"",
                                                ""ApplyAdadelta"",
                                                ""ApplyAdagrad"",
                                                ""ApplyProximalAdagrad"",
                                                ""ApplyAdagradDA"",
                                                ""ApplyFtrl"",
                                                ""ApplyMomentum"",
                                                ""ApplyAdam"",
                                                ""ApplyRMSProp"",
                                                ""ApplyCenteredRMSProp""};
```

### Source code / logs"
13328,Why use tensorflow1.3.0GPU training to get the validation loss is much higher than the CPU training,"when I use tf1.3.0 CPU vesion, the validation loss is 0.54; but get 0.74 using GPU instead. I can't handle it.
Cudnn version is 6.0
"
13327,v1.3.0 protobuf and sha256sum does not match,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I am compiling tensorflow_cc.so using bazel, and the head was at v1.3.0, it encountered an ERROR 

tensorflow/BUILD:446:1: error loading package 'tensorflow/c': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': java.io.IOException: Error downloading [https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz, http://mirror.bazel.build/github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz] to .cache/bazel/_bazel_xiaochunlin/8e84b434cfbb634410b719fa2fe8ff20/external/protobuf/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz: Checksum was e5fdeee6b28cf6c38d61243adff06628baa434a22b5ebb7432d2a7fbabbdb13d but wanted 6d43b9d223ce09e5d4ce8b0060cb8a7513577a35a64c7e3dad10f0703bf3ad93 and referenced by '//tensorflow:libtensorflow_cc.so'.

I manually downloaded the protobuf with the given link in tensorflow/workspace.bzl , https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz , and the sha256sum indeed is e5fdeee6b28cf6c38d61243adff06628baa434a22b5ebb7432d2a7fbabbdb13d. So I guess the sha256sum and the given version protobuf don't match, can anyone verify this ??


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13319,Intel MKL FATAL ERROR: Cannot load mkl_intel_thread.dll.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10.
- **TensorFlow installed from (source or binary)**: Binary.
- **TensorFlow version (use command below)**: tensorflow-1.3.0-cp36-cp36m-win_amd64.whl
- **Python version**: 3.6.
- **Bazel version (if compiling from source)**: N/A.
- **CUDA/cuDNN version**: N/A.
- **GPU model and memory**: GT 650M.
- **Exact command to reproduce**: `python` followed by `import tensorflow as tf`.

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I followed exactly on how to install Tensorflow, but whenever I try to do something involving Tnesorflow this error comes up. I installed Tensorflow using Anaconda 4.4. I first typed `conda create -n tensorflow python=3.6` then `activate tensorflow` then `pip install --ignore-installed --upgrade tensorflow`. After that, I typed `python` then `import tensorflow as tf` I get this error: `Intel MKL FATAL ERROR: Cannot load mkl_intel_thread.dll.`
![untitled](https://user-images.githubusercontent.com/31815007/30871645-1adda5ac-a31a-11e7-8ec8-8a323e8054a8.png)


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13318,The pip upgrade in windows does not get up-to-date tensorflow codes on github,"Tensorflow version: 1.3
OS: Windows 10

When I use pip upgrade to get tensorflow, the codes I got did not reflect the newest updates in Github. For example, in ""tensorflow/tensorflow/contrib/layers/python/layers/layers.py"" the up-to-date batch_norm() function supports not None param_regularizers. However, the codes updated by pip still not support param_regularizers, as the codes are not up-to-date.

Is there any other way to get the most updated tensorflow in Windows? Thank you."
13317,TF v1.3 slower than v1.2 when used with ResNets,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
It's a custom code with fully convolutional ResNet using tf.slim implementation (with diluted kernels)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Debian 8.9
- **TensorFlow installed from (source or binary)**:
Binary (pip)
- **TensorFlow version (use command below)**:
1.3
- **Python version**: 
Python 3.4
- **Bazel version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
CUDA 8.0/cuDNN 6.0
- **GPU model and memory**:
NVIDIA K40m 12Gb

I run fully convolutional ResNet-101 on the images which vary in size. When moving from TF 1.2 to TF 1.3 inference became about 3x slower. With TF1.2 I use CUDA 8.0 and cudnn 5.1. To make sure variable sized images are processed fast I set env variable TF_CUDNN_USE_AUTOTUNE=0 to switch off auto-tuning of convolutions.

In case it is not to do with convolutions, but the data loading, here's how I feed the input data (numpy arrays) into the convnet:

```python
outputs_np = sess.run(outputs, feed_dict={inputs: batch})
```

Could you suggest how I can troubleshoot that?"
13316,Concatenate in alternate fashion two tensors,"Good Afternoon.
I'd like to concatenate two tensors of shape (None,16) in alternate fashion.
For example, with simple arrays, if the inputs are:
a=[[1,2,3],[1,2,3],...]
b=[[4,5,6],[4,5,6],...]
I want this output:
c=[[1,4,2,5,3,6],[1,4,2,5,3,6],...]
How can I do it? 
I can't loop on tensors because of unknown shape[0], zip function isn't supported for tensors (tensor object is not iterable).
Thank u all in advance"
13313,"No gradients provided for any variable, check your graph for ops that do not support gradients, between variables","I download a code about vgg from web, and use it on my own datasets. But it shows ""No gradients provided for any variable, check your graph for ops that do not support gradients, between variables"" error.
code as `below:`
# -*- coding: UTF-8 -*-
import tensorflow as tf
import read_data
import numpy as np
def conv_op(input_op, name, kh, kw, n_out, dh, dw, p):
    '''
    Args:
    input_op：输入的tensor
    name：这一层的名称
    kh：kernel height即卷积核的高
    kw：kernel weight即卷积核的宽
    n_out：卷积核数量即输出通道数
    dh：步长的高
    dw：步长的宽
    p：参数列表
    '''
    n_in = input_op.get_shape()[-1].value # 获取input_op的通道数

    with tf.name_scope(name) as scope: # 设置scope，生成的Variable使用默认的命名
        kernel = tf.get_variable(scope+""w"",  # kernel（即卷积核参数）使用tf.get_variable创建
                                 shape=[kh, kw, n_in, n_out], # 【卷积核的高，卷积核的宽、输入通道数，输出通道数】
                                 dtype=tf.float32, 
                                 initializer=tf.contrib.layers.xavier_initializer_conv2d()) # 参数初始化
        # 使用tf.nn.conv2d对input_op进行卷积处理，卷积核kernel，步长dh*dw，padding模式为SAME
        conv = tf.nn.conv2d(input_op, kernel, (1, dh, dw, 1), padding='SAME') 
        bias_init_val = tf.constant(0.0, shape=[n_out], dtype=tf.float32) # biases使用tf.constant赋值为0
        biases = tf.Variable(bias_init_val, trainable=True, name='b') # 将bias_init_val转成可训练的参数
        z = tf.nn.bias_add(conv, biases) # 将卷积结果conv和bias相加
        activation = tf.nn.relu(z, name=scope) # 对z进行非线性处理得到activation
        p += [kernel, biases]  # 创建卷积层时用到的参数kernel和bias添加进参数列表
        return activation # 将卷积层的输出activation作为函数结果返回

# 定义全连接层的创建函数
def fc_op(input_op, name, n_out, p):  
    n_in = input_op.get_shape()[-1].value # 获取tensor的通道数 
    with tf.name_scope(name) as scope:
        kernel = tf.get_variable(scope+""w"", # 使用tf.get_variable创建全连接层的参数
                                 shape=[n_in, n_out], # 参数的维度有两个，输入通道数和输出通道数
                                 dtype=tf.float32, 
                                 initializer=tf.contrib.layers.xavier_initializer())
        # biases赋值0.1以避免dead neuron
        biases = tf.Variable(tf.constant(0.1, shape=[n_out], dtype=tf.float32), name='b') 
        # 对输入变量input_op和kernel做矩阵乘法并加上biases。再做非线性变换activation
        activation = tf.nn.relu_layer(input_op, kernel, biases, name=scope) 
        p += [kernel, biases]
        return activation

# 定义最大池化层的创建函数
def mpool_op(input_op, name, kh, kw, dh, dw): 
    return tf.nn.max_pool(input_op,
                          ksize=[1, kh, kw, 1], # 池化层尺寸kh*kw
                          strides=[1, dh, dw, 1], # 步长dh*dw
                          padding='SAME',
                          name=name)


########开始创建VGGNet-16的网络结构########
def inference_op(input_op, keep_prob):
    '''
    VGGNet-16的网络结构主要分为6个部分：前五段为卷积网络，最后一段是全连接网络。
    Args:
    input_op：输入Tensor
    keep_prob：控制Dropout的一个placeholder
    '''
    # 初始化参数列表p
    p = []
    # assume input_op shape is 224x224x3（第一个卷积层的输入input_op）

    # 创建第一段卷积网络 -- outputs 112x112x64
    # 两个卷积层的卷积核都是3*3，卷积核数量（输出通道数）均为64，步长1*1，全像素扫描。
    conv1_1 = conv_op(input_op, name=""conv1_1"", kh=3, kw=3, n_out=64, dh=1, dw=1, p=p) # outputs 224x224x64
    conv1_2 = conv_op(conv1_1,  name=""conv1_2"", kh=3, kw=3, n_out=64, dh=1, dw=1, p=p) # outputs 224x224x64
    pool1  = mpool_op(conv1_2,  name=""pool1"",   kh=2, kw=2, dw=2, dh=2) # 标准的2*2的最大池化-outputs 112x112x64

    # 创建第二段卷积网络 -- outputs 56x56x128
    conv2_1 = conv_op(pool1,    name=""conv2_1"", kh=3, kw=3, n_out=128, dh=1, dw=1, p=p)
    conv2_2 = conv_op(conv2_1,  name=""conv2_2"", kh=3, kw=3, n_out=128, dh=1, dw=1, p=p)
    pool2 = mpool_op(conv2_2,   name=""pool2"",   kh=2, kw=2, dh=2, dw=2)

    # 创建第三段卷积网络 -- outputs 28x28x256
    # conv3_1 = conv_op(pool2,    name=""conv3_1"", kh=3, kw=3, n_out=256, dh=1, dw=1, p=p)
    # conv3_2 = conv_op(conv3_1,  name=""conv3_2"", kh=3, kw=3, n_out=256, dh=1, dw=1, p=p)
    # conv3_3 = conv_op(conv3_2,  name=""conv3_3"", kh=3, kw=3, n_out=256, dh=1, dw=1, p=p)    
    # pool3 = mpool_op(conv3_3,   name=""pool3"",   kh=2, kw=2, dh=2, dw=2)

    # # 创建第四段卷积网络 -- outputs 14x14x512
    # conv4_1 = conv_op(pool3,    name=""conv4_1"", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)
    # conv4_2 = conv_op(conv4_1,  name=""conv4_2"", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)
    # conv4_3 = conv_op(conv4_2,  name=""conv4_3"", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)
    # pool4 = mpool_op(conv4_3,   name=""pool4"",   kh=2, kw=2, dh=2, dw=2)

    # # 创建第五段卷积网络 -- outputs 7x7x512
    # conv5_1 = conv_op(pool4,    name=""conv5_1"", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)
    # conv5_2 = conv_op(conv5_1,  name=""conv5_2"", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)
    # conv5_3 = conv_op(conv5_2,  name=""conv5_3"", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)
    # pool5 = mpool_op(conv5_3,   name=""pool5"",   kh=2, kw=2, dw=2, dh=2)

    # 备注：VGGNet-16的每一段卷积网络都会将图像的边长缩小一半，但是将卷积输出通道数翻倍。
    # 第五段卷积输出的通道数不再增加。

    # flatten 将第五段卷积网络的输出结果进行扁平化
    shp = pool2.get_shape()
    flattened_shape = shp[1].value * shp[2].value * shp[3].value

    # tf.reshape函数将每个样本化为长度7*7*512 = 25088的向量
    resh1 = tf.reshape(pool2, [-1, flattened_shape], name=""resh1"") 

    # fully connected 隐含节点4096的全连接层
    fc6 = fc_op(resh1, name=""fc6"", n_out=4096, p=p)
    fc6_drop = tf.nn.dropout(fc6, keep_prob, name=""fc6_drop"")

    fc7 = fc_op(fc6_drop, name=""fc7"", n_out=4096, p=p)
    fc7_drop = tf.nn.dropout(fc7, keep_prob, name=""fc7_drop"")

    fc8 = fc_op(fc7_drop, name=""fc8"", n_out=10, p=p)
    softmax = tf.nn.softmax(fc8) # 得到分类输出概率
    predictions = tf.argmax(softmax, 1) # tf.argmax求输出概率最大类别
    print predictions.get_shape()
    return predictions, softmax, fc8, p

def main(file_name, batch_size, iter_times):
    x = tf.placeholder('float', shape = [batch_size, 32, 32, 3])
    y = tf.placeholder('float', shape = [batch_size, 1])
    predictions, _, _, _ = inference_op(x, keep_prob = 0.5)
    predictions = tf.cast(predictions, tf.float32)
    
    ##### tf.equal 返回的是bool tensor  ######
    ##### tf.reduce_mean() 不能是bool值 ######
    print predictions.get_shape()
    loss = tf.nn.softmax_cross_entropy_with_logits(labels= tf.transpose(y, perm = [1, 0]), logits = predictions)
    loss = tf.reduce_mean(loss)
    train = tf.train.GradientDescentOptimizer(0.01).minimize(loss)

    sess = tf.Session()
    init = tf.global_variables_initializer()

    for i in xrange(iter_times):
        train_x, train_y = read_data.fetch_data(file_name, batch_size)
        sess.run(init)
        train_x, train_y = sess.run([train_x, train_y])
        train.eval(feedict = {x: train_x, y: train_y})
        print sess.run(predictions)
        if i % 100 == 0 :
            print ""%d step accuarcy is %f"" % (i, sess.run(loss))

main('train.tfrecords', 30, 2000)

"
13311,gru_ops.py trying to load a .so file on windows,"gru_ops.py is attempting to load an .so library on windows when using CPU version (vs GPU), is this the correct behavior? It fails in my case. 

LINE 33
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/gru_ops.py
"
13310,Feature Request: Mixed Sparse and Dense Tensors,"### Describe the problem
I am trying to implement a sparse convolution operation. This means I have spatially sparse locations(in this case *NHW* of *NHWC* in 2d convolutions)
and at each of these locations I have a dense vector of values (*C* of *NHWC*).
In tensorflow there are currently two classes for sparse representations:

1. **SparseTensor**:
Consisting of an indices matrix, value and shape vector.
  This allows for a fully sparse tensor, however it does not fit this use case, because representing the channels as just another sparse dimension, I cannot simply compute a matrix multiplication, with the corresponding kernel parameters. This extremely reduces efficiency. Apart from this, also the storage
is inefficient, since It redundantly stores the indices for the dense sub tensor.
2. **IndexedSlices**
Consisting of an indices vector and an arbitrarily shaped value tensor.
This is a mixed sparse dense data structure. So the individual sub tensors are stored sparsely. It does however not fit the use case, because the indexing is only a vector and not (as compared to the sparse
tensor) a matrix. So we can only address only a single index. While it is possible to encode an index vector
as a scalar index, it imposes this effort on the user, which seems to me to be non optimal.

So both data structures are inadequate for this use case. While I am talking about a single use case, I cannot imagine, that no one else stumbled across this issue, since its so general. 

### Proposed Solution
To solve this issue I propose to introduce a new class **MixedSparseDenseTensor**, which takes an indices matrix (as opposed to IndexedSlices vector), an arbitrarily shaped values tensor (as opposed to SparseTensors vector) and a shape vector. So it would thus be either a generalization of IndexedSlices to multiple dimensional indices or a generalization of SparseTensor to arbitrarily shaped sub tensors.

The dimensionality of the shape vector should then be the rank of the sparse tensor + the rank of the dense tensor.

"
13308,Attempting to use the CPU Work Sharder segfaults on g++ 5.4.0,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
  
I've adapted the ZeroOut operator from the [Adding a New Op](https://www.tensorflow.org/extend/adding_an_op) example.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Linux Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:

binary GPU 1.3.0

- **TensorFlow version (use command below)**:

$ python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.3.0-rc2-20-g0787eee', '1.3.0')

- **Python version**: 

2.7.12

- **Bazel version (if compiling from source)**:

N/A

- **CUDA/cuDNN version**:

N/A

- **GPU model and memory**:

N/A

- **Exact command to reproduce**:

Test operator: [shard_fails.zip](https://github.com/tensorflow/tensorflow/files/1332745/shard_fails.zip)

```bash
$ make
$ python test_op.py
```



### Describe the problem

When the above C++ operator runs, it'll print the number of threads in the pool (8) and then segfault on the Shard call.

### Source code / logs

C++ operator code:

```cpp
#define EIGEN_USE_THREADS

#include ""tensorflow/core/lib/core/threadpool.h""
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/op_kernel.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/util/work_sharder.h""

using namespace tensorflow;

REGISTER_OP(""ZeroOut"")
    .Input(""to_zero: int32"")
    .Output(""zeroed: int32"")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      c->set_output(0, c->input(0));
      return Status::OK();
    });

class ZeroOutOp : public OpKernel {
 public:
  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& input_tensor = context->input(0);
    auto input = input_tensor.flat<int32>();

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                                                     &output_tensor));
    auto output_flat = output_tensor->flat<int32>();

    // Set all but the first element of the output tensor to 0.
    const int N = input.size();

    auto pool = context->device()->tensorflow_cpu_worker_threads()->workers;
    printf(""Pool Threads %d\n"", pool->NumThreads());
    Shard(pool->NumThreads(), pool, N, 10, [&](int64 start, int64 end) {
        for(int64 i=start; i<end; ++i)
            { output_flat(i) = 0; }
    });

    if(N > 0)
        { output_flat(0) = input(0); }
  }
};

REGISTER_KERNEL_BUILDER(Name(""ZeroOut"").Device(DEVICE_CPU), ZeroOutOp);
```


See below the gdb trace:

```
Core was generated by `python test_op.py'.
Program terminated with signal SIGSEGV, Segmentation fault.
#0  std::_Function_handler<void (long long, long long), ZeroOutOp::Compute(tensorflow::OpKernelContext*)::{lambda(long long, long long)#1}>::_M_invoke(std::_Any_data const&, long long&&, std::_Any_data const&) (__functor=..., __args#0=<unknown type in tfop.so, CU 0x0, DIE 0x41c73>, __args#1=<unknown type in tfop.so, CU 0x0, DIE 0x41c78>) at /usr/include/c++/5/functional:1871
1871		(*_Base::_M_get_pointer(__functor))(
[Current thread is 1 (Thread 0x7f38a6605700 (LWP 3771))]
(gdb) bt
#0  std::_Function_handler<void (long long, long long), ZeroOutOp::Compute(tensorflow::OpKernelContext*)::{lambda(long long, long long)#1}>::_M_invoke(std::_Any_data const&, long long&&, std::_Any_data const&) (__functor=..., __args#0=<unknown type in tfop.so, CU 0x0, DIE 0x41c73>, __args#1=<unknown type in tfop.so, CU 0x0, DIE 0x41c78>) at /usr/include/c++/5/functional:1871
#1  0x00007f3879dcc75d in tensorflow::thread::ThreadPool::Impl::ParallelFor(long long, long long, std::function<void (long long, long long)>) ()
   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007f3879dcc93f in tensorflow::thread::ThreadPool::ParallelFor(long long, long long, std::function<void (long long, long long)>) ()
   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007f3879d4d995 in tensorflow::Shard(int, tensorflow::thread::ThreadPool*, long long, long long, std::function<void (long long, long long)>) ()
   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007f3850bfb79e in ZeroOutOp::Compute (this=0x62dacc0, context=0x7ffd1a20fe30) at tf_op.cpp:42
#5  0x00007f3879a2563c in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()
   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007f38799f5a58 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()
   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007f38799f61fa in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8> const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007f3879a035c4 in std::_Function_handler<void (std::function<void ()>), tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*)::{lambda(std::function<void ()>)#1}>::_M_invoke(std::_Any_data const&, std::function<void ()>) ()
   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007f38799e895b in std::function<void (std::function<void ()>)>::operator()(std::function<void ()>) const ()
   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007f38799e9043 in tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8> const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*) [clone .part.246] () from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007f38799ecf5e in tensorflow::(anonymous namespace)::ExecutorImpl::RunAsync(tensorflow::Executor::Args const&, std::function<void (tensorflow::Status const&)>) ()
   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#12 0x00007f3879a045e4 in tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) ()
   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#13 0x00007f38799dce27 in tensorflow::ConstantFold(tensorflow::ConstantFoldingOptions const&, tensorflow::FunctionLibraryRuntime*, tensorflow::Env*, tensorflow::Device*, tensorflow::Graph*, bool*) ()
   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#14 0x00007f3879a02fea in tensorflow::GraphOptimizer::Optimize(tensorflow::FunctionLibraryRuntime*, tensorflow::Env*, tensorflow::Device*, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >*) () from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#15 0x00007f38799a9469 in tensorflow::DirectSession::GetOrCreateExecutors(tensorflow::thread::ThreadPool*, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) ()
   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#16 0x00007f38799aa06c in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#17 0x00007f387799b2d7 in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) ()
   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#18 0x00007f387799b604 in TF_Run () from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#19 0x00007f38778037e2 in tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()
   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#20 0x00007f3877803be1 in tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#21 0x00007f38777ca793 in _wrap_TF_Run () from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#22 0x00000000004c468a in PyEval_EvalFrameEx ()
#23 0x00000000004c2765 in PyEval_EvalCodeEx ()
#24 0x00000000004de6fe in ?? ()
#25 0x00000000004b0cb3 in PyObject_Call ()
#26 0x00000000004c6ad1 in PyEval_EvalFrameEx ()
#27 0x00000000004c2765 in PyEval_EvalCodeEx ()
#28 0x00000000004ca8d1 in PyEval_EvalFrameEx ()
#29 0x00000000004c2765 in PyEval_EvalCodeEx ()
---Type <return> to continue, or q <return> to quit---
#30 0x00000000004ca8d1 in PyEval_EvalFrameEx ()
#31 0x00000000004c2765 in PyEval_EvalCodeEx ()
#32 0x00000000004ca8d1 in PyEval_EvalFrameEx ()
#33 0x00000000004c2765 in PyEval_EvalCodeEx ()
#34 0x00000000004ca099 in PyEval_EvalFrameEx ()
#35 0x00000000004c2765 in PyEval_EvalCodeEx ()
#36 0x00000000004c2509 in PyEval_EvalCode ()
#37 0x00000000004f1def in ?? ()
#38 0x00000000004ec652 in PyRun_FileExFlags ()
#39 0x00000000004eae31 in PyRun_SimpleFileExFlags ()
#40 0x000000000049e14a in Py_Main ()
#41 0x00007f38a5e4c830 in __libc_start_main (main=0x49dab0 <main>, argc=2, argv=0x7ffd1a213618, init=<optimised out>, fini=<optimised out>, rtld_fini=<optimised out>, stack_end=0x7ffd1a213608)
    at ../csu/libc-start.c:291
#42 0x000000000049d9d9 in _start ()
```"
13306,How to set include path and lib path when building custom code on macOS,"Firstly, I set CC=/usr/local/bin/gcc-6 and CXX=/usr/local/bin/g++-6. Then I built tensorflow from source using `sh tensorflow/contrib/makefile/build_all_ios.sh` on macOS 10.12.5 and it done successfully. Lastly, I built a test cpp using CMake but it failed. 

The reasons I guess maybe:
1. Tensorflow built using default clang but not g++-6. So how to set compiler when using `tensorflow/contrib/makefile/build_all_ios.sh`?
2. The include and lib path in CMakeLists.txt may be wrong.

```
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/core/platform/env.h""
using namespace tensorflow;
int main(int argc, char* argv[]) {
  // Initialize a tensorflow session
  Session* session;
  Status status = NewSession(SessionOptions(), &session);
  if (!status.ok()) {
    std::cout << status.ToString() << ""\n"";
    return 1;
  }
  session->Close();
  return 0;
}
```
In my CMakeLists.txt, I set include path 
```
${PROJECT_SOURCE_DIR}/../tensorflow
${PROJECT_SOURCE_DIR}/../tensorflow/tensorflow/contrib/makefile/gen/proto
${PROJECT_SOURCE_DIR}/../tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/include
${PROJECT_SOURCE_DIR}/../tensorflow/tensorflow/contrib/makefile/downloads/eigen
${PROJECT_SOURCE_DIR}/../tensorflow/tensorflow/contrib/makefile/downloads/nsync/public
```
and the library path
```
link_directories(
	${PROJECT_SOURCE_DIR}/../tensorflow/tensorflow/contrib/makefile/gen/lib/ios_X86_64
	${PROJECT_SOURCE_DIR}/../tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_x86_64/lib)
set(DEMO_LINKER_LIBS """")
list(APPEND DEMO_LINKER_LIBS libtensorflow-core-x86_64.a libprotobuf-lite.a libprotobuf.a)
```

Compile ok but errors occur when linking. What's wrong in my use? Thanks.
```
ld: warning: URGENT: building for OSX, but linking in object file (/Users/formath/github/tensorflow/tensorflow/contrib/makefile/gen/lib/ios_X86_64/libtensorflow-core-x86_64.a(session.o)) built for iOS. Note: This will be an error in the future.
ld: warning: URGENT: building for OSX, but linking in object file (/Users/formath/github/tensorflow/tensorflow/contrib/makefile/gen/lib/ios_X86_64/libtensorflow-core-x86_64.a(config.pb.o)) built for iOS. Note: This will be an error in the future.(/Users/formath/github/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(config.pb.o)) built for iOS. Note: This will be an error in the future.
Undefined symbols for architecture x86_64:
  ""tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()"", referenced from:
      std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*) in test.cc.o
  ""nsync::nsync_mu_init(nsync::nsync_mu_s_*)"", referenced from:
      tensorflow::SessionFactory::Register(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::SessionFactory*) in libtensorflow-core-x86_64.a(session_factory.o)
      tensorflow::SessionFactory::GetFactory(tensorflow::SessionOptions const&, tensorflow::SessionFactory**) in libtensorflow-core-x86_64.a(session_factory.o)
      tensorflow::Env::Env()   in libtensorflow-core-x86_64.a(env.o)
  ""nsync::nsync_mu_lock(nsync::nsync_mu_s_*)"", referenced from:
      tensorflow::SessionFactory::Register(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::SessionFactory*) in libtensorflow-core-x86_64.a(session_factory.o)
      tensorflow::SessionFactory::GetFactory(tensorflow::SessionOptions const&, tensorflow::SessionFactory**) in libtensorflow-core-x86_64.a(session_factory.o)
      tensorflow::FileSystemRegistryImpl::Register(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<tensorflow::FileSystem* ()>) in libtensorflow-core-x86_64.a(env.o)
      tensorflow::FileSystemRegistryImpl::Lookup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in libtensorflow-core-x86_64.a(env.o)
      tensorflow::FileSystemRegistryImpl::GetRegisteredFileSystemSchemes(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >*) in libtensorflow-core-x86_64.a(env.o)
  ""nsync::nsync_mu_unlock(nsync::nsync_mu_s_*)"", referenced from:
      tensorflow::SessionFactory::Register(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::SessionFactory*) in libtensorflow-core-x86_64.a(session_factory.o)
      tensorflow::SessionFactory::GetFactory(tensorflow::SessionOptions const&, tensorflow::SessionFactory**) in libtensorflow-core-x86_64.a(session_factory.o)
      tensorflow::FileSystemRegistryImpl::Register(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<tensorflow::FileSystem* ()>) in libtensorflow-core-x86_64.a(env.o)
      tensorflow::FileSystemRegistryImpl::Lookup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in libtensorflow-core-x86_64.a(env.o)
      tensorflow::FileSystemRegistryImpl::GetRegisteredFileSystemSchemes(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >*) in libtensorflow-core-x86_64.a(env.o)
  ""tensorflow::Status::ToString[abi:cxx11]() const"", referenced from:
      _main in test.cc.o
  ""std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::at(unsigned long) const"", referenced from:
      google::protobuf::io::Tokenizer::IsIdentifier(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in libprotobuf.a(tokenizer.o)
  ""std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::find(char const*, unsigned long, unsigned long) const"", referenced from:
      google::protobuf::GlobalReplaceSubstring(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*) in libprotobuf-lite.a(strutil.o)
      tensorflow::str_util::StringReplace(tensorflow::StringPiece, tensorflow::StringPiece, tensorflow::StringPiece, bool) in libtensorflow-core-x86_64.a(str_util.o)
```"
13305,not able to install tensor flow from pip using 3.5 64 bit version of python.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13303,[Missing Feature] Input Pipeline for Models with Multi-step Optimization,"### Describe the problem

After some searching and reading (e.g. #7951), I found the current input pipeline framework
is generally lack of support for models with multi-step optimization.

In most of the models before GAN (General Adversarial Networks), a model almost has one
optimization function, that is, we optimize over a mini-batch of input data once in a step. While
starting from GAN, many models have two or more optimization functions, in other words, they
sequentially optimize on several functions using the same batch of data (Adversarial Autoencoders). 

The current input pipeline works fine with a unique optimization operation per step, where the `tf.Session` will pull a batch of data from the input queue once. For multi-step optimization models, if you link all steps with a unique input queue, then the queue will be pulled several times
if you link all sequential optimization steps with that queue. Apparently, this is completely wrong.

From my point of view, I think we should add a peek() op in `tf.QueueBase` for supporting
multistep sequential optimization. So for example, for GAN, we can link optimization over discriminator with a peek_many op and link optimization over generator with a dequeue_many op.

In general, for multiple steps, we can do `peek_many` -> `peek_many` -> ... -> `dequeue_many`.

For the new tf.data.Dataset API, when we called `session.run()`, we advance the iterator. So in the new data importing API, we still lack of this feature.

I think currently a workaround of this would be building a buffer with tf.Variable. and make all subsequent optimization step depends on the snapshot of the buffer. Like,

```
buffer = tf.get_variable(""buffer"",
                                        shape=(**, **),
                                        trainable=False,
                                        initializer=tf.zeros_initializer)
input_op = input_function()
assign_op = tf.assign(buffer, input_op)

cache = buffer.value()
# based everything on cache for the subsequent step.

with tf.Session() as sess:
    sess.run(assign_op)
    # then run all other steps.
```

In this case, we have to copy the entire batch of data for each step."
13302,setuptools pip wheel failed with error code 2,"```
dengw-iMac:~ chars$ virtualenv --system-site-packages ~/tensorflow
New python executable in /Users/chars/tensorflow/bin/python
Installing setuptools, pip, wheel...
  Complete output from command /Users/chars/tensorflow/bin/python - setuptools pip wheel:
  Collecting setuptools
Exception:
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/commands/install.py"", line 324, in run
    requirement_set.prepare_files(finder)
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/req/req_set.py"", line 380, in prepare_files
    ignore_dependencies=self.ignore_dependencies))
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/req/req_set.py"", line 554, in _prepare_file
    require_hashes
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/req/req_install.py"", line 278, in populate_link
    self.link = finder.find_requirement(self, upgrade)
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/index.py"", line 465, in find_requirement
    all_candidates = self.find_all_candidates(req.name)
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/index.py"", line 423, in find_all_candidates
    for page in self._get_pages(url_locations, project_name):
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/index.py"", line 568, in _get_pages
    page = self._get_page(location)
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/index.py"", line 683, in _get_page
    return HTMLPage.get_page(link, session=self.session)
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/index.py"", line 792, in get_page
    ""Cache-Control"": ""max-age=600"",
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/_vendor/requests/sessions.py"", line 488, in get
    return self.request('GET', url, **kwargs)
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/download.py"", line 386, in request
    return super(PipSession, self).request(method, url, *args, **kwargs)
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/_vendor/requests/sessions.py"", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/_vendor/requests/sessions.py"", line 596, in send
    r = adapter.send(request, **kwargs)
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/_vendor/cachecontrol/adapter.py"", line 47, in send
    resp = super(CacheControlAdapter, self).send(request, **kw)
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/_vendor/requests/adapters.py"", line 390, in send
    conn = self.get_connection(request.url, proxies)
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/_vendor/requests/adapters.py"", line 290, in get_connection
    proxy_manager = self.proxy_manager_for(proxy)
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/_vendor/requests/adapters.py"", line 184, in proxy_manager_for
    **proxy_kwargs
  File ""/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/_vendor/requests/adapters.py"", line 43, in SOCKSProxyManager
    raise InvalidSchema(""Missing dependencies for SOCKS support."")
InvalidSchema: Missing dependencies for SOCKS support.
----------------------------------------
...Installing setuptools, pip, wheel...done.
Traceback (most recent call last):
  File ""/usr/local/bin/virtualenv"", line 11, in <module>
    sys.exit(main())
  File ""/Library/Python/2.7/site-packages/virtualenv.py"", line 713, in main
    symlink=options.symlink)
  File ""/Library/Python/2.7/site-packages/virtualenv.py"", line 945, in create_environment
    download=download,
  File ""/Library/Python/2.7/site-packages/virtualenv.py"", line 901, in install_wheel
    call_subprocess(cmd, show_stdout=False, extra_env=env, stdin=SCRIPT)
  File ""/Library/Python/2.7/site-packages/virtualenv.py"", line 797, in call_subprocess
    % (cmd_desc, proc.returncode))
OSError: Command /Users/chars/tensorflow/bin/python - setuptools pip wheel failed with error code 2
```

How can I do it? Thanks
"
13298,wrong tf.svd documentation in tensorflow 1.3.0 version,"In tf.svd documentation, it was said that the tensorflow implementation of svd is simply np.linalg.svd

in fact it was not the same, in tensorflow, the index for V matrix is transposed..different from np.linalg.svd default setting. Funny thing is, this type of problem is a famous bug in matlab code people usually write, which is row selection and transpose is not equal to transpose then column selection.

code demonstration:


```
import tensorflow as tf
tf.__version__

## benchmark using numpy svd
import numpy as np

x = np.array([1.0,2.0,3.0,4.0,5.0,6,7,8,9])
x = x.reshape(3,3)

u_,s_,v_=np.linalg.svd(x,compute_uv=True)
# print s_
print u_
print v_

r = 2
diagS = np.diag(s_)
print diagS
diagS = diagS[:r,:r]
x_rank1 = np.matmul(u_[:,:r], np.matmul(diagS, v_[:r,:]))

print 'original = \n',x
print 'rank1 approximation\n',x_rank1
```
Result is
```
[[-0.21483724  0.88723069  0.40824829]
 [-0.52058739  0.24964395 -0.81649658]
 [-0.82633754 -0.38794278  0.40824829]]
[[-0.47967118 -0.57236779 -0.66506441]
 [-0.77669099 -0.07568647  0.62531805]
 [-0.40824829  0.81649658 -0.40824829]]
[[  1.68481034e+01   0.00000000e+00   0.00000000e+00]
 [  0.00000000e+00   1.06836951e+00   0.00000000e+00]
 [  0.00000000e+00   0.00000000e+00   4.41842475e-16]]
original = 
[[ 1.  2.  3.]
 [ 4.  5.  6.]
 [ 7.  8.  9.]]
rank1 approximation
[[ 1.  2.  3.]
 [ 4.  5.  6.]
 [ 7.  8.  9.]]
```

But in tensorflow svd
```
X = tf.constant([1.0,2.0,3.0,4.0,5.0,6,7,8,9],shape=[3,3])

sess = tf.InteractiveSession()
    
print sess.run(X)

s,u,v = tf.svd(X)

print sess.run(u)
print sess.run(v)

diagS_tf = tf.diag(s)
# print sess.run(diagS_tf)

tmp_correct =  tf.matmul(diagS_tf[:r,:r], tf.transpose(v[:,:r]))
tmp_wrong = tf.matmul(diagS_tf[:r,:r], v[:r,:])

print 'following differs'
print sess.run(tf.transpose(v[:,:r]))
print sess.run(v[:r,:])

# print sess.run(tmp)

x_rank1_tf_correct= tf.matmul(u[:,:r], tmp_correct)
x_rank1_tf_wrong= tf.matmul(u[:,:r], tmp_wrong)

print 'printing rank1 approximation from tensorflow'
print sess.run(x_rank1_tf_correct)
print sess.run(x_rank1_tf_wrong)
```

the output is 
```
[[ 1.  2.  3.]
 [ 4.  5.  6.]
 [ 7.  8.  9.]]
[[ 0.21483716  0.88723052 -0.40824857]
 [ 0.52058721  0.24964423  0.81649649]
 [ 0.82633758 -0.38794291 -0.4082481 ]]
[[ 0.47967106 -0.77669096  0.40824836]
 [ 0.57236761 -0.07568647 -0.81649655]
 [ 0.66506428  0.62531805  0.40824822]]
following differs
[[ 0.47967106  0.57236761  0.66506428]
 [-0.77669096 -0.07568647  0.62531805]]
[[ 0.47967106 -0.77669096  0.40824836]
 [ 0.57236761 -0.07568647 -0.81649655]]
printing rank1 approximation from tensorflow
[[ 0.99999887  1.99999869  2.99999857]
 [ 3.9999969   4.99999666  5.99999666]
 [ 6.99999809  7.99999809  8.99999809]]
[[  2.27875805  -2.88305187   0.7037462 ]
 [  4.35980749  -6.83247042   3.36293888]
 [  6.44085884 -10.78189278   6.0221343 ]]

```


"
13297,Feature request for graph visualizer: turn off node collapsing,"There's a simple feed-forward pbtxt [here](https://github.com/yaroslavvb/stuff/blob/master/resnet_8_simple.pbtxt) 

It can be visualized without any edges crossing. However latest TensorBoard collapses Relu-Relu_6 nodes together, so that complicates the diagram and introduces edge crossings. It would be nice to have a way to visualize without node collapsing

![screenshot 2017-09-25 11 35 53](https://user-images.githubusercontent.com/23068/30824844-bf81f922-a1e5-11e7-9cf4-5995ba19e5a4.png)

cc @dsmilkov "
13295,Pre-built binaries with symbol information?,"What you you guys think about providing CI version of TensorFlow with built-in symbol information? This would make it easier to report bugs.

I believe the following is sufficient to get optimized version with symbol tables.
`blaze build --cxxopt=-g2 --linkopt=-g2 --strip never -c opt`

My current problem is that I'm occasionally hitting segfaults due to `tensorflow::strings::FloatToBuffer`. I can't reproduce this in any small example. If there were a version of tf with symbol tables, I could just gdb on the core file and do `info locals` to get the value of offending float that causes the crash.

```
#0  0x00007ffb54a4941d in std::_Hashtable<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, float>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, float> >, std::__detail::_Select1st, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node(unsigned long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long) const ()
   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#1  0x00007ffb5745b76e in float tensorflow::(anonymous namespace)::locale_independent_strtonum<float>(char const*, char const**) ()
   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007ffb5745bf4c in tensorflow::strings::safe_strtof(char const*, float*)
    ()
   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007ffb5745bfe9 in tensorflow::strings::FloatToBuffer(float, char*) ()
   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007ffb5735d7ce in tensorflow::Tensor::SummarizeValue[abi:cxx11](long long) const ()
   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007ffb5735e3b5 in tensorflow::Tensor::DebugString[abi:cxx11]() const ()
   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007ffb572f21ab in tensorflow::(anonymous namespace)::SummarizeTensor(tensorflow::TensorProto const&) ()
   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007ffb572f5529 in tensorflow::SummarizeAttrValue[abi:cxx11](tensorflow::AttrValue const&) ()
   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007ffb57322b9d in tensorflow::SummarizeAttrsHelper(tensorflow::AttrSlice, tensorflow::StringPiece) ()
   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007ffb573231f3 in tensorflow::SummarizeNodeDef[abi:cxx11](tensorflow::NodeDef const&) ()
   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007ffb5732332f in tensorflow::SummarizeNode[abi:cxx11](tensorflow::Node const&) ()
   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007ffb55fc7886 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()
   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#12 0x00007ffb55fc93cf in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8> const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#13 0x00007ffb57442c51 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#14 0x00007ffb57440d37 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#15 0x00007ffb6174c260 in ?? ()
   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/scipy/sparse/../../../../libstdc++.so.6
#16 0x00007ffb6d9926ba in start_thread (arg=0x7ffaa4906700)
    at pthread_create.c:333
#17 0x00007ffb6cdb03dd in clone ()
    at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109

```"
13294,Input Pipeline for High-performance Model,"### Describe the problem
When using data_flow_op.RecordInput in input pipeline on distributed TensorFlow, every input thread seems load all files in the data_dir to the local buffer after shuffling and left-shifting all matched file names in data_dir (https://github.com/tensorflow/tensorflow/blob/40eef4473bda90442bb55fcc67842f097c024580/tensorflow/core/kernels/record_yielder.cc#L139). 
Based on the code, it seems like data input for each epoch ends with loading all files instead of part of the files on every worker.

 If I understand correctly, each input thread from each worker task should read a portion of the files, which should be the shift_ratio * file_num. It will be very helpful, if anyone can explain this. 
"
13293,BUG: tf.reduce_max does not support int64 tensor on GPU.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
binary (pip)
- **TensorFlow version (use command below)**:
v1.3.0-rc2-20-g0787eee 1.3.0
Also tested with:
v1.3.0-rc1-2523-g1e1b3d9 1.4.0-dev20170925
- **Python version**: 
Python 3.5.2
- **Bazel version (if compiling from source)**:
n/a
- **CUDA/cuDNN version**:
CUDA-8.0 / cuDNN-5.1
- **GPU model and memory**:
NVidia GeForce GTX TITAN with 5.93GiB
- **Exact command to reproduce**:
```
import tensorflow as tf

with tf.device(""/gpu:0""):
    values = tf.reshape(tf.range(10, dtype=tf.int64), [-1, 1])
    max_val = tf.reduce_max(values)

    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    print(sess.run(max_val))
```

### Describe the problem
**tf.reduce_max** does not support **int64** tensor on GPU.

### Source code / logs

```
2017-09-25 17:14:14.906614: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2
2017-09-25 17:14:15.106125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-09-25 17:14:15.106623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties:
name: GeForce GTX TITAN major: 3 minor: 5 memoryClockRate(GHz): 0.8755
pciBusID: 0000:04:00.0
totalMemory: 5.93GiB freeMemory: 5.59GiB
2017-09-25 17:14:15.106706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:04:00.0, compute capability: 3.5)
Traceback (most recent call last):
  File ""/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1323, in _do_call
    return fn(*args)
  File ""/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1293, in _run_fn
    self._extend_graph()
  File ""/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1354, in _extend_graph
    self._session, graph_def.SerializeToString(), status)
  File ""/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 467, in raise_exception_on_not_ok_status
    c_api.TF_GetCode(status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Max': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: Max = Max[T=DT_INT64, Tidx=DT_INT32, keep_dims=false, _device=""/device:GPU:0""](Reshape, Const)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""bug2.py"", line 9, in <module>
    sess.run(tf.global_variables_initializer())
  File ""/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Max': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: Max = Max[T=DT_INT64, Tidx=DT_INT32, keep_dims=false, _device=""/device:GPU:0""](Reshape, Const)]]

Caused by op 'Max', defined at:
  File ""bug2.py"", line 6, in <module>
    max_val = tf.reduce_max(values)
  File ""/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py"", line 1525, in reduce_max
    name=name)
  File ""/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 2436, in _max
    keep_dims=keep_dims, name=name)
  File ""/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3090, in create_op
    op_def=op_def)
  File ""/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1638, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Max': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: Max = Max[T=DT_INT64, Tidx=DT_INT32, keep_dims=false, _device=""/device:GPU:0""](Reshape, Const)]]

```"
13292,cudnn PoolBackward launch failed when using tf.nn.max_pool on Tensorflow GPU (Windows 10),"When using max_pool, the error below shows and stops the code. I used the code available here: [https://github.com/charlesashby/CharLSTM](https://github.com/charlesashby/CharLSTM). Specifically, I used the `lib_model/bidirectional_lstm.py` and the error occurs at the `tdnn` function when `tf.nn.max_pool` is ran.

What does this error mean? Thanks!

```
Caused by op 'gradients/TDNN/MaxPool_grad/MaxPoolGrad', defined at:
  File ""main.py"", line 33, in <module>
    network.train()
  File ""E:\rktamplayo\Personalized\CharLSTM\lib_model\bidirectional_lstm.py"", line 165, in train
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)
  File ""C:\Users\seung-won\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\optimizer.py"", line 315, in minimize
    grad_loss=grad_loss)
  File ""C:\Users\seung-won\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\optimizer.py"", line 386, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""C:\Users\seung-won\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 542, in gradients
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""C:\Users\seung-won\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 348, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""C:\Users\seung-won\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 542, in <lambda>
    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
  File ""C:\Users\seung-won\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\nn_grad.py"", line 526, in _MaxPoolGrad
    data_format=op.get_attr(""data_format""))
  File ""C:\Users\seung-won\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 1752, in _max_pool_grad
    data_format=data_format, name=name)
  File ""C:\Users\seung-won\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""C:\Users\seung-won\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\seung-won\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'TDNN/MaxPool', defined at:
  File ""main.py"", line 31, in <module>
    network.build()
  File ""E:\rktamplayo\Personalized\CharLSTM\lib_model\bidirectional_lstm.py"", line 103, in build
    cnn = tdnn(self.X, kernels, kernel_features)
  File ""E:\rktamplayo\Personalized\CharLSTM\lib_model\bidirectional_lstm.py"", line 92, in tdnn
    pool = tf.nn.max_pool(tf.nn.tanh(conv), [1, 1, reduced_length, 1], [1, 1, 1, 1], 'VALID')
  File ""C:\Users\seung-won\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 1772, in max_pool
    name=name)
  File ""C:\Users\seung-won\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 1605, in _max_pool
    data_format=data_format, name=name)
  File ""C:\Users\seung-won\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""C:\Users\seung-won\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\seung-won\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InternalError (see above for traceback): cudnn PoolBackward launch failed
         [[Node: gradients/TDNN/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, data_format=""NHWC"", ksize=[1, 1, 16, 1], padding=""VALID"", strides=[1, 1, 1, 1], _device=""/job:localhost/replica:0/task:0/gpu:0""](TDNN/Tanh, TDNN/MaxPool, gradients/TDNN/Squeeze_grad/Reshape)]]
```

**What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?**

I have searched through all websites including GitHub and StackOverflow, and it seems like this error has not been reported anywhere else.

**Environment info**

- Tensorflow 1.3.0
- Python 3.5.3
- CUDA 8.0
- cuDNN 6.0
- OS: Windows 10
- GPU: GeForce GTX 1080ti"
13290,issue while installing tensorflow ,"sws@sws-Aspire-5830T:~$ source activate tensorflow
(tensorflow) sws@sws-Aspire-5830T:~$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl
(tensorflow) sws@sws-Aspire-5830T:~$ (tensorflow)$ pip3 install --ignore-installed --upgrade $TF_BINARY_URL
bash: syntax error near unexpected token `$'
(tensorflow) sws@sws-Aspire-5830T:~$ (tensorflow)$ pip3 install --ignore-installed --upgrade $TF_BINARY_URL
bash: syntax error near unexpected token `$'
(tensorflow) sws@sws-Aspire-5830T:~$ (tensorflow)$ pip3 install --ignore-installed --upgrade TF_BINARY_URL
bash: syntax error near unexpected token `$'
(tensorflow) sws@sws-Aspire-5830T:~$ pip3 install --ignore-installed --upgrade $TF_BINARY_URL
Collecting tensorflow==0.12.1 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl
  Downloading https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl (43.1MB)
    100% |████████████████████████████████| 43.1MB 22kB/s 
Collecting protobuf>=3.1.0 (from tensorflow==0.12.1)
  Downloading protobuf-3.4.0-cp35-cp35m-manylinux1_x86_64.whl (6.2MB)
    100% |████████████████████████████████| 6.2MB 124kB/s 
Collecting wheel>=0.26 (from tensorflow==0.12.1)
  Downloading wheel-0.30.0-py2.py3-none-any.whl (49kB)
    100% |████████████████████████████████| 51kB 3.6MB/s 
Collecting numpy>=1.11.0 (from tensorflow==0.12.1)
  Downloading numpy-1.13.1-cp35-cp35m-manylinux1_x86_64.whl (16.9MB)
    100% |████████████████████████████████| 16.9MB 62kB/s 
Collecting six>=1.10.0 (from tensorflow==0.12.1)
  Using cached six-1.11.0-py2.py3-none-any.whl
Collecting setuptools (from protobuf>=3.1.0->tensorflow==0.12.1)
  Downloading setuptools-36.5.0-py2.py3-none-any.whl (478kB)
    100% |████████████████████████████████| 481kB 202kB/s 
Installing collected packages: six, setuptools, protobuf, wheel, numpy, tensorflow
Exception:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/site-packages/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/usr/local/lib/python3.5/site-packages/pip/commands/install.py"", line 342, in run
    prefix=options.prefix_path,
  File ""/usr/local/lib/python3.5/site-packages/pip/req/req_set.py"", line 784, in install
    **kwargs
  File ""/usr/local/lib/python3.5/site-packages/pip/req/req_install.py"", line 851, in install
    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)
  File ""/usr/local/lib/python3.5/site-packages/pip/req/req_install.py"", line 1064, in move_wheel_files
    isolated=self.isolated,
  File ""/usr/local/lib/python3.5/site-packages/pip/wheel.py"", line 345, in move_wheel_files
    clobber(source, lib_dir, True)
  File ""/usr/local/lib/python3.5/site-packages/pip/wheel.py"", line 323, in clobber
    shutil.copyfile(srcfile, destfile)
  File ""/usr/local/lib/python3.5/shutil.py"", line 115, in copyfile
    with open(dst, 'wb') as fdst:
PermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.5/site-packages/six.py'
(tensorflow) sws@sws-Aspire-5830T:~$ pip3 install --ignore-installed --upgrade $TF_BINARY_URL
Collecting tensorflow==0.12.1 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl
  Using cached https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl
Collecting six>=1.10.0 (from tensorflow==0.12.1)
  Using cached six-1.11.0-py2.py3-none-any.whl
Collecting wheel>=0.26 (from tensorflow==0.12.1)
  Using cached wheel-0.30.0-py2.py3-none-any.whl
Collecting numpy>=1.11.0 (from tensorflow==0.12.1)
  Using cached numpy-1.13.1-cp35-cp35m-manylinux1_x86_64.whl
Collecting protobuf>=3.1.0 (from tensorflow==0.12.1)
  Using cached protobuf-3.4.0-cp35-cp35m-manylinux1_x86_64.whl
Collecting setuptools (from protobuf>=3.1.0->tensorflow==0.12.1)
  Using cached setuptools-36.5.0-py2.py3-none-any.whl
Installing collected packages: six, wheel, numpy, setuptools, protobuf, tensorflow
Exception:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/site-packages/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/usr/local/lib/python3.5/site-packages/pip/commands/install.py"", line 342, in run
    prefix=options.prefix_path,
  File ""/usr/local/lib/python3.5/site-packages/pip/req/req_set.py"", line 784, in install
    **kwargs
  File ""/usr/local/lib/python3.5/site-packages/pip/req/req_install.py"", line 851, in install
    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)
  File ""/usr/local/lib/python3.5/site-packages/pip/req/req_install.py"", line 1064, in move_wheel_files
    isolated=self.isolated,
  File ""/usr/local/lib/python3.5/site-packages/pip/wheel.py"", line 345, in move_wheel_files
    clobber(source, lib_dir, True)
  File ""/usr/local/lib/python3.5/site-packages/pip/wheel.py"", line 329, in clobber
    os.utime(destfile, (st.st_atime, st.st_mtime))
PermissionError: [Errno 1] Operation not permitted

"
13289,Bug: SDCAModel missbehave/bug when mixing sparse and dense features,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Up-to-date Arch Linux
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.3.0
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
Any toy problem using [https://www.tensorflow.org/api_docs/python/tf/contrib/linear_optimizer/SdcaModel](SDCAModel) that mixes sparse and dense features.

### Describe the problem
It's a bug. 
When using both sparse and dense features in [SDCAModel](https://www.tensorflow.org/api_docs/python/tf/contrib/linear_optimizer/SdcaModel) the _linear_predictions method output a tensor of wrong shape. This happens because the sparse results and dense results ranks are inconsistent in the current version and summing them trigger an unexpected broadcast. 
https://github.com/tensorflow/tensorflow/pull/13279 fixes the bug.
"
13288,Cannot put a Tensor into a StagingArea,"TensorFlow version: 1.3.0

According to the document, I should be able to put a tensor to a StagingArea<sup>[[1]](https://www.tensorflow.org/api_docs/python/tf/contrib/staging/StagingArea#put)</sup>. But the following code does not work:

```python
import tensorflow as tf
from tensorflow.contrib import staging

staging.StagingArea(dtypes=[tf.int32]).put(tf.constant(1))
```

The output says:

```
Traceback (most recent call last):
  File ""/███████/dataset.py"", line 4, in <module>
    staging.StagingArea(dtypes=[tf.int32]).put(tf.constant(1))
  File ""/███████/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 1671, in put
    vals, _ = self._check_put_dtypes(values, indices)
  File ""/███████/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 1480, in _check_put_dtypes
    raise ValueError(""Indices must be supplied when inserting a list ""
ValueError: Indices must be supplied when inserting a list of tensors
```"
13287,"Make a tools CLI for easy access to freeze_graph, saved_model_cli, etc.","Related to #6134. Right now, the users of the `tensorflow/python/tools` scripts have these options:

* Download and save a copy of the individual `.py` file with whatever model/project they are working on, so that it is bundled with their code. This causes tons of code duplication across projects.
* Import the tools programmatically and use them there. Being able to do this is extremely handy, but often the most convenient way to use the tool (and the way they are generally designed to be used) are as ad-hoc command-line calls.
* Build a binary using Bazel. This is extremely inconvenient, and doesn't really solve the problem of code-reuse from above. I'd also argue that building a binary for what amounts to a Python script is overkill.
* Create a custom CLI/module/alias that bundles these tools together (or allows for a single tool to be called from anywhere). This is convenient, but complicates setting up a development environment, and leads to divergent patterns across the user base.

I think having some sort of unified tool that is able to provide uniform access to the CLI of frequently used tools, such as `freeze_graph.py` and `saved_model_cli.py` would greatly improve UX, as well as make it much easier to explain how to use these tools. Additionally, it would help prevent version mismatches of tooling, as the correct version of the tool would already be at their fingertips (instead of fumbling through GitHub branches).

I think having some sort of simple umbrella module, let's call it `tftools`, which provides access to the underlying python files would suffice. E.g.

```
$ tftools freeze_graph --input_graph=... --output_graph=...

$ tftools saved_model_cli show --dir /tmp/saved_model_dir
```

This would be a little bit of legwork, but I think the benefit-out to effort-in ratio is pretty large, at least from a user perspective. Not sure if the TensorFlow team would want to handle this or leave it to the community (if I can get company approval, I'd be happy to take a crack at it)."
13286,[bug/docs] tf.Estimator: 'train' method is missing,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Used doc

- **OS Platform and Distribution, versions etc**:
<details>
== cat /etc/issue ===============================================
Darwin PEDS-0MAHH2C-LT 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64
Mac OS X 10.12.6

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 8.1.0 (clang-802.0.42)
Target: x86_64-apple-darwin16.7.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== uname -a =====================================================
Darwin PEDS-0MAHH2C-LT 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.13.1)
protobuf (3.3.0)
tensorflow (1.2.1)

== check for virtualenv =========================================
False
== tensorflow import ============================================
tf.VERSION = 1.2.1
tf.GIT_VERSION = v1.2.0-5-g435cdfc
tf.COMPILER_VERSION = v1.2.0-5-g435cdfc
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 105: nvidia-smi: command not found

No GPU
</details>

- **Exact command to reproduce**:
I am modifying this [script that works](http://localhost:8888/notebooks/keras_tfest/keras-estimator/Integration%20of%20Keras%20with%20Tensorflow.ipynb) + documentation as referenced below. The resulting code is following:

<details>

    # coding: utf-8
    import sys
    sys.path.append(""/data/dlituiev/target2/"")
    from inception import get_model
    from keras.models import Sequential, Model
    from keras.layers import Dense, Dropout, Activation, Flatten, InputLayer, Reshape, Input
    import tensorflow as tf
    from tensorflow.python.framework.ops import _get_graph_from_inputs
    from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib
    import numpy as np
    from keras import backend as K
    K.set_image_dim_ordering('tf')

    BATCH_SIZE = 4
    n_classes=10
    final_activation='linear'
    lr=1e-4
    opt_name = ""Adadelta""
    epochs = 20
    HEIGHT, WIDTH = 512, 512

    def get_model():
        model = Sequential()
        model.add(InputLayer(input_shape=[512 , 512, 1], batch_size=BATCH_SIZE))
        #     model.add(Reshape([512*512,]))
        model.add(Flatten())
        model.add(Dense(n_classes, activation='relu', ))
        return model


    g = tf.Graph()
    with g.as_default():
        model = get_model()
        model.build()
        funcmodel = model.model

        def model_fn(features, labels, params):
            optimizer = params[""optimizer""]
            opt_params= params.get(""opt_params"", {})
        #     x = tf.placeholder(tf.float32, shape=model.layers[0].input.shape)
            print(""features"", features.shape)
            logyhat = funcmodel(features)

            if (mode == tf.estimator.ModeKeys.TRAIN or
                mode == tf.estimator.ModeKeys.EVAL):
        #         loss = tf.contrib.keras.backend.categorical_crossentropy()
                loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logyhat)
            else:
                loss = None
            if mode == tf.estimator.ModeKeys.TRAIN:
                optimizer = getattr(tf.train, optimizer)
                train_op = optimizer(opt_params).minimize(loss)
            else:
                train_op = None
            if mode == tf.estimator.ModeKeys.PREDICT:
                predictions = tf.nn.softmax(logyhat)
            else:
                predictions = None

            """"""
            return tf.estimator.EstimatorSpec(
                  mode=mode,
                  predictions=predictions,
                  loss=loss,
                  train_op=train_op)
                  """"""

            return model_fn_lib.ModelFnOps(
                  mode=mode,
                  predictions=predictions,
                  loss=loss,
                  train_op=train_op,
                  #eval_metric_ops=eval_metric_ops
                  )
        ##############################################

        def parser(record):
            keys_to_features = {
                'height': tf.FixedLenFeature([], tf.int64),
                'width': tf.FixedLenFeature([], tf.int64),
                'image_raw': tf.FixedLenFeature([], tf.string),
                'label': tf.FixedLenFeature([], tf.int64)
                }
            features = tf.parse_single_example(
              record,
              features=keys_to_features)

            # Convert from a scalar string tensor (whose single string has
            # length mnist.IMAGE_PIXELS) to a uint8 tensor with shape
            # [mnist.IMAGE_PIXELS].
            image = tf.decode_raw(features['image_raw'], tf.float32)
            #annotation = tf.decode_raw(features['mask_raw'], tf.uint8)

            height = tf.cast(features['height'], tf.int32)
            width = tf.cast(features['width'], tf.int32)

    #         image_shape = tf.stack([height, width, 1])
            image_shape = tf.stack([HEIGHT, WIDTH,1])
            #annotation_shape = tf.pack([height, width, 1])

            image = tf.reshape(image, image_shape)
            label = tf.cast(features[""label""], tf.int32)
            return image, label

        def get_dataset_inp_fn(filenames, epochs=20, batch_size=2,
                               buffer_size=100):
            def dataset_input_fn():
                dataset = tf.contrib.data.TFRecordDataset(filenames)

                # Use `tf.parse_single_example()` to extract data from a `tf.Example`
                # protocol buffer, and perform any additional per-record preprocessing.

                # Use `Dataset.map()` to build a pair of a feature dictionary and a label
                # tensor for each example.
                dataset = dataset.map(parser)
                dataset = dataset.shuffle(buffer_size=buffer_size)
                dataset = dataset.batch(batch_size)
                dataset = dataset.repeat(epochs)
                iterator = dataset.make_one_shot_iterator()

                # `features` is a dictionary in which each value is a batch of values for
                # that feature; `labels` is a batch of labels.
                features, labels = iterator.get_next()
                return features, labels
            return dataset_input_fn

    ##############################################
    def get_optimizer(opt_name = ""Adadelta"", **kwargs):
        #lr=kwargs.pop(""lr"", 1e-4)
        opt_name = opt_name if opt_name.endswith(""Optimizer"") else opt_name.title()+""Optimizer""
        optimizer = getattr(tf.train, opt_name)
        return optimizer#(lr, **kwargs)
    ##############################################

    from tensorflow.contrib.learn.python.learn.estimators import estimator
    LEARNING_RATE = 0.001
    # Set model params
    model_params = {""opt_params"":{""learning_rate"": LEARNING_RATE}, 'optimizer' : get_optimizer()}

    inpfun = get_dataset_inp_fn([""example.tfrecords""],
                                    epochs=epochs,
                                    batch_size=BATCH_SIZE)
    # Instantiate Estimator
    est = estimator.Estimator(model_fn=model_fn, params=model_params)

    print(help(est))

    ### Option 1 ###
    est.train(input_fn=inpfun, steps=5000)

    ### Option 2 ###
    est.fit(input_fn=inpfun, steps=5000)


</details>

### Describe the problem
In this manual it implies that `tf.Estimator` has a method `train()`:
https://www.tensorflow.org/extend/estimators

When I instantiate an estimator and run `est.train(input_fn=inpfun, steps=5000)` I am getting:

     AttributeError: 'Estimator' object has no attribute 'train'
"
13285,"After installing tenserflow the following error is generated  Python 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tenserflow as tf Traceback (most recent call last):   File ""<stdin>"", line 1, in <module> ModuleNotFoundError: No module named 'tenserflow'","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **No Custom code written. Only codes that have been used are from Tensorflow official documentation  (as opposed to using a stock example script provided in TensorFlow)**:
- **Windows 10 Enterprise x64 (e.g., Linux Ubuntu 16.04)**:
- **Tensorflow installed from cmd using pip3 install command (source or binary)**:
- **TensorFlow with CPU support only :
- **Python version 3.6.2**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU Intel HD Graphics 5500 RAM 8GB, Total Graphics Memory 4161MB**:
- pip3 install --upgrade tensorflow

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
After I install Tensorflow using the command pip3 install --upgrade tensorflow the installation completes without any issues. When I put in the line ""import tensorflow as tf"", in the pyton interactive shell I get the following error File ""<stdin>"", line 1, in <module> ModuleNotFoundError: No module named 'tensorflow'

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13283,'Cannot interpret feed_dict key as Tensor: Can not convert a int into a Tensor' when using probabilities.eval?,"I want to use tensor-flow to print all the probabilities for my own image. The following code works well for any mnist image that i get from online but does not work with my own images. Here is the whole code.

    from PIL import Image
    import random
    from tensorflow.examples.tutorials.mnist import input_data
    import tensorflow as tf
    import numpy as np
    import scipy.ndimage
    from PIL import Image
    
    
    np.set_printoptions(threshold='nan')
    
    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)
    
    
    x = tf.placeholder(tf.float32, [None, 784])
    W = tf.Variable(tf.zeros([784, 10]))
    b = tf.Variable(tf.zeros([10]))
    
    y = tf.nn.softmax(tf.matmul(x, W) + b)
    y_ = tf.placeholder(tf.float32, [None, 10])
    
    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
    
    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
    
    init = tf.initialize_all_variables()
    
    sess = tf.Session()
    sess.run(init)
    
    for i in range(1000):
        batch_xs, batch_ys = mnist.train.next_batch(1000)
        sess.run(train_step, feed_dict= {x: batch_xs, y_: batch_ys})
    
    print (""done with training"")
    print (""\n\n\n"")
    
    #Weights = (W.eval())
    ##print Weights
    #
    #Bias = (b.eval())
    ##print Bias
    
    probabilities  = y
     
    #digit = 80
    #batch_xs = np.reshape(mnist.test.images[digit], (1, 784))
    ##print isinstance(mnist.test.images[digit], tuple)
    #print batch_xs
    
    img = Image.open('mnist_7_367.jpg').convert('L')  # convert image to 8-bit grayscale
    data = list(img.getdata()) # convert image data to a list of integers
    newList = [float(x) / 255.0 for x in data]
    newList1 = np.asarray(newList)
    batch_xs = np.reshape(newList1, (1, 784))
    batch_xs[batch_xs < 0.1] = 0
    #print isinstance(newList1, list)
    print batch_xs
    
    
    counter = 0
    for x in probabilities.eval(feed_dict={x: batch_xs}, session = sess):
        print ('[')
        for y in x:
            print (counter, '{:f}'.format(float(y)))
            counter += 1
        print (']')
    
    hello = np.reshape(batch_xs, (28, 28))  
    
    
    Matrix = np.array([])
    
    for x in hello:
        for y in x:
            Matrix = np.append(Matrix, int(255 * y))    
    
    
    imageReal = np.reshape(Matrix, (28, 28))
    
    img = Image.fromarray(imageReal)
    img.show()
    
    sess.close()

The problem I am having is that

    for x in probabilities.eval(feed_dict={x: batch_xs}, session = sess): 

is producing the following error:

    Cannot interpret feed_dict key as Tensor: Can not convert a int into a Tensor."
13281,"Non-working example in documentation, with recommendations for how to fix","== cat /etc/issue ===============================================
Linux EricDesktop 4.4.0-96-generic #119-Ubuntu SMP Tue Sep 12 14:59:54 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.2 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux EricDesktop 4.4.0-96-generic #119-Ubuntu SMP Tue Sep 12 14:59:54 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.1)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.6)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc1-2409-g5ee3804
tf.COMPILER_VERSION = v1.3.0-rc1-2409-g5ee3804
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Sun Sep 24 14:08:03 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.82                 Driver Version: 375.82                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 670     Off  | 0000:03:00.0     N/A |                  N/A |
| 33%   50C    P0    N/A /  N/A |    254MiB /  4031MiB |     N/A      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 670     Off  | 0000:04:00.0     N/A |                  N/A |
| 32%   48C    P0    N/A /  N/A |    253MiB /  4036MiB |     N/A      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0                  Not Supported                                         |
|    1                  Not Supported                                         |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61

### Describe the problem

    I have been trying to get a custom op working for many hours, and have finally collected enough information to know that it isn't entirely my fault, and I can help you improve the custom op documentation to save others such effort.  
Walkthrough to see what the problem is:
1) Following the instructions on the documentation page (https://www.tensorflow.org/extend/adding_an_op) I copy the zero_out.cc code
2) Compile with Bazel
3) Test with python.  Everything works, this example behaves exactly as it should.
4) But I want a GPU kernel, so now I try the ""example"" example.
Copy the example.h, example.cc and example.cu.cc files on the documentation page.  Didn't make any changes.
5) Now I hit some problems.  The documentation isn't very clear on how to compile this more complicated op with bazel.  I managed to figure it out, but I would strongly recommend notifying users about the ""gpu_srcs"" argument that can be used inside the BUILD file, since this took me quite a while to discover.

Working BUILD file:
load(""//tensorflow:tensorflow.bzl"", ""tf_custom_op_library"")

tf_custom_op_library(
    name = ""example.so"",
    srcs = [""example.cc"", ""example.h""],
    gpu_srcs = [""example.cu.cc"", ""example.h""],
)
Working Bazel command to invoke the BUILD file:
bazel build --config opt --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/core/user_ops:example.so

6) Now it compiles.  But the op does not load properly in python; the module created by tf.load_op_library does not contain the actual op, as you can see by my tests under the source code section.
7) After a long time, I discover the extremely obvious problem: the example code in the documentation does not have any REGISTER_OP macro call.  This is clearly an omission in the documentation.
8) I try copying the REGISTER_OP macro call from the zero_out example, and it doesn't really work since the ""example"" example is more complicated, but python now at least recognizes the op.  I am sure if I were to spend the time to figure out how to correctly call the REGISTER_OP macro in example.cc, the example would work correctly for me.

    I could make some other recommendations about changing the ""Adding a New Op"" documentation page, though I will refrain for now since I am not sure this is the appropriate place to do so.  I would be happy to take a stab at editing the doc page myself, though since I am very new to github I am concerned that I would end up creating more problems than I would fix, so it might be better to have someone with more experience do it.  I would be happy to help though.

    One thing I will say though: I would really love it if some TF expert would add a fully fleshed out template op to the user op documentation page with all the bells and whistles, and with clear instructions on exactly how to compile and run it.  It should have both GPU support and a gradient implementation.  And ideally it would be multi-threaded (if the ""example"" example isn't already - its not obvious to me either way).  The template would be provided with everything you need, and would have a clearly labeled code block where the user can add their own code:  ""here is a for loop that iterates over every element in the tensor.  write whatever you want here.""  If you can make it really easy for users to add their own, high-quality operators, you might be able to cut down on your workload responding to problems with custom ops and with users requesting new ops.  And I think that a template that we can fill in would be enough to do that for many people.

### Source code / logs

=========================================== start test.py:
import tensorflow as tf
zero_out_module = tf.load_op_library('./zero_out.so')
with tf.Session(''):
  print ""zero_out:""
  print(zero_out_module.zero_out([[1, 2], [3, 4]]).eval())

example_module = tf.load_op_library('./example.so')
with tf.Session(''):
  print ""example:""
  try:
    print(example_module.example([[1, 2], [3, 4]]).eval())
  except AttributeError as err:
    print ""Error: "", err
    
  
print ""Analysis of zero_out_module:""  
print zero_out_module.__dict__.keys()
print zero_out_module.OP_LIST

print ""Analysis of example_module:""
print example_module.__dict__.keys()
print example_module.OP_LIST

====================================== end test.py

Output of running test.py
eric@EricDesktop:~/tensorflow/tensorflow/core/user_ops$ python test.py2017-09-24 14:14:08.183509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-09-24 14:14:08.183794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties: 
name: GeForce GTX 670 major: 3 minor: 0 memoryClockRate(GHz): 1.0455
pciBusID: 0000:04:00.0
totalMemory: 3.94GiB freeMemory: 3.66GiB
2017-09-24 14:14:08.208238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-09-24 14:14:08.208514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 1 with properties: 
name: GeForce GTX 670 major: 3 minor: 0 memoryClockRate(GHz): 1.0455
pciBusID: 0000:03:00.0
totalMemory: 3.94GiB freeMemory: 3.66GiB
2017-09-24 14:14:08.208964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:980] Device peer to peer matrix
2017-09-24 14:14:08.208988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] DMA: 0 1 
2017-09-24 14:14:08.208994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] 0:   Y Y 
2017-09-24 14:14:08.209000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] 1:   Y Y 
2017-09-24 14:14:08.209012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:04:00.0, compute capability: 3.0)
2017-09-24 14:14:08.209020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: GeForce GTX 670, pci bus id: 0000:03:00.0, compute capability: 3.0)
zero_out:
[[1 0]
 [0 0]]
2017-09-24 14:14:08.239952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:04:00.0, compute capability: 3.0)
2017-09-24 14:14:08.239970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: GeForce GTX 670, pci bus id: 0000:03:00.0, compute capability: 3.0)
example:
Error:  'module' object has no attribute 'example'
Analysis of zero_out_module:
['_op_def_pb2', '_op_def_lib', '_op_def_registry', '_ops', '_collections', '_common_shapes', '__builtins__', 'zero_out', '__package__', '_op_def_library', 'OP_LIST', 'LIB_HANDLE', '__name__', '_InitOpDefLibrary', '__doc__']
op {
  name: ""ZeroOut""
  input_arg {
    name: ""to_zero""
    type: DT_INT32
  }
  output_arg {
    name: ""zeroed""
    type: DT_INT32
  }
}

Analysis of example_module:
['_op_def_pb2', '_op_def_lib', '_op_def_registry', '_ops', '_collections', '_common_shapes', '__builtins__', '__package__', '_op_def_library', 'OP_LIST', 'LIB_HANDLE', '__name__', '_InitOpDefLibrary', '__doc__']


"
13278,tf_cc_binary() makes opencv unable to load an image,"I try to load an image with opencv and work further on it with the tensorflow framework. Unfortunately I get a really weird behaviour:

The image is loaded without problems using `cc_binary(...)` in Bazel. Changing it to `tf_cc_binary(...)` doesn't stop the code from compilation or running, but opencv can't load any images any more.

### Source code / logs
This is my BUILD file:

    load(""//tensorflow:tensorflow.bzl"", ""tf_cc_binary"")

    #tf_cc_binary( <-- using this, no image could be loaded anymore
    cc_binary(
        name = ""main"",
        srcs = [""main.cpp""],
        linkopts = [
            ""-lopencv_core"",
            ""-lopencv_highgui"",
            ""-lopencv_imgcodecs"",
            ""-lopencv_imgproc"",
        ],
        visibility=[""//visibility:public""]
    )

I use the standard example code from the opencv website. Again, it is working and the image gets loaded using `cc_binary(`:


    #include <opencv2/core/core.hpp>
    #include <opencv2/highgui/highgui.hpp>
    #include <iostream>
    
    using namespace cv;
    using namespace std;
    
    int main( int argc, char** argv )
    {
        Mat image;
        image = imread(""tensorflow/test/imageHolder/data/example.jpg"", CV_LOAD_IMAGE_COLOR);   // Read the file
    
        if(! image.data )                              // Check for invalid input
        {
            cout <<  ""Could not open or find the image"" << std::endl ;
            return -1;
        }
    
        namedWindow( ""Display window"", WINDOW_AUTOSIZE );// Create a window for display.
        imshow( ""Display window"", image );                   // Show our image inside it.
    
        waitKey(0);                                          // Wait for a keystroke in the window
        return 0;
    }

This is my file structure in case it matters:

    ├── data
        ├── example.jpg
    └── src
        ├── BUILD
        ├── main.cpp
"
13276,tensor flow optimizer,"![desto](https://user-images.githubusercontent.com/29457825/30785133-38286aee-a17f-11e7-9130-2e271e4543a3.PNG)
"
13272,Why will tf.PaddingFIFOQueue print some information? ,"I have asked this question on [stackoverflow](https://stackoverflow.com/questions/46390289/why-is-padding-fifo-queue-closed), but never receives a response.  I think it is not error, but  log information?"
13269,TensorFlow 1.0 and 1.2 behave differently on MultiRNNCell.,"The following code works well on TF 1.0.1, but doesn't work on TF 1.2 and above, leaving error massage, 

File ""pb_OE_column_on_the_spot.py"", line 318, in <module>
    outputs1, _states = tf.nn.dynamic_rnn(_multi_cells, tf.one_hot(_X, data_dim), dtype=tf.float32)
ValueError: Trying to share variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel, but specified shape (256, 512) and found shape (132, 512).

I followed the change after version 1.2: [https://github.com/tensorflow/tensorflow/releases](url),
where states: To get 5 layers each with their own parameters, write: MultiRNNCell([LSTMCell(...) for _ in range(5)])

and I also reviewed similar posts regarding same issue: [https://github.com/udacity/deep-learning/issues/132](url)

    seq_length = 6
    data_dim =4
    hidden_dim = 12
    X = tf.placeholder(tf.int32, [None, seq_length])
    Y = tf.placeholder(tf.int32, [None]) 
    keep_prob = tf.placeholder(tf.float32) 

    X_one_hot = tf.one_hot(X, data_dim)
    Y_one_hot = tf.one_hot(Y, 2) 

    def lstm_cell():
        lstm = tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True)
        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)
        return drop

    multi_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(3)], state_is_tuple=True)
    outputs1, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype=tf.float32)

Why does this error happen?
.
.
.
by the way, would 12 for the number of units in the LSTM cell be too small?"
13267,"Ubuntu 16.04 + CUDA8.0 +CUDNN5.1, C++ compilation fails","Operating System: Ubuntu 16.04 LTS

Installed version of CUDA and cuDNN: 8.0.61 + 5.1.10
```
-rw-r--r-- 1 root root   556000  9月 23 23:17 /usr/local/cuda/lib64/libcudadevrt.a
lrwxrwxrwx 1 root root       16  9月 23 23:17 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
lrwxrwxrwx 1 root root       19  9月 23 23:17 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
-rwxr-xr-x 1 root root   415432  9月 23 23:17 /usr/local/cuda/lib64/libcudart.so.8.0.61
-rw-r--r-- 1 root root   775162  9月 23 23:17 /usr/local/cuda/lib64/libcudart_static.a
-rwxr-xr-x 1 root root 84163560  9月 23 23:35 /usr/local/cuda/lib64/libcudnn.so
-rwxr-xr-x 1 root root 84163560  9月 23 23:35 /usr/local/cuda/lib64/libcudnn.so.5
-rwxr-xr-x 1 root root 84163560  9月 23 23:35 /usr/local/cuda/lib64/libcudnn.so.5.1.10
-rw-r--r-- 1 root root 70364814  9月 23 23:35 /usr/local/cuda/lib64/libcudnn_static.a

```

```
Build label: 0.5.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Aug 25 10:00:00 2017 (1503655200)
Build timestamp: 1503655200
Build timestamp as int: 1503655200
```
source:https://github.com/ymgaq/AQ

I use command: ""bazel build :AQ"" compile it
error message have 52mb ,cant upload
The error message is similar
```
Loading: 
Loading: 0 packages loaded
WARNING: /home/jack/tensorflow/tensorflow/core/BUILD:1773:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/jack/tensorflow/tensorflow/tensorflow.bzl:1029:30
INFO: Analysed target //AQ/src:AQ (0 packages loaded).
INFO: Found 1 target...
[0 / 2] BazelWorkspaceStatusAction stable-status.txt
[1 / 2] Linking AQ/src/AQ; 1s local
[1 / 2] Linking AQ/src/AQ; 11s local
ERROR: /home/jack/tensorflow/AQ/src/BUILD:1:1: Linking of rule '//AQ/src:AQ' failed (Exit 1)
bazel-out/local_linux-opt/bin/AQ/src/_objs/AQ/AQ/src/nueral_net.o: In function `std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>::~pair()':
nueral_net.cpp:(.text._ZNSt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEED2Ev[_ZNSt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEED5Ev]+0x15): undefined reference to `tensorflow::Tensor::~Tensor()'
bazel-out/local_linux-opt/bin/AQ/src/_objs/AQ/AQ/src/nueral_net.o: In function `tensorflow::TTypes<float, 2ul, long>::Tensor tensorflow::Tensor::tensor<float, 2ul>()':
nueral_net.cpp:(.text._ZN10tensorflow6Tensor6tensorIfLm2EEENS_6TTypesIT_XT0_ElE6TensorEv[_ZN10tensorflow6Tensor6tensorIfLm2EEENS_6TTypesIT_XT0_ElE6TensorEv]+0x20): undefined reference to `tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType) const'
nueral_net.cpp:(.text._ZN10tensorflow6Tensor6tensorIfLm2EEENS_6TTypesIT_XT0_ElE6TensorEv[_ZN10tensorflow6Tensor6tensorIfLm2EEENS_6TTypesIT_XT0_ElE6TensorEv]+0x2d): undefined reference to `tensorflow::TensorShape::CheckDimsEqual(int) const'
nueral_net.cpp:(.text._ZN10tensorflow6Tensor6tensorIfLm2EEENS_6TTypesIT_XT0_ElE6TensorEv[_ZN10tensorflow6Tensor6tensorIfLm2EEENS_6TTypesIT_XT0_ElE6TensorEv]+0x3a): undefined reference to `tensorflow::TensorShape::CheckDimsAtLeast(int) const'
nueral_net.cpp:(.text._ZN10tensorflow6Tensor6tensorIfLm2EEENS_6TTypesIT_XT0_ElE6TensorEv[_ZN10tensorflow6Tensor6tensorIfLm2EEENS_6TTypesIT_XT0_ElE6TensorEv]+0x64): undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dim_size(int) const'
bazel-out/local_linux-opt/bin/AQ/src/_objs/AQ/AQ/src/nueral_net.o: In function `void std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > >::_M_assign_aux<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> const*>(std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> const*, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> const*, std::forward_iterator_tag)':
nueral_net.cpp:(.text._ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag[_ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag]+0x90): undefined reference to `tensorflow::Tensor::CopyFromInternal(tensorflow::Tensor const&, tensorflow::TensorShape const&)'
nueral_net.cpp:(.text._ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag[_ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag]+0xb5): undefined reference to `tensorflow::Tensor::~Tensor()'
nueral_net.cpp:(.text._ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag[_ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag]+0x11f): undefined reference to `tensorflow::Tensor::CopyFromInternal(tensorflow::Tensor const&, tensorflow::TensorShape const&)'
nueral_net.cpp:(.text._ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag[_ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag]+0x226): undefined reference to `tensorflow::TensorShapeRep::SlowCopyFrom(tensorflow::TensorShapeRep const&)'
nueral_net.cpp:(.text._ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag[_ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag]+0x38d): undefined reference to `tensorflow::Tensor::~Tensor()'
nueral_net.cpp:(.text._ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag[_ZNSt6vectorISt4pairINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEN10tensorflow6TensorEESaIS9_EE13_M_assign_auxIPKS9_EEvT_SF_St20forward_iterator_tag]+0x400): undefined reference to `tensorflow::TensorShapeRep::SlowCopyFrom(tensorflow::TensorShapeRep const&)'
bazel-out/local_linux-opt/bin/AQ/src/_objs/AQ/AQ/src/nueral_net.o: In function `PolicyNet(tensorflow::Session*, std::vector<FeedTensor, std::allocator<FeedTensor> >&, std::vector<std::array<double, 441ul>, std::allocator<std::array<double, 441ul> > >&, double, int)':
nueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0xb6): undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(tensorflow::gtl::ArraySlice<long long>)'
nueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0xd1): undefined reference to `tensorflow::Tensor::Tensor(tensorflow::DataType, tensorflow::TensorShape const&)'
nueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0xe3): undefined reference to `tensorflow::TensorShapeRep::DestructorOutOfLine()'
nueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0xf7): undefined reference to `tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType) const'
nueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0x104): undefined reference to `tensorflow::TensorShape::CheckDimsEqual(int) const'
nueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0x111): undefined reference to `tensorflow::TensorShape::CheckDimsAtLeast(int) const'
nueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0x146): undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dim_size(int) const'
nueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0x1ee): undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase()'
nueral_net.cpp:(.text._Z9PolicyNetPN10tensorflow7SessionERSt6vectorI10FeedTensorSaIS3_EERS2_ISt5arrayIdLm441EESaIS8_EEdi+0x202): undefined reference to `tensorflow::Tensor::Tensor(tensorflow::DataType, tensorflow::TensorShape const&)'
```
```
functional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x8f): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
functional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0xa3): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
functional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0xb7): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
functional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0xc6): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*))'
functional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0xda): undefined reference to `tensorflow::OpDefBuilder::Doc(tensorflow::StringPiece)'
functional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0xe9): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'
functional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x114): undefined reference to `tensorflow::OpDefBuilder::Input(tensorflow::StringPiece)'
functional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x128): undefined reference to `tensorflow::OpDefBuilder::Input(tensorflow::StringPiece)'
functional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x13c): undefined reference to `tensorflow::OpDefBuilder::Output(tensorflow::StringPiece)'
functional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x150): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
functional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x164): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
functional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x178): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
functional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x17f): undefined reference to `tensorflow::shape_inference::UnknownShape(tensorflow::shape_inference::InferenceContext*)'
functional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x187): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*))'
functional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x19b): undefined reference to `tensorflow::OpDefBuilder::Doc(tensorflow::StringPiece)'
functional_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.17+0x1aa): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'
bazel-out/local_linux-opt/bin/tensorflow/core/libno_op_op_lib.lo(no_op.o): In function `__static_initialization_and_destruction_0(int, int) [clone .constprop.11]':
no_op.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.11+0x5c): undefined reference to `tensorflow::OpDefBuilder::OpDefBuilder(tensorflow::StringPiece)'
no_op.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.11+0x6b): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*))'
no_op.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.11+0x7f): undefined reference to `tensorflow::OpDefBuilder::Doc(tensorflow::StringPiece)'
no_op.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.11+0x8e): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'
no_op.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.11+0x1bd): undefined reference to `tensorflow::OpDef::~OpDef()'
bazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o): In function `__static_initialization_and_destruction_0(int, int) [clone .constprop.10]':
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x69): undefined reference to `tensorflow::OpDefBuilder::Input(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x7d): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x91): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0xa5): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0xb9): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0xcd): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
bazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o):sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0xe1): more undefined references to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)' follow
bazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o): In function `__static_initialization_and_destruction_0(int, int) [clone .constprop.10]':
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0xe9): undefined reference to `tensorflow::OpDefBuilder::SetIsStateful()'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0xf0): undefined reference to `tensorflow::shape_inference::UnknownShape(tensorflow::shape_inference::InferenceContext*)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0xfb): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*))'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x10f): undefined reference to `tensorflow::OpDefBuilder::Doc(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x11e): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x149): undefined reference to `tensorflow::OpDefBuilder::Output(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x15d): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x171): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x185): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x199): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x1ad): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
bazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o):sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x1c1): more undefined references to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)' follow
bazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o): In function `__static_initialization_and_destruction_0(int, int) [clone .constprop.10]':
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x1c9): undefined reference to `tensorflow::OpDefBuilder::SetIsStateful()'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x1d4): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*))'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x1e8): undefined reference to `tensorflow::OpDefBuilder::Doc(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x1f7): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x222): undefined reference to `tensorflow::OpDefBuilder::Input(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x236): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x24a): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x25e): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x272): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x286): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
bazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o):sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x29a): more undefined references to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)' follow
bazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o): In function `__static_initialization_and_destruction_0(int, int) [clone .constprop.10]':
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x2a2): undefined reference to `tensorflow::OpDefBuilder::SetIsStateful()'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x2ad): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*))'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x2c1): undefined reference to `tensorflow::OpDefBuilder::Doc(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x2d0): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x2fb): undefined reference to `tensorflow::OpDefBuilder::Output(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x30f): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x323): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x337): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x34b): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x35f): undefined reference to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)'
bazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o):sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x373): more undefined references to `tensorflow::OpDefBuilder::Attr(tensorflow::StringPiece)' follow
bazel-out/local_linux-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo(sendrecv_ops.o): In function `__static_initialization_and_destruction_0(int, int) [clone .constprop.10]':
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x37b): undefined reference to `tensorflow::OpDefBuilder::SetIsStateful()'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x386): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*))'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x39a): undefined reference to `tensorflow::OpDefBuilder::Doc(tensorflow::StringPiece)'
sendrecv_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.10+0x3a9): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'
bazel-out/local_linux-opt/bin/tensorflow/core/libcore_cpu_base.lo(algorithm.o): In function `tensorflow::DFS(tensorflow::Graph const&, std::function<void (tensorflow::Node*)> const&, std::function<void (tensorflow::Node*)> const&)':
algorithm.cc:(.text._ZN10tensorflow3DFSERKNS_5GraphERKSt8functionIFvPNS_4NodeEEES9_+0x223): undefined reference to `tensorflow::Node::out_nodes() const'
bazel-out/local_linux-opt/bin/tensorflow/core/libcore_cpu_base.lo(algorithm.o): In function `tensorflow::FixupSourceAndSinkEdges(tensorflow::Graph*)':
algorithm.cc:(.text._ZN10tensorflow23FixupSourceAndSinkEdgesEPNS_5GraphE+0x29): undefined reference to `tensorflow::Graph::kControlSlot'
algorithm.cc:(.text._ZN10tensorflow23FixupSourceAndSinkEdgesEPNS_5GraphE+0x102): undefined reference to `tensorflow::Graph::AddEdge(tensorflow::Node*, int, tensorflow::Node*, int)'
algorithm.cc:(.text._ZN10tensorflow23FixupSourceAndSinkEdgesEPNS_5GraphE+0x135): undefined reference to `tensorflow::Graph::kControlSlot'
algorithm.cc:(.text._ZN10tensorflow23FixupSourceAndSinkEdgesEPNS_5GraphE+0x146): undefined reference to `tensorflow::Graph::AddEdge(tensorflow::Node*, int, tensorflow::Node*, int)'
bazel-out/local_linux-opt/bin/tensorflow/core/libcore_cpu_base.lo(algorithm.o): In function `tensorflow::PruneForReverseReachability(tensorflow::Graph*, std::unordered_set<tensorflow::Node const*, std::hash<tensorflow::Node const*>, std::equal_to<tensorflow::Node const*>, std::allocator<tensorflow::Node const*> >)':
algorithm.cc:(.text._ZN10tensorflow27PruneForReverseReachabilityEPNS_5GraphESt13unordered_setIPKNS_4NodeESt4hashIS5_ESt8equal_toIS5_ESaIS5_EE+0x157): undefined reference to `tensorflow::Node::name[abi:cxx11]() const'
algorithm.cc:(.text._ZN10tensorflow27PruneForReverseReachabilityEPNS_5GraphESt13unordered_setIPKNS_4NodeESt4hashIS5_ESt8equal_toIS5_ESaIS5_EE+0x27b): undefined reference to `tensorflow::Node::in_nodes() const'
algorithm.cc:(.text._ZN10tensorflow27PruneForReverseReachabilityEPNS_5GraphESt13unordered_setIPKNS_4NodeESt4hashIS5_ESt8equal_toIS5_ESaIS5_EE+0x42d): undefined reference to `tensorflow::Node::name[abi:cxx11]() const'
algorithm.cc:(.text._ZN10tensorflow27PruneForReverseReachabilityEPNS_5GraphESt13unordered_setIPKNS_4NodeESt4hashIS5_ESt8equal_toIS5_ESaIS5_EE+0x440): undefined reference to `tensorflow::Node::name[abi:cxx11]() const'
algorithm.cc:(.text._ZN10tensorflow27PruneForReverseReachabilityEPNS_5GraphESt13unordered_setIPKNS_4NodeESt4hashIS5_ESt8equal_toIS5_ESaIS5_EE+0x6d5): undefined reference to `tensorflow::Graph::RemoveNode(tensorflow::Node*)'
bazel-out/local_linux-opt/bin/tensorflow/core/libcore_cpu_base.lo(tensor_id.o): In function `tensorflow::ParseTensorName(tensorflow::StringPiece)':
tensor_id.cc:(.text._ZN10tensorflow15ParseTensorNameENS_11StringPieceE+0x89): undefined reference to `tensorflow::Graph::kControlSlot'
bazel-out/local_linux-opt/bin/tensorflow/core/libcore_cpu_base.lo(validate.o): In function `tensorflow::graph::ValidateGraphDef(tensorflow::GraphDef const&, tensorflow::OpRegistryInterface const&)':
validate.cc:(.text._ZN10tensorflow5graph16ValidateGraphDefERKNS_8GraphDefERKNS_19OpRegistryInterfaceE+0x13): undefined reference to `tensorflow::_VersionDef_default_instance_'
validate.cc:(.text._ZN10tensorflow5graph16ValidateGraphDefERKNS_8GraphDefERKNS_19OpRegistryInterfaceE+0x7a): undefined reference to `tensorflow::OpRegistryInterface::LookUpOpDef(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::OpDef const**) const'
validate.cc:(.text._ZN10tensorflow5graph16ValidateGraphDefERKNS_8GraphDefERKNS_19OpRegistryInterfaceE+0x92): undefined reference to `tensorflow::ValidateNodeDef(tensorflow::NodeDef const&, tensorflow::OpDef const&)'
validate.cc:(.text._ZN10tensorflow5graph16ValidateGraphDefERKNS_8GraphDefERKNS_19OpRegistryInterfaceE+0xaa): undefined reference to `tensorflow::CheckOpDeprecation(tensorflow::OpDef const&, int)'
bazel-out/local_linux-opt/bin/tensorflow/core/util/tensor_bundle/libnaming.a(naming.o): In function `tensorflow::MetaFilename[abi:cxx11](tensorflow::StringPiece)':
naming.cc:(.text._ZN10tensorflow12MetaFilenameB5cxx11ENS_11StringPieceE+0x19): undefined reference to `tensorflow::strings::Printf[abi:cxx11](char const*, ...)'
bazel-out/local_linux-opt/bin/tensorflow/core/util/tensor_bundle/libnaming.a(naming.o): In function `tensorflow::DataFilename[abi:cxx11](tensorflow::StringPiece, int, int)':
naming.cc:(.text._ZN10tensorflow12DataFilenameB5cxx11ENS_11StringPieceEii+0x1f): undefined reference to `tensorflow::strings::Printf[abi:cxx11](char const*, ...)'
collect2: error: ld returned 1 exit status
Target //AQ/src:AQ failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 18.153s, Critical Path: 17.81s
FAILED: Build did NOT complete successfully
```

"
13265,CheckpointSaverHook does not check Graph of Saver,"The following code will run just fine, but will not save any Variables to checkpoints:
```python
import tensorflow as tf

graph = tf.Graph()
with graph.as_default():
    v = tf.get_variable(""test"", shape=(100, 100), dtype=tf.float32)

save = tf.train.CheckpointSaverHook(""test_dir"", 10)
with graph.as_default():
    tf.train.create_global_step()
    a = tf.constant(1)
    with tf.train.MonitoredSession(hooks=[save]) as sess:
        sess.run(a)
```

This is because the `CheckpointSaverHook` does not find an existing Saver, so it creates a new one in its constructor, which is executed with a different default Graph. 
"
13263,`params` is Not Used for Canned Estimators Implementations,"The canned estimators implementations do not seem to pass the `params` to their parent class `Estimator`. For example, `DNNClassifier` puts all the parameters, e.g. `hidden_units`, `optimizer`, etc. inside the `model_fn` and then pass the `model_fn` to the parent class [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/canned/dnn.py#L314). This makes it difficult for reporting or parameter tuning purposes. 

I saw [`params` is an exposed property for `Estimator`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L188), however, it's mostly empty for canned estimators since it's not used. Is this on purpose?

Thanks!

"
13262,Training Mini-batches of data (without labels) for unsupervised learning problem,"Hi Everyone,

Has anyone trained mini-batches of data for an unsupervised learning problem? 
The feed_dict uses the label and in an unsupervised setting. How do you overcome that? Could we use fake labels that never contribute to the loss function?

Basically, I want to iterate over my huge dataset and then optimize a custom loss function. However, I couldn't figure out how to retain my training parameters (weights) when using a new mini-batch from the data explicitly.

For example, the whole dataset is 6000 points and the mini-batch size is 600.
Currently, for every mini-batch I could only use new independent weight parameters because the weights are initialized based on the data points from this mini-batch. When we optimize the loss over the first mini-batch of 600 data points, we get some optimized weights. How does one use these weights to optimize the next mini-batch of 600 data points and so on. The problem is we cannot use a shared global variable.

Any help or pointers in this regard would be really appreciated. Thanks in advance!"
13260,"ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients,"," I am green in a tensorflow

I do not know where it is wrong

here is a warning

ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ["""", ""

here is my code

import tensorflow as tf
import numpy as np
import pandas as pd
import math
from sklearn import tree
from sklearn.cross_validation import train_test_split


COLUMNS = [""CLD_A"",""DDI"",""DFS_A"",""DFS_B"",""deg1"",""deg2"",""y""]

filename=""D:/outputs/train.csv""
file = pd.read_csv(filename, skipinitialspace=True,
                       skiprows=0, names=COLUMNS)


n_samples = file[['CLD_A','DDI','DFS_A','DFS_B','deg1','deg2']]
n_samples=n_samples.as_matrix()

n_features= file[['y']]
n_features=n_features.as_matrix()

train_X, test_X, train_y, test_y = train_test_split(n_samples, n_features, 
test_size = 0.8,random_state=0)


x=tf.placeholder(tf.int32,shape=[None, 6]) 
y=tf.placeholder(tf.int32,shape=[None, 1])


l1 = tf.layers.dense(x, 10, tf.nn.relu)          # hidden layer
output = tf.layers.dense(l1, 1) 


loss = tf.losses.mean_squared_error(y, output)
optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)
train_op = optimizer.minimize(loss)
with tf.Session() as sess:
  init=tf.global_variables_initializer()
  sess = tf.Session()
  sess.run(init)
for i in range(1000):
    total_loss = 0
    for i in range(len(train_X)):
        feed={x:train_X,y:train_y}
        sess.run(train_op,feed_dict={x:train_X,y:train_y})
        if i%100==0:
            print(sess.run(loss, feed ={x:train_X,y:train_y}))

"
13258,small mistake,"https://github.com/tensorflow/tensorflow/blob/b46340f40fe5e2ec9bfcd385b07cfb914055fb51/tensorflow/contrib/distributions/python/ops/mvn_full_covariance.py#L48

this should be only y, not ||y||^2"
13256,segfaults in Saver.restore with missing files,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: centos 7
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.0-rc1-2456-g7abd587 1.4.0-dev20170922
- **Python version**: 3.6

```python
import tensorflow as tf
import os

a = tf.get_variable('W', shape=[1,1,256,60], dtype=tf.float32)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    saver = tf.train.Saver()
    saver.save(sess, './model')

os.remove('model.data-00000-of-00001')

with tf.Session() as sess:
    saver = tf.train.Saver()
    saver.restore(sess, './model')
```
The above code segfaults in today's nightly build. I expect an exception."
13254,RNNParamsSavable breaks when there is more than one RNN,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 5.2
- **CUDA/cuDNN version**: 6.0
- **GPU model and memory**:  Tesla K80, 12gb
- **Exact command to reproduce**:

```
    sess = tf.Session()
    cell1 = cudnn_rnn_ops.CudnnGRU(1, 10, 10, input_mode=""linear_input"")
    sz = cell1.params_size()
    params1 = tf.get_variable(""rnn1/rnn"", sess.run(sz))
    save1 = cudnn_rnn_ops.RNNParamsSaveable(
        cell1, cell1.params_to_canonical, cell1.canonical_to_params,
        [params1], name=""rnn1/rnn"")
    tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, save1)

    params2 = tf.get_variable(""rnn2/rnn"", sess.run(sz))
    save2 = cudnn_rnn_ops.RNNParamsSaveable(
        cell1, cell1.params_to_canonical, cell1.canonical_to_params,
        [params2], name=""rnn2/rnn"")
    tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, save2)
    sess.run(tf.global_variables_initializer())

    saver = tf.train.Saver()
    saver.save(sess, ""save"")
```

### Describe the problem
An exception is thrown: 
""ValueError: At least two variables have the same name: rnn1/rnn""

This works if there is only one RNN. If I remove the name parameters the model will save, however only one of the two RNNs will have their weights/biases saved to the checkpoint as individual tensors, so it will break if I try to restore two `CudnnCompatibleGRUCell` RNNs from this checkpoint.

As far as I can tell there is some kind of name clobbering happening within tf.Saver, maybe because `RNNParamsSaveable` set `op` to none for the super class."
13249,Compilation error since today morning with cudnn 5.1.10,"------------------------

### System information
Memory : 16GB
Processor: Intel® Core™ i7-7700HQ CPU @ 2.80GHz × 8 
GPU: GeForce GTX 1060/PCIe/SSE2
OS Type- 64 bit

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.3
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.4
- **CUDA/cuDNN version**: CUDA 8.0 CUDNN 5.1.10
- **GPU model and memory**: GTX 1060 (6GB)
- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package


### Describe the problem
Since today morning, I've been getting this error in compiling TF with cudnn 5.1.10 where it seems to be looking for a v6 file even though I've configured it to use 5.1.10. I do not see the same error when building with cudnn6.0.  I compiled it last night with the same exact configuration and it was compiling fine. I've even wiped my system clean and reinstalled the whole environment to make sure there weren't any corrupted libraries. But the error still persists.

### Source code / logs
ERROR: /tensorflow/tensorflow/stream_executor/BUILD:52:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1).
tensorflow/stream_executor/cuda/cuda_dnn.cc: In member function 'cudnnStatus_t perftools::gputools::cuda::wrap::WrapperShim__cudnnSetRNNDescriptor_v6::operator()(perftools::gputools::cuda::CUDAExecutor*, Args ...)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:140:30: error: '::cudnnSetRNNDescriptor_v6' has not been declared
       cudnnStatus_t retval = ::__name(args...);                    \
                              ^
tensorflow/stream_executor/cuda/cuda_dnn.cc:235:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'
   __macro(cudnnSetRNNDescriptor_v6)                           \
   ^
tensorflow/stream_executor/cuda/cuda_dnn.cc:240:1: note: in expansion of macro 'CUDNN_DNN_ROUTINE_EACH_R5'
 CUDNN_DNN_ROUTINE_EACH_R5(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'int perftools::gputools::cuda::{anonymous}::CudnnDataTypeToByteSize(cudnnDataType_t)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:902:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In member function 'int perftools::gputools::cuda::CudnnRnnParamsDescriptor::GetRegionCountPerLayer() const':
tensorflow/stream_executor/cuda/cuda_dnn.cc:1255:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnRNNInputMode_t perftools::gputools::cuda::{anonymous}::ToCudnnRnnInputMode(perftools::gputools::dnn::RnnInputMode)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:865:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnDirectionMode_t perftools::gputools::cuda::{anonymous}::ToCudnnRnnDirectionMode(perftools::gputools::dnn::RnnDirectionMode)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:877:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnRNNMode_t perftools::gputools::cuda::{anonymous}::ToCudnnRnnMode(perftools::gputools::dnn::RnnMode)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:889:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnDataType_t perftools::gputools::cuda::{anonymous}::ToCudnnDataType(perftools::gputools::dnn::DataType, perftools::gputools::dnn::DataLayout)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:853:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnConvolutionFwdAlgo_t perftools::gputools::cuda::{anonymous}::ToConvForwardAlgo(perftools::gputools::dnn::AlgorithmDesc)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:297:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnConvolutionBwdDataAlgo_t perftools::gputools::cuda::{anonymous}::ToConvBackwardDataAlgo(perftools::gputools::dnn::AlgorithmDesc)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:320:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnConvolutionBwdFilterAlgo_t perftools::gputools::cuda::{anonymous}::ToConvBackwardFilterAlgo(perftools::gputools::dnn::AlgorithmDesc)':
tensorflow/stream_executor/cuda/cuda_dnn.cc:342:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^
tensorflow/stream_executor/cuda/cuda_dnn.cc: At global scope:
tensorflow/stream_executor/cuda/cuda_dnn.cc:560:13: warning: 'bool perftools::gputools::cuda::TensorOpMathEnabled()' defined but not used [-Wunused-function]
 static bool TensorOpMathEnabled() {
             ^
tensorflow/stream_executor/cuda/cuda_dnn.cc:129:26: warning: 'tensorflow::thread::ThreadPool* perftools::gputools::cuda::wrap::GetCudaThreadpool()' defined but not used [-Wunused-function]
 static port::ThreadPool* GetCudaThreadpool() {
                          ^
tensorflow/stream_executor/cuda/cuda_dnn.cc:2001:20: warning: 'perftools::gputools::dnn::AlgorithmDesc perftools::gputools::cuda::{anonymous}::GetCudnnConvolutionForwardAlgorithm(perftools::gputools::Stream*, perftools::gputools::cuda::CUDAExecutor*, void*, int, const perftools::gputools::dnn::AlgorithmConfig&, bool, const perftools::gputools::cuda::ScopedTensorDescriptor&, const perftools::gputools::cuda::ScopedFilterDescriptor&, const perftools::gputools::cuda::ScopedConvolutionDescriptor&, const perftools::gputools::cuda::ScopedTensorDescriptor&, perftools::gputools::ScratchAllocator*, perftools::gputools::DeviceMemory<unsigned char>*)' defined but not used [-Wunused-function]
 dnn::AlgorithmDesc GetCudnnConvolutionForwardAlgorithm(
                    ^
Target //tensorflow:libtensorflow_all.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 666.701s, Critical Path: 125.43s
"
13246,Documentation correction,"Hi, 

I think there might be a possible documentation error in that of `tf.stack`. It's said that the numpy equivalent is `np.asarray`. But `np.asarray([])` will take lists of arbitrary shapes and makes it to a `ndarray` whereas `tf.stack` requires all the dimensions of objects in the list to be same. So the equivalent ideally would be `np.stack`.

Cheers, 
Ramana"
13245,Links to Object_detection model no longer work,"Examples of broken links include

https://github.com/tensorflow/models/blob/master/object_detection/create_pascal_tf_record.py
https://github.com/tensorflow/models/blob/master/object_detection/g3doc/running_pets.md

Thanks!
"
13244,BUG:Memory leak in tf.string_split,"[profile_9734.0066.txt](https://github.com/tensorflow/tensorflow/files/1325612/profile_9734.0066.txt)
[profile_9734.0100.txt](https://github.com/tensorflow/tensorflow/files/1325613/profile_9734.0100.txt)
[profile_9734.0150.txt](https://github.com/tensorflow/tensorflow/files/1325611/profile_9734.0150.txt)


Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Centos Linux version 3.10.0-229.4.2.el7.x86_64 (gcc version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC) )
- **TensorFlow installed from (source or binary)**: pip install tensorflow
- **TensorFlow version (use command below)**: tensorflow==1.3.0
- **Python version**: Python 2.7.5
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A (CPU only)
- **GPU model and memory**: N/A
- **Exact command to reproduce**:  

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I am seeing noticeably large memory usage when I use `tf.string_split`  within the map function of a dataset API.  I have attached a sample code below. I tried to do a heap analysis and I see `std::basic_string::_Rep::_S_create` constantly growing in size and not freeing up its memory. 
If i remove the `tf.string_split` and just return the line as is, there is no memory held over. This issue is a blocker for us to scale up the tensorflow pipeline to large datasets. 
I have attached three output files of pprof over time . 
`6447.1  96.8%  96.8%   6447.1  96.8% std::basic_string::_Rep::_S_create`
`  9765.5  96.5%  96.5%   9765.5  96.5% std::basic_string::_Rep::_S_create`
` 14704.7  95.5%  95.5%  14704.7  95.5% std::basic_string::_Rep::_S_create`
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

    import tensorflow as tf

    def mapper(line):
        line = tf.identity(line)
        tokens = tf.string_split(line, delimiter='\t')
        return tokens.indices

    def run():
        filenames = ""sample.txt""
        # Cluster spec
        cluster = tf.train.ClusterSpec({
            ""worker"": [""localhost:2223""]
        })
        server = tf.train.Server(cluster, job_name=""worker"", task_index=0)

        dataset = tf.contrib.data.TextLineDataset(filenames)
        dataset = dataset.batch(1000)
        dataset = dataset.map(mapper, 8).repeat()
        iterator = dataset.make_one_shot_iterator()
        next_element = iterator.get_next()

        with tf.Session(target=server.target) as session:
            while True:
                try:
                    session.run(next_element)
                except tf.errors.OutOfRangeError:
                    break
    run()
#You can run this script by 
`LD_PRELOAD=/usr/lib64/libtcmalloc.so.4 HEAPPROFILE=/tmp/profile nohup python -u bug.py > output.log &`"
13243,Linking of rule '//tensorflow/contrib/factorization:gen_gen_clustering_ops_py_wrappers_cc' failed (missing -lcuda?),"Trying to build Tensorflow from a6f856b2f7920d4f74d7ca4e71967258423cc9f0 with CUDA and just started running into this issue:

```
____[7,182 / 7,674] Compiling tensorflow/core/graph/node_builder.cc
____[7,183 / 7,675] Compiling tensorflow/core/graph/costmodel.cc
____[7,245 / 7,715] Linking tensorflow/contrib/factorization/gen_gen_clustering_ops_py_wrappers_cc [for host]
____[7,246 / 7,715] Linking tensorflow/contrib/tensor_forest/hybrid/gen_training_ops_py_wrappers_cc [for host]
____[7,247 / 7,715] Linking tensorflow/cc/ops/lookup_ops_gen_cc [for host]
____[7,248 / 7,715] Linking tensorflow/contrib/tensor_forest/gen_gen_model_ops_py_py_wrappers_cc [for host]
____[7,249 / 7,715] Linking tensorflow/contrib/tensor_forest/gen_gen_tensor_forest_ops_py_wrappers_cc [for host]
____[7,250 / 7,715] Linking tensorflow/python/gen_set_ops_py_wrappers_cc [for host]
____[7,251 / 7,715] Linking tensorflow/python/gen_linalg_ops_py_wrappers_cc [for host]
ERROR: /build/tensorflow-git/src/tensorflow-cuda/tensorflow/contrib/factorization/BUILD:106:1: Linking of rule '//tensorflow/contrib/factorization:gen_gen_clustering_ops_py_wrappers_cc' failed (Exit 1).
/usr/bin/ld: warning: libcuda.so.1, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemFree_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemsetD32Async'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuEventCreate'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuStreamAddCallback'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuModuleLoadFatBinary'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuCtxEnablePeerAccess'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemGetInfo_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuLaunchKernel'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuStreamSynchronize'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuEventQuery'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuEventElapsedTime'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuDeviceCanAccessPeer'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuCtxSynchronize'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuDeviceGetAttribute'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuFuncGetAttribute'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemcpyDtoH_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuStreamQuery'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuDevicePrimaryCtxGetState'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuCtxSetCurrent'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuStreamWaitEvent'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuEventSynchronize'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuModuleUnload'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuDeviceGet'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemsetD32_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuCtxGetSharedMemConfig'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemFreeHost'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuFuncSetCacheConfig'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuStreamCreate'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemGetAddressRange_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuCtxGetDevice'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuDeviceGetProperties'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemcpyDtoHAsync_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuDeviceGetCount'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuModuleGetFunction'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemHostRegister_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuDevicePrimaryCtxRelease'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemcpyHtoDAsync_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemcpyDtoD_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuModuleLoadDataEx'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuDevicePrimaryCtxRetain'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemHostAlloc'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuInit'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuDriverGetVersion'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuDeviceGetPCIBusId'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuEventRecord'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuPointerGetAttribute'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuDeviceTotalMem_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemsetD8_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuDeviceComputeCapability'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemAlloc_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuDeviceGetName'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuDevicePrimaryCtxSetFlags'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemHostUnregister'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuModuleGetGlobal_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemcpyDtoDAsync_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuEventDestroy_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuOccupancyMaxActiveBlocksPerMultiprocessor'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuCtxGetCurrent'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuStreamDestroy_v2'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuCtxSetSharedMemConfig'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemsetD8Async'
bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sfactorization_Cgen_Ugen_Uclustering_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemcpyHtoD_v2'
collect2: error: ld returned 1 exit status
____Building complete.
____Elapsed time: 1247.002s, Critical Path: 405.23s
```

My previous successful build (I haven't tried revisions in between) was from abfc9deb7. My build environment hasn't changed significantly since then (no compiler, CUDA, or relevant library changes). I suspect it must be a recent change in the Tensorflow repo that's the cause.

The only difference I see between abfc9deb7 and a6f856b2f7920d4f74d7ca4e71967258423cc9f0  in the tensorflow/contrib/factorization path is this:

```diff
diff --git a/tensorflow/contrib/factorization/kernels/BUILD b/tensorflow/contrib/factorization/kernels/BUILD
index 9a6d3c6f5..44eab5601 100644
--- a/tensorflow/contrib/factorization/kernels/BUILD
+++ b/tensorflow/contrib/factorization/kernels/BUILD
@@ -6,6 +6,8 @@ exports_files([""LICENSE""])

 package(default_visibility = [""//tensorflow:__subpackages__""])

+load(""//tensorflow:tensorflow.bzl"", ""tf_cc_test"")
+
 cc_library(
     name = ""all_kernels"",
     deps = [
@@ -50,7 +52,7 @@ cc_library(
     alwayslink = 1,
 )

-cc_test(
+tf_cc_test(
     name = ""clustering_ops_test"",
     srcs = [""clustering_ops_test.cc""],
     deps = [
```

I'm not super familiar with how Bazel works, but this looks innocuous to me. I guess something else must be implicated?

Any idea what could cause it to not add `-lcuda` on the link line? Or perhaps why it didn't need to before but does now?

```
$ pacman -Q | grep -e gcc-mult -e ^cuda -e ^glibc -e ^cudnn -e ^gcc5 -e '^python ' -e bazel
bazel 0.5.4-1
cuda 8.0.61-3
cudnn6 6.0.21-2
gcc-multilib 7.2.1.20170910-1
gcc5 5.4.0-1
glibc 2.26-3
python 3.6.2-1
```"
13238,TensorFlow Debugger Colors Are Unreadable on Windows,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version 1.3.0**:
- **TensorFlow version (use command below)**: Python 3.5.2
- **Bazel version (if compiling from source)**: not applicable
- **CUDA/cuDNN version**: not applicable
- **GPU model and memory**: not applicable
- **Exact command to reproduce**:  https://www.tensorflow.org/versions/r1.1/programmers_guide/debugger

### Describe the problem
When I run the TensorFlow Debugger (tfdbg) on a Windows command prompt the colors on the tfdbg screen are practically unreadable on my monitor.   Screen capture in the comment below. The text color is indigo and the background color is black.

I have tried changing the colors on my command prompt but the curses package on tfdbg is changing both the foreground and the background colors on the blue text so that text remains unreadable.
 
Please change tfdbg so that default palette is readable on all monitors and so that colors can be modified.

### Source code / logs

Screen capture below.
"
13234,the version of tensorflow on TensorFlow-experiment in iOS,"i get tensorflow from cocoapods, and the func TF_Version() in ""c_api.h"" can not run.
i want to get the version of tensorflow in ""tensorflow-experiment"",how can i know?"
13233,Leaky ReLU as part of tf.nn,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Ν/Α
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: bazel release 0.5.4
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: GeForce 940MX, 4Gb
- **Exact command to reproduce**: N/A

### Describe the problem

A leaky relu function could be part of the standard tensorflow package, under tf.nn. or some other non-contrib module. It could be something like

```
tf.maximum(x, alpha * x)
```

Custom leaky relus, which is what people do now, may be largely non-efficient. See for example the discussion in https://stackoverflow.com/q/45307072/5615276 ."
13231,Makefile build symbol(s) not found for architecture x86_64,"I'm trying to build tensorflow on a Mac OSx Sierra to be able to use it in a C++ project but when i compile it it gives me this error:
[https://pastebin.com/6K8qBNUF](https://pastebin.com/6K8qBNUF)"
13230,pb:tensorflow-gpu with cuda 7.5 and cudnn 4 is faster then tensorflow-gpu cuda8 and cudnn 6,"I tested tensorflow 0.8 with cuda 7.5 and cudnn 4 (Maxwell Quadro K620), execution time=0.075s when i test the same code with cuda 8 and cudnn 5 , execution time=0.75 s
I tested the some code with tensorflow 1.3 , cuda 8 ,cudnn 6 ,jetpack 3.1 (jetson tx2) , execution time =0.6s when i try to get the timeline with chromium::/tracing i added tensorflow.python.client import timeline that use libcputi , execution time become =0.3s ,the code run on gpu , because when i run it in cpu execution time become 0.9 s.

the code uses face detection with cnn.
i don't understand why the code is faster when i use tensorflow with cuda 7.5 and cudnn4 then cuda 8 and cudnn 5 or 6 . any help please? "
13229,where is tensorflow slim pretrained models??,"It seems that the url of the pretrained model below has been deleted.
[https://github.com/tensorflow/models/tree/master/slim#Pretrained](https://github.com/tensorflow/models/tree/master/slim#Pretrained)

So, where are the tensorflow slim pretrained models now?"
13228,add image gradient op,"A [paper][1] I am reimplementing recently uses [image gradient loss][2]. Numpy offers [np.gradient][3] to achieve this task i.e. `np.gradient(image, axis=0), np.gradient(image, axis=1)` however tensorflow lacks this feature or at least documentation about how to use `tf.gradients` to get this done.

Therefore I propose to either send a PR where I add a gradient image op which uses fixed 2d convolution, i.e.:

```python
import numpy as np
import scipy as sp
import tensorflow as tf

from skimage import io
from skimage import color

from matplotlib import pyplot as plt

image = color.rgb2gray(io.imread('image.jpg'))
image_rs = image.reshape([1] + list(image.shape) + [1])

xgrad = np.gradient(image, axis=0)
ygrad = np.gradient(image, axis=1)

image_ph = tf.placeholder(tf.float32, image_rs.shape)

x_weight = tf.reshape(tf.constant([-1, 0, +1], tf.float32), [3, 1, 1, 1])
y_weight = tf.reshape(x_weight, [1, 3, 1, 1])

xgrad_ts = tf.nn.conv2d(image_ph, x_weight, [1, 1, 1, 1], 'SAME')
ygrad_ts = tf.nn.conv2d(image_ph, y_weight, [1, 1, 1, 1], 'SAME')

with tf.Session() as sess:
    xgrad2, ygrad2 = sess.run([xgrad_ts, ygrad_ts], feed_dict={image_ph: image_rs})

    print(xgrad2.shape)
    fig, axes = plt.subplots(2, 3)
    axes[0, 0].imshow(image)
    axes[0, 1].imshow(xgrad)
    axes[0, 2].imshow(ygrad)
    axes[1, 0].imshow(image)
    axes[1, 1].imshow(xgrad2[0,:,:,0])
    axes[1, 2].imshow(ygrad2[0,:,:,0])
    plt.show()

```

or have someone update documentation of `tf.gradients`.

For the first I could provide a PR if this is considered interesting for tensorflow and not ""too implementation specific"".

[1]: https://arxiv.org/abs/1612.05362
[2]: https://en.wikipedia.org/wiki/Image_gradient
[3]: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.gradient.html
"
13227,commit 5c7f9e3 breaks windows bazel build,"commit 5c7f9e316d8c7735308a217310350d416d7498cc in #13224 breaks windows bazel build

First, _rpath_linkopts() in tensorflow/tensorflow.bzl should not add rpath args to link.exe
Second,  the link command generated from bazel is incorrect, 

link.exe /nologo /OUT:bazel-out/host/bin/tensorflow/python/gen_set_ops_py_wrappers_cc.exe -Lbazel-out/host/bin/_solib_x64_windows/_U_S_Stensorflow_Spython_Cgen_Uset_Uops_Upy_Uwrappers_Ucc___Utensorflow **tensorflow_framework** /SUBSYSTEM:CONSOLE -pthread /MACHINE:X64 @bazel-out/host/bin/tensorflow/python/gen_set_ops_py_wrappers_cc.exe-2.params /DEFAULTLIB:msvcrt.lib

The addDynamicInputLinkOptions function in src/main/java/com/google/devtools/build/lib/rules/cpp/CppLinkActionBuilder.java doesn't support Windows."
13225,Is there any tool to see the tensorflow net's topology?,"As you known, we could use ""http://ethereon.github.io/netscope/quickstart.html"" to catch sight of the caffe network's  topology structure. Is there any one for tensorflow?"
13222,SVD in TensorFlow is slower than in numpy,"I am observing that on my machine SVD in tensorflow is running significantly slower than in numpy. I have GTX 1080 GPU, and expecting SVD to be at least as fast as when running the code using CPU (numpy).

**Environment Info**

Operating System

    lsb_release -a
    No LSB modules are available.
    Distributor ID:	Ubuntu
    Description:	Ubuntu 16.10
    Release:	16.10
    Codename:	yakkety

Installed version of CUDA and cuDNN:

    ls -l /usr/local/cuda-8.0/lib64/libcud*
    -rw-r--r-- 1 root      root    556000 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudadevrt.a
    lrwxrwxrwx 1 root      root        16 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
    lrwxrwxrwx 1 root      root        19 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61
    -rwxr-xr-x 1 root      root    415432 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
    -rw-r--r-- 1 root      root    775162 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudart_static.a
    lrwxrwxrwx 1 voldemaro users       13 Nov  6  2016 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5
    lrwxrwxrwx 1 voldemaro users       18 Nov  6  2016 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10
    -rwxr-xr-x 1 voldemaro users 84163560 Nov  6  2016 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10
    -rw-r--r-- 1 voldemaro users 70364814 Nov  6  2016 /usr/local/cuda-8.0/lib64/libcudnn_static.a

TensorFlow Setup

    python -c ""import tensorflow; print(tensorflow.__version__)""
    I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
    I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
    I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
    I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
    I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
    1.0.0

**Code:**

    '''
    Created on Sep 21, 2017
    
    @author: voldemaro
    '''
    import numpy as np
    import tensorflow as tf
    import time;
    import numpy.linalg as NLA;
    
    
    
    
    N=1534;
    
    svd_array = np.random.random_sample((N,N));
    svd_array = svd_array.astype(complex);
    
    specVar = tf.Variable(svd_array, dtype=tf.complex64);
    
    [D2, E1,  E2] = tf.svd(specVar);
    
    init_OP = tf.global_variables_initializer();
    
    with tf.Session() as sess:
        # Initialize all tensorflow variables
        start = time.time();
        sess.run(init_OP);
        print 'initializing variables: {} s'.format(time.time()-start);
        
        start_time = time.time();
        [d, e1, e2]  = sess.run([D2, E1,  E2]);
        print(""Tensorflow SVD ---: {} s"" . format(time.time() - start_time));
    
    
    # Equivalent numpy 
    start = time.time();
    
    u, s, v = NLA.svd(svd_array);   
    print 'numpy SVD  ---: {} s'.format(time.time() - start);


Code Trace:

    W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
    W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
    W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
    W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
    I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
    name: GeForce GTX 1080
    major: 6 minor: 1 memoryClockRate (GHz) 1.7335
    pciBusID 0000:01:00.0
    Total memory: 7.92GiB
    Free memory: 7.11GiB
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
    I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
    initializing variables: 0.230546951294 s
    Tensorflow SVD ---: 6.56117296219 s
    numpy SVD  ---: 4.41714000702 s

"
13221,Memory leak in zeros_like/Tile,"So I'm trying to figure out why my resnets are running out of memory, and it seems that there's a memory leak in Tile and zeros_like operations.

Those ops have memory allocated during each session run but there's no `__LOG_MEMORY__` deallocation messages corresponding to them. The sum of missing deallocations matches the amount of memory leaked as reported by allocator as `max_bytes_in_use` (accessed through `tf.contrib.memory_stats.MaxBytesInUse` op)

Here's a simplified repro, at each sess.run, the memory grows by 1.15 GB until it crashes with OOM
https://github.com/yaroslavvb/stuff/blob/master/resnet_leak_report2.py

When I run it, I see
```
Run 0, GBs in use 2.30
Run 1, GBs in use 3.60
Run 2, GBs in use 4.75
Run 3, GBs in use 5.90
2017-09-21 14:56:31.994302: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc) ran out of memory trying to allocate 137.33MiB.  Current allocation summary follows....
```

Offending ops:
```
gradients/leaky_relu_grad/zeros_like 576MB
gradients/Sum_grad/Tile  576MB
```
Version:
Ubuntu 16:04
official TensorFlow Linux GPU Python 3.5 nightly wheel from [today](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-linux/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tf_nightly_gpu-1.head-cp35-cp35m-linux_x86_64.whl)

```
version: 1.4.0-dev20170921
__git_version__: v1.3.0-rc1-2408-ge9d5ee1
Commit https://github.com/tensorflow/tensorflow/commit/e9d5ee1

```"
13220,CPU Build Fails on OSX Sierra ,"I'm getting a cpu-only Bazel build failure on OSX.  TensorFlow has already been .configure'd with default options.

Message below. 

```
$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures
WARNING: /Users/kevin/projects/tensorflow/tensorflow/core/BUILD:1653:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /Users/kevin/projects/tensorflow/tensorflow/tensorflow.bzl:913:30.
WARNING: /Users/kevin/projects/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /Users/kevin/projects/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Found 1 target...
ERROR: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/external/nsync/BUILD:397:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): cc_wrapper.sh failed: error executing command
  (cd /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/org_tensorflow && \
  exec env - \
    PATH=/anaconda/bin:/Users/kevin/bin:/usr/local/Cellar/coreutils/8.28/libexec/gnubin:/usr/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/anaconda/bin/python \
    PYTHON_LIB_PATH=/anaconda/lib/python3.5/site-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL=0 \
    TMPDIR=/var/folders/g9/fdv74qn92qs7yw31tlj4q0lw0000gn/T/ \
  external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' -MD -MF bazel-out/local-py3-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/time_internal.pic.d -fPIC -iquote external/nsync -iquote bazel-out/local-py3-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/local-py3-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/local-py3-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/nsync/internal/time_internal.c -o bazel-out/local-py3-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/time_internal.pic.o).
In file included from external/nsync/internal/time_internal.c:16:
In file included from ./external/nsync//platform/c++11/platform.h:29:
In file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/mutex:189:
In file included from /Library/Developer/CommandLineTools/usr/include/c++/v1/__mutex_base:17:
/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:156:1: error: unknown type name 'mach_port_t'
mach_port_t __libcpp_thread_get_port();
^
/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:300:1: error: unknown type name 'mach_port_t'
mach_port_t __libcpp_thread_get_port() {
^
/Library/Developer/CommandLineTools/usr/include/c++/v1/__threading_support:301:12: error: use of undeclared identifier 'pthread_mach_thread_np'
    return pthread_mach_thread_np(pthread_self());
           ^
3 errors generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 0.830s, Critical Path: 0.26s
```

**TensorFlow commit:** e9d5ee1ebffba25cef65f1f354b9e4ca9bcea10c

**Mac OSX Sierra 10.12.6 (16G29)**
**Command Line Tools:**
```
$ pkgutil --pkg-info=com.apple.pkg.CLTools_Executables
package-id: com.apple.pkg.CLTools_Executables
version: 9.0.0.0.1.1504363082
volume: /
location: /
install-time: 1505966986
groups: com.apple.FindSystemFiles.pkg-group
```

**Environment Capture Script:**
```
$ cat tf_env.txt

== cat /etc/issue ===============================================
Darwin MacBook-Pro.localdomain 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64
Mac OS X 10.12.6

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.0.0 (clang-900.0.37)
Target: x86_64-apple-darwin16.7.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== uname -a =====================================================
Darwin MacBook-Pro.localdomain 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.13.1)
numpydoc (0.7.0)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.6)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)
Traceback (most recent call last):
  File ""/Users/kevin/projects/tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 48, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
ImportError: No module named 'tensorflow.python.pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/Users/kevin/projects/tensorflow/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/Users/kevin/projects/tensorflow/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Users/kevin/projects/tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 59, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/Users/kevin/projects/tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 48, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
ImportError: No module named 'tensorflow.python.pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tools/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```

**Bazel Information:**
```
$ bazel info
bazel-bin: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/org_tensorflow/bazel-out/local-py3-opt/bin
bazel-genfiles: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/org_tensorflow/bazel-out/local-py3-opt/genfiles
bazel-testlogs: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/org_tensorflow/bazel-out/local-py3-opt/testlogs
character-encoding: file.encoding = ISO-8859-1, defaultCharset = ISO-8859-1
command_log: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/command.log
committed-heap-size: 1180MB
execution_root: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/org_tensorflow
gc-count: 15
gc-time: 1146ms
install_base: /var/tmp/_bazel_kevin/install/ed43083c802b447b0d9313e2450af83f
java-home: /Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home/jre
java-runtime: Java(TM) SE Runtime Environment (build 1.8.0_144-b01) by Oracle Corporation
java-vm: Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode) by Oracle Corporation
max-heap-size: 3817MB
message_log: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/message.log
output_base: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8
output_path: /private/var/tmp/_bazel_kevin/1cea9193b5038a270f5e322e515767d8/execroot/tensorflow/bazel-out
package_path: %workspace%
release: release 0.5.4-homebrew
server_pid: 37949
used-heap-size: 394MB
workspace: /Users/kevin/projects/tensorflow
```"
13216,[feature request] set some dimension in row_shape to the maximum in each batch in tf.contrib.data.Dataset.dense_to_sparse_batch,"The docstring says:

**row_shape:** A `tf.TensorShape` or `tf.int64` vector tensor-like
    object representing the equivalent dense shape of a row in the
    resulting `tf.SparseTensor`. Each element of this dataset must
    have the same rank as `row_shape`, and must have size less
    than or equal to `row_shape` in each dimension.

The row_shape must be the same for every batch, so if the elements are sequences it's necessary to know the maximum length beforehand. It would be nice if it was possible to set some dimension of row_shape to -1, meaning that this dimension will be the maximum for each batch (just like padded_batch)."
13215,how to compute out product with tf,how to compute out product with tf
13214,Tensorflow 1.3.0 not buildable because bazel fails to download protobuf,"> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 2,859,576 bytes
> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 3,046,594 bytes
> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 3,235,206 bytes
> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 3,422,448 bytes
> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 3,609,690 bytes
> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 3,796,932 bytes
> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 3,985,320 bytes
> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 4,172,562 bytes
> ____Downloading https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz via codeload.github.com: 4,359,804 bytes
> ERROR: /build/python-tensorflow-cuda-1.3.0/tensorflow/tools/pip_package/BUILD:100:1: no such package '@protobuf//': java.io.IOException: Error downloading [https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz, http://mirror.bazel.build/github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz] to /build/python-tensorflow-cuda-1.3.0/.cache/bazel/_bazel_pbuilder/f9c4bbbece8e6d872cda536e5e92a13c/external/protobuf/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz: Checksum was e5fdeee6b28cf6c38d61243adff06628baa434a22b5ebb7432d2a7fbabbdb13d but wanted 6d43b9d223ce09e5d4ce8b0060cb8a7513577a35a64c7e3dad10f0703bf3ad93 and referenced by '//tensorflow/tools/pip_package:licenses'.
> ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.
> ____Elapsed time: 12.129s

It's not a local issue, reproduced several time with different Internet connection....

Thanks"
13213,"GRPC causes training to pause in individual worker (distributed tensorflow, synchronised)","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 8.9 (jessie)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1
- **Python version**: 3.6.2
- **CUDA/cuDNN version**: cuda-8.0 / cudnn-5.1.5
- **GPU model and memory**: GeForce GTX Titan X, 12 GB
- **Exact command to reproduce**:

### Describe the problem

The distributed synchronized ( between graph replication, 4 workers, 3 ps ) training works fine until one of the ps tasks reports following error. After that, one of the worker processes just stops, and the rest of the workers may also stop later with same error. 

   ```
 2017-09-21 16:45:55.606842: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2000, 1 -> localhost:2001, 2 -> localhost:2002}
    2017-09-21 16:45:55.606877: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2003, 1 -> localhost:2004, 2 -> localhost:2005, 3 -> localhost:2006}
    2017-09-21 16:45:55.608066: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2002
    E0921 16:48:52.596846076    3037 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=12325, new grpc_chttp2_stream id=12317
    2017-09-21 16:48:57.497244: W tensorflow/core/framework/op_kernel.cc:1158] Out of range: End of sequence
         [[Node: data_source_task_index_0/IteratorGetNext = IteratorGetNext[output_shapes=[[-1,-1], [-1,-1], [-1,-1], [-1,-1], [-1,-1]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64], _device=""/job:ps/replica:0/task:0/cpu:0""](data_source_task_index_0/Iterator)]]
         [[Node: data_source_task_index_0/cond/Merge_2_S341 = _Recv[client_terminated=false, recv_device=""/job:ps/replica:0/task:2/cpu:0"", send_device=""/job:ps/replica:0/task:0/cpu:0"", send_device_incarnation=-6450759800525444137, tensor_name=""edge_359_data_source_task_index_0/cond/Merge_2"", tensor_type=DT_INT64, _device=""/job:ps/replica:0/task:2/cpu:0""]()]]
    E0921 16:49:58.462749643    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24775, new grpc_chttp2_stream id=24769
    E0921 16:49:58.462780714    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24775, new grpc_chttp2_stream id=24773
    E0921 16:49:58.463260203    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24777
    E0921 16:49:58.463277333    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24779
    E0921 16:49:58.463283953    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24781
    E0921 16:49:58.463289625    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24783
    E0921 16:49:58.463295275    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24785
```


For more detail see the stackoverflow post: 
https://stackoverflow.com/questions/46322337/frozen-training-in-distributed-tensorflow  "
13211,transform_graph does not allow ~/ in path,"when using ~/ in path, transform_graph returns file not found"
13210,tensorboard failed  on win 10,"I tried to use tensorboard(win10+python3.5.2+tensorflow-gpu 1.1.0), but failed with the error:
![image](https://user-images.githubusercontent.com/13776012/30694040-dde10e00-9f04-11e7-9b1f-ab55cac9f4f9.png)
(module 'tensorflow' has no attribute 'make_tensor_proto')
but I tried on win 7 with the same events, and succeed.(win7+python3.5.2+tensorflow-gpu 1.0.1)
![image](https://user-images.githubusercontent.com/13776012/30694521-ec36d8fc-9f06-11e7-96ad-d720974838fb.png)
Is there a bug?

 "
13207,Failed to compile tensorflow offline with '--fetch=false' after all external dependencies fetched by bazel,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3.0 from master branch
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**: 0.5.4
- **CUDA/cuDNN version**: null
- **GPU model and memory**: null
- **Exact command to reproduce**: 
1. Fetch all external dependencies by docker image with internet access.
```
bazel fetch //tensorflow/tools/pip_package:build_pip_package
```
2. Complie tensorflow offline without internet access with `--fetch=false`.
```
bazel build --fetch=false --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package
```
### Describe the problem
I fetched all external dependencies successfully by docker image, and then committed and pushed it into our docker hub. 
```
#bazel fetch //tensorflow/tools/pip_package:build_pip_package
INFO: All external dependencies fetched successfully.
```
After that, I tried to compile tensorflow offline by using the image with `bazel build --fetch=false` since there is no internet access on my server, but it failed with the error ""no such package '@xxx'"", as follows: 
```
#bazel build --fetch=false --copt=-mavx2 --copt=-mfma --config=opt //tensorflow/tools/pip_package:build_pip_package
WARNING: /home/admin/src/tensorflow/tensorflow/workspace.bzl:323:3: External repository 'six_archive' is not up-to-date and fetching is disabled. To update, run the build without the '--nofetch' command line option.
WARNING: /home/admin/src/tensorflow/tensorflow/workspace.bzl:173:3: External repository 'eigen_archive' is not up-to-date and fetching is disabled. To update, run the build without the '--nofetch' command line option.
ERROR: /home/admin/src/tensorflow/third_party/eigen3/BUILD:20:1: no such package '@eigen_archive//': BUILD file not found on package path and referenced by '//third_party/eigen3:eigen3'.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.
INFO: Elapsed time: 0.349s
```
In fact, be sure that the package `eigen_archive` was existing in the container, as below:
```
#ls /root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a/external/ | grep eigen_archive
eigen_archive
```
And the `ps` stdout of bazel process was as follows:
```
#ps aux | grep bazel
root        664  0.8  0.8 20850492 570628 ?     Ssl  16:39   0:29 bazel(src) -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a -Xverify:none -Djava.util.logging.config.file=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a/javalog.properties -Djava.library.path=/root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f/_embedded_binaries/ -Dfile.encoding=ISO-8859-1 -jar /root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f/_embedded_binaries/A-server.jar --max_idle_secs=10800 --connect_timeout_secs=10 --install_base=/root/.cache/bazel/_bazel_root/install/2211725bdc2c34f807246fe9fb601a7f --install_md5=2211725bdc2c34f807246fe9fb601a7f --output_base=/root/.cache/bazel/_bazel_root/cb177c55a0e1ff115d6dbd74b4d4974a --workspace_directory=/home/admin/src/tensorflow --deep_execroot --experimental_oom_more_eagerly_threshold=100 --nofatal_event_bus_exceptions --client_debug=false --product_name=Bazel --option_sources=
```
So, please help for that and let me know if something wrong with my operation, thanks.
"
13206,Feature Request - Check parameter types at C++ compile time ,"e.g. Tensor oTensor( DT_INT64, TensorShape( { 4, 3 } ) );

The type of parameters **{4, 3}** should be checked at compile time whether is corresponding to the **DataType** of **Tensor(DataType type, const TensorShape& shape);**.

See #12501

"
13204,TF Record Reader/Writer C API,@asimshankar Would you be open to adding support for reading/writing TF records using the C API? Similar to the `lib/io` stuff in the Python API.
13203,"Brand new windows 10 install, TF doesn't run: ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'","I'm submitting this as a bug, instead of a support question on SO, because I'm getting this error after a very clean install of the operating system. The error occurs when I attempt to do the most basic validation, but just importing tensorflow!

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No custom code, the error is occurring when I do `import tensorflow as tf` at the repl 

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 Home, version 1703, fresh install (the only other thing installed on the computer is Chrome and Anaconda distribution of Python 3.6)

- **TensorFlow installed from (source or binary)**:
Binary. Followed the instruction to the letter, except type in 'pip' instead of 'pip3', as in:
`pip3 install --upgrade tensorflow-gpu` (this is after installing cuda and cudablas)

- **TensorFlow version (use command below)**:
1.3 (whatever is in the repo as of about 30 minutes ago)

- **Python version**:  Python 3.6.1 :: Anaconda custom (64-bit)

- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
>nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Mon_Jan__9_17:32:33_CST_2017
Cuda compilation tools, release 8.0, V8.0.60

- **GPU model and memory**:
NVIDIA GeForce GTX 960M
Approx. Total Memory: 18313 MB

- **Exact command to reproduce**:
import tensorflow as tf

### Describe the problem
I re-imaged my windows 10 laptop, installed Chrome, installed the latest anaconda python 3.6 and attempted to install tensorflow following the official instructions on the web page (all of this within the last hour or two of filing this ticket).

Installed Cuda and Cudablas from nvidia's website, as required by TF instructions.
Installed tensorflow-gpu

Saw an exception related to some setuptools file not being found (sorry, didn't keep a record of that)
Upgraded setuptools

Did a reinstall of tensorflow, but added `--force-reinstall` flag, which claimed to have installed tensorflow without errors:

Successfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.6.9 numpy-1.13.1 protobuf-3.4.0 setuptools-36.5.0 six-1.11.0 tensorflow-gpu-1.3.0 tensorflow-tensorboard-0.1.6 werkzeug-0.12.2 wheel-0.30.0

Ran the import command and saw these exception trace:


### Source code / logs
```
(C:\Users\someuser\Anaconda3) C:\Users\someuser>python
Python 3.6.1 |Anaconda custom (64-bit)| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\someuser\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\someuser\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\someuser\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\someuser\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\someuser\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\someuser\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\someuser\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\someuser\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\someuser\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\someuser\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\someuser\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\someuser\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\someuser\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\someuser\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\someuser\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```"
13202,tf.InteractiveSession leaks sessions,"The following works fine with tf.Session() but will fail to release resources in tf.InteractiveSession

```
sess = tf.InteractiveSession()
# do stuff  
sess.close()
del sess
```
The reason is that interactive session enters a context using `__enter()__` and never quits it, leaving a reference from a DefaultStack object. I found this when debugging why my notebook was hogging all GPU RAM.

The two work-arounds:
1. Force C_API to close the session using `sess.__del__()`
2. Get rid of the dangling reference

```
    sess._default_session.__exit__(None, None, None)
    del sess
    import gc
    gc.collect()
```

I think a better solution would be to have `sess.close()` call both `TF_CloseSession` and `TF_DeleteSession`, or have a method that will reset all sessions like `session_lib.Reset`"
13201,Multiple infiniband cards support in tensorflow with RDMA,"It seems that tensorflow with RDMA  doesn't support multiple infiniband cards now.
It only uses the first device in the infiniband device list.
[source rdma.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc)
```
ibv_context* open_default_device() {
  ibv_device** dev_list;
  ibv_device* ib_dev;
  dev_list = ibv_get_device_list(NULL);
  CHECK(dev_list) << ""No InfiniBand device found"";
  ib_dev = dev_list[0];
  CHECK(ib_dev) << ""No InfiniBand device found"";
  ibv_context* context = ibv_open_device(ib_dev);
  CHECK(context) << ""Open context failed for "" << ibv_get_device_name(ib_dev);
  return context;
}
```
This will fail to communicate when there are different type infiniband cards in one node, and the order of infiniband device list is different in every node.
Besides that, user can't specify one card to use.

Usually, we will specify the IP:PORT when doing the distributed training.
I think it's better to use the infiniband device which is corresponding to the IP in multiple infiniband cards environment.
```
bazel-bin/inception/imagenet_distributed_train \
--batch_size=32 \
--data_dir=/test/ILSVRC2012 \
--job_name='worker' \
--task_id=0 \
--ps_hosts='10.0.20.14:2276' \
--worker_hosts='10.0.20.15:2276,10.0.20.16:2276' \
--protocol='grpc+verbs'
```
What do you think about this?"
13200,speech_commands using python speech_recognition as input [working example],"using your sample code /tensorflow/examples/speech_commands/label_wav.py I tweeked to use speech_recognition  as input that dumps wave data to trained network...

feel free to add to examples for others to use..

many thanks 
calvin
(ubuntu 16.04, python 2.7)

<pre>
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import sys

import tensorflow as tf
import speech_recognition as sr
from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio

FLAGS = None


def load_graph(filename):
  with tf.gfile.FastGFile(filename, 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
    tf.import_graph_def(graph_def, name='')


def load_labels(filename):
  return [line.rstrip() for line in tf.gfile.GFile(filename)]


def run_graph(wav_data, labels, input_layer_name, output_layer_name,
              num_top_predictions):
  with tf.Session() as sess:
    softmax_tensor = sess.graph.get_tensor_by_name(output_layer_name)
    predictions, = sess.run(softmax_tensor, {input_layer_name: wav_data})
    top_k = predictions.argsort()[-num_top_predictions:][::-1]
    for node_id in top_k:
      human_string = labels[node_id]
      score = predictions[node_id]
      print('%s (score = %.5f)' % (human_string, score))

    return 0

def listen(r,source,labels_list):
    audio = r.listen(source)
	
    run_graph(audio.get_wav_data(convert_rate = 16000, convert_width = 2), labels_list, FLAGS.input_name,
            FLAGS.output_name, FLAGS.how_many_labels)

def main(_):
  print('start')
  labels_list = load_labels(FLAGS.labels)
  load_graph(FLAGS.graph)
  
  r = sr.Recognizer()
  with sr.Microphone() as source:
    print(""Say something!"")
    while 1:
        listen(r,source,labels_list)
        print(""------------------"")
  print('emd')
 

if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument(
      '--graph', type=str, default='', help='Model to use for identification.')
  parser.add_argument(
      '--labels', type=str, default='', help='Path to file containing labels.')
  parser.add_argument(
      '--input_name',
      type=str,
      default='wav_data:0',
      help='Name of WAVE data input node in model.')
  parser.add_argument(
      '--output_name',
      type=str,
      default='labels_softmax:0',
      help='Name of node outputting a prediction in the model.')
  parser.add_argument(
      '--how_many_labels',
      type=int,
      default=3,
      help='Number of results to show.')

  FLAGS, unparsed = parser.parse_known_args()
  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
</pre>"
13197,"""In-Graph Replication"" Multi-GPU training in local/single machine not working.","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Wrote a minimal version custom code using TensorFlow API's (see attachment)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Installed through pip 
- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: Cuda Version 8.0, cuDNN version 1.6
- **GPU model and memory**: GeForce GTX Titan X GPU 12 GB memory
- **Exact command to reproduce**: sh run_dist_tf_exp.sh (attached as zip file)
[distributed_tf_issue.zip](https://github.com/tensorflow/tensorflow/files/1319333/distributed_tf_issue.zip)


### Describe the problem
We are trying to get single machine multi-gpu training with Distributed Tensor Flow for increasing model throughput. Our set-up is as follows, We have a single compute machine running Ubuntu 16.04 with 8 GPU's and we would like to enable ""Data Parallelism"" by training model on multiple(4 GPU's) device present in the local machine to increase throughput. 

I will explain three scenarios below which uses minimal code that just runs a LinearClassifier (see attached code), 

Scenario 1:
tf_config set to run single worker and single parameter server config below,
```
tf_config = {
    ""cluster"": {
        'ps': ['127.0.0.1:9000'],
        'worker': ['127.0.0.1:9001']
    }
  }
```
Does not run, execution just freezes. Also tried the suggestion that came in logs to use `cloud` as environment in tf_config, still didn't work. Read similar issue written [here](https://github.com/tensorflow/tensorflow/issues/8796) commented out the line in [Experiment.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/experiment.py#L336) that checks if Environment is not LOCAL. By doing so it was successfully running training and worker exited after finished with training_steps. Should that line be commented for local distributed training runs?

Scenario 2:
tf_config set to run two worker and single parameter server config below,
```
tf_config = {
    ""cluster"": {
        'ps': ['127.0.0.1:9000'],
        'worker': ['127.0.0.1:9001', '127.0.0.1:9002']
    }
  }
```
Had commented out [Experiment.py] (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/experiment.py#L336) line as explained in Scenario 1, Did not work, the script was just hanging forever.

Scenario 3:
tf_config set to run one worker and a single parameter server config similar to Scenario 1.

Change done here was `learn_runner.run(..., schedule='continuous_train_and_eval)`, In continuous_train_and_eval mode the training just froze. Is continuous_train_and_eval not supported with distributed training? Seems to have some problem. Only change here compared to the working Scenario 1 setup with Experiment.py line commented out was changing the schedule to continuous_train_and eval.


### Source code / logs
Following modification was done to Experiment.py in run function, Had to comment out LOCAL environment check.
```
    if (#config.environment != run_config.Environment.LOCAL and
        config.environment != run_config.Environment.GOOGLE and
        config.cluster_spec and config.master):
      self._start_server()
```"
13196,ModuleNotFoundError: No module named '_pywrap_tensorflow_internal',"Hi - Thanks for all your hard work on this! - I've been having a problem getting Tensorflow-GPU to work on my Windows 10 notebook with a GTX 1080. I've tried to make sure all the paths are correct, etc. and have followed all the tips I can find.

I ran the tensorflow_self_check.py script and got the following result:

PS D:\Users\Frank Davidson\Documents\python> python .\tensorflow_self_check.py
ERROR: Failed to import the TensorFlow module.

- Python version is 3.6.

- TensorFlow is installed at: C:\Program Files\Python36\lib\site-packages\tensorflow

- All required DLLs appear to be present. Please open an issue on the
  TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues
PS D:\Users\Frank Davidson\Documents\python>

Here is the full stack trace when I try to import tensorflow:

PS C:\Users\Frank Davidson> python
Python 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_i
mport_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <modul
e>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_i
mport_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Program Files\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_i
mport_helper
    return importlib.import_module(mname)
  File ""C:\Program Files\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in <modul
e>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_i
mport_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Program Files\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Any help is greatly appreciated!

Frank"
13191,after the training process of DNNRegressor model i ran the log to check for the loss during training but the loss is not decreasing gradually and infact in my case its starting from 12654.4 and at final step=5131.29,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13190,TF_AddGradients gradients returns wrong result when multiple outputs specified,"### System information
Darwin Mac-Admin.local 15.6.0 Darwin Kernel Version 15.6.0: Thu Jun 23 18:25:34 PDT 2016; root:xnu-3248.60.10~1/RELEASE_X86_64 x86_64
Mac OS X 10.11.6

### Describe the problem
Hi. I've added a unit test for TF_AddGradients API (see code below) which is similar to
[this python test](https://github.com/tensorflow/tensorflow/blob/ca3bc0f1c2f917cf6e7c49d58f5ec604a9af9367/tensorflow/python/ops/gradients_test.py#L337)

In the test, I provide two outputs 
y[0]=x[0] ** 2 
y[1] = y[0] ** 8

where input x[0]=3.
According to the [documentation](https://github.com/tensorflow/tensorflow/blob/03619fab3f4dd6f28b67418455a953b0fccdd9bf/tensorflow/c/c_api.h#L1018)  result should be calculated by formula d(y_1 + y_2 + ...)/dx_1 and be equal to 17502, but the API prints 6.

What am I missing? Thanks.

### Source code / logs

```
    TF_Status* s = TF_NewStatus();
    TF_Graph* graph = TF_NewGraph();

    const int ny = 2;
    const int nx = 1;
    TF_Output inputs[nx];
    TF_Output outputs[ny];
    TF_Output grad_outputs[nx];

    TF_Operation* ph0 = Placeholder(graph, s);

    TF_Operation* y0 = Square(graph, s, ph0, ""Square0"");
    TF_Operation* y1 = Square(graph, s, y0, ""Square1"");
    TF_Operation* y2 = Square(graph, s, y1, ""Square2"");
    inputs[0] = TF_Output{ph0, 0};
    outputs[0] = TF_Output{y0, 0};
    outputs[1] = TF_Output{y2, 0};

    TF_AddGradients(graph, outputs, ny, inputs, nx, nullptr, s, grad_outputs);
    EXPECT_EQ(TF_OK, TF_GetCode(s)) << TF_Message(s);

    std::unique_ptr<CSession> csession(new CSession(graph, s));

    std::vector<TF_Output> grad_outputs_vec;
    grad_outputs_vec.assign(grad_outputs, grad_outputs + 2);
    csession->SetInputs({{ph0, Int32Tensor(3)}});
    csession->SetOutputs(grad_outputs_vec);
    csession->Run(s);
    ASSERT_EQ(TF_OK, TF_GetCode(s)) << TF_Message(s);

    TF_Tensor* out0 = csession->output_tensor(0);
    int* data = static_cast<int*>(TF_TensorData(out0));
    ASSERT_EQ(17502, *data); 

Failure
      Expected: 17502
To be equal to: *data
      Which is: 6
```

"
13188,third_party/zlib: use -DZ_HAVE_UNISTD_H instead of suppressing warnings,"Hi,

I noticed that you built zlib by suppressing warnings about it using undeclared functions. However, I discovered that adding `copts = [""-DZ_HAVE_UNISTD_H""]` could make zlib to include unistd.h and therefore get rid of the warnings completely.

While this change is very minor, I think declaring the macro is better than suppressing the warnings."
13187,parse_example is awfully slow,"@skearnes
you have indicated in this post of yours https://github.com/tensorflow/tensorflow/issues/390 way back in 2015 that parse_example is about 30 times faster than parse_single_example.
I have tried different options to modify my simple training script which only prints about 100000 tfrecords batched in 1000 and just does a print of feature and label after session.run(feature, label). Feature is a sparseTensor BTW.
Can you please put a test sample which proved that parse_example was that fast. parse_single_example was taking ~320 secs, now parse_example takes ~240 secs.

@Admin, please do not close this issue and refer to stackoverflow, as I don't think this is something to do with API usage or wrong parameters.
This is to do with the performance of queues (enqueue, dequeue) & threads"
13186,Renaming checkpoint directory,"If you want to rename a checkpoint directory, you currently need to also do a find-replace in the 'checkpoint' file. It might be nice to have that handled automatically.
"
13183,"""Controlling Dreams"" in TensorFlow","Hi guys,

I am trying to re-implement ""controlling dreams"" using TensorFlow which was implemented in the original google deepdreams algorithm using Caffe (see the last part of https://github.com/google/deepdream/blob/master/dream.ipynb). But I think I got problems. This ""controlling dreams"" part does not exist in this tutorial. I am looking for someone, who knows Caffe and deepdream, to help.

I have never used Caffe before, but I tried to understand what they were doing in the script. To ""control the dream"", they firstly input a small guide image (roughly 240 x 240) into the neural network, and extract the features in each channel in certain layer specified. These features are called ""guide features"". Then they input the large image, which they want to modify, into the model. The large image was ""octaved"". For each octave image, they input the octave image into the neural network, extract the features in each channel in the same layer specified to the guide image. Then they have to find the best matching guide features of certain channel to each of the features in the layer of octave image. Here ""the best matching"" means the dot product of the two feature vectors is the largest.

Here comes the question. They extract the guide features beforehand. Therefore the matrix shape of ""guide features"" is constant. But the shape of the feature matrix in octave can vary. This means that I may not be able to calculate the dot product of these two matrices. I believe I did not understand correctly their algorithm. I would like to have someone to explain to me or provide me a reference. Thank you.

Best,

Lei


"
13182,No OpKernel was registered to support Op 'Transpose' with these attrs,"### System information
- **Windows 10**:
- **TensorFlow installed from source**:
- **TensorFlow version r1.3**:
- **Python version 3.5.3**: 
- **Bazel version n/a**:
- **CUDA/cuDNN version n/a**:
- **GPU model and memory n/a**:
- **Exact command to reproduce n/a**:

### Describe the problem
The 'Transpose"" Op is not supported sufficiently in C++ environment. I have got this below. Does transpose operation ever works in a pure C++ project? 

### Source code / logs
No OpKernel was registered to support Op 'Transpose' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; T in [DT_INT64]; Tperm in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tperm in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tperm in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tperm in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tperm in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tperm in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tperm in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tperm in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tperm in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tperm in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tperm in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tperm in [DT_INT32]
  device='CPU'; T in [DT_STRING]; Tperm in [DT_INT32]
  device='CPU'; T in [DT_RESOURCE]; Tperm in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tperm in [DT_INT32]
"
13181,E tensorflow/core/platform/cloud/http_request.cc:514,"I run the train.py  in the object detection api, use the code in the terminal 
python3 train.py --logtostderr --train_dir=training --pipeline_conf
ig_path=training/ssd_mobilenet_v1_pets.config 

encounter with the problems like that,
Please use tf.global_variables instead.
2017-09-20 21:10:31.065044: E tensorflow/core/platform/cloud/http_request.cc:514] The transmission has been stuck at 0 bytes for 61 seconds and will be aborted.
2017-09-20 21:10:31.065307: I tensorflow/core/platform/cloud/retrying_utils.cc:77] The operation failed and will be automatically retried in 0.980452 seconds (attempt 1 out of 10), caused by: Unavailable: Error executing an HTTP request (HTTP response code 0, error code 42, error message 'Callback aborted')
2017-09-20 21:11:33.330807: E tensorflow/core/platform/cloud/http_request.cc:514] The transmission has been stuck at 0 bytes for 61 seconds and will be aborted.
2017-09-20 21:11:33.331297: I tensorflow/core/platform/cloud/retrying_utils.cc:77] The operation failed and will be automatically retried in 1.92775 seconds (attempt 2 out of 10), caused by: Unavailable: Error executing an HTTP request (HTTP response code 0, error code 42, error message 'Callback aborted')

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **Ubuntu 16.04)**:
- **TensorFlow install from pip**:
- **TensorFlow 1.3.0**:
- **Python 3.5**: 
- **Only CPU**:
-: i also install python2 and anaconda 2.7 in my linux




### Source code / logs
python3 train.py --logtostderr --train_dir=training --pipeline_config_path=training/ssd_mobilenet_v1_pets.config 
"
13180,how to avoid the flag --whole-archive when using the static library in windows,"Hi everyone,

I have built the static library of tensorflow for C++ in windows. And I am trying to use this library in qtCreator, however I have the issue (no session factory registered for the given options). But, it looks adding these flags in qtCreator doesn´t work so I still have the issue. Does someone know how to build the library in Tensorflow  so I can use the library without the flag -whole-archive?
"
13179,Function categorical_column_with_identity with big number as num_buckets parameter causes training hang and crash,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.6
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: not use GPU
- **GPU model and memory**: no GPU
- **Exact command to reproduce**: 

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

When I try to use use_id or item_id as an feature and process it through function ```categorical_column_with_identity```, because of the id category will be very big out of upper limit of int32, the training job will hang a few minutes then crash.  The crash without any useful information to reminder that this problem was caused by the ```num_buckets=${a_big_number}```.

Maybe this kinds of feature like uid or item_id should not be used like this, and I believe if there are some diagnostics information will be better.

And any suggestions about how to process this kinds of feature like uid or item_id, these features are very big number. I think this problem appeared in many cases.

Thanks

### Source code / logs
```python
# coding: utf-8
import tensorflow as tf
from tensorflow import feature_column as fc

f = fc.embedding_column(
    fc.categorical_column_with_identity(key='vid', num_buckets=1500000000),
    dimension=10)
e = tf.estimator.DNNClassifier(
    feature_columns=[f],
    hidden_units=[10])


def input_fn():
    return (
        {""vid"": tf.identity(tf.constant(
            [1000, 1000, 100, 1000, 10000, 1000, 1000, 1000, 100],
            name=""vid""), name=""vid_trainning"")},
        tf.constant([1, 0, 1, 0, 0, 0, 0, 0, 1])
    )

e.train(input_fn, steps=10)
```
"
13177,cmake: No session factory registered for the given session options ,"Hi everyone,

I have built the static library for tensorflow with cmake in Windows and I have the error ""No session factory registered for the given session options "". This issue is fixed with the flag /Wholearchive, however it seems there is not equivalent flag for Qt creator. So, my question is if someone knows if the static library built with bazel has the same issue (add the flag /wholearchive) in windows? I have read that someone people didin´t have this issue with bazel in Linux, but it would be great if someone can confirm it for Windows.

Thanks for your help."
13176,Raspberry Pi Makefile issues with proto_text,"### System information
- **Raspberry Pi on Ubuntu Mate 16.04 (Also tried Raspbian Stretch but GUI would freeze a lot and still gave similar errors)**:

### Describe the problem
Building tensorflow from source using makefile using the code provided at: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile. I've included two log files in which i ran the initial setup and ran into a zlib.h error which i resolved by running 'sudo apt-get install libz-dev' but then re-run the last make-f line which gave me another error.

### Error:

```
/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/env.o: In function `tensorflow::mutex::mutex()':
env.cc:(.text._ZN10tensorflow5mutexC2Ev[_ZN10tensorflow5mutexC5Ev]+0xc): undefined reference to `nsync::nsync_mu_init(nsync::nsync_mu_s_*)'
/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/env.o: In function `tensorflow::mutex::lock()':
env.cc:(.text._ZN10tensorflow5mutex4lockEv[_ZN10tensorflow5mutex4lockEv]+0xc): undefined reference to `nsync::nsync_mu_lock(nsync::nsync_mu_s_*)'
/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/env.o: In function `tensorflow::mutex::unlock()':
env.cc:(.text._ZN10tensorflow5mutex6unlockEv[_ZN10tensorflow5mutex6unlockEv]+0xc): undefined reference to `nsync::nsync_mu_unlock(nsync::nsync_mu_s_*)'
collect2: error: ld returned 1 exit status
tensorflow/contrib/makefile/Makefile:632: recipe for target '/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text' failed
make: *** [/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1
```

### Logs
Output file 1 (initial run, stopped at zlib error):  https://pastebin.com/dmqWYAs6
Output file 2 (current error after fixing zlib error):  https://pastebin.com/aE51br80

#### What I've tried
Looked at similar issues but they were related to iOS and didn't make sense to me. 
"
13175,BUG: same feature column creates duplicate tensors for DNNLinearCombinedRegressor,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.11.6
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem

It seems that the same feature column creates two tensors for `DNNLinearCombinedRegressor` (see `voc_embed` in graph below). Is this the right behavior we expected?

The behavior stems from that feature column is processed by DNN and Linear independently, see code: [dnn](https://github.com/tensorflow/tensorflow/blob/b46340f40fe5e2ec9bfcd385b07cfb914055fb51/tensorflow/python/estimator/canned/dnn_linear_combined.py#L154) and [linear](https://github.com/tensorflow/tensorflow/blob/b46340f40fe5e2ec9bfcd385b07cfb914055fb51/tensorflow/python/estimator/canned/dnn_linear_combined.py#L196)

### Source code / logs

```python
# coding: utf-8
import tensorflow as tf
from tensorflow import feature_column as fc

f = fc.embedding_column(
        fc.categorical_column_with_vocabulary_list(""voc"", [""a"", ""b""]),
        dimension=8)
e = tf.estimator.DNNLinearCombinedRegressor(
        model_dir=""/tmp/tf/test_dnn"",
        linear_feature_columns=[f],
        dnn_feature_columns=[f],
        dnn_hidden_units=[10])

def input_fn():
    return ({""voc"": tf.identity(
                        tf.constant([""a"", ""b"", ""a""], name=""voc_input""),
                        name=""identity"")},
            tf.constant([1, 0, 1]))

e.train(input_fn, steps=10)
```

![graph-large_attrs_key _too_large_attrs limit_attr_size 1024 run 1](https://user-images.githubusercontent.com/1112263/30632078-63853988-9e19-11e7-94c8-c660c6ed55e2.png)
"
13174,variable assign incorrect,"**When the two assignment operations are the same:**
var = tf.Variable(1)
assign_1 = var.assign(tf.multiply(var, 2))
assign_2 = var.assign(tf.multiply(var, 2))

with tf.Session() as sess:
    sess.run(var.initializer)
    sess.run([assign_1, assign_2])
    print(sess.run(var))  # >>  2

**When the two assignment operations are not the same:**
var = tf.Variable(1)
assign_1 = var.assign(tf.multiply(var, 2))
assign_2 = var.assign(tf.multiply(var, 3))

with tf.Session() as sess:
    sess.run(var.initializer)
    sess.run([assign_1, assign_2])
    print(sess.run(var))  # >>  6"
13173,Optimizer loss function without desired paramater,"Hi all,
For Tensorflow, Is it possible to train  a neural network to optimize a loss function, e.g, minimize cost function, without a desired parameter? And what is optimizer in Tensorflow for the objective?
thank you
"
13172,send/recv operators bug in constructing the graph.,"Running the following snippet several times shows some weird operator in the graph.
```python
import tensorflow as tf

with tf.device(""/gpu:0""):
    x = tf.constant(1.0)

graph_def = tf.get_default_graph().as_graph_def()
options = tf.RunOptions(output_partition_graphs=True)
metadata = tf.RunMetadata()
config = tf.ConfigProto(device_count={""CPU"": 8},
                        inter_op_parallelism_threads=1,
                        intra_op_parallelism_threads=1)
sess = tf.Session(config=config)
sess.run(x, options=options, run_metadata=metadata)
for partition_graph_def in metadata.partition_graphs:
   print ""\n"" * 5
   print partition_graph_def
```
in the cpu part of graph, it shows
```text
node {
  name: ""Const/_1""
  op: ""_Recv""
  device: ""/job:localhost/replica:0/task:0/cpu:0""
  attr {
    key: ""client_terminated""
    value {
      b: false
    }
  }
  attr {
    key: ""recv_device""
    value {
      s: ""/job:localhost/replica:0/task:0/cpu:0""
    }
  }
  attr {
    key: ""send_device""
    value {
      s: ""/job:localhost/replica:0/task:0/gpu:0""
    }
  }
  attr {
    key: ""send_device_incarnation""
    value {
      i: 1
    }
  }
  attr {
    key: ""tensor_name""
    value {
      s: ""edge_5_Const""
    }
  }
  attr {
    key: ""tensor_type""
    value {
      type: DT_FLOAT
    }
  }
}
node {
  name: ""_send_Const_0""
  op: ""_Send""
  input: ""Const/_1""
  device: ""/job:localhost/replica:0/task:0/cpu:0""
  attr {
    key: ""T""
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: ""client_terminated""
    value {
      b: true
    }
  }
  attr {
    key: ""recv_device""
    value {
      s: ""/job:localhost/replica:0/task:0/cpu:0""
    }
  }
  attr {
    key: ""send_device""
    value {
      s: ""/job:localhost/replica:0/task:0/cpu:0""
    }
  }
  attr {
    key: ""send_device_incarnation""
    value {
      i: -3155862594799619836
    }
  }
  attr {
    key: ""tensor_name""
    value {
      s: ""Const:0""
    }
  }
}
versions {
  producer: 21
}
```
There is a weird operator in it. ` name: ""_send_Const_0""` just send tensor from cpu0 to cpu0. This device already has the right result of `Add` operator.
And I notice the `send_device_incarnation` is a random number, which generates by [New64](https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/core/common_runtime/device.cc#L43). 

The whole log of this snippet available in https://gist.github.com/dzhwinter/3c2257351774260382dd3e130aaa072b. Thanks."
13171,tf.reduce_*(mean/sum) runs very slow on GPU," 
I notice that tf.reduce_*(mean/sum) runs very slow on GPU in some cases, which can happen in the following simple examle:

x = tf.Variable(tf.ones([80, 80, 80, 80])) # 4-D tensor.
y = tf.reduce_sum(x, [0, 2, 3]) # Sum over all dims except the 2nd.

The execution time on GPU is very large and is approximate same as (or more than) the time on CPU, which probably means the GPU is not used at all. The same result can be obtained by choosing the other axes, except for the first and the last axes, in which case the execution on GPU is significantly faster than CPU.

Here is the [code](https://gist.github.com/vs-zhehangd/8a547094cfa0efc181b814bfb20b31ce) that reproduces the problem. You can run with `--keep_dim k` k=0,1,2,3 to select different axes.

I am using the following PC system:

* Kubuntu 16.04
* TensorFlow 1.2.1 ('v1.2.0-5-g435cdfc', '1.2.1')
* Python 2.7
* GeForce GTX 1080 Ti
* CUDA-8.0

The execution time on GPU, CPU, and NumPy is given as follows:

 exec time (s) | [1,2,3] | [0,2,3] | [0,1,3] | [0,1,2] |
|---------------|---------|---------|---------|---------|
| GPU           | 0.00918 | 0.40572 | 0.55388 | 0.01905 |
| CPU           | 0.05921 | 0.22461 | 0.56524 | 0.16172 |
| NumPy         | 0.24799 | 0.24847 | 0.24886 | 0.26601 |


Similar result was observed also on my laptop.

-------------------------------------------------------

I met this problem when I tried to implement Batch Normalization (initially when I ran tf.nn.moment(), then realized the key was tf.reduce_*). I wanted to collect means of different channels. Using NHWC data format the problem does not matter as C is the last dimension. However, slow execution occurs if I want to implement for NCHW format as C is the second dimension, this makes the execution time on Batch Normalization overwhelms all other ops such as convolutions. This is surely a nightmare for training and evaluation.

I hope it will be fixed if it is a bug. Or, if the reason is that the oprations are just not implemented for GPU by now, I wonder if there is a way to walk around."
13169,nightly-devel-gpu Docker broken with ImportError: libcuda.so.1: cannot open shared object file,"To reproduce:
sudo docker run -it --name t2 tensorflow/tensorflow:nightly-devel-gpu
python -c ""import tensorflow""

It's looking for libcuda.so.1, but I can't find that file in the image
Furthermore, LD_LIBRARY_PATH is pointing to /usr/local/nvidia/lib64 but there's no such folder. There's /usr/local/cuda/lib64, but no libcuda.so.1 there either (should it be loading libcudart.so.8.0 instead?)

cc @craigcitro "
13167,*IGNORE*,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13165,Doc nicety: use * to indicate when arguments must be keywords,"https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits shows the signature

    softmax_cross_entropy_with_logits(
        _sentinel=None,
        labels=None,
        logits=None,
        dim=-1,
        name=None
    )

In Python 3, the syntax for `_sentinel=None` is just `*`, and the signature would be

    softmax_cross_entropy_with_logits(
        *,
        labels,
        logits,
        dim=-1,
        name=None
    )

where I've additionally removed the false defaults from labels and logits.  This can't be done to the code which has to be 2.7 compatible, but it could be done to the documentation."
13164,BUG: No GPU kernel for tf.scatter_nd and tf.gather_nd with int32 or int64 tensors,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
binary (pip)
- **TensorFlow version (use command below)**:
v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 
Python 3.5.2
- **Bazel version (if compiling from source)**:
n/a
- **CUDA/cuDNN version**:
CUDA-8.0 / cuDNN-5.1
- **GPU model and memory**:
NVidia GeForce GTX TITAN with 5.93GiB
- **Exact command to reproduce**:
```
import tensorflow as tf
import numpy as np

val_num = 5
val_dim = 2

with tf.device(""/gpu:0""):
    indices = tf.reshape(tf.range(val_num, dtype=tf.int64), [-1, 1])
    updates = tf.constant(np.tile(np.expand_dims(np.arange(val_num, dtype=np.int64), 1), [1, val_dim]))

    res = tf.scatter_nd(indices, updates, [val_num, val_dim])
    #res = tf.gather_nd(updates, indices)

    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    print(sess.run(res))
```

### Describe the problem
**tf.scatter_nd** and **tf.gather_nd** do not support **int32** or **int64** tensors on GPU.

### Source code / logs
tf.scatter_nd:
```
2017-09-19 22:33:55.447883: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-19 22:33:55.447965: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-19 22:33:55.649706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-09-19 22:33:55.650199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: GeForce GTX TITAN
major: 3 minor: 5 memoryClockRate (GHz) 0.8755
pciBusID 0000:04:00.0
Total memory: 5.93GiB
Free memory: 5.63GiB
2017-09-19 22:33:55.650286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0
2017-09-19 22:33:55.650299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y
2017-09-19 22:33:55.650316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:04:00.0)
Traceback (most recent call last):
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1297, in _run_fn
    self._extend_graph()
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1358, in _extend_graph
    self._session, graph_def.SerializeToString(), status)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'ScatterNd': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: ScatterNd = ScatterNd[T=DT_INT64, Tindices=DT_INT64, _device=""/device:GPU:0""](Reshape, Const, ScatterNd/shape)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""bug.py"", line 15, in <module>
    sess.run(tf.global_variables_initializer())
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'ScatterNd': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: ScatterNd = ScatterNd[T=DT_INT64, Tindices=DT_INT64, _device=""/device:GPU:0""](Reshape, Const, ScatterNd/shape)]]

Caused by op 'ScatterNd', defined at:
  File ""bug.py"", line 11, in <module>
    res = tf.scatter_nd(indices, updates, [val_num, val_dim])
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 2961, in scatter_nd
    shape=shape, name=name)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'ScatterNd': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: ScatterNd = ScatterNd[T=DT_INT64, Tindices=DT_INT64, _device=""/device:GPU:0""](Reshape, Const, ScatterNd/shape)]]

```

tf.gather_nd:
```
2017-09-19 22:34:26.298344: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-19 22:34:26.298436: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-19 22:34:26.504483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-09-19 22:34:26.504948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: GeForce GTX TITAN
major: 3 minor: 5 memoryClockRate (GHz) 0.8755
pciBusID 0000:04:00.0
Total memory: 5.93GiB
Free memory: 5.61GiB
2017-09-19 22:34:26.505032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0
2017-09-19 22:34:26.505044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y
2017-09-19 22:34:26.505061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:04:00.0)
Traceback (most recent call last):
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1297, in _run_fn
    self._extend_graph()
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1358, in _extend_graph
    self._session, graph_def.SerializeToString(), status)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'GatherNd': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: GatherNd = GatherNd[Tindices=DT_INT64, Tparams=DT_INT64, _device=""/device:GPU:0""](Const, Reshape)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""bug.py"", line 15, in <module>
    sess.run(tf.global_variables_initializer())
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'GatherNd': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: GatherNd = GatherNd[Tindices=DT_INT64, Tparams=DT_INT64, _device=""/device:GPU:0""](Const, Reshape)]]

Caused by op 'GatherNd', defined at:
  File ""bug.py"", line 12, in <module>
    res = tf.gather_nd(updates, indices)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1338, in gather_nd
    name=name)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'GatherNd': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: GatherNd = GatherNd[Tindices=DT_INT64, Tparams=DT_INT64, _device=""/device:GPU:0""](Const, Reshape)]]

```"
13163,Resolved: No GPU kernel for tf.tile with int32 or int64 tensors,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**:
binary (pip)
- **TensorFlow version (use command below)**:
v1.3.0-rc2-20-g0787eee 1.3.0
- **Python version**: 
Python 3.5.2
- **Bazel version (if compiling from source)**:
n/a
- **CUDA/cuDNN version**:
CUDA-8.0 / cuDNN-5.1
- **GPU model and memory**:
NVidia GeForce GTX TITAN with 5.93GiB
- **Exact command to reproduce**:

```
import tensorflow as tf
import numpy as np

key_num = 5
key_dim = 2

with tf.device(""/gpu:0""):
    keys = tf.tile(tf.expand_dims(tf.range(key_num, dtype=tf.int32), 1), [1, key_dim])

    sess = tf.Session()
    sess.run(tf.global_variables_initializer())

    print(sess.run(keys))
```

### Describe the problem
tf.tile doesn't support **tf.int32** or **tf.int64** tensors on GPU.

### Source code / logs
```
2017-09-19 22:23:42.175225: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-19 22:23:42.175316: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-19 22:23:42.380410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-09-19 22:23:42.380904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: GeForce GTX TITAN
major: 3 minor: 5 memoryClockRate (GHz) 0.8755
pciBusID 0000:04:00.0
Total memory: 5.93GiB
Free memory: 5.59GiB
2017-09-19 22:23:42.380985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0
2017-09-19 22:23:42.380997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y
2017-09-19 22:23:42.381014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:04:00.0)
Traceback (most recent call last):
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
    return fn(*args)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1297, in _run_fn
    self._extend_graph()
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1358, in _extend_graph
    self._session, graph_def.SerializeToString(), status)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Tile': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: Tile = Tile[T=DT_INT32, Tmultiples=DT_INT32, _device=""/device:GPU:0""](ExpandDims, Tile/multiples)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""bug.py"", line 11, in <module>
    sess.run(tf.global_variables_initializer())
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    options, run_metadata)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Tile': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: Tile = Tile[T=DT_INT32, Tmultiples=DT_INT32, _device=""/device:GPU:0""](ExpandDims, Tile/multiples)]]

Caused by op 'Tile', defined at:
  File ""bug.py"", line 8, in <module>
    keys = tf.tile(tf.expand_dims(tf.range(key_num, dtype=tf.int32), 1), [1, key_dim])
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3847, in tile
    name=name)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/daniyar/anaconda2/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Tile': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
	 [[Node: Tile = Tile[T=DT_INT32, Tmultiples=DT_INT32, _device=""/device:GPU:0""](ExpandDims, Tile/multiples)]]
```"
13162,Fail to build android example,"Hi. I have the same problem - https://github.com/tensorflow/tensorflow/issues/11474

**System information**
bazel version:
Build label: 0.5.4-homebrew

java version: ""1.8.0_144""

android/sdk/platforms/android-26
android/sdk/system-images/android-26
android-ndk-r14b

**workspace config:**
android_sdk_repository(
    name = ""androidsdk"",
    api_level = 26,
    build_tools_version = ""26.0.1"",
    path = ""/Users/akrasnoperov/Library/Android/sdk"",
)

android_ndk_repository(
    name=""androidndk"",
    path=""/Users/akrasnoperov/Library/Android/android-ndk-r14b"",
    api_level=14)

**Describe the problem**
Error when I run ""bazel build -c opt //tensorflow/examples/android:tensorflow_demo""

**Source code / logs**
bazel build -c opt //tensorflow/examples/android:tensorflow_demo
ERROR: /private/var/tmp/_bazel_akrasnoperov/8a0891b65f3fddb9d9a6d9e66927aea5/external/androidsdk/BUILD.bazel:64:1: Traceback (most recent call last):
	File ""/private/var/tmp/_bazel_akrasnoperov/8a0891b65f3fddb9d9a6d9e66927aea5/external/androidsdk/BUILD.bazel"", line 64
		create_system_images_filegroups(system_image_dirs = [""system-ima...""])
	File ""/private/var/tmp/_bazel_akrasnoperov/8a0891b65f3fddb9d9a6d9e66927aea5/external/bazel_tools/tools/android/android_sdk_repository_template.bzl"", line 298, in create_system_images_filegroups
		int(apidir.split(""-"")[1])
invalid literal for int() with base 10: ""MNC"".
ERROR: /private/var/tmp/_bazel_akrasnoperov/8a0891b65f3fddb9d9a6d9e66927aea5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:build-tools/26.0.1/lib/dx.jar' contains an error and its package is in error and referenced by '@androidsdk//:dx_jar'.
ERROR: /private/var/tmp/_bazel_akrasnoperov/8a0891b65f3fddb9d9a6d9e66927aea5/external/androidsdk/BUILD.bazel:8:1: Target '@androidsdk//:dx_jar' contains an error and its package is in error and referenced by '@androidsdk//:dx_jar_import'.
ERROR: /Users/akrasnoperov/Developer/Repo/tensorflow/WORKSPACE:20:1: Target '@androidsdk//:dx_jar_import' contains an error and its package is in error and referenced by '//external:android/dx_jar_import'.
ERROR: Analysis of target '//tensorflow/examples/android:tensorflow_demo' failed; build aborted.
INFO: Elapsed time: 0,463s"
13161,tf.contrib.data.Dataset outputs have only partial shape,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.2
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0 / 6
- **GPU model and memory**: Titan X 12GB
- **Exact command to reproduce**: see below

### Describe the problem

Dataset outputs should have the first shape dimension specified when dataset.batch is specified. Instead, ""?"" is given.

### Source code / logs

```python
import tensorflow as tf

dataset = tf.contrib.data.Dataset.range(100).map(lambda x: [x,x])
dataset = dataset.batch(32)
iterator = dataset.make_initializable_iterator()
inputs = iterator.get_next()
print(inputs.shape)
```

Output: ```(?, 2)```

Expected: ```(32, 2)```"
13160,[feature request]recomputable operation annotation,"'Training Deep Nets with Sublinear Memory Cost' and 'Memory-Efficient Implementation of DenseNets' indicate that use drop intermediate feature map and recompute it if needed can save memory(while add computation burden),

so we need some mechanism to annotate some op's inputs  `recomputable` , and drop this input memory after op finish(set input's reference count to 0), when need this op again, recompute it.

```python
a = tf.get_varible(shape=[None,10])
b = tf.get_varible(shape=[None,10])
c = tf.get_varible(shape=[None,10])
d = a*b
d_recomputable = tf.recomputable(d)
e = d_recomputable+c
```
relate issue 
- https://github.com/tensorflow/tensorflow/issues/1934
- https://github.com/tensorflow/tensorflow/issues/12948

in short, normal reference add reference count, recomputable reference do not add reference count
"
13157,Error for slim dataset using fixed length reader,"I want to use tensorflow slim data provider. All examples I can find only are only reading tfrecord files. However, I want to read binary files directly by my own reader. I use fixed length reader to extract Cifar10 binary data. 

However, ""dataBytes = tf.decode_raw(data, tf.unit8)"" always produce error that ""AttributeError: module 'tensorflow' has no attribute 'unit8'"" when using in slim DatasetDataProvider. 

But no such error occurs when I use the reader directly without using DatasetDataProvider. Did I not use it correctly or does DatasetDataProvider only support tfrecord readers? Thank you.

My codes are as below:  	

	CIFAR_LABEL_BYTE = 1
	CIFAR_HEIGHT = 32
	CIFAR_WIDTH = 32
	CIFAR_DEPTH = 3
	CIFAR_RECORD_BYTE = CIFAR_HEIGHT * CIFAR_WIDTH * CIFAR_DEPTH + CIFAR_LABEL_BYTE

	CIFAR_CLASS_NUM = 10
	CIFAR_TRAINING_NUM = 50000
	CIFAR_TEST_NUM = 10000

	_ITEMS_TO_DESCRIPTIONS = {
		'image': 'A [32 x 32 x 3] color image.',
		'label': 'A single integer between 0 and 9',
	}

    class CifarBinaryDecoder(DataDecoder):
	  def decode(self, data, items):
		outputs = []
                print(data.shape)
		dataBytes = tf.decode_raw(data, tf.uint8)
		for item in items:
		  if item == 'label':
			currLabel = tf.cast(tf.strided_slice(dataBytes, [0], CIFAR_LABEL_BYTE), tf.int32)
			outputs.append(currLabel)
		  if item == 'image':
			imageData = tf.reshpae(tf.strided_slice(dataBytes, [CIFAR_LABEL_BYTE],[CIFAR_RECORD_BYTE]), [CIFAR_DEPTH, CIFAR_HEIGHT, CIFAR_WIDTH])
			imageData = tf.transpose(imageData, [1, 2, 0])
			outputs.append(imageData)

		return outputs

	 def list_items(self):
		return ['label', 'image']

	def get_cifar10_training_filenames(binary_data_dir):
	  file_pattern = ['test_batch.bin']
	  return file_pattern

	def get_cifar_training_dataset(binary_data_dir):
	  file_pattern = get_cifar10_training_filenames(binary_data_dir)
	  decoder = CifarBinaryDecoder()
	  return slim.dataset.Dataset(data_sources = file_pattern,
								  reader = tf.FixedLengthRecordReader,
								  decoder = decoder,
								  num_samples = CIFAR_TRAINING_NUM,
								  items_to_descriptions = _ITEMS_TO_DESCRIPTIONS,
								  num_classes = CIFAR_CLASS_NUM)

	def main(_):  
	  dataset = get_cifar_training_dataset(FLAGS.cifar_data_dir)
	  provider = slim.dataset_data_provider.DatasetDataProvider(
		  dataset,
		  num_readers = 1,
		  common_queue_capacity = 1000,
		  common_queue_min = 500,
		  reader_kwargs={'record_bytes' : CIFAR_RECORD_BYTE})

	 with tf.Session() as sess:
		init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
		sess.run(init_op)
		tf.train.start_queue_runners()

		for i in range(2):
		  [img, lab] = provider.get(['image', 'label'])
                  image, label = sess.run([img, lab])"
13156,tar: Unrecognized archive format tar: Error exit delayed from previous errors.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13155,error ,I get error when I make placeholder..
13154,BeamSearchDecoder should support an AttentionWrapper cell with alignment_history enabled,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Ubuntu 16.04
- **TensorFlow installed from**: binary
- **TensorFlow version**: 1.3.0
- **Python version**: 2.7
- **Exact command to reproduce**: see the code snippet below.

### Describe the problem

Currently, setting `tf.contrib.seq2seq.AttentionWrapper`'s `alignment_history` argument to `True` and using this cell in a `tf.contrib.seq2seq.BeamSearchDecoder` does not work for 2 reasons:

1. In this configuration, the `tf.contrib.seq2seq.AttentionWrapper.state_size` property is invalid as it does not have the same structure as `zero_state` (see the code below). The [decoder state initialization](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py#L193) is failing because of this.
2. `tf.contrib.seq2seq.BeamSearchDecoder` [raises an error](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py#L123) when the state contains a `TensorArray`, which is the type currently used to gather alignments.

I believe this configuration should be supported as it is a standard use case for sequence to sequence models.

To address both of these limitations, it seems this `alignment_history` could be a `Tensor` on which alignments are repeatedly concatenated. Would it work?

### Source code / logs

This code sample reproduces 1. which is the error directly visible when using this configuration.

```python
import tensorflow as tf

batch_size = 2
num_units = 10

memory = tf.placeholder(tf.float32, shape=(None, None, num_units))

attention_mechanism = tf.contrib.seq2seq.LuongAttention(
  num_units,
  memory)

cell = tf.contrib.seq2seq.AttentionWrapper(
  tf.contrib.rnn.LSTMCell(num_units),
  attention_mechanism,
  alignment_history=True) # Set this to False to make it work.

tf.contrib.framework.nest.assert_same_structure(
  cell.zero_state(batch_size, dtype=tf.float32),
  cell.state_size)
```

It exits with this error:

```text
Traceback (most recent call last):
  File ""<file>"", line 19, in <module>
    cell.state_size)
  File ""<dir>/local/lib/python2.7/site-packages/tensorflow/python/util/nest.py"", line 199, in assert_same_structure
    % (len_nest1, nest1, len_nest2, nest2))
ValueError: The two structures don't have the same number of elements.

First structure (6 elements): AttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'AttentionWrapperZeroState/checked_cell_state:0' shape=(2, 10) dtype=float32>, h=<tf.Tensor 'AttentionWrapperZeroState/checked_cell_state_1:0' shape=(2, 10) dtype=float32>), attention=<tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=(2, 10) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(2, ?) dtype=float32>, alignment_history=<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fbd6326ec50>)

Second structure (5 elements): AttentionWrapperState(cell_state=LSTMStateTuple(c=10, h=10), attention=10, time=TensorShape([]), alignments=<tf.Tensor 'LuongAttention/strided_slice_2:0' shape=() dtype=int32>, alignment_history=())
```"
13152,"GRPC causes training to pause in individual worker (distributed tensorflow, synchronised)","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 8.9 (jessie)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1
- **Python version**: 3.6.2
- **CUDA/cuDNN version**: cuda-8.0 / cudnn-5.1.5
- **GPU model and memory**: GeForce GTX Titan X, 12 GB
- **Exact command to reproduce**:

### Describe the problem

The distributed synchronized ( between graph replication, 4 workers, 3 ps ) training works fine until one of the ps tasks reports following error. After that, one of the worker processes just stops, and the rest of the workers may also stop later with same error. 

   ```
 2017-09-21 16:45:55.606842: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2000, 1 -> localhost:2001, 2 -> localhost:2002}
    2017-09-21 16:45:55.606877: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2003, 1 -> localhost:2004, 2 -> localhost:2005, 3 -> localhost:2006}
    2017-09-21 16:45:55.608066: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2002
    E0921 16:48:52.596846076    3037 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=12325, new grpc_chttp2_stream id=12317
    2017-09-21 16:48:57.497244: W tensorflow/core/framework/op_kernel.cc:1158] Out of range: End of sequence
         [[Node: data_source_task_index_0/IteratorGetNext = IteratorGetNext[output_shapes=[[-1,-1], [-1,-1], [-1,-1], [-1,-1], [-1,-1]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64], _device=""/job:ps/replica:0/task:0/cpu:0""](data_source_task_index_0/Iterator)]]
         [[Node: data_source_task_index_0/cond/Merge_2_S341 = _Recv[client_terminated=false, recv_device=""/job:ps/replica:0/task:2/cpu:0"", send_device=""/job:ps/replica:0/task:0/cpu:0"", send_device_incarnation=-6450759800525444137, tensor_name=""edge_359_data_source_task_index_0/cond/Merge_2"", tensor_type=DT_INT64, _device=""/job:ps/replica:0/task:2/cpu:0""]()]]
    E0921 16:49:58.462749643    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24775, new grpc_chttp2_stream id=24769
    E0921 16:49:58.462780714    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24775, new grpc_chttp2_stream id=24773
    E0921 16:49:58.463260203    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24777
    E0921 16:49:58.463277333    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24779
    E0921 16:49:58.463283953    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24781
    E0921 16:49:58.463289625    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24783
    E0921 16:49:58.463295275    3036 parsing.c:801]              ignoring out of order new grpc_chttp2_stream request on server; last grpc_chttp2_stream id=24793, new grpc_chttp2_stream id=24785
```


For more detail see the stackoverflow post: 
https://stackoverflow.com/questions/46322337/frozen-training-in-distributed-tensorflow  "
13150,Handling of * in pattern of tf.contrib.data.Dataset.list_files undocumented,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (see below)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.5.4
- **Bazel version (if compiling from source)**: -
- **CUDA/cuDNN version**: 8/6
- **GPU model and memory**: GTX 1080, 8GB
- **Exact command to reproduce**: run the script

### Describe the problem
It seems that by default `list_files`' behaviour when a pattern contains `*`s is to match files at any depth in the directory tree. This is in contrast, for example, with `glob`'s default behaviour.
I could not find any mention of how `*` is evaluated in [its documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/data/Dataset#list_files) or any examples of its usage in the [pogrammer's guide](https://www.tensorflow.org/programmers_guide/datasets).
Could the documentation be improved specifying how exactly `*`s are handled?

### Source code / logs
Sample dataset structure on filesystem:

```
DATASET_ROOT
    _should_be_ignored
        class3
            subclass31
                sample.txt
            subclass32
                sample.txt
        class4
            subclass41
                sample.txt
            subclass42
                sample.txt
    class1
        subclass11
            sample.txt
        subclass12
            sample.txt
    class2
        subclass21
            sample.txt
        subclass22
            sample.txt
```

Small script to test the behaviour:

```
import tensorflow as tf
import glob

ROOT = 'C:/Users/1/Desktop/test_dataset'
glob_files = glob.glob('{}/*/*/*.txt'.format(ROOT))

dataset = tf.contrib.data.Dataset.list_files('{}/*/*/*.txt'.format(ROOT))
it = dataset.make_one_shot_iterator()

files_found = []
with tf.Session() as sess:
  while True:
    try:
      files_found.append(sess.run(it.get_next()))
    except tf.errors.OutOfRangeError:
      break
```

Outputs:
```
glob_files
Out[16]: 
['C:/Users/1/Desktop/test_dataset\\class1\\subclass11\\sample.txt',
 'C:/Users/1/Desktop/test_dataset\\class1\\subclass12\\sample.txt',
 'C:/Users/1/Desktop/test_dataset\\class2\\subclass21\\sample.txt',
 'C:/Users/1/Desktop/test_dataset\\class2\\subclass22\\sample.txt']

files_found
Out[4]: 
[b'C:\\Users\\1\\Desktop\\test_dataset\\class1\\subclass11\\sample.txt',
 b'C:\\Users\\1\\Desktop\\test_dataset\\class1\\subclass12\\sample.txt',
 b'C:\\Users\\1\\Desktop\\test_dataset\\class2\\subclass21\\sample.txt',
 b'C:\\Users\\1\\Desktop\\test_dataset\\class2\\subclass22\\sample.txt',
 b'C:\\Users\\1\\Desktop\\test_dataset\\_should_be_ignored\\class3\\subclass31\\sample.txt',
 b'C:\\Users\\1\\Desktop\\test_dataset\\_should_be_ignored\\class3\\subclass32\\sample.txt',
 b'C:\\Users\\1\\Desktop\\test_dataset\\_should_be_ignored\\class4\\subclass41\\sample.txt',
 b'C:\\Users\\1\\Desktop\\test_dataset\\_should_be_ignored\\class4\\subclass42\\sample.txt']
```
"
13148,Add data dynamically to pre-exisiting contrib.data.dataset,"(As far as I'm aware this isn't possible, so I think this is a feature request)

It would be great to be able to add data to a pre-exisiting dataset object, which already has an iterator attached. For example, in reinforcement learning, one often collects data while exploring, and needs to add this data to the training dataset. I can't see how using the `concatenate()` function is sufficient, as this returns a new dataset object and would require a new iterator etc.
@mrry 

Thanks!"
13147,How do I compile Tensorflow source with Altera FPGA library (AOCLUtils & OPENCL),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform** : Linux Ubuntu 16.04
- **TensorFlow installed from source**
- **TensorFlow version** : 1.3
- **Python version**: v2.7.10
- **Bazel version**: 0.5.4
- **Exact command to reproduce**: set all as default value  ./configure , not use cuda


### Describe the problem
I am studying posting matmul_op calculation  into Altera FPGA Arria 10 via OPENCL and combine source and OpenCL/AOCLUtils library as python package.
However, I encountered much problem about this and always build (Bazel) failed when compile.
The following share some experience in every stages and some problem over the past month.

AOCLUtils library package
<pre><code>
/AOCLUtils/BUILD     --->for bazel
/AOCLUtils/aocl_utils.h
/AOCLUtils/opencl.h
/AOCLUtils/options.h
/AOCLUtils/opencl.cpp
/AOCLUtils/options.cpp
/AOCLUtils/scoped_ptrs.h
</code></pre>

OPENCL header library package in /home/mater/intelFPGA_pro/17.0/hld/host/include
compiled library (*.so) in /home/mater/intelFPGA_pro/17.0/hld/host/linux64/lib ;  /home/mater/intelFPGA_pro/17.0/hld/board/a10_ref/linux64/lib
<pre><code>
CL/cl_d3d10.h
CL/cl_ext.h
CL/cl_gl.h
CL/cl.hpp
CL/opencl.h
CL/cl_ext_altera.h
CL/cl_gl_ext.h
CL/cl.h
CL/cl_platform.h
</code></pre>
------------------------
1. Study third party library - SYCL  (Seems not support Altera FPGA)
Original, tensorflow supports OpenCL via SYCL.
However, it only seems support GPU & CPU, not Altera FPGA.
Besides, I gave up this method.
------------------------
2. Success build a New(external) Op with AOCLUtils/OPENCL via g++, but it's my purpose.
Refer : https://www.tensorflow.org/extend/adding_an_op
I study example for a new op and build as library with AOCLUtils/OPENCL.
The g++ command is as follow then built as zero_out_cl.so.
Tensorflow can load this external library and run some calculation in FPGA
(zero_out_module = tf.load_op_library('./zero_out_all.so'))

Although my purpose is tensorflow python installation with AOCLUtils/OPENCL library,
it makes sure that tensorflow can support FPGA library

<pre><code>
g++ -std=c++11 -shared main.cpp -o zero_out_cl.so -fPIC -I /home/mater/tensorflowCPU_1.3/lib/python2.7/site-packages/tensorflow/include -I/home/mater/intelFPGA/17.0/hld/host/include /home/mater/AI/FPGA/TF_OPENCL_ZeroOut/common/src/AOCLUtils/opencl.cpp /home/mater/AI/FPGA/TF_OPENCL_ZeroOut/common/src/AOCLUtils/options.cpp -L/home/mater/intelFPGA_pro/17.0/hld/board/a10_ref/linux64/lib -L/home/mater/intelFPGA_pro/17.0/hld/host/linux64/lib -Wl,--no-as-needed -lalteracl -laltera_a10_ref_mmd -lelf -I/home/mater/AI/FPGA/TF_OPENCL_ZeroOut/common/inc -O2 -D_GLIBCXX_USE_CXX11_ABI=0 
</code></pre>
------------------------
3. Modify Tensorflow source code with AOCLUtils/OPENCL and always bazel build failed..
*step 1. git clone into /home/mater/git/test/tensorflow_opencl (tensorflow root folder)
*step 2. copy AOCLUtils package into tensorflow root folder
*step 3. Add cc_library into WORKSPACE for external opencl library (not sure correct method)
<pre><code>
new_local_repository(
    name = ""opencl_headers"",
    path = ""/home/mater/intelFPGA_pro/17.0/hld/host/include"",
    build_file_content = """"""
cc_library(
	name = ""CL"",
	hdrs = glob([""CL/*.h""]),
	visibility = [""//visibility:public""],
	linkopts=[""-shared""],
)
"""""",
)

new_local_repository(
    name = ""opencl_libs"",
    path = ""/home/mater/intelFPGA_pro/17.0/hld/host/linux64/lib"",
    build_file_content = """"""
cc_library(
	name = ""libopencl"",
	srcs = glob([""*.so""]),
	visibility = [""//visibility:public""],
	linkopts=[""-shared""],
)
"""""",
)

new_local_repository(
    name = ""a10_lib"",
    path = ""/home/mater/intelFPGA_pro/17.0/hld/board/a10_ref/linux64/lib"",
    build_file_content = """"""
cc_library(
	name = ""liba10"",
	srcs = [""libaltera_a10_ref_mmd.so""],
	visibility = [""//visibility:public""],
	linkopts=[""-shared""],
)
"""""",
)
</code></pre>

*step 4. Add BUILD file into AOCLUtils folder
<pre><code>
cc_library(
	name=""aocutils"",
	srcs = glob([""*.cpp""]),
	hdrs = glob([""*.h""]),
	deps = [""@opencl_libs//:libopencl"", ""@opencl_headers//:CL"", ""@a10_lib//:liba10"",],
	visibility=[""//visibility:public""],
)
</code></pre>

*step 5. Add deps into tf_kernel_library in tensorflow root folder/tensorflow/core/kernels/BUILD
I would like to run matmul_op calculation in FPGA so try to include FPGA library
<pre><code>
tf_kernel_library(
    name = ""matmul_op"",
...
    deps = MATH_DEPS + [
...
    ]) + [""//AOCLUtils:aocutils"",],
</code></pre>

*I tried 3 methods to build tensorflow source code as python installation package, but always failed
------------------------
*step 6A.  Original bazel build command (Failed situation A)
Use original Bazel build command, but failed to find CL library.
exception :   fatal error: CL/cl.h: No such file or directory

command
<pre><code>
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""
</code></pre>
result
<pre><code>
ERROR: /home/mater/git/test/tensorflow_opencl/AOCLUtils/BUILD:1:1: C++ compilation of rule '//AOCLUtils:aocutils' failed (Exit 1): gcc failed: error executing command 
  (cd /home/mater/.cache/bazel/_bazel_mater/cf4207c477b73da1da7e3336942f640b/execroot/org_tensorflow && \
  exec env - \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL=0 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++0x' '-march=native' '-D_GLIBCXX_USE_CXX11_ABI=0' -MD -MF bazel-out/local-opt/bin/AOCLUtils/_objs/aocutils/AOCLUtils/options.pic.d '-frandom-seed=bazel-out/local-opt/bin/AOCLUtils/_objs/aocutils/AOCLUtils/options.pic.o' -fPIC -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/opencl_libs -iquote bazel-out/local-opt/genfiles/external/opencl_libs -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/opencl_headers -iquote bazel-out/local-opt/genfiles/external/opencl_headers -iquote external/a10_lib -iquote bazel-out/local-opt/genfiles/external/a10_lib -isystem external/bazel_tools/tools/cpp/gcc3 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c AOCLUtils/options.cpp -o bazel-out/local-opt/bin/AOCLUtils/_objs/aocutils/AOCLUtils/options.pic.o)
In file included from ./AOCLUtils/opencl.h:32:0,
                 from ./AOCLUtils/aocl_utils.h:27,
                 from AOCLUtils/options.cpp:22:
external/opencl_headers/CL/opencl.h:42:19: fatal error: CL/cl.h: No such file or directory
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 12.957s, Critical Path: 5.11s
FAILED: Build did NOT complete successfully
</code></pre>
------------------------
*step 6B.  Add link path in command (Failed situation B)
Use original Bazel build command with CL external path included, but failed to missing dependency declarations.
exception :   missing dependency declarations for the following files included by 'AOCLUtils/options.cpp'

I think this method should be ok and bazel can find CL library location, but I can't understand what happen for this.
I study some information about 'missing dependency declarations', and found 'CROSSTOOL' for GPU for this title. It didn't help me to solve it because I don't use GPU configuration.

command like building a New(external) Op method
<pre><code>
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --copt=""-I/home/mater/intelFPGA/17.0/hld/host/include"" --verbose_failures  --copt=""-L/home/mater/intelFPGA_pro/17.0/hld/board/a10_ref/linux64/lib"" --copt=""-L/home/mater/intelFPGA_pro/17.0/hld/host/linux64/lib"" --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""
</code></pre>
result
<pre><code>
ERROR: /home/mater/git/test/tensorflow_opencl/AOCLUtils/BUILD:1:1: undeclared inclusion(s) in rule '//AOCLUtils:aocutils':
this rule is missing dependency declarations for the following files included by 'AOCLUtils/options.cpp':
  '/home/mater/intelFPGA/17.0/hld/host/include/CL/cl.h'
  '/home/mater/intelFPGA/17.0/hld/host/include/CL/cl_platform.h'
  '/home/mater/intelFPGA/17.0/hld/host/include/CL/cl_gl.h'
  '/home/mater/intelFPGA/17.0/hld/host/include/CL/cl_gl_ext.h'
  '/home/mater/intelFPGA/17.0/hld/host/include/CL/cl_ext.h'
  '/home/mater/intelFPGA/17.0/hld/host/include/CL/cl_ext_altera.h'
In file included from /home/mater/intelFPGA/17.0/hld/host/include/CL/cl_ext.h:42:0,
                 from external/opencl_headers/CL/opencl.h:45,
                 from ./AOCLUtils/opencl.h:32,
                 from ./AOCLUtils/aocl_utils.h:27,
                 from AOCLUtils/options.cpp:22:
/home/mater/intelFPGA/17.0/hld/host/include/CL/cl_ext_altera.h:442:0: warning: ignoring #pragma warning  [-Wunknown-pragmas]
 #pragma warning( push )
 ^
/home/mater/intelFPGA/17.0/hld/host/include/CL/cl_ext_altera.h:443:0: warning: ignoring #pragma warning  [-Wunknown-pragmas]
 #pragma warning( disable:4201 )
 ^
/home/mater/intelFPGA/17.0/hld/host/include/CL/cl_ext_altera.h:459:0: warning: ignoring #pragma warning  [-Wunknown-pragmas]
 #pragma warning( pop )
 ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
</code></pre>
------------------------
*step 6C.  Build AOCLUtils as library,AOCUtils.so, at first, then build tensorflow with this library (Failed situation C)
Due to the below two situation, I tried to build AOCUtils package as library. Maybe it can skip some exception for CL library missing.
Although it passed c++ code compiled and continue to build about 10 minutes (below two situation show failed message immediately), it still be failed at packaging python due to not find AOCLUtils

First command for AOCUtils.so
<pre><code>
g++ -std=c++11 -shared AOCLUtils/opencl.cpp AOCLUtils/options.cpp -o AOCLUtils/AOCUtils.so -fPIC -I /home/mater/tensorflowCPU_1.3/lib/python2.7/site-packages/tensorflow/include -I/home/mater/intelFPGA/17.0/hld/host/include -L/home/mater/intelFPGA_pro/17.0/hld/board/a10_ref/linux64/lib -L/home/mater/intelFPGA_pro/17.0/hld/host/linux64/lib -Wl,--no-as-needed -lalteracl -laltera_a10_ref_mmd -lelf -I/home/mater/git/test/tensorflow_opencl -O2 -D_GLIBCXX_USE_CXX11_ABI=0
</code></pre>

Modify AOCLUtils/BUILD
<pre><code>
cc_library(
	name=""aocutils"",
	srcs = [""AOCLUtils.so""],
	hdrs = glob([""*.h""]),
	visibility=[""//visibility:public""],
)
</code></pre>

Second command for build
<pre><code>
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --copt=""-I/home/mater/intelFPGA/17.0/hld/host/include"" --verbose_failures  --copt=""-L/home/mater/intelFPGA_pro/17.0/hld/board/a10_ref/linux64/lib"" --copt=""-L/home/mater/intelFPGA_pro/17.0/hld/host/linux64/lib"" --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""
</code></pre>

result
<pre><code>
ERROR: /home/mater/git/test/tensorflow_opencl/tensorflow/python/BUILD:2908:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1): gcc failed: error executing command 
  (cd /home/mater/.cache/bazel/_bazel_mater/cf4207c477b73da1da7e3336942f640b/execroot/org_tensorflow && \
  exec env - \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL=0 \
  /usr/bin/gcc -shared -o bazel-out/local-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so '-Wl,-rpath,$ORIGIN/../../_solib_k8/_U_S_SAOCLUtils_Caocutils___UAOCLUtils' -Lbazel-out/local-opt/bin/_solib_k8/_U_S_SAOCLUtils_Caocutils___UAOCLUtils -Wl,--version-script tensorflow/tf_version_script.lds -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,@bazel-out/local-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params)
/usr/bin/ld.gold: error: cannot find -lAOCLUtils
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 817.337s, Critical Path: 40.20s
FAILED: Build did NOT complete successfully
</code></pre>

Could someone help me find some suggestion or share information about this? Thanks!! 
"
13146,Standalone graph trained using on NCHW data_format giving errors on CPU when testing (running forward pass),"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
     Yes,
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: Binary (.whl file)
- **TensorFlow version (use command below)**: 1.2.0-rc1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:  N/A
- **CUDA/cuDNN version**:
CUDA 8.0.61
CuDNN 5.1.10
- **GPU model and memory**: GeForce GTX 1080
- **Exact command to reproduce**:


### Describe the problem
 I've a standalone graph create by freezing my model/net  which has a few slim.conv2d, slim.max_pool2d operations defined with NCHW data format.  I've used freeze_graph.py utility to create the stand alone graph using a trained checkpoint (trained using NCHW format). When I tested the graph (ran forward pass) on a Ubuntu machine with GPU, it was running very well. But when I ran the same code to test (forward pass) with the same stand alone graph a Ubuntu machine that has no GPU but CPU, I got the following errors:

```
2017-09-18 17:57:48.890761: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Default MaxPoolingOp only supports NHWC.
         [[Node: text_box_300/pool1/MaxPool = MaxPool[T=DT_FLOAT, data_format=""NCHW"", ksize=[1, 1, 2, 2], padding=""SAME"", strides=[1, 1, 2, 2], _device=""/job:localhost/replica:0/task:0/cpu:0""](text_box_300/conv1/conv1_2/Relu)]]
2017-09-18 17:57:48.892768: E main_textd_test.cc:457] Running model failed: Invalid argument: Default MaxPoolingOp only supports NHWC.
         [[Node: text_box_300/pool1/MaxPool = MaxPool[T=DT_FLOAT, data_format=""NCHW"", ksize=[1, 1, 2, 2], padding=""SAME"", strides=[1, 1, 2, 2], _device=""/job:localhost/replica:0/task:0/cpu:0""](text_box_300/conv1/conv1_2/Relu)]]
```

I believe the errors are related to [https://github.com/tensorflow/tensorflow/issues/2660](https://github.com/tensorflow/tensorflow/issues/2660)
Is it possible to modify the stand alone graph that is running well on GPU to make it run successfully on CPU machine ? Is it possible to avoid retraining the model using NHWC format and recreate the standalone model ?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13144,No gradient defined for Relu6Grad,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes. MWE below is not a stock example.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
Binary (Miniconda)

- **TensorFlow version (use command below)**:
print(tf.GIT_VERSION, tf.VERSION)
('v1.2.0-5-g435cdfc', '1.2.1')

- **Python version**: 
2.7.13

- **Bazel version (if compiling from source)**:
N/A

- **CUDA/cuDNN version**:
CUDA 8.0.61
CuDNN 5.1.10

- **GPU model and memory**:
GeForce GTX TITAN X
Total Memory 11.91GiB

- **Exact command to reproduce**:
See below.

### Describe the problem

Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

One can not apply the `gradient` operator to a `nn.Relu6` more than once.
Applying it once gives a `Relu6Grad` node,
applying it a second time (i.e applying it to the `Relu6Grad`)  throws an error.

Unless I have messed up my math (quiet possible),
Relu6 is infinitely differentiable, except at 2 points,
albeit very boring

 - f(x) = relu6(x) 
 - df/dx = 1 if 0<x<6 else 0
 - d2f/dxx = 0
 - all further derivatives also 0.

How-ever, it is marginally more interesting if one is applying the chain rule to it

 - f(x1,x2) = relu6(x1*x2) 
 - df/dx1 = x2 if 0<x1*x2<6 else 0
 - d2f/dx1x2 = 1 if 0<x1*x2<6 else 0

This is a feature request to make the gradient of `Relu6Grad` defined.


### Source code / logs

MWE:
```python

import tensorflow as tf
sess = tf.Session()

x1 = tf.placeholder(tf.float32)
x2 = tf.placeholder(tf.float32)

y = tf.nn.relu6(x1*x2)
d1 = tf.gradients(y,x1)
d2 = tf.gradients(d1, x2)
```

Errors on the final line  with
```
---------------------------------------------------------------------------
LookupError                               Traceback (most recent call last)
<ipython-input-7-4250ab355a8b> in <module>()
----> 1 d2 = tf.gradients(d1, x2)

...usr/lib/python2.7/site-packages/te
nsorflow/python/ops/gradients_impl.pyc in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops,
 gate_gradients, aggregation_method)
    512               raise LookupError(
    513                   ""No gradient defined for operation '%s' (op type: %s)"" %
--> 514                   (op.name, op.type))
    515         if loop_state:
    516           loop_state.EnterGradWhileContext(op, before=False)

LookupError: No gradient defined for operation 'gradients/Relu6_grad/Relu6Grad' (op type: Relu6Grad)
```
"
13143,how can i convert a frozen.pb file to .ckpt file?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13142,"ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients","Hello everyone,
I have error when programming tensorflow:

ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [""<tf.Variable 'Variable:0' shape=(4, 2) dtype=float32_ref>"", ""<tf.Variable 'Variable_1:0' shape=(4, 2) dtype=float32_ref>"", ""<tf.Variable 'Variable_2:0' shape=(1, 3) dtype=float32_ref>"", ""<tf.Variable 'Variable_3:0' shape=(1, 2) dtype=float32_ref>"", ""<tf.Variable 'Variable_4:0' shape=(1, 3) dtype=float32_ref>"", ""<tf.Variable 'Variable_5:0' shape=(1, 3) dtype=float32_ref>""] and loss Tensor(""Sum:0"", dtype=float32).

My code is here


=============================
import tensorflow as tf

def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev = 0.1)
    return tf.Variable(initial)

def bias_variable(shape):
    initial = tf.constant(0.1, shape = shape)
    return tf.Variable(initial)

# Model parameters
#W = tf.Variable([.3], dtype=tf.float32)
#b = tf.Variable([-.3], dtype=tf.float32)
#W=weight_variable([1])
#b=bias_variable([1])
indice= tf.constant([0,1])
segment_id= tf.constant([0,0,1,1])
W=weight_variable([4,2])
b=bias_variable([4,2])
b_=tf.Variable(tf.zeros([1,3]))
t0=tf.Variable(tf.zeros([1,2]))
t1=tf.Variable(tf.zeros([1,3]))
g=tf.Variable(tf.zeros([1,3]))


# Model input and output
x = tf.placeholder(tf.float32)
#linear_model = W * x + b

#forward transform

linear_function = tf.add(tf.matmul(W,x),b)
linear_function_col_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function), indice[0]))
linear_function_col_2 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function), indice[1]))
min_1=tf.reduce_min(tf.segment_max(linear_function_col_1,segment_id))
min_2=tf.reduce_min(tf.segment_max(linear_function_col_1,segment_id))
#b_=tf.assign(b_,[[min_1,min_2,0]])
b_=tf.assign(b_,[[min_1,min_2,0.0]])
t0=tf.assign(t0,[[min_2,min_1]])
g = tf.nn.softmax(tf.scalar_mul(1000,b_),dim=-1)
#g = tf.nn.softmax(b_)

#inverse transform

linear_function_inv = tf.divide(tf.transpose(tf.transpose(t0)-tf.transpose(b)),W)
linear_function_inv_col_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function_inv), indice[0]))
linear_function_inv_col_2= tf.transpose(tf.nn.embedding_lookup(tf.transpose(linear_function_inv ), indice[1]))
max_1=tf.reduce_max(tf.segment_min(linear_function_inv_col_1,segment_id))
max_2=tf.reduce_max(tf.segment_min(linear_function_inv_col_2,segment_id))
t1=tf.assign(t1,[[max_1,max_2,0.0]])

y = tf.placeholder(tf.float32)

# loss
#loss = tf.reduce_sum(tf.square(linear_model-y)) # sum of the squares
#loss = tf.reduce_sum(tf.square(g)) # sum of the squares
#loss = tf.reduce_sum(tf.square(y-tf.matmul(g,tf.transpose(t1))))
loss = tf.reduce_sum(y-tf.matmul(g,tf.transpose(t1)))
# optimizer
optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)

# training data
#x_train = [1, 2, 3, 4]
x_train_array = tf.constant([0.58975124,0.22815752])
x_train=tf.diag(x_train_array)
#y_train = [0, -1, -2, -3]
y_train=tf.constant([[0.530]])
# training loop
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init) # reset values to wrong
for i in range(1000):
  sess.run(train, {x: x_train, y: y_train})

# evaluate training accuracy
curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})
print(""W: %s b: %s loss: %s""%(curr_W, curr_b, curr_loss))
==================================================

Hope to get your help.
Thank you




"
13141,label image run slower and slower?,"Hello ,Everyone
Is there anybody who had ever run the code label_image.py in tensorflow/tensorflow/examples/label_image/label_image.py
I have modify it to run on a dataset and read and calssify image one by one,and as the number of images goes,the speed is slower and slower,at first,that's about ten images per second,and when the number of image goes to 1000,the time is about 7s,Incredibly!and  I find the problem is in the function  read_tensor_from_image_file in label_image.py and this part is read and preprocess images, so what's the matter?and I want to know how to speed up?and how to modify the code so as to making it run for batches ?"
13139,Where can I get the pre-trained models in slim?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
13137,RNN cells parameter naming inconsistency,"In most RNN cells the size is spified by the `num_units` parameter.
This is true for LSTMCell, LSTMBlockCell and GRUCell.
For GRUBlockCell the same parameter is called `cell_size`.
This discrepancy could be problematic in some cases like this code I'm using to create a RNN cell depending on a string parameter passed by the user:

```
def get_cell_fn(cell_type):
    if cell_type == 'rnn':
        cell_fn = tf.nn.rnn_cell.BasicRNNCell
    elif cell_type == 'lstm_basic':
        cell_fn = tf.nn.rnn_cell.BasicLSTMCell
    elif cell_type == 'lstm_basic_ln':
        cell_fn = tf.contrib.rnn.LayerNormBasicLSTMCell
    elif cell_type == 'lstm_block':
        cell_fn = tf.contrib.rnn.LSTMBlockCell
    elif cell_type == 'lstm':
        cell_fn = tf.nn.rnn_cell.LSTMCell
    elif cell_type == 'gru':
        cell_fn = tf.nn.rnn_cell.GRUCell
    elif cell_type == 'gru_block':
        # Faster version of GRU (25% faster in my tests)
        cell_fn = tf.contrib.rnn.GRUBlockCell
    else:
        cell_fn = tf.nn.rnn_cell.BasicRNNCell
    return cell_fn

cell = get_cell_fn(cell_type)(num_units=256)
```

As this will return an error in case of GRUBlockCell.

The solution is just renaming that parameter and possibly add **kwargs to all cell constructors,as right now only RNNCell has **kwargs."
13133,Session.run with fetches = list apparently works incorrectly,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: The problem manifests in a standard tutorial and I wrote custom code to better understand it.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **Python version**: Python 2.7.12 (default, Nov 19 2016, 06:48:10) [GCC 5.4.0 20160609] on linux2
- **Bazel version (if compiling from source)**:
- **CUDA/cuDNN version**: no GPU
- **GPU model and memory**: none

### Describe the problem
The [TensorFlow Mechanics 101 tutorial](https://tensorflow.rstudio.com/tensorflow/articles/tutorial_tensorflow_mechanics.html) code outputs value of the same Op, loss, in two ways: summary and console output.  The two values do not match.

The code is located [here](https://tensorflow.rstudio.com/tensorflow/articles/examples/mnist_fully_connected_feed.html).
To make the problem clear, I changed the console output format on line 305 to 8 decimals:

      cat(sprintf('Step %d: loss = %.8f (%.3f sec)\n',

I compared the console output for loss with the results of the summary Op (the op is defined on lines 120 and 262, and run on line 308).  I obtained the results of the summary op from the csv file downloaded from tensorboard.

The console output is close to the csv file but different, often in the third decimal; they should be identical since the op is the same on the same step.

I do not copy the output here since it is unstable, changes from one run to another.  (I have not been able to stabilize it with `use_session_with_seed` or with `tf$set_random_seed`.)

I changed lines 297-298 from

    values <- sess$run(list(train_op, loss), feed_dict = feed_dict)
    loss_value <- values[[2]]

to a supposedly equivalent code:

    sess$run(train_op, feed_dict = feed_dict)
    loss_value = sess$run(loss, feed_dict = feed_dict)
 
After that the console output agreed fully with summary data obtained from csv.

I take it that `sess$run(list(...` works incorrectly.

I do not know whether the problem is in TensorFlow or in the R interface to it.

"
13130,Build fails at patch command - different line endings,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 8.1 64bit
- **TensorFlow installed from (source or binary)**:
I'm trying to build from source to use CPU optimizations
- **TensorFlow version (use command below)**:
1.3 cloned from master
- **Python version**: 
3.6.1
- **Bazel version (if compiling from source)**:
0.5.4
- **CUDA/cuDNN version**:
CPU only
### Describe the problem
I'm trying to build TensorFlow from source to use CPU optimizations. I was getting `patch` error exactly as in [Issue #10435](https://github.com/tensorflow/tensorflow/issues/10435) so I used the [recommended solution](https://github.com/tensorflow/tensorflow/issues/10435#issuecomment-306422472) by adding `--binary` to `patch` call at _workspace.bzl_ function `_apply_patch()`. The _tensorflow/core_ is now loaded, bur I get another error with _tensorflow/contrib/session_bundle_ - see log.
### Source code / logs
```
$ ./configure
You have bazel 0.5.4 installed.
Found possible Python library paths:
  D:\anaconda\lib\site-packages
Please input the desired Python library path to use.  Default is [D:\anaconda\lib\site-packages]
Do you wish to build TensorFlow with XLA JIT support? [y/N]: n
No XLA JIT support will be enabled for TensorFlow.
Do you wish to build TensorFlow with GDR support? [y/N]: n
No GDR support will be enabled for TensorFlow.
Do you wish to build TensorFlow with VERBS support? [y/N]: n
No VERBS support will be enabled for TensorFlow.
Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.
Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]:
Add ""--config=mkl"" to your bazel command to build with MKL support.
Please note that MKL on MacOS or windows is still not supported.
If you would like to use a local MKL instead of downloading, please set the environment variable ""TF_MKL_ROOT"" every time before build.
Configuration finished

$ ./bazel build --config=opt --cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --copt=-w --host_copt=-w //tensorflow/tools/pip_package:build_pip_package
.............
____Loading package: tensorflow/tools/pip_package
____Loading package: @bazel_tools//tools/cpp
____Loading package: @bazel_tools//tools/jdk
____Loading package: @local_config_xcode//
____Loading package: @local_config_cc//
____Loading package: @local_jdk//
____Loading complete.  Analyzing...
____Loading package: @bazel_tools//tools/launcher
____Loading package: tensorflow/contrib/nn
____Loading package: tensorflow/python/tools
____Loading package: tensorflow/python/eager
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz: 1,500,767 bytes
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz: 3,866,533 bytes
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz: 1,351,924 bytes
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz: 3,602,866 bytes
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz: 1,223,337 bytes
____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz: 3,589,997 bytes
ERROR: D:/tensorflow/tensorflow/tools/pip_package/BUILD:70:1: error loading package 'tensorflow/contrib/session_bundle': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):
        File ""D:/tensorflow/tensorflow/workspace.bzl"", line 123
                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)
        File ""D:/tensorflow/tensorflow/workspace.bzl"", line 114, in _apply_patch
                _execute_and_check_ret_code(repo_ctx, cmd)
        File ""D:/tensorflow/tensorflow/workspace.bzl"", line 94, in _execute_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(1) when executing 'D:\msys\usr\bin\bash.exe -c patch --binary -p1 -d D:/msys/tmp/_bazel_admin/aeorbm85/external/protobuf_archive -i D:/tensorflow/third_party/protobuf/add_noinlines.patch':
Stdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc
Hunk #1 FAILED at 557 (different line endings).
Hunk #2 FAILED at 656 (different line endings).
Hunk #3 FAILED at 737 (different line endings).
3 out of 3 hunks FAILED -- saving rejects to file src/google/protobuf/compiler/cpp/cpp_file.cc.rej

Stderr:  and referenced by '//tensorflow/tools/pip_package:simple_console_for_windows'.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.
____Elapsed time: 34,135s
```"
13129,random crashes while serving multiple frozen models in parallel using go api,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

Yes, I have written custom code

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Linux x86_64 SLES12

- **TensorFlow installed from (source or binary)**:

Source, latest master at the moment

- **TensorFlow version (use command below)**:

('v1.3.0-rc1-2265-g6e7539b', '1.4.0-dev')
I also tried r1.3 with similar result.

- **Python version**: 

Python 2.7.9

- **Bazel version (if compiling from source)**:

Build label: 0.5.4
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Aug 25 10:00:00 2017 (1503655200)
Build timestamp: 1503655200
Build timestamp as int: 1503655200

- **CUDA/cuDNN version**:

Not used.

- **GPU model and memory**:

Not used.

- **Exact command to reproduce**:

$ ./tfcrash --n_models 16 --n_images 100

An output of this program can be different. The bug has random nature. In some cases the process just segfaults. Typical output is:

$ ./tfcrash --n_models 16 --n_images 100
2017/09/18 16:45:43 setting 8 cpu
2017/09/18 16:45:43 launching 16 models
2017/09/18 16:45:43 feeding 100 images
2017/09/18 16:45:43 waiting
2017/09/18 16:45:51 session.Run() failed: Expects arg[0] to be uint8 but INVALID is provided

Or:

$ ./tfcrash 
2017/09/18 16:57:54 setting 8 cpu
2017/09/18 16:57:54 launching 16 models
2017/09/18 16:57:54 feeding 100 images
2017/09/18 16:57:54 waiting
Segmentation fault (core dumped)


### Describe the problem

I'm trying to serve predictions from multiple frozen models that I have trained and generated previously using python script. My programming language for serving predictions is golang. I have found that sometimes my process crashes randomly. Exact conditions needed to reproduce this behaviour are unknown. It is also unknown if this bug related to golang bindings or tensorflow itself.
I also tried different builds of tensorflow, all of them are affected so far, including one built with cuda support. I also noticed that setting lower numbers for --n_images and --n_models parameters decreases probability of bug reproduction. In my experience setting --n_models to 16 and up gives 100% probability of crash.

I tried both go-1.8.3 and go-1.9 with similar result.

### Source code / logs

I wrote a short program (less than 100 lines in go) which is able to reproduce crash with >90% probability:
https://gist.github.com/a33c892b17d9ec1da1e40e4fb68fdcf9

The model file (type: .frozen.pb, size: 34Mb): https://drive.google.com/file/d/0B9jZHp3Hh0s2MnAxekRGYlVTVHM/

"
13127,Build Tensorflow on Windows with /MT option,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64-bit
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.3.0 release
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: - (using CMake 3.9.1)
- **Exact command to reproduce**:

```
First: add /MT key to CMAKE_CXX_FLAGS_RELEASE in CMakeLists.txt
set(CMAKE_CXX_FLAGS_RELEASE ""${CMAKE_CXX_FLAGS_RELEASE} /MT /D_ITERATOR_DEBUG_LEVEL=0"")

Then, CMake and build:

$ cmake .. -G""Visual Studio 15 2017 Win64"" -Dtensorflow_BUILD_PYTHON_BINDINGS=OFF -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF -Dtensorflow_BUILD_SHARED_LIB=ON
$ MSBuild /m:2 /p:PreferredToolArchitecture=x64 /p:Configuration=Release tensorflow.vcxproj
```

### Describe the problem
I am getting a lot of link errors of the following type for many 3rd party libraries tensorflow links against:

```
(Link target) ->
         nsync.lib(mu.obj) : error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MD_DynamicRelease' doesn't match value 'MT_StaticRelease' in gen_proto_text_functions.obj [C:\all\lib\tensorflow\tensorflow\contrib\cmake\bu
       ild\proto_text.vcxproj]
```

I am not really sure if that's supposed to work, so it can be considered a feature request. I would really like to build tensorflow with /MT to make my application as portable as possible.
Currently, I build tensorflow.dll with Visual Studio 2017 and default switches (/MD), and therefore the resulting application requires Microsoft Redistributable C++ libraries for Visual Studio 2017.

If building with /MT is supposed to work, maybe someone could give me a hint how to get it right (and then we can close the issue)? I guess, digging into build options of every third-party library and adding /MT there?"
13126,installing tensorflow to a local folder results in import error with realtive paths,"this is on linux with python 3.6.2 and tf 1.3
i install tensorflow into a folder named 'site-packages' inside my project. i then add it to the path with a relative reference. works fine when path is absolute

pip install tensorflow -t site-packages
python
import sys
sys.path.insert(1,""site-packages"") # will fail
#sys.path.insert(1,""/home/absolute_path/site-packages"") #works
from tensorflow.contrib.framework.python.ops.checkpoint_ops import *
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""site-packages/tensorflow/contrib/framework/python/ops/__init__.py"", line 24, in <module>
    from tensorflow.contrib.framework.python.ops.checkpoint_ops import *
  File ""site-packages/tensorflow/contrib/framework/python/ops/checkpoint_ops.py"", line 32, in <module>
    resource_loader.get_path_to_datafile(""_checkpoint_ops.so""))
  File ""site-packages/tensorflow/contrib/util/loader.py"", line 55, in load_op_library
    ret = load_library.load_op_library(path)
  File ""site-packages/tensorflow/python/framework/load_library.py"", line 64, in load_op_library
    None, None, error_msg, error_code)
tensorflow.python.framework.errors_impl.NotFoundError: site-packages/tensorflow/contrib/util/site-packages/tensorflow/contrib/framework/python/ops/_checkpoint_ops.so: cannot open shared object file: No such file or directory
"
