Issue Number,Issue Title,Issue Body
36228,tensorflow java GPU compute capabilties 6.0 instead of 3.7,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Linux 18.06
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.13.0, 1.13.1, 1.13.2, 1.14.0, 1.15.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: Nope
- CUDA/cuDNN version: 10.1
- GPU model and memory: K80

**Describe the problem**
I am loading a model in a Java Server using the Java API to do inference. The inference is working but not running on the GPU. I get an error message provided below.

We have been using the python version of the same tensorflow release on the same GPU (K80) without an issue. This is only a problem with the java driver. 

I believe the culprit is this line:
https://github.com/tensorflow/tensorflow/blob/aa50d2b624c7e8d56b4b1644c4ccf489d8e8c55c/tensorflow/tools/ci_build/presubmit/ubuntu_16/gpu_py36_full/build.sh#L41-L44

The comment seems to indicate that this env variable is not used. I don't really understand the setup but it looks like the Java build is picking it up.

No other build seems to be requiring compute 6.0, everything is set to 3.7 (K80) which would make sense since K80 are probably the cheapest GPUs available on cloud.

https://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/tools/ci_build/xla/linux/gpu/run_py3.sh#L31

https://github.com/tensorflow/tensorflow/blob/938c08e5e612c5a581262e409cda08db61b25d25/tensorflow/tools/ci_build/Dockerfile.gpu.ppc64le#L28

[Search of all  TF_CUDA_COMPUTE_CAPABILITIES in the Repo](https://github.com/tensorflow/tensorflow/search?q=TF_CUDA_COMPUTE_CAPABILITIES&unscoped_q=TF_CUDA_COMPUTE_CAPABILITIES)


**Provide the exact sequence of commands / steps that you executed before running into the problem**
Use the Java Driver on GPU.

**Any other info / logs**
```
 2020-01-25 23:17:43.458562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1717] Ignoring visible gpu device (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7) with Cuda compute capability 3.7. The min │
│ imum required Cuda capability is 6.0.                                                                                                                                                                                                      │
│ 2020-01-25 23:17:43.519712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:                                                                                       │
│ 2020-01-25 23:17:43.519748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0                                                                                                                                                │
│ 2020-01-25 23:17:43.519762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
```

CI Logs that contain compute restriction:
https://source.cloud.google.com/results/invocations/37ab790b-7136-4d77-a415-f03b9909e3b2/targets/tensorflow%2Fgithub%2Fubuntu_16%2Fgpu_py36_full%2Fcontinuous/log
"
36227,Poor performance and warning for GradientTape.jacobian with experimental_use_pfor=False,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Scientific Linux 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 2
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Extremely slow gradient calculation. 
Also the following warning: 
```
WARNING:tensorflow:From /home/grad3/cmich/miniconda3/envs/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py:502: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.identity instead.
```

**Describe the expected behavior**
There most be a much faster way of getting these gradients. After all the TensorFlow optimization must use these and those evaluations do not take so long. 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import time
import tensorflow as tf
import numpy as np

class Model(tf.keras.Model):
    def __init__(self,):
        super(Model, self).__init__()
        self.layers_list = [tf.keras.layers.Dense(2)] + \
            [tf.keras.layers.Dense(1000)] + \
            [tf.keras.layers.Dense(500)]

    def call(self, x):
        for layer in self.layers_list:
            x = layer(x)
        return x

model = Model()
model.build((1,2))
weights = model.trainable_variables

input = np.zeros([1,2])

with tf.GradientTape(persistent=True) as tape:
    ts = time.time()
    output = model(input)
    print(f'Forward took {time.time()-ts:.2f}s')

ts = time.time()
gradients = tape.jacobian(output, weights, experimental_use_pfor=False)
print(f'Backward took {time.time()-ts:.2f}s')
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Output:
```
Forward took 0.01s
WARNING:tensorflow:From /home/grad3/cmich/miniconda3/envs/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py:502: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.identity instead.
Backward took 5.75s
```
The gradient calculation takes several orders of magnitude more time (for my real case the respective times are about 0.03s and 18s which is too much). 

Is this the best way of doing this or this a faster way? Also should I care about the warning?"
36226,How is data laid out in TfLiteTensor.data.uint8[]?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 4.19.0-6-amd64 #1 SMP Debian 4.19.67-2+deb10u2 (2019-11-11) x86_64 GNU/Linux
- TensorFlow installed from (source or binary):  source at https://github.com/tensorflow/tensorflow/archive/v2.1.0.tar.gz
- TensorFlow version (or github SHA if from source): 2.1.0

I am loading a MobileNet V2 in tensorflow-lite and then getting a reference to a tensor with index `4` as follows:
```
TfLiteTensor *avgpool_input = interpreter->tensor(4);
```
The dimension of this tensor is: `avgpool_input->dims->data = [1 7 7 1280]`. I know that the data of this tensor is stored in `avgpool_input.data.uint8[]`. I do not know how the 4-D tensor data is laid out in this flat array. I looked into the `tensorflow/lite/c/c_api_internal.h` file but I could not find any documentation about the layout of data. Can someone please help me understand the layout of data or point me out to relevant parts of the source code?
"
36225,download_and_extract URL DIR download_dependencies.sh,"I follow [the build for arm64](https://www.tensorflow.org/lite/guide/build_arm64) and try  to execute `download_dependencies.sh` script but end up with issue:

```
~/work/tensorflow$ ./tensorflow/lite/tools/make/download_dependencies.sh
./tensorflow/lite/tools/make/download_dependencies.sh: line 59: 1: Usage: download_and_extract URL DIR
```

Anybody can advise on this issue?
"
36224,tf.keras NaN loss when using multiple GPUs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Pip
- TensorFlow version (use command below): 2.0
- Python version: 3.7.3
- Bazel version (if compiling from source): no
- GCC/Compiler version (if compiling from source): no
- CUDA/cuDNN version: 10
- GPU model and memory: 2x NVIDIA GTX 1080 Ti 11GB
- Driver Version: 440.33.01


I am currently using Tensorflow 2.0 (Python) and the` tf.keras` library to train a CNN. However, I am encountering an issue when I try to train my model by calling `model.fit()`. 

After I begin training, the loss is normal for 1 ~ 2 steps for the first epoch. But after that, it suddenly becomes **NaN** loss.

This issue only happens when using multiple GPUs. The code I'm using works perfectly fine on a single GPU. I have wrapped all of my code inside the scope of a `tf.distribute.MirroredStrategy ` using `with strategy.scope():`. I am feeding my network with data from a `tf.data.Dataset` (though this error occurs regardless of the data format).

I then ran some tests:

1) I tried to replace the data in my dataset with random number, but the loss still went to **NaN**.

2) I also tried feeding the numpy arrays directly to `.fit()`, but that didn't solve the issue.

3) I tried using different optimizers (Adam, RMSprop, SGD), batch sizes (4, 8, 16, 32), and learning rates, none of which helped to solve this problem.

4) I swapped out my network for a simple Multi-layer Perceptron, but the error persisted.

This doesn't appear to be an OOM issue, since the data is relatively small and running `watch -n0.1 nvidia-smi` reveals that memory usage never exceeds 30% on either of my GPUs. There doesn't seem to be any warning or error in the console output that might hint at the issue either.

Any help is appreciated"
36223,tensor flow installing error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.0
- Python version: 3.7.6
- Installed using virtualenv? pip? conda?: conda
- CUDA/cuDNN version: 10.1
- GPU model and memory: Nvidia GEFORCE GTX 1050



**Describe the problem**
Im trying to demo image detection in darkflow and it lead to this error

**Provide the exact sequence of commands / steps that you executed before running into the problem**:
(darkflow-env) C:\Binod\Deep_Learning\Object_Detection\YOLO_Detection\darkflow-master>python flow --model cfg --load bin\yolov2.weights --imgdir sample_dog --gpu 0.7
Traceback (most recent call last):
  File ""flow"", line 4, in <module>
    from darkflow.cli import cliHandler
  File ""C:\Binod\Deep_Learning\Object_Detection\YOLO_Detection\darkflow-master\darkflow\cli.py"", line 3, in <module>
    from .net.build import TFNet
  File ""C:\Binod\Deep_Learning\Object_Detection\YOLO_Detection\darkflow-master\darkflow\net\build.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\darkflow\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\darkflow\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\darkflow\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\darkflow\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\darkflow\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\darkflow\lib\site-packages\tensorflow_core\python\__init__.py"", line 47, in <module>
    import numpy as np
  File ""C:\darkflow\lib\site-packages\numpy\__init__.py"", line 140, in <module>
    from . import _distributor_init
  File ""C:\darkflow\lib\site-packages\numpy\_distributor_init.py"", line 34, in <module>
    from . import _mklinit
ImportError: DLL load failed: The specified module could not be found.
"
36222,Could we have conv2d_transpose for C++?,"**System information**
- TensorFlow version : 2.1

In python we have conv2d_transpose to upsample. Could we have something similar for the C++ api inaddition to ResizeBilinear, ResizeNearestNeighbor and ResizeBicubic and ResizeArea?"
36221,TF-Lite example label Image w shared library,"I have built tf-lite with shared `libtensorflowlite.so` library, I  am looking for help to get compiled label_image example using that shared library. 
Basically to separate projects from the tf framework.
How do I set a shared library to use with label image example?
"
36219,tf.lite.converter.convert() Erroe,"**System information**
- OS Platform and Distribution Ubuntu 18.04
- TensorFlow version (or github SHA if from source):2.0.0b1

I am trying to test LSTM/GRU code using TensorFlow 2.0.0b1 in ubuntu 18.04.
Below you can find my simple model and converter code

```
def create_gru_model():
    model = tf.keras.models.Sequential();
    model.add(tf.keras.layers.GRU(128))
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(49,activation=tf.nn.softmax))
    model.compile(optimizer='adam',loss=customLoss,metrics = ['accuracy'])
    return model
model = create_gru_model()
model.fit(x_train, y_train, epochs = 3, callbacks = [cp_callback], validation_data = (x_valid,y_valid), verbose=0)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.experimental_new_converter=True
tflite_model = converter.convert()
```

I get the following error

```
Traceback (most recent call last):
  File ""test.py"", line 148, in <module>
    tflite_model = converter.convert()
  File ""/home/pramod/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 392, in convert
    **converter_kwargs)
  File ""/home/pramod/.local/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 404, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/pramod/.local/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 172, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
/home/pramod/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
2020-01-26 12:56:17.818849: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-01-26 12:56:17.818886: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-01-26 12:56:17.818892: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-01-26 12:56:17.818987: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: StatefulPartitionedCall
2020-01-26 12:56:17.827230: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)
Fatal Python error: Aborted

Current thread 0x00007f4c3f6b7740 (most recent call first):
  File ""/home/pramod/.local/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33 in execute
  File ""/home/pramod/.local/lib/python3.7/site-packages/absl/app.py"", line 250 in _run_main
  File ""/home/pramod/.local/lib/python3.7/site-packages/absl/app.py"", line 299 in run
  File ""/home/pramod/.local/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40 in run
  File ""/home/pramod/.local/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59 in main
  File ""/home/pramod/.local/bin/toco_from_protos"", line 11 in <module>
Aborted (core dumped)



Error in sys.excepthook:
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/apport_python_hook.py"", line 63, in apport_excepthook
    from apport.fileutils import likely_packaged, get_recent_crashes
  File ""/usr/lib/python3/dist-packages/apport/__init__.py"", line 5, in <module>
    from apport.report import Report
  File ""/usr/lib/python3/dist-packages/apport/report.py"", line 30, in <module>
    import apport.fileutils
  File ""/usr/lib/python3/dist-packages/apport/fileutils.py"", line 23, in <module>
    from apport.packaging_impl import impl as packaging
  File ""/usr/lib/python3/dist-packages/apport/packaging_impl.py"", line 24, in <module>
    import apt
  File ""/usr/lib/python3/dist-packages/apt/__init__.py"", line 23, in <module>
    import apt_pkg
ModuleNotFoundError: No module named 'apt_pkg'

Original exception was:
Traceback (most recent call last):
  File ""test.py"", line 148, in <module>
    tflite_model = converter.convert()
  File ""/home/pramod/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 392, in convert
    **converter_kwargs)
  File ""/home/pramod/.local/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 404, in toco_convert_impl
    input_data.SerializeToString())
  File ""/home/pramod/.local/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 172, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
/home/pramod/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/pramod/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
2020-01-26 12:56:17.818849: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-01-26 12:56:17.818886: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-01-26 12:56:17.818892: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-01-26 12:56:17.818987: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: StatefulPartitionedCall
2020-01-26 12:56:17.827230: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)
Fatal Python error: Aborted

Current thread 0x00007f4c3f6b7740 (most recent call first):
  File ""/home/pramod/.local/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33 in execute
  File ""/home/pramod/.local/lib/python3.7/site-packages/absl/app.py"", line 250 in _run_main
  File ""/home/pramod/.local/lib/python3.7/site-packages/absl/app.py"", line 299 in run
  File ""/home/pramod/.local/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40 in run
  File ""/home/pramod/.local/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59 in main
  File ""/home/pramod/.local/bin/toco_from_protos"", line 11 in <module>
Aborted (core dumped)

```
"
36217,"Report ""model_pruner failed: Invalid argument: Invalid input graph."" when i call C++ API","model format "".pb""
python are success to predict, input dim is {1,512},code like this:


sess = tf.Session()
graph = tf.get_default_graph()
with tf.gfile.FastGFile('./model_tags.pb','rb') as f:
	graph_def = tf.GraphDef()
	graph_def.ParseFromString(f.read())
	sess.graph.as_default()
	g_in = tf.import_graph_def(graph_def)
	tensor_input = sess.graph.get_tensor_by_name('import/input_1:0')
	tensor_output = sess.graph.get_tensor_by_name('import/time_distributed_1/Reshape_1:0')
	predictions = sess.run(tensor_output, {tensor_input: np.expand_dims(str_word_list_np, axis=0)})

when i call c++ api,code:


    tensorflow::Session *m_pTFSes;
    tensorflow::GraphDef m_grBiLSTM;
    tensorflow::Status status = tensorflow::NewSession(tensorflow::SessionOptions(), &m_pTFSes);
    if (!status.ok())
    {
        std::cout << status.ToString() << ""\n"";
        return false;
    }
    status = tensorflow::ReadBinaryProto(tensorflow::Env::Default(), ""model_tags.pb"", &m_grBiLSTM);
    if (!status.ok())
    {
        std::cout << status.ToString() << ""\n"";
        return false;
    }
    status = m_pTFSes->Create(m_grBiLSTM);
    if (!status.ok())
    {
        std::cout << status.ToString() << ""\n"";
        return false;
    }
    tensorflow::Tensor tInput(tensorflow::DT_INT32, tensorflow::TensorShape({ 1, 512 }));
    int*pSeq = tInput.flat<int>().data();
    std::vector<std::string> vecLetters;
    SplitString(strText, vecLetters);
    for(int i = 0;i < 512;i++)
    {
        if(i < vecLetters.size())
        {
            pSeq[i] = m_umLetters[vecLetters[i]];
        }
        else
        {
            pSeq[i] = 0;
        }
    }
    std::vector<std::pair<std::string, tensorflow::Tensor>> inSeq;
    inSeq.push_back(std::make_pair(""import/input_1:0"", tInput));
    std::vector<tensorflow::Tensor> outSeq;
    m_pTFSes->Run(inSeq, {""import/time_distributed_1/Reshape_1:0""}, {}, &outSeq);


that report 
2020-01-26 04:16:36.014125: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:562] model_pruner failed: Invalid argument: Invalid input graph.
2020-01-26 04:16:36.525743: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:562] model_pruner failed: Invalid argument: Invalid input graph.

"
36216,Keras: expose loss metric of singular output,"**System information**
- TensorFlow version (you are using): 2.1.0
- Are you willing to contribute it (Yes/No): maybe



**Describe the feature and the current behavior/state.**
Currently when training a keras model with multiple outputs, you get multiple ""loss"" metrics. One for each output, and then an overall loss for the whole model. However when your model only contains a single output, the metrics only contain a single loss value. [The documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=stable#compile) implies that this is because the overall loss is simply a weighted sum of the individual outputs, and thus is irrelevant when only one output is present. However this is incorrect. The overall loss used to train the model also includes other factors, such as [layer losses due to regularizer penalties](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/Regularizer). Thus even with a single output, the output's loss can be different from the model's overall loss.
To address this discrepancy, a metric for the output loss should be provided even when the model only has one output.

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Anyone with layer losses, such as due to regularizers.

**Any Other info.**
"
36213,Build using nvcc + clang?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: commit c044803bcd710a040d6baa6f39588581ae1c5c01 (with the problem in https://github.com/tensorflow/tensorflow/issues/36170 fixed locally)
- Python version: 3.7.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 1.2.1
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
- CUDA/cuDNN version: V10.1.243/7.6.5
- GPU model and memory: GeForce GTX 1080, 8116 MiB 


**Describe the problem**

I was able to build Tensorflow with (`bazel build tensorflow/tools/pip_package:build_pip_package`) using nvcc + gcc. But if I tried to manully override gcc using clang (8.0.1), the build will fail with
```
clang: error: unknown argument: '-fno-canonical-system-headers'
``` 
It seems the CC toolchain auto config still generates flags for nvcc + gcc, even though I choose to use clang. I confirmed that my gcc has this flag while my clang does not.

If I choose to use clang as CUDA compiler, it would fail with
```
 error: cannot find libdevice for sm_61.
``` 
which seems to mean that Clang cannot compile for this cuda compute capabilities (6.1). So I am wondering if there is any way of using nvcc + clang to build since I use clang as default (for all other builds). Shouldn't this be possible since clang is used by default on Macs? Thanks!"
36212,Issue on importing tensorflow,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac Catalina version 10.15.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): installed from source
- TensorFlow version: 2.1
- Python version: 3.7
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**I've installed tensorflow on a virtual environment but I can't import to use it**

** I've Followed the installation instructions from the tensorflow website on https://www.tensorflow.org/install/pip **


**I don't know if this is linked but I can't find tensorflow on the Anaconda list of package after creating a new environment in the Anaconda Navigator **

----------------------------------------------- 
(venvtensorflow) tim@MacBook-Pro-de-Timothee ~ % pip list
Package              Version   
_________.           ___________
absl-py              0.9.0     
astor                0.8.1     
cachetools           4.0.0     
certifi              2019.11.28
chardet              3.0.4     
gast                 0.2.2     
google-auth          1.11.0    
google-auth-oauthlib 0.4.1     
google-pasta         0.1.8     
grpcio               1.26.0    
h5py                 2.10.0    
idna                 2.8       
Keras-Applications   1.0.8     
Keras-Preprocessing  1.1.0     
Markdown             3.1.1     
numpy                1.18.1    
oauthlib             3.1.0     
opt-einsum           3.1.0     
pip                  20.0.2    
protobuf             3.11.2    
pyasn1               0.4.8     
pyasn1-modules       0.2.8     
requests             2.22.0    
requests-oauthlib    1.3.0     
rsa                  4.0       
scipy                1.4.1     
setuptools           45.1.0    
six                  1.12.0    
tensorboard          2.1.0     
tensorflow           2.1.0                         <- Tensorflow seems installed
tensorflow-estimator 2.1.0     
termcolor            1.1.0     
urllib3              1.25.8    
Werkzeug             0.16.0    
wheel                0.33.6    
wrapt                1.11.2  


----------------------------------------------- 
(venvtensorflow) tim@MacBook-Pro-de-Timothee ~ % python
Python 3.7.3 (default, Dec 13 2019, 19:58:14) 
[Clang 11.0.0 (clang-1100.0.33.17)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/tim/venvtensorflow/lib/python3.7/site-packages/tensorflow/__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""/Users/tim/venvtensorflow/lib/python3.7/site-packages/tensorflow_core/__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 959, in _find_and_load_unlocked
  File ""/Users/tim/venvtensorflow/lib/python3.7/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/Users/tim/venvtensorflow/lib/python3.7/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/Users/tim/venvtensorflow/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/Users/tim/venvtensorflow/lib/python3.7/site-packages/tensorflow_core/python/__init__.py"", line 64, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/Users/tim/venvtensorflow/lib/python3.7/site-packages/tensorflow_core/core/framework/graph_pb2.py"", line 7, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""/Users/tim/venvtensorflow/lib/python3.7/site-packages/google/protobuf/__init__.py"", line 37, in <module>
    __import__('pkg_resources').declare_namespace(__name__)
  File ""/Users/tim/venvtensorflow/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 84, in <module>
    __import__('pkg_resources.extern.packaging.requirements')
  File ""/Users/tim/venvtensorflow/lib/python3.7/site-packages/pkg_resources/_vendor/packaging/requirements.py"", line 9, in <module>
    from pkg_resources.extern.pyparsing import stringStart, stringEnd, originalTextFor, ParseException
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 668, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 638, in _load_backward_compatible
  File ""/Users/tim/venvtensorflow/lib/python3.7/site-packages/pkg_resources/extern/__init__.py"", line 43, in load_module
    __import__(extant)
  File ""/Users/tim/venvtensorflow/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py"", line 4756, in <module>
    _escapedPunc = Word( _bslash, r""\[]-*.$+^?()~ "", exact=2 ).setParseAction(lambda s,l,t:t[0][1])
  File ""/Users/tim/venvtensorflow/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py"", line 1284, in setParseAction
    self.parseAction = list(map(_trim_arity, list(fns)))
  File ""/Users/tim/venvtensorflow/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py"", line 1066, in _trim_arity
    this_line = extract_stack(limit=2)[-1]
  File ""/Users/tim/venvtensorflow/lib/python3.7/site-packages/pkg_resources/_vendor/pyparsing.py"", line 1050, in extract_stack
    frame_summary = traceback.extract_stack(limit=-offset+limit-1)[offset]
  File ""/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/traceback.py"", line 211, in extract_stack
    stack = StackSummary.extract(walk_stack(f), limit=limit)
  File ""/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/traceback.py"", line 363, in extract
    f.line
  File ""/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/traceback.py"", line 285, in line
    self._line = linecache.getline(self.filename, self.lineno).strip()
  File ""/Users/tim/venvtensorflow/lib/python3.7/linecache.py"", line 16, in getline
    lines = getlines(filename, module_globals)
  File ""/Users/tim/venvtensorflow/lib/python3.7/linecache.py"", line 48, in getlines
    for mod in sys.modules.values():
RuntimeError: dictionary changed size during iteration"
36209,Segmentation fault when training DeepLab segmentation model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): `conda install tensorflow=1`
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6.7

**Describe the current behavior**
I am trying to train the DeepLab model following the directions in this [DeepLab example](https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/pascal.md) using the following command:
```
$ python deeplab/train.py \
    --logtostderr \
    --training_number_of_steps=30000 \
    --train_split=""train"" \
    --model_variant=""xception_65"" \
    --atrous_rates=6 \
    --atrous_rates=12 \
    --atrous_rates=18 \
    --output_stride=16 \
    --decoder_output_stride=4 \
    --train_crop_size=""513,513"" \
    --train_batch_size=1 \
    --dataset=""basins"" \
    --tf_initial_checkpoint=/home/james/deeplab/pretrained/x65-b2u1s2p-d48-2-3x256-sc-cr300k_init.ckpt.data-00000-of-00001 \
    --train_logdir=./deeplab/datasets/basins/exp/train_on_train_set/train \
    --dataset_dir=./deeplab/datasets/basins
```

I get the following output/error:
```
<thousands of useless/confusing warning messages>
...
2020-01-25 16:03:48.084849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz
2020-01-25 16:03:48.086476: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x559e7db9e710 executing computations on platform Host. Devices:
2020-01-25 16:03:48.086916: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Starting Session.
INFO:tensorflow:Saving checkpoint to path ./deeplab/datasets/basins/exp/train_on_train_set/train/model.ckpt
INFO:tensorflow:Starting Queues.
Segmentation fault (core dumped)
```
**Describe the expected behavior**
I am hoping that the model will train as advertised.

**Code to reproduce the issue**
Run a DeepLab model training as described in the DeepLab tutorial documentation referenced above.

BTW I have had this happen on two separate machines, both of which are running Ubuntu 18.04. One is a Dell laptop with CPU and the other is an AWS EC2 instance with T4 GPU. On the EC2 instance the TF version installed is 1.15.0 and I also tried using TensorFlow 2.0 but when I did that the code failed immediately with a message indicating that tf.contrib is no longer included in TensorFlow so my assumption is that this code has not been ported to work for TF2. Please advise if there is a known version of TF that this code works with, it appears to be broken in its current state using the versions of TF that I've tried.

Thanks in advance for any insight or suggestions. And/or if there is a more up-to-date semantic segmentation model from TensorFlow other than DeepLab then please point me in the right direction."
36208,tensorflow lite model prediction cause Segmentation fault (core dumped),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from source 
- TensorFlow version : 2.0
- Python version: 3.6.9
- Bazel version : 1.1
- GCC/Compiler version (if compiling from source): 8.30
- CUDA/cuDNN version: 10.0.13
- GPU model and memory: GTX 1050 ti


**Describe the current behavior**
the code below always cause Segmentation fault (core dumped), I just trained a model convert to tflite and try to test my tflite model

**Describe the expected behavior**
it suppose to generate result from testing data

**Code to reproduce the issue**
``````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````
import numpy as np
import tensorflow as tf
import os
import pathlib
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow_datasets as dataset

import os
import numpy as np
import matplotlib.pyplot as plt

interpreter= tf.lite.Interpreter(model_path=""../model/CNN_mobileV2_2.tflite"")
interpreter.allocate_tensors()

PATH='/home/linxiang/test/clarius_img'

validation_dir=os.path.join(PATH, 'validation')
validation_image_generator = ImageDataGenerator(validation_split=0.9)
batch_size = 16
epochs = 40
IMG_HEIGHT = 224#160
IMG_WIDTH = 224#700
val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,
                                                              directory=validation_dir,
							      shuffle=True,
							      subset='validation',
                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                              class_mode='categorical',
                                                              classes=['abd','bladder','cardiac','lung'],
						              color_mode='rgb')

wrongNumber=0;
input_details=interpreter.get_input_details()
output_details=interpreter.get_output_details()
interpreter.resize_tensor_input(input_details[0]['index'],(16,224,224,3))
interpreter.resize_tensor_input(output_details[0]['index'],(16,4))

val_img,val_label=next(iter(val_data_gen))
#val_img_gen,val_label_gen=next(iter(val_data_gen))
true_label_ids = np.argmax(val_label, axis=-1)

interpreter.set_tensor(input_details[0]['index'], val_img)
interpreter.invoke()
tflite_model_predictions = interpreter.get_tensor(output_details[0]['index'])
print(""Prediction results:"", tflite_model_predictions)

"
36207,"Need help in finding out the file where exactly TFlite ""Post training Quantization"" is implimented in the tensorflow Repository","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.0
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:xiomi a2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:1.14
- **Python version**:3.7
- **Bazel version (if compiling from source)**:None
- **GCC/Compiler version (if compiling from source)**:None
- **CUDA/cuDNN version**:None
- **GPU model and memory**:Geforce 150Mx 2GB
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
I am Requesting a Feature
Wanted to locate the exact file where ""8 bit Post training Quantization"" scheme is impimented 
in the tensorflow repository.
### Source code / logs
i have traversed until here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/python/toco_from_protos.py
But not able to trace the code from here."
36205,ModuleNotFoundError: No module named 'pycocotools._mask' PLEASE HELP ME!!!!,"I am running a Windows 10 x64 machine and I do find the _mask.pyx file in the path specified but it still can't find it for some reason! I have followed the instructions off of this website. Any help is much appreciated, I have run the build.py off of the cocoAPI since I am on windows. I have tried to install pycotools through pip. I have gone through every website with this issue mentioned and nothing has worked!!!! Thanks!

**Describe the current behavior**
its giving me this error:

    Traceback (most recent call last):
    File ""model_main.py"", line 25, in <module>
    from object_detection import model_lib
    File ""C:\Python37\models\research\object_detection\model_lib.py"", line 27, in <module>
    from object_detection import eval_util
    File ""C:\Python37\models\research\object_detection\eval_util.py"", line 33, in <module>
    from object_detection.metrics import coco_evaluation
    File ""C:\Python37\models\research\object_detection\metrics\coco_evaluation.py"", line 25, in <module>
    from object_detection.metrics import coco_tools
    File ""C:\Python37\models\research\object_detection\metrics\coco_tools.py"", line 51, in <module>
    from pycocotools import coco
    File ""C:\Python37\models\research\pycocotools\coco.py"", line 55, in <module>
    from . import mask as maskUtils
    File ""C:\Python37\models\research\pycocotools\mask.py"", line 3, in <module>
    import pycocotools._mask as _mask
    ModuleNotFoundError: No module named 'pycocotools._mask'

**Describe the expected behavior**
It should run this command:
`python model_main.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssdlite_mobilenet_v2_coco.config`

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Idk

**Other info / logs**
I included the logs above I really need help soon!!!
"
36203,unable to use TimeseriesGenerator with fit_generator,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 19.3 Tricia
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary from pip
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0
- Python version: Python 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce GTX 970 4GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
`fit_generator`or `fit` can't handle a `TimeSeriesGenerator`
**Describe the expected behavior**
Should be handled
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
# if we uncomment this line and comment the next one no more problems 
# import keras # version 2.3.1
from tensorflow import keras # version 2.2.4-tf
import numpy as np
from keras.preprocessing.sequence import TimeseriesGenerator
print(keras.__version__)
# define dataset
series = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
# define generator
n_input = 2
generator = TimeseriesGenerator(series, series, length=n_input, batch_size=8)

model = keras.Sequential([
    keras.layers.Dense(1, activation='relu', input_dim=2),
    keras.layers.Dense(1)
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

model.fit_generator(generator, steps_per_epoch=1, epochs=10)
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-18-603f0c2b45d6> in <module>
     17 model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
     18 
---> 19 model.fit_generator(generator, steps_per_epoch=1, epochs=10)

~/master/.venv/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py in new_func(*args, **kwargs)
    322               'in a future version' if date is None else ('after %s' % date),
    323               instructions)
--> 324       return func(*args, **kwargs)
    325     return tf_decorator.make_decorator(
    326         func, new_func, 'deprecated',

~/master/.venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1304         use_multiprocessing=use_multiprocessing,
   1305         shuffle=shuffle,
-> 1306         initial_epoch=initial_epoch)
   1307 
   1308   @deprecation.deprecated(

~/master/.venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    817         max_queue_size=max_queue_size,
    818         workers=workers,
--> 819         use_multiprocessing=use_multiprocessing)
    820 
    821   def evaluate(self,

~/master/.venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    233           max_queue_size=max_queue_size,
    234           workers=workers,
--> 235           use_multiprocessing=use_multiprocessing)
    236 
    237       total_samples = _get_total_number_of_samples(training_data_adapter)

~/master/.venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in _process_training_inputs(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)
    531                      'at same time.')
    532 
--> 533   adapter_cls = data_adapter.select_data_adapter(x, y)
    534 
    535   # Handle validation_split, we want to split the data and get the training

~/master/.venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/data_adapter.py in select_data_adapter(x, y)
    996         ""Failed to find data adapter that can handle ""
    997         ""input: {}, {}"".format(
--> 998             _type_name(x), _type_name(y)))
    999   elif len(adapter_cls) > 1:
   1000     raise RuntimeError(

ValueError: Failed to find data adapter that can handle input: <class 'keras.preprocessing.sequence.TimeseriesGenerator'>, <class 'NoneType'>
```
If you need more logs just ask :)."
36201,"""Could not load dynamic library 'libnvinfer.so.6'"" when installing from WSL 2","**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu through **WSL 2** on Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version: 2.1.0
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: pip, no virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: no, machine is Dell XPS 13 9530
- GPU model and memory: no

**Describe the problem**
When installing tensorflow using pip I'm getting:

    In [1]: import tensorflow as tf
    2020-01-25 11:20:08.541504: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
    2020-01-25 11:20:08.541639: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
    2020-01-25 11:20:08.541689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

A fresh install of WSL 2 on Windows 10, I did:

    sudo apt update -y && sudo apt upgarde -y
    sudo apt install -y python3-pip
    python3 -m pip install pip --upgrade --user
    python3 -m pip install tensorflow --user
"
36200,Copyright years in the files,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.


Umm, a lot of the files have copyright 2015 (or some other year) shouldn't they be 2020?
I dont really know whether this is right or wrong... I'm just posting what I saw in the files"
36199,./tensorflow/lite/tools/make/download_dependencies.sh: line 59: 1: Usage: download_and_extract URL DIR,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 4.19.0-6-amd64 #1 SMP Debian 4.19.67-2+deb10u2 (2019-11-11) x86_64 GNU/Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): from source at https://github.com/tensorflow/tensorflow/archive/v2.1.0.tar.gz
- TensorFlow version: 2.1.0
- Python version: 3.7.3 (but not relevant to this issue as I am only compiling tf lite using make)
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): gcc (Debian 8.3.0-6) 8.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a



**Describe the problem**

```
$ wget https://github.com/tensorflow/tensorflow/archive/v2.1.0.tar.gz
$ tar -zxvf v2.1.0.tar.gz 
$ cd tensorflow-2.1.0/
$ ./tensorflow/lite/tools/make/download_dependencies.sh 
./tensorflow/lite/tools/make/download_dependencies.sh: line 59: 1: Usage: download_and_extract URL DIR
```


**Any other info / logs**
It seems that the `EIGEN_URL` variable is not being set correctly because the following and `grep -o` fails in the following command: `EIGEN_URL=""$(grep -o 'http.*bitbucket.org/eigen/eigen/get/.*tar\.gz' ""${BZL_FILE_PATH}"" | grep -v mirror.tensorflow | head -n1)""`. The eigen URL should be updated to something like `https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen/`.

```
bash -x ./tensorflow/lite/tools/make/download_dependencies.sh 
+ set -e
+++ dirname ./tensorflow/lite/tools/make/download_dependencies.sh
++ cd ./tensorflow/lite/tools/make
++ pwd
+ SCRIPT_DIR=/home/tlm/_tmp/imfind/tf/tensorflow-2.1.0/tensorflow/lite/tools/make
+ cd /home/tlm/_tmp/imfind/tf/tensorflow-2.1.0/tensorflow/lite/tools/make/../../../..
+ DOWNLOADS_DIR=tensorflow/lite/tools/make/downloads
+ BZL_FILE_PATH=tensorflow/workspace.bzl
+ '[' '!' -f tensorflow/workspace.bzl ']'
++ grep -o 'http.*bitbucket.org/eigen/eigen/get/.*tar\.gz' tensorflow/workspace.bzl
++ grep -v mirror.tensorflow
++ head -n1
+ EIGEN_URL=
++ grep -o 'https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/gemmlowp/.*zip' tensorflow/workspace.bzl
++ head -n1
+ GEMMLOWP_URL=https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/gemmlowp/archive/12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3.zip
+ GOOGLETEST_URL=https://github.com/google/googletest/archive/release-1.8.0.tar.gz
++ grep -o 'https://github.com/abseil/abseil-cpp/.*tar.gz' tensorflow/workspace.bzl
++ head -n1
+ ABSL_URL=https://github.com/abseil/abseil-cpp/archive/43ef2148c0936ebf7cb4be6b19927a9d9d145b8f.tar.gz
+ NEON_2_SSE_URL=https://github.com/intel/ARM_NEON_2_x86_SSE/archive/master.zip
+ FARMHASH_URL=https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/farmhash/archive/816a4ae622e964763ca0862d9dbd19324a1eaf45.tar.gz
+ FLATBUFFERS_URL=https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz
+ FFT2D_URL=https://storage.googleapis.com/mirror.tensorflow.org/www.kurims.kyoto-u.ac.jp/~ooura/fft2d.tgz
+ download_and_extract '' tensorflow/lite/tools/make/downloads/eigen
+ local 'usage=Usage: download_and_extract URL DIR'
./tensorflow/lite/tools/make/download_dependencies.sh: line 59: 1: Usage: download_and_extract URL DIR
```"
36198,model.summary() Does not Work in Some Cases,"Tf Version 2.1
Having been suggested by @reedwm I am filing this bug. Please see the last comment in issue #35441 for details.
/CC @reedwm"
36197,not even getting detected for the python3.8.1,"![image](https://user-images.githubusercontent.com/48592314/73117091-98bda180-3f66-11ea-9ad4-163ea27b8904.png)
"
36196,not getting installed in python 3.8.1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
36194,Can not train a BERT fine tuning model using TF on TPU,"I am training a tensorflow model defined as follows on a TPU. But when it starts training, I run into this error: 
UnimplementedError: {{function_node __inference_distributed_function_299068}} Compilation failure: Dynamic concatenation is not supported yet: %reshape.3993 = s32[832]{0} reshape(s32[2,416]{1,0} %reshape.621), metadata={op_type=""Reshape"" op_name=""Reshape_1636""}
	TPU compilation failed
	 [[{{node tpu_compile_succeeded_assert/_14468581000393706631/_10}}]].

I searched on google but did not find any clue to solve this. Any ideas?

def create_model(pretrained_path):

    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)

    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
    config = BertConfig() 

    config.output_hidden_states = False  
    bert_model = TFBertModel.from_pretrained(pretrained_path, config=config)
    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]
    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]
    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)
    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)

    x =  tf.keras.layers.Concatenate()([q, a])

    x = tf.keras.layers.Dropout(0.2)(x)

    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)

    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn, ], outputs=x)
    return model"
36193,ModuleNotFoundError: No module named 'tensorflow_estimator.contrib',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary):
I have installed tensorflow into my conda environment via `pip install tensorflow`
It shows as version 2.1.0 when I list the installed version, but when I import tensorflow within a Python interpreter I see version 1.15.0-rc1
```
$ conda list tensorflow
# packages in environment at /home/james/miniconda3/envs/deeplab:
#
# Name                    Version                   Build  Channel
tensorflow                2.1.0                    pypi_0    pypi
tensorflow-estimator      2.1.0                    pypi_0    pypi

$ python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
v1.15.0-rc1-42-g5adb433 1.15.0-rc2
```
- TensorFlow version (use command below): I am not sure, based on the above.
- Python version: Python 3.7.6

**Describe the current behavior**
I am trying to run the [DeepLab example](https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/pascal.md) using this command:
```
$ python deeplab/train.py \
    --logtostderr \
    --training_number_of_steps=30000 \
    --train_split=""train"" \
    --model_variant=""xception_65"" \
    --atrous_rates=6 \
    --atrous_rates=12 \
    --atrous_rates=18 \
    --output_stride=16 \
    --decoder_output_stride=4 \
    --train_crop_size=""513,513"" \
    --train_batch_size=1 \
    --dataset=""basins"" \
    --tf_initial_checkpoint=/home/james/deeplab/pretrained/x65-b2u1s2p-d48-2-3x256-sc-cr300k_init.ckpt.data-00000-of-00001 \
    --train_logdir=./deeplab/datasets/basins/exp/train_on_train_set/train \
    --dataset_dir=./deeplab/datasets/basins
```
I get the following output/error:
```
Traceback (most recent call last):
  File ""deeplab/train.py"", line 26, in <module>
    from tensorflow.contrib import quantize as contrib_quantize
  File ""/home/james/.local/lib/python3.7/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/home/james/.local/lib/python3.7/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/home/james/miniconda3/envs/deeplab/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/home/james/.local/lib/python3.7/site-packages/tensorflow_core/contrib/__init__.py"", line 48, in <module>
    from tensorflow.contrib import estimator
  File ""/home/james/.local/lib/python3.7/site-packages/tensorflow_core/contrib/estimator/__init__.py"", line 30, in <module>
    from tensorflow_estimator.contrib import estimator
ModuleNotFoundError: No module named 'tensorflow_estimator.contrib'
```

**Describe the expected behavior**
I am hoping that the model will train as advertised.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
$ python -c 'import tensorflow as tf; tf.contrib.summary'
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
36192,TensorFlow fit() and GradientTape - number of epochs are different,"if I define the architecture of a neural network using only dense fully connected layers and train them such that there are two models which are trained using **model.fit()** and **GradientTape**. Both the methods of training use the *same* model architecture.

The randomly initialized weights are shared between the two models and all other parameters such as optimizer, loss function and metrics are also the same.

Dimensions of training and testing sets are:
X_train = (960, 4), y_train = (960,), X_test = (412, 4) & y_test = (412,)

    import pandas as pd, numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
    from sklearn.preprocessing import LabelEncoder
    from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
    
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense
    import tensorflow_model_optimization as tfmot
    from tensorflow_model_optimization.sparsity import keras as sparsity
    
    
    def create_nn():
        """"""
        Function to create a
        Neural Network
        """"""
        model = Sequential()                                                    
    
        model.add(
            Dense(
                units = 4, activation = 'relu',
                kernel_initializer = tf.keras.initializers.GlorotNormal(),
                input_shape = (4,)
            )
        )
    
        model.add(
            Dense(
                units = 3, activation = 'relu',
                kernel_initializer = tf.keras.initializers.GlorotNormal()
            )
        )
    
        model.add(
            Dense(
                units = 1, activation = 'sigmoid'
            )
        )
    
        """"""
        # Compile the defined NN model above-
        model.compile(
            loss = 'binary_crossentropy',  # loss = 'categorical_crossentropy'
            optimizer = tf.keras.optimizers.Adam(lr = 0.001),
            metrics=['accuracy']
        )
        """"""
    
        return model
    
    
    # Instantiate a model- model = create_nn()
    
    # Save weights for fair comparison- model.save_weights(""Random_Weights.h5"", overwrite=True)
    
    
    # Create datasets to be used for GradientTape-
    # Use tf.data to batch and shuffle the dataset train_ds = tf.data.Dataset.from_tensor_slices(
        (X_train, y_train)).shuffle(100).batch(32)
    
    test_ds = tf.data.Dataset.from_tensor_slices(
        (X_test, y_test)).shuffle(100).batch(32)
    
    # Define early stopping- callback = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss', patience=3,
        min_delta = 0.001, mode = 'min' )
    
    # Train defined model- history_orig = model.fit(
        x = X_train, y = y_train,
        batch_size = 32, epochs = 500,
        validation_data = (X_test, y_test),
        callbacks = [callback],
        verbose = 1 )
    
    
    # Instantiate a model- model_gt = create_nn()
    
    # Restore random weights as used by the previous model for fair comparison- model_gt.load_weights(""Random_Weights.h5"")
    
    
    # Choose an optimizer and loss function for training- loss_fn = tf.keras.losses.BinaryCrossentropy() optimizer = tf.keras.optimizers.Adam(lr = 0.001)
    
    # Select metrics to measure the error & accuracy of model.
    # These metrics accumulate the values over epochs and then
    # print the overall result- train_loss = tf.keras.metrics.Mean(name = 'train_loss') train_accuracy = tf.keras.metrics.BinaryAccuracy(name = 'train_accuracy')
    
    test_loss = tf.keras.metrics.Mean(name = 'test_loss') test_accuracy = tf.keras.metrics.BinaryAccuracy(name = 'train_accuracy')
    
    
    # Use tf.GradientTape to train the model-
    
    @tf.function def train_step(data, labels):
        """"""
        Function to perform one step of Gradient
        Descent optimization
        """"""
    
        with tf.GradientTape() as tape:
            predictions = model_gt(data)
            loss = loss_fn(labels, predictions)
    
        gradients = tape.gradient(loss, model_gt.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model_gt.trainable_variables))
    
        train_loss(loss)
        train_accuracy(labels, predictions)
    
    
    @tf.function def test_step(data, labels):
        """"""
        Function to test model performance
        on testing dataset
        """"""
        
        predictions = model_gt(data)
        t_loss = loss_fn(labels, predictions)
    
        test_loss(t_loss)
        test_accuracy(labels, predictions)
    
    
    EPOCHS = 100
    
    # User input- minimum_delta = 0.001 patience = 3
    
    patience_val = np.zeros(patience)
    
    
    # Dictionary to hold scalar metrics- history = {}
    
    history['accuracy'] = np.zeros(EPOCHS) history['val_accuracy'] = np.zeros(EPOCHS) history['loss'] = np.zeros(EPOCHS) history['val_loss'] = np.zeros(EPOCHS)
    
    for epoch in range(EPOCHS):
        # Reset the metrics at the start of the next epoch
        train_loss.reset_states()
        train_accuracy.reset_states()
        test_loss.reset_states()
        test_accuracy.reset_states()
    
        for x, y in train_ds:
            train_step(x, y)
    
        for x_t, y_t in test_ds:
            test_step(x_t, y_t)
    
        template = 'Epoch {0}, Loss: {1:.4f}, Accuracy: {2:.4f}, Test Loss: {3:.4f}, Test Accuracy: {4:4f}'
    
        history['accuracy'][epoch] = train_accuracy.result()
        history['loss'][epoch] = train_loss.result()
        history['val_loss'][epoch] = test_loss.result()
        history['val_accuracy'][epoch] = test_accuracy.result()
    
        print(template.format(epoch + 1, 
                              train_loss.result(), train_accuracy.result()*100,
                              test_loss.result(), test_accuracy.result()*100))
    
        if epoch > 2:
            # Computes absolute differences between 3 consecutive loss values-
            differences = np.abs(np.diff(history['val_loss'][epoch - 3:epoch], n = 1))
            
            # Checks whether the absolute differences is greater than 'minimum_delta'-
            check =  differences > minimum_delta
            
            # print('differences: {0}'.format(differences))
            
            # Count unique element with it's counts-
            # elem, count = np.unique(check, return_counts=True)
            # print('\nelem = {0}, count = {1}'.format(elem, count))
            
            if np.all(check == False):
            # if elem.all() == False and count == 2:
                print(""\n\nEarlyStopping Evoked! Stopping training\n\n"")
                break


In ""model.fit()"" method, it takes around 82 epochs, while GradientTape method takes 52 epochs.

Why is there this discrepancy in the number of epochs?

Thanks!


Complete code in GitHub-
https://github.com/arjun-majumdar/tensorflow_codes/blob/master/EarlyStopping_with_GradientTape_using_TensorFlow_2.ipynb

"
36189,"Unable to build ""Hello World"" for ESP32","@tensorflow/micro

**System information**
- Host OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from: source
- Tensorflow version: ca0d5142f640d42037f22367ce3530b6b7a23b44
- Target platform: ESP32

**Describe the problem**
Building the example Hello World for ESp32 fails.

**Please provide the exact sequence of commands/steps when you ran into the problem**
1. I successfully generated example by command:
`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project`

2. I run building project by command:
`idf.py build`

3. I receved compilation error:
`[ 84%] Building C object esp-idf/protocomm/CMakeFiles/__idf_protocomm.dir/proto-c/constants.pb-c.c.obj
In file included from /home/dmytro/esp/projects/hello_world_tf/components/tfmicro/tensorflow/lite/micro/kernels/svdf.cc:25:
/home/dmytro/esp/projects/hello_world_tf/components/tfmicro/tensorflow/lite/micro/kernels/activation_utils.h: In function 'float tflite::ops::micro::ActivationValFloat(TfLiteFusedActivation, float)':
/home/dmytro/esp/projects/hello_world_tf/components/tfmicro/tensorflow/lite/micro/kernels/activation_utils.h:45:1: error: control reaches end of non-void function [-Werror=return-type]
 }
 ^
`
"
36188,Functions removed from tensorflow namespace between 1.15.0 and 1.15.1,"I'm not sure if this was intended behavior, but if so it wasn't documented in the 1.15.1 release notes. I first hit this for `tf.set_random_seed()`, but it even effects the sample code for getting the version.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): pip (binary wheel)
- TensorFlow version (use command below): `v1.15.0-85-gfdb85890df 1.15.1`
- Python version:` Python 3.7.3 (default, Mar 27 2019, 09:23:15)  [Clang 10.0.1 (clang-1001.0.46.3)]`
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Methods/variables such as tensorflow.set_random_seed and tensorflow.VERSION were present in tensorflow 1.15.0 but are removed in 1.15.1

**Describe the expected behavior**
The interface should be identical between 1.15.0 and 1.15.1.

**Code to reproduce the issue**
```shell
# Works as expected
$ pip install -q tensorflow==1.15.0
$ python -c ""import tensorflow as tf; tf.set_random_seed(42)""
# Same command fails on 1.15.1
$ pip install -q tensorflow==1.15.1
$ python -c ""import tensorflow as tf; tf.set_random_seed(42)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'set_random_seed'
```
Using the provided sample code to get the tensorflow version:
```shell
$ pip install -q tensorflow==1.15.0
$ python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.15.0-rc3-22-g590d6eef7e 1.15.0

# TF 1.0 interface fails
$ pip install -q tensorflow==1.15.1
$ python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: module 'tensorflow' has no attribute 'GIT_VERSION'

# TF 2.0 interface succeeds
$ python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
v1.15.0-85-gfdb85890df 1.15.1
```
"
36185,tf.data.Dataset.from_tensor_slices throws error when using an array of numbers instead of a string,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (64 bit), Windows-10-10.0.18362-SP0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

I'm using tf.data.Dataset.from_tensor_slices to create a dataset from a dataframe. The dataframe has a column called ""Text"", which usually contains strings as entries. Executing it like that works fine. However, when I encode those strings to arrays of number, the function throws an error.

The number of the failed arrays is 2312, while the complete number of arrays is actually around 3600. So not every array is failing.

**Describe the expected behavior**
It should work without error.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
def df_to_dataset(dataframe):
  dataframe = dataframe.copy()
  labels = dataframe.pop('Emotion')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  return ds
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
Traceback (most recent call last):
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\data\util\structure.py"", line 91, in normalize_element
    spec = type_spec_from_value(t, use_fallback=False)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\data\util\structure.py"", line 457, in type_spec_from_value
    (element, type(element).__name__))
TypeError: Could not build a TypeSpec for 2666    [1424, 11572, 1372, 11572, 2289, 11584, 11572,...
2091    [11637, 11572, 690, 11572, 11, 11572, 3376, 11...
832     [11613, 11572, 72, 11572, 65, 11572, 472, 1157...
2572    [355, 11572, 11613, 11572, 36, 11572, 11645, 1...
2187              [182, 11572, 405, 11572, 1, 11572, 335]
                              ...
3427    [4416, 11572, 37, 11572, 4408, 11572, 2, 11572...
2868    [20, 11572, 23, 11572, 11637, 11572, 254, 1157...
1806    [6870, 11572, 9118, 11572, 5883, 11572, 5190, ...
1266    [44, 11572, 525, 11572, 702, 11572, 63, 11572,...
3047    [1795, 11572, 46, 11572, 503, 11572, 5, 11572,...
Name: Text, Length: 2312, dtype: object with type Series

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "".\Mood recognition.py"", line 85, in <module>
    train_ds = df_to_dataset(train, batch_size=batch_size)
  File "".\Mood recognition.py"", line 70, in df_to_dataset
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 435, in from_tensor_slices
    return TensorSliceDataset(tensors)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 2354, in __init__
    element = structure.normalize_element(element)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\data\util\structure.py"", line 96, in normalize_element
    ops.convert_to_tensor(t, name=""component_%d"" % i))
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1184, in convert_to_tensor
    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1242, in convert_to_tensor_v2
    as_ref=False)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1296, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 286, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 227, in constant
    allow_broadcast=True)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 235, in _constant_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 96, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: Can't convert non-rectangular Python sequence to Tensor.
```

"
36184,./build_aarch64_lib.sh fails in densify.cc,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Unable to build the  TensorFlow Lite static library for ARM64  using the master branch of the repo. It fails during compilation (logs added at the bottom)

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Using the following instructions to build tensorflow lite for arm64
https://www.tensorflow.org/lite/guide/build_arm64

**Any other info / logs**

`aarch64-linux-gnu-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv8-a -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/root/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/root/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/root/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/root/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/root/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/root/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/root/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/root/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include \
-o /root/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal /root/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/obj/tensorflow/lite/examples/minimal/minimal.o \
 /root/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lrt -lstdc++ -lpthread -lm -ldl
/root/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a(densify.o): In function `tflite::ops::builtin::densify::Eval(TfLiteContext*, TfLiteNode*)':
densify.cc:(.text+0x378): undefined reference to `tflite::optimize::sparsity::FormatConverter<signed char>::FormatConverter(std::vector<int, std::allocator<int> > const&, TfLiteSparsity const&)'
densify.cc:(.text+0x384): undefined reference to `tflite::optimize::sparsity::FormatConverter<signed char>::SparseToDense(signed char const*)'
densify.cc:(.text+0x5f0): undefined reference to `tflite::optimize::sparsity::FormatConverter<float>::FormatConverter(std::vector<int, std::allocator<int> > const&, TfLiteSparsity const&)'
densify.cc:(.text+0x5fc): undefined reference to `tflite::optimize::sparsity::FormatConverter<float>::SparseToDense(float const*)'
collect2: error: ld returned 1 exit status
tensorflow/lite/tools/make/Makefile:286: recipe for target '/root/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal' failed
make: *** [/root/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/bin/minimal] Error 1
make: *** Waiting for unfinished jobs....
make: Leaving directory '/root/tensorflow'`"
36183,TFLite new quantizer brokes ALBERT model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): docker container with ubuntu 16.04, running on host with fedora 31
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.2.0-dev20200119


**Command used to run the converter or code if you’re using the Python API**

```
config = tr.AlbertConfig.from_pretrained(model_dir)
    model = tr.TFAlbertForSequenceClassification.from_pretrained(
        model_dir + 'pytorch_model.bin',
        from_pt=True,
        config=config
    )
    
input_ids: tf.TensorSpec = tf.TensorSpec((1, 128), dtype=tf.int32, name='input_ids')
attn_mask: tf.TensorSpec = tf.TensorSpec((1, 128), dtype=tf.int32, name='attn_mask')
model._set_inputs([input_ids, attn_mask], training=False)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.experimental_new_converter = True
converter.experimental_new_quantizer = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]

converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
converter.representative_dataset = representative_dataset
tflite_model = converter.convert()

```

**The output from the converter invocation**

```
2020-01-24 16:32:16.530758: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2020-01-24 16:32:16.554102: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-01-24 16:32:16.557035: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-01-24 16:32:16.557052: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: bb9f01c415b3
2020-01-24 16:32:16.557059: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: bb9f01c415b3
2020-01-24 16:32:16.557145: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 435.21.0
2020-01-24 16:32:16.557160: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 435.21.0
2020-01-24 16:32:16.557164: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 435.21.0
2020-01-24 16:32:16.557353: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-24 16:32:16.574660: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 3892860000 Hz
2020-01-24 16:32:16.575312: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562d071745f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-01-24 16:32:16.575337: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-01-24 16:32:18.432 | INFO     | __main__:<module>:114 - loaded model & tokenizer
2020-01-24 16:32:18.443 | INFO     | __main__:<module>:117 - loaded representative dataset
2020-01-24 16:32:19.227068: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-01-24 16:32:19.227222: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-01-24 16:32:19.254757: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: graph_to_optimize
2020-01-24 16:32:19.254798: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.004ms.
2020-01-24 16:32:19.254802: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-24 16:32:21.189883: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-01-24 16:32:21.190007: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-01-24 16:32:22.539819: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: graph_to_optimize
2020-01-24 16:32:22.539870: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 2861 nodes (-184), 3659 edges (-361), time = 906.902ms.
2020-01-24 16:32:22.539875: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 2861 nodes (0), 3659 edges (0), time = 359.704ms.
INFO:absl:Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False
2020-01-24 16:35:00.745 | INFO     | __main__:<module>:123 - model conversion finished, writing to file..
2020-01-24 16:35:00.752 | INFO     | __main__:<module>:128 - All Done!
```

**Also, please include a link to the saved model or GraphDef**

Sadly, tensorflow fails to save my model. I'm using fine-tuned version of ALBERT-base-v2(for sequence classification) from huggingface\transformers.

**Failure details**
Conversion went without any failures, but produced model is completely wrong, as it is 18x slower than model produced without calibration and experimental_new_quantizer(but with experimantal_new_converter) and this model produces trashed metrics:
```
# experimental_new_quantizer=True, experimental_new_converter=True and calibration is on
{""accuracy"": 0.025161092359619514, ""micro-f1"": 0.3419799704482023, ""macro-f1"": 0.17553049991957947}
inference time: 18 sec \ sample (on x86_64)
# experimantal_new_quantizer=False, experimental_new_converter=True and no calibration
{""accuracy"": 0.4894139306535747, ""micro-f1"": 0.6913144244564617, ""macro-f1"": 0.5892280589520447}
inference time: 1 sec \ sample (on x86_64)
```"
36182,"Broken link ""Share your TensorFlow Lite story""","## URL(s) with the issue:

https://www.tensorflow.org/lite/

## Description of issue (what needs changing):

This button 
![image](https://user-images.githubusercontent.com/2697890/73086114-09ba7600-3ee1-11ea-9b27-1f75816b4e94.png)

links to 403 page
![image](https://user-images.githubusercontent.com/2697890/73086137-1a6aec00-3ee1-11ea-9fa1-0ace2995a872.png)"
36181,AttributeError: 'Tensor' object has no attribute 'log_prob',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina (Version: 10.15.2 (19C57))
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1
- Python version: 3.7.5
- GPU model and memory: Intel Iris Pro 1536 MB

**Describe the current behavior**

I get the error

> AttributeError: 'Tensor' object has no attribute 'log_prob'

with TensorFlow Probability 0.9 (and TF 2.1).

**Describe the expected behavior**

No error.

**Code to reproduce the issue**

The following code

```
import tensorflow as tf
import tensorflow_probability as tfp
from tensorflow_probability import distributions as tfd


def get_mnist_data(normalize=True):
    img_rows, img_cols = 28, 28
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

    if tf.keras.backend.image_data_format() == 'channels_first':
        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
        input_shape = (1, img_rows, img_cols)
    else:
        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
        input_shape = (img_rows, img_cols, 1)

    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')

    if normalize:
        x_train /= 255
        x_test /= 255

    return x_train, y_train, x_test, y_test, input_shape


def get_bayesian_cnn(input_shape, num_classes=10):
    model_input = tf.keras.layers.Input(shape=input_shape)

    # kernel_divergence_fn=None to solve a symbolic exception.
    x = tfp.layers.Convolution2DFlipout(6, kernel_size=(5, 5), padding=""SAME"", activation=tf.nn.relu,
                                        kernel_divergence_fn=None)(model_input)
    x = tf.keras.layers.Flatten()(x)
    x = tfp.layers.DenseFlipout(84, activation=tf.nn.relu)(x)
    x = tfp.layers.DenseFlipout(num_classes)(x)

    model_output = tfp.layers.DistributionLambda(lambda t: tfd.Categorical(logits=t, validate_args=True))(x)

    model = tf.keras.Model(model_input, model_output)

    return model


def neg_log_likelihood(y_true, y_pred):
    return -tf.reduce_mean(y_pred.log_prob(tf.cast(tf.argmax(y_true, axis=-1), tf.int32)))


def train():
    x_train, y_train, x_test, y_test, input_shape = get_mnist_data()

    model = get_bayesian_cnn(input_shape=input_shape)

    model.compile(optimizer=tf.keras.optimizers.Adam(), loss=neg_log_likelihood,
                  metrics=[neg_log_likelihood])

    model.fit(x_train, y_train, batch_size=128, epochs=1, verbose=1)


if __name__ == ""__main__"":
    train()
```

**Comments**

This error seems to be due to the fact that `y_pred` is a tensor when the loss is called, while it should be a distribution. Meanwhile, I found a [question on Stack Overflow related to the third issue I mentioned above](https://stackoverflow.com/q/59743872/3924118). 

(_This is a duplicate issue of https://github.com/tensorflow/probability/issues/742, but, for completeness, I decided to open it here too._)"
36179,tensorflow 1.15.1 produces V2 style pip with V2 enabled by default,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
Macbook Pro
- TensorFlow installed from (source or binary):
source
- TensorFlow version:
1.15
- Python version:
2.7
- Installed using virtualenv? pip? conda?:
pip
- Bazel version (if compiling from source):
bazel-0.26.1
- GCC/Compiler version (if compiling from source):
clang
- CUDA/cuDNN version:
No
- GPU model and memory:
No



**Describe the problem**

I am building tensorflow 1.15 wheel with MKL support from sources, branch r1.15. After I installed wheel ""pip install tensorflow-1.15.1-cp27-cp27m-macosx_10_14_x86_64.whl"", I see no backward compatibility.

```
keras.backend.get_session()
AttributeError: 'module' object has no attribute 'get_session'
```

I suspect that the following diff https://github.com/tensorflow/tensorflow/commit/e686d9c0bafce7749a7410aa2c0304c30e498f1d disabled V1 style pip.

As result package I produces is not behaving as official 1.15 package.

is it expected?

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build --local_ram_resources=2048 --config=mkl -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma //tensorflow/tools/pip_package:build_pip_package

Note that I don't use --config=v1 because branch itself is 1.15.

Then build a wheel 

bazel-bin/tensorflow/tools/pip_package/build_pip_package ~/tf_wheel 

it produces tensorflow-1.15.1-cp27-cp27m-macosx_10_14_x86_64.whl
After I install it, v1 code doesn't work and I see that V2 is enabled:

python
Python 2.7.16 (default, Sep  2 2019, 11:59:44)
[GCC 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from tensorflow.python import tf2
>>> print(tf2.enabled())
True

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

pip show tensorflow
Name: tensorflow
Version: 1.15.1
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /Users/vkuzmin/gocode/src/code.uber.internal/data/michelangelo-deeplearning-inference/env/lib/python2.7/site-packages
Requires: keras-preprocessing, functools32, tensorflow-estimator, wheel, six, keras-applications, astor, tensorboard, wrapt, opt-einsum, termcolor, grpcio, mock, numpy, absl-py, gast, protobuf, backports.weakref, enum34, google-pasta
Required-by:

"
36178,How to build Tensorflow lite shared library ?,"Hello

I am looking for a modification of build to be created share ""so"" library for TensorFlow-lite -please can help what/where to modify to create a shared library?

Thank you."
36177,While compiling `benchmark_model_plus_flex`: error: target 'saved_model_portable_proto' not declared in package ,"- other issues: I found similar issues https://github.com/tensorflow/tensorflow/issues/30226 - with similar error message

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: na
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0, 2.1
- Python version: 3.6.6
- Installed using virtualenv
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): clang, ndk *21*
- CUDA/cuDNN version: na
- GPU model and memory: na

I want to compile `benchmark_model_plus_flex` for tf v2.1 with ndk 21 but it fails.

I also try to compile both `benchmark_model` and `benchmark_model_plus_flex` for ndk18 (as it is the last one offically supported) but it fails with `error: undefined reference to 'uselocale'` so I switched to ndk21 where at least `benchmark_model` is building.
 
Command:
```bash
bazel build -c opt   --config=android_arm   --cxxopt='--std=c++11'  \
tensorflow/lite/tools/benchmark:benchmark_model_plus_flex
```
LOG:
```
WARNING: /home/k.nowicki/code/tensorflow-master/tensorflow/core/kernels/BUILD:6549:12: in srcs attribute of cc_library rule //tensorflow/core/kernels:android_tensorflow_kernels: please do not import '//tenso
rflow/c/kernels:bitcast_op.cc' directly. You should either move the file to this package or depend on an appropriate rule there
WARNING: /home/k.nowicki/code/tensorflow-master/tensorflow/core/BUILD:2024:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib: please do not import '//tensorflow/c/kernels:ops/
bitcast.cc' directly. You should either move the file to this package or depend on an appropriate rule there
ERROR: /home/k.nowicki/code/tensorflow-master/tensorflow/cc/saved_model/BUILD:40:1: no such target '//tensorflow/core:saved_model_portable_proto': target 'saved_model_portable_proto' not declared in package 
'tensorflow/core' defined by /home/k.nowicki/code/tensorflow-master/tensorflow/core/BUILD and referenced by '//tensorflow/cc/saved_model:reader'
ERROR: Analysis of target '//tensorflow/lite/tools/benchmark:benchmark_model_plus_flex' failed; build aborted: Analysis failed
INFO: Elapsed time: 7.415s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (113 packages loaded, 10710 targets configured)
```
"
36176,ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType).,"Hello, I'm trying to convert my pytorch model to keras and I have ready onnx file for It. When I started to converting onnx to keras, I've got next error:
```
DEBUG:onnx2keras:Check if all inputs are available:
DEBUG:onnx2keras:Check input 0 (name 645).
DEBUG:onnx2keras:Check input 1 (name 646).
DEBUG:onnx2keras:... found all, continue
DEBUG:onnx2keras:mul:Convert inputs to Keras/TF layers if needed.
WARNING:onnx2keras:mul:Failed to use keras.layers.Multiply. Fallback to TF lambda.
WARNING:tensorflow:Layer 647 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

WARNING:tensorflow:Layer 647 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

DEBUG:onnx2keras:######
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Converting ONNX operation
DEBUG:onnx2keras:type: Cast
DEBUG:onnx2keras:node_name: 648
DEBUG:onnx2keras:node_params: {'to': 7, 'change_ordering': False, 'name_policy': None}
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Check if all inputs are available:
DEBUG:onnx2keras:Check input 0 (name 647).
DEBUG:onnx2keras:... found all, continue
DEBUG:onnx2keras:######
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Converting ONNX operation
DEBUG:onnx2keras:type: Cast
DEBUG:onnx2keras:node_name: 649
DEBUG:onnx2keras:node_params: {'to': 11, 'change_ordering': False, 'name_policy': None}
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Check if all inputs are available:
DEBUG:onnx2keras:Check input 0 (name 648).
DEBUG:onnx2keras:... found all, continue
DEBUG:onnx2keras:######
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Converting ONNX operation
DEBUG:onnx2keras:type: Constant
DEBUG:onnx2keras:node_name: 650
DEBUG:onnx2keras:node_params: {'value': array(1.), 'change_ordering': False, 'name_policy': None}
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Check if all inputs are available:
DEBUG:onnx2keras:... found all, continue
DEBUG:onnx2keras:######
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Converting ONNX operation
DEBUG:onnx2keras:type: Div
DEBUG:onnx2keras:node_name: 651
DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Check if all inputs are available:
DEBUG:onnx2keras:Check input 0 (name 650).
DEBUG:onnx2keras:Check input 1 (name 649).
DEBUG:onnx2keras:... found all, continue
DEBUG:onnx2keras:div:Convert inputs to Keras/TF layers if needed.
WARNING:tensorflow:Layer 651 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

WARNING:tensorflow:Layer 651 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

DEBUG:onnx2keras:######
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Converting ONNX operation
DEBUG:onnx2keras:type: Constant
DEBUG:onnx2keras:node_name: 652
DEBUG:onnx2keras:node_params: {'value': array(224.), 'change_ordering': False, 'name_policy': None}
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Check if all inputs are available:
DEBUG:onnx2keras:... found all, continue
DEBUG:onnx2keras:######
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Converting ONNX operation
DEBUG:onnx2keras:type: Mul
DEBUG:onnx2keras:node_name: 653
DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Check if all inputs are available:
DEBUG:onnx2keras:Check input 0 (name 651).
DEBUG:onnx2keras:Check input 1 (name 652).
DEBUG:onnx2keras:... found all, continue
DEBUG:onnx2keras:mul:Convert inputs to Keras/TF layers if needed.
WARNING:onnx2keras:mul:Failed to use keras.layers.Multiply. Fallback to TF lambda.
WARNING:tensorflow:Layer 653 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

WARNING:tensorflow:Layer 653 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

DEBUG:onnx2keras:######
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Converting ONNX operation
DEBUG:onnx2keras:type: Cast
DEBUG:onnx2keras:node_name: 654
DEBUG:onnx2keras:node_params: {'to': 7, 'change_ordering': False, 'name_policy': None}
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Check if all inputs are available:
DEBUG:onnx2keras:Check input 0 (name 653).
DEBUG:onnx2keras:... found all, continue
DEBUG:onnx2keras:######
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Converting ONNX operation
DEBUG:onnx2keras:type: Mul
DEBUG:onnx2keras:node_name: 655
DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Check if all inputs are available:
DEBUG:onnx2keras:Check input 0 (name 654).
DEBUG:onnx2keras:Check input 1 (name 654).
DEBUG:onnx2keras:... found all, continue
DEBUG:onnx2keras:mul:Convert inputs to Keras/TF layers if needed.
WARNING:onnx2keras:mul:Failed to use keras.layers.Multiply. Fallback to TF lambda.
DEBUG:onnx2keras:######
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Converting ONNX operation
DEBUG:onnx2keras:type: Unsqueeze
DEBUG:onnx2keras:node_name: 657
DEBUG:onnx2keras:node_params: {'axes': [0], 'change_ordering': False, 'name_policy': None}
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Check if all inputs are available:
DEBUG:onnx2keras:Check input 0 (name 639).
DEBUG:onnx2keras:... found all, continue
DEBUG:onnx2keras:unsqueeze:Work with numpy types.
DEBUG:onnx2keras:######
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Converting ONNX operation
DEBUG:onnx2keras:type: Unsqueeze
DEBUG:onnx2keras:node_name: 659
DEBUG:onnx2keras:node_params: {'axes': [0], 'change_ordering': False, 'name_policy': None}
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Check if all inputs are available:
DEBUG:onnx2keras:Check input 0 (name 655).
DEBUG:onnx2keras:... found all, continue
DEBUG:onnx2keras:######
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Converting ONNX operation
DEBUG:onnx2keras:type: Concat
DEBUG:onnx2keras:node_name: 660
DEBUG:onnx2keras:node_params: {'axis': 0, 'change_ordering': False, 'name_policy': None}
DEBUG:onnx2keras:...
DEBUG:onnx2keras:Check if all inputs are available:
DEBUG:onnx2keras:Check input 0 (name 657).
DEBUG:onnx2keras:Check input 1 (name 1057).
DEBUG:onnx2keras:The input not found in layers / model inputs.
DEBUG:onnx2keras:Found in weights, add as a numpy constant.
DEBUG:onnx2keras:Check input 2 (name 659).
DEBUG:onnx2keras:... found all, continue
DEBUG:onnx2keras:concat:Concat Keras layers.
WARNING:onnx2keras:concat:!!! IMPORTANT INFORMATION !!!
WARNING:onnx2keras:concat:Something goes wrong with concat layers. Will use TF fallback.
WARNING:onnx2keras:concat:---
Traceback (most recent call last):
  File ""C:\Users\1\Anaconda3\lib\site-packages\onnx2keras\reshape_layers.py"", line 110, in convert_concat
    name=keras_name)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\keras\layers\merge.py"", line 705, in concatenate
    return Concatenate(axis=axis, **kwargs)(inputs)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 745, in __call__
    inputs = nest.map_structure(_convert_non_tensor, inputs)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\util\nest.py"", line 535, in map_structure
    structure[0], [func(*x) for x in entries],
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\util\nest.py"", line 535, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 743, in _convert_non_tensor
    return ops.convert_to_tensor(x)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1184, in convert_to_tensor
    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1242, in convert_to_tensor_v2
    as_ref=False)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1296, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\framework\tensor_conversion_registry.py"", line 52, in _default_conversion_function
    return constant_op.constant(value, dtype, name=name)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 227, in constant
    allow_broadcast=True)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 235, in _constant_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 96, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/1/PycharmProjects/untitled/onnx_.py"", line 6, in <module>
    k_model = onnx2keras.onnx_to_keras(model, [""input_data""])
  File ""C:\Users\1\Anaconda3\lib\site-packages\onnx2keras\converter.py"", line 177, in onnx_to_keras
    keras_names
  File ""C:\Users\1\Anaconda3\lib\site-packages\onnx2keras\reshape_layers.py"", line 122, in convert_concat
    layers[node_name] = lambda_layer(layer_input)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 745, in __call__
    inputs = nest.map_structure(_convert_non_tensor, inputs)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\util\nest.py"", line 535, in map_structure
    structure[0], [func(*x) for x in entries],
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\util\nest.py"", line 535, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 743, in _convert_non_tensor
    return ops.convert_to_tensor(x)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1184, in convert_to_tensor
    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1242, in convert_to_tensor_v2
    as_ref=False)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1296, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\framework\tensor_conversion_registry.py"", line 52, in _default_conversion_function
    return constant_op.constant(value, dtype, name=name)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 227, in constant
    allow_broadcast=True)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 235, in _constant_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""C:\Users\1\Anaconda3\lib\site-packages\tensorflow_core\python\framework\constant_op.py"", line 96, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType).
```

There is my code:
```
import onnx
import onnx2keras

model = onnx.load_model(""model.onnx"")

k_model = onnx2keras.onnx_to_keras(model, [""input_data""])
```

And here is my onnx file to reproduce my errors:
https://drive.google.com/file/d/1NjYMbidm5WOB4SeeBw22dGzn-jZIrU2-/view?usp=sharing

Is there a possibillity to convert onnx to keras using onnxruntime?

Thank you."
36175,Low GPU usage while executing CNN with keras (tf backend),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow version (use command below): tensorflow 2.0.0
- Python version: Python 3.7.6
- CUDA/cuDNN version: cuda10.0_0 / cudnn 7.6.5
- GPU model and memory: RTX 2060 - 6GB

**Describe the current behavior**
While executing my conv neural net  I am getting GPU usage of 1%.

**Describe the expected behavior**
As no other application is using my resource I should be able to use my GPU at least at 80-85 % 

**Code to reproduce the issue**
```
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)

    except RuntimeError as e:
        print(e)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Convolution2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense

classifier = Sequential()

classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))

classifier.add(MaxPooling2D(pool_size = (2, 2)))

classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))
classifier.add(MaxPooling2D(pool_size = (2, 2)))

classifier.add(Flatten())

classifier.add(Dense(units = 128, activation = 'relu'))
classifier.add(Dense(units = 1, activation = 'sigmoid'))

classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator(rescale = 1./255)

training_set = train_datagen.flow_from_directory('dataset/training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 4,
                                                 class_mode = 'binary')

test_set = test_datagen.flow_from_directory('dataset/test_set',
                                            target_size = (64, 64),
                                            batch_size = 4,
                                            class_mode = 'binary')

classifier.fit_generator(training_set,
                         steps_per_epoch = 8000,
                         epochs = 25,
                         validation_data = test_set,
                         validation_steps = 2000)
Got error after execution of last section :

classifier.fit_generator(training_set,
                             steps_per_epoch = 8000,
                             epochs = 25,
                             validation_data = test_set,
                             validation_steps = 2000)

```

"
36174,UnknownError: Failed to get convolution algorithm.,"**System information**

- OS Platform and Distribution : Windows 10
- TensorFlow version: tensorflow 2.0.0
- Python version: Python 3.7.6
- Installed using virtualenv? pip? conda?: conda
- CUDA/cuDNN version: cuda10.0_0 / cudnn 7.6.5 
- GPU model and memory: RTX 2060 - 6GB



**Description of problem**
UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]

**Exact sequence of commands / steps that are executed before running into the problem**

**code :**

```
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Convolution2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense

classifier = Sequential()

classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))

classifier.add(MaxPooling2D(pool_size = (2, 2)))

classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))
classifier.add(MaxPooling2D(pool_size = (2, 2)))

classifier.add(Flatten())

classifier.add(Dense(units = 128, activation = 'relu'))
classifier.add(Dense(units = 1, activation = 'sigmoid'))

classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator(rescale = 1./255)

training_set = train_datagen.flow_from_directory('dataset/training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 4,
                                                 class_mode = 'binary')

test_set = test_datagen.flow_from_directory('dataset/test_set',
                                            target_size = (64, 64),
                                            batch_size = 4,
                                            class_mode = 'binary')

classifier.fit_generator(training_set,
                         steps_per_epoch = 8000,
                         epochs = 25,
                         validation_data = test_set,
                         validation_steps = 2000)
```
 
**Got error after execution of last section :** 

```
classifier.fit_generator(training_set,
                             steps_per_epoch = 8000,
                             epochs = 25,
                             validation_data = test_set,
                             validation_steps = 2000)
```

**Traceback**

```
Traceback (most recent call last):

  File ""D:\Machine Learning\Machine Learning A-Z Template Folder\Part 8 - Deep Learning\Section 40 - Convolutional Neural Networks (CNN)\cnn.py"", line 70, in <module>
    validation_steps = 2000)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 1297, in fit_generator
    steps_name='steps_per_epoch')

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\keras\engine\training_generator.py"", line 265, in model_iteration
    batch_outs = batch_function(*batch_data)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 973, in train_on_batch
    class_weight=class_weight, reset_metrics=reset_metrics)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 264, in train_on_batch
    output_loss_metrics=model._output_loss_metrics)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\keras\engine\training_eager.py"", line 311, in train_on_batch
    output_loss_metrics=output_loss_metrics))

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\keras\engine\training_eager.py"", line 252, in _process_single_batch
    training=training))

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\keras\engine\training_eager.py"", line 127, in _model_loss
    outs = model(inputs, **kwargs)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\keras\engine\sequential.py"", line 256, in call
    return super(Sequential, self).call(inputs, training=training, mask=mask)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 708, in call
    convert_kwargs_to_constants=base_layer_utils.call_context().saving)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\keras\engine\network.py"", line 860, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\keras\layers\convolutional.py"", line 197, in call
    outputs = self._convolution_op(inputs, self.kernel)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\ops\nn_ops.py"", line 1134, in __call__
    return self.conv_op(inp, filter)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\ops\nn_ops.py"", line 639, in __call__
    return self.call(inp, filter)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\ops\nn_ops.py"", line 238, in __call__
    name=self.name)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\ops\nn_ops.py"", line 2010, in conv2d
    name=name)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\ops\gen_nn_ops.py"", line 1031, in conv2d
    data_format=data_format, dilations=dilations, name=name, ctx=_ctx)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\ops\gen_nn_ops.py"", line 1130, in conv2d_eager_fallback
    ctx=_ctx, name=name)

  File ""C:\Anaconda\envs\ML\lib\site-packages\tensorflow_core\python\eager\execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)

  File ""<string>"", line 3, in raise_from

UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]

```

**Note :** 
`tf.test.is_gpu_available()` returns `True`"
36172,AttributeError: module 'dataset_utils' has no attribute 'int64_list_feature',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
no, it is code pulled off a website
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 x64
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (use command 2.1.0
- Python version: 3.7
- CUDA/cuDNN version: 10.1
- GPU model and memory: 960m 4gb

**Describe the current behavior**
I am running this script that is below and I get this error, anyone have any help??

**Describe the expected behavior**
The script to run

**Code to reproduce the issue**
I run python then that location of the script

**Other info / logs**
2020-01-23 22:14:41.289298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
Traceback (most recent call last):
  File ""C:\Users\minec\Desktop\Robotics_stuff\TFconvert.py"", line 90, in <module>
    xml_to_tf(PATH_TEST, PATH_RECORD_TEST)
  File ""C:\Users\minec\Desktop\Robotics_stuff\TFconvert.py"", line 82, in xml_to_tf
    'image/object/class/label': dataset_util.int64_list_feature(classes),
AttributeError: module 'dataset_utils' has no attribute 'int64_list_feature'

___________________________________________________________________________________________________________
# Importing all necessary libraries
import os
import sys
import xml.etree.ElementTree as ET
import tensorflow as tf
sys.path.append(r""C:\Python37\models\models-master\research\slim\datasets""); 
import dataset_utils as dataset_util

# This are the path to the datasets and to the output files.
# NEED TO BE UPDATED IN CASE THE DATASET CHANGES
PATH_TEST = ""C:/Users/minec/Desktop/Robotics_stuff/FIRSTballDatabase/test/""
PATH_RECORD_TEST = ""test2.record""
PATH_TRAIN = ""C:/Users/minec/Desktop/Robotics_stuff/FIRSTballDatabase/train/""
PATH_RECORD_TRAIN = ""train2.record""

IMAGE_EXT = "".jpg""
IMAGE_FORMAT = b'jpg'

# This function defines the different classes the dataset has and return a different number per each.
# NEED TO BE UPDATED IN CASE THE DATASET CHANGES
def class_text_to_int(row_label):
    if row_label == 'FIRSTball':
        return 1
    else:
        return 0

# Reads the xml and the images, and create the tf records files. 
def xml_to_tf(path_input, path_output):
    xml_list = []
    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']

    writer = tf.io.TFRecordWriter(path_output)

    files = os.listdir(path_input)
    for file in files:
        if file.endswith("".xml""):
            xmlFile = path_input + file

            tree = ET.parse(xmlFile)
            root = tree.getroot()
            
            filename = root[1].text + IMAGE_EXT
            width = int(root[4][0].text)
            height = int(root[4][1].text)

            xmins = []
            xmaxs = []
            ymins = []
            ymaxs = []
            classes_text = []
            classes = []
            

            for member in root.findall('object'):
                beer = member[0].text
                xmin = int(member[4][0].text)
                ymin = int(member[4][1].text)
                xmax = int(member[4][2].text)
                ymax = int(member[4][3].text)
                
                xmins.append(xmin/width)
                xmaxs.append(xmax/width)
                ymins.append(ymin/height)
                ymaxs.append(ymax/height)
                classes_text.append(beer.encode('utf8'))
                classes.append(class_text_to_int(beer))

            with tf.io.gfile.GFile(os.path.join(path_input, '{}'.format(filename)), 'rb') as fid:
                encoded_jpg = fid.read()
            tf_example = tf.train.Example(features=tf.train.Features(feature={
                'image/height': dataset_util.int64_feature(height),
                'image/width': dataset_util.int64_feature(width),
                'image/filename': dataset_util.bytes_feature(filename.encode('utf8')),
                'image/source_id': dataset_util.bytes_feature(filename.encode('utf8')),
                'image/encoded': dataset_util.bytes_feature(encoded_jpg),
                'image/format': dataset_util.bytes_feature(IMAGE_FORMAT),
                'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),
                'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),
                'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),
                'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),
                'image/object/class/text': dataset_util.bytes_list_feature(classes_text),
                'image/object/class/label': dataset_util.int64_list_feature(classes),
            }))
            
            writer.write(tf_example.SerializeToString())
    writer.close()             
    output_path = os.path.join(os.getcwd(), path_output)
    print('Successfully created the TFRecords: {}'.format(output_path))

xml_to_tf(PATH_TEST, PATH_RECORD_TEST)
xml_to_tf(PATH_TRAIN, PATH_RECORD_TRAIN)
___________________________________________________________________________________________________________
If anyone is able to help thanks!!"
36170,"Build fails with messages including ""undefined reference to symbol 'acos@@GLIBC_2.2.5' "", ""Linking of rule '@flatbuffers//:flatc' failed""","**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Attempting to build from source
- TensorFlow version: 2.1.0 (attempting to build from master)
- Python version: 3.8.0
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 1.2.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.5
- GPU model and memory: NVidia RTX 2080 Ti

**Describe the problem**

I am trying to build for python 3.8.0 since issues relating to python 3.8.0 should now be fixed (see #34433 ).  There is also no pip package that you can install with ""pip install tensorflow"" for python 3.8.0.

Build fails with the following messages:

```
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (345 packages loaded, 27926 targets configured).
INFO: Found 1 target...
INFO: Deleting stale sandbox base /home/daniel/.cache/bazel/_bazel_daniel/79db702fc9f94af7d11e11c5d64854d0/sandbox
INFO: From Compiling tensorflow/core/platform/default/logging.cc [for host]:
tensorflow/core/platform/default/logging.cc: In member function ‘bool tensorflow::internal::LogFirstNState::ShouldLog(int)’:
tensorflow/core/platform/default/logging.cc:369:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (counter_value < n) {
       ~~~~~~~~~~~~~~^~~
INFO: From Compiling external/snappy/snappy-sinksource.cc [for host]:
cc1plus: warning: command line option ‘-Wno-implicit-function-declaration’ is valid for C/ObjC but not for C++
INFO: From Compiling external/snappy/snappy-stubs-internal.cc [for host]:
cc1plus: warning: command line option ‘-Wno-implicit-function-declaration’ is valid for C/ObjC but not for C++
INFO: From Compiling external/snappy/snappy.cc [for host]:
cc1plus: warning: command line option ‘-Wno-implicit-function-declaration’ is valid for C/ObjC but not for C++
INFO: From Compiling tensorflow/core/platform/protobuf.cc [for host]:
tensorflow/core/platform/protobuf.cc: In member function ‘virtual bool tensorflow::TStringOutputStream::Next(void**, int*)’:
tensorflow/core/platform/protobuf.cc:30:16: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (old_size < target_->capacity()) {
       ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
INFO: From Compiling tensorflow/core/platform/numbers.cc [for host]:
tensorflow/core/platform/numbers.cc: In instantiation of ‘T tensorflow::{anonymous}::locale_independent_strtonum(const char*, const char**) [with T = double]’:
tensorflow/core/platform/numbers.cc:197:60:   required from here
tensorflow/core/platform/numbers.cc:65:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < special_num_str.length(); ++i) {
                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/core/platform/numbers.cc: In function ‘std::__cxx11::string tensorflow::strings::HumanReadableNumBytes(tensorflow::int64)’:
tensorflow/core/platform/numbers.cc:459:8: warning: ‘%lld’ directive output may be truncated writing between 1 and 19 bytes into a region of size between 7 and 8 [-Wformat-truncation=]
 string HumanReadableNumBytes(int64 num_bytes) {
        ^~~~~~~~~~~~~~~~~~~~~
tensorflow/core/platform/numbers.cc:459:8: note: directive argument in the range [0, 9223372036854775807]
In file included from /usr/include/stdio.h:862:0,
                 from /usr/include/c++/7/cstdio:42,
                 from /usr/include/c++/7/ext/string_conversions.h:43,
                 from /usr/include/c++/7/bits/basic_string.h:6361,
                 from /usr/include/c++/7/string:52,
                 from ./tensorflow/core/platform/numbers.h:19,
                 from tensorflow/core/platform/numbers.cc:15:
/usr/include/x86_64-linux-gnu/bits/stdio2.h:65:44: note: ‘__builtin_snprintf’ output between 3 and 22 bytes into a destination of size 8
        __bos (__s), __fmt, __va_arg_pack ());
                                            ^
INFO: From Compiling external/llvm-project/llvm/lib/Support/APFloat.cpp [for host]:
external/llvm-project/llvm/lib/Support/APFloat.cpp: In member function ‘llvm::Expected<llvm::APFloatBase::opStatus> llvm::detail::IEEEFloat::convertFromDecimalString(llvm::StringRef, llvm::APFloatBase::roundingMode)’:
external/llvm-project/llvm/lib/Support/APFloat.cpp:2625:38: warning: ‘D.llvm::decimalInfo::exponent’ may be used uninitialized in this function [-Wmaybe-uninitialized]
     fs = roundSignificandWithExponent(decSignificand, partCount,
          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
                                       D.exponent, rounding_mode);
                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~
external/llvm-project/llvm/lib/Support/APFloat.cpp:2561:36: warning: ‘D.llvm::decimalInfo::normalizedExponent’ may be used uninitialized in this function [-Wmaybe-uninitialized]
              (D.normalizedExponent + 1) * 28738 <=
              ~~~~~~~~~~~~~~~~~~~~~~^~~~
external/llvm-project/llvm/lib/Support/APFloat.cpp:2622:16: warning: ‘D.llvm::decimalInfo::lastSigDigit’ may be used uninitialized in this function [-Wmaybe-uninitialized]
     } while (p <= D.lastSigDigit);
              ~~^~~~~~~~~~~~~~~~~
external/llvm-project/llvm/lib/Support/APFloat.cpp:2581:58: warning: ‘D.llvm::decimalInfo::firstSigDigit’ may be used uninitialized in this function [-Wmaybe-uninitialized]
     partCount = static_cast<unsigned int>(D.lastSigDigit - D.firstSigDigit) + 1;
                                           ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~
INFO: From Compiling external/llvm-project/llvm/lib/Support/UnicodeCaseFold.cpp [for host]:
external/llvm-project/llvm/lib/Support/UnicodeCaseFold.cpp:8:1: warning: multi-line comment [-Wcomment]
 //   utils/unicode-case-fold.py \
 ^
INFO: From Compiling external/llvm-project/llvm/lib/Support/VirtualFileSystem.cpp [for host]:
external/llvm-project/llvm/lib/Support/VirtualFileSystem.cpp: In member function ‘std::unique_ptr<llvm::vfs::RedirectingFileSystem::Entry> llvm::vfs::RedirectingFileSystemParser::parseEntry(llvm::yaml::Node*, llvm::vfs::RedirectingFileSystem*, bool)’:
external/llvm-project/llvm/lib/Support/VirtualFileSystem.cpp:1471:5: warning: ‘Kind’ may be used uninitialized in this function [-Wmaybe-uninitialized]
     switch (Kind) {
     ^~~~~~
INFO: From Compiling external/llvm-project/llvm/lib/MC/ELFObjectWriter.cpp [for host]:
external/llvm-project/llvm/lib/MC/ELFObjectWriter.cpp: In function ‘uint64_t {anonymous}::ELFWriter::writeObject(llvm::MCAssembler&, const llvm::MCAsmLayout&)’:
external/llvm-project/llvm/lib/MC/ELFObjectWriter.cpp:1193:36: warning: ‘AddrsigSection’ may be used uninitialized in this function [-Wmaybe-uninitialized]
       SectionOffsets[AddrsigSection] = std::make_pair(SecStart, SecEnd);
                                    ^
INFO: From Compiling external/llvm-project/llvm/lib/MC/MachObjectWriter.cpp [for host]:
external/llvm-project/llvm/lib/MC/MachObjectWriter.cpp: In member function ‘void llvm::MachObjectWriter::writeNlist(llvm::MachObjectWriter::MachSymbolData&, const llvm::MCAsmLayout&)’:
external/llvm-project/llvm/lib/MC/MachObjectWriter.cpp:381:13: warning: ‘AliaseeInfo’ may be used uninitialized in this function [-Wmaybe-uninitialized]
     Address = AliaseeInfo->StringIndex;
     ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~
ERROR: /home/daniel/.cache/bazel/_bazel_daniel/79db702fc9f94af7d11e11c5d64854d0/external/flatbuffers/BUILD.bazel:51:1: Linking of rule '@flatbuffers//:flatc' failed (Exit 1)
/usr/bin/ld: bazel-out/host/bin/external/flatbuffers/src/libflatbuffers.a(idl_parser.o): undefined reference to symbol 'acos@@GLIBC_2.2.5'
//lib/x86_64-linux-gnu/libm.so.6: error adding symbols: DSO missing from command line
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/daniel/tensorflow/tensorflow/python/tools/BUILD:141:1 Linking of rule '@flatbuffers//:flatc' failed (Exit 1)
INFO: Elapsed time: 103.160s, Critical Path: 25.44s
INFO: 1225 processes: 1225 local.
FAILED: Build did NOT complete successfully
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
./configure
You have bazel 1.2.1 installed.
Please specify the location of python. [Default is /home/daniel/tf38/bin/python]: Default accepted
Python library path to use.  Default is [/home/daniel/tf38/lib/python3.8/site-packages]: Default accepted
Do you wish to build TensorFlow with XLA JIT support? [Y/n]: Y
XLA JIT support will be enabled for TensorFlow.
Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N
No OpenCL SYCL support will be enabled for TensorFlow.
Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.
Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.
Do you wish to build TensorFlow with TensorRT support? [y/N]: y
TensorRT support will be enabled for TensorFlow.
Found CUDA 10.1 in:
    /usr/local/cuda/lib64
    /usr/local/cuda/include
Found cuDNN 7 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include
Found TensorRT 6 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include/x86_64-linux-gnu
CUDA compute capabilities [Default is: 3.5,7.0]: 7.5
Do you want to use clang as CUDA compiler? [y/N]: N
nvcc will be used as CUDA compiler.
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: Default accepted
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: Default accepted
Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.
Configuration finished.
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```
Build fails with the above messages.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I have noticed in the past that builds can sometimes build with different bazel versions.  I tried bazel 0.26.1, 1.0.0 and 2.0.0.  However the build still fails with all these bazel versions."
36169,Can't run TensorFlow Model Benchmark Tool,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
Couldn't run on mobile
- TensorFlow installed from (source or binary): 
source
- TensorFlow version:
Version: 1.14.0
- Python version:
2.7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 
1.2.1
- GCC/Compiler version (if compiling from source):
7.4.0
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I followed the tutorial in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark#tensorflow-model-benchmark-tool , but couldn't get past the (0) step.
Ran ./configure and set ndk to 14 and then tried to run
`bazel build --cxxopt='--std=c++11' -c opt //tensorflow/examples/android:tensorflow_demo`
Also tried with 
`bazel build -c opt \
  --crosstool_top=//external:android/crosstool \
  --cpu=armeabi-v7a \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  --config monolithic \
  tensorflow/tools/benchmark:benchmark_model`

Also tried with ndk 20 and no luck. log files for 1st and 2nd run are attached

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
2 files attached


[log_1st_run.txt](https://github.com/tensorflow/tensorflow/files/4106252/log_1st_run.txt)
[log_2nd_run.txt](https://github.com/tensorflow/tensorflow/files/4106253/log_2nd_run.txt)"
36168,@tf.custom_gradient does not behave as expected when used with @tf.function (for 2nd order derivatives),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04

- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 2.2.0-dev20200123 (observed in other versions)
- **Python version**: 3.7.6



- **CUDA/cuDNN version**: These are examples of versions though this has been replicated in other versions of cuda/cudnn
```
CUDA Version 10.1.243

#define CUDNN_MAJOR 7
#define CUDNN_MINOR 6
#define CUDNN_PATCHLEVEL 4
--
#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)


- **GPU model and memory**: 
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro P3200        On   | 00000000:01:00.0 Off |                  N/A |
| N/A   53C    P5     9W /  N/A |   5949MiB /  6078MiB |     10%      Default |
+-------------------------------+----------------------+----------------------+
```

- **Exact command to reproduce**:
```
import tensorflow as tf

print(tf.__version__)

@tf.function
@tf.custom_gradient
def _fn2(a, logdet_a):

    def sec_order_grad(a_dash): # bug still appears if this fn outside custom grad call
        return a_dash * 5., tf.ones_like(logdet_a) * 2.
    
    return a, sec_order_grad

@tf.function
@tf.custom_gradient
def fwd(a):
    sign_a, logdet_a = tf.linalg.slogdet(a)

#     logdet_a = tf.stop_gradient(logdet_a) # include to function
    def grad(dy):
        da = _fn2(a, logdet_a)
        return dy * da

    return 1., grad

ndim = 2

def call_gradient_1st():
    a = tf.zeros((ndim, ndim))
    with tf.GradientTape() as g:
        g.watch(a)
        y = fwd(a)
    z = g.gradient(y, a)
    return y, z


def call_gradient_2nd():
    a = tf.zeros((ndim, ndim))
    with tf.GradientTape() as g:
        g.watch(a)
        with tf.GradientTape() as gg:
            gg.watch(a)
            y = fwd(a)
        z = gg.gradient(y, a)
    gg = g.gradient(z, a)
    return y, z, gg

y, z = call_gradient_1st()
print(y, z)
y, z, gg = call_gradient_2nd()
print(y, z, gg)
```

### Describe the problem

Autograd attempts to call the gradient of a variable inside a custom gradient function. If the gradient is not computable (i.e. in this case when logdet has inf values) then it crashes. 

This only appears in the second order (see trace: first order calls, second order doesn't).

This doesn't appear if we stop the gradient (see note in function 'fwd').

This doesn't appear if we *do not* use the @tf.function wrapper.

### Source code / logs
```
2.2.0-dev20200123
tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(
[[0. 0.]
 [0. 0.]], shape=(2, 2), dtype=float32)

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-6-af7d2f010d71> in <module>
     48 y, z = call_gradient_1st()
     49 print(y, z)
---> 50 y, z, gg = call_gradient_2nd()
     51 print(y, z, gg)

<ipython-input-6-af7d2f010d71> in call_gradient_2nd()
     42             gg.watch(a)
     43             y = fwd(a)
---> 44         z = gg.gradient(y, a)
     45     gg = g.gradient(z, a)
     46     return y, z, gg

~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
   1032         output_gradients=output_gradients,
   1033         sources_raw=flat_sources_raw,
-> 1034         unconnected_gradients=unconnected_gradients)
   1035 
   1036     if not self._persistent:

~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
     75       output_gradients,
     76       sources_raw,
---> 77       compat.as_str(unconnected_gradients.value))

~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _backward_function_wrapper(*args)
   1303           break
   1304       return backward._call_flat(  # pylint: disable=protected-access
-> 1305           processed_args, remapped_captures)
   1306 
   1307     return _backward_function_wrapper, recorded_outputs

~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1748       flat_outputs = forward_function.call(
   1749           ctx, args_with_tangents,
-> 1750           cancellation_manager=cancellation_manager)
   1751     else:
   1752       with ops.get_default_graph()._override_gradient_function(  # pylint: disable=protected-access

~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    596               inputs=args,
    597               attrs=attrs,
--> 598               ctx=ctx)
    599         else:
    600           outputs = execute.execute_with_cancellation(

~/anaconda3/envs/fermi21/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Input is not invertible.
	 [[node gradients/LogMatrixDeterminant_grad/MatrixInverse (defined at <ipython-input-6-af7d2f010d71>:43) ]]
	 [[gradients/grad_ys_0/_4]]
  (1) Invalid argument:  Input is not invertible.
	 [[node gradients/LogMatrixDeterminant_grad/MatrixInverse (defined at <ipython-input-6-af7d2f010d71>:43) ]]
0 successful operations.
0 derived errors ignored. [Op:__forward___backward_fwd_655_746]

Function call stack:
__backward_fwd_655 -> __backward_fwd_655
```

Thanks for any insight!
"
36167,Help! I have an issue when importing TF!,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
no, it is custom code but I did not write it https://medium.com/swlh/nvidia-jetson-nano-custom-object-detection-from-scratch-using-tensorflow-and-opencv-113fe4dba134 I got it from here and the TF conversion script
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- TensorFlow installed from (source or binary): I installed with pip install Tensorflow and Tensorflow-gpu
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.6
- CUDA/cuDNN version: 10.1
- GPU model and memory: 960M 4gb

**Describe the current behavior**
When I run the script I get the error Failed to load the native TensorFlow runtime. with a bunch of other errors that I will paste down bellow.

**Describe the expected behavior**
To run the script lol

**Code to reproduce the issue**
I am just running python and then the directory to the .py file

Any help is appreciated thanks!!!!

**Other info / logs**
Traceback (most recent call last):
  File ""C:\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\minec\Desktop\Robotics_stuff\TFconvert.py"", line 4, in <module>
    import tensorflow as tf
  File ""C:\Python37\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Python37\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Python37\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Python37\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Python37\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
36166,Tensorflow issue,"I am getting this error when I run in PyCharm

Using TensorFlow backend.
Traceback (most recent call last):
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/Siraj/PycharmProjects/cnn/cnn.py"", line 1, in <module>
    from keras import layers
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\site-packages\keras\backend\load_backend.py"", line 90, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Siraj\PycharmProjects\cnn\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.

Process finished with exit code 1


The following Code
-----------------------------------
`from keras import layers
from keras import models
from keras import regularizers

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(256, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dropout(0.65))
model.add(layers.Dense(128, activation='relu' ,kernel_regularizer=regularizers.l2(0.002)))
model.add(layers.Dense(6, activation='softmax'))

model.summary()`"
36164,Memory leaks when doing a random tf.op on a previously converted to NumPy Tensor in TF2.1 ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nope
- TensorFlow installed from (source or binary): binary (docker image)
- TensorFlow version (use command below): 2.1.0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce GTX TITAN X (12Go)

**Describe the current behavior**

When converting a Tensor to NumPy before doing a random operation, there is a quite intense memory leak. Intriguingly, if we do not set a general seed or if we don't convert to numpy before doing the random operation, the leak is not happening. I really don't see why this is happening. I'm maybe doing something wrong, but I'm sceptical about it, the example code below being very very simple.

**Describe the expected behavior**

There should not be a leak.

**Code to reproduce the issue**

Here is a script that uses [memory-profiler](https://pypi.org/project/memory-profiler/) to profile memory in each functions. 

To generate the profiling data: 

`mprof run --python <script-name>`

To generate the figure below:

`mprof plot -o <figure-name>` 

```python
""""""This script aims to show the memory leak that exists when tf has a
general seed and a conversion to numpy is done.
""""""

import os
import random

import numpy as np
import tensorflow as tf
import tqdm


def transform_with_numpy():
    def transform(x):
        x = x.numpy() # Get a numpy version of x
        # Some NumPy operations here
        x = tf.cast(x, tf.float32) # Repass to tf version of x
        # A random TF function
        x = tf.image.random_contrast(x, 0.1, 0.8)
        return x
    return transform


def transform_with_tf():
    def transform(x):
        # Some TF operations here
        x = tf.cast(x, tf.float32) # Recast, just to isolate cause
        x = tf.image.random_contrast(x, 0.1, 0.8)
        return x
    return transform


def get_example(T):
    example = tf.cast(np.random.rand(224, 224), tf.float32)[tf.newaxis]
    return T(example)


#-----------------------------------------------------------------------


@profile
def function_with_conversion():
    T = transform_with_numpy()
    print(f'function_with_conversion')
    for i in tqdm.tqdm(range(1000)):
        examples = [get_example(T) for i in range(32)]


@profile
def function_with_seed():
    np.random.seed(100)
    random.seed(100)
    tf.random.set_seed(100)
    T = transform_with_tf()
    print(f'function_with_seed')
    for i in tqdm.tqdm(range(1000)):
        examples = [get_example(T) for i in range(32)]


@profile
def function_with_seed_and_conversion():
    np.random.seed(100)
    random.seed(100)
    tf.random.set_seed(100)
    T = transform_with_numpy()
    print(f'function_with_seed_and_conversion')
    for i in tqdm.tqdm(range(1000)):
        examples = [get_example(T) for i in range(32)]


if __name__ == '__main__':
    # Disable INFO and WARNING tf messages
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

    # Sets the memory growth, experimental...
    physical_devices = tf.config.experimental.list_physical_devices('GPU')
    if physical_devices:
        tf.config.experimental.set_memory_growth(physical_devices[0], True)

    function_with_conversion()
    function_with_seed()
    function_with_seed_and_conversion()
```

**Other info / logs**

![test_mem_leak_2](https://user-images.githubusercontent.com/59838807/73010435-6aae5500-3de0-11ea-8615-1887894d7594.png)

**Maybe related to**
- https://github.com/tensorflow/tensorflow/issues/36034
- https://github.com/tensorflow/tensorflow/issues/30873

**EDIT:** 

After running the script a few more times, the output of the memory profiling data is more random than I thought. The second time I run the script, the leak disappears from the third function, but there seem to be a problem with the second function:

![test_mem_leak_all_seed2](https://user-images.githubusercontent.com/59838807/73025706-0ac6a700-3dfe-11ea-9031-592d829af162.png)

Then, I change the size of the examples to be (112, 112) and the leak reappears in the third function:

![test_mem_leak_7](https://user-images.githubusercontent.com/59838807/73025808-41042680-3dfe-11ea-8730-f70024673baa.png)

Then I rerun the code with (224, 224) sized examples and the leak reappears here as well:

![test_mem_leak_all_seed1](https://user-images.githubusercontent.com/59838807/73025871-5bd69b00-3dfe-11ea-974a-5de5f24fc897.png)

**EDIT 2:**

The memory leak is actually also there where the seed is set but no conversion is made. The conversion only makes it worst. For more details see [this repo](https://github.com/lerobitaille/tf-issue-36164-workaround).   "
36163,undeclared inclusion(s) in rule when build from source,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.7
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.1.0-rc2
- Python version: intel 3.6.9
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source): 7.3
- CUDA/cuDNN version: 10.0 or 10.1 (tested the boths)
- GPU model and memory: Tesla K80



**Describe the problem**
FAILED: Build did NOT complete successfully


**Provide the exact sequence of commands / steps that you executed before running into the problem**
#compiling from source
cd /xxxx/install/tf2
git clone https://github.com/tensorflow/tensorflow
cd tensorflow
git checkout tags/v2.1.0-rc2
git checkout -b v2.1
conda activate tf2
source scl_source enable devtoolset-7 llvm-toolset-6.0
./configure
#input: /usr/bin/python3
#input: /opt/anaconda3/lib/python3.6/site-packages
#sequence: ok=enter, ok, ok, ok, y, y, 10.0; 7.6.4; 6.0.1; ok
#input: /usr/local/cuda-10.0,/usr/local/cuda-10.0/bin,/usr/local/cuda-10.0/lib64,/usr/local/cuda-10.0/include,/opt/TensorRT-6.0.1.5,/opt/TensorRT-6.0.1.5/bin,/opt/TensorRT-6.0.1.5/lib,/opt/TensorRT-6.0.1.5/include,/opt/rh/devtoolset-7/root/usr/bin,/opt/rh/devtoolset-7/root/usr/lib64,/opt/rh/devtoolset-7/root/usr/include,/opt/rh/devtoolset-7/root/usr/libexec/gcc/x86_64-redhat-linux/7,/opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/include
#input: 3.5,3.7,6.0,7.0,7.5
#sequence: ok, ok, ok, ok
bazel build --verbose_failures --config=v2 --config=opt --config=mkl --config=cuda //tensorflow/tools/pip_package:build_pip_package
#ERROR IS HERE ...

#cleaning for next try
bazel clean --expunge --async
rm -rf $HOME/.cache/bazel
git checkout tags/v2.1.0-rc2
git branch -d v2.1
git checkout -b v2.1

**Any other info / logs**
ERROR: /home/xxx/.cache/bazel/_bazel_xxx/26195a8102390a78bf7f86e6341cc9bb/external/boringssl/BUILD:130:1: undeclared inclusion(s) in rule '@boringssl//:crypto':
this rule is missing dependency declarations for the following files included by 'external/boringssl/src/crypto/asn1/tasn_fre.c':
  '/opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/include/stddef.h'
  '/opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/include/stdint.h'
  '/opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/include/stdarg.h'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/xxx/install/tf2/tensorflow/tensorflow/lite/toco/python/BUILD:77:1 undeclared inclusion(s) in rule '@boringssl//:crypto':
this rule is missing dependency declarations for the following files included by 'external/boringssl/src/crypto/asn1/tasn_fre.c':
  '/opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/include/stddef.h'
  '/opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/include/stdint.h'
  '/opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/include/stdarg.h'
INFO: Elapsed time: 61.246s, Critical Path: 3.70s
INFO: 198 processes: 198 local.
FAILED: Build did NOT complete successfully
"
36162,Error importing tensorflow,"**System information**
- My code has been custom made
- Windows 10
- TensorFlow installed from pip
- TensorFlow version 2.1.0
- Python version 3.7
- h5py version 2.10

**Brief Description**
I am part of a team that develops a standalone software for Windows that encountered an error when trying to import TensorFlow.
The software, called ""Hera"", is developed in Python and we are now trying to use a previously trained and saved model of a Neural Network to predict a specific feature. For this TensorFlow was used to create, train and save a neural network (outside of the Hera software), and now, when trying to implement TensorFlow on the software we are experiencing some issues.

**Current behavior**
When trying to use the library, importing it as ""import tensorflow as tf"", we get an error message related to h5py, which the program did not use before TensorFlow was added. 

**Things I have tried**
- _Downgrading to TensorFlow to 2.0:_ Another error occurs, this being: ImportError(""cannot import name 'Layer' from 'tensorflow.python.keras.engine.base_layer' (C:\\Development\\Hera3\\hera.dependencies\\OSGEO4W\\apps\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py)"")

- _Downgrading h5py to 2.8:_ The same error continued to occur, without change.

- _Uninstalling h5py:_ The program loads without a problem, however when trying to load the model, that is saved in an h5 file (the format that TensorFlow saves as), it is unable to do so since it does not possess the h5py module.

**Error log line**
AttributeError(""type object 'h5py.h5.H5PYConfig' has no attribute '__reduce_cython__'"")

**Full log**
```
2020-01-22 17:41:05 INFO: Config.configEnvironment - Configuring environment
2020-01-22 17:41:05 INFO: Config.configEnvironment - Checking GRASS path
2020-01-22 17:41:05 INFO: Config.configEnvironment - Checking QGis path
2020-01-22 17:41:05 INFO: Config.configEnvironment - Setting locale directory
2020-01-22 17:41:05 INFO: Config.configEnvironment - Setting resource directory
2020-01-22 17:41:05 INFO: Config.configEnvironment - Setting plugins directory
2020-01-22 17:41:05 INFO: Config.configEnvironment - Setting documentation directory
2020-01-22 17:41:05 INFO: Config.configEnvironment - Initializing GRASS
2020-01-22 17:41:05 DEBUG: Config.configEnvironment - Setting PSRClasses directory: C:\Development\Hera3\hera.dependencies\psrclassesinterfacepython
2020-01-22 17:41:05 INFO: Config.configEnvironment - Initializing PSRClasses
2020-01-22 17:41:05 INFO: Config.configEnvironment - Initializing QGis
2020-01-22 17:41:05 INFO: Config.configEnvironment -   Creating QgsApplication
2020-01-22 17:41:05 INFO: Config.configEnvironment -   Calling initQgis() - Prefix path: C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\qgis
2020-01-22 17:41:05 DEBUG: Config.configEnvironment - Setting environment variables: 
2020-01-22 17:41:05 DEBUG: Config.configEnvironment - GDAL_DATA: C:\Development\Hera3\hera.dependencies\OSGEO4W\share\gdal
2020-01-22 17:41:05 DEBUG: Config.configEnvironment - PATH: C:\DEVELO~1\Hera3\HERA~1.DEP\OSGEO4W\apps\qgis\bin;C:\DEVELO~1\Hera3\HERA~1.DEP\OSGEO4W\apps\Python37;C:\DEVELO~1\Hera3\HERA~1.DEP\OSGEO4W\apps\Python37\Scripts;C:\DEVELO~1\Hera3\HERA~1.DEP\OSGEO4W\apps\qt5\bin;C:\DEVELO~1\Hera3\HERA~1.DEP\OSGEO4W\apps\Python37\Scripts;C:\DEVELO~1\Hera3\HERA~1.DEP\OSGEO4W\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\system32\WBem

2020-01-22 17:41:05 DEBUG: Config.configEnvironment - PATHEXT: .COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.PY;.PYW;.py
2020-01-22 17:41:05 DEBUG: Config.configEnvironment - LD_LIBRARY_PATH: C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\grass\grass78\lib
2020-01-22 17:41:05 DEBUG: Config.configEnvironment - GISBASE: C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\grass\grass78
2020-01-22 17:41:05 DEBUG: Config.configEnvironment - GRASS_SH: C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\msys\bin\sh.exe
2020-01-22 17:41:05 DEBUG: Config.configEnvironment - PYTHONPATH: C:\Users\pedro\.p2\pool\plugins\org.python.pydev.core_7.3.0.201908161924\pysrc\pydev_sitecustomize;C:\Development\Hera3\hera.application\src;C:\Development\Hera3\hera.application\src\plugins;C:\Development\Hera3\hera.application\test;C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\qgis\python;C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\grass\grass78\bin;C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\grass\grass78\etc;C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\grass\grass78\etc\python;C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\Python37\DLLs;C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\Python37\lib;C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\Python37;C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\Python37\lib\site-packages;C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\grass\grass78\etc\python;C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\grass\grass78\etc;C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\grass\grass78\bin
2020-01-22 17:41:05 INFO: Config.configEnvironment - Loading language messages
2020-01-22 17:41:05 INFO: Config.configEnvironment - Finished application initialization
2020-01-22 17:41:06 DEBUG: __init__.wrapper - $HOME=C:\Users\pedro
2020-01-22 17:41:06 DEBUG: __init__.wrapper - CONFIGDIR=C:\Users\pedro\.matplotlib
2020-01-22 17:41:06 DEBUG: __init__.wrapper - matplotlib data path: C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\Python37\lib\site-packages\matplotlib\mpl-data
2020-01-22 17:41:06 DEBUG: __init__.rc_params_from_file - loaded rc file C:\Development\Hera3\hera.dependencies\OSGEO4W\apps\Python37\lib\site-packages\matplotlib\mpl-data\matplotlibrc
2020-01-22 17:41:06 DEBUG: __init__.<module> - matplotlib version 3.0.0
2020-01-22 17:41:06 DEBUG: __init__.<module> - interactive is False
2020-01-22 17:41:06 DEBUG: __init__.<module> - platform is win32
2020-01-22 17:41:06 DEBUG: __init__.<module> - loaded modules: ['sys', 'builtins', '_frozen_importlib', '_imp', '_thread', '_warnings', '_weakref', 'zipimport', '_frozen_importlib_external', '_io', 'marshal', 'nt', 'winreg', 'encodings', 'codecs', '_codecs', 'encodings.aliases', 'encodings.utf_8', '_signal', '__main__', 'encodings.latin_1', 'io', 'abc', '_abc', 'encodings.cp1252', 'site', 'os', 'stat', '_stat', 'ntpath', 'genericpath', 'os.path', '_collections_abc', '_sitebuiltins', '_bootlocale', '_locale', 'types', 'importlib', 'importlib._bootstrap', 'importlib._bootstrap_external', 'warnings', 'importlib.util', 'importlib.abc', 'importlib.machinery', 'contextlib', 'collections', 'operator', '_operator', 'keyword', 'heapq', '_heapq', 'itertools', 'reprlib', '_collections', 'functools', '_functools', 'google', 'mpl_toolkits', 'ruamel', 'sitecustomize', 'getpass', 'msvcrt', 'HeraDevEnv', 'hera', 'hera.Config', 'hera.PathManager', 'logging', 'time', 'traceback', 'linecache', 'tokenize', 're', 'enum', 'sre_compile', '_sre', 'sre_parse', 'sre_constants', 'copyreg', 'token', 'weakref', '_weakrefset', 'collections.abc', 'string', '_string', 'threading', 'atexit', 'PSRClassesIO', 'ctypes', '_ctypes', 'struct', '_struct', 'ctypes._endian', 'distutils', 'distutils.sysconfig', 'distutils.errors', 'PSRClasses', 'qgis', 'pathlib', 'fnmatch', 'posixpath', 'errno', 'urllib', 'urllib.parse', 'qgis.PyQt', 'qgis.PyQt.QtCore', 'PyQt5', 'sip', 'PyQt5.QtCore', 'qgis.gui', 'PyQt5.QtGui', 'PyQt5.QtWidgets', 'PyQt5.QtPrintSupport', 'PyQt5.Qsci', 'PyQt5.QtXml', 'PyQt5.QtSql', 'PyQt5.QtNetwork', 'qgis._core', 'qgis._gui', 'qgs25drendererwidget', 'qgsabstractdatasourcewidget', 'qgsabstractfilecontentsourcelineedit', 'qgsabstractprocessingparameterwidgetwrapper', 'qgsactionmenu', 'qgsadvanceddigitizingcanvasitem', 'qgsadvanceddigitizingdockwidget', 'qgsadvanceddigitizingfloater', 'qgsaggregatetoolbutton', 'qgsalignmentcombobox', 'qgsarrowsymbollayerwidget', 'qgsattributedialog', 'qgsattributeeditorcontext', 'qgsattributeform', 'qgsattributeformeditorwidget', 'qgsattributeforminterface', 'qgsattributeformrelationeditorwidget', 'qgsattributeformwidget', 'qgsattributetableaction', 'qgsattributetabledelegate', 'qgsattributetablefiltermodel', 'qgsattributetablemaplayeraction', 'qgsattributetablemodel', 'qgsattributetableview', 'qgsattributetypeloaddialog', 'qgsauthauthoritieseditor', 'qgsauthcerteditors', 'qgsauthcertinfo', 'qgsauthcertinfodialog', 'qgsauthcertmanager', 'qgsauthcerttrustpolicycombobox', 'qgsauthconfigeditor', 'qgsauthconfigselect', 'qgsauthconfiguriedit', 'qgsautheditorwidgets', 'qgsauthidentitieseditor', 'qgsauthimportcertdialog', 'qgsauthimportidentitydialog', 'qgsauthmethodedit', 'qgsauthmethodplugins', 'qgsauthserverseditor', 'qgsauthsettingswidget', 'qgsauthsslconfigdialog', 'qgsauthsslconfigwidget', 'qgsauthsslerrorsdialog', 'qgsauthsslimportdialog', 'qgsauthtrustedcasdialog', 'qgsblendmodecombobox', 'qgsblurwidget', 'qgsbrowserdockwidget', 'qgsbrowsertreeview', 'qgsbrushstylecombobox', 'qgsbusyindicatordialog', 'qgscategorizedsymbolrendererwidget', 'qgscentroidfillsymbollayerwidget', 'qgscharacterselectordialog', 'qgscheckablecombobox', 'qgscheckboxsearchwidgetwrapper', 'qgscodeeditor', 'qgscodeeditorcss', 'qgscodeeditorexpression', 'qgscodeeditorhtml', 'qgscodeeditorpython', 'qgscodeeditorsql', 'qgscollapsiblegroupbox', 'qgscollapsiblegroupboxbasic', 'qgscolorbox', 'qgscolorbrewercolorrampdialog', 'qgscolorbrewercolorrampwidget', 'qgscolorbutton', 'qgscolordialog', 'qgscoloreffectwidget', 'qgscolorpreviewwidget', 'qgscolorrampbutton', 'qgscolorrampshaderwidget', 'qgscolorrampwidget', 'qgscolorschemelist', 'qgscolorschememodel', 'qgscolorsliderwidget', 'qgscolorswatchdelegate', 'qgscolorswatchgrid', 'qgscolorswatchgridaction', 'qgscolortextwidget', 'qgscolorwheel', 'qgscolorwidget', 'qgscolorwidgetaction', 'qgscompoundcolorwidget', 'qgsconfigureshortcutsdialog', 'qgscoordinateboundspreviewmapwidget', 'qgscptcitycolorrampdialog', 'qgscredentialdialog', 'qgscurveeditorwidget', 'qgscustomdrophandler', 'qgscustomlayerorderwidget', 'qgsdashspacedialog', 'qgsdashspacewidget', 'qgsdatadefinedrotationdialog', 'qgsdatadefinedsizedialog', 'qgsdatadefinedsizelegendwidget', 'qgsdatadefinedvaluedialog', 'qgsdatadefinedwidthdialog', 'qgsdataitemguicontext', 'qgsdataitemguiprovider', 'qgsdataitemguiproviderregistry', 'qgsdatasourceselectdialog', 'qgsdatetimeedit', 'qgsdatetimesearchwidgetwrapper', 'qgsdefaultsearchwidgetwrapper', 'qgsdetaileditemdata', 'qgsdetaileditemdelegate', 'qgsdetaileditemwidget', 'qgsdial', 'qgsdialog', 'qgsdockwidget', 'qgsdoublespinbox', 'qgsdrawsourcewidget', 'qgsdualview', 'qgseditorconfigwidget', 'qgseditorwidgetautoconfplugin', 'qgseditorwidgetfactory', 'qgseditorwidgetregistry', 'qgseditorwidgetwrapper', 'qgseffectdrawmodecombobox', 'qgseffectstackcompactwidget', 'qgseffectstackpropertiesdialog', 'qgseffectstackpropertieswidget', 'qgsellipsesymbollayerwidget', 'qgsencodingfiledialog', 'qgsencodingselectiondialog', 'qgserrordialog', 'qgsexpressionbuilderdialog', 'qgsexpressionbuilderwidget', 'qgsexpressionhighlighter', 'qgsexpressionitem', 'qgsexpressionitemsearchproxy', 'qgsexpressionlineedit', 'qgsexpressionselectiondialog', 'qgsextentgroupbox', 'qgsexternalresourcewidget', 'qgsfeaturelistcombobox', 'qgsfeaturelistmodel', 'qgsfeaturelistview', 'qgsfeaturelistviewdelegate', 'qgsfeaturemodel', 'qgsfeatureselectiondlg', 'qgsfeatureselectionmodel', 'qgsfieldcombobox', 'qgsfieldconditionalformatwidget', 'qgsfieldexpressionwidget', 'qgsfieldvalidator', 'qgsfieldvalueslineedit', 'qgsfiledownloaderdialog', 'qgsfilewidget', 'qgsfilledmarkersymbollayerwidget', 'qgsfilterlineedit', 'qgsfindfilesbypatterndialog', 'qgsfindfilesbypatternwidget', 'qgsfloatingwidget', 'qgsfocuswatcher', 'qgsfontbutton', 'qgsfontmarkersymbollayerwidget', 'qgsformannotation', 'qgsgeometrygeneratorsymbollayerwidget', 'qgsgeometryrubberband', 'qgsglowwidget', 'qgsgradientcolorrampdialog', 'qgsgradientfillsymbollayerwidget', 'qgsgradientstopeditor', 'qgsgraduatedhistogramwidget', 'qgsgraduatedsymbolrendererwidget', 'qgsgroupboxcollapsebutton', 'qgsgroupwmsdatadialog', 'qgsgui', 'qgshashedlinesymbollayerwidget', 'qgsheatmaprendererwidget', 'qgshelp', 'qgshighlight', 'qgshillshaderendererwidget', 'qgshistogramwidget', 'qgshtmlwidgetwrapper', 'qgsifeatureselectionmanager', 'qgsidentifymenu', 'qgsimagesourcelineedit', 'qgsinvertedpolygonrendererwidget', 'qgskeyvaluewidget', 'qgsludialog', 'qgslayerpropertieswidget', 'qgslayertreeembeddedconfigwidget', 'qgslayertreeembeddedwidgetprovider', 'qgslayertreeembeddedwidgetregistry', 'qgslayertreemapcanvasbridge', 'qgslayertreeview', 'qgslayertreeviewdefaultactions', 'qgslayertreeviewindicator', 'qgslayertreeviewmenuprovider', 'qgslayoutcombobox', 'qgslayoutconfigobject', 'qgslayoutcustomdrophandler', 'qgslayoutdesignerinterface', 'qgslayoutitemabstractguimetadata', 'qgslayoutitembasewidget', 'qgslayoutitemcombobox', 'qgslayoutitemguigroup', 'qgslayoutitemguiregistry', 'qgslayoutitempropertiesdialog', 'qgslayoutitempropertieswidget', 'qgslayoutruler', 'qgslayoutunitscombobox', 'qgslayoutview', 'qgslayoutviewellipticalrubberband', 'qgslayoutviewmenuprovider', 'qgslayoutviewmouseevent', 'qgslayoutviewrectangularrubberband', 'qgslayoutviewrubberband', 'qgslayoutviewtool', 'qgslayoutviewtooladditem', 'qgslayoutviewtooladdnodeitem', 'qgslayoutviewtooleditnodes', 'qgslayoutviewtoolmoveitemcontent', 'qgslayoutviewtoolpan', 'qgslayoutviewtoolselect', 'qgslayoutviewtooltemporarykeypan', 'qgslayoutviewtooltemporarykeyzoom', 'qgslayoutviewtooltemporarymousepan', 'qgslayoutviewtoolzoom', 'qgslayoutviewtrianglerubberband', 'qgslegendfilterbutton', 'qgslimitedrandomcolorrampdialog', 'qgslimitedrandomcolorrampwidget', 'qgslinepatternfillsymbollayerwidget', 'qgslistwidget', 'qgslocatorwidget', 'qgslonglongvalidator', 'qgsmanageconnectionsdialog', 'qgsmapcanvas', 'qgsmapcanvasannotationitem', 'qgsmapcanvasitem', 'qgsmapcanvassnappingutils', 'qgsmapcanvastracer', 'qgsmaplayeraction', 'qgsmaplayeractionregistry', 'qgsmaplayercombobox', 'qgsmaplayerconfigwidget', 'qgsmaplayerconfigwidgetfactory', 'qgsmaplayerstylemanagerwidget', 'qgsmapmouseevent', 'qgsmapoverviewcanvas', 'qgsmaptip', 'qgsmaptool', 'qgsmaptooladvanceddigitizing', 'qgsmaptoolcapture', 'qgsmaptooledit', 'qgsmaptoolemitpoint', 'qgsmaptoolextent', 'qgsmaptoolidentify', 'qgsmaptoolidentifyfeature', 'qgsmaptoolpan', 'qgsmaptoolzoom', 'qgsmapunitscaledialog', 'qgsmapunitscalewidget', 'qgsmarkerlinesymbollayerwidget', 'qgsmenuheader', 'qgsmenuheaderwidgetaction', 'qgsmessagebar', 'qgsmessagebaritem', 'qgsmessagelogviewer', 'qgsmessageviewer', 'qgsmetadatawidget', 'qgsmultibandcolorrendererwidget', 'qgsmultiedittoolbutton', 'qgsnewauxiliaryfielddialog', 'qgsnewauxiliarylayerdialog', 'qgsnewgeopackagelayerdialog', 'qgsnewhttpconnection', 'qgsnewmemorylayerdialog', 'qgsnewnamedialog', 'qgsnewvectorlayerdialog', 'qgsnullsymbolrendererwidget', 'qgsowssourceselect', 'qgsopacitywidget', 'qgsoptionsdialogbase', 'qgsoptionsdialoghighlightbutton', 'qgsoptionsdialoghighlightcheckbox', 'qgsoptionsdialoghighlightgroupbox', 'qgsoptionsdialoghighlightlabel', 'qgsoptionsdialoghighlighttree', 'qgsoptionsdialoghighlightwidget', 'qgsoptionspagewidget', 'qgsoptionswidgetfactory', 'qgsorderbydialog', 'qgsorganizetablecolumnsdialog', 'qgspainteffectpropertieswidget', 'qgspainteffectwidget', 'qgspalettedrendererwidget', 'qgspanelwidget', 'qgspanelwidgetstack', 'qgspanelwidgetwrapper', 'qgspasswordlineedit', 'qgspencapstylecombobox', 'qgspenjoinstylecombobox', 'qgspenstylecombobox', 'qgspixmaplabel', 'qgspluginmanagerinterface', 'qgspointclusterrendererwidget', 'qgspointdisplacementrendererwidget', 'qgspointpatternfillsymbollayerwidget', 'qgspresetcolorrampdialog', 'qgspresetcolorrampwidget', 'qgsprevieweffect', 'qgsprocessingalgorithmconfigurationwidget', 'qgsprocessingalgorithmconfigurationwidgetfactory', 'qgsprocessingalgorithmdialogbase', 'qgsprocessingcontextgenerator', 'qgsprocessinggui', 'qgsprocessingguiregistry', 'qgsprocessingmaplayercombobox', 'qgsprocessingmodelerparameterwidget', 'qgsprocessingmultipleselectiondialog', 'qgsprocessingparameterwidgetcontext', 'qgsprocessingparameterwidgetfactoryinterface', 'qgsprocessingrecentalgorithmlog', 'qgsprocessingtoolboxmodel', 'qgsprocessingtoolboxmodelalgorithmnode', 'qgsprocessingtoolboxmodelgroupnode', 'qgsprocessingtoolboxmodelnode', 'qgsprocessingtoolboxmodelprovidernode', 'qgsprocessingtoolboxmodelrecentnode', 'qgsprocessingtoolboxproxymodel', 'qgsprocessingtoolboxtreeview', 'qgsprojectionselectiondialog', 'qgsprojectionselectiontreewidget', 'qgsprojectionselectionwidget', 'qgspropertyassistantwidget', 'qgspropertyoverridebutton', 'qgsproxystyle', 'qgsqmlwidgetwrapper', 'qgsquerybuilder', 'qgsrasterbandcombobox', 'qgsrasterfillsymbollayerwidget', 'qgsrasterformatsaveoptionswidget', 'qgsrasterhistogramwidget', 'qgsrasterlayersaveasdialog', 'qgsrastermarkersymbollayerwidget', 'qgsrasterminmaxwidget', 'qgsrasterpyramidsoptionswidget', 'qgsrasterrendererwidget', 'qgsrastertransparencywidget', 'qgsratiolockbutton', 'qgsrelationaggregatesearchwidgetwrapper', 'qgsrelationeditorwidget', 'qgsrelationreferencesearchwidgetwrapper', 'qgsrelationreferencewidget', 'qgsrelationreferencewidgetwrapper', 'qgsrelationwidgetwrapper', 'qgsrendererpropertiesdialog', 'qgsrendererrasterpropertieswidget', 'qgsrendererrulepropsdialog', 'qgsrendererrulepropswidget', 'qgsrendererwidget', 'qgsrubberband', 'qgsrulebasedrenderermodel', 'qgsrulebasedrendererwidget', 'qgssvgfillsymbollayerwidget', 'qgsscalecombobox', 'qgsscalerangewidget', 'qgsscalevisibilitydialog', 'qgsscalewidget', 'qgssearchquerybuilder', 'qgssearchwidgettoolbutton', 'qgssearchwidgetwrapper', 'qgsshadoweffectwidget', 'qgsshapeburstfillsymbollayerwidget', 'qgsshortcutsmanager', 'qgssimplefillsymbollayerwidget', 'qgssimplelinesymbollayerwidget', 'qgssimplemarkersymbollayerwidget', 'qgssinglebandgrayrendererwidget', 'qgssinglebandpseudocolorrendererwidget', 'qgssinglesymbolrendererwidget', 'qgsslider', 'qgssmartgroupcondition', 'qgssmartgroupeditordialog', 'qgssnapindicator', 'qgssnaptogridcanvasitem', 'qgssourceselectprovider', 'qgssourceselectproviderregistry', 'qgsspinbox', 'qgsstatusbar', 'qgsstyleexportimportdialog', 'qgsstylegroupselectiondialog', 'qgsstylemanagerdialog', 'qgsstylesavedialog', 'qgssublayersdialog', 'qgssubstitutionlistdialog', 'qgssubstitutionlistwidget', 'qgssvgmarkersymbollayerwidget', 'qgssvgselectordialog', 'qgssvgselectorgroupsmodel', 'qgssvgselectorlistmodel', 'qgssvgselectorwidget', 'qgssvgsourcelineedit', 'qgssymbolbutton', 'qgssymbollayerwidget', 'qgssymbollevelsdialog', 'qgssymbollevelswidget', 'qgssymbolselectordialog', 'qgssymbolselectorwidget', 'qgssymbolwidgetcontext', 'qgssymbolslistwidget', 'qgstabwidget', 'qgstablewidgetbase', 'qgstablewidgetitem', 'qgstaskmanagerwidget', 'qgstextformatdialog', 'qgstextformatpanelwidget', 'qgstextformatwidget', 'qgstextpreview', 'qgstransformwidget', 'qgstreewidgetitem', 'qgstreewidgetitemobject', 'qgsunitselectionwidget', 'qgsuserinputwidget', 'qgsvscrollarea', 'qgsvaliditycheckresultsmodel', 'qgsvaliditycheckresultswidget', 'qgsvaluemapsearchwidgetwrapper', 'qgsvaluerelationsearchwidgetwrapper', 'qgsvariableeditorwidget', 'qgsvectorfieldsymbollayerwidget', 'qgsvertexmarker', 'qgswidgetwrapper', 'qgswindowmanagerinterface', 'qgis.core', 'qgis.core.additions', 'qgis.core.additions.edit', 'qgis.core.additions.fromfunction', 'qgis.core.additions.qgstaskwrapper', 'qgis.core.additions.markerlinesymbollayer', 'qgis.core.additions.metaenum', 'qgis.core.additions.processing', 'qgis.core.additions.projectdirtyblocker', 'qgis.core.additions.qgsfeature', 'qgis.core.additions.qgsfunction', 'inspect', 'dis', 'opcode', '_opcode', 'qgis.core.additions.qgsgeometry', 'qgis.core.additions.qgssettings', 'qgis.core.additions.readwritecontextentercategory', 'qgis.core.additions.validitycheck', 'hera.I18N', 'gettext', 'locale', 'unicodedata', 'hera.PropertyManager', 'hera.EntityModel', 'json', 'json.decoder', 'json.scanner', '_json', 'json.encoder', 'hera.FileUtils', 'osgeo', 'imp', 'swig_runtime_data4', '_gdal', 'osgeo.ogr', 'osgeo._ogr', 'osgeo.osr', 'osgeo._osr', 'shutil', 'zlib', 'bz2', '_compression', '_bz2', 'lzma', '_lzma', 'tempfile', 'random', 'math', 'hashlib', '_hashlib', '_blake2', '_sha3', 'bisect', '_bisect', '_random', 'hera.DataModel', 'datetime', '_datetime', 'copy', 'psrl', 'psrl.psrl', 'hera.PluginManager', 'hera.DynamicGui', 'matplotlib', 'distutils.version', 'pprint', 'subprocess', 'signal', '_winapi', 'urllib.request', 'base64', 'binascii', 'email', 'http', 'http.client', 'email.parser', 'email.feedparser', 'email.errors', 'email._policybase', 'email.header', 'email.quoprimime', 'email.base64mime', 'email.charset', 'email.encoders', 'quopri', 'email.utils', 'socket', '_socket', 'selectors', 'select', 'email._parseaddr', 'calendar', 'email.message', 'uu', 'email._encoded_words', 'email.iterators', 'ssl', '_ssl', 'urllib.error', 'urllib.response', 'nturl2path', 'matplotlib.cbook', 'glob', 'gzip', 'numbers', 'numpy', '__future__', 'numpy._globals', 'numpy.__config__', 'numpy.version', 'numpy._distributor_init', 'numpy.core', 'numpy.core.info', 'numpy.core.multiarray', 'numpy.core.overrides', 'numpy.core._multiarray_umath', 'numpy.compat', 'numpy.compat._inspect', 'numpy.compat.py3k', 'numpy.core.umath', 'numpy.core.numerictypes', 'numpy.core._string_helpers', 'numpy.core._type_aliases', 'numpy.core._dtype', 'numpy.core.numeric', 'numpy.core._internal', 'pickle', '_compat_pickle', '_pickle', 'numpy.core.fromnumeric', 'numpy.core._methods', 'numpy.core.arrayprint', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.function_base', 'numpy.core.machar', 'numpy.core.getlimits', 'numpy.core.shape_base', 'numpy.core.einsumfunc', 'numpy.core._add_newdocs', 'numpy.core._multiarray_tests', 'numpy.core._dtype_ctypes', 'numpy._pytesttester', 'numpy.lib', 'numpy.lib.info', 'numpy.lib.type_check', 'numpy.lib.ufunclike', 'numpy.lib.index_tricks', 'numpy.matrixlib', 'numpy.matrixlib.defmatrix', 'ast', '_ast', 'numpy.linalg', 'numpy.linalg.info', 'numpy.linalg.linalg', 'numpy.lib.twodim_base', 'numpy.linalg.lapack_lite', 'numpy.linalg._umath_linalg', 'numpy.lib.function_base', 'numpy.lib.utils', 'numpy.lib.histograms', 'numpy.lib.stride_tricks', 'numpy.lib.mixins', 'numpy.lib.nanfunctions', 'numpy.lib.shape_base', 'numpy.lib.scimath', 'numpy.lib.polynomial', 'numpy.lib.arraysetops', 'numpy.lib.npyio', 'numpy.lib.format', 'numpy.lib._datasource', 'numpy.lib._iotools', 'numpy.lib.financial', 'decimal', '_decimal', 'numpy.lib.arrayterator', 'numpy.lib.arraypad', 'numpy.lib._version', 'numpy.fft', 'numpy.fft.info', 'numpy.fft.fftpack', 'numpy.fft.fftpack_lite', 'numpy.fft.helper', 'numpy.polynomial', 'numpy.polynomial.polynomial', 'numpy.polynomial.polyutils', 'numpy.polynomial._polybase', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermite_e', 'numpy.polynomial.laguerre', 'numpy.random', 'numpy.random.mtrand', 'cython_runtime', 'mtrand', 'numpy.ctypeslib', 'numpy.ma', 'numpy.ma.core', 'textwrap', 'numpy.ma.extras', 'numpy.testing', 'unittest', 'unittest.result', 'unittest.util', 'unittest.case', 'difflib', 'unittest.suite', 'unittest.loader', 'unittest.main', 'argparse', 'unittest.runner', 'unittest.signals', 'numpy.testing._private', 'numpy.testing._private.utils', 'gc', 'numpy.testing._private.decorators', 'numpy.testing._private.nosetester', 'matplotlib.cbook.deprecation', 'matplotlib.rcsetup', 'matplotlib.fontconfig_pattern', 'pyparsing', 'matplotlib.colors', 'matplotlib._color_data', 'cycler', 'six', 'six.moves', 'matplotlib._version', 'dateutil', 'dateutil._version']
2020-01-22 17:41:06 DEBUG: __init__.wrapper - CACHEDIR=C:\Users\pedro\.matplotlib
2020-01-22 17:41:06 DEBUG: font_manager.<module> - Using fontManager instance from C:\Users\pedro\.matplotlib\fontlist-v300.json
2020-01-22 17:41:06 DEBUG: pyplot.switch_backend - Loaded backend Qt5Agg version unknown.
2020-01-22 17:41:06 INFO: PluginManager.findAndLoadPlugins - Looking for plugins
2020-01-22 17:41:06 INFO: PluginManager.findAndLoadPlugins - Loading plugin DevTools
2020-01-22 17:41:06.767810: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-01-22 17:41:06.768117: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2020-01-22 17:41:07 INFO: PluginManager.findAndLoadPlugins - Loaded module: DevTools.PHSAlgorithms
2020-01-22 17:41:07 INFO: PluginManager.findAndLoadPlugins - Loaded module: DevTools.ReservoirTool
Traceback (most recent call last):
  File ""C:\Development\Hera3\hera.application\src\HeraDev.py"", line 1, in <module>
    import HeraDevEnv
  File ""C:\Development\Hera3\hera.application\src\HeraDevEnv.py"", line 29, in <module>
    PluginManager.findAndLoadPlugins()
  File ""C:\Development\Hera3\hera.application\src\hera\PluginManager.py"", line 39, in findAndLoadPlugins
    raise Exception('Could not import plugin modules: {:}. Exceptions: {:}'.format([n for _,n,_ in nextModules], exceptions))
Exception: Could not import plugin modules: ['Config', 'RiverDataTool']. Exceptions: [AttributeError(""type object 'h5py.h5.H5PYConfig' has no attribute '__reduce_cython__'""), AttributeError(""type object 'h5py.h5.H5PYConfig' has no attribute '__reduce_cython__'"")]
```"
36161,Logic Error in memory planner,"Host OS Platform and Distribution 
NAME=""Ubuntu""
VERSION=""18.04 LTS""
ID=ubuntu
PRETTY_NAME=""Ubuntu 18.04 LTS""
VERSION_ID=""18.04""
Python versionL 2.7.15+
Target platform : K64F

Describe the problem
I am trying to deploy a GRU model on the ARM Cortex M board(K64F). During memory allocation of the tensor, I get the following error message. I don't understand the error.
It would be quite helpful if somebody knows the solution or an explanation for this following error report.

```
Logic error in memory planner, tensor 17 has an invalid lifetime
AllocateTensors() failed
```

Thank You"
36160,Op Request,"**System information**
- Windows 10
- TensorFlow installed from source:
- TensorFlow version 2.1 (cpu)

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, DIV, EXP, FILL, FULLY_CONNECTED, GATHER, LOGISTIC, MAX_POOL_2D, MUL, PACK, PRELU, REDUCE_MAX, RESHAPE, REVERSE_V2, SHAPE, STRIDED_SLICE, SUB, SUM, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.
Traceback (most recent call last):
  File ""c:\users\evan.chu$\appdata\local\programs\python\python37\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\evan.chu$\appdata\local\programs\python\python37\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Evan.Chu$\AppData\Local\Programs\Python\Python37\Scripts\toco_from_protos.exe\__main__.py"", line 9, in <module>
  File ""c:\users\evan.chu$\appdata\local\programs\python\python37\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""c:\users\evan.chu$\appdata\local\programs\python\python37\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""c:\users\evan.chu$\appdata\local\programs\python\python37\lib\site-packages\absl\app.py"", line 299, in run
    _run_main(main, args)
  File ""c:\users\evan.chu$\appdata\local\programs\python\python37\lib\site-packages\absl\app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""c:\users\evan.chu$\appdata\local\programs\python\python37\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 56, in execute
    enable_mlir_converter)

```
Also, please include a link to a GraphDef or the model if possible.
https://github.com/arthurflor23/handwritten-text-recognition/blob/master/src/network/model.py (Flor implementation, not Bluche or Puigcerver)

"
36156,Output from TFLite model converted using experimental converter does not match old converter,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- TensorFlow installed from (source or binary): pip install tf-nightly
- TensorFlow version (or github SHA if from source): 2.2.0-dev20200123

EDIT: I am experimenting with converting the SSD MobileNet V2 models. Using the old converter, the model seems to work without any issues. However, the model converted with the new experimental converter does not match the results. I also tried a Float16 quantized version of the model and it also doesn't match the output of any the two float32 models.

**Command used to run the converter or code if you’re using the Python API**

```
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_model, input_arrays, output_arrays, input_shapes)
converter.allow_custom_ops = True
converter.experimental_new_converter = True
tflite_model = converter.convert()
open('ssd_mobilenet_v2_float32_experimental.tflite', ""wb"").write(tflite_model)
```

**The output from the converter invocation**
Comparison of model output with old and new experimental converter.
```
AssertionError: 
Arrays are not almost equal to 5 decimals

Mismatched elements: 40 / 40 (100%)
Max absolute difference: 11.113381
Max relative difference: 3501.543
 x: array([[ -8.2944 , -10.18906,  -9.16781,  -8.85916],
       [ -8.63264,  -9.66667,  -8.13753, -11.32026],
       [ -7.67533,  -9.24478,  -9.12119, -11.07866],...
 y: array([[ 2.46333e-01, -4.78567e-01, -7.21600e-01,  4.30845e-02],
       [ 3.55053e-01,  1.12653e+00, -3.39829e+00, -5.85306e+00],
       [ 7.73718e-01,  4.47561e-01,  1.09934e+00, -1.17987e-02],...
```

**Also, please include a link to the saved model or GraphDef**
[Colab notebook with model](https://colab.research.google.com/drive/1J4As3g3BY1zkdRdHHXySz_SgvfuThqHF)

**Failure details**
The output produced by models using various options all output different results as demonstrated in notebook. I also created a Float16 Quantized version of the SSD MobileNet V2 model however that doesn't produce a similar precision output to the two F32 models."
36155,Memory leak in Tensorflow Graph on CPU,"I have a Face Detector that I'm trying to use for inference in Golang via official [Tensorflow bindings](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go). However, I faced a stepwise memory leak that causes the killing of an application due to OOM.


# Screenshot with the leak:
![image](https://user-images.githubusercontent.com/2982775/72971506-d8f01900-3dda-11ea-8b7e-8057879f15de.png)

Heap samples from pprof does not show anything interesting in Go code. It looks like the leak on C++ backend side:
```
go tool pprof http://localhost:8200/debug/pprof/heap
Fetching profile over HTTP from http://localhost:8200/debug/pprof/heap
Saved profile in /pprof.facedetector.alloc_objects.alloc_space.inuse_objects.inuse_space.073.pb.gz
File: facedetector
Build ID: c4dba901e690718468bdd4fa7d1a631daca9e65a
Type: inuse_space
Time: Jan 22, 2020 at 11:44pm (MSK)
Entering interactive mode (type ""help"" for commands, ""o"" for options)
(pprof)
(pprof) top20
Showing nodes accounting for 1861.64kB, 100% of 1861.64kB total
      flat  flat%   sum%        cum   cum%
  930.82kB 50.00% 50.00%   930.82kB 50.00%  bytes.makeSlice
  930.82kB 50.00%   100%   930.82kB 50.00%  main.(*FaceDetector).FindFaces
         0     0%   100%   930.82kB 50.00%  bytes.(*Buffer).Grow
         0     0%   100%   930.82kB 50.00%  bytes.(*Buffer).grow
         0     0%   100%   930.82kB 50.00%  io/ioutil.ReadFile
         0     0%   100%   930.82kB 50.00%  io/ioutil.readAll
         0     0%   100%  1861.64kB   100%  main.main
         0     0%   100%  1861.64kB   100%  runtime.main
```


# Code to reproduce the issue
This code just reading an image from a file system in a for-loop and feed it to the model.

```go
package main

import (
	""io/ioutil""
	""log""
	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""net/http""
	_ ""net/http/pprof""
)

type FaceDetector struct {
	session         *tf.Session
	graph           *tf.Graph

	inputOp  tf.Output
	bboxesOp tf.Output
	scoresOp tf.Output
}

func NewFaceDetector(frozenGraphPath string) (*FaceDetector, error) {
	fd := &FaceDetector{}
	model, err := ioutil.ReadFile(frozenGraphPath)
	if err != nil {
		return nil, err
	}

	fd.graph = tf.NewGraph()
	if err := fd.graph.Import(model, """"); err != nil {
		return nil, err
	}

	fd.session, err = tf.NewSession(fd.graph, nil)
	if err != nil {
		return nil, err
	}

	fd.inputOp = fd.graph.Operation(""input_image"").Output(0)
	fd.bboxesOp = fd.graph.Operation(""bboxes"").Output(0)
	fd.scoresOp = fd.graph.Operation(""scores_1/GatherV2"").Output(0)

	return fd, nil
}

func (fd *FaceDetector) Close() error {
	if err := fd.session.Close(); err != nil {
		return err
	}
	return nil
}

func (fd *FaceDetector) FindFaces(image []byte) ([]float32, [][]int32, error) {
	imageTensor, err := tf.NewTensor(string(image))
	if err != nil {
		return nil, nil, err
	}

	output, err := fd.session.Run(
		map[tf.Output]*tf.Tensor{
			fd.inputOp: imageTensor,
		},
		[]tf.Output{
			fd.bboxesOp,
			fd.scoresOp,
		},
		nil)

	if err != nil {
		return nil, nil, err
	}

	bboxes := output[0].Value().([][]int32)
	scores := output[1].Value().([]float32)

	return scores, bboxes, nil
}

func main() {
	log.SetFlags(log.LstdFlags | log.Lshortfile | log.Lmicroseconds)

	go func() {
		log.Println(http.ListenAndServe(""0.0.0.0:8200"", nil))
	}()

	fd, err := NewFaceDetector(""OptimizedGraph.pb"")

	if err != nil {
		panic(err)
	}

	defer fd.Close()

	for {
		img, err := ioutil.ReadFile(""photo.jpg"")

		if err != nil {
			log.Println(""Fail to read image:"", err)
		}

		scores, boxes, err := fd.FindFaces(img)

		if err != nil {
			log.Println(""Fail to infer:"", err)
		}

		log.Println(scores, boxes)
	}
}

```


# System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu-based Docker images**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Not tested**
- TensorFlow installed from (source or binary): **Binary/Pre-compiled** (https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.15.0.tar.gz)
- TensorFlow version (use command below): **1.15.0**
- Python version: -
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -
- Golang version: **1.13**

**photo.jpg:** https://user-images.githubusercontent.com/2982775/72982648-96393b80-3df0-11ea-807f-c4979b4c3af2.jpg
**TF Graph:** [graph.zip](https://github.com/tensorflow/tensorflow/files/4102930/graph.zip)
"
36154,Error while converting to quantized tflite.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3 LTS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.1.0
- Python version : 3.5.6 anaconda

**Describe the Problem**
I implemented lite-weight custom model based on SSD Paper to detect just one Object, Person.
I want to run that model on Coral EdgeTpu which is connected to Raspberry Pi 4 B.
I follow this doc (https://coral.ai/docs/accelerator/get-started/)  to set up coral on rpi and 
follow this doc (https://coral.ai/docs/edgetpu/compiler/) to install EdgeTpu Compiler on my laptop.
Want to convert tflite to edgetpu_tflite, but tflite need to be quantized so follow that doc
(https://www.tensorflow.org/lite/performance/post_training_integer_quant) to do that.

**Command used to run the converter or code if you’re using the Python API**

```
import tensorflow as tf
import cv2
import numpy as np
import os

converter = tf.lite.TFLiteConverter.from_saved_model('./frozen_models/ssd_tf/')
# fitst set the optimizations flag to optimize for size
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]

train_data_dir = '/home/shashank/Documents/SSD7_tf/Training-data/test/'

images = []
for each_img in os.listdir(train_data_dir):
    read_img = cv2.imread(train_data_dir + each_img).astype(np.float32)
    norm_img = (read_img - np.min(read_img, axis=(0,1,2), keepdims=True))/ (np.max(read_img, axis=(0,1,2), keepdims=True) - np.min(read_img, axis=(0,1,2), keepdims=True))
    images.append(norm_img)

images = np.array(images)
images_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)

def representative_data_gen():
    for input_value in images_ds.take(100):
        yield [input_value]

converter.representative_dataset = representative_data_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

tflite_model_quant = converter.convert()
tflite_model_quant_file = 'model_quant_io.tflite'
tflite_model_quant_file.write_bytes(tflite_model_quant)
print('Done!')
```

**The output from the converter invocation**

```
2020-01-23 15:15:34.464864: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2020-01-23 15:15:34.464958: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2020-01-23 15:15:34.464974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-01-23 15:15:35.849036: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-01-23 15:15:35.849060: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)
2020-01-23 15:15:35.849095: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (shashank): /proc/driver/nvidia/version does not exist
2020-01-23 15:15:35.849245: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-23 15:15:35.873039: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1800000000 Hz
2020-01-23 15:15:35.873694: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559e284c8740 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-01-23 15:15:35.873733: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-01-23 15:15:38.481817: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 544320000 exceeds 10% of system memory.
2020-01-23 15:15:38.675897: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-01-23 15:15:38.676034: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-01-23 15:15:38.717217: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize
2020-01-23 15:15:38.717254: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 543 nodes (474), 995 edges (926), time = 12.469ms.
2020-01-23 15:15:38.717260: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 543 nodes (0), 995 edges (0), time = 8.12ms.
2020-01-23 15:15:38.717264: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_model_1_decoded_predictions_cond_true_2404_6083
2020-01-23 15:15:38.717287: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-01-23 15:15:38.717290: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-23 15:15:38.717294: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_1_false_2596_2599
2020-01-23 15:15:38.717298: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-23 15:15:38.717301: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-23 15:15:38.717305: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_false_2547_5852
2020-01-23 15:15:38.717308: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-01-23 15:15:38.717312: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-23 15:15:38.717315: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_true_2546_1326
2020-01-23 15:15:38.717319: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-01-23 15:15:38.717323: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-23 15:15:38.717326: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_model_1_decoded_predictions_loop_over_batch_while_cond_2488_1300
2020-01-23 15:15:38.717331: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-23 15:15:38.717335: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-23 15:15:38.717339: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_2_false_2652_4375
2020-01-23 15:15:38.717343: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-23 15:15:38.717346: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-01-23 15:15:38.717350: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_model_1_decoded_predictions_loop_over_batch_while_body_2489_5925
2020-01-23 15:15:38.717354: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 80 nodes (0), 85 edges (0), time = 1.227ms.
2020-01-23 15:15:38.717357: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 80 nodes (0), 85 edges (0), time = 1.357ms.
2020-01-23 15:15:38.717361: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_2_true_2651_2758
2020-01-23 15:15:38.717365: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-01-23 15:15:38.717369: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-23 15:15:38.717372: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_1_true_2595_4150
2020-01-23 15:15:38.717377: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-23 15:15:38.717381: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-23 15:15:38.717384: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_model_1_decoded_predictions_cond_false_2405_759
2020-01-23 15:15:38.717388: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-01-23 15:15:38.717392: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-23 15:15:39.008345: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-01-23 15:15:39.008521: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-01-23 15:15:39.071925: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize
2020-01-23 15:15:39.071961: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 435 nodes (-60), 804 edges (-126), time = 21.02ms.
2020-01-23 15:15:39.071983: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 435 nodes (0), 804 edges (0), time = 11.699ms.
2020-01-23 15:15:39.071987: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_model_1_decoded_predictions_cond_true_2404_6083_frozen
2020-01-23 15:15:39.071992: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 26 nodes (0), 24 edges (0), time = 0.477ms.
2020-01-23 15:15:39.072011: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 26 nodes (0), 24 edges (0), time = 0.382ms.
2020-01-23 15:15:39.072016: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_model_1_decoded_predictions_cond_false_2405_759_frozen
2020-01-23 15:15:39.072020: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 22 nodes (0), 16 edges (0), time = 0.366ms.
2020-01-23 15:15:39.072023: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 22 nodes (0), 16 edges (0), time = 0.293ms.
2020-01-23 15:15:39.072026: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_2_false_2652_4375_frozen
2020-01-23 15:15:39.072030: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 22 nodes (0), 24 edges (0), time = 0.473ms.
2020-01-23 15:15:39.072033: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 22 nodes (0), 24 edges (0), time = 0.391ms.
2020-01-23 15:15:39.072037: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_true_2546_1326_frozen
2020-01-23 15:15:39.072040: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 4 nodes (-1), 1 edges (-1), time = 0.226ms.
2020-01-23 15:15:39.072044: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 4 nodes (0), 1 edges (0), time = 0.066ms.
2020-01-23 15:15:39.072047: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_model_1_decoded_predictions_loop_over_batch_while_cond_2488_1300_frozen
2020-01-23 15:15:39.072051: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 8 edges (0), time = 0.255ms.
2020-01-23 15:15:39.072055: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 8 edges (0), time = 0.179ms.
2020-01-23 15:15:39.072059: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_2_true_2651_2758_frozen
2020-01-23 15:15:39.072062: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 11 nodes (0), 11 edges (0), time = 0.284ms.
2020-01-23 15:15:39.072066: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 11 nodes (0), 11 edges (0), time = 0.188ms.
2020-01-23 15:15:39.072069: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_1_true_2595_4150_frozen
2020-01-23 15:15:39.072073: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 4 nodes (-1), 1 edges (-1), time = 0.21ms.
2020-01-23 15:15:39.072077: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 4 nodes (0), 1 edges (0), time = 0.078ms.
2020-01-23 15:15:39.072081: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_1_false_2596_2599_frozen
2020-01-23 15:15:39.072084: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 39 nodes (0), 43 edges (0), time = 0.726ms.
2020-01-23 15:15:39.072088: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 39 nodes (0), 43 edges (0), time = 0.745ms.
2020-01-23 15:15:39.072092: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_model_1_decoded_predictions_loop_over_batch_while_body_2489_5925_frozen
2020-01-23 15:15:39.072096: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 81 nodes (1), 87 edges (2), time = 2.628ms.
2020-01-23 15:15:39.072100: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 81 nodes (0), 87 edges (0), time = 1.5ms.
2020-01-23 15:15:39.072103: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_false_2547_5852_frozen
2020-01-23 15:15:39.072107: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 37 nodes (1), 41 edges (2), time = 1.256ms.
2020-01-23 15:15:39.072111: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 37 nodes (0), 41 edges (0), time = 0.653ms.
Traceback (most recent call last):
  File ""generate_quant_tflite.py"", line 30, in <module>
    tflite_model_quant = converter.convert()
  File ""/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py"", line 464, in convert
    **converter_kwargs)
  File ""/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/tensorflow_core/lite/python/convert.py"", line 457, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/tensorflow_core/lite/python/convert.py"", line 203, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2020-01-23 15:15:40.457472: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2020-01-23 15:15:40.457547: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2020-01-23 15:15:40.457557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-01-23 15:15:41.229352: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-23 15:15:41.253051: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1800000000 Hz
2020-01-23 15:15:41.253841: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f574b1d500 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-01-23 15:15:41.253867: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-01-23 15:15:41.255187: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-01-23 15:15:41.255200: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)
2020-01-23 15:15:41.255215: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (shashank): /proc/driver/nvidia/version does not exist
2020-01-23 15:15:41.266040: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: StatelessIf
2020-01-23 15:15:41.266100: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)
Fatal Python error: Aborted

Current thread 0x00007f4c7a502740 (most recent call first):
  File ""/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 56 in execute
  File ""/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/absl/app.py"", line 250 in _run_main
  File ""/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/absl/app.py"", line 299 in run
  File ""/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/tensorflow_core/python/platform/app.py"", line 40 in run
  File ""/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 93 in main
  File ""/home/shashank/ENTER/envs/dl_py35/bin/toco_from_protos"", line 8 in <module>
Aborted (core dumped)
```

**Also, please include a link to the saved model or GraphDef**

```
Please copy and paste this link to browser.
(https://github.com/tensorflow/tensorflow/files/4102761/tflite_bug_related_folder.zip)
This folder contains source code to generate the saved model folder.
It also contain saved model folder also.
run python ssd7_inference.py
```"
36153,MultiWorkerMirroredStrategy training does not work with multiple epochs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Minor adaption of https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.5
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):2.1.0
- Python version: 3.7.4
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: K80

**Describe the current behavior**

Executing training over multiple epochs and workers as per the example fails with ""Empty training data"" and
> WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1407 batches). You may need to use the repeat() function when building your dataset.

**Describe the expected behavior**

Running training over multiple epochs automatically repeats the training data

**Code to reproduce the issue**

Pretty much the exact example code from the MultiWorkerDistributed tutorial:

```
#!/usr/bin/env python

import os
import json
import tensorflow_datasets as tfds
import tensorflow as tf
from slurm_utils import create_tf_config
tfds.disable_progress_bar()

BUFFER_SIZE = 10000
BATCH_SIZE = 64


def make_datasets_unbatched():
    # Scaling MNIST data from (0, 255] to (0., 1.]
    def scale(image, label):
        image = tf.cast(image, tf.float32)
        image /= 255
        return image, label

    datasets, info = tfds.load(name='mnist',
                               with_info=True,
                               as_supervised=True)

    return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)


def build_and_compile_cnn_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32,
                               3,
                               activation='relu',
                               input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,
                  optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
                  metrics=['accuracy'])
    return model


tfConfig = create_tf_config(gpus_per_task=2)
print('Used Config: {}'.format(tfConfig))
os.environ['TF_CONFIG'] = json.dumps(tfConfig)

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
print('Number of workers: {}\nParameter devices: {}\nWorkers: {}'.format(
    strategy.num_replicas_in_sync, strategy.extended.parameter_devices,
    strategy.extended.worker_devices))

# Here the batch size scales up by number of workers since
# `tf.data.Dataset.batch` expects the global batch size.
GLOBAL_BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync
with strategy.scope():
    # Creation of dataset, and model building/compiling need to be within
    # `strategy.scope()`.
    train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)
    multi_worker_model = build_and_compile_cnn_model()

multi_worker_model.fit(x=train_datasets, epochs=3)
```

**Other info / logs**
The above `create_tf_config` creates the following config from SLURM environment: `{'cluster': {'worker': ['taurusa9:8888']}, 'task': {'type': 'worker', 'index': 0, 'trial': 1}, 'job': None}`


Running with 1 epoch works without any other code changes"
36152,Error while converting to tflite,"Hi I am using Tensorflow 2.0 installed through anaconda
I was trying to convert an RNN model to TFlite and I got an error.
ConverterError                            Traceback (most recent call last)
<ipython-input-9-d0e813d03e06> in <module>
      1 converter=tf.lite.TFLiteConverter.from_keras_model(model)
      2 converter.experimental_new_converter = True
----> 3 tflite=converter.convert()

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\lite\python\lite.py in convert(self)
    444         input_tensors=input_tensors,
    445         output_tensors=output_tensors,
--> 446         **converter_kwargs)
    447 
    448     if self._is_calibration_quantize():

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\lite\python\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)
    447       input_data.SerializeToString(),
    448       debug_info_str=debug_info_str,
--> 449       enable_mlir_converter=enable_mlir_converter)
    450   return data
    451 

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\lite\python\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    198       stdout = _try_convert_to_unicode(stdout)
    199       stderr = _try_convert_to_unicode(stderr)
--> 200       raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
    201   finally:
    202     # Must manually cleanup files.

ConverterError: See console for info.
2020-01-23 15:09:32.712287: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2020-01-23 15:10:13.660078: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2020-01-23 15:10:13.667155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-01-23 15:10:14.122950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce 940M major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:01:00.0
2020-01-23 15:10:14.125064: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2020-01-23 15:10:14.131581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-01-23 15:10:17.047944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-23 15:10:17.049761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-01-23 15:10:17.050571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-01-23 15:10:17.052847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3048 MB memory) -> physical GPU (device: 0, name: GeForce 940M, pci bus id: 0000:01:00.0, compute capability: 5.0)
2020-01-23 15:10:17.115704: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor
2020-01-23 15:10:17.116879: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-23 15:10:17.118307: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve
2020-01-23 15:10:17.119396: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-23 15:10:17.120452: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While
2020-01-23 15:10:17.121448: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-23 15:10:17.122442: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-23 15:10:17.123502: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack
2020-01-23 15:10:17.147899: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 18 operators, 54 arrays (0 quantized)
2020-01-23 15:10:17.149816: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 18 operators, 54 arrays (0 quantized)
2020-01-23 15:10:17.157346: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 6 operators, 34 arrays (0 quantized)
2020-01-23 15:10:17.158946: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 6 operators, 34 arrays (0 quantized)
2020-01-23 15:10:17.161212: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 6 operators, 34 arrays (0 quantized)
2020-01-23 15:10:17.172833: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.
2020-01-23 15:10:17.214437: E tensorflow/lite/toco/toco_tooling.cc:466] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, STRIDED_SLICE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\Scripts\toco_from_protos-script.py"", line 10, in <module>
    sys.exit(main())
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\absl\app.py"", line 299, in run
    _run_main(main, args)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\absl\app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 52, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, STRIDED_SLICE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.

"
36151,Error in pywrap_tensorflow.py after installing tensorflow,"Hi!
I have this error after installing Tensor Flow. I downgraded my python version to python 3.6.9 and it still doesn't work. 


ImportError: Traceback (most recent call last):
  File ""C:\Anaconda3_x64\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Anaconda3_x64\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Anaconda3_x64\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Anaconda3_x64\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Anaconda3_x64\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.


I also tried to uninstall it and after installing  tf-nightly and it gives the same error when importing tensorflow."
36150,Cause: No module named 'tensorflow_core.estimator',"WARNING:tensorflow:Entity <bound method TopLevelFeature.decode_example of Translation({
    'en': Text(shape=(), dtype=tf.string),
    'pt': Text(shape=(), dtype=tf.string),
})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: cannot import name 'dense_features' from 'tensorflow.python.feature_column' (/home/local/ZOHOCORP/dharani-pt3410/.local/lib/python3.7/site-packages/tensorflow_core/python/feature_column/__init__.py)
WARNING:tensorflow:Entity <bound method TopLevelFeature.decode_example of Translation({
    'en': Text(shape=(), dtype=tf.string),
    'pt': Text(shape=(), dtype=tf.string),
})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: cannot import name 'dense_features' from 'tensorflow.python.feature_column' (/home/local/ZOHOCORP/dharani-pt3410/.local/lib/python3.7/site-packages/tensorflow_core/python/feature_column/__init__.py)
WARNING: Entity <bound method TopLevelFeature.decode_example of Translation({
    'en': Text(shape=(), dtype=tf.string),
    'pt': Text(shape=(), dtype=tf.string),
})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: cannot import name 'dense_features' from 'tensorflow.python.feature_column' (/home/local/ZOHOCORP/dharani-pt3410/.local/lib/python3.7/site-packages/tensorflow_core/python/feature_column/__init__.py)
WARNING:tensorflow:Entity <bound method TopLevelFeature.decode_example of Translation({
    'en': Text(shape=(), dtype=tf.string),
    'pt': Text(shape=(), dtype=tf.string),
})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'
WARNING:tensorflow:Entity <bound method TopLevelFeature.decode_example of Translation({
    'en': Text(shape=(), dtype=tf.string),
    'pt': Text(shape=(), dtype=tf.string),
})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'
WARNING: Entity <bound method TopLevelFeature.decode_example of Translation({
    'en': Text(shape=(), dtype=tf.string),
    'pt': Text(shape=(), dtype=tf.string),
})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'
WARNING:tensorflow:Entity <bound method TopLevelFeature.decode_example of Translation({
    'en': Text(shape=(), dtype=tf.string),
    'pt': Text(shape=(), dtype=tf.string),
})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'
WARNING:tensorflow:Entity <bound method TopLevelFeature.decode_example of Translation({
    'en': Text(shape=(), dtype=tf.string),
    'pt': Text(shape=(), dtype=tf.string),
})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'
WARNING: Entity <bound method TopLevelFeature.decode_example of Translation({
    'en': Text(shape=(), dtype=tf.string),
    'pt': Text(shape=(), dtype=tf.string),
})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'"
36148,Compatibility issue with cudnn 7.0.5 and cuda 9.0,"My source code required tensorflow 1.7.0 with cuda 9.0 and cudnn 7.0.5 version. However when i install cudnn 7.0.5 then my cuda 9.0 version downgraded to 8.0 that arise error during training. While when i install cuda 9.0 and cudnn 7.1.2 then i got following error:

**E tensorflow/stream_executor/cuda/cuda_dnn.cc:396] Loaded runtime CuDNN library: 7102 (compatibility version 7100) but source was compiled with 7005 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.**"
36147,What factors influence training time through distributed strategy?,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04
- TensorFlow installed from (source or binary):
Source
- TensorFlow version (use command below):
TF2.1.0
- Python version:
Python3.5
- CUDA/cuDNN version:
cuda10.1
- GPU model and memory:
v100 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I tried to get the training time / epoch under [1, 2, 4, 8] GPUs with tf.distribute.MirroredStrategy. However, I got a very interesting result: the time cost is not only linear to the size fo dataset or GPU num, but also influenced by the training loop style, custom training loop or Keras API.
Here is the result table I make. It's not a  serious experiment but hopefully, we could get some inspiration.
![image](https://user-images.githubusercontent.com/33815430/72955745-25862500-3dd8-11ea-9e0f-ea8b00ab28db.png)


Some key points should know beforehand
1. I accumulate and average last 4 epochs out of total 5 epochs, since the first epoch always takes longer than the rest of epochs. 
2. loading styles are two: tfds.load() and the TFrecords loading recommended by tensorflow.
3. For TFrecords loading style, I first converted original images into TFrecords files, and then using TFRecords loading.
4. I promise shards number of TFrecords, global_total_batch_size, loading style don't affect training time greatly.
5. I downloaded the original datasets of tf_flowers to my disk. The meaning of _n_ in tf_flowers * n are the times I copied and pasted.  The only purpose is to enlarge the size of the dataset.
6. Please look at the rows of tf_fowers*n. With the size of the tf_flowers doubles, training time doubles with all GPU numbers. It makes sense and is same with our expectation.
7. However, changing the dataset from tf_flowers to 10k dogs(my datasets including 10k dogs of 10 classes), but keep an equivalent size. The time doesn't change as expects. 
tf_flowers*8 and 10k dogs are in similar size but tf_flowers*8 takes 3 times longer than 10k dogs
tf_flowers*20 and 10k dogs*2 are in similar size but tf_flowers*20 takes 3 times longer than 10k dogs*2
8. Keras API performs strangely as well. GPU number doesn't affect the training time linearly


**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Keras API
```python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

import tensorflow as tf
import tensorflow_datasets as tfds
import time
import datetime

import argparse
import sys
import shutil
import time
import os
import numpy as np

from functools import partial
from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, GlobalAveragePooling2D
from tensorflow.keras import Model
from tensorflow.keras.datasets.fashion_mnist import load_data
import tensorflow_datasets as tfds
import read_params
from train_config import configure_model, configure_optimizer, configure_lossfunc
from datasets.readtf_utils.dataset import get_dataset 
from datasets.readtf_utils.dataset import _parse_fn

os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1,2,3,4,5,6,7""
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    # Currently, memory growth needs to be the same across GPUs
    for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True)
    logical_gpus = tf.config.experimental.list_logical_devices('GPU')
  except RuntimeError as e:
    # Memory growth must be set before GPUs have been initialized
    print(e)


num_epochs = 5
batch_size_per_replica = 128
learning_rate = 0.001
setting_GPUs_num = 8
devices = [""/gpu:""+str(i) for i in range(setting_GPUs_num)]

strategy = tf.distribute.MirroredStrategy(devices)
GPUs_num = strategy.num_replicas_in_sync
print('Number of devices: %d' % GPUs_num)  # 输出设备数量

batch_size = batch_size_per_replica

# 载入数据集并预处理
def resize(image, label):
    image = tf.image.resize(image, [224, 224]) / 255.0
    return image, label



class MyCustomCallback(tf.keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.train_begin = time.time()
        self.times = []
        # print('trian begins at {}'.format(self.train_begin))
        # d

    def on_epoch_begin(self, epoch, logs=None):
        self.epoch_begin = time.time()
        # print('epoch: {} begins at {}'.format(epoch, self.epoch_begin))

    def on_epoch_end(self, epoch, logs=None):
        self.epoch_end = time.time()
        self.epoch = epoch
        # print('epoch: {} ends at {}'.format(epoch, self.epoch_end))
        self.times.append(self.epoch_end-self.epoch_begin)
        print("" epoch: {} takes: {}"".format(epoch, self.epoch_end-self.epoch_begin))

    def on_train_end(self, epoch, logs=None):
        self.train_end = time.time()
        # print('training takes {} secs/epoch: '.format((self.train_end - self.train_begin)/self.epoch))
        print('training takes average {:.2f} secs/epoch'.format(sum(self.times[1::]) / (self.epoch)))



tfrecords_dir = ""/data121/lijiayuan/test/classify_flowers/datasets/""
dataset, _ = get_dataset(tfrecords_dir, subset=""train"", batch_size=batch_size)


if GPUs_num == 1:
    model = tf.keras.applications.MobileNetV2()
    model.compile(
        optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        metrics=[tf.keras.metrics.sparse_categorical_accuracy]
    )
else:
    with strategy.scope():
        model = tf.keras.applications.MobileNetV2()
        model.compile(
            optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),
            loss=tf.keras.losses.sparse_categorical_crossentropy,
            metrics=[tf.keras.metrics.sparse_categorical_accuracy]
            # run_eagerly=False
        )


start = time.time()
model.fit(dataset, epochs=num_epochs, callbacks=[MyCustomCallback()])
end = time.time()

print(""{} GPUs takes {:.2f} secs/epoch = {:.2f} mins/epoch"".format(strategy.num_replicas_in_sync, 
                                                                    (end-start)/num_epochs, 
                                                                    (end-start)/60/num_epochs))

```

Custom training loop
```python
from __future__ import absolute_import, division, print_function, unicode_literals

# Import TensorFlow
import tensorflow as tf

# Helper libraries
import numpy as np
import os
import time
import tensorflow_datasets as tfds
from datasets.readtf_utils.dataset import get_dataset 
from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, GlobalAveragePooling2D



os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1,2,3,4,5,6,7""
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    # Currently, memory growth needs to be the same across GPUs
    for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True)
    logical_gpus = tf.config.experimental.list_logical_devices('GPU')
  except RuntimeError as e:
    # Memory growth must be set before GPUs have been initialized
    print(e)



def resize(image, label):
    image = tf.image.resize(image, [224, 224]) / 255.0
    return image, label

def assemble_model(num_classes, model_name='MobileNetV2'):
    import tensorflow as tf 
    base_model = tf.keras.applications.ResNet50(input_shape=(224,224,3),
                                                    weights='imagenet',
                                                    include_top=False)
    model = tf.keras.Sequential([
                                base_model,
                                GlobalAveragePooling2D(),
                                Dense(num_classes, activation='softmax')
                                ])
    model.trainable = True
    return model


print(tf.__version__)

setting_GPUs_num = 8
devices = [""/gpu:""+str(i) for i in range(setting_GPUs_num)]

strategy = tf.distribute.MirroredStrategy(devices)
print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))
BATCH_SIZE_PER_REPLICA = 256
GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

EPOCHS = 5


##-----dataset------##
tfrecords_dir = ""/data121/lijiayuan/test/classify_flowers/datasets""
train_ds, classes_num = get_dataset(tfrecords_dir, subset=""train"", batch_size=GLOBAL_BATCH_SIZE)


train_ds = strategy.experimental_distribute_dataset(train_ds)






with strategy.scope():
  # Set reduction to `none` so we can do the reduction afterwards and divide by
  # global batch size.
  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
      reduction=tf.keras.losses.Reduction.NONE)
  # or loss_fn = tf.keras.losses.sparse_categorical_crossentropy
  def compute_loss(labels, predictions):
    per_example_loss = loss_object(labels, predictions)
    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)


with strategy.scope():

  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
      name='train_accuracy')

# model and optimizer must be created under `strategy.scope`.
with strategy.scope():
  model = assemble_model(num_classes=classes_num)
  optimizer = tf.keras.optimizers.Adam()
  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)

with strategy.scope():
  def train_step(inputs):
    images, labels = inputs

    with tf.GradientTape() as tape:
      predictions = model(images, training=True)
      loss = compute_loss(labels, predictions)

    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    train_accuracy.update_state(labels, predictions)
    return loss 




with strategy.scope():

  @tf.function
  def distributed_train_step(dataset_inputs):
    per_replica_losses = strategy.experimental_run_v2(train_step,
                                                      args=(dataset_inputs,))
    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,
                           axis=None)
 
 
  times = []
  for epoch in range(EPOCHS):
    # TRAIN LOOP

    total_loss = 0.0
    num_batches = 0
    epoch_start = time.time()
    for x in train_ds:
      total_loss += distributed_train_step(x)
      num_batches += 1
    train_loss = total_loss / num_batches
    epoch_end = time.time()
    
    if epoch != 0:
      times.append(epoch_end-epoch_start)
    



    template = (""Epoch {}, Loss: {:.2f}, Accuracy: {:.2f}, ""
                "" Takes: {:.2f}"")
    print (template.format(epoch+1, train_loss,
                           train_accuracy.result()*100, 
                           epoch_end-epoch_start))

    train_accuracy.reset_states()
  print(""{} GPUs takes average {:.2f} secs"".format(setting_GPUs_num, 
                                                    sum(times)/(EPOCHS-1)))


```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
36146,tf.function and tf.nest break for valid Mapping instances,"`tf.function` makes invalid assumptions about arguments that are `Mapping` instances. In general, there are no requirements for `Mapping` instances to have constructors that accept `[(key, value)] ` initializers, [as assumed here](https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/util/nest.py#L145).

This leads to cryptic exceptions when used with perfectly valid `Mapping` subclasses such as this one:
```python
class CustomMapping(Mapping):

  def __init__(self, **kwargs):
    self.mapping = kwargs
  
  def __getitem__(self, key):
    return self.mapping[key]
  
  def __iter__(self):
    return iter(self.mapping)

  def __len__(self):
    return len(self.mapping)
```

See this [Colab notebook for an example](https://colab.research.google.com/drive/1sIbvyPVtQexCWqVO7QD3KlY7PoLdPeKq).
"
36144,makefile fails to execute a.out file,"I am new to TensorFlow. At the moment, I am learning TensorFlow for C code on my Macbook. I follow the instructions by Google and manage to run the 'hello world' in C.

I set the paths on my bash_profile as follows:

export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/tensorflow/lib

export DYLD_LIBRARY_PATH=$DYLD_LIBRARY_PATH:/usr/local/tensorflow/lib.

My makefile is as follow.

tensorlib = -L/usr/local/tensorflow-1.15.0/lib/

tensorlnk = -I/usr/local/tensorflow-1.15.0/include/

task:

mpicc -c hello.c $(tensorlnk)

mpicc    hello.o $(tensorlib)
run:

mpirun -np 4 ./a.out
I can compile the program and generate the executable, a.out. In my terminal, I can run the program using 'mpirun -np 4 ./a.out' to execute. But the command 'make run' fails. Here is the error message

dyld: Library not loaded: @rpath/libtensorflow.1.dylib Referenced from: /Users/usr/Documents/code/dev/TensorFlow/c/./a.out Reason: image not found

Could anyone tell me what the problem is?

Many thanks in advance.
"
36143,Memory leak at tflite::Interpreter::Invoke(),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): tflite example to inference with mobilenetV2
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): tflite 2.1
- Python version: 3.6
- Bazel version (if compiling from source): 1.1.0
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: NO
- GPU model and memory: NO


**Describe the current behavior**
We are running and tflite example using the tensorflow-lite.a library generated by `./tensorflow/lite/tools/make/build_lib.sh`  this command.
The example works but it has a memory leak
**Other info / logs**
This is the log that I get using heapusage, as you can see in bold the problem is with tflite::Interpreter::Invoke() 

`
==1359== 1216 bytes in 4 block(s) are lost, originally allocated at:
==1359==    at 0x00007fd0cce977f2: calloc + 68
==1359==    at 0x00007fd0cd0ba4a7: _dl_allocate_tls + 39
==1359==    at 0x00007fd0cbdc1228: pthread_create + 2168
==1359==    at 0x00007fd0cc64b925: std::thread::_M_start_thread(std::unique_ptr<std::thread::_State, std::default_delete<std::thread::_State> >, void (*)()) + 21
==1359==    at 0x00007fd0ccb488f5: tflite::eigen_support::GetThreadPoolDevice(TfLiteContext*) + 2005
==1359==    at 0x00007fd0ccb1d9d2: void tflite::ops::builtin::conv::EvalFloat<(tflite::ops::builtin::conv::KernelType)2>(TfLiteContext*, TfLiteNode*, TfLiteConvParams*, tflite::ops::builtin::conv::OpData*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) + 1250
==1359==    at 0x00007fd0ccb1e515: TfLiteStatus tflite::ops::builtin::conv::Eval<(tflite::ops::builtin::conv::KernelType)2>(TfLiteContext*, TfLiteNode*) + 501
==1359==    at 0x00007fd0cca71ba7: tflite::Subgraph::Invoke() + 775
**==1359==    at 0x00007fd0cc9d7b89: tflite::Interpreter::Invoke() + 25
==1359==    at 0x00007fd0cc9caa44: r2i::tflite::Engine::Predict(std::shared_ptr<r2i::IFrame>, r2i::RuntimeError&) + 1122**
==1359==    at 0x000055f42ee148eb: 
==1359==    at 0x00007fd0cb9e9b97: __libc_start_main + 231
==1359==    at 0x000055f42edf2bda: 
==1359== 

`
"
36142,TF2.0 to 2.1 breaking for tf.image.random_jpeg_quality,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- TensorFlow version (use command below): 2.1.0
- Python version: 3.5

**Describe the current behavior**
I upgraded to tensorflow 2.1 and everything in my project is working fine except for one thing. Calling `tf.image.random_jpeg_quality` throws a ValueError: `as_list() is not defined on an unknown TensorShape`. The full error is pasted at the bottom of this issue.

The code works after removing the call to random_jpeg_quality. That works for me at this point. Just raising this issue in case it hasn't been noted yet.

**Other info / logs**
```
ValueError: in converted code:

    /data.py:261 _parse_example_augment  *
        image = tf.image.random_jpeg_quality(image=image, min_jpeg_quality=60, max_jpeg_quality=100)
    /root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/ops/image_ops_impl.py:2031 random_jpeg_quality
        return adjust_jpeg_quality(image, jpeg_quality)
    /root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/ops/image_ops_impl.py:2064 adjust_jpeg_quality
        channels = image.shape.as_list()[-1]
    /root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/framework/tensor_shape.py:1166 as_list
        raise ValueError(""as_list() is not defined on an unknown TensorShape."")

    ValueError: as_list() is not defined on an unknown TensorShape.
```
"
36141,HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory,"**System information**
- Linux tutkain 4.19.93-v7l+ #1290 SMP Fri Jan 10 16:45:11 GMT 2020 armv7l GNU/Linux
- Debian 10.2 on Raspberry PI 4+ 4GB

**Describe the problem**
(venv) tas@tutkain:~ $ python -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
2020-01-22 20:21:52.312444: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory
Tensor(""Sum:0"", shape=(), dtype=float32)

**Provide the exact sequence of commands / steps that you executed before running into the problem**
- Installation exactly as described https://www.tensorflow.org/install/pip

**Any other info / logs**
(venv) tas@tutkain:~ $ pip list
Package              Version
-------------------- ---------
absl-py              0.9.0
asn1crypto           0.24.0
astor                0.8.1
cachetools           4.0.0
certifi              2018.8.24
chardet              3.0.4
cryptography         2.6.1
distro-info          0.21
entrypoints          0.3
gast                 0.2.2
google-auth          1.10.2
google-auth-oauthlib 0.4.1
google-pasta         0.1.8
grpcio               1.26.0
h5py                 2.8.0
idna                 2.6
Keras-Applications   1.0.8
Keras-Preprocessing  1.1.0
keyring              17.1.1
keyrings.alt         3.1.1
Markdown             3.1.1
numpy                1.16.2
oauthlib             3.1.0
opt-einsum           3.1.0
pip                  20.0.1
protobuf             3.11.2
pyasn1               0.4.8
pyasn1-modules       0.2.8
pycrypto             2.6.1
pycurl               7.43.0.2
PyGObject            3.30.4
python-apt           1.8.4
pyxdg                0.25
requests             2.21.0
requests-oauthlib    1.3.0
rsa                  4.0
SecretStorage        2.3.1
setuptools           45.1.0
six                  1.12.0
ssh-import-id        5.7
tensorboard          2.0.2
tensorflow           1.14.0
tensorflow-estimator 1.14.0
termcolor            1.1.0
unattended-upgrades  0.1
urllib3              1.24.1
virtualenv           16.7.9
Werkzeug             0.16.0
wheel                0.33.6
wrapt                1.11.2
"
36140,ValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.,"### System information
2.0.0 Tensorflow version
IDE Spyder(Python 3.7)

### Source code / logs
import sys
import os
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.keras import optimizers
from tensorflow.python.keras import Sequential
from tensorflow.python.keras.layers import Flatten, Dense, Activation
from tensorflow.python.keras.layers import Convolution2D, MaxPooling2D
from tensorflow.python.keras.layers import Dropout
from tensorflow.python.keras.optimizers import Adam
from tensorflow.python.keras import backend as K

K.clear_session()
data_entrenamiento='./data/entrenamiento'
data_validacion= './data/validacion'

epocas=10
altura, longitud= 100, 100
batch_size=10 
pasos= 50 
pasos_validacion= 20 

filtrosConv1=32
filtrosConv2=64

tamaño_filtro1=(3,3)#Conv1
tamaño_filtro2=(2,2)#Conv2

tamaño_pool=(2,2)#Maxpooling
clases=2
lr=0.005

entrenamiento_datagen= ImageDataGenerator(rescale=1./255, #255 a 1
                                          shear_range=0.3,#Inclina la imageb
                                          zoom_range=0.3, #Realiza un zoom en la imagen
                                          horizontal_flip=True #Inversión de la imagen
                                          )

validacion_datagen=ImageDataGenerator(
                                        rescale=1./255
                                      )
imagen_entrenamiento=entrenamiento_datagen.flow_from_directory(
        data_entrenamiento,
        target_size=(altura,longitud),
        batch_size=batch_size,
        class_mode='categorical'
        )

imagen_validacion=validacion_datagen.flow_from_directory(
        data_validacion,
        target_size=(altura,longitud),
        batch_size=batch_size,
        class_mode='categorical'
        )

cnn=Sequential()

cnn.add(Convolution2D(filtrosConv1, tamaño_filtro1, padding='same', input_shape=(altura, longitud, 3), activation='relu'))
cnn.add(MaxPooling2D(pool_size=tamaño_pool))

cnn.add(Convolution2D(filtrosConv2, tamaño_filtro2, padding='same', activation='relu'))
cnn.add(MaxPooling2D(pool_size=tamaño_pool))

cnn.add(Flatten())
cnn.add(Dense(256, activation= 'relu'))
cnn.add(Dropout(0.5))
cnn.add(Dense(clases, activation='softmax'))

cnn.compile(loss='categorical_crossentropy', optimizer = optimizers.Adam(lr=lr), metrics=['accuracy'])

cnn.fit(imagen_entrenamiento, steps_per_epoch= pasos, epochs=epocas, validation_data =imagen_validacion, validation_steps=pasos_validacion)

### Problem complete
Found 4001 images belonging to 2 classes.
Found 2000 images belonging to 2 classes.
Epoch 1/10
Traceback (most recent call last):

  File ""<ipython-input-11-083720e5df85>"", line 1, in <module>
    runfile('C:/Users/Acer/Documents/Resp_roja/PROGRAMAS TESIS/python/practica4_TF.py', wdir='C:/Users/Acer/Documents/Resp_roja/PROGRAMAS TESIS/python')

  File ""C:\Users\Acer\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 827, in runfile
    execfile(filename, namespace)

  File ""C:\Users\Acer\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/Acer/Documents/Resp_roja/PROGRAMAS TESIS/python/practica4_TF.py"", line 84, in <module>
    cnn.fit(imagen_entrenamiento, steps_per_epoch= pasos, epochs=epocas, validation_data =imagen_validacion, validation_steps=pasos_validacion)

  File ""C:\Users\Acer\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 728, in fit
    use_multiprocessing=use_multiprocessing)

  File ""C:\Users\Acer\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_generator.py"", line 603, in fit
    steps_name='steps_per_epoch')

  File ""C:\Users\Acer\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_generator.py"", line 265, in model_iteration
    batch_outs = batch_function(*batch_data)

  File ""C:\Users\Acer\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 1017, in train_on_batch
    self._make_train_function()

  File ""C:\Users\Acer\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2116, in _make_train_function
    params=self._collected_trainable_weights, loss=self.total_loss)

  File ""C:\Users\Acer\Anaconda3\lib\site-packages\tensorflow_core\python\keras\optimizers.py"", line 476, in get_updates
    grads = self.get_gradients(loss, params)

  File ""C:\Users\Acer\Anaconda3\lib\site-packages\tensorflow_core\python\keras\optimizers.py"", line 92, in get_gradients
    if None in grads:

  File ""C:\Users\Acer\Anaconda3\lib\site-packages\tensorflow_core\python\ops\math_ops.py"", line 1336, in tensor_equals
    return gen_math_ops.equal(self, other)

  File ""C:\Users\Acer\Anaconda3\lib\site-packages\tensorflow_core\python\ops\gen_math_ops.py"", line 3626, in equal
    name=name)

  File ""C:\Users\Acer\Anaconda3\lib\site-packages\tensorflow_core\python\framework\op_def_library.py"", line 545, in _apply_op_helper
    (input_name, err))

ValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.

### Please help me"
36139,"Inconsistent crashing behavior with CuDNN, tensorflow RNNs, and padding","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0
- Python version: Python 3.6.8
- CUDA/cuDNN version: CUDA Version: 10.1/cuDNN v7.6.5
- GPU model and memory: GTX Titan X

**Describe the current behavior**

CuDNN has inconsistent behavior with RNNs and fully masked sequences. In some cases, handing CuDNN a fully masked sequence works fine --- it happily returns an all zero vector without error, even if the input sequence has no valid tokens in it. This is the behavior that occurs on CPU as well. However --- in other cases, you get a CuDNN error (`CUDNN_STATUS_BAD_PARAM`) when handing fully padded sequences. Strangely, the thing that seems to control whether or not CuDNN error is thrown is: whether or not any of the non-fully-padded sequences in the batch have any padding.

**Describe the expected behavior**

The RNNs should return all-zero vectors for fully padded sequences on CuDNN even if none of the non-fully-padded sequences have padding. (see the below code example)

**Code to reproduce the issue**

```python
import tensorflow as tf
import numpy as np

seq_len = 5
vocab_size = 10
batch_size = 2
hidden_size = 20

seq_input = tf.keras.layers.Input(seq_len, dtype=tf.int32)
embs = tf.keras.layers.Embedding(vocab_size, hidden_size, mask_zero=True)
rnn = tf.keras.layers.GRU(hidden_size)
out = rnn(embs(seq_input))

keras_model = tf.keras.Model(
    inputs=seq_input,
    outputs=out)

working_input = np.array(
    [[0, 1, 2, 3, 4], # if this sequence has a pad, CuDNN is happy
     [0, 0, 0, 0, 0]])

broken_input = np.array(
    [[1, 2, 3, 4, 5], # if this sequence doesn't have a pad, CuDNN is sad
     [0, 0, 0, 0, 0]])

print('Computing the working input:')
res1 = keras_model.predict(working_input)
print(res1.shape)
print(res1[1])
print('Computing the broken input:')
# this line causes the CuDNN error if running on GPU
res2 = keras_model.predict(broken_input)
print(res2.shape)
print(res2[1])
```"
36138,"Tensorflow 2.0.0 cpu, import error: DLL load failed","**System information**
- OS Platform and Distribution: x86_64-w64-mingw32/x64 (64-bit) under Win10 x64 (build 18363),
  Bios V1.08
- Processor: Intel(R) Pentium(R) CPU N3710 @ 1,6GHz, RAM 4GB (DDR), 4 cores, L1 Cache 56kB, 
  L2 Cache 1024kB
- Maker & Model: Acer Notebook TravelMate B117-M Signature Edition
- TensorFlow installed from (source or binary): binary 
  i. under Rstudio 1.2.5033 with R 3.6.2 (20191212) & Miniconda3 4.7.12 (Py 3.7.4-64bit)
  ii. under conda (since i. failed, see below)
- TensorFlow version: 2.0.0
- Python version: 3.6.10 (under conda)
- Installed using virtualenv? pip? conda?: conda=>pip install 

**Describe the problem**
i. Since I used keras (including reticulate & tensorflow) for the last 3 yrs under RStudio, but had various problems with my very old installation, I definitely want to continue with tensorflow + keras under RStudio and tried to install tensorflow 2.0.0. [under RStudio](https://tensorflow.rstudio.com/installation/):
```
install.packages(""tensorflow"", version = ""2.0.0."")   
library(tensorflow)
install_tensorflow(method = ""conda"")
```
then test sequence - is tensorflow installed?
```
library(reticulate)
use_condaenv(condaenv = ""r-reticulate"", required = TRUE)
library(tensorflow)
```
```
> reticulate::py_config()
python:         C:/.../Miniconda3/envs/r-reticulate/python.exe
libpython:      C:/.../Miniconda3/envs/r-reticulate/python36.dll
pythonhome:     C:/.../Miniconda3/envs/r-reticulate
version:        3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 15:18:16) [MSC v.1916 64 bit (AMD64)]
Architecture:   64bit
numpy:          C:/.../Miniconda3/envs/r-reticulate/Lib/site-packages/numpy
numpy_version:  1.18.1
tensorflow:     C:\...\MINICO~1\envs\R-RETI~1\lib\site-packages\tensorflow\__init__.p
python versions found: 
 C:/.../Miniconda3/envs/r-reticulate/python.exe
 C:/.../Miniconda3/python.exe
```
```
> tensorflow::tf_config()
**Installation of TensorFlow not found.**
Python environments searched for 'tensorflow' package:
 C:\...\Miniconda3\envs\r-reticulate\python.exe
 C:\...Miniconda3\python.exe
**You can install TensorFlow using the install_tensorflow() function.**
```
Thus, although installation was terminated correctly, tensorflow was not found, i.e. not working.

ii. After forwarding this issue under [Installation w/ Miniconda, Reticulate 1.14, Tensorflow 2.0.0 & Keras 2.2.5.0 failing #964](https://github.com/rstudio/keras/issues/964) to RStudio/keras, tests were made to get tensorflow 2.0.0 pip installed directly under Miniconda3 (also under Anaconda3 and starting with R3.5.3 and R3.6.1), all with the same result: 
```
conda activate r-reticulate
pip install tensorflow==2.0
python 
import tensorflow as tf
```
This produced the following error log (sorry, I'm unable to copy text from the python shell window), indicating `ImportError: DLL load failed` twice:

![import_tensorflow_as_tf_20200122-1800](https://user-images.githubusercontent.com/22592465/72921761-2a04fc00-3d4c-11ea-9fe5-57ab7aaa1545.PNG)

**Any other info**
Unfortunately, I am an absolute newbie as regards Python - taking that into account, please help...
"
36136,TPU Socket Closed,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
Platform: Colab TPU
- TensorFlow version (use command below): 2.1.0rc1

**Describe the current behavior**
[TPU_Socket_Traceback.pdf](https://github.com/tensorflow/tensorflow/files/4098637/TPU_Socket_Traceback.pdf)


Using model.fit raises an error which does not occur with GPU backend.

`UnavailableError: Socket closed
Additional GRPC error information:
{""created"":""@1579711911.023658059"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":""Socket closed"",""grpc_status"":14}`

**Describe the expected behavior**
Code should run the same in both instances.

**Code to reproduce the issue**
https://colab.research.google.com/drive/156Q2BsM9gVS3cvDytovQzEGKmjJpP4s8

**Other info / logs**
Attached.
"
36135,model.save() Error: object has no attribute '_compile_metrics',"<em>With python3 model saving stopped to work.</em>

After switching from python2 to python3 I started to get for my not compiled model the following error:
Error: 'Model' object has no attribute '_compile_metrics' 
Later I switched to TF 2 - right now running with 2.0.1 - but nothing has changed.

**System information**
- Ubuntu, Debian
- TensorFlow installed from binary
- TensorFlow version from 1.x -> 2.0.1
- Python version: 3.5.2
- CUDA/cuDNN version: 8 -> 10.1  based on TF needs
- GPU model and memory: Various.

"
36134,Cannot build TensorFlow from source on Mac OS Catalina,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0
- Python version: 3.7.6
- Installed using virtualenv? pip? conda?: miniconda
- Bazel version (if compiling from source): Using Bazelisk, so I presume 0.12.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
Trying to install TensorFlow from source for CPU only, I encounter this issue after following the install instructions precisely and then using the following code after installing bazelisk with Go.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
bazelisk clean --expunge
sudo xcode-select -s /Applications/Xcode.app/Contents/Developer
sudo xcodebuild -license
bazelisk clean --expunge
bazelisk build --config=opt //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
```
ERROR: /Users/harrisonwilde/python/tensorflow/tensorflow/lite/tools/optimize/calibration/BUILD:106:1: undeclared inclusion(s) in rule '//tensorflow/lite/tools/optimize/calibration:calibration_reader':
this rule is missing dependency declarations for the following files included by 'tensorflow/lite/tools/optimize/calibration/calibration_reader.cc':
  'external/com_google_absl/absl/strings/string_view.h'
  'external/com_google_absl/absl/base/internal/throw_delegate.h'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /Users/harrisonwilde/python/tensorflow/tensorflow/python/tools/BUILD:80:1 undeclared inclusion(s) in rule '//tensorflow/lite/tools/optimize/calibration:calibration_reader':
this rule is missing dependency declarations for the following files included by 'tensorflow/lite/tools/optimize/calibration/calibration_reader.cc':
  'external/com_google_absl/absl/strings/string_view.h'
  'external/com_google_absl/absl/base/internal/throw_delegate.h'
INFO: Elapsed time: 325.470s, Critical Path: 117.47s
INFO: 1821 processes: 1821 local.
FAILED: Build did NOT complete successfully
```
"
36133,initializer() method has no attribute run(),"**System information**
- Windows 10(x64)
- TensorFlow 2.1.0
- Python 3.7
- CUDA 10.0:
- GPU - NVIDIA GeForce GTX 1660 Ti:
__________________________________________________________________________________________________________

I wanted to repeat some code from book named ""Machine Learning with TensorFlow"" of Nishant Shukla (@BinRoot)


![Снимок](https://user-images.githubusercontent.com/43477533/72905924-83285c00-3d5b-11ea-81f7-dd8b37468814.PNG)

However there's an error:
`File ""C:/Users/PC2/PycharmProjects/Adil/newTensor/Main.py"", line 6, in <module>`
`spikes.initializer.run()`
`AttributeError: 'NoneType' object has no attribute 'run'`
__________________________________________________________________________________________________________
**And I would like to know if somebody has faced that before. I tried to reinstall tensorforce but it didn't help. I think it's because of the version update. However I don't know to fix that. I would really appreciate if somebody could help me! Thanks in advance!**"
36132,"Build Tensorflow 2.0 error (CUDA 10.1, Windows 10, conda)","
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0.0
- Python version: 3.7.6 (conda)
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1/ 7.6
- GPU model and memory: GeForce GTX 1660 Ti (6GB)
- Procesor: Intel Core i7 9th Generation (AVX2 support)


**Describe the problem**
I need tensorflow==2.0.0 to use with https://github.com/matterport/Mask_RCNN/pull/1896. (Or fix that pull request to run on my computer).
I couldn't use the official binaries due to CUDA version (I have 10.1 but tf 2.0 needs CUDA 10.0). I can't downgrade it (GPU driver is 10.1 or 10.2). I found https://github.com/tensorflow/tensorflow/issues/33026 issue and there was advice about building tf from source (https://github.com/tensorflow/tensorflow/issues/33026#issuecomment-539915647)

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I was running comands from https://medium.com/@amsokol.com/how-to-build-and-install-tensorflow-2-0-1aab4eaa4c1a which is based on official documentation.

Diffs:
* Git for Windows and MSYS2 are installed on C:/ drive directly (so I set system variables properly).
* Python is installed from conda (3.7.6) (path is set correctly)
* instead of venv I'm using conda. Due to recent pip update I don't run pip update (I have 19.3.1 version)
* I set compute capability as 3.5 (there is no official info on nvidia website https://developer.nvidia.com/cuda-gpus though it's Turing series and should be 7.x)

After executing ```bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package``` it throws the errors:

```ERROR: An error occurred during the fetch of repository ‘io_bazel_rules_docker’: Traceback (most recent call last): File “C:/users/konta/_bazel_konta/cs3mthex/external/bazel_tools/tools/build_defs/repo/git.bzl”, line 234 _clone_or_update(ctx) File “C:/users/konta/_bazel_konta/cs3mthex/external/bazel_tools/tools/build_defs/repo/git.bzl”, line 74, in _clone_or_update fail((“error cloning %s:\n%s” % (ctx….))) error cloning io_bazel_rules_docker: java.io.IOException: ERROR: src/main/native/windows/process.cc(179): CreateProcessW(“C:\msys64\usr\bin” -c “ cd C:/users/konta/_bazel_konta/cs3mthex/external set -ex ( cd C:/users/konta/_bazel_konta/cs3mthex/external && if ! ( cd ‘C:/users/konta/_bazel_konta/cs3mthex/externa(…)): Odmowa dost?pu. ERROR: error loading package ‘’: Encountered error while reading extension file ‘repositories/repositories.bzl’: no such package ‘@io_bazel_rules_docker//repositories’: Traceback (most recent call last): File “C:/users/konta/_bazel_konta/cs3mthex/external/bazel_tools/tools/build_defs/repo/git.bzl”, line 234 _clone_or_update(ctx) File “C:/users/konta/_bazel_konta/cs3mthex/external/bazel_tools/tools/build_defs/repo/git.bzl”, line 74, in _clone_or_update fail((“error cloning %s:\n%s” % (ctx….))) error cloning io_bazel_rules_docker: java.io.IOException: ERROR: src/main/native/windows/process.cc(179): CreateProcessW(“C:\msys64\usr\bin” -c “ cd C:/users/konta/_bazel_konta/cs3mthex/external set -ex ( cd C:/users/konta/_bazel_konta/cs3mthex/external && if ! ( cd ‘C:/users/konta/_bazel_konta/cs3mthex/externa(…)): Odmowa dost?pu. ERROR: error loading package ‘’: Encountered error while reading extension file ‘repositories/repositories.bzl’: no such package ‘@io_bazel_rules_docker//repositories’: Traceback (most recent call last): File “C:/users/konta/_bazel_konta/cs3mthex/external/bazel_tools/tools/build_defs/repo/git.bzl”, line 234 _clone_or_update(ctx) File “C:/users/konta/_bazel_konta/cs3mthex/external/bazel_tools/tools/build_defs/repo/git.bzl”, line 74, in _clone_or_update fail((“error cloning %s:\n%s” % (ctx….))) error cloning io_bazel_rules_docker: java.io.IOException: ERROR: src/main/native/windows/process.cc(179): CreateProcessW(“C:\msys64\usr\bin” -c “ cd C:/users/konta/_bazel_konta/cs3mthex/external set -ex ( cd C:/users/konta/_bazel_konta/cs3mthex/external && if ! ( cd ‘C:/users/konta/_bazel_konta/cs3mthex/externa(…)): Odmowa dost?pu. INFO: Elapsed time: 0.103s INFO: 0 processes. FAILED: Build did NOT complete successfully (0 packages loaded)```

I found the solution of this here: https://github.com/tensorflow/tensorflow/issues/28824#issuecomment-536669038 - I added these lines to WORKSPACE file in tensorflow directory (tensorflow repo).

It gave me the next errors, so I needed to add also 

```
http_archive(
    name = ""com_google_protobuf"",
#    build_file = ""//:third_party/protobuf.BUILD"",
    sha256 = ""b9e92f9af8819bbbc514e2902aec860415b70209f31dfc8c4fa72515a5df9d59"",
    # This protobuf release is based on protobuf 3.8.0.
    strip_prefix = ""protobuf-310ba5ee72661c081129eb878c1bbcec936b20f0"",
    urls = [
        ""https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz"",
        ""https://github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz"",
    ],
)
```
 to my WORKSPACE file. This fix was working. Now it can't fetch of repository 'png_archive' (see log at the bottom). I tried to comment it in BUILD files but it was causing the next lacks (missing flatbuffers). I have no idea how it can be fixed.


**Any other info / logs**

```Odmowa dost?pu.``` means 'Permission/access denied'. I thought it's caused by Windows Defender or antivirus but it's turned off. I'm using terminal with admin privileges and user access control is set as 'never notify'. Probably there is problem with repos (when I added newer config of com_google_protobuf and  io_bazel_rules_docker there wasn't previous errors).

Full log after running build:
```(tf2.0) C:\Users\konta\Documents\Repos\tf-2.0\tensorflow>bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package                                                                          Starting local Bazel server and connecting to it...                                                                     INFO: Options provided by the client:                                                                                     Inherited 'common' options: --isatty=1 --terminal_columns=120                                                         INFO: Options provided by the client:                                                                                     'build' options: --python_path=C:/Users/konta/Miniconda3/envs/tf2.0/python.exe                                        INFO: Reading rc options for 'build' from c:\users\konta\documents\repos\tf-2.0\tensorflow\.bazelrc:                      'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --config=v2                                                                                       INFO: Reading rc options for 'build' from c:\users\konta\documents\repos\tf-2.0\tensorflow\.tf_configure.bazelrc:         'build' options: --action_env PYTHON_BIN_PATH=C:/Users/konta/Miniconda3/envs/tf2.0/python.exe --action_env PYTHON_LIB_PATH=C:/Users/konta/Miniconda3/envs/tf2.0/lib/site-packages --python_path=C:/Users/konta/Miniconda3/envs/tf2.0/python.exe --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5 --config=cuda --config monolithic --copt=-w --host_copt=-w --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --verbose_failures --distinct_host_configuration=false --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0                                                          INFO: Found applicable config definition build:v2 in file c:\users\konta\documents\repos\tf-2.0\tensorflow\.bazelrc: --define=tf_api_version=2                                                                                                  INFO: Found applicable config definition build:cuda in file c:\users\konta\documents\repos\tf-2.0\tensorflow\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true                                                                        INFO: Found applicable config definition build:using_cuda in file c:\users\konta\documents\repos\tf-2.0\tensorflow\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain       INFO: Found applicable config definition build:monolithic in file c:\users\konta\documents\repos\tf-2.0\tensorflow\.bazelrc: --define framework_shared_object=false                                                                             INFO: Found applicable config definition build:opt in file c:\users\konta\documents\repos\tf-2.0\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX2 --define with_default_optimizations=true                                                   INFO: Call stack for the definition of repository 'png_archive' which is a tf_http_archive (rule definition at C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl:124:19):                                                    - C:/users/konta/documents/repos/tf-2.0/tensorflow/tensorflow/workspace.bzl:260:5                                       - C:/users/konta/documents/repos/tf-2.0/tensorflow/WORKSPACE:38:1                                                      INFO: Repository 'png_archive' used the following cache hits instead of downloading the corresponding file.              * Hash 'ca74a0dace179a8422187671aee97dd3892b53e168627145271cad5b5ac81307' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/glennrp/libpng/archive/v1.6.37.tar.gz                                                            If the definition of 'png_archive' was updated, verify that the hashes were also updated.                               ERROR: An error occurred during the fetch of repository 'png_archive':                                                     Traceback (most recent call last):                                                                                           File ""C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl"", line 104                                          _apply_patch(ctx, ctx.attr.patch_file)                                                                          File ""C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl"", line 71, in _apply_patch                          _execute_and_check_ret_code(ctx, cmd)                                                                           File ""C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl"", line 52, in _execute_and_check_ret_code                                                                                                                                   fail(""Non-zero return code({1}) when ...))                                                              Non-zero return code(256) when executing 'C:\msys64\usr\bin -l -c ""patch"" ""-p1"" ""-d"" ""C:/users/konta/_bazel_konta/cs3mthex/external/png_archive"" ""-i"" ""C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/png_fix_rpi.patch""':        Stdout:                                                                                                                 Stderr: java.io.IOException: ERROR: src/main/native/windows/process.cc(179): CreateProcessW(""C:\msys64\usr\bin"" -l -c ""\""patch\"" \""-p1\"" \""-d\"" \""C:/users/konta/_bazel_konta/cs3mthex/external/png_archive\"" \""-i\"" \""C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/png_fix_rpi(...)): Odmowa dost?pu.                                                  INFO: Call stack for the definition of repository 'flatbuffers' which is a third_party_http_archive (rule definition at C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl:204:28):                                           - C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/flatbuffers/workspace.bzl:6:5                            - C:/users/konta/documents/repos/tf-2.0/tensorflow/tensorflow/workspace.bzl:39:5                                        - C:/users/konta/documents/repos/tf-2.0/tensorflow/tensorflow/workspace.bzl:82:5                                        - C:/users/konta/documents/repos/tf-2.0/tensorflow/WORKSPACE:38:1                                                      INFO: Repository 'flatbuffers' used the following cache hits instead of downloading the corresponding file.              * Hash '3f4a286642094f45b1b77228656fbd7ea123964f19502f9ecfd29933fd23a50b' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz                                                        If the definition of 'flatbuffers' was updated, verify that the hashes were also updated.                               INFO: Call stack for the definition of repository 'cython' which is a tf_http_archive (rule definition at C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl:124:19):                                                         - C:/users/konta/documents/repos/tf-2.0/tensorflow/tensorflow/workspace.bzl:754:5                                       - C:/users/konta/documents/repos/tf-2.0/tensorflow/WORKSPACE:38:1                                                      INFO: Repository 'cython' used the following cache hits instead of downloading the corresponding file.                   * Hash 'bccc9aa050ea02595b2440188813b936eaf345e85fb9692790cecfe095cf91aa' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/cython/cython/archive/0.28.4.tar.gz                                                              If the definition of 'cython' was updated, verify that the hashes were also updated.                                    INFO: Call stack for the definition of repository 'eigen_archive' which is a tf_http_archive (rule definition at C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl:124:19):                                                  - C:/users/konta/documents/repos/tf-2.0/tensorflow/tensorflow/workspace.bzl:168:5                                       - C:/users/konta/documents/repos/tf-2.0/tensorflow/WORKSPACE:38:1                                                      INFO: Repository 'eigen_archive' used the following cache hits instead of downloading the corresponding file.            * Hash 'f3d69ac773ecaf3602cb940040390d4e71a501bb145ca9e01ce5464cf6d4eb68' for https://storage.googleapis.com/mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/049af2f56331.tar.gz                                                           If the definition of 'eigen_archive' was updated, verify that the hashes were also updated.                             INFO: Call stack for the definition of repository 'icu' which is a third_party_http_archive (rule definition at C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl:204:28):                                                   - C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/icu/workspace.bzl:11:5                                   - C:/users/konta/documents/repos/tf-2.0/tensorflow/tensorflow/workspace.bzl:42:5                                        - C:/users/konta/documents/repos/tf-2.0/tensorflow/tensorflow/workspace.bzl:82:5                                        - C:/users/konta/documents/repos/tf-2.0/tensorflow/WORKSPACE:38:1                                                      INFO: Repository 'icu' used the following cache hits instead of downloading the corresponding file.                      * Hash 'e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/unicode-org/icu/archive/release-62-1.tar.gz                                                      If the definition of 'icu' was updated, verify that the hashes were also updated.                                       ERROR: C:/users/konta/documents/repos/tf-2.0/tensorflow/tensorflow/tools/pip_package/BUILD:157:1: no such package '@png_archive//': Traceback (most recent call last):                                                                                  File ""C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl"", line 104                                          _apply_patch(ctx, ctx.attr.patch_file)                                                                          File ""C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl"", line 71, in _apply_patch                          _execute_and_check_ret_code(ctx, cmd)                                                                           File ""C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl"", line 52, in _execute_and_check_ret_code                                                                                                                                   fail(""Non-zero return code({1}) when ...))                                                              Non-zero return code(256) when executing 'C:\msys64\usr\bin -l -c ""patch"" ""-p1"" ""-d"" ""C:/users/konta/_bazel_konta/cs3mthex/external/png_archive"" ""-i"" ""C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/png_fix_rpi.patch""':        Stdout:                                                                                                                 Stderr: java.io.IOException: ERROR: src/main/native/windows/process.cc(179): CreateProcessW(""C:\msys64\usr\bin"" -l -c ""\""patch\"" \""-p1\"" \""-d\"" \""C:/users/konta/_bazel_konta/cs3mthex/external/png_archive\"" \""-i\"" \""C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/png_fix_rpi(...)): Odmowa dost?pu.                                                   and referenced by '//tensorflow/tools/pip_package:licenses'                                                            ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@png_archive//': Traceback (most recent call last):                                                                                File ""C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl"", line 104                                          _apply_patch(ctx, ctx.attr.patch_file)                                                                          File ""C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl"", line 71, in _apply_patch                          _execute_and_check_ret_code(ctx, cmd)                                                                           File ""C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl"", line 52, in _execute_and_check_ret_code                                                                                                                                   fail(""Non-zero return code({1}) when ...))                                                              Non-zero return code(256) when executing 'C:\msys64\usr\bin -l -c ""patch"" ""-p1"" ""-d"" ""C:/users/konta/_bazel_konta/cs3mthex/external/png_archive"" ""-i"" ""C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/png_fix_rpi.patch""':        Stdout:                                                                                                                 Stderr: java.io.IOException: ERROR: src/main/native/windows/process.cc(179): CreateProcessW(""C:\msys64\usr\bin"" -l -c ""\""patch\"" \""-p1\"" \""-d\"" \""C:/users/konta/_bazel_konta/cs3mthex/external/png_archive\"" \""-i\"" \""C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/png_fix_rpi(...)): Odmowa dost?pu.                                                  INFO: Elapsed time: 92.788s                                                                                             INFO: 0 processes.                                                                                                      FAILED: Build did NOT complete successfully (174 packages loaded, 4346 targets configured)```
"
36128,Tensorflow.summary.histogram produces wrong output,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
**Tested on my machine with Linux Ubuntu 16.04 and in Google Colab**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): 
**Installed with pip on my machine or using the one preinstalled in Colab**
- TensorFlow version (use command below): 
**v2.0.0-rc2-26-g64c3d38 2.0.0 on my machine or v2.1.0-rc1-0-g064e1535a7 2.1.0-rc1 in Colab**
- Python version: 
**3.5.2 on my machine or 3.6.9 in Colab**
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 
**run on CPU on my machine or run on CPU in Colab or run on GPU in 10.0, V10.0.130 in Colab (there seems to be no problem when running on GPU)**
- GPU model and memory: 
**no GPU on my machine or Colab GPU Tesla P100-PCIE-16GB**

**Describe the current behavior**
;tldr
`tensorboard.plugins.histogram.summary_v2._buckets` produces wrong result when run with CPU on arrays with more than approx. `7e7` elements.

Longer description:
I wanted to add a histogram of my model outputs (not only weights) to tensorboard using `tf.summary.histogram()` in a custom keras callback. All works, histogram is shown, but the values displayed are not correct. After I have inspected further, I found out that the function `tensorboard.plugins.histogram.summary_v2._buckets`, which is internally used in the `tf.summary.histogram()` produces wrong results for larger inputs. Namely it returns incorrect counts for some of the buckets. It is very simple to check this, because the size of the data should equal the sum of counts, which are stored in the first column of the `_buckets` output. For larger arrays, this is not true.

Wierdly, the discrepancy seems to occur only when the code is run on CPU. On the GPU, the histogram is correct (up to some tiny difference - missing one or two elements). But I can not check larger arrays than `100e6`, because running in Colab on GPU gives an error when calling `_buckets()`: 
`InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:GPU:0 to /job:localhost/replica:0/task:0/device:CPU:0 in order to run LinSpace: GPU sync failed [Op:LinSpace]`

Description of each step is in Gist along with the code.

**Describe the expected behavior**
Expected behavior of `tensorboard.plugins.histogram.summary_v2._buckets` would be to produce the correct counts of elements in a specific bucket for all sizes of data.

**Code to reproduce the issue**

Simplified code:
```python
import numpy as np
from tensorboard.plugins.histogram.summary_v2 import _buckets
# Create random data
data = np.random.standard_normal(int(100e6)).astype(np.float16)
# Compute the histogram with tf
tfhist = _buckets(data, bucket_count=20)
counts_tf = tfhist[:,2] # Counts are in the last column
edges_tf = tfhist[:,0] # Edges are in the first column
np.equal(np.sum(counts_tf), data.size)  # If false, there is a bug
```

Full example in Gist (can be run in Colab): https://gist.github.com/paloha/d079aa9afb832b996657fb97a2763a47#file-tensorflow-produces-wrong-histogram-ipynb

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I think that the error I get when running the gist on GPU in Colab (mentioned above) should be addressed as well."
36127,Make TensorFlow build with system packages,"We want to control what versions of libraries are getting used on our cluster. TF using included and downloaded stuff is a misbehavor.

Is it possible to get a list of dependencies and whether the system version is used/found?

Can we interactively pass some arguments and have it use the system versions with TF showing if it does before running the build?

Examples that I've seen are zlib and protobuf with the protoc.

We are using a Github release and running `./configure && bazel <...>` with lots of TF_NEED_* and *_PATH variables already set."
36126,Tensorflow: Model.Fit Error - [[{{node IteratorGetNext}}]],"Hey Tensorflow-Team,

there is an error, that accurs when you try to follow the tutorial how to Load CSV data. [(Link)](https://www.tensorflow.org/tutorials/load_data/csv) When the model runs through all the epochs, each time there is this error-code: `BaseCollectiveExecutor::StartAbort Out of range: End of sequence [[{{node IteratorGetNext}}]]`. There are already some entries here on GitHub, but none of them gives a solution or a workaround. The people say that this is an issue with TF V2.0.0 and 2.1.0 . Is there a solution for this problem? I will leave my complete code here, it´s just 161 lines. 
Maybe I did a mistake. I also attached a screenshot from the error.

Thank you very much in advance! 

Kind regards
Christian Richter

Code:
```python
from __future__ import absolute_import, division, print_function, unicode_literals
import functools

import tensorflow as tf

import xlrd

import pandas as pd
import csv
import numpy as np
import csv

tf.compat.v1.enable_eager_execution()

train_data_url = ""https://www.dropbox.com/s/mug8rjlniftu065/train_data_csv.csv?dl=0""

test_data_url = ""https://www.dropbox.com/s/std8rt6lezl79ti/test_data_csv.csv?dl=0""

train_file_path = tf.keras.utils.get_file(""train_data_csv.csv"", train_data_url)
test_file_path = tf.keras.utils.get_file(""test_data_csv.csv"", test_data_url)

np.set_printoptions(precision = 3, suppress=True)

#!head {train_file_path}

Label_Column = 'Besucher'
Labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100, 200, 300, 400, 500, 600, 700, 800, 900]

train_dataset = tf.data.experimental.make_csv_dataset(
    './Data/train_data_csv.csv',
    batch_size = 52609,
    select_columns = ['Datum','Uhrzeit','Wochentag','Wochenende','Ferien','Feiertag','Brueckentag','Schneechaos','Streik','Besucher'],
    label_name = 'Besucher',
    num_epochs = 1,
    shuffle = False)



test_dataset = tf.data.experimental.make_csv_dataset(
    './Data/alt_test_data_csv.csv',
    batch_size = 1,
    select_columns = ['Datum','Uhrzeit','Wochentag','Wochenende','Feiertag','Besucher'],
    label_name = 'Besucher',
    num_epochs = 1 ,
    shuffle = False)


def show_batch(dataset):
  for batch, label in dataset.take(1):
    for key, value in batch.items():
      print(""{:20s}: {}"".format(key,value.numpy()))




show_batch(train_dataset)

def pack(features, label):
    return tf.stack(list(features.values()), axis = -1), label

packed_dataset1 = train_dataset.map(pack)
#packed_dataset2 = test_dataset.map(pack)

for features, labels in packed_dataset1.take(1):
    print(features.numpy())
    print()
    print(labels.numpy())

example_batch, labels_batch = next(iter(train_dataset))


class PackNumericFeatures(object):
    def __init__(self, names):
        self.names = names

    def __call__(self, features, labels):

        numeric_features = [features.pop(name) for name in self.names]
        numeric_features = [tf. cast(feat, tf.float32) for feat in numeric_features]
        numeric_features = tf.stack(numeric_features, axis = -1)
        features['numeric'] = numeric_features

        return features, labels

NUMERIC_FEATURES = ['Datum','Uhrzeit','Wochentag','Wochenende','Ferien','Feiertag','Brueckentag','Schneechaos','Streik']

packed_train_data = train_dataset.map(
    PackNumericFeatures(NUMERIC_FEATURES)
)
packed_test_data = train_dataset.map(
    PackNumericFeatures(NUMERIC_FEATURES)
)

show_batch(packed_train_data)

example_batch, labels_batch = next(iter(packed_train_data))


desc = pd.read_csv(""./Data/train_data_csv.csv"")[NUMERIC_FEATURES].describe()
desc

MEAN = np.array(desc.T['mean'])
STD = np.array(desc.T['std'])

def normalize_numeric_data(data, mean, std):

    return(data-mean)/std

normalizer = functools.partial(normalize_numeric_data, mean = MEAN, std = STD)

numeric_column = tf.feature_column.numeric_column('numeric', normalizer_fn=normalizer, shape=[len(NUMERIC_FEATURES)])
numeric_columns = [numeric_column]
numeric_column

example_batch['numeric']

numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns)
numeric_layer(example_batch).numpy()

preprocessing_layer = numeric_layer

print(preprocessing_layer(example_batch).numpy()[0])

model = tf.keras.Sequential([
    preprocessing_layer,
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid'),
])

model.compile(
    loss='binary_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

train_data = packed_train_data.shuffle(500)
test_data = packed_test_data

model.fit(train_data, epochs = 20)

test_loss, test_accuracy = model.evaluate(test_data)

print('\n\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy))
```

<img width=""1138"" alt=""Bildschirmfoto 2020-01-22 um 11 49 44"" src=""https://user-images.githubusercontent.com/60178097/72888143-4df51d00-3d0d-11ea-9c35-4246f5b36093.png"">

"
36123,ERROR: intel-tensorflow has an invalid wheel,"Are you maintaining intel-tensorflow?

With pip 20.0.1:
```
$ pip install intel-tensorflow
Collecting intel-tensorflow
  Downloading intel_tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (133.7 MB)
     |████████████████████████████████| 133.7 MB 53.8 MB/s 
ERROR: intel-tensorflow has an invalid wheel, multiple .dist-info directories found: intel_tensorflow-2.0.0.dist-info, tensorflow-2.0.0.dist-info
```
I guess the error is related to new pip."
36122,Code will not compile Image classification,"https://www.tensorflow.org/tutorials/images/classification

## Description of issue (what needs changing):

### Clear description

There's a bug in code

`history = model.fit_generator(
    train_data_gen,
    steps_per_epoch=total_train // batch_size,
    epochs=epochs,
    validation_data=val_data_gen,
    validation_steps=total_val // batch_size
)`

where
`    steps_per_epoch=total_train // batch_size,
`
will not compile because the , is behind the comment. The correct code should be
`history = model.fit_generator(
    train_data_gen,
    steps_per_epoch=total_train, // batch_size
    epochs=epochs,
    validation_data=val_data_gen,
    validation_steps=total_val // batch_size
)`"
36121,Installation broken - Tensorflow 2.1 Cuda 10.1 Ubuntu 18.04,"Having followed the installation guide for GPU support multiple times, each time starting from a blank Ubuntu 18.04 LTS instance it breaks when installing cuda 10-1 after installing the driver and rebooting.

See guide for Ubuntu 18.04 + Cuda 10.1: https://www.tensorflow.org/install/gpu
Cuda 10.1 is the version with which Tensorflow 2.1 is compiled and therefore Cuda 10.1 needs to be installed.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- TensorFlow installed from (source or binary): Tensorflow 2.1 binary
- TensorFlow version: 2.1
- Python version: 3.6
- Installed using virtualenv? pip? conda?: virtualenv
- CUDA/cuDNN version: 10.1
- GPU model and memory: Tesla T4

Having restarted the machine and confirmed that the driver recognises the GPU:
The installation guide for GPU support breaks at this section:
```
sudo apt-get install --no-install-recommends \
    cuda-10-1 \
    libcudnn7=7.6.4.38-1+cuda10.1  \
    libcudnn7-dev=7.6.4.38-1+cuda10.1
```
Which results in:
```
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 cuda-10-1 : Depends: cuda-runtime-10-1 (>= 10.1.243) but it is not going to be installed
             Depends: cuda-demo-suite-10-1 (>= 10.1.243) but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
```
Having searched stackoverflow, github and the Tensorflow website, it seems that the dependencies list can not be installed. 

Again, I have rerun this installation multiple times on a *blank* machine, without running *anything else* before trying to run this installation.
"
36120,TFLite for Microcontrollers: Compilation issues for operators for bare metal,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Host: macOS 10.14.6, Target Tiva TM4C123
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14 
    - The pre-generated projects in the [porting guide of the TFLite micro readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/README.md) are based on 1.14, I tried to upgrade to a more recent version but somehow the micro interpreter fails to read the model correctly.
    - When verifying the model and its inputs, `input->dims->size` contains a very large number which is incorrect.
- Python version: 3.6.9 (Anaconda)
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 1.2.1
- GCC/Compiler version (if compiling from source): TI ARM C/C++ Compiler                   v18.12.4.LTS
- CUDA/cuDNN version: none
- GPU model and memory: none

**Describe the problem**
I am trying to port tflite for microcontrollers to a new target. As described in [this guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/README.md), I used an example project (the fully connected test project), which I managed to compile quickly without any issues. I am aware that I am not using a compiler for which there are any example projects but that has not been an issue so far. I have managed to initialize and run the model, up to the point where some TF operators are missing, such as `SUB`, or `CAST`. 

Now that I've added `lite/kernels/sub.cc` (there is no `sub.cc` in the folder `experimental/micro`) I need to include `lite/kernels/optimized/cpu_check.h` which depends on `lite/kernels/cpu_backend_context.h`, which in turn depends on `gemmlowp`. I haven't managed to compile `gemmlowp` because it depends on many OS dependent sources. However I read in the readme for TFLite for microcontrollers that there should be no such dependencies.

It seems to me that these operators have not yet been ported, and I am aware that this is in a highly experimental state and subject to radical changes. Can someone give me any hints on what I could do? Is there a way that could help me include these operators in the project by maybe stripping things from the ""normal"" tflite implementation? I don't think I need anything from `unistd.h` which `gemmlowp` depends on.

**Any other info / logs**
My model is generated by [ml-agents](https://github.com/Unity-Technologies/ml-agents) which uses `tensorflow>=1.7,<2.1`, and stores the models as frozen graphs. This may cause limitations, if not issues.

I am more than happy to provide anything that might provide any more necessary information
"
36119,null,null
36116,Multiple outputs from a keras model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.15.0-rc1-11276-gc9f7f636eb 2.1.0
- Python version: 3.6.9
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): gcc-7
- CUDA/cuDNN version: 10.1/7.6.3
- GPU model and memory: GTX1080Ti 11GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I have a custom (keras) CNN model as well as a custom loss function.

The model has two inputs at one resolution and multiple (6) outputs at different resolutions (each output has a different resolution). 

The dataset, from a TFRecord file, has the 2 image inputs and 1 ground truth image as an output.

The loss function expects to receive the single ground truth image as `y_true` and the 6 outputs in a list as `y_pred` and will then calculate the loss value based on this.

With this scenario, I get the following error
```
ValueError: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 6 array(s), for inputs ['output_1', 'output_2', 'output_3', 'output_4', 'output_5', 'output_6'] but instead got the following list of 1 arrays: [<tf.Tensor 'args_5:0' shape=(None, 384, 768, 1) dtype=float32>]...
```

If I modify my dataset loading code so that it resizes the ground truth image into a list of images with appropriate resolutions to match my networks output, I get the following error
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes of all inputs must match: values[0].shape = [270,480,1] != values[1].shape = [135,240,1]
	 [[{{node packed}}]]
```

**Describe the expected behavior**
I expect that TF/keras would allow at least one of these scenarios.


Is there an accepted way to handle this sort of situation?"
36115,Sampled Softmax for Tensorflow Keras,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.1.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

This is a feature request for sampled softmax loss in Tensorflow-2 Keras. The sampled softmax loss is important when dealing with large number of target classes mainly in sequence to sequence models.

Sampled softmax is available in tensorflow via `tf.nn.sampled_softmax` but I couldn't find any resource that can help me to use it with tensorflow keras.

_

> **Note: If anyone have **implemented it**, **please** share the sample code on how you've achieved it. There are 10's of questions in StackOverflow regarding this without any answer. **

_

**Will this change the current api? How?**
No.
**Who will benefit with this feature?**
This will benefit those who are dealing with large number of target classes to predict. In such cases softmax is not a ideal solution."
36114,Memory leak when use model.fit method with datagenerator,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
it's increasing ram when I use mode.fit() method
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): installed via pip
- TensorFlow version (use command below): tensorflow-gpu 2.1.0
- Python version: 3.7
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce GTX 1060 6GB

**Describe the current behavior**
RAM memory increases after I fit model. I cannot clear it with any garbage collector etc.
**Describe the expected behavior**
RAM shoud not increases at each epoch of training any model in that way.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
I'm using my own image generator with multioutput:

**
class MultiOutputDataGenerator(tf.keras.preprocessing.image.ImageDataGenerator):

    def flow(
        self,
        x,
        y=None,
        batch_size=32,
        shuffle=True,
        sample_weight=None,
        seed=None,
        save_to_dir=None,
        save_prefix='',
        save_format='png',
        subset=None
    ):

        targets = None
        target_lengths = {}
        ordered_outputs = []
        
        for output, target in y.items():
            
            if targets is None:
                
                targets = target
                
            else:
                
                targets = np.concatenate((targets, target), axis=1)
                
            target_lengths[output] = target.shape[1]
            ordered_outputs.append(output)
        
            gc.collect()

        for flowx, flowy in super().flow(x, targets, batch_size=batch_size, shuffle=shuffle):
            
            target_dict = {}
            i = 0
            
            for output in ordered_outputs:
                
                target_length = target_lengths[output]
                target_dict[output] = flowy[:, i: i + target_length]
                i += target_length
                
                gc.collect()

            yield flowx, target_dict
**

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
36111,"cudart64_101.dll not found, but I have CUDA installed","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (10.0.18362 Build 18362)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.1.0
- Python version: 3.7.6 (from windows store)
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10 and 10.1, but I also tried with just 10.1 and just 10.2
- GPU model and memory: NVIDIA GeForce GTX 1060 OC with 6GB GDDR5



**Describe the problem**
I'm getting the error Could not load dynamic library ""'cudart64_101.dll'; dlerror: cudart64_101.dll
 not found"", even though cuda is installed and the file exists on my computer
![Annotation 2020-01-21 162719](https://user-images.githubusercontent.com/31412003/72844651-f7cf9d80-3c6a-11ea-8133-ffec9d410cca.png)
**Provide the exact sequence of commands / steps that you executed before running into the problem**
import tensorflow as tf
import pathlib
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
np.set_printoptions(precision=4)

I get these from [https://www.tensorflow.org/guide/data](https://www.tensorflow.org/guide/data)

**Any other info / logs**
2020-01-21 16:18:17.197417: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-01-21 16:18:17.197832: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine."
36109,TFTRT not converting dilated convolutions,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.15
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10, 7
- TensorRT: 5
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
1.15.0

**Describe the current behavior**
TFTRT skips dilated convolutions and gives leaves this in the output. Other convolutions work fine.

This is what it looks like in Tensorboard: 
![dilated_convolution](https://user-images.githubusercontent.com/16327442/72840584-da172e00-3c94-11ea-9068-4c6c93b80c18.png)

It appears that there are some BatchToSpaceND and SpaceToBatchND operations. Perhaps these are not supported?

**Describe the expected behavior**

Based on the PR that was merged into Tensorflow in January last year (TFTRT: Support Dilated Convolutions #24674) I would expect that converting a convolution with dilations would be supported. Instead it's not... I tried dilation rates of 2, 8, 16...

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
from tensorflow.python.compiler.tensorrt import trt_convert as trt

import tensorflow as tf

inp = tf.keras.layers.Input(input=(400, 400, 3))
x = tf.keras.layers.Conv2D(8, kernel_size=(3,3), strides=(1,1), dilation_rate=(16,16))(inp)
model = tf.keras.models.Model(inp, x) 

sess = tf.keras.backend.get_session()

output_nodes = [n.name[:-2] for n in model.outputs]

graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), output_nodes)

converter = trt.TrtGraphConverter(input_graph_def=graph_def,
                                              nodes_blacklist=model.outputs,
                                              precision_mode=""FP32"",
                                              max_batch_size=1,
                                              minimum_segment_size=1,
                                              max_workspace_size_bytes=(2048>>20),
                                              use_calibration=True)
calib_graph = converter.convert()

with tf.gfile.GFile(""model.trt.pbtxt"", ""w"") as f:
        f.write(str(calib_graph))
```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
36107,Tensorflow install breaks curl on Ubuntu,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 1.15
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: Virtualenv/pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: NVidia 640

**Describe the problem**

Rasa install of Tensorflow 1.15 causes a Ubuntu dependency error which prevents ""curl"" from installing, which silently breaks the iDrive backup system.

The underlying problem seems to be that Tensorflow tries to build a modified ""curl"" with support for some Google remote file system. See this Tensorflow mod to Curl:

https://github.com/tensorflow/tensorflow/commit/bb890ea5fa4c8a2100a295e93c5bf37b3c88b55a

That would be fine if the modified ""curl"" was private to the Tensorflow environment, but somehow the nonstandard version of ""curl"" or ""libcurl"" got installed in the Ubuntu system. This broke the Ubuntu package dependency system, and now a standard ""curl"" cannot be installed. This, in turn, broke things which depend on ""curl"" being present, such as iDrive backups. 

Details here: https://ubuntuforums.org/showthread.php?t=2435412

Also see: https://github.com/RasaHQ/rasa/issues/5104"
36105,how to install openpose library for python in windows 10 ?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
36102,CONV_2d convert to DEPTHWISE_CONV when input depth=1,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A

**Describe the current behavior**
CONV_2d convert to DEPTHWISE_CONV when input depth=1

**Describe the expected behavior**
CONV_2d should be CONV_2d

**Code to reproduce the issue**
if input depth=1 for CON_2D layer, TFLite converter will convert it to DEPTHWISE_CONV.
Although it's computationally identical, it can cause confusion and potentially bug...

To replicate the problem:
```python
import tensorflow as tf
import numpy as np


def gen_calibration_dataset():
    for _ in range(10):
        yield [np.random.rand(1,28,28,1).astype(np.float32)]

def get_keras_model_conv():

    input_0 = tf.keras.layers.Input(shape=[28, 28, 1])

    conv_0 = tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3),
                                    activation=tf.nn.relu)(input_0)
    conv_1 = tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3),
                                    activation=tf.nn.relu)(conv_0)

    model = tf.keras.models.Model(inputs=[input_0], outputs=[conv_1])

    model.summary()

    return model

def gen_model():

    keras_model = get_keras_model_conv()
    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
    tflite_quant_model = converter.convert()
    open('conv.tflite', 'wb').write(tflite_quant_model)

gen_model()
```
keras model summary gives:
```
Model: ""model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 28, 28, 1)]       0         
_________________________________________________________________
conv2d (Conv2D)              (None, 26, 26, 12)        120       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 24, 24, 12)        1308      
=================================================================

```

But,
inspect the model with [netron](https://github.com/lutzroeder/netron) or `bazel run //tensorflow/lite/tools:visualize conv.tflite conv.html` shows as below:

![image](https://user-images.githubusercontent.com/55463253/72816765-9276af00-3c60-11ea-8daf-fc42ced34568.png)

![image](https://user-images.githubusercontent.com/55463253/72816851-b2a66e00-3c60-11ea-9b7d-14ac076b5de4.png)


**Other info / logs**
N/A
"
36099,Update the documentation link to the TFLITE conversion commands ,"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/toco

Tflite converter is missing the usage documentation exmaple links :
[link 1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/convert/cmdline_reference.md)
[link 2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/convert/cmdline_examples.md)

the flags are needed to run such command : 

```
bazel run --config=opt tensorflow/lite/toco:toco -- \
--input_file=$OUTPUT_DIR/tflite_graph.pb \
--output_file=$OUTPUT_DIR/detect.tflite \
--input_shapes=1,300,300,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \
--inference_type=FLOAT \
--allow_custom_ops
```

Please update the links or some one points me at the file that contains these flags to investigate more other possibilities and methods.


"
36097,tridiagonal_matmul: Process finished with exit code 139 (interrupted by signal 11: SIGSEGV),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ArchLinux
- TensorFlow installed from (source or binary): https://www.archlinux.org/packages/community/x86_64/tensorflow-opt/
- TensorFlow version (use command below): 2.1.0
- Python version: 3.81

```
== check python ===================================================
python version: 3.8.1
python branch: 
python build version: ('default', 'Jan  8 2020 23:09:20')
python compiler version: GCC 9.2.0
python implementation: CPython

== check os platform ===============================================

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 9.2.0
Copyright (C) 2019 Free Software Foundation, Inc.

== check pips ===================================================
numpy                      1.18.1             
numpy-quaternion           2019.12.11.22.25.52
protobuf                   3.11.2             
tensorflow                 2.1.0              
tensorflow-estimator       2.1.0              
tensorflow-serving-api     2.0.0              
tensorflow-serving-api-gpu 2.0.0              

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.1.0
tf.version.GIT_VERSION = unknown
tf.version.COMPILER_VERSION = 9.2.0
```

**Describe the current behavior**
Segfault (see log) or sigserv
[dump.txt](https://github.com/tensorflow/tensorflow/files/4091136/dump.txt)
 
```
2020-01-21 13:56:11.320166: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1800000000 Hz
2020-01-21 13:56:11.320500: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557f9e6e4c30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-01-21 13:56:11.320521: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version

Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)
```

**Describe the expected behavior**
Propagate layer

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers

np.set_printoptions(precision=4)


class Tridiagonal(layers.Layer):
    def __init__(self):
        super(Tridiagonal, self).__init__()

    def build(self, input_shape):
        self.super = self.add_weight(shape=(input_shape[-1], 1),
                                     initializer='random_normal',
                                     trainable=True)
        self.main = self.add_weight(shape=(input_shape[-1], 1),
                                    initializer='random_normal',
                                    trainable=True)
        self.sub = self.add_weight(shape=(input_shape[-1], 1),
                                   initializer='random_normal',
                                   trainable=True)

    def call(self, inputs):
        return tf.linalg.tridiagonal_matmul(
            (self.super, self.main, self.sub), inputs,
            diagonals_format='sequence')


model = tf.keras.Sequential()
model.add(Tridiagonal())

model.compile(optimizer=tf.keras.optimizers.Adam(0.01),
              loss='mse',  # mean squared error
              metrics=['mae'])  # mean absolute error

data = tf.convert_to_tensor(np.random.random((10, 2)), dtype=tf.float32)
labels = tf.convert_to_tensor(np.random.random((10, 2)), dtype=tf.float32)

td = Tridiagonal()
td(data)
```

**Other info / logs**
See attachment
[dump.txt](https://github.com/tensorflow/tensorflow/files/4091139/dump.txt)

"
36096,Performance problems with tf.nn.embedding_lookup in eager mode,"**System information**
- Have I written custom code: Yes - I condensed it to a self-contained example and attach it to the issue.
- OS Platform and Distribution: Ubuntu 18.04.3 LTS
- TensorFlow installed from: binary (pip)
- TensorFlow version: 2.0.0
- Python version: 3.6.8
- CUDA/cuDNN version: CPU only
- GPU model and memory: CPU only

**Describe the current behavior**
tf.nn.embedding_lookup with eager mode enabled is so slow for me that it is practically unusable. Falling back to Numpy is faster by an order of magnitude.

**Describe the expected behavior**
The embedding lookup should perform well enough to be used. Or the information on how to use it properly should be improved.

**Code to reproduce the issue**
Below is my benchmarking code. It simulates a static word2vec (3M, 300d) lookup with artificial data under different configurations. I use a static document length of 500 tokens which I found to be the average of my dataset (OpenWebText sample).
[tf-embedding-benchmark.zip](https://github.com/tensorflow/tensorflow/files/4090909/tf-embedding-benchmark.zip)

**Other info / logs**
Below is a plot of my benchmarks. I did the benchmarks with little data and therefore there's a bit of variance in the charts. I'm currently running them with some more data, but it takes time because it is so slow. As far I can tell already, the results do not look significantly different though. `embedding_type` refers to the embedding lookup: `numpy` means Numpy indexing using `tf.py_func`. The TensorFlow variants refer to the underlying embedding data structure: `tf.constant` and `tf.Variable` with and without the `trainable` flag set.

![embedding_lookup_bm](https://user-images.githubusercontent.com/779877/72803346-2be89580-3c4e-11ea-9fd7-c0a4a5525eeb.png)
"
36094,SlurmClusterResolver should use env variables ob job step and return correct num_accelerators,"The [SlurmClusterResolver](https://github.com/tensorflow/tensorflow/blob/96d0f42d1b236d21157d32805d4aa87e136083b3/tensorflow/python/distribute/cluster_resolver/slurm_cluster_resolver.py) has a few issues:

- `num_accelerators` returns the GPUs per **node**  although they should be per ** task** according to usage
- It uses scontrol which may not return the correct values (see next point)
- Slurm job steps might use different configurations than its job. E.g. after allocating 1 task with 10 cpus and gpus one might run a job step with 10 tasks and 1 cpu and gpu each. This is currently ignored leading to wrong behavior

Example: From inside an allocation with 1 task and 1 node the following will print a single hostname each, and not 2 times the same hostname each: `srun --ntasks 2 --cpus-per-task=5 scontrol show hostname`

Proposed fix:
- Use environment variables `SLURM_*` instead of `scontrol`
- When exists, use `SLURM_STEP_*` variables instead of its corresponding job variable. It might even be reasonable to NOT use the job variables at all as it doesn't make sense to run TF in the context of a job and not a job step (difference: after `salloc` or `sbatch` you'll run in the context of a job in a single instance, `srun` from inside such an allocation then starts a step and distributes execution. `srun` can also be used to implicitly call `salloc`)

I have developed a version based on the current cluster resolver which implements the above and uses https://pypi.org/project/python-hostlist to parse the tasks per node (it is possible that slurm allocates 3 tasks on the first node and 2 tasks on the second when allocating 5 tasks) and expanding the hostlist (the format used is something like `[n1-n3, n5]`.   

In the current version it makes heavy use of default values so it is possible to simply create an instance of `SlurmClusterResolver` with **no** arguments and it will fill out everything from the environment of the current job step or job. The goal was to relieve users from any fiddling with information available already and just use e.g. `MultiWorkerMirroredStrategy`

Is there interest in reviewing and accepting a PR with my improved version?"
36093,Saving tf.keras.Sequential model fails with RNN containing more than one GRUCell,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution: Linux Ubuntu 18.04
- Mobile device if the issue happens on mobile device: -
- TensorFlow installed from: binary (tf-nightly-2.0-preview)
- TensorFlow version: GIT_VERSION = v1.12.1-7529-g3e0ad8a004, VERSION = 2.0.0-dev20190731
- Python version: 3.6.9
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: CPU only
- GPU model and memory: CPU only

**Describe the current behavior**
Saving a `tf.keras.Sequential` model with `tf.keras.layers.RNN` fails when containing more than one `tf.keras.layers.GRUCell` with error message `RuntimeError: Unable to create link (name already exists)`.

**Describe the expected behavior**
Saving should succeed, not only when having one cell.

**Code to reproduce the issue**
```python
import tensorflow as tf

# saving succeeds for number_of_cells = 1, but fails for number_of_cells > 1
number_of_cells = 2

model = tf.keras.Sequential()

model.add(tf.keras.layers.Input(shape=(1, 1,)))

cells = []

for i in range(number_of_cells):
    cells.append(tf.keras.layers.GRUCell(10))

model.add(tf.keras.layers.RNN(cells))

model.save(""rnn.h5"")
```

**Other info / logs**
Behavior is the same when using `SimpleRNNCell` instead of `GRUCell`.

Traceback in case of failure:
```bash
Traceback (most recent call last):
  File ""test_rnn_gru.py"", line 17, in <module>
    model.save(""rnn.h5"")
  File ""/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1157, in save
    saving.save_model(self, filepath, overwrite, include_optimizer, save_format)
  File ""/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py"", line 105, in save_model
    model, filepath, overwrite, include_optimizer)
  File ""/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 103, in save_model_to_hdf5
    save_weights_to_hdf5_group(model_weights_group, model_layers)
  File ""/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 625, in save_weights_to_hdf5_group
    param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)
  File ""/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/h5py/_hl/group.py"", line 139, in create_dataset
    self[name] = dset
  File ""/home/test/dev/tensorflow/tf2/lib/python3.6/site-packages/h5py/_hl/group.py"", line 373, in __setitem__
    h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)
  File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper
  File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper
  File ""h5py/h5o.pyx"", line 202, in h5py.h5o.link
RuntimeError: Unable to create link (name already exists)
```
"
36092,creating virtual environment and installing tensorflow package,can any one tell me how to  create virtual environment  and also install the tensor flow in python 3.7.2
36090,tf.GradientTape.gradients(),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
36089,Frozen Graph generation warning lead to error in running the model,"**System information**
- OS Platform and Distribution:Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: tensorflow-gpu       1.15.0
- Python version: 3.6.10
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4
- CUDA/cuDNN version:10.2
- GPU model and memory: GEFORCE GTX 960M - 16GB



**Describe the problem**
Aim is to convert .ckpt and .config files to .tflite for ssd_mobilenet_v2_quantized_coco model.
Following error is seen when converting from .ckpt and .config files to .pb
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/Conv/add_fold

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Following is the sequence of commands

`(py36_tf_gpu) ridlr@ridlr107:~/TensorFlow/models-master/research/object_detection$ python export_tflite_ssd_graph.py --pipeline_config_path /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/pipeline.config --trained_checkpoint_prefix /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt --output_directory /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/tflite_graph_export_tflite/`

The above command generated a tflite_graph.pb file

`tflite_convert --output_file tflite_graph.tflite --graph_def_file tflite_graph.pb --input_arrays normalized_input_image_tensor --output_arrays TFLite_Detection_PostProcess --input_shapes 1,300,300,3 --inference_type QUANTIZED_UINT8 --std_dev_values 0 --mean_values 1 --default_ranges_min 0 --default_ranges_max 6 --allow_custom_ops
`
The above command generated a tflite_graph.tflite file

```
(py36_tf_gpu) ridlr@ridlr107:~/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/tflite_graph_export_tflite$ edgetpu_compiler tflite_graph.tflite 
Edge TPU Compiler version 2.0.267685300
ERROR: :106 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.
ERROR: Node number 0 (CONV_2D) failed to prepare.


Model compiled successfully in 11 ms.

Input model: tflite_graph.tflite
Input size: 5.89MiB
Output model: tflite_graph_edgetpu.tflite
Output size: 5.88MiB
On-chip memory available for caching model parameters: 0.00B
On-chip memory used for caching model parameters: 0.00B
Off-chip memory used for streaming uncached model parameters: 0.00B
Number of Edge TPU subgraphs: 0
Total number of operations: 0
Operation log: tflite_graph_edgetpu.log
See the operation log file for individual operation details.
```


The above command generated a tflite_grapf_edgetpu.tflite file inspite of the error. When I run this model on the Coral I get the following error

```
INFO: Initialized TensorFlow Lite runtime.
Traceback (most recent call last):
  File ""detect_image.py"", line 124, in <module>
    main()
  File ""detect_image.py"", line 91, in main
    interpreter.allocate_tensors()
  File ""/home/ankit/anaconda3/envs/py35/lib/python3.5/site-packages/tflite_runtime/interpreter.py"", line 244, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File ""/home/ankit/anaconda3/envs/py35/lib/python3.5/site-packages/tflite_runtime/interpreter_wrapper.py"", line 114, in AllocateTensors
    return _interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: tensorflow/lite/kernels/kernel_util.cc:119 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.Node number 0 (CONV_2D) failed to prepare.

```

I suspect that there is warning/Info displayed when generating the tflite_graph.pb file which is leading to the above error. The build log for the same is below. The line from the beolow log that concerns me is 
`INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/Conv/add_fold
`
How Can I resolve this warning?
```
(py36_tf_gpu) ridlr@ridlr107:~/TensorFlow/models-master/research/object_detection$ python export_tflite_ssd_graph.py --pipeline_config_path /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/pipeline.config --trained_checkpoint_prefix /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt --output_directory /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/tflite_graph_export_tflite/
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/slim/nets/inception_resnet_v2.py:374: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

WARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.

WARNING:tensorflow:From export_tflite_ssd_graph.py:143: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

WARNING:tensorflow:From export_tflite_ssd_graph.py:133: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.

W0121 12:09:58.011585 140284674922304 module_wrapper.py:139] From export_tflite_ssd_graph.py:133: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.

WARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/export_tflite_ssd_graph_lib.py:193: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.

W0121 12:09:58.016252 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/export_tflite_ssd_graph_lib.py:193: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.

WARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/export_tflite_ssd_graph_lib.py:237: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0121 12:09:58.016631 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/export_tflite_ssd_graph_lib.py:237: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0121 12:09:58.020018 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/ridlr/anaconda3/envs/py36_tf_gpu/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
W0121 12:09:58.022555 140284674922304 deprecation.py:323] From /home/ridlr/anaconda3/envs/py36_tf_gpu/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.

W0121 12:09:59.660506 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.

WARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

W0121 12:09:59.668454 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

INFO:tensorflow:depth of additional conv before box predictor: 0
I0121 12:09:59.668638 140284674922304 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0121 12:09:59.696758 140284674922304 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0121 12:09:59.722609 140284674922304 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0121 12:09:59.747994 140284674922304 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0121 12:09:59.774057 140284674922304 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
I0121 12:09:59.801787 140284674922304 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0
WARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/export_tflite_ssd_graph_lib.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

W0121 12:09:59.837386 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/export_tflite_ssd_graph_lib.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-01-21 12:09:59.838240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-01-21 12:09:59.847343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-21 12:09:59.847611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:01:00.0
2020-01-21 12:09:59.847750: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory
2020-01-21 12:09:59.847852: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory
2020-01-21 12:09:59.847962: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory
2020-01-21 12:09:59.848044: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory
2020-01-21 12:09:59.848109: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory
2020-01-21 12:09:59.848188: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory
2020-01-21 12:09:59.851251: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-21 12:09:59.851275: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-01-21 12:09:59.851543: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-21 12:09:59.875285: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz
2020-01-21 12:09:59.876004: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559d0e122aa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-01-21 12:09:59.876041: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-01-21 12:09:59.903778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-21 12:09:59.904104: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559d0e124900 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-01-21 12:09:59.904122: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 960M, Compute Capability 5.0
2020-01-21 12:09:59.904227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 12:09:59.904235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      
WARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/export_tflite_ssd_graph_lib.py:267: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

W0121 12:10:00.032385 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/export_tflite_ssd_graph_lib.py:267: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

WARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/builders/graph_rewriter_builder.py:41: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

W0121 12:10:00.034862 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/builders/graph_rewriter_builder.py:41: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/Conv/add_fold
I0121 12:10:01.108431 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/Conv/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv/depthwise/add_fold
I0121 12:10:01.108743 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/expand/add_fold
I0121 12:10:01.108995 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/expand/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/add_fold
I0121 12:10:01.109181 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/expand/add_fold
I0121 12:10:01.109415 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/expand/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/add_fold
I0121 12:10:01.109586 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/expand/add_fold
I0121 12:10:01.109818 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/expand/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/add_fold
I0121 12:10:01.109977 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/expand/add_fold
I0121 12:10:01.110195 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/expand/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/add_fold
I0121 12:10:01.110363 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/expand/add_fold
I0121 12:10:01.110591 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/expand/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/add_fold
I0121 12:10:01.110761 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/expand/add_fold
I0121 12:10:01.111005 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/expand/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/add_fold
I0121 12:10:01.111187 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/expand/add_fold
I0121 12:10:01.111420 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/expand/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/add_fold
I0121 12:10:01.111594 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/expand/add_fold
I0121 12:10:01.111815 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/expand/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/add_fold
I0121 12:10:01.112004 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/expand/add_fold
I0121 12:10:01.112233 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/expand/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/add_fold
I0121 12:10:01.112395 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/expand/add_fold
I0121 12:10:01.112610 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/expand/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/add_fold
I0121 12:10:01.112753 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/expand/add_fold
I0121 12:10:01.112978 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/expand/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/add_fold
I0121 12:10:01.113132 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/expand/add_fold
I0121 12:10:01.113349 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/expand/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/add_fold
I0121 12:10:01.113510 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/expand/add_fold
I0121 12:10:01.113732 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/expand/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/add_fold
I0121 12:10:01.113906 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/expand/add_fold
I0121 12:10:01.114142 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/expand/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/add_fold
I0121 12:10:01.114297 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/expand/add_fold
I0121 12:10:01.114525 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/expand/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/add_fold
I0121 12:10:01.114687 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/expand/add_fold
I0121 12:10:01.114912 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/expand/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/add_fold
I0121 12:10:01.115073 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/Conv_1/add_fold
I0121 12:10:01.115322 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/Conv_1/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/add_fold
I0121 12:10:01.115458 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/add_fold
I0121 12:10:01.115600 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/add_fold
I0121 12:10:01.115738 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/add_fold
I0121 12:10:01.115870 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/add_fold
I0121 12:10:01.116004 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/add_fold
I0121 12:10:01.116144 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/add_fold
I0121 12:10:01.116278 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/add_fold
INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/add_fold
I0121 12:10:01.116429 140284674922304 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/add_fold
2020-01-21 12:10:01.121862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 12:10:01.121904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      
WARNING:tensorflow:From /home/ridlr/TensorFlow/models-master/research/object_detection/exporter.py:111: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

W0121 12:10:01.122092 140284674922304 module_wrapper.py:139] From /home/ridlr/TensorFlow/models-master/research/object_detection/exporter.py:111: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

INFO:tensorflow:Restoring parameters from /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt
I0121 12:10:01.465342 140284674922304 saver.py:1284] Restoring parameters from /home/ridlr/TensorFlow/base_model/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt
WARNING:tensorflow:From /home/ridlr/anaconda3/envs/py36_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0121 12:10:03.793860 140284674922304 deprecation.py:323] From /home/ridlr/anaconda3/envs/py36_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2020-01-21 12:10:04.520189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 12:10:04.520234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      
INFO:tensorflow:Restoring parameters from /tmp/tmpkmw_fvex
I0121 12:10:04.521256 140284674922304 saver.py:1284] Restoring parameters from /tmp/tmpkmw_fvex
WARNING:tensorflow:From /home/ridlr/anaconda3/envs/py36_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0121 12:10:05.583929 140284674922304 deprecation.py:323] From /home/ridlr/anaconda3/envs/py36_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /home/ridlr/anaconda3/envs/py36_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0121 12:10:05.584106 140284674922304 deprecation.py:323] From /home/ridlr/anaconda3/envs/py36_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 632 variables.
I0121 12:10:06.121339 140284674922304 graph_util_impl.py:334] Froze 632 variables.
INFO:tensorflow:Converted 632 variables to const ops.
I0121 12:10:06.188232 140284674922304 graph_util_impl.py:394] Converted 632 variables to const ops.
2020-01-21 12:10:06.305956: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes

```"
36088,tensorflow nightly still has a problem reported at #35029,"I am using 2.2.0-dev20200119 with CUDA 10.1 on 1080Ti. I still meet the problem #35029  when I run the following code.

```python
#!/usr/bin/python3
import numpy as np;
import tensorflow as tf;
gpus = tf.config.experimental.list_physical_devices('GPU');
tf.config.experimental.set_memory_growth(gpus[0], True);
a=tf.constant(np.random.normal(size=(8,100)), dtype = tf.float32);
b=tf.keras.layers.Dense(units=200)(a);
```"
36087,master branch build failed ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: master branch commit-ID: 
4ab86d026ff419a5d35ee41493e29611f29a555d

- Python version: 3.6
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 1.2.1
- GCC/Compiler version (if compiling from source): 6.3
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Build steps:
```
yes  """" | python configure.py 
bazel build --config=mkl  -c opt  //tensorflow/tools/pip_package:build_pip_package
```
Crash with the error:
```
ERROR: /home/lesliefang/tensorflow_master/tensorflow/tensorflow/core/common_runtime/eager/BUILD:352:1: C++ compilation of rule '//tensorflow/core/common_runtime/eager:mkl_eager_op_rewrite' failed (Exit 1)
tensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc: In static member function 'static tensorflow::Status tensorflow::MklEagerOpRewrite::SetupNewOp(tensorflow::EagerOperation*, std::string, std::unique_ptr<tensorflow::EagerOperation>*)':
tensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc:119:45: error: cannot convert 'tensorflow::EagerContext' to 'tensorflow::EagerContext*' in initialization
   EagerContext* ctx = orig_op->EagerContext();
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
36084,"""TypeError: Not JSON Serializable: Tensor"" when save model!","Hi,
I just meet a problem when add a Variable to model as a changable weight for loss. But I have error when I save model. I don't find any way work for this problem.

This is my code. if I don't use changable weights, it works good.
`model.alpha = tf.Variable(args.alpha, trainable=False, name='alpha', dtype=tf.float32)
model.compile(optimizer=opt, loss=['mae', TVdist], loss_weights=[1.0 / (1 + model.alpha), model.alpha / (1 + model.alpha)])`

This is my error info:
Traceback (most recent call last):
  File ""train_sgdnet_change.py"", line 321, in <module>
    save_best_only=False), reduce_lr, AlphaChange])
  File ""/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
  File ""/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/engine/training.py"", line 1418, in fit_generator
    initial_epoch=initial_epoch)
  File ""/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/engine/training_generator.py"", line 251, in fit_generator
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/callbacks.py"", line 79, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/callbacks.py"", line 457, in on_epoch_end
    self.model.save(filepath, overwrite=True)
  File ""/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/engine/network.py"", line 1090, in save
    save_model(self, filepath, overwrite, include_optimizer)
  File ""/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/engine/saving.py"", line 382, in save_model
    _serialize_model(model, f, include_optimizer)
  File ""/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/engine/saving.py"", line 138, in _serialize_model
    }, default=get_json_type).encode('utf8')
  File ""/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/json/__init__.py"", line 251, in dumps
    sort_keys=sort_keys, **kw).encode(obj)
  File ""/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/home/bpfsrw1/wangmingkai/python_env/lib/python2.7/site-packages/keras/engine/saving.py"", line 74, in get_json_type
    raise TypeError('Not JSON Serializable: %s' % (obj,))
TypeError: Not JSON Serializable: Tensor(""truediv:0"", shape=(), dtype=float32)

This is my environment:
keras 2.2.4
tensorflow 1.10.1

Looking forward for a solution!
Aaron"
36083,TensorBoard showing nothing after upgrade ,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 1809
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410
- Python version: 3.7.3
- CUDA/cuDNN version: cuda_10.1.243_426.00_win10 / cudnn-10.1-windows10-x64-v7.6.5.32
- GPU model and memory: gtx 1070 / 6g

**Describe the current behavior**
I log some data by tf.summary.scalar, every things is ok UNTILL i upgrade tf from 2.0.0 to 2.1.0
through pip install -upgrade tensorflow.
Then I use TensorBoard but both chrome and edge show nothing.
I can get correct result by TensorBoard in tf 2.0.0 which is installed on other mechine.
Then i try to reinstall tensorboard by pip uninstall tensorboard then pip install tensorboard,
but still not working.

**Describe the expected behavior**
Web page show nothing on both chrome and edge.

**Code to reproduce the issue**
I run tensorboard by CMD: tensorboard --logdir=log

**Other info / logs**
CMD show
2020-01-21 12:26:21.211807: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all
TensorBoard 2.1.0 at http://localhost:6006/ (Press CTRL+C to quit)"
36081,Which distributed strategy should I use when running training without GPU?,"There are many distributed strategy in Tensorflow,

- MirroredStrategy
- TPUStrategy
- MultiWorkerMirroredStrategy
- CentralStorageStrategy
- ParameterServerStrategy
- OneDeviceStrategy

Some of them run on one machine and broadcast model to different GPUs, and some of them use different GPUs on different machines.

My question if I don't have GPU in my server, which strategy should I use, so I can get the benefit when running distributed training?

I try to run MirroredStrategy, ParameterServerStrategy on four machines, but it seems that it's slower than running on a single machine."
36078,[Speech Commands] export function playRawAudio() no bit depth is set.,"**System information**
-  stock example script provided in TensorFlow `tfjs/SpeechCommands`
- Windows 10
- Mobile device N/A
- TensorFlow installed from (source):
- TensorFlow version (use command below): Latest
- Python version: N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: sli gtx



**Describe the current behavior**
Current playback of captured speech command produces audible degradation. Setting breakpoints on `export function playRawAudio()` indicates a sample rate of `44100` which is good but I am not seeing the bit depth set anywhere. The playback seems to be using a very low bit depth 8bit? 

**Describe the expected behavior**
The expected behavior should play back recorded audio sample with clarity, changing the bit depth to 16, 24 or 32 bit float would be ideal.

**Code to reproduce the issue**
**browser_fft_utils.ts**

```
export function playRawAudio(
    rawAudio: RawAudioData, onEnded: () => void|Promise<void>): void {
  const audioContextConstructor =
      // tslint:disable-next-line:no-any
      (window as any).AudioContext || (window as any).webkitAudioContext;
  const audioContext: AudioContext = new audioContextConstructor();
  const arrayBuffer =
      audioContext.createBuffer(1, rawAudio.data.length, rawAudio.sampleRateHz);
  const nowBuffering = arrayBuffer.getChannelData(0);
  nowBuffering.set(rawAudio.data);
  const source = audioContext.createBufferSource();
  source.buffer = arrayBuffer;
  source.connect(audioContext.destination);
  source.start();
  source.onended = () => {
    if (onEnded != null) {
      onEnded();
    }
  };

```

**Other info / logs**
https://en.wikipedia.org/wiki/Audio_bit_depth
"
36077,Migration issue make_one_shot_iterator,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
**ValueError: For performance reasons Keras `fit`, `evaluate` and`predict` accept tf.data `Datasets` as input but not iterators that have been manually generated from Datasets by users. Please directly pass in the original `Dataset` object instead of passing in `iter(dataset)`.**

**Describe the expected behavior**

**Code to reproduce the issue**

Trying to migrate the code from tensorflow 1.10 to tensorflow 2.0. using  
**import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()**

if train_obj.if_validate:
            tr_history = model.fit(tf.data.make_one_shot_iterator(tr_dataset), epochs=self.epochs, callbacks=callbacks, steps_per_epoch=self.steps_per_epoch, validation_data=tf.data.make_one_shot_iterator(val_dataset), validation_steps=self.validation_steps)
        else:
            tr_history = model.fit(tr_dataset.make_one_shot_iterator(), epochs=self.epochs, callbacks=callbacks, steps_per_epoch=self.steps_per_epoch)
        
Provide a reproducible test case that is the bare minimum necessary to generate the problem.


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


Since I'm importing tensorflow.compat.v1 as tf and disabling v2 behaviour. Why am I still not able to use make_one_shot_iterator?

"
36076,Migration issue from tensorflow 1.10 to tensorflow 2.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
36075,Question about GLES delegate 3.1 spec support,"Hello,
Not sure if this is right issue category, but I am interested in learning details about the GLES delegate.
In particular, I would like to know what part of the 3.1 spec is needed for the delegate, as I am looking into creating a new delegate for a system with incomplete support for 3.1. It does support SSBO and compute shaders.
Thanks!
"
36074,sklearn requires KerasRegressor/KerasClassifier to have _estimator_type set,"Various sklearn functions validate _estimator_type. This in the respective constructors fixes that:

KerasRegressor:
`self._estimator_type = 'regressor'`

KerasClassifier:
`self._estimator_type = 'classifier'`
"
36073,Subclassed tf.keras.Model always in training mode,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
- Python version: 3.6.8
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 10.2
- GPU model and memory: P100 16GB

**Describe the current behavior**

As per the [docs](https://github.com/tensorflow/docs/blob/r1.14/site/en/api_docs/python/tf/keras/models/Model.md)

> If you subclass `Model`, you can optionally have a `training` argument (boolean) in `call`, which you can use to specify a different behavior in training and inference

So I expect the dropout in MyModel to affect the result when training.


But this script outputs 
```
Predicting :
[[1.]]
Training :
1.0
```

**Describe the expected behavior**

It should output
```
Predicting :
[[1.]]
Training :
0.5
```

**Code to reproduce the issue**

```python
import tensorflow as tf
import numpy as np

class MyModel(tf.keras.Model):

  def __init__(self):

    super(MyModel, self).__init__()

    self.dense = tf.keras.layers.Dense(100, kernel_initializer=""ones"", trainable=False)
    self.dropout = tf.keras.layers.Dropout(0.5)


  def call(self, inputs, training=False):

    x = self.dense(inputs)

    if training:
      x = self.dropout(x, training=training)

    x = tf.reshape(tf.reduce_sum(x)/100., [1, 1]) # If we dont reshape, we get RuntimeError: Attempted to aggregate unsupported object 1.0.
    
    return x

model = MyModel()

def loss(y_true, y_pred):
    return y_pred

model.compile(optimizer=""sgd"", loss=loss)

x = np.ones((1, 1), dtype=np.float32)

print(""Predicting :"")
print(model.predict(x)) # No dropout, output is 1 as expected

print(""Training :"")
print(model.train_on_batch(x)) # dropout should put half of the activations of model.dense to 0, so I expect this value to be 0.5
```"
36072,hangs on model.fit,"Although I can't extract the code to reproduce the problem, I think that documenting it here will help improve this project and anyone who encounters the same problem.

System: Windows 10
Version: tf-nightly-gpu 2020.1.19

I use `tf.data.Dataset` to provide samples
When I use `GPU + eager + batch_size > 16`, it will hang on `model.fit` and continue to occupy a core CPU
When I use cpu or turn off eager mode or set batch <= 16, he will run normally.

It will continue like this

> 2020-01-21 01:20:24.249995: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-01-21 01:20:28.403903: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-01-21 01:20:29.708097: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
   9/1406 [..............................] - ETA: 42:30 - loss: 30.3274 - accuracy: 0.0000e+00"
36071,Can't set None on TextVectorization layer's split parameter.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `no`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 18.04`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `no`
- TensorFlow installed from (source or binary): `no`
- TensorFlow version (use command below): `2.0.0`
- Python version: `3.7.3`
- Bazel version (if compiling from source): `-`
- GCC/Compiler version (if compiling from source): `-`
- CUDA/cuDNN version: `-`
- GPU model and memory: `-`

**Describe the current behavior**
The TextVectorization layer `split` parameter expects `None` as a possible value but can't handle it.

**Describe the expected behavior**
The layer should work properly when `None` is passed as a `split` parameter, or documentation should be updated.

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from pprint import pprint
assert tf.__version__ == '2.1.0-rc1'

dummy_data = [""Foo"", ""bar"", ""foo foo"", ""foo bar"", ""foobar.""]
predict_data = [""foo"", ""bar"", ""foobar"", ""foo foo"", ""OOV""]
inputs = tf.keras.layers.Input(shape=(1, ), dtype=tf.string, name=""text"")
vectorize_layer = TextVectorization(output_mode=""binary"", max_tokens=5, split=None)
vectorize_layer.adapt(np.asarray(dummy_data))
print(f""Vocabulary:\t\t{vectorize_layer.get_vocabulary()}"")
outputs = vectorize_layer(inputs)
model = tf.keras.Model(inputs, outputs)
print(f""Prediction data:\t{predict_data}"")
predictions = model.predict(predict_data)
print(f""Predictions:"")
pprint(predictions)

AttributeError                            Traceback (most recent call last)
<ipython-input-3-f1a03cb1e414> in <module>()
      9 inputs = tf.keras.layers.Input(shape=(1, ), dtype=tf.string, name=""text"")
     10 vectorize_layer = TextVectorization(output_mode=""binary"", max_tokens=5, split=None)
---> 11 vectorize_layer.adapt(np.asarray(dummy_data))
     12 print(f""Vocabulary:\t\t{vectorize_layer.get_vocabulary()}"")
     13 outputs = vectorize_layer(inputs)

/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/layers/preprocessing/text_vectorization.py in _to_numpy(self, preprocessed_data)
    334     if isinstance(preprocessed_data, np.ndarray):
    335       return preprocessed_data
    336     return np.array(preprocessed_data.to_list())
AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'to_list'
```"
36069,setting allow_nudging_weights_to_use_fast_gemm_kernel in the python API does not work,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, 18.04 Ubuntu
- TensorFlow installed from (source or binary): pip3
- TensorFlow version (or github SHA if from source): 1.x


**Command used to run the converter or code if you’re using the Python API**

```
...
from tensorflow.lite.toco import toco_flags_pb2 as toco_flags
toco = toco_flags.TocoFlags()
toco.allow_nudging_weights_to_use_fast_gemm_kernel = True
converter.convert()
...
```

**The output from the converter invocation**

```
Bad value for conv/weights at index 187, previous bad value at index 175, distance=12, kMinDistanceBetweenBadValues=16. Consider passing --allow_nudging_weights_to_use_fast_gemm_kernel if you don't care about accuracy.
```
In particular
```Consider passing --allow_nudging_weights_to_use_fast_gemm_kernel if you don't care about accuracy.```

**Also, please include a link to the saved model or GraphDef**

```
N/A - see below
```

**Failure details**
- expected behaviour - the erroring code continues past this step and either fails elsewhere, or continues to pass. It should not suggest the same fix.

**Any other info / logs**

I am restricted in the details I can post of my application.
It looks like the source does not have a check in there, so probably still in 2.x"
36067,saved_model_cli breaks nightly packages,"Our in-house nightly builds were broken since 2020-01-16 when auditwheel tries to repair my nightly packages. The reason under the hood seems to be an incorrect link from the recent change of adding XLA support to `saved_model_cli` in 9959c04433623e0b7ebf6248e0f75bc7a24bd7cb.

Install the latest nightly, and navigate to the directory of `tensorflow_core/compiler/aot`:

```
$ ldd _pywrap_tfcompile.so
	linux-vdso.so.1 (0x00007ffc5e064000)
	libtensorflow_framework.so.2 => /usr/local/lib/python3.7/dist-packages/tensorflow_core/compiler/aot/./../../libtensorflow_framework.so.2 (0x00007fa798bba000)
	_pywrap_tensorflow_internal.so => not found
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fa798b77000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fa798b56000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fa7989d1000)
	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fa79884d000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fa798833000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fa798672000)
	/lib64/ld-linux-x86-64.so.2 (0x00007fa79afbc000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fa798668000)
```

Obviously it links to `_pywrap_tensorflow_internal.so` but it is not found with the relative path.

PS: we are using auditwheel==3.0.0 to produce manylinux2014 builds, but the official tf-nightly uses an older version which fails to catch this.

PPS: directly using `saved_model_cli` does not give this error as `_pywrap_tensorflow_internal.so` seems to be preloaded. But I am pretty sure this is a bug that we need to fix.

Ping @ebrevdo @mihaimaruseac."
36065,tf.keras MobileNetV2 with weights=None fails to train,"**System information**
- Google Colab notebook
- TensorFlow version: 2.1.0-rc1
- Python version: 3.7
- CUDA/cuDNN version: 10.0.130
- GPU model and memory: Tesla T4 12 GB

**Describe the current behavior**
Based on the tutorial: https://www.tensorflow.org/tutorials/images/transfer_learning#format_the_data

Start running the cells inside the notebook of the tutorial.

 _#Create the base model from the pre-trained model MobileNet V2_
```
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights=None)

base_model.trainable = True
```
Then trained the model for 10 epochs, with the parameters specified in the tutorial, but the validation loss does not go down, the accuracy remains stuck. 

The results of training:
`Epoch 1/10
582/582 [==============================] - 87s 149ms/step - loss: 0.6606 - accuracy: 0.5788 - val_loss: 0.6953 - val_accuracy: 0.5216
Epoch 2/10
582/582 [==============================] - 80s 138ms/step - loss: 0.6157 - accuracy: 0.6425 - val_loss: 0.7064 - val_accuracy: 0.5216
Epoch 3/10
582/582 [==============================] - 81s 139ms/step - loss: 0.5765 - accuracy: 0.6769 - val_loss: 0.7014 - val_accuracy: 0.5216
Epoch 4/10
582/582 [==============================] - 81s 139ms/step - loss: 0.5378 - accuracy: 0.7143 - val_loss: 0.7488 - val_accuracy: 0.4784
Epoch 5/10
582/582 [==============================] - 81s 139ms/step - loss: 0.5072 - accuracy: 0.7368 - val_loss: 0.8380 - val_accuracy: 0.4784
Epoch 6/10
582/582 [==============================] - 80s 138ms/step - loss: 0.4777 - accuracy: 0.7601 - val_loss: 0.9534 - val_accuracy: 0.4784
Epoch 7/10
582/582 [==============================] - 81s 138ms/step - loss: 0.4354 - accuracy: 0.7894 - val_loss: 1.0138 - val_accuracy: 0.4784
Epoch 8/10
582/582 [==============================] - 81s 138ms/step - loss: 0.3937 - accuracy: 0.8110 - val_loss: 1.2038 - val_accuracy: 0.4784
Epoch 9/10
582/582 [==============================] - 80s 138ms/step - loss: 0.3593 - accuracy: 0.8288 - val_loss: 1.7442 - val_accuracy: 0.4784
Epoch 10/10
582/582 [==============================] - 81s 139ms/step - loss: 0.3166 - accuracy: 0.8547 - val_loss: 1.6888 - val_accuracy: 0.4784`

![image](https://user-images.githubusercontent.com/60096583/72727499-ba4c1100-3b93-11ea-818e-531f342382d8.png)

**Describe the expected behavior**
If MobileNet V1 is used instead, with the same weight initialization and same training parameters, the results are the following:

`Epoch 1/10
582/582 [==============================] - 74s 126ms/step - loss: 0.6596 - accuracy: 0.5840 - val_loss: 0.7098 - val_accuracy: 0.5216
Epoch 2/10
582/582 [==============================] - 70s 120ms/step - loss: 0.6310 - accuracy: 0.6248 - val_loss: 0.6099 - val_accuracy: 0.6483
Epoch 3/10
582/582 [==============================] - 71s 122ms/step - loss: 0.6102 - accuracy: 0.6479 - val_loss: 0.6191 - val_accuracy: 0.6858
Epoch 4/10
582/582 [==============================] - 70s 121ms/step - loss: 0.5850 - accuracy: 0.6729 - val_loss: 0.5983 - val_accuracy: 0.6634
Epoch 5/10
582/582 [==============================] - 71s 122ms/step - loss: 0.5620 - accuracy: 0.6954 - val_loss: 0.6043 - val_accuracy: 0.6573
Epoch 6/10
582/582 [==============================] - 71s 122ms/step - loss: 0.5383 - accuracy: 0.7128 - val_loss: 0.5575 - val_accuracy: 0.6935
Epoch 7/10
582/582 [==============================] - 71s 122ms/step - loss: 0.5179 - accuracy: 0.7291 - val_loss: 0.6238 - val_accuracy: 0.7220
Epoch 8/10
582/582 [==============================] - 70s 121ms/step - loss: 0.4906 - accuracy: 0.7491 - val_loss: 0.5965 - val_accuracy: 0.6905
Epoch 9/10
582/582 [==============================] - 70s 121ms/step - loss: 0.4636 - accuracy: 0.7711 - val_loss: 0.5580 - val_accuracy: 0.7310
Epoch 10/10
582/582 [==============================] - 70s 120ms/step - loss: 0.4292 - accuracy: 0.7894 - val_loss: 0.5737 - val_accuracy: 0.7233`

![image](https://user-images.githubusercontent.com/60096583/72729377-b3bf9880-3b97-11ea-93d4-2c14c4ea7348.png)

In this case, the loss and the accuracy are going into the right direction

**Code to reproduce the issue**
Opened the Google Colab notebook, run all the cells up to section named _Create the base model from the pre-trained convnets_. Modified the cell first cell under this heading to the following:
`IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)

 _#Create the base model from the pre-trained model MobileNet V2_
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               #weights='imagenet')
                                               weights=None)

base_model.trainable = True
`
Then proceed by running the following cells inside the notebook, which create the classification head:
`global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
feature_batch_average = global_average_layer(feature_batch)
print(feature_batch_average.shape)

prediction_layer = keras.layers.Dense(1)
prediction_batch = prediction_layer(feature_batch_average)
print(prediction_batch.shape)

model = tf.keras.Sequential([
  base_model,
  global_average_layer,
  prediction_layer
])
`
Compile the model as in the tutorial, with the same parameters:
`base_learning_rate = 0.0001
model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])`

Then train the model. The initial loss is 0.69 and initial accuracy is 0.51:
`num_train, num_val, num_test = (
  metadata.splits['train'].num_examples*weight/10
  for weight in SPLIT_WEIGHTS
)

initial_epochs = 10
steps_per_epoch = round(num_train)//BATCH_SIZE
validation_steps=20

loss0,accuracy0 = model.evaluate(validation_batches, steps = validation_steps)

history = model.fit(train_batches,
                    epochs=initial_epochs,
                    validation_data=validation_batches)
`
"
36064,Custom C++ operator compilation error with Tensorflow 2.1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.1
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6.5.32
- GPU model and memory: NVIDIA Quadro M2000 4GB



**Describe the problem**
Compilation error in tensorblockv2.h (see log below) when building a custom C++ operator for Tensorflow 2.1 on Microsoft Visual Studio 2017. Note that the same custom operator compiles without any error for Tensorflow 2.0, this issue is specific to Tensorflow 2.1.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
1>e:\programfiles\anaconda3\envs\tensorflow2_python36\lib\site-packages\tensorflow_core\include\unsupported\eigen\cxx11\src\tensor\tensorblockv2.h(736): warning C4346: 'Eigen::internal::StridedLinearBufferCopy<Scalar,IndexType>::Kind': dependent name is not a type
1>e:\programfiles\anaconda3\envs\tensorflow2_python36\lib\site-packages\tensorflow_core\include\unsupported\eigen\cxx11\src\tensor\tensorblockv2.h(736): note: prefix with 'typename' to indicate a type
1>e:\programfiles\anaconda3\envs\tensorflow2_python36\lib\site-packages\tensorflow_core\include\unsupported\eigen\cxx11\src\tensor\tensorblockv2.h(842): note: see reference to class template instantiation 'Eigen::internal::StridedLinearBufferCopy<Scalar,IndexType>' being compiled
1>e:\programfiles\anaconda3\envs\tensorflow2_python36\lib\site-packages\tensorflow_core\include\unsupported\eigen\cxx11\src\tensor\tensorblockv2.h(736): error C2061: syntax error: identifier 'Kind'
1>e:\programfiles\anaconda3\envs\tensorflow2_python36\lib\site-packages\tensorflow_core\include\unsupported\eigen\cxx11\src\tensor\tensorblockv2.h(745): warning C4346: 'Eigen::internal::StridedLinearBufferCopy<Scalar,IndexType>::Kind': dependent name is not a type
1>e:\programfiles\anaconda3\envs\tensorflow2_python36\lib\site-packages\tensorflow_core\include\unsupported\eigen\cxx11\src\tensor\tensorblockv2.h(745): note: prefix with 'typename' to indicate a type
1>e:\programfiles\anaconda3\envs\tensorflow2_python36\lib\site-packages\tensorflow_core\include\unsupported\eigen\cxx11\src\tensor\tensorblockv2.h(745): error C2061: syntax error: identifier 'Kind'
```


"
36063,Request for tflite using Android NDK documentaion,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs


## URL(s) with the issue: https://groups.google.com/a/tensorflow.org/d/msg/discuss/sye03udicMI/sDr6MQvIDAAJ

## Description of issue (what needs changing): No resources anywhere about tflite usage using Android NDK.

### Clear description
I have a .tflite model which involves lots of post processing. 
After following https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_c  I have written the code in cpp and tested on linux and adb shell.

Now I want to use this code in the sample Android apk .(https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android)

Is there any documentation/sample usage of tflite cpp in Android? 
I have only found this(https://github.com/zimenglyu/TFLiteExample) but not able to follow this :(

Any help would be desperately appreciated.
"
36062,Request example for object detection  on Tensorflow >= 2.0 in CPP,"Hello,
I want to use Tensorflow frozen models in c++. I did find examples on CPP in Tensorflow < 2.0:
https://github.com/CasiaFan/tensorflow_cpp_object_detection_web_server

Where could I get an simple SSD object detection on CPP in Tensorflow >= 2.0 ?
  

**System information** : Ubuntu 18.04
- TensorFlow version (you are using): 2.0 
- Are you willing to contribute it (Yes/No): Yes

"
36061,Segmentation fault error when running Makefile,"@tensorflow/micro

- Host OS Platform and Distribution: Windows 10
- TensorFlow installed using ""git clone https://github.com/tensorflow/tensorflow.git""
- Tensorflow version: 2.1.0
- Target platform: Laptop

Hello, I'm following the tutorial provided on the book ""TinyML - Machine Learning with TensorFlow Lite on Arduino and Ultra-Low-Power Microcontrollers"". The aim is to run the ""hello world"" test. The problem is that I got a ""Segmentation fault"" error when I run the command:

mingw32-make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test

In the following I attach a screenshot of the error:
![Immagine](https://user-images.githubusercontent.com/59959289/72727161-87e9e600-3b8a-11ea-8fe5-b8f994d61ab5.png)

How can I fix this issue?
"
36059,Cannot convert between a TensorFlowLite buffer with 1392640 bytes and a Java Buffer with 4177920 bytes.,"I am using Tensorflow.Lite.Support function for Inference of a model that takes two input and gives output in the form of Image. The first input is an RGB image whereas the second image is a single-channel image.When I run the application for inference I get the error that is:

**Cannot convert between a TensorFlowLite buffer with 1392640 bytes and a Java Buffer with 4177920 bytes.**

I have attached my code snippet below:
```
protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);
        Toolbar toolbar = findViewById(R.id.toolbar);
        setSupportActionBar(toolbar);

        FloatingActionButton fab = findViewById(R.id.fab);
        fab.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                Snackbar.make(view, ""Replace with your own action"", Snackbar.LENGTH_LONG)
                        .setAction(""Action"", null).show();
            }
        });

        try {
            tflite = new Interpreter(loadModelFile(MainActivity.this, ""converted_model.tflite""));


            Bitmap Image = getBitmapFromAsset(this,""001.png"");
            Bitmap Mask = getBitmapFromAsset(this,""tmp_mask.png"");

            int imageTensorIndex = 0;
            int[] imageShape = tflite.getInputTensor(imageTensorIndex).shape(); // {1, height, width, 3}
            int imageSizeY = imageShape[1];
            int imageSizeX = imageShape[2];
            DataType imageDataType = tflite.getInputTensor(imageTensorIndex).dataType();

            TensorImage inputImageBuffer = new TensorImage(imageDataType);
            inputImageBuffer.load(Image);

            int imageTensorIndex1 = 1;
            int[] imageShape1 = tflite.getInputTensor(imageTensorIndex1).shape(); // {1, height, width, 3}
            int imageSizeY1 = imageShape1[1];
            int imageSizeX1 = imageShape1[2];
            DataType imageDataType1 = tflite.getInputTensor(imageTensorIndex1).dataType();

            TensorImage inputImageBuffer1 = new TensorImage(imageDataType1);
            inputImageBuffer1.load(Mask);


            int OutputTensorIndex = 0;
            int[] OutputShape =
                    tflite.getOutputTensor(OutputTensorIndex).shape(); // {1, NUM_CLASSES}
            DataType OutputDataType = tflite.getOutputTensor(OutputTensorIndex).dataType();

            TensorBuffer outputBuffer = TensorBuffer.createFixedSize(OutputShape, OutputDataType);

            Object[] Inputs = {inputImageBuffer.getBuffer(),inputImageBuffer1.getBuffer()};

            Map<Integer, Object> outputs = new HashMap<>();
            outputs.put(0,outputBuffer);
            tflite.runForMultipleInputsOutputs(Inputs, outputs);

            Toast.makeText(this,""Working"",Toast.LENGTH_LONG).show();
  
        } catch (IOException e) {
            Toast.makeText(this,""Failed"",Toast.LENGTH_LONG).show();
            e.printStackTrace();
        }
    }

```"
36057,Execution platform: @bazel_tools//platforms:host_platform: bash failed: error executing command,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04.3
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.x
- Python version: 3.8.1
- Installed using virtualenv? pip? conda?: pip 
- Bazel version (if compiling from source): 0.26.1 with 
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
- CUDA/cuDNN version: 10.2 / 7.6.5
- TensorRT: 7.0.0
- GPU model and memory: compute capability 5.2 



**Describe the problem**

```console
ERROR: ....../tensorflow/python/BUILD:7692:1: Executing genrule //tensorflow/python:framework/fast_tensor_util.pyx_cython_translation failed (Profiling timer expired): bash failed: error executing command 
  (cd ~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 \
    LD_LIBRARY_PATH=/opt/Qt/Current/gcc_64/lib:/opt/Qt/Current/gcc_64/plugins/platforms:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib:/lib:/lib64:/lib/x86_64-linux-gnu:/opt/intel/compilers_and_libraries/linux/daal/lib/intel64:/opt/intel/compilers_and_libraries/linux/ipp/lib/intel64:/opt/intel/compilers_and_libraries/linux/mkl/lib/intel64:/opt/intel/compilers_and_libraries/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries/linux/mpi/intel64/lib/release:/opt/intel/compilers_and_libraries/linux/tbb/lib/intel64/gcc4.7:/opt/intel/compilers_and_libraries/linux/lib/intel64:/opt/intel/openvino/inference_engine/lib/intel64:/usr/local/cuda/targets/x86_64-linux/lib:/opt/mcu/arduino/lib:/usr/lib/jvm/default-java/lib: \
    PATH=~/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/opt/Qt/Current/gcc_64/bin:/opt/intel/compilers_and_libraries/linux/daal/bin:/opt/intel/compilers_and_libraries/linux/ipp/bin:/opt/intel/compilers_and_libraries/linux/mkl/bin:/opt/intel/compilers_and_libraries/linux/mpi/intel64/bin:/opt/intel/compilers_and_libraries/linux/tbb/bin:/opt/intel/compilers_and_libraries/linux/bin:/usr/local/cuda/bin:/usr/local/cuda/samples/bin/x86_64/linux/release:/opt/node/bin:/usr/lib/jvm/default-java/bin \
    PYTHONPATH=~/.local/lib/python3.8/site-packages:/usr/lib/python3.8/site-packages:/usr/local/lib/python3.8/site-packages:/usr/lib/python3/dist-packages \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=~/.local/lib/python3.8/site-packages \
    TF2_BEHAVIOR=1 \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=5.2 \
    TF_NEED_CUDA=1 \
    TF_NEED_TENSORRT=1 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; PYTHONHASHSEED=0 bazel-out/host/bin/external/cython/cython_binary --cplus tensorflow/python/framework/fast_tensor_util.pyx --output-file bazel-out/k8-opt/bin/tensorflow/python/framework/fast_tensor_util.cpp')
Execution platform: @bazel_tools//platforms:host_platform: bash failed: error executing command 
  (cd ~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 \
    LD_LIBRARY_PATH=/opt/Qt/Current/gcc_64/lib:/opt/Qt/Current/gcc_64/plugins/platforms:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib:/lib:/lib64:/lib/x86_64-linux-gnu:/opt/intel/compilers_and_libraries/linux/daal/lib/intel64:/opt/intel/compilers_and_libraries/linux/ipp/lib/intel64:/opt/intel/compilers_and_libraries/linux/mkl/lib/intel64:/opt/intel/compilers_and_libraries/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries/linux/mpi/intel64/lib/release:/opt/intel/compilers_and_libraries/linux/tbb/lib/intel64/gcc4.7:/opt/intel/compilers_and_libraries/linux/lib/intel64:/opt/intel/openvino/inference_engine/lib/intel64:/usr/local/cuda/targets/x86_64-linux/lib:/opt/mcu/arduino/lib:/usr/lib/jvm/default-java/lib: \
    PATH=~/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/opt/Qt/Current/gcc_64/bin:/opt/intel/compilers_and_libraries/linux/daal/bin:/opt/intel/compilers_and_libraries/linux/ipp/bin:/opt/intel/compilers_and_libraries/linux/mkl/bin:/opt/intel/compilers_and_libraries/linux/mpi/intel64/bin:/opt/intel/compilers_and_libraries/linux/tbb/bin:/opt/intel/compilers_and_libraries/linux/bin:/usr/local/cuda/bin:/usr/local/cuda/samples/bin/x86_64/linux/release:/opt/node/bin:/usr/lib/jvm/default-java/bin \
    PYTHONPATH=~/.local/lib/python3.8/site-packages:/usr/lib/python3.8/site-packages:/usr/local/lib/python3.8/site-packages:/usr/lib/python3/dist-packages \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=~/.local/lib/python3.8/site-packages \
    TF2_BEHAVIOR=1 \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=5.2 \
    TF_NEED_CUDA=1 \
    TF_NEED_TENSORRT=1 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; PYTHONHASHSEED=0 bazel-out/host/bin/external/cython/cython_binary --cplus tensorflow/python/framework/fast_tensor_util.pyx --output-file bazel-out/k8-opt/bin/tensorflow/python/framework/fast_tensor_util.cpp')
Execution platform: @bazel_tools//platforms:host_platform
/bin/bash: line 1:  2347 Profiling timer expired PYTHONHASHSEED=0 bazel-out/host/bin/external/cython/cython_binary --cplus tensorflow/python/framework/fast_tensor_util.pyx --output-file bazel-out/k8-opt/bin/tensorflow/python/framework/fast_tensor_util.cpp
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 374.150s, Critical Path: 84.85s
INFO: 1334 processes: 1334 local.
FAILED: Build did NOT complete successfully
➜  tensorflow git:(master) ✗ 
```
"
36055,TFLite Android Model Benchmark Tool -- results not showing up in adb logcat,"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- Mobile device: Pixel 3
- TensorFlow installed from: source
- TensorFlow version: From master branch, commit hash 69ac54d92de18ca18f9a110d6dd464aeb1116342. Compiled tensorflow with `./configure` and then `bazel build --config=v1 //tensorflow/tools/pip_package:build_pip_package`.
- Python version: 3.6
- Installed using: conda
- Bazel version (if compiling from source): 1.2.1
- GCC/Compiler version (if compiling from source): 7.4, and compiling with c++14 flag
- CUDA/cuDNN version: not using gpu for this experiment
- GPU model and memory: not using gpu for this experiment
- TFLite model that I am trying to benchmark: `mobilenet_v1_0.25_192_quant.tflite` from [this page](https://www.tensorflow.org/lite/guide/hosted_models). Note that I do have some more exotic models that I'd like to benchmark, but I am starting with an off-the-shelf model from an official tensorflow page.


**Describe the problem**

I am trying to benchmark the speed of a tflite model on a pixel 3. 

On an ubuntu computer, with a Pixel 3 in developer mode connected via USB, I followed steps 1 to 5 in **[this readme](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark/android/README.md)**. I did not get a single error message. When I get to step 4, I get the following output:

```
Stopping: org.tensorflow.lite.benchmark
Starting: Intent { cmp=org.tensorflow.lite.benchmark/.BenchmarkModelActivity (has extras) }
```

While running step 4, I have an other terminal window open with `adb logcat` running. Unfortunately, the phrase `Average inference` doesn't show up in the `logcat` output, and the only relevant thing in `logcat` is `01-19 23:30:20.449  1357  3307 I ActivityTaskManager: START u0 {flg=0x10000000 cmp=org.tensorflow.lite.benchmark/.BenchmarkModelActivity (has extras)} from uid 2000`, but there are no latency numbers showing up in `logcat`. 

Any idea what I am doing wrong, and what's preventing the latency numbers from showing up in `logcat`?"
36054,Failed build on windows wtih MKL,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.1 master branch, git rev-parse HEAD : 6bc6e8df2c515c229735c4bd85b7ff5084dfccef
- Python version: 3.8
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 1.2.1
- GCC/Compiler version (if compiling from source): msvc 2019
- CUDA/cuDNN version: disabled
- GPU model and memory: 

configure: No XLA/Cuda/RocM, no ""disable Eigen strong inline""
optimization flags: -O2 -fp:fast -Ob3

applied patch from https://github.com/tensorflow/tensorflow/issues/25213#issuecomment-501564390

build command:
`bazel --output_base=C:\Lib\.bazel\tf-noavx-mkl build --jobs 6 --config=mkl --config=monolithic --config=opt --config=v2 //tensorflow/tools/pip_package:build_pip_package`

build error:
```
ERROR: F:/lib/tensorflow/tensorflow/BUILD:868:1: Executing genrule //tensorflow:tf_python_api_gen_v2 failed (Exit 1)
Traceback (most recent call last):
  File ""\\?\F:\Temp\Bazel.runfiles_ejm1kz55\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""\\?\F:\Temp\Bazel.runfiles_ejm1kz55\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""\\?\F:\Temp\Bazel.runfiles_ejm1kz55\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\K13\tf-noavx\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\K13\tf-noavx\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""\\?\F:\Temp\Bazel.runfiles_ejm1kz55\runfiles\org_tensorflow\tensorflow\python\tools\api\generator\create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""\\?\F:\Temp\Bazel.runfiles_ejm1kz55\runfiles\org_tensorflow\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""\\?\F:\Temp\Bazel.runfiles_ejm1kz55\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""\\?\F:\Temp\Bazel.runfiles_ejm1kz55\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""\\?\F:\Temp\Bazel.runfiles_ejm1kz55\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""\\?\F:\Temp\Bazel.runfiles_ejm1kz55\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\K13\tf-noavx\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\K13\tf-noavx\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.

```"
36048,tf.keras.losses.MeanSquaredError doesnt support scalar inputs,"I am using tensorflow version 2.2.0-dev20200119. tf.keras.losses.MeanSquaredError can't handle input scalar values properly. The following code can reproduce the problem.

```python
#!/usr/bin/python3
import tensorflow as tf;
tf.keras.losses.MeanSquaredError()(1,0);
```
"
36045,Using TensorFlow backend. ERROR:root:Internal Python error in the inspect module.,"**TensorFlow isn't working - Please help**

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 3319, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-88d96843a926>"", line 1, in <module>
    import keras
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\backend\load_backend.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Program Files\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 2034, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\ultratb.py"", line 1151, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\ultratb.py"", line 319, in wrapped
    return f(*args, **kwargs)
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\ultratb.py"", line 353, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File ""C:\Program Files\Python37\lib\inspect.py"", line 1502, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File ""C:\Program Files\Python37\lib\inspect.py"", line 1460, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""C:\Program Files\Python37\lib\inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""C:\Program Files\Python37\lib\inspect.py"", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Program Files\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Program Files\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 3319, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-88d96843a926>"", line 1, in <module>
    import keras
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\backend\load_backend.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Program Files\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 2034, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 3319, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-88d96843a926>"", line 1, in <module>
    import keras
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\backend\load_backend.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Program Files\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 2034, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 3242, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 3336, in run_code
    self.showtraceback(running_compiled_code=True)
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 2037, in showtraceback
    value, tb, tb_offset=tb_offset)
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\ultratb.py"", line 1418, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\ultratb.py"", line 1318, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\ultratb.py"", line 1186, in structured_traceback
    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)
TypeError: can only concatenate str (not ""list"") to str

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 2034, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'TypeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\ultratb.py"", line 1151, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\ultratb.py"", line 319, in wrapped
    return f(*args, **kwargs)
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\ultratb.py"", line 353, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File ""C:\Program Files\Python37\lib\inspect.py"", line 1502, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File ""C:\Program Files\Python37\lib\inspect.py"", line 1460, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""C:\Program Files\Python37\lib\inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""C:\Program Files\Python37\lib\inspect.py"", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Program Files\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Program Files\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 3319, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-88d96843a926>"", line 1, in <module>
    import keras
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\backend\load_backend.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Program Files\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 2034, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 3242, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 3336, in run_code
    self.showtraceback(running_compiled_code=True)
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 2037, in showtraceback
    value, tb, tb_offset=tb_offset)
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\ultratb.py"", line 1418, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\ultratb.py"", line 1318, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\ultratb.py"", line 1186, in structured_traceback
    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)
TypeError: can only concatenate str (not ""list"") to str

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py"", line 2034, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'TypeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

C:\Program Files\Python37\lib\imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

C:\Program Files\Python37\lib\imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:


During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py in showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)
   2033                         # in the engines. This should return a list of strings.
-> 2034                         stb = value._render_traceback_()
   2035                     except Exception:

AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py in run_code(self, code_obj, result, async_)
   3334             if result is not None:
   3335                 result.error_in_exec = sys.exc_info()[1]
-> 3336             self.showtraceback(running_compiled_code=True)
   3337         else:
   3338             outflag = False

c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\interactiveshell.py in showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)
   2035                     except Exception:
   2036                         stb = self.InteractiveTB.structured_traceback(etype,
-> 2037                                             value, tb, tb_offset=tb_offset)
   2038 
   2039                     self._showtraceback(etype, value, stb)

c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\ultratb.py in structured_traceback(self, etype, value, tb, tb_offset, number_of_lines_of_context)
   1416             self.tb = tb
   1417         return FormattedTB.structured_traceback(
-> 1418             self, etype, value, tb, tb_offset, number_of_lines_of_context)
   1419 
   1420 

c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\ultratb.py in structured_traceback(self, etype, value, tb, tb_offset, number_of_lines_of_context)
   1316             # Verbose modes need a full traceback
   1317             return VerboseTB.structured_traceback(
-> 1318                 self, etype, value, tb, tb_offset, number_of_lines_of_context
   1319             )
   1320         elif mode == 'Minimal':

c:\users\f174s322\pycharmprojects\nn_trial run\trial_1_jan_19_2020\venv\lib\site-packages\IPython\core\ultratb.py in structured_traceback(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)
   1184         exception = self.get_parts_of_chained_exception(evalue)
   1185         if exception:
-> 1186             formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)
   1187             etype, evalue, etb = exception
   1188         else:

TypeError: can only concatenate str (not ""list"") to str"
36044,"Simple keras model, Model.fit() does not learn unless experimental_run_tf_function=False at compile","**System information**
- Windos 10, 64 Bit
- TensorFlow installed from pip
- TensorFlow version: v2.1.0-rc2-17-ge5bf8de410 2.1.0
- Python version: 3.7

**Describe the current behavior**

Simple Model (see below), compiles without error. It is used in a reinforcement scenario, i.e. iterations of `predict()` and `fit()` calls to iteratively train the model. 
Currently the model does not seem to improve with calls to `fit()` unless `compile()` was called with `compile(..., experimental_run_tf_function = False)`. 

**Describe the expected behavior**

Model should train equally well whether `experimental_run_tf_function = False` was passed to `model.compile()` or not.

**Code to reproduce the issue**

Example code can be found at [https://github.com/fcarsten/tic-tac-toe/blob/tf-2.1-issue/test_nn_q_tf2.py]( https://github.com/fcarsten/tic-tac-toe/blob/tf-2.1-issue/test_nn_q_tf2.py) - need to check out the whole branch as it uses other code from this repository. 

Either run as is `run_test(True)` to see failing, or `run_test(False)` to see how it should run. 

When running as expected ""Player 1 win %"" should increase and end up over 80%, usually somewhere around 90%. When not running as expected, ""Player 1 win %"" will randomly meander up and down.

The model is defined in File SimpleNNQPlayerTF2.py line 29 ff:
```
input_layer = tf.keras.Input(shape=(BOARD_SIZE * 3,))
x = tf.keras.layers.Dense(BOARD_SIZE * 3 * 9, activation='relu')(input_layer)
x = tf.keras.layers.Dense(BOARD_SIZE * 3 * 100, activation='relu')(x)
x = tf.keras.layers.Dense(BOARD_SIZE * 3 * 9, activation='relu')(x)
q_values = tf.keras.layers.Dense(BOARD_SIZE, activation=None, name='q_values')(x)
probabilities = tf.keras.layers.Softmax(name='probabilities')(q_values)

self.model = tf.keras.Model(inputs=input_layer, outputs=[probabilities, q_values])
if run_tf_function:
    self.model.compile(optimizer='adam', loss = [None, tf.keras.losses.MeanSquaredError()])
else:
    self.model.compile(optimizer='adam', loss = [None, tf.keras.losses.MeanSquaredError()], experimental_run_tf_function = False)
```

**Other info / logs**

While the model is a very simple sequential model, note that it has 2 output layers and the training target layer is not the final layer in the model. Not sure this makes any difference, 
"
36042,NotImplementedError in conv2 when i use in tf.distribute.experimental.CentralStorageStrategy() for multi gpus in tf2,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):
- TensorFlow installed from conda install tensorflow.
- TensorFlow version : 2.0.0
- Python version: Python 3.7.6 
- GCC/Compiler version (if compiling from source): gcc version 7.4.0
- CUDA/cuDNN version: CUDA Version 10.1.243
- GPU model and memory: 4 gpus of Tesla V100-SXM2-16Gb

**Describe the current behavior**
I used in `tf.distribute.experimental.CentralStorageStrategy()` for use all the gpus in my computer
 I run the following code and got `NotImplementedError`

```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, Input

strategy = tf.distribute.experimental.CentralStorageStrategy()
with strategy.scope():
    Xi = np.zeros((2,1,1,396))
    Pi = Input(shape=(1,1,4), name = 'input_layer_preamble')
    Pd = Conv2D(int(Xi.shape[3]//Pi.shape[3]), (1, 1), padding='same', data_format='channels_first', use_bias=False)(Pi)
```
 


**Describe the current behavior**
```python
Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/envs/my_tf2/lib/python3.7/contextlib.py"", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/ubuntu/anaconda3/envs/my_tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py"", line 2712, in variable_creator_scope
    yield
  File ""tf2_nulty_gpus.py"", line 9, in <module>
    Pd = Conv2D(int(Xi.shape[3]//Pi.shape[3]), (1, 1), padding='same', data_format='channels_first', use_bias=False)(Pi)
  File ""/home/ubuntu/anaconda3/envs/my_tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 817, in __call__
    self._maybe_build(inputs)
  File ""/home/ubuntu/anaconda3/envs/my_tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2141, in _maybe_build
    self.build(input_shapes)
  File ""/home/ubuntu/anaconda3/envs/my_tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py"", line 165, in build
    dtype=self.dtype)
  File ""/home/ubuntu/anaconda3/envs/my_tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2311, in __setattr__
    if val.trainable:
  File ""/home/ubuntu/anaconda3/envs/my_tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py"", line 477, in trainable
    raise NotImplementedError
NotImplementedError
```"
36039,ADD NEW DOCUMENTATION ON HOW TO INTERACT WITH ASTRONOMY WITH ANIMATION ?,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs




## Description of issue (what needs changing):
We need a new documentation to interact with astronomy.
### Clear description

Like we think astronomical animation is good for research. We need a new documentaion how to interact with Phoebe or with other by Tensorflow .


"
36038,Showing index instead of labels in word embedding projector,"For large embedding words, I can't show the word itself but the index instead appear. And so, I can't estimate the performance of the word embedding. So, I think words should appear instead of index so, how I can benefit from it?

**System information**
- http://projector.tensorflow.org/

You can add an option so, I can choose either index or words.

Sorry for my bad English.
"
36036,Allow CuDNN 7.2 to convert float -> fp16 for tensor core ops,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): Tensorflow using cuDNN 7.2 and above
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Now, if users want, cuDNN can convert fp32 inputs to fp16 to allow using tensor cores even when inputs are fp32. Before, users had to explicitly convert fp32 to fp16 using AMP. [Link](https://devblogs.nvidia.com/tensor-ops-made-easier-in-cudnn/). This is as simple as setting `CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION` enum value and passing it to `cudnnSetConvolutionMathType()` functioin.

```cpp
cudnnSetConvolutionMathType(cudnnConvDesc, CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION)
```

and this is the counterpart for RNN:

```cpp
cudnnSetRNNMatrixMathType(cudnnRnnDesc, CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION)
```


We should let users chose if they want this option, since it involves converting fp32 to fp16."
36034,Possible mem leak in tf.random.categorical,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.0.0
- Python version:3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.0.130
- GPU model and memory:1060  6G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
If I set seed, the memory keeps increasing between loops.But if I remove seed, the memory will remains the same.

**Describe the expected behavior**
Whether I set random seed or not，the memory will remains the same.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import tensorflow as tf
import numpy as np
import tensorflow_probability as tfp
import psutil
import os

if __name__ == '__main__':
    act_dim = 8

    model = tf.keras.Sequential([tf.keras.layers.Input(500, ),
                                 tf.keras.layers.Dense(500, activation=""relu""),
                                 tf.keras.layers.Dense(act_dim, activation=""softmax"")])

    tf.random.set_seed(0)
    print(""memory used:"", psutil.Process(os.getpid()).memory_info().rss)

    for j in range(100):
        for i in range(1000):
            x = np.random.rand(500, 500)
            prob = model(x)

            dist = tfp.distributions.Categorical(probs=prob)
            a = dist.sample()
        print(""memory used:"", psutil.Process(os.getpid()).memory_info().rss)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
36033,tensorflow1.12 hangs at MapAndBatchDatasetOp::Dataset::Iterator::RunnerThread,"- OS Platform and Distribution: aarch64 
- TensorFlow installed from (source or binary):source
- TensorFlow version: 1.12
- Python version: 3.6.1
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):

preprocessing script is MapAndBatchDataset+PrefetchDataset, When we run training for a long time, we find that the training will be hung. We found that when hanging, the value of batch_results_.front()->num_calls in the following logic of map_and_batch_dataset_op.cc is always 1

    Status GetNextInternal(IteratorContext* ctx,
                             std::vector<Tensor>* out_tensors,
                             bool* end_of_sequence) override {
        std::shared_ptr<BatchResult> result;
        {
          mutex_lock l(*mu_);
          EnsureRunnerThreadStarted(ctx);
          while (batch_results_.empty() ||
                 batch_results_.front()->num_calls > 0) {
            RecordStop(ctx);
            cond_var_->wait(l);
            RecordStart(ctx);
          }
          std::swap(result, batch_results_.front());
          batch_results_.pop_front();
          cond_var_->notify_all();
        }
        return ProcessResult(ctx, result, out_tensors, end_of_sequence);
      }

We found another function, which will reduce result-> num_calls by one. We will modify the property of result-> num_calls to atomic and there will be no problem. Does this mean that this code does not have a memory barrier.
     void CallCompleted(const std::shared_ptr<BatchResult>& result)
          LOCKS_EXCLUDED(*mu_) {
        mutex_lock l(*mu_);
        num_calls_--;
        result->num_calls--;
        cond_var_->notify_all();
      }

One more thing to say is that we only found the problem on ARM OS(aarch64), but not on Linux Ubuntu OS(X86).

We want to know if there is a bug in this snippet of TensorFlow on aarch64 and how to fix it.

Here is the call stack when hung:
Thread 374 (LWP 20847):
#0  0x0000ffffb997d22c in pthread_cond_wait@@GLIBC_2.17 () from target:/lib/aarch64-linux-gnu/libpthread.so.0
#1  0x0000ffffb11adb30 in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from target:/usr/lib/aarch64-linux-gnu/libstdc++.so.6
#2  0x0000ffffb5501c5c in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()
   from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x0000ffffb55012d4 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()
   from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x0000ffffb54fe7f0 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()
   from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x0000ffffb3c962d4 in tensorflow::data::(anonymous namespace)::MapAndBatchDatasetOp::Dataset::Iterator::RunnerThread(std::shared_ptr<tensorflow::data::IteratorContext> const&) ()
   from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x0000ffffb11b3e14 in ?? () from target:/usr/lib/aarch64-linux-gnu/libstdc++.so.6
#7  0x0000ffffb9977088 in start_thread () from target:/lib/aarch64-linux-gnu/libpthread.so.0
#8  0x0000ffffb98054ec in ?? () from target:/lib/aarch64-linux-gnu/libc.so.6

Thread 372 (LWP 20844):
#0  0x0000ffffb997d22c in pthread_cond_wait@@GLIBC_2.17 () from target:/lib/aarch64-linux-gnu/libpthread.so.0
#1  0x0000ffffb11adb30 in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from target:/usr/lib/aarch64-linux-gnu/libstdc++.so.6
#2  0x0000ffffb5501c5c in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()
   from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x0000ffffb55012d4 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()
   from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x0000ffffb54fe7f0 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()
   from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x0000ffffb3c938e4 in tensorflow::data::(anonymous namespace)::MapAndBatchDatasetOp::Dataset::Iterator::GetNextInternal(tensorflow::data::IteratorContext*, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, bool*) () from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x0000ffffb3c41918 in tensorflow::data::DatasetBaseIterator::GetNext(tensorflow::data::IteratorContext*, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, bool*) ()
   from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x0000ffffb3d25644 in std::_Function_handler<void (), tensorflow::data::PrefetchDatasetOp::Dataset::Iterator::EnsurePrefetchThreadStarted(tensorflow::data::IteratorContext*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x0000ffffb11b3e14 in ?? () from target:/usr/lib/aarch64-linux-gnu/libstdc++.so.6
#9  0x0000ffffb9977088 in start_thread () from target:/lib/aarch64-linux-gnu/libpthread.so.0
#10 0x0000ffffb98054ec in ?? () from target:/lib/aarch64-linux-gnu/libc.so.6"
36029,inference result is unstable,"when i do inference use the same input at several times,  the inference result will be error at some times, this is terrible.  I don't know why, and anyone can help me? Thank you. "
36025,"Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro, Kernel 5.3.18
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `v2.1.0-rc2-17-ge5bf8de 2.1.0`
- Python version: 3.7.4
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Cuda 10.1, cuDNN 7.6.5
- GPU model and memory: RTX 2070 8GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

Import a ResNet .pb model and run it. The following error will occur and whether setting `allow_growth` or not does not help:

```
2020-01-18 12:56:02.661655: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2020-01-18 12:56:02.666829: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
Traceback (most recent call last):
  File ""/home/abcdabcd987/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1367, in _do_call
    return fn(*args)
  File ""/home/abcdabcd987/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1352, in _run_fn
    target_list, run_metadata)
  File ""/home/abcdabcd987/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow_core/python/client/session.py"", line 1445, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node resnet_v1_50/conv1/Conv2D}}]]
	 [[output/_3]]
  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node resnet_v1_50/conv1/Conv2D}}]]
0 successful operations.
0 derived errors ignored.
```

**Describe the expected behavior**

Run it successfully

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import os
import numpy as np
import tensorflow as tf

MODEL_PB_FILENAME = 'resnet_v1_50.pb'
INPUT_TENSOR_NAME = 'input:0'
OUTPUT_TENSOR_NAME = 'output:0'

def main():
    with tf.device('/GPU:0'):
        with tf.compat.v1.gfile.GFile(MODEL_PB_FILENAME, ""rb"") as f:
            graph_def = tf.compat.v1.GraphDef()
            graph_def.ParseFromString(f.read())
            g_in = tf.import_graph_def(graph_def, name="""")

    config = tf.compat.v1.ConfigProto()
    # config.allow_soft_placement = True
    # config.log_device_placement = True
    # config.gpu_options.allow_growth = True

    sess = tf.compat.v1.Session(graph=g_in, config=config)

    batch_x = np.random.randn(1, 224, 224, 3);
    output = sess.run(
        OUTPUT_TENSOR_NAME,
        feed_dict={INPUT_TENSOR_NAME: batch_x})[0]
    print(output)
    print(tf.test.is_gpu_available())
    print('config', list(map(hex, config.SerializeToString())))


main()
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[test_tf.log](https://github.com/tensorflow/tensorflow/files/4081770/test_tf.log)
"
36024,Tensorflow Lite New Converter does not allow to use inference_input_type and inference_output_type with V2 APIs,"**System information**
- OS Platform and Distribution (Linux, MacOS):
- TensorFlow installed from (official python wheel):
- TensorFlow version (v2.1.0):


```python
    converter = tf.lite.TFLiteConverter.from_keras_model(model)

    converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
    converter.experimental_new_converter = True
    converter.experimental_new_quantizer = True
    converter.representative_dataset = representative_data_gen
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8

    tflite_model = converter.convert()
```

Above code segment is expected to produce uint8 inference_input_type and inference_output_type, yet there is no uint8 conversion of the input layers and it ended up in identifying as floating_model when used in example [label_image.cc](https://github.com/tensorflow/tensorflow/blob/6ef62c6d2e90675eed0bb6ed10d8c5761ab365c1/tensorflow/lite/examples/label_image/label_image.cc#L226)

```
...
mobilenetv2_1.00_224/global_average_pooling2d/Mean <type 'numpy.int8'>
mobilenetv2_1.00_224/global_average_pooling2d/Mean/reduction_indices <type 'numpy.int32'>
mobilenetv2_1.00_224/out_relu/Relu <type 'numpy.int8'>
input_1 <type 'numpy.float32'>
Identity <type 'numpy.float32'>
```
Model used: [mobilenet_v2_1.0_224_quant.tgz](https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz)


**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Unable to produce true uint8 input/output layers.
"
36021,Loading a tf.Module with tf.saved_model.load missing attributes,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos Catalina
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `v2.1.0-rc2-17-ge5bf8de410 2.1.0`
- Python version: `3.7.5`
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
When saving and then loading a `tf.Module` instance using `tf.saved_model`, some of the attributes, such as `trainable_variables` are missing from the loaded object.

**Describe the expected behavior**
I would expect attributes like `trainable_variables` to persist through the serialization/deserialization.

**Code and logs to reproduce the issue**
For example:

```python
import tensorflow as tf


class TestModule(tf.Module):
    def __init__(self, value):
        self.variable = tf.Variable(value)


module_1 = TestModule(value=9000)
tf.saved_model.save(module_1, ""./foo"")
module_2 = tf.saved_model.load(""./foo"")

assert module_1.variable.numpy() == module_2.variable.numpy()
assert module_1.trainable_variables == (module_1.variable, )
assert module_2.trainable_variables == (module_2.variable, )
assert module_1.trainable_variables == module_2.trainable_variables
```

raises 

```
$ python -m tests.test_tf_saved_model_trainable_variables_missing
Traceback (most recent call last):
  File ""/Users/hartikainen/conda/envs/softlearning-3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/hartikainen/conda/envs/softlearning-3/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/hartikainen/github/rail-berkeley/softlearning-3/tests/test_tf_saved_model_trainable_variables_missing.py"", line 15, in <module>
    assert module_2.trainable_variables == (module_2.variable, )
AttributeError: '_UserObject' object has no attribute 'trainable_variables'
```
"
36020,can't import tensorflow ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): don't understand question
- TensorFlow version: tensorflow-gpu 1.8.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 9.1.85
- GPU model and memory: Quadro k2100M cuda cores 3.0 2GB memory


I installed CUDA 9.1.85 and  CUDnn files and then i ran a wheel supporting my pc requirements but getting this error

Microsoft Windows [Version 6.3.9600]
(c) 2013 Microsoft Corporation. All rights reserved.

C:\Users\AliJunaid>python
Python 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\AliJunaid\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\AliJunaid\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Users\AliJunaid\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\core\framework\graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""C:\Users\AliJunaid\AppData\Local\Programs\Python\Python36\lib\site-packages\google\protobuf\descriptor.py"", line 47, in <module>
    from google.protobuf.pyext import _message
ImportError: DLL load failed: The specified procedure could not be found.
>>> exit()

C:\Users\AliJunaid>
"
36016,Error optimizing my TFLite model for GPU usage,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro for Workstations (Build: 18363.592)
- TensorFlow installed from (source or binary): pip install tf-nightly
- TensorFlow version (or github SHA if from source): 2.2.0-dev20200115


**Command used to run the converter or code if you’re using the Python API**

```
tf_lite_converter.py :

import tensorflow as tf
from tensorflow_core.lite.python.interpreter import Interpreter
from tensorflow_core.lite.python.lite import TFLiteConverter, Optimize

MODEL_PATH = <path to keras model file>
TFLITE_MODEL_PATH = <path to optimized TFLite model file>

converter = TFLiteConverter.from_keras_model_file(MODEL_PATH)
converter.optimizations.append(Optimize.DEFAULT)
converter.target_spec.supported_types.append(tf.float16)
converter.experimental_new_converter = True
tflite_model = converter.convert()

open(TFLITE_MODEL_PATH, ""wb"").write(tflite_model)

```

**The output from the converter invocation**

```
(tf-n) D:\development\tensorflow\ANPR>python tf_lite_converter.py
2020-01-18 14:20:47.702258: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
Optimizer:    adagrad
Model path:   output\adagrad\glpr-model.h5
2020-01-18 14:20:53.240667: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-01-18 14:20:53.279154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2020-01-18 14:20:53.287304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-01-18 14:20:53.361116: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-01-18 14:20:53.421716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-01-18 14:20:53.447055: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-01-18 14:20:53.530899: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-01-18 14:20:53.570409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-01-18 14:20:53.702842: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-01-18 14:20:53.708873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0
2020-01-18 14:20:53.715765: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-01-18 14:20:53.723989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2020-01-18 14:20:53.734035: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-01-18 14:20:53.739179: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-01-18 14:20:53.744328: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-01-18 14:20:53.749625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-01-18 14:20:53.754539: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-01-18 14:20:53.759888: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-01-18 14:20:53.764633: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-01-18 14:20:53.768758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0
2020-01-18 14:20:56.460164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-18 14:20:56.464829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0
2020-01-18 14:20:56.467340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N
2020-01-18 14:20:56.485286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6302 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.
2020-01-18 14:20:58.024625: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-01-18 14:20:58.029712: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-01-18 14:20:58.039753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2020-01-18 14:20:58.049235: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-01-18 14:20:58.055409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-01-18 14:20:58.061247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-01-18 14:20:58.066364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-01-18 14:20:58.072194: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-01-18 14:20:58.077274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-01-18 14:20:58.081844: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-01-18 14:20:58.086684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0
2020-01-18 14:20:58.090252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-18 14:20:58.094291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0
2020-01-18 14:20:58.097674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N
2020-01-18 14:20:58.101224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6302 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-01-18 14:20:58.210900: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: graph_to_optimize
2020-01-18 14:20:58.215597: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: Graph size after: 402 nodes (0), 510 edges (0), time = 12.053ms.
2020-01-18 14:20:58.223386: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: Graph size after: 402 nodes (0), 510 edges (0), time = 3.914ms.
2020-01-18 14:20:58.228466: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_1_bidirectional_1_forward_gru_1_while_body_1537
2020-01-18 14:20:58.234325: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-18 14:20:58.239123: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-18 14:20:58.243351: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_1_bidirectional_1_forward_gru_1_while_cond_1536
2020-01-18 14:20:58.249016: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-01-18 14:20:58.253361: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-18 14:20:58.257939: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_1_bidirectional_1_backward_gru_1_while_body_1693
2020-01-18 14:20:58.263220: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-18 14:20:58.267683: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-18 14:20:58.271917: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_1_bidirectional_backward_gru_while_body_1380
2020-01-18 14:20:58.277337: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-18 14:20:58.281620: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-01-18 14:20:58.286374: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_1_bidirectional_backward_gru_while_cond_1379
2020-01-18 14:20:58.291542: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-18 14:20:58.296176: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-18 14:20:58.300253: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_1_bidirectional_forward_gru_while_body_1224
2020-01-18 14:20:58.305713: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-18 14:20:58.309768: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-18 14:20:58.314440: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_1_bidirectional_forward_gru_while_cond_1223
2020-01-18 14:20:58.319815: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-18 14:20:58.323996: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-18 14:20:58.328567: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_1_bidirectional_1_backward_gru_1_while_cond_1692
2020-01-18 14:20:58.333697: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-18 14:20:58.338136: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
Traceback (most recent call last):
  File ""tf_lite_converter.py"", line 30, in <module>
    tflite_model = converter.convert()
  File ""C:\Users\Andreas\Anaconda3\lib\site-packages\tensorflow_core\lite\python\lite.py"", line 1051, in convert
    **converter_kwargs)
  File ""C:\Users\Andreas\Anaconda3\lib\site-packages\tensorflow_core\lite\python\convert.py"", line 476, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""C:\Users\Andreas\Anaconda3\lib\site-packages\tensorflow_core\lite\python\convert.py"", line 215, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2020-01-18 14:20:59.719318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-01-18 14:21:01.906381: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:108] Ignored output_format.
2020-01-18 14:21:01.906537: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:111] Ignored drop_control_dependency.
2020-01-18 14:21:02.036186: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-01-18 14:21:02.040149: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-01-18 14:21:02.059114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5
coreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2020-01-18 14:21:02.059504: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-01-18 14:21:02.062864: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-01-18 14:21:02.065500: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-01-18 14:21:02.066696: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-01-18 14:21:02.069723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-01-18 14:21:02.071795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-01-18 14:21:02.076283: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-01-18 14:21:02.076923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0
2020-01-18 14:21:02.582665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-18 14:21:02.582866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0
2020-01-18 14:21:02.582978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N
2020-01-18 14:21:02.583637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6287 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-01-18 14:21:05.868343: E tensorflow/lite/tools/optimize/quantize_weights.cc:474] Quantize weights tool only supports tflite models with one subgraph.
Traceback (most recent call last):
  File ""c:\users\andreas\anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\andreas\anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Andreas\Anaconda3\Scripts\toco_from_protos.exe\__main__.py"", line 7, in <module>
  File ""c:\users\andreas\anaconda3\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""c:\users\andreas\anaconda3\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""c:\users\andreas\anaconda3\lib\site-packages\absl\app.py"", line 299, in run
    _run_main(main, args)
  File ""c:\users\andreas\anaconda3\lib\site-packages\absl\app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""c:\users\andreas\anaconda3\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 56, in execute
    enable_mlir_converter)
Exception: Quantize weights transformation failed.
```

**Also, please include a link to the saved model or GraphDef**

[download keras-model](https://drive.google.com/open?id=1GpQkBigddPvprAdplkr8ksT2nYRiAQNQ)

[download saved_model](https://drive.google.com/open?id=1uEpnjYdWuC9yni9iqS4-Lt-A2sA-l4WB)

**Failure details**

**Any other info / logs**

Without using the GPU optimization I can convert the Keras model to a TFLite model. This works correctly, but under Android unfortunately only very slowly!

[GitHub-Project](https://github.com/aboerzel/ALPR-keras)
"
36015,Some of the operators in the model are not supported by the standard TensorFlow Lite runtime.,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):1.15.0


**Provide the text output from tflite_convert**
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CAST, CONCATENATION, CONV_2D, EXPAND_DIMS, FULLY_CONNECTED, MAX_POOL_2D, RESIZE_BILINEAR, SOFTMAX. Here is a list of operators for which you will need custom implementations: DecodeJpeg.
```
# Copy and paste here
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
36014,[tf1] tf.keras.layers.CudnnGRU cannot be converted and loaded when in subclass of tf.keras.Model,"**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**

Following codes run well in training. But when I inference on CPU, I cannot load model because of [tensorflow/hdf5_format.py at r1.14 · tensorflow/tensorflow · GitHub](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/saving/hdf5_format.py#L373). The code here only works when class is ""Model"" or ""Sequential"" instead of its subclasses. 

I can fix this by change the class Foo's name, but it's not a good idea to hack this. 

```
from tensorflow import keras
class Foo(keras.Model):
  def __init__(self, rnn_dims, use_gpu, **kwargs):
    super().__init__(**kwargs)
    # fixed with self.__class__.__name__ = 'Model'
    if use_gpu:
      self.gru1 = keras.layers.CuDnnGRU(rnn_dims,
                                        return_sequences=True,
                                        return_state=True)
    else:
      self.gru1 = keras.layers.GRU(rnn_dims,
                                        return_sequences=True,
                                        return_state=True,
                                        recurrent_activation='sigmoid',
                                        reset_after=True)    
  def call(self, inputs):
    return self.gru1(inputs)

def create_model(rnn_dims, use_gpu):
  foo = Foo(rnn_dims, use_gpu)
  input_layer = keras.Input(shape=(None, rnn_dims))
  output = foo(input_layer)
  # actually there are some other calculations, so I don't want to write everything in Foo here.
  return keras.Model([input_layer], [output])

# Cannot load model
model = create_model(rnn_dims, False)
model.load_weights('path_to_model')
```




**Will this change the current api? How?**
No
**Who will benefit with this feature?**

People use tensorflow 1.1x versions along with tf.keras.layers.CudnnGRU in subclass of tf.keras.Model. 

**Any Other info.**
I know tf2 has a tf.keras.layers.GRU compatibale with GRU and CudnnGRU，but it's a long way to upgrade for now for me.
"
36010,Custom layer go_backwards does not work,"I am implementing a custom recurrent class that inherits tf.layers.Layer, when using the Bidirectional wrapper I get the error:

```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-3-7bd5b5269810> in <module>
----> 1 a = TimeDistributed(Bidirectional(char_recurrent_cell))

~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py in __init__(self, layer, merge_mode, weights, backward_layer, **kwargs)
    434     if backward_layer is None:
    435       self.backward_layer = self._recreate_layer_from_config(
--> 436           layer, go_backwards=True)
    437     else:
    438       self.backward_layer = backward_layer

~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py in _recreate_layer_from_config(self, layer, go_backwards)
    493     config = layer.get_config()
    494     if go_backwards:
--> 495       config['go_backwards'] = not config['go_backwards']
    496     if 'custom_objects' in tf_inspect.getfullargspec(
    497         layer.__class__.from_config).args:

KeyError: 'go_backwards'
```
This is the code for the layer itself:

```
class RecurrentConfig(BaseLayer):
    '''Basic configurable recurrent layer'''
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.layers: List[layers.Layer] = stack_layers(self.params,
                                                       self.num_layers,
                                                       self.layer_name)

    def call(self, inputs: np.ndarray) -> layers.Layer:
        '''This function is a sequential/functional call to this layers logic
        Args:
            inputs: Array to be processed within this layer
        Returns:
            inputs processed through this layer'''
        processed = inputs
        for layer in self.layers:
            processed = layer(processed)
        return processed

    @staticmethod
    def default_params() -> Dict[Any, Any]:
        return{
            'units': 32,
            'recurrent_initializer': 'glorot_uniform',
            'dropout': 0,
            'recurrent_dropout': 0,
            'activation': None,
            'return_sequences': True
        }
```

I have attempted to add the go_backwards to the config that is retrieved when get_config() is called but this results in another error:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-3-7bd5b5269810> in <module>
----> 1 a = TimeDistributed(Bidirectional(char_recurrent_cell))

~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py in __init__(self, layer, merge_mode, weights, backward_layer, **kwargs)
    430     # Recreate the forward layer from the original layer config, so that it will
    431     # not carry over any state from the layer.
--> 432     self.forward_layer = self._recreate_layer_from_config(layer)
    433 
    434     if backward_layer is None:

~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py in _recreate_layer_from_config(self, layer, go_backwards)
    506       return layer.__class__.from_config(config, custom_objects=custom_objects)
    507     else:
--> 508       return layer.__class__.from_config(config)
    509 
    510   @tf_utils.shape_type_conversion

~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in from_config(cls, config)
    517         A layer instance.
    518     """"""
--> 519     return cls(**config)
    520 
    521   def compute_output_shape(self, input_shape):

~/nlpv3-general/nlp-lib/src/main/python/mosaix_py/mosaix_learn/layers/recurrent_layers.py in __init__(self, *args, **kwargs)
     12     '''Basic configurable recurrent layer'''
     13     def __init__(self, *args, **kwargs):
---> 14         super().__init__(*args, **kwargs)
     15         self.layers: List[layers.Layer] = stack_layers(self.params,
     16                                                        self.num_layers,

~/nlpv3-general/nlp-lib/src/main/python/mosaix_py/mosaix_learn/layers/base_layer.py in __init__(self, params, mode, layer_name, num_layers, cust_name, **kwargs)
     17                  cust_name: str = '',
     18                  **kwargs):
---> 19         super().__init__(params, mode, **kwargs)
     20         self.layer_name = layer_name
     21         self.cust_name = cust_name

~/nlpv3-general/nlp-lib/src/main/python/mosaix_py/mosaix_learn/configurable.py in __init__(self, params, mode, **kwargs)
     61 
     62     def __init__(self, params: Dict[AnyStr, Any], mode: ModeKeys, **kwargs):
---> 63         super().__init__(**kwargs) #type: ignore
     64         self._params = _parse_params(params, self.default_params())
     65         self._mode = mode

~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __init__(self, trainable, name, dtype, dynamic, **kwargs)
    184     }
    185     # Validate optional keyword arguments.
--> 186     generic_utils.validate_kwargs(kwargs, allowed_kwargs)
    187 
    188     # Mutable properties

~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py in validate_kwargs(kwargs, allowed_kwargs, error_message)
    716   for kwarg in kwargs:
    717     if kwarg not in allowed_kwargs:
--> 718       raise TypeError(error_message, kwarg)

TypeError: ('Keyword argument not understood:', 'go_backwards')
```

Version info is:
tf_version: '2.1.0-dev20191125'
git_version: 'v1.12.1-19144-gf39f4ea3fa'"
36008,XLA drops performance across the nodes,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): TF benchmarks
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): Running with Horovod 0.18.2 via docker
- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5
- Python version: 2.7
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): 4.8
- CUDA/cuDNN version: 10.0 / 7.6.5
- GPU model and memory: 8xV100-16GB (2 nodes, each with 4xV100GPU's)
- MPI version: 4.0.0
- Mellanox OFED version: 4.7-3.2.9.0
- GPUDirect RDMA - nvidia-peer-memory version: 1.0-8

**Describe the current behavior**
I am running TF benchmarks with Horovod in distributed mode, I see the scaling efficiency drops to from 76% (within the node) to 98% (when crossing the nodes), see below the throughput:

-1xGPU within the node total images/sec:     ~1200
-4xGPU's within the node total images/sec:   ~4712  | Scaling Efficiency (1200x4) / 4712 = 98%
-8xGPU's across the nodes total images/sec: ~7302 |  Scaling Efficiency (1200x8) / 7302 = 76%

**Describe the expected behavior**
We are expecting scaling efficiency across the nodes over 95%, it is around ~9120 img/sec. Please notice that the scaling efficiency drops only when crossing the nodes

**Code to reproduce the issue**
`mpirun -np 8 -H <ib>:4,<ib>:4 --allow-run-as-root -x NCCL_NET_GDR_LEVEL=3 -x NCCL_DEBUG_SUBSYS=NET -x NCCL_IB_DISABLE=0 -mca btl_tcp_if_include ib0 -x NCCL_SOCKET_IFNAME=ib0 -x NCCL_DEBUG=INFO -x HOROVOD_MPI_THREADS_DISABLE=1 --bind-to none --map-by slot --mca plm_rsh_args ""-p 12345"" python tf_cnn_benchmarks.py --data_format=NCHW --distortions=false --use_fp16=True --variable_update=horovod --model=resnet50 --batch_size=128 --xla=true`

**Other info / logs**

**Tracelog 1xGPU:**
```
Step    Img/sec total_loss
1       images/sec: 1187.6 +/- 0.0 (jitter = 0.0)       7.817
10      images/sec: 1201.3 +/- 3.4 (jitter = 9.5)       7.869
20      images/sec: 1200.5 +/- 2.5 (jitter = 6.3)       7.915
30      images/sec: 1202.0 +/- 3.3 (jitter = 8.5)       7.884
40      images/sec: 1202.0 +/- 2.7 (jitter = 7.0)       7.836
50      images/sec: 1200.3 +/- 2.2 (jitter = 6.4)       7.806
60      images/sec: 1201.2 +/- 2.0 (jitter = 6.4)       7.852
70      images/sec: 1202.1 +/- 2.0 (jitter = 7.5)       7.856
80      images/sec: 1201.1 +/- 1.9 (jitter = 7.2)       7.754
90      images/sec: 1200.5 +/- 1.7 (jitter = 6.8)       7.806
100     images/sec: 1200.7 +/- 1.6 (jitter = 6.7)       7.764
----------------------------------------------------------------
total images/sec: 1199.87
----------------------------------------------------------------
```


**Tracelog 8xGPU's:**
```
Step    Img/sec total_loss
1       images/sec: 922.9 +/- 0.0 (jitter = 0.0)        7.787
1       images/sec: 926.8 +/- 0.0 (jitter = 0.0)        7.707
1       images/sec: 927.0 +/- 0.0 (jitter = 0.0)        7.720
1       images/sec: 925.4 +/- 0.0 (jitter = 0.0)        7.881
1       images/sec: 922.4 +/- 0.0 (jitter = 0.0)        7.846
1       images/sec: 926.3 +/- 0.0 (jitter = 0.0)        7.694
1       images/sec: 924.9 +/- 0.0 (jitter = 0.0)        7.683
1       images/sec: 902.0 +/- 0.0 (jitter = 0.0)        7.762
10      images/sec: 926.2 +/- 7.6 (jitter = 25.6)       7.725
10      images/sec: 926.6 +/- 8.5 (jitter = 18.2)       7.647
10      images/sec: 926.6 +/- 7.6 (jitter = 26.9)       7.659
10      images/sec: 926.7 +/- 7.4 (jitter = 26.2)       7.703
10      images/sec: 927.9 +/- 8.2 (jitter = 8.3)        7.613
10      images/sec: 927.2 +/- 7.9 (jitter = 23.9)       7.708
10      images/sec: 927.6 +/- 7.0 (jitter = 11.8)       7.710
10      images/sec: 927.2 +/- 7.4 (jitter = 24.8)       7.669
20      images/sec: 925.7 +/- 4.5 (jitter = 20.9)       7.620
20      images/sec: 926.0 +/- 4.5 (jitter = 21.1)       7.585
20      images/sec: 926.0 +/- 5.1 (jitter = 18.1)       7.530
20      images/sec: 926.0 +/- 4.7 (jitter = 21.0)       7.600
20      images/sec: 926.8 +/- 4.5 (jitter = 17.3)       7.524
20      images/sec: 926.9 +/- 4.9 (jitter = 14.5)       7.638
20      images/sec: 926.6 +/- 4.4 (jitter = 21.8)       7.588
20      images/sec: 926.5 +/- 5.1 (jitter = 19.2)       7.682
30      images/sec: 919.9 +/- 4.1 (jitter = 25.1)       7.567
30      images/sec: 920.1 +/- 4.4 (jitter = 24.5)       7.529
30      images/sec: 920.1 +/- 4.1 (jitter = 24.0)       7.546
30      images/sec: 920.0 +/- 4.1 (jitter = 24.5)       7.578
30      images/sec: 920.5 +/- 4.4 (jitter = 23.7)       7.469
30      images/sec: 920.5 +/- 4.2 (jitter = 24.9)       7.483
30      images/sec: 920.7 +/- 4.2 (jitter = 18.8)       7.489
30      images/sec: 920.6 +/- 4.3 (jitter = 25.2)       7.526
40      images/sec: 916.3 +/- 3.9 (jitter = 25.3)       7.541
40      images/sec: 916.2 +/- 3.9 (jitter = 23.2)       7.444
40      images/sec: 916.2 +/- 4.1 (jitter = 25.1)       7.515
40      images/sec: 915.9 +/- 4.0 (jitter = 26.4)       7.476
40      images/sec: 917.0 +/- 4.1 (jitter = 26.4)       7.534
40      images/sec: 916.8 +/- 4.1 (jitter = 27.9)       7.502
40      images/sec: 916.7 +/- 4.0 (jitter = 26.5)       7.500
40      images/sec: 916.5 +/- 4.3 (jitter = 23.3)       7.529
50      images/sec: 911.2 +/- 3.7 (jitter = 24.8)       7.523
50      images/sec: 911.2 +/- 3.8 (jitter = 31.4)       7.461
50      images/sec: 911.1 +/- 3.7 (jitter = 27.8)       7.461
50      images/sec: 911.6 +/- 3.9 (jitter = 29.1)       7.524
50      images/sec: 911.7 +/- 3.8 (jitter = 24.2)       7.512
50      images/sec: 911.7 +/- 3.9 (jitter = 24.0)       7.419
50      images/sec: 910.9 +/- 3.7 (jitter = 27.9)       7.446
50      images/sec: 911.8 +/- 3.8 (jitter = 27.6)       7.467
60      images/sec: 910.2 +/- 3.5 (jitter = 29.9)       7.513
60      images/sec: 910.1 +/- 3.4 (jitter = 29.8)       7.435
60      images/sec: 910.2 +/- 3.5 (jitter = 23.2)       7.478
60      images/sec: 910.2 +/- 3.4 (jitter = 31.2)       7.463
60      images/sec: 910.7 +/- 3.5 (jitter = 26.3)       7.487
60      images/sec: 910.5 +/- 3.6 (jitter = 28.8)       7.436
60      images/sec: 910.6 +/- 3.6 (jitter = 26.5)       7.408
60      images/sec: 910.6 +/- 3.5 (jitter = 27.9)       7.446
70      images/sec: 913.4 +/- 3.2 (jitter = 26.4)       7.409
70      images/sec: 913.3 +/- 3.2 (jitter = 28.5)       7.441
70      images/sec: 913.4 +/- 3.2 (jitter = 27.8)       7.459
70      images/sec: 913.4 +/- 3.3 (jitter = 24.5)       7.440
70      images/sec: 913.8 +/- 3.4 (jitter = 27.0)       7.459
70      images/sec: 913.8 +/- 3.3 (jitter = 26.4)       7.483
70      images/sec: 913.9 +/- 3.2 (jitter = 28.7)       7.385
70      images/sec: 913.6 +/- 3.3 (jitter = 26.3)       7.445
80      images/sec: 915.7 +/- 3.0 (jitter = 30.2)       7.407
80      images/sec: 915.8 +/- 3.0 (jitter = 26.0)       7.407
80      images/sec: 915.8 +/- 3.0 (jitter = 25.1)       7.468
80      images/sec: 915.8 +/- 3.0 (jitter = 25.3)       7.472
80      images/sec: 916.2 +/- 3.0 (jitter = 27.4)       7.467
80      images/sec: 916.3 +/- 3.1 (jitter = 27.9)       7.443
80      images/sec: 916.1 +/- 3.1 (jitter = 27.7)       7.491
80      images/sec: 915.4 +/- 3.2 (jitter = 24.0)       7.553
90      images/sec: 914.7 +/- 2.8 (jitter = 26.5)       7.404
90      images/sec: 914.6 +/- 2.8 (jitter = 28.5)       7.408
90      images/sec: 914.7 +/- 2.8 (jitter = 25.3)       7.457
90      images/sec: 914.7 +/- 2.9 (jitter = 25.1)       7.405
90      images/sec: 915.0 +/- 2.9 (jitter = 24.0)       7.437
90      images/sec: 915.0 +/- 2.8 (jitter = 27.7)       7.504
90      images/sec: 915.0 +/- 3.0 (jitter = 28.1)       7.431
90      images/sec: 915.1 +/- 2.9 (jitter = 26.5)       7.430
100     images/sec: 913.2 +/- 2.6 (jitter = 26.9)       7.890
----------------------------------------------------------------
total images/sec: 7301.54
----------------------------------------------------------------
```



"
36006,keras model training - total loss discrepancy with multiple outputs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 31
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): v2.1.0-1-ga9af83a149 2.1.0
- Python version: 3.7.5
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source): 8.3.1
- CUDA/cuDNN version: 10.2.89 / 7.6.5.33
- GPU model and memory: Nvidia GeForce GTX 1070 TI 8GB

**Describe the current behavior**
While training a model with multiple outputs via keras, the `loss` and `val_loss` values do not agree with the documented definition. The documentation states:

> The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients.

However as can be seen by the following screencap of tensorboard, this does not match up:
![image](https://user-images.githubusercontent.com/1826947/72649484-d2c9ea80-394b-11ea-8f44-818ff8b526f3.png)


And the verbose training log output agrees with the tensorboard charts:
```
Epoch 1/30
37/37 [==============================] - 3s 74ms/step - loss: 776.4654 - out_0_loss: 0.0281 - out_1_loss: 0.0189 - out_2_loss: 0.0310 - out_3_loss: 0.0319 - out_4_loss: 0.0525 - out_5_loss: 0.0643 - out_0_mean_squared_error: 0.0281 - out_1_mean_squared_error: 0.0189 - out_2_mean_squared_error: 0.0310 - out_3_mean_squared_error: 0.0319 - out_4_mean_squared_error: 0.0525 - out_5_mean_squared_error: 0.0643 - val_loss: 121.1278 - val_out_0_loss: 0.0116 - val_out_1_loss: 0.0119 - val_out_2_loss: 0.0110 - val_out_3_loss: 0.0136 - val_out_4_loss: 0.0186 - val_out_5_loss: 0.0273 - val_out_0_mean_squared_error: 0.0116 - val_out_1_mean_squared_error: 0.0119 - val_out_2_mean_squared_error: 0.0110 - val_out_3_mean_squared_error: 0.0136 - val_out_4_mean_squared_error: 0.0186 - val_out_5_mean_squared_error: 0.0273

Epoch 2/30
37/37 [==============================] - 2s 45ms/step - loss: 57.5657 - out_0_loss: 0.0251 - out_1_loss: 0.0237 - out_2_loss: 0.0244 - out_3_loss: 0.0311 - out_4_loss: 0.0361 - out_5_loss: 0.0479 - out_0_mean_squared_error: 0.0251 - out_1_mean_squared_error: 0.0237 - out_2_mean_squared_error: 0.0244 - out_3_mean_squared_error: 0.0311 - out_4_mean_squared_error: 0.0361 - out_5_mean_squared_error: 0.0479 - val_loss: 33.7718 - val_out_0_loss: 0.0241 - val_out_1_loss: 0.0222 - val_out_2_loss: 0.0226 - val_out_3_loss: 0.0290 - val_out_4_loss: 0.0349 - val_out_5_loss: 0.0447 - val_out_0_mean_squared_error: 0.0241 - val_out_1_mean_squared_error: 0.0222 - val_out_2_mean_squared_error: 0.0226 - val_out_3_mean_squared_error: 0.0290 - val_out_4_mean_squared_error: 0.0349 - val_out_5_mean_squared_error: 0.0447
```

And since I am using weighted values, the weights are:
```
[1.0, 1.0, 0.8, 0.6, 0.45, 0.3]
```

I'm not using weighted samples or anything. The weights are only defined within `model.compile()`.
The data is fed as a `tf.data.Dataset`, where the shape of the training & validation data is:
```
(TensorSpec(shape=(None, 3, 19546), dtype=tf.float32, name=None), (TensorSpec(shape=(None,), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None)))
```

**Describe the expected behavior**
For epoch 1:
```
weights = np.array([1.0, 1.0, 0.8, 0.6, 0.45, 0.3])
losses = np.array([0.0281, 0.0189, 0.0310, 0.0319, 0.0525, 0.0643])
val_losses = np.array([0.0116, 0.0119, 0.0110, 0.0136, 0.0186, 0.0273])

loss = np.sum(losses * weights)
0.133855

val_loss = np.sum(val_losses * weights)
0.05702
```

**Code to reproduce the issue**
Have been trying to reproduce with a simple example I can share, but am having difficulty. The little bit of info I can provide is how the model was compiled, and fit:
```
optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)
model.compile(loss='mean_squared_error', loss_weights=[1.0,1.0,0.8,0.6,0.45,0.3], optimizer=optimizer, metrics=['mean_squared_error'])
model.fit(
        dataset_train,
        epochs=30,
        validation_data=dataset_validation,
        callbacks=[
            tf.keras.callbacks.LearningRateScheduler(lambda i: 0.0005 * 10 ** -(i/10)),
            keras.callbacks.TensorBoard(log_dir=tensorboard_path, profile_batch=0),
        ],
    )
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
36005,Compiling tf.keras.Model with hub.KerasLayer fails in distributed scope,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Debian 9.11 (Google Cloud tf2-latest-gpu image)
- TensorFlow installed from: Preinstalled on Google Cloud image
- TensorFlow version: v2.1.0-rc2-17-ge5bf8de
- Python version: 3.5.3
- CUDA/cuDNN version: N/A
- GPU model and memory: None (Using Google Cloud TPU v3-8)

**Describe the current behavior**
When compiling a tf.keras.Model that includes a hub.KerasLayer (tensorflow hub), it fails to compile in a distribution strategy scope:

```
ValueError                                Traceback (most recent call last)
<ipython-input-23-7b06371889c9> in <module>
     16         optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
     17         loss=tf.keras.losses.binary_crossentropy,
---> 18         metrics=[""accuracy""]
     19     )

1 frames
/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)
    469                 'with strategy.scope():\n'
    470                 '  model=_create_model()\n'
--> 471                 '  model.compile(...)'% (v, strategy))
    472 
    473   @trackable.no_automatic_dependency_tracking

ValueError: Variable (<tf.Variable 'bert/embeddings/word_embeddings:0' shape=(119547, 768) dtype=float32>) was not created in the distribution strategy scope of (<tensorflow.python.distribute.tpu_strategy.TPUStrategy object at 0x7f4df23cddd8>). It is most likely due to not all layers or the model or optimizer being created outside the distribution strategy scope. Try to make sure your code looks similar to the following.
with strategy.scope():
  model=_create_model()
  model.compile(...)
```

**Describe the expected behavior**
Model should be able to compile.

**Code to reproduce the issue**
Code used to create scope:
```python3
TPU_ADDRESS = ""grpc://"" + ""10.0.0.2:8470""

with tf.compat.v1.Session(TPU_ADDRESS) as session:
    print('TPU devices:')
    pprint.pprint(session.list_devices())

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)
try:
    tf.config.experimental_connect_to_cluster(resolver)
except tf.errors.UnimplementedError as uie:
    print(uie, ""This appears to be caused by the TPU already being connected. Ignoring."", sep='\n')
tf.tpu.experimental.initialize_tpu_system(resolver)
tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)
```

Code used to compile model:
```python3
with tpu_strategy.scope():
    in_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=""input_ids"", dtype=np.int32)
    in_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=""input_masks"", dtype=np.int32)
    in_segment = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=""segment_ids"", dtype=np.int32)
    bert_inputs = {""input_ids"": in_id, ""input_mask"": in_mask, ""segment_ids"": in_segment}

    bert_layer = hub.KerasLayer(BERT_MODEL_HUB, signature=""tokens"", output_key=""pooled_output"")(bert_inputs)
    bert_layer.trainable = True

    dense = tf.keras.layers.Dense(256, activation='relu')(bert_layer)
    pred = tf.keras.layers.Dense(len(unique_labels), activation='sigmoid')(dense)

    model = tf.keras.Model(inputs=bert_inputs, outputs=pred)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss=tf.keras.losses.binary_crossentropy,
        metrics=[""accuracy""]
    )
```

**Other info / logs**
Previously opened issue with TF Hub here: https://github.com/tensorflow/hub/issues/469

Google Cloud TPU v3-8 is running TPU Software 2.1
"
36004,`python': double free or corruption,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
**Scientific Linux 7.7 (Nitrogen) 
Linux 3.10.0-1062.9.1.el7.x86_64** 
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below):
**v1.15.0-rc3-22-g590d6ee 1.15.0**
Same problem occurs with tf 1.14
- Python version:
**3.6**
- Bazel version (if compiling from source):
**NA**
- GCC/Compiler version (if compiling from source):
**4.8.5**
- CUDA/cuDNN version:
**NA, running on CPU**
- GPU model and memory:
**NA**

**Describe the current behavior**

I am using PBS to submit jobs to a homogeneous cluster. Each job is allocated up to 192GB memory, and the job itself uses only 50GB at peak. Each job is a loop that iteratively solves around 1000 Gaussian process regression problems using TF. In pseudo-code it is,
``` python
def job():
  N=1000
  for i in range(N):
    with tf.Session(graph=tf.Graph()) as sess:
      solve_gp_problem(sess, sess.graph)
      # around i ~ 300 we get `python': double free or corruption
      # but only in ~50% of the jobs that are submitted (with the exact same data)
```
Each Gaussian process problem creates its own graph and session, and lasts approximately 10 seconds. Then the graph and session are not referenced again (and should be garbage collected). The job uses no multiprocessing or threading outside of TF's inherent use. 

Approximately 50% of my jobs are abruptly failing with ``python': double free or corruption` and a long stack trace, after approximately 1/3 of the work is done. This happens also when I give the jobs the exact same input data.

**Describe the expected behavior**

Some memory is trying to be freed twice in tensorflow, which should not happen. 

**Code to reproduce the issue**
This error cannot be deterministically reproduced since it doesn't occur all of the time. However, it looks like this,
``` python
import tensorflow.compat.v1 as tf
def loss(x):
  return tf.reduce_sum(x ** 2)

def loss_and_grad(x):
  l = loss(x)
  return l, tf.gradients(l, [x])[0]

def build_loss_np(sess):
  x_pl = tf.placeholder(tf.float32)
  l_g = loss_and_grad(x_pl)
  def _loss(x):
    return sess.run(l_g, {x_pl: x})
  return _loss

from scipy.optimize import minimize
for i in range(1000):
  with tf.Session(graph=tf.Graph()) as sess:
    loss_func = build_loss_np(sess)
    # stand-in for GP hyper parameter optimisation and regression
    res = minimize(loss_func, [0], method='BFGS', jac=True)
```

**Other info / logs**
The only consistent thing I notice is that the failure usually occurs around the same point after about 300 loops. Therefore there would be 300 graphs and sessions created.
The code that performs the gaussian process regression is a solid code base called GPFlow.
[traceback.log](https://github.com/tensorflow/tensorflow/files/4079155/traceback.log)
"
36003,ImportError: DLL Load failed on tensorflow 2.1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (Windows10 64 bit):

- TensorFlow installed from (source or binary):pip
- TensorFlow version:2.1
- Python version:3.7
- Installed using virtualenv? pip? conda?: pip

- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: I have cuda runtime 8.0 and 10.1 (don't see cuDNN installed in the list, but at one point I could run tensorflow. Not sure what broke since that was months ago that I last needed it.
- GPU model and memory:







**Describe the problem**
Encountered the problem using Anaconda to create my environment as well. Anaconda has been uninstalled, so these steps were to try and fix the issue by starting at ground 0.

I installed Python 3.7 from the python.org site (after uninstalling Anaconda which also had this issue)
Used command prompt to create virtual env using python -m venv name
pip install packagename one line per package in this order pandas, numpy, wheel, six, scikit-learn, tensorflow
also pip is version 19.3
So now my virutal env should be ready to go and I activated it.
pip list provides:
(envNoShow) C:\Users\USERNAME\AllProjects\noShow>pip list
Package              Version
-------------------- ----------
absl-py              0.9.0
astor                0.8.1
cachetools           4.0.0
certifi              2019.11.28
chardet              3.0.4
gast                 0.2.2
google-auth          1.10.1
google-auth-oauthlib 0.4.1
google-pasta         0.1.8
grpcio               1.26.0
h5py                 2.10.0
idna                 2.8
joblib               0.14.1
Keras-Applications   1.0.8
Keras-Preprocessing  1.1.0
Markdown             3.1.1
numpy                1.18.1
oauthlib             3.1.0
opt-einsum           3.1.0
pandas               0.25.3
pip                  19.3.1
protobuf             3.11.2
pyasn1               0.4.8
pyasn1-modules       0.2.8
python-dateutil      2.8.1
pytz                 2019.3
requests             2.22.0
requests-oauthlib    1.3.0
rsa                  4.0
scikit-learn         0.22.1
scipy                1.4.1
setuptools           41.2.0
six                  1.14.0
tensorboard          2.1.0
tensorflow           2.1.0
tensorflow-estimator 2.1.0
termcolor            1.1.0
urllib3              1.25.7
virtualenv           16.7.9
Werkzeug             0.16.0
wheel                0.33.6
wrapt                1.11.2



ran the command python -c ""import tensorflow as tf"" and get error in the log file.





**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
(envNoShow) C:\Users\USERNAME\AllProjects\noShow>python -c ""import tensorflow as tf""
Traceback (most recent call last):
  File ""C:\Users\USERNAME\AllProjects\noShow\envNoShow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\USERNAME\AllProjects\noShow\envNoShow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\USERNAME\AllProjects\noShow\envNoShow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\USERNAME\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\USERNAME\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\USERNAME\AllProjects\noShow\envNoShow\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\USERNAME\AllProjects\noShow\envNoShow\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\USERNAME\AllProjects\noShow\envNoShow\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\USERNAME\AllProjects\noShow\envNoShow\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\USERNAME\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\USERNAME\AllProjects\noShow\envNoShow\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\USERNAME\AllProjects\noShow\envNoShow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\USERNAME\AllProjects\noShow\envNoShow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\USERNAME\AllProjects\noShow\envNoShow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\USERNAME\AllProjects\noShow\envNoShow\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\USERNAME\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\USERNAME\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
36001,Allow Optimizers to perform global gradient clipping,"**System information**
- TensorFlow version: 2.1
- Are you willing to contribute it: Yes

**Describe the feature and the current behavior/state.**

Currently, passing `clipnorm` to a `tf.keras.optimizers.Optimizer` makes it clip the gradient for each weight tensor locally, or *independently of other weight gradients*:

https://github.com/tensorflow/tensorflow/blob/d8bcf5e4a24f9904e9a97b41da393e53149e999e/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L357-L358

I am proposing clipping the weight gradient *globally*, i.e.,

```python
    if hasattr(self, 'clipnorm'):
      grads, _ = clip_ops.clip_by_global_norm(grads, self.clipnorm)
```

This was already proposed in #29108 and #29114.

This change has at least two benefits:

1. Theoretical: the per-batch descent direction is preserved when gradients are clipped globally, but not when clipped locally.
2. Practical: [standalone Keras implements global gradient clipping](https://github.com/keras-team/keras/blob/7a39b6c62d43c25472b2c2476bd2a8983ae4f682/keras/optimizers.py#L98-L100):
    ```python
        if hasattr(self, 'clipnorm') and self.clipnorm > 0:
            norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))
            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]
    ```
    so this change will unify `keras` and `tf.keras` behavior.

In fact, clipping by global norm is the behavior I *expected* when I first learned about the `clipnorm` parameter. The documentation does not clarify whether gradients are clipped locally or globally:

https://github.com/tensorflow/tensorflow/blob/d8bcf5e4a24f9904e9a97b41da393e53149e999e/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L241-L245

**Will this change the current api? How?**

The API will remain the same. Only the weight update behavior will change when the user passes `clipnorm` to an optimizer.

**Who will benefit with this feature?**

Users who use `clipnorm` with their optimizers.

**Any Other info.**

One of the first papers introducing gradient clipping by norm is [Pascanu, Mikolov, Bengio (2013)](http://proceedings.mlr.press/v28/pascanu13.html), where they propose
![Screen Shot 2020-01-17 at 12 28 38 PM](https://user-images.githubusercontent.com/12015989/72644067-f299e800-3924-11ea-8a27-2c4d48880e52.png)

In Section 1.1, θ is defined to be the collection of *all* weights in the model, not just a particular weight matrix or bias vector. This addresses [a question](https://github.com/tensorflow/tensorflow/pull/29114/files#r294862824
) In #29114 about whether the paper proposes global clipping or local clipping. Moreover, following Algorithm 1, Pascanu *et al* write,

> we only diverged from the original proposal in an attempt to provide a better theoretical justification (see section 2.3; we also move in a descent direction for the current mini-batch)

so they appear to be advocating altering the gradient in a way that preserves the descent direction (i.e., using global clipping).

Other recent sources also appear to suggest *global* gradient clipping (e.g., see equation (5) of [this ICLR 2020 paper](https://openreview.net/forum?id=BJgnXpVYwS))."
36000,Keras Hub Layer Does Not Work with Functional API. Throws Exception.,"TensorFlow Version: 2.1
Python Version: 3.76
OS: Windows 10
Issue:
Keras Hub Layer Does Not Work with Functional API. Throws Exception. Unable to load IMDB dataset. See below code and exception:

```
import tensorflow as tf
from tensorflow.keras.layers import Dense, Concatenate, RepeatVector, Activation, Dot, Bidirectional, Flatten, Embedding, Input, SpatialDropout1D, LSTM, Dropout, Lambda, Conv2D, Conv1D, Attention, AdditiveAttention, GlobalAveragePooling1D, TimeDistributed, AveragePooling1D
from tensorflow.keras.models import Model
from tensorflow.keras.backend import backend
import tensorflow_hub as hub
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import tensorflow_datasets as tfds

def cnn_classifier():
    # Encode each timestep
    # embedding = Embedding(10000, 300, trainable=True)(input)

    embedding = ""https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1""
    input = Input(shape=(None,), name=""Input"")
    hub_layer = hub.KerasLayer(embedding, trainable=True)(input)
    cnn = Conv1D(64,3, padding=""same"", activation=""relu"")(hub_layer)
    cnn = Conv1D(32, 3, padding=""same"", activation=""relu"")(cnn)
    cnn = Conv1D(16, 3, padding=""same"", activation=""relu"")(cnn)
    cnn = Flatten()(cnn)
    output = Dense(1, activation=""sigmoid"")(cnn)

    model = Model(input, output)
    model.compile(loss=""binary_crossentropy"",
                  optimizer=""adam"",
                  metrics=[""accuracy""])
    model.summary()
    return model

model = cnn_classifier()

train_validation_split = tfds.Split.TRAIN.subsplit([8, 2])

(train_data, validation_data), test_data = tfds.load(
    name=""imdb_reviews"",
    split=(train_validation_split, tfds.Split.TEST),
    as_supervised=True)

#train_data = pad_sequences(train_data, maxlen=1100, dtype='int32', padding='post', truncating='post', value=0.0)
#test_data = pad_sequences(test_data, maxlen=1100, dtype='int32', padding='post', truncating='post', value=0.0)

model.fit(train_data.shuffle(25000).batch(512),
                    epochs=20,
                    validation_data=validation_data.batch(512),
                    verbose=10)
```
**Exception:**
```
2020-01-17 15:46:42.273900: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:AutoGraph could not transform <tensorflow.python.saved_model.function_deserialization.RestoredFunction object at 0x0000029880A70D88> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Python inputs incompatible with input_signature:
  inputs: (
    Tensor(""Input:0"", shape=(None, None), dtype=float32))
  input_signature: (
    TensorSpec(shape=(None,), dtype=tf.string, name=None))
WARNING:tensorflow:AutoGraph could not transform <tensorflow.python.saved_model.function_deserialization.RestoredFunction object at 0x0000029880A70D88> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Python inputs incompatible with input_signature:
  inputs: (
    Tensor(""Input:0"", shape=(None, None), dtype=float32))
  input_signature: (
    TensorSpec(shape=(None,), dtype=tf.string, name=None))
Traceback (most recent call last):
  File ""C:/Development/Projects/TensorFlow_2.0_Hierarchical_Attention/Classifiers.py"", line 47, in <module>
    model = cnn_classifier()
  File ""C:/Development/Projects/TensorFlow_2.0_Hierarchical_Attention/Classifiers.py"", line 17, in cnn_classifier
    hub_layer = hub.KerasLayer(embedding, trainable=True)(input)
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 773, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\autograph\impl\api.py"", line 237, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in converted code:

    C:\Development\Python\Python37\lib\site-packages\tensorflow_hub\keras_layer.py:209 call  *
        result = f()
    C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\saved_model\load.py:438 _call_attribute
        return instance.__call__(*args, **kwargs)
    C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\eager\def_function.py:568 __call__
        result = self._call(*args, **kwds)
    C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\eager\def_function.py:606 _call
        results = self._stateful_fn(*args, **kwds)
    C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py:2362 __call__
        graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
    C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py:2661 _maybe_define_function
        *args, **kwargs)
    C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py:2185 canonicalize_function_inputs
        self._flat_input_signature)
    C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py:2252 _convert_inputs_to_signature
        format_error_message(inputs, input_signature))

    ValueError: Python inputs incompatible with input_signature:
      inputs: (
        Tensor(""Input:0"", shape=(None, None), dtype=float32))
      input_signature: (
        TensorSpec(shape=(None,), dtype=tf.string, name=None))


Process finished with exit code 1
```

<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35995,The summary method could be changed for improved understandability and readability,"**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): Maybe

**Describe the feature and the current behavior/state.**

Consider the following program

```
import tensorflow as tf
import tensorflow_probability as tfp


def get_model():
    inp = tf.keras.layers.Input(shape=(1,))
    x = tfp.layers.DenseFlipout(8)(inp)
    x = tfp.layers.DenseFlipout(16)(x)
    out = tfp.layers.DenseFlipout(1)(x)
    model = tf.keras.Model(inputs=inp, outputs=out)
    model.summary()
    return model


if __name__ == '__main__':
    get_model()
```

which produces the following output

```
Model: ""model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
dense_flipout (DenseFlipout) (None, 8)                 24        
_________________________________________________________________
dense_flipout_1 (DenseFlipou (None, 16)                272       
_________________________________________________________________
dense_flipout_2 (DenseFlipou (None, 1)                 33        
=================================================================
Total params: 329
Trainable params: 329
Non-trainable params: 0
_________________________________________________________________
```

The first layer has 24 parameters because there are 8 means, 8 standard deviations and 8 biases, for a total of `8+8+8=24`. Similarly, the second contains 272 parameters because there are `8*16` means 
 `8*16` standard deviations and 16 biases. And the last layer contains 16 means, 16 standard deviations and 1 biases.  Now, in this case, I have domain knowledge and I was able to understand these numbers, but, in certain cases, for example, when Bayesian convolution layers are used, the number of parameters of certain layers may not be obvious. 

Therefore, if possible, it would be nice that `my_model.summary()` provided a more detailed description of the number of parameters for each layer (e.g. the actual calculation with a description).  This feature may be implemented by providing a `verbose` parameter for the `summary` method, which allows us to set the verbosity level of the output.

Furthermore, there are cases where the output is also cut. For example, consider the following output of the `summary` method

```
Model: ""model""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
camera (InputLayer)             [(None, 64, 80, 3)]  0                                            
__________________________________________________________________________________________________
conv2d_flipout (Conv2DFlipout)  (None, 64, 80, 8)    441         camera[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 64, 80, 8)    0           conv2d_flipout[0][0]             
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 32, 40, 8)    0           activation[0][0]                 
__________________________________________________________________________________________________
flatten (Flatten)               (None, 10240)        0           max_pooling2d[0][0]              
__________________________________________________________________________________________________
dense_flipout (DenseFlipout)    (None, 512)          10486273    flatten[0][0]                    
__________________________________________________________________________________________________
target_locations (DenseFlipout) (None, 289)          296226      dense_flipout[0][0]              
__________________________________________________________________________________________________
target_rhos (DenseFlipout)      (None, 289)          296226      dense_flipout[0][0]              
__________________________________________________________________________________________________
target (DistributionLambda)     ((None, 289), (None, 0           target_locations[0][0]           
                                                                 target_rhos[0][0]                
==================================================================================================
```

In this case, the output partially cuts the output shape of the last layer, so there should also be a way of adjusting the style of the output of `summary`.

**Will this change the current api? How?**

I am not sure.

**Who will benefit with this feature?**

Everyone."
35994,Expand registered kernels for variable ops on GPU,"**System information**
- TensorFlow version (you are using):1.14/1.15
- Are you willing to contribute it (Yes/No): yes



**Describe the feature and the current behavior/state.**

The current variable op on GPU is restricted to float, double, and int64.  Through the TF_CALL_GPU_NUMBER_TYPES and
TF_CALL_int64 macros.

The notes say to restrict the dtype to those supported by the assignment op.  The assignment op has a broader range of dtypes registered through the
TF_CALL_GPU_ALL_TYPES and 
TF_CALL_int64 macros.

This disallows users to store boolian data with small memory footprints and forces unnecessary casts.  For that matter int32's and half precision could probably also be added.

Its a very easy change to make.  However, I am not sure if there is further rational for the restricted types.

**Will this change the current api? How?**
Nothing other than allowing more dtypes to be passed into variable constuctors


**Who will benefit with this feature?**
Anyone who wants to lower variable memory footprint 


**Any Other info.**
"
35993,variable_ops.h missing from pip install,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.14.0
- Python version:3.6
- Installed using virtualenv? pip? conda?: Pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: p100



**Describe the problem**
When building custom ops, we want to link against pip installation for end user simplicity.  We were testing expanding the registered dtypes for variableson GPUs through subclassing.  We found that the header for the variable op is missing from the pip installation.  We had to copy the file locally from source and reference it in our cuatom op code which is undesirable from a maintenance perspective.  Can the file be added to the proper place in the includes?"
35992,Keras TextVectorization AttributeError during save_weights,"**System information**
- Colab code: https://colab.research.google.com/drive/1Lf1s9O6Z5ss33GEtb4UXEgjq4wiHw7iy
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.04/19.10, Google Colab
- TensorFlow installed from (source or binary): pypi binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7

**Describe the current behavior**

AttributeError when calling save_weights on a trained model.

```
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py in <listcomp>(.0)
   3274   """"""
   3275   if context.executing_eagerly():
-> 3276     return [x.numpy() for x in tensors]
   3277   elif ops.inside_function():  # pylint: disable=protected-access
   3278     raise RuntimeError('Cannot get value inside Tensorflow graph function.')

AttributeError: 'TrackableWeightHandler' object has no attribute 'numpy'
```

**Describe the expected behavior**

Model weights can be saved

**Code to reproduce the issue**

I simply modified François Chollet's (@fchollet) collab and added save_weights at the end:

https://colab.research.google.com/drive/1Lf1s9O6Z5ss33GEtb4UXEgjq4wiHw7iy#scrollTo=ZXQKEh53v948

Original colab here:

https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3


"
35990,Text Classification RNN tutorial doesn't run under TF2.1,"## URL(s) with the issue:

https://www.tensorflow.org/tutorials/text/text_classification_rnn

## Description of issue (what needs changing):

The sample for LSTM-based text classification doesn't run under Tensorflow 2.1 anymore.

The line:

```
train_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)
```

Does fail with the error:

```
AttributeError: 'ShuffleDataset' object has no attribute 'output_shapes'
```
"
35989,tf.function stable doc typo issue,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/function?version=stable 

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

Original:
> It also restricts the dhape and datatype of Tensors that can be used:

So, I think it should be updated as 

It also restricts the **shape** and datatype of Tensors that can be used:

### Clear description

Find a typo.

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
35988,"""ImportError: DLL load failed: The specified module could not be found."" while trying to import tensorflow in tensorflow_cpu environment.","Traceback (most recent call last):   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>     from tensorflow.python.pywrap_tensorflow_internal import *   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>     _pywrap_tensorflow_internal = swig_import_helper()   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\imp.py"", line 243, in load_module     return load_dynamic(name, filename, file)   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\imp.py"", line 343, in load_dynamic     return _load(spec) ImportError: DLL load failed: The specified module could not be found.  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File ""<stdin>"", line 1, in <module>   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>     from tensorflow_core import *   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>     from tensorflow.python.tools import module_util as _module_util   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__     module = self._load()   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\site-packages\tensorflow\__init__.py"", line 44, in _load     module = _importlib.import_module(self.__name__)   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\importlib\__init__.py"", line 126, in import_module     return _bootstrap._gcd_import(name[level:], package, level)   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>     from tensorflow.python import pywrap_tensorflow   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>     raise ImportError(msg) ImportError: Traceback (most recent call last):   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>     from tensorflow.python.pywrap_tensorflow_internal import *   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>     _pywrap_tensorflow_internal = swig_import_helper()   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\imp.py"", line 243, in load_module     return load_dynamic(name, filename, file)   File ""C:\Users\Shivani\Anaconda3\envs\tensorflow_cpu\lib\imp.py"", line 343, in load_dynamic     return _load(spec) ImportError: DLL load failed: The specified module could not be found."
35987,interpreter.invoke() of tflite model causes Aborted (core dumped) despite successful tflite conversion under tensorflow version 1.14.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Kubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip3
- TensorFlow version (use command below): 1.14.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
The  [mobilenetvlad model](https://github.com/ethz-asl/hierarchical_loc/tree/master/global-loc/models/mobilenetvlad_depth-0.35) was successfuly converted to a tf lite model by [the sample code provided](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/lite/TFLiteConverter) from the tensorflow website.  To archieve this I added the parameter `input_shapes` to the `from_saved_model` call:
```converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, input_shapes={""image"": [1, 640, 480, None]})```

But if inference is tested with the according [sample code](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python) from the tensorflow website, the program is aborted and the core dumped.

**Describe the expected behavior**
Sucessfull inference and output of a [1, 4096] sized image descriptor.

**Code to reproduce the issue**
1. Download the  [mobilenetvlad model](https://github.com/ethz-asl/hierarchical_loc/tree/master/global-loc/models/mobilenetvlad_depth-0.35)
2. With tensorflow 1.14.0 (tf2 will not work): Convert the saved_model to tflite, by setting an input shape (was set to None, None, None, 1]) in the [model description](https://github.com/ethz-asl/hierarchical_loc/blob/master/retrievalnet/retrievalnet/models/mobilenetvlad.py), but should be 640x480 according to the [paper.pdf](https://arxiv.org/pdf/1809.01019.pdf):
```
import tensorflow as tf
saved_model_dir='hierarchical_loc/global-loc/models/mobilenetvlad_depth-0.35'
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, input_shapes={""image"": [1, 640, 480, None]}) #only change of code beside filenames
tflite_model = converter.convert()
open(""converted_model_1_640_480.tflite"", ""wb"").write(tflite_model) 
```
3. Run the inference with the [sample code](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python):
```
import numpy as np
import tensorflow as tf

print(tf.__version__)

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""converted_model_1_640_480.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
print(input_details)
print(output_details)

# Test model on random input data.
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
print(input_data.shape)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)
```



**Other info / logs**

Output of the tflite conversion. [Edited: copied the wrong output, sorry]. 
And don't worry about the virtual environmnet naming `VirtualEnvironments/tensorflow_1_15/`. I initially wanted to install tf 1.15.0, but only tf 1.14.0 worked:
```/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
2020-01-22 14:00:03.226815: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-22 14:00:03.250753: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2194920000 Hz
2020-01-22 14:00:03.251151: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4a4df80 executing computations on platform Host. Devices:
2020-01-22 14:00:03.251185: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
WARNING:tensorflow:From /home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
WARNING:tensorflow:From /home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2020-01-22 14:00:06.393038: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2020-01-22 14:00:10.226966: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-01-22 14:00:10.227122: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session                                                                                                                                 
2020-01-22 14:00:10.347863: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize                                                                                          
2020-01-22 14:00:10.347910: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.002ms.                                                                              
2020-01-22 14:00:10.347930: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.                                                                              
WARNING:tensorflow:From /home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/lite/python/util.py:238: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.                                                                                                                                                                                               
Instructions for updating:                                                                                                                                                                                                                  
Use `tf.compat.v1.graph_util.convert_variables_to_constants`                                                                                                                                                                                
WARNING:tensorflow:From /home/alex/Documents/VirtualEnvironments/tensorflow_1_15/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.                                                                                                                                                                                            
Instructions for updating:                                                                                                                                                                                                                  
Use `tf.compat.v1.graph_util.extract_sub_graph`                                                                                                                                                                                             
2020-01-22 14:00:11.150707: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)                                         
2020-01-22 14:00:11.151014: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session                                                                                                                                 
2020-01-22 14:00:12.806255: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize                                                                                          
2020-01-22 14:00:12.806306: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 554 nodes (-265), 570 edges (-265), time = 1091.39502ms.                                                     
2020-01-22 14:00:12.806326: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 554 nodes (0), 570 edges (0), time = 372.614ms.      
```
The created model is then used within tfLiteInference.py:
```
import numpy as np
import tensorflow as tf

print(tf.__version__)

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""converted_model_1_640_480_test.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
print(input_details)
print(output_details)

# Test model on random input data.
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
print(input_data.shape)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)
```

Output of gdb debugging `gdb -ex r --args python3 tfLiteinference.py ` --> Aborted --> `py-list:`
```
Thread 1 ""python3"" received signal SIGABRT, Aborted.
__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
51      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) py-list
 104    
 105        def AllocateTensors(self):
 106            return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
 107    
 108        def Invoke(self):
>109            return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)
 110    
 111        def InputIndices(self):
 112            return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_InputIndices(self)
 113    
 114        def OutputIndices(self):
```

I hope anyone can help me with this.

Cheers,
Alex"
35986,"tflite_convert deletes (almost) all nodes and edges, .pb to .tflite, 585929kb to 1kb","Hello,
I'm having problems with conversion from frozen graph (.pb) to tflite, using tflite_convert.
It does safe a .tflite file, but it's only got a size of 1kb.
Apparently it gets rid of almost every node and all edges.
Can anybody give me advice on how to do it without losing all the information or what I'm doing wrong?
Best regards

**System information**
Windows 10
Tensorflow Version 1.14.0 obtained via pip

**Command used to run the converter or code if you’re using the Python API**

```
tflite_convert --graph_def_file=categorical_model.pb --output_file=optimized_graph.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,224,224,3 --input_array=input_1 --output_array=training/SGD/Variable_215
```

**The output from the converter invocation**

```
2020-01-17 12:22:40.734881: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.
2020-01-17 12:22:51.425546: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-01-17 12:22:51.432588: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2020-01-17 12:23:42.194315: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2020-01-17 12:23:42.199502: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 2 nodes (-1929), 0 edges (-2004), time = 45374.7031ms.
```

**Failure details**
Original file (categorical_model.pb) has got 585 929kb, output file (optimized_graph.tflite) has only got 1kb.
"
35983,AOT tests fail on Windows (MSVC 2019),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10 Pro x64
- TensorFlow installed from (source or binary):
Source
- TensorFlow version:
master (7be9f53)
- Python version:
Python 3.7.4 (Miniconda3)
- Bazel version (if compiling from source):
Build label: 1.2.1
Build time: Tue Nov 26 15:24:17 2019 (1574781857)
Build timestamp: 1574781857
Build timestamp as int: 1574781857
- GCC/Compiler version (if compiling from source):
MSVC 2019
- CUDA/cuDNN version:
10.1/7
- GPU model and memory:
GTX 1080 Ti 11 GB

**Describe the current behavior**
Compiling a graph using AOT fails with linker error LNK1107:
```
INFO: Analyzed target //tensorflow/compiler/aot:test_graph_tfadd (172 packages loaded, 15111 targets configured).
INFO: Found 1 target...
INFO: Deleting stale sandbox base E:/tmp/_bazel_patrik/asdmfctl/sandbox
[10 / 1,550] [Prepa] BazelWorkspaceStatusAction stable-status.txt
[727 / 4,195] checking cached actions
[7,806 / 7,807] [Prepa] Linking tensorflow/compiler/aot/test_graph_tfadd.lib
ERROR: E:/doremir/onset-detector/graph/tensorflow/tensorflow/compiler/aot/BUILD:121:1: Linking of rule '//tensorflow/compiler/aot:test_graph_tfadd' failed (Exit 1107)
bazel-out\x64_windows-opt\bin\tensorflow\compiler\aot\test_graph_tfadd_tfcompile_function.o : fatal error LNK1107: invalid or corrupt file: cannot read at 0x340
Target //tensorflow/compiler/aot:test_graph_tfadd failed to build
INFO: Elapsed time: 13.822s, Critical Path: 1.89s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```
**Describe the expected behavior**
Successful build with a functioning executable at 
`bazel-bin/tensorflow/compiler/aot/test_graph_tfadd`.
**Code to reproduce the issue**
Using MSYS2 (GNU bash, version 4.4.23(1)-release (x86_64-pc-msys))
```bash
git clone --depth=1 https://github.com/tensorflow/tensorflow
cd tensorflow
python configure.py # xla - yes, rest default.
bazel build //tensorflow/compiler/aot:test_graph_tfadd
```
**Other info / logs**
Environment setup:
```bash
# Use a temporary directory with a short name.
export TMPDIR=""E:/tmp""
export TMP=""E:/tmp""
export TEMP=""E:/tmp""
export TEMPDIR=""E:/tmp""
export TEST_TMPDIR=""E:/tmp""

# Add timestamps before each command.
export PS4='+ $(date) + '

# Set bash path
export BAZEL_SH=""E:/msys64/usr/bin/bash""
export BAZEL_VC=""E:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC""
export BAZEL_VS=""""

export PYTHON_BASE_PATH=""E:/Miniconda3/envs/tf2.0""

# Set the path to find bazel.
export PATH=""/c/bazel-root/:$PATH""

# Set Python path for ./configure
export PYTHON_BIN_PATH=""${PYTHON_BASE_PATH}/python.exe""
export PYTHON_LIB_PATH=""${PYTHON_BASE_PATH}/lib/site-packages""

# Add python into PATH, it's needed because gen_git_source.py uses
# '/usr/bin/env python' as a shebang
export PATH=""${PYTHON_BASE_PATH}:$PATH""

# Make sure we have pip in PATH
export PATH=""${PYTHON_BASE_PATH}/Scripts:$PATH""

# Setting default values to CUDA related environment variables
export TF_CUDA_VERSION=${TF_CUDA_VERSION:-10.1}
export TF_CUDNN_VERSION=${TF_CUDNN_VERSION:-7}
export TF_CUDA_COMPUTE_CAPABILITIES=${TF_CUDA_COMPUTE_CAPABILITIES:-6.0}
export CUDA_TOOLKIT_PATH=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v${TF_CUDA_VERSION}""
export CUDNN_INSTALL_PATH=""E:/cuda""

# Add Cuda and Cudnn dll directories into PATH
export PATH=""${CUDA_TOOLKIT_PATH}/bin:$PATH""
export PATH=""${CUDA_TOOLKIT_PATH}/extras/CUPTI/libx64:$PATH""
export PATH=""${CUDNN_INSTALL_PATH}/bin:$PATH""
```
"
35982,model.fit requests and leaks extra data from validation_data generator than the specified validation_steps,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
I tried this in two environments:

The first enviornment is: 
```
macOS Catalina 10.15.2
python 3.6.5
tensorflow 2.1 (v2.1.0-rc2-17-ge5bf8de410 2.1.0)
keras 2.3.1
```
The second environment is:
```
Linux Ubuntu 16.04
python 3.5.2
tensorflow-gpu 2.1  (v2.1.0-rc2-17-ge5bf8de 2.1.0)
keras 2.3.1
```


**Describe the current behavior**
Upon using the validation generator while fitting the model, the validation generator is somehow requested to generate more data batches per epoch than the actually needed for validation (specified by `validation_steps`). I'm unable to identify where this extra data is used, however, this extra data grows indefinitely with more training (as shown when I increase the number of epochs from 2 to 5). This irregular behavior ruins the validation because it's not exactly the same data that is being passed every epoch.

This behavior only happens when I used `tensorflow.keras`, however, it works properly when using just `keras`.

**Describe the expected behavior**
For every epoch, the validation generator generates a fixed number of data batches, and if there's extra data, it shouldn't grow indefinitely with more training epochs. This expected behavior happens below in the first two examples when I used `keras` instead of `tensorflow.keras`.

**Code to reproduce the issue**

```python
#!/usr/bin/env python3                                                                                                     
import os                                                                                                                  
import sys                                                                                                                 
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'                                                                                   
                                                                                                                           
import numpy as np                                                                                                         
# Environment setup, using keras vs. tensorflow.keras                                                                      
                                                                                                                           
import tensorflow as tf                                                                                                    
if sys.argv[1] == 'tensorflow':                                                                                            
    import tensorflow.keras as K                                                                                           
    from tensorflow.keras.models import Model                                                                              
    from tensorflow.keras.layers import Dense, Input, Dropout                                                              
elif sys.argv[1] == 'keras':                                                                                               
    import keras as K                                                                                                      
    from keras.models import Model                                                                                         
    from keras.layers import Dense, Input, Dropout                                                                         
else:                                                                                                                      
    sys.exit()                                                                                                             
                                                                                                                           
epochs = int(sys.argv[2])                                                                                                  
                                                                                                                           
                                                                                                                           
# Trackers of the generated data                                                                                           
train_cntX = 0  # updated for each extracted training batch                                                                
train_cntY = 0  # updated for each extracted training batch                                                                
hacked = []  # updated for each extracted validation batch                                                                 
                                                                                                                           
# Reporting the size of hacked before/after each epoch                                                                     
class TrackingCB(K.callbacks.Callback):                                                                                    
    def __init__(self, steps, *args, **kwargs):                                                                            
        super().__init__(*args, **kwargs)                                                                                  
        self.steps = steps                                                                                                 
                                                                                                                           
    def on_epoch_begin(self, epoch, logs={}):                                                                              
        self.start_length = len(hacked)                                                                                    
                                                                                                                           
    def on_epoch_end(self, epoch, logs={}):                                                                                
        for _ in range(self.steps):                                                                                        
            hacked.pop(0)                                                                                                  
        print('start: %d, end: %d' % (self.start_length, len(hacked)))                                                     
        del self.start_length                                                                                              
                                                                                                                           
# Data generators                                                                                                          
def genX(stddev=0.01, update=True):                                                                                        
    while True:                                                                                                            
        for i in range(20):                                                                                                
            if update:                                                                                                     
                global train_cntX                                                                                          
                train_cntX += 1                                                                                            
            yield {'input': np.random.normal(i + 1, stddev, (32, 10))}                                                     
    return                                                                                                                 
                                                                                                                           
def geny(update=True):                                                                                                     
    while True:                                                                                                            
        for i in range(20):                                                                                                
            if update:                                                                                                     
                global train_cntY                                                                                          
                train_cntY += 1                                                                                            
            yield {'output': np.ones((32, 1)) * ((i + 10) % 20)}                                                           
    return                                                                                                                 
                                                                                                                           
def genTrain():                                                                                                            
    for x, y in zip(genX(), geny()):                                                                                       
        yield x, y                                                                                                         
    return                                                                                                                 
                                                                                                                           
def genValid():                                                                                                            
    for x, y in zip(genX(1e-4, update=False), geny(update=False)):                                                         
        data = x, y                                                                                                        
        hacked.append(data)                                                                                                
        yield data                                                                                                         
    return                                                                                                                 
                                                                                                                           
                                                                                                                           
# Model & training                                                                                                         
inp = Input((10,), name='input')                                                                                           
out = Dense(20, activation='relu')(inp)                                                                                    
out = Dense(20, activation='relu')(out)                                                                                    
out = Dropout(0.5)(out)                                                                                                    
out = Dense(1, name='output')(out)                                                                                         
model = Model(inp, out)                                                                                                    
model.compile(loss=K.losses.mean_squared_error,                                                                            
              optimizer=K.optimizers.Adadelta(0.1, decay=0.01))                                                            
                                                                                                                           
model.fit(genTrain(),                                                                                                      
          epochs=epochs, steps_per_epoch=20,                                                                               
          validation_data=genValid(), validation_steps=20,                                                                 
          callbacks=[TrackingCB(20)], verbose=0)                                                                           
                                                                                                                           
                                                                                                                           
print('tensorflow:', tf.__version__)                                                                                       
print('keras:', K.__version__)                                                                                             
print('train generator counts:', train_cntX, train_cntY)                                                                   
print('validation remaining data length:', len(hacked))                                                                    

```
**Other info / logs**

In keras, the amount of extra data extracted is fixed after each epoch, and it doesn't grow with more epochs, which shows that the generator is used for exactly 20 validation steps as specified.
```
$ python3 keras_gen_debug.py keras 2
Using TensorFlow backend.
start: 0, end: 11
start: 11, end: 11
tensorflow: 2.1.0
keras: 2.3.1
train generator counts: 51 51
validation remaining data length: 11
```

```
$ python3 keras_gen_debug.py keras 5
Using TensorFlow backend.
start: 0, end: 11
start: 11, end: 11
start: 11, end: 11
start: 11, end: 11
start: 11, end: 11
tensorflow: 2.1.0
keras: 2.3.1
train generator counts: 111 111
validation remaining data length: 11
```

In tensorflow.keras, the amount of extra data extracted is growing after each epoch, which shows that after the 20 validation steps, there are extra 13 batches that were generated after each epoch. (this can be noticed in detail when `verbose=1`)

```
$ python3 keras_gen_debug.py tensorflow 2
WARNING:tensorflow:sample_weight modes were coerced from
  {'output': '...'}
    to  
  ['...']
WARNING:tensorflow:sample_weight modes were coerced from
  {'output': '...'}
    to  
  ['...']
start: 1, end: 13
start: 13, end: 26
tensorflow: 2.1.0
keras: 2.2.4-tf
train generator counts: 53 53
validation remaining data length: 26
```

```
$ python3 keras_gen_debug.py tensorflow 5
WARNING:tensorflow:sample_weight modes were coerced from
  {'output': '...'}
    to  
  ['...']
WARNING:tensorflow:sample_weight modes were coerced from
  {'output': '...'}
    to  
  ['...']
start: 1, end: 13
start: 13, end: 26
start: 26, end: 39
start: 39, end: 52
start: 52, end: 65
tensorflow: 2.1.0
keras: 2.2.4-tf
train generator counts: 113 113
validation remaining data length: 65
```"
35981,Error while generating TensorflowLite file,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
Exception ignored in: <bound method _CheckpointRestoreCoordinator.__del__ of <tensorflow.python.training.tracking.util._CheckpointRestoreCoordinator object at 0x7f0f9ab16f98>>
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py"", line 244, in __del__
    .format(pretty_printer.node_names[node_id]))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py"", line 93, in node_names
    path_to_root[node_id] + (child.local_name,))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/object_identity.py"", line 76, in __getitem__
    return self._storage[self._wrap_key(key)]
KeyError: (<tensorflow.python.training.tracking.object_identity._ObjectIdentityWrapper object at 0x7f0f91483ac8>,)
---------------------------------------------------------------------------
ConverterError                            Traceback (most recent call last)
<ipython-input-64-e4d06fcc1815> in <module>()
     13 # Convert the model to standard TensorFlow Lite model
     14 converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
---> 15 converted_tflite_model = converter.convert()
     16 open(TFLITE_MODEL, ""wb"").write(converted_tflite_model)

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)
    170       stderr = _try_convert_to_unicode(stderr)
    171       raise ConverterError(
--> 172           ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
    173   finally:
    174     # Must manually cleanup files.

ConverterError: TOCO failed. See console for info.
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
2020-01-17 11:53:28.108739: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: IdentityN
2020-01-17 11:53:28.144111: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 707 operators, 1294 arrays (0 quantized)
2020-01-17 11:53:28.168667: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 707 operators, 1294 arrays (0 quantized)
2020-01-17 11:53:28.306950: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 128 operators, 326 arrays (0 quantized)
2020-01-17 11:53:28.316129: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 125 operators, 321 arrays (0 quantized)
2020-01-17 11:53:28.319357: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 124 operators, 319 arrays (0 quantized)
2020-01-17 11:53:28.322587: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 124 operators, 319 arrays (0 quantized)
2020-01-17 11:53:28.325002: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 124 operators, 319 arrays (0 quantized)
2020-01-17 11:53:28.333533: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 11139584 bytes, theoretical optimal value: 8297856 bytes.
2020-01-17 11:53:28.334861: E tensorflow/lite/toco/toco_tooling.cc:462] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CONCATENATION, CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, MEAN, RESHAPE, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.
Traceback (most recent call last):
  File ""/usr/local/bin/toco_from_protos"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CONCATENATION, CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, MEAN, RESHAPE, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.


```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35980,Tensorflow predict call crashes when loading a model with gevent enabled,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6, also tried it on the Docker container nvidia/cuda:10.1-cudnn7-runtime
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0
- Python version: 3.7.4 on Mac, Python 3.6.9 :: Anaconda, Inc. in Docker
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Tensorflow crashes after calling `predict` on the model, this happens with gevent 1.4.0 and also 1.5a2

**Describe the expected behavior**
Tensorflow doesn't crash

**Code to reproduce the issue**
```
from gevent import monkey
monkey.patch_all()

import numpy as np
import tensorflow as tf

classifier = tf.keras.models.load_model('tensorflow_model_dir')
classifier.predict(np.array(
    np.zeros((1, 12623))
))
```
**Other info / logs**

```
Traceback (most recent call last):
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py"", line 430, in eager_learning_phase_scope
    _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = value
  File ""/opt/conda/envs/py36/lib/python3.6/weakref.py"", line 407, in __setitem__
    self.data[ref(key, self._remove)] = value
TypeError: cannot create weak reference to 'gevent._local.local' object

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""scripts/gevent_load_classifier.py"", line 9, in <module>
    np.zeros((1, 12623))
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1013, in predict
    use_multiprocessing=use_multiprocessing)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 498, in predict
    workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 475, in _model_iteration
    total_epochs=1)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 128, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 98, in execution_function
    distributed_function(input_fn))
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 615, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 497, in _initialize
    *args, **kwds))
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2389, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 439, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 85, in distributed_function
    per_replica_function, args=args)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 763, in experimental_run_v2
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 1819, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 2164, in _call_for_each_replica
    return fn(*args, **kwargs)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 292, in wrapper
    return func(*args, **kwargs)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 212, in _predict_on_batch
    result = predict_on_batch(model, x)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 555, in predict_on_batch
    with backend.eager_learning_phase_scope(0):
  File ""/opt/conda/envs/py36/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py"", line 437, in eager_learning_phase_scope
    del _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH]
  File ""/opt/conda/envs/py36/lib/python3.6/weakref.py"", line 391, in __delitem__
    del self.data[ref(key)]
TypeError: cannot create weak reference to 'gevent._local.local' object

```
"
35979,What's the difference between Keras applications and TF Model Garden?,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/applications
https://github.com/tensorflow/models/tree/master/official

## Description of issue (what needs changing):

There are the pre-trained models for Keras found in `tf.keras.applications`. And there are those models found on GitHub in the Model Garden (under `/models`, as linked above). Now, from reading the docs (both for the applications as well as those in the Model Garden) I don't get the difference. Why do we have those two different model repos?"
35977,windows build failed with master branch,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: master 2.1.0
- Python version: 3.7.6
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 1.2.1
- GCC/Compiler version (if compiling from source):  Visual Studio 2019 C++ compiler
- CUDA/cuDNN version: 10.2 / 7.6.5
- GPU model and memory:

RTX2080Ti GDDR6 11GB



**Describe the problem**
bazel build failed
**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build //tensorflwo/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
ERROR: D:/repo/tensorflow/tensorflow/BUILD:867:1: Executing genrule //tensorflow:tf_python_api_gen_v2 failed (Exit 1)
C:\Anaconda3\lib\site-packages\numpy\__init__.py:140: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service
  from . import _distributor_init
Traceback (most recent call last):
  File ""C:\Anaconda3\lib\site-packages\numpy\core\__init__.py"", line 24, in <module>
    from . import multiarray
  File ""C:\Anaconda3\lib\site-packages\numpy\core\multiarray.py"", line 14, in <module>
    from . import overrides
  File ""C:\Anaconda3\lib\site-packages\numpy\core\overrides.py"", line 7, in <module>
    from numpy.core._multiarray_umath import (
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""\\?\C:\Users\ALAN-W~1\AppData\Local\Temp\Bazel.runfiles_t25kvwuy\runfiles\org_tensorflow\tensorflow\python\tools\api\generator\create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""\\?\C:\Users\ALAN-W~1\AppData\Local\Temp\Bazel.runfiles_t25kvwuy\runfiles\org_tensorflow\tensorflow\python\__init__.py"", line 48, in <module>
    import numpy as np
  File ""C:\Anaconda3\lib\site-packages\numpy\__init__.py"", line 142, in <module>
    from . import core
  File ""C:\Anaconda3\lib\site-packages\numpy\core\__init__.py"", line 54, in <module>
    raise ImportError(msg)
ImportError:

IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the numpy c-extensions failed.
- Try uninstalling and reinstalling numpy.
- If you have already done that, then:
  1. Check that you expected to use Python3.7 from ""C:\Anaconda3\python.exe"",
     and that you have no directories in your PATH or PYTHONPATH that can
     interfere with the Python and numpy version ""1.18.1"" you're trying to use.
  2. If (1) looks fine, you can open a new issue at
     https://github.com/numpy/numpy/issues.  Please include details on:
     - how you installed Python
     - how you installed numpy
     - your operating system
     - whether or not you have multiple versions of Python installed
     - if you built from source, your compiler versions and ideally a build log

- If you're working with a numpy git repository, try `git clean -xdf`
  (removes all files not under version control) and rebuild numpy.

Note: this error has many possible causes, so please don't comment on
an existing issue about this - open a new one instead.

Original error was: DLL load failed: The specified module could not be found.

Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: D:/repo/tensorflow/tensorflow/tools/pip_package/BUILD:229:1 Executing genrule //tensorflow:tf_python_api_gen_v2 failed (Exit 1)
```
"
35975,ResNet model in keras and tf.keras give different output for the same image,"For a given image, I'm extracting ResNet features i.e. after all conv layers and global max pooling, which gives a 2048 length vector per image. 

Earlier, I was using `keras==2.3.1` with backend `tensorflow==1.13.1`. Now, I've shifted to `tensorflow==2.0.0` since keras has been merged with tensorflow. I replaced my code with `tf.keras` instead of `keras`.

But now the features extracted are not the same as the features extracted earlier. ResNet is a model which is independent of tensorflow/keras or even pytorch for that matter. It's functionality is predefined. Why is this difference occuring? Are there any parameters that can be tweaked to get the same functionality?\

```
    from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
    resnet_model = ResNet50(include_top=False)
```"
35970,Allow relative min_delta for EarlyStopping,"**System information**
- TensorFlow version (you are using): 2.1.0
- Are you willing to contribute it (Yes/No): maybe



**Describe the feature and the current behavior/state.**
Currently the [EarlyStopping callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) `min_delta` is an absolute value. Meaning that the metric being monitored must improve by that fixed amount. The proposal is to add the ability to use a relative threshold instead (e.g. a ratio or percentage).

An example use case is probably easier to understand. Lets say we're doing a hyperparameter search, so we're running a bunch of different models, and we want to stop early on the bad ones. On good models our loss value is in the single digits. So we set the parameters `baseline=50` (a little high so that we allow models to start off somewhat bad), and `min_delta=0.1`. Now with this, if a model gets under 50, but drops to just 48.9, that'll keep it from triggering the abort threshold. If this were instead a ratio, such that `min_delta=0.1` means `10%`, the loss would have to go from 50 to 45 to keep it from triggering, and a good model at say 1.2 would only have to drop to 1.08.

Now we could try other approaches to address this problem, such as lowering `baseline`, and increasing `patience` (allowing bad starts a little longer to come down further), but by increasing patience, this increases the amount of time we have to wait for good models to stop as well.

&nbsp;

Instead of adding a flag for toggling whether `min_delta` is absolute or relative, I think it might be better to allow the user to provide their own ""is this new value good enough"" function/lambda. Currently you can technically override `EarlyStopping.monitor_op`, but it's clearly not designed to operate that way as the arguments to `monitor_op` aren't simple ""best"" and ""current"" values, and there's also code which directly checks whether `monitor_op == np.less`, which overriding breaks.

So ultimately the request is to either add a new argument to the initializer accepting the signature `(current:float, best:float) -> bool`, or adjust the code to allow overriding `monitor_op` with that signature.

**Will this change the current api? How?**
Maybe, if the direction is to add an additional parameter to the initializer.

**Who will benefit with this feature?**
Those doing hyperparameter searches, or potentially other cases.

**Any Other info.**
"
35969,ImportError: cannot import name '_tf_stack' from 'tensorflow_core.python' ,"When i am trying to execute the below commands in jupyter notebook
from keras.models import Sequential
from keras.utils import to_categorical
from keras.layers import Dense
from keras.datasets import mnist

am getting the below error.

Using TensorFlow backend.
ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

Traceback (most recent call last):
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 2453, in <module>
    from tensorflow.python.util import deprecation
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 25, in <module>
    from tensorflow.python.platform import tf_logging as logging
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\platform\tf_logging.py"", line 38, in <module>
    from tensorflow.python.util.tf_export import tf_export
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_export.py"", line 48, in <module>
    from tensorflow.python.util import tf_decorator
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_decorator.py"", line 64, in <module>
    from tensorflow.python.util import tf_stack
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_stack.py"", line 29, in <module>
    from tensorflow.python import _tf_stack
ImportError: cannot import name '_tf_stack' from 'tensorflow_core.python' (C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-15-8bc4ca592e3e>"", line 2, in <module>
    from keras.models import Sequential
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\backend\__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 6, in <module>
    from tensorflow.python.framework import ops as tf_ops
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 31, in <module>
    from tensorflow.core.framework import attr_value_pb2
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Saranya\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Saranya\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 2453, in <module>
    from tensorflow.python.util import deprecation
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 25, in <module>
    from tensorflow.python.platform import tf_logging as logging
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\platform\tf_logging.py"", line 38, in <module>
    from tensorflow.python.util.tf_export import tf_export
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_export.py"", line 48, in <module>
    from tensorflow.python.util import tf_decorator
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_decorator.py"", line 64, in <module>
    from tensorflow.python.util import tf_stack
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_stack.py"", line 29, in <module>
    from tensorflow.python import _tf_stack
ImportError: cannot import name '_tf_stack' from 'tensorflow_core.python' (C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2040, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 2453, in <module>
    from tensorflow.python.util import deprecation
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 25, in <module>
    from tensorflow.python.platform import tf_logging as logging
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\platform\tf_logging.py"", line 38, in <module>
    from tensorflow.python.util.tf_export import tf_export
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_export.py"", line 48, in <module>
    from tensorflow.python.util import tf_decorator
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_decorator.py"", line 64, in <module>
    from tensorflow.python.util import tf_stack
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_stack.py"", line 29, in <module>
    from tensorflow.python import _tf_stack
ImportError: cannot import name '_tf_stack' from 'tensorflow_core.python' (C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1101, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 319, in wrapped
    return f(*args, **kwargs)
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 353, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File ""C:\Users\Saranya\Anaconda3\lib\inspect.py"", line 1502, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File ""C:\Users\Saranya\Anaconda3\lib\inspect.py"", line 1460, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""C:\Users\Saranya\Anaconda3\lib\inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""C:\Users\Saranya\Anaconda3\lib\inspect.py"", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Saranya\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Saranya\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 2453, in <module>
    from tensorflow.python.util import deprecation
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 25, in <module>
    from tensorflow.python.platform import tf_logging as logging
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\platform\tf_logging.py"", line 38, in <module>
    from tensorflow.python.util.tf_export import tf_export
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_export.py"", line 48, in <module>
    from tensorflow.python.util import tf_decorator
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_decorator.py"", line 64, in <module>
    from tensorflow.python.util import tf_stack
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_stack.py"", line 29, in <module>
    from tensorflow.python import _tf_stack
ImportError: cannot import name '_tf_stack' from 'tensorflow_core.python' (C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-15-8bc4ca592e3e>"", line 2, in <module>
    from keras.models import Sequential
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\backend\__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 6, in <module>
    from tensorflow.python.framework import ops as tf_ops
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 31, in <module>
    from tensorflow.core.framework import attr_value_pb2
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Saranya\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Saranya\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 2453, in <module>
    from tensorflow.python.util import deprecation
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 25, in <module>
    from tensorflow.python.platform import tf_logging as logging
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\platform\tf_logging.py"", line 38, in <module>
    from tensorflow.python.util.tf_export import tf_export
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_export.py"", line 48, in <module>
    from tensorflow.python.util import tf_decorator
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_decorator.py"", line 64, in <module>
    from tensorflow.python.util import tf_stack
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_stack.py"", line 29, in <module>
    from tensorflow.python import _tf_stack
ImportError: cannot import name '_tf_stack' from 'tensorflow_core.python' (C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2040, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 2453, in <module>
    from tensorflow.python.util import deprecation
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 25, in <module>
    from tensorflow.python.platform import tf_logging as logging
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\platform\tf_logging.py"", line 38, in <module>
    from tensorflow.python.util.tf_export import tf_export
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_export.py"", line 48, in <module>
    from tensorflow.python.util import tf_decorator
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_decorator.py"", line 64, in <module>
    from tensorflow.python.util import tf_stack
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_stack.py"", line 29, in <module>
    from tensorflow.python import _tf_stack
ImportError: cannot import name '_tf_stack' from 'tensorflow_core.python' (C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Traceback (most recent call last):
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 2453, in <module>
    from tensorflow.python.util import deprecation
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 25, in <module>
    from tensorflow.python.platform import tf_logging as logging
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\platform\tf_logging.py"", line 38, in <module>
    from tensorflow.python.util.tf_export import tf_export
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_export.py"", line 48, in <module>
    from tensorflow.python.util import tf_decorator
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_decorator.py"", line 64, in <module>
    from tensorflow.python.util import tf_stack
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_stack.py"", line 29, in <module>
    from tensorflow.python import _tf_stack
ImportError: cannot import name '_tf_stack' from 'tensorflow_core.python' (C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-15-8bc4ca592e3e>"", line 2, in <module>
    from keras.models import Sequential
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\backend\__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 6, in <module>
    from tensorflow.python.framework import ops as tf_ops
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 31, in <module>
    from tensorflow.core.framework import attr_value_pb2
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Saranya\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Saranya\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 2453, in <module>
    from tensorflow.python.util import deprecation
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 25, in <module>
    from tensorflow.python.platform import tf_logging as logging
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\platform\tf_logging.py"", line 38, in <module>
    from tensorflow.python.util.tf_export import tf_export
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_export.py"", line 48, in <module>
    from tensorflow.python.util import tf_decorator
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_decorator.py"", line 64, in <module>
    from tensorflow.python.util import tf_stack
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_stack.py"", line 29, in <module>
    from tensorflow.python import _tf_stack
ImportError: cannot import name '_tf_stack' from 'tensorflow_core.python' (C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2040, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3249, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3343, in run_code
    self.showtraceback(running_compiled_code=True)
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2043, in showtraceback
    value, tb, tb_offset=tb_offset)
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1385, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1288, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1150, in structured_traceback
    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)
TypeError: can only concatenate str (not ""list"") to str

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2040, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'TypeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 2453, in <module>
    from tensorflow.python.util import deprecation
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 25, in <module>
    from tensorflow.python.platform import tf_logging as logging
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\platform\tf_logging.py"", line 38, in <module>
    from tensorflow.python.util.tf_export import tf_export
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_export.py"", line 48, in <module>
    from tensorflow.python.util import tf_decorator
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_decorator.py"", line 64, in <module>
    from tensorflow.python.util import tf_stack
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_stack.py"", line 29, in <module>
    from tensorflow.python import _tf_stack
ImportError: cannot import name '_tf_stack' from 'tensorflow_core.python' (C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1101, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 319, in wrapped
    return f(*args, **kwargs)
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 353, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File ""C:\Users\Saranya\Anaconda3\lib\inspect.py"", line 1502, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File ""C:\Users\Saranya\Anaconda3\lib\inspect.py"", line 1460, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""C:\Users\Saranya\Anaconda3\lib\inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""C:\Users\Saranya\Anaconda3\lib\inspect.py"", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Saranya\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Saranya\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 2453, in <module>
    from tensorflow.python.util import deprecation
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 25, in <module>
    from tensorflow.python.platform import tf_logging as logging
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\platform\tf_logging.py"", line 38, in <module>
    from tensorflow.python.util.tf_export import tf_export
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_export.py"", line 48, in <module>
    from tensorflow.python.util import tf_decorator
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_decorator.py"", line 64, in <module>
    from tensorflow.python.util import tf_stack
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_stack.py"", line 29, in <module>
    from tensorflow.python import _tf_stack
ImportError: cannot import name '_tf_stack' from 'tensorflow_core.python' (C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-15-8bc4ca592e3e>"", line 2, in <module>
    from keras.models import Sequential
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\backend\__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 6, in <module>
    from tensorflow.python.framework import ops as tf_ops
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 31, in <module>
    from tensorflow.core.framework import attr_value_pb2
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Saranya\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Saranya\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 2453, in <module>
    from tensorflow.python.util import deprecation
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 25, in <module>
    from tensorflow.python.platform import tf_logging as logging
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\platform\tf_logging.py"", line 38, in <module>
    from tensorflow.python.util.tf_export import tf_export
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_export.py"", line 48, in <module>
    from tensorflow.python.util import tf_decorator
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_decorator.py"", line 64, in <module>
    from tensorflow.python.util import tf_stack
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_stack.py"", line 29, in <module>
    from tensorflow.python import _tf_stack
ImportError: cannot import name '_tf_stack' from 'tensorflow_core.python' (C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2040, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3249, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3343, in run_code
    self.showtraceback(running_compiled_code=True)
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2043, in showtraceback
    value, tb, tb_offset=tb_offset)
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1385, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1288, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1150, in structured_traceback
    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)
TypeError: can only concatenate str (not ""list"") to str

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2040, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'TypeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 2453, in <module>
    from tensorflow.python.util import deprecation
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 25, in <module>
    from tensorflow.python.platform import tf_logging as logging
  File ""C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\platform\tf_logging.py"", line 38, in <module>
    from tensorflow.python.util.tf_export import tf_export
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_export.py"", line 48, in <module>
    from tensorflow.python.util import tf_decorator
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_decorator.py"", line 64, in <module>
    from tensorflow.python.util import tf_stack
  File ""C:\Users\Saranya\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_stack.py"", line 29, in <module>
    from tensorflow.python import _tf_stack
ImportError: cannot import name '_tf_stack' from 'tensorflow_core.python' (C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py in <module>
   2452 
-> 2453 from tensorflow.python.util import deprecation
   2454 from tensorflow.python.util.tf_export import tf_export

~\Anaconda3\lib\site-packages\tensorflow_core\python\util\deprecation.py in <module>
     24 
---> 25 from tensorflow.python.platform import tf_logging as logging
     26 from tensorflow.python.util import decorator_utils

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\platform\tf_logging.py in <module>
     37 
---> 38 from tensorflow.python.util.tf_export import tf_export
     39 

~\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_export.py in <module>
     47 
---> 48 from tensorflow.python.util import tf_decorator
     49 from tensorflow.python.util import tf_inspect

~\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_decorator.py in <module>
     63 
---> 64 from tensorflow.python.util import tf_stack
     65 

~\Anaconda3\lib\site-packages\tensorflow_core\python\util\tf_stack.py in <module>
     28 # TODO(b/138203821): change to from ...util import ... once the bug is fixed.
---> 29 from tensorflow.python import _tf_stack
     30 

ImportError: cannot import name '_tf_stack' from 'tensorflow_core.python' (C:\Users\Saranya\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\__init__.py)

During handling of the above exception, another exception occurred:


During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
~\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py in showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)
   2039                         # in the engines. This should return a list of strings.
-> 2040                         stb = value._render_traceback_()
   2041                     except Exception:

AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
~\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py in run_code(self, code_obj, result, async_)
   3341             if result is not None:
   3342                 result.error_in_exec = sys.exc_info()[1]
-> 3343             self.showtraceback(running_compiled_code=True)
   3344         else:
   3345             outflag = False

~\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py in showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)
   2041                     except Exception:
   2042                         stb = self.InteractiveTB.structured_traceback(etype,
-> 2043                                             value, tb, tb_offset=tb_offset)
   2044 
   2045                     self._showtraceback(etype, value, stb)

~\Anaconda3\lib\site-packages\IPython\core\ultratb.py in structured_traceback(self, etype, value, tb, tb_offset, number_of_lines_of_context)
   1383         self.tb = tb
   1384         return FormattedTB.structured_traceback(
-> 1385             self, etype, value, tb, tb_offset, number_of_lines_of_context)
   1386 
   1387 

~\Anaconda3\lib\site-packages\IPython\core\ultratb.py in structured_traceback(self, etype, value, tb, tb_offset, number_of_lines_of_context)
   1286             # Verbose modes need a full traceback
   1287             return VerboseTB.structured_traceback(
-> 1288                 self, etype, value, tb, tb_offset, number_of_lines_of_context
   1289             )
   1290         elif mode == 'Minimal':

~\Anaconda3\lib\site-packages\IPython\core\ultratb.py in structured_traceback(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)
   1148         exception = self.get_parts_of_chained_exception(evalue)
   1149         if exception:
-> 1150             formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)
   1151             etype, evalue, etb = exception
   1152         else:

TypeError: can only concatenate str (not ""list"") to str


Please help me in rectifying this error."
35968,Could not load dynamic library 'libnvinfer_plugin.so.6',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): wheel
- TensorFlow version: 2.1.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip in virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: Titan XP

**Describe the problem**
```
2020-01-16 20:21:32.912603: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6 

2020-01-16 20:21:32.912768: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6:  cannot open shared object file: No such file or directory 

2020-01-16 20:21:32.912782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
```
**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
import tensorflow
```


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
cuda-10-1/unknown,now 10.1.243-1 amd64 [installed]
cuda-command-line-tools-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-compiler-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-cudart-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-cudart-dev-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-cufft-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-cufft-dev-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-cuobjdump-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-cupti-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-curand-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-curand-dev-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-cusolver-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-cusolver-dev-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-cusparse-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-cusparse-dev-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-demo-suite-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-documentation-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-driver-dev-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-drivers/unknown,now 440.33.01-1 amd64 [installed,automatic]
cuda-gdb-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-gpu-library-advisor-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-libraries-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-libraries-dev-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-license-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-license-10-2/unknown,now 10.2.89-1 amd64 [installed,automatic]
cuda-memcheck-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-misc-headers-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-npp-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-npp-dev-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-nsight-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-nsight-compute-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-nsight-systems-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-nvcc-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-nvdisasm-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-nvgraph-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-nvgraph-dev-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-nvjpeg-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-nvjpeg-dev-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-nvml-dev-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-nvprof-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-nvprune-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-nvrtc-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-nvrtc-dev-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-nvtx-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-nvvp-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-repo-ubuntu1604/unknown,now 10.1.243-1 amd64 [installed,upgradable to: 10.2.89-1]
cuda-runtime-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-samples-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-sanitizer-api-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-toolkit-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-tools-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
cuda-visual-tools-10-1/unknown,now 10.1.243-1 amd64 [installed,automatic]
libcuda1-440/unknown,now 440.33.01-0ubuntu1 amd64 [installed,automatic]
libcudnn7/unknown,now 7.6.4.38-1+cuda10.1 amd64 [installed,upgradable to: 7.6.5.32-1+cuda10.2]
libcudnn7-dev/unknown,now 7.6.4.38-1+cuda10.1 amd64 [installed,upgradable to: 7.6.5.32-1+cuda10.2]
libnvinfer-dev/unknown,now 6.0.1-1+cuda10.1 amd64 [installed,upgradable to: 7.0.0-1+cuda10.2]
libnvinfer6/unknown,now 6.0.1-1+cuda10.1 amd64 [installed,upgradable to: 6.0.1-1+cuda10.2]
```

Note this doesn't occur on nightly"
35960,Add python dev (alpha version) for testing and building in CI,Add python dev in CI can efficiently avoid the problem in next release. it it can make TensorFlow always support latest python stable release.
35958,Details about Tensorflow's I/O,"I'm currently trying to develop an I/O optimization for DL frameworks and for this purpose I would like to understand exactly how Tensorflow behaves when the dataset doesn't fit in memory. More precisely, I want to know how much data is fetched at a time from storage and how frequently it is fetched. I also need to get in more detail about how the prefetching and shuffling features work. I have already been looking for information on these matters and have also read the documentation, but I still haven't got the details I need. Therefore, can anyone tell me where can I find an extensive explanation of Tensorflow's I/O and prefetching and shuffling methods? Or what's the code responsible for handling this?

Thanks"
35955,Allow tf.image.extract_image_patches to work with complex numbers,"It would be nice if `tf.image.extract_image_patches` were extended to work with complex numbers as well.

I work extensively with audio data and after applying an STFT, I get a tensor of complex numbers. As I perform my training on patches from this tensor, it would be great if these patches could be extracted using `tf.image.extract_image_patches`.

I think this is something that would be valuable to many people working in the audio space.
"
35953,failed to convert attention-ocr frozen graph to tflite,"**System information**
- OS Platform and Distribution Linux Ubuntu 18.04
- TensorFlow installed from binary
- TensorFlow version 1.15.0

**I am using the following chunk of code to convert attention-ocr frozen graph to tflite**

```
import tensorflow as tf


PATH_TO_MODEL = ""./exported-model/frozen_graph.pb""


input_arrays = ['input_image_as_bytes']
output_arrays = ['prediction']

converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file=PATH_TO_MODEL,
                                                      input_arrays=input_arrays,
                                                      output_arrays=output_arrays,
                                                      input_shapes= {""input_image_as_bytes"" : (1, 128, 32)})

tflite_model = converter.convert()

open(""converted_model.tflite"", ""wb"").write(tflite_model)
```

**The output from the converter invocation**

```
2020-01-16 21:04:14.913210: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2020-01-16 21:04:14.913239: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 2801 nodes (-133), 3997 edges (-163), time = 2018.08203ms.
2020-01-16 21:04:14.913245: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 2801 nodes (0), 3997 edges (0), time = 159.867ms.
Traceback (most recent call last):
  File ""converter.py"", line 18, in <module>
    tflite_model = converter.convert()
  File ""venv/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py"", line 983, in convert
    **converter_kwargs)
  File ""venv/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py"", line 449, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""venv/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py"", line 200, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2020-01-16 21:04:16.445609: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: MutableHashTableV2
2020-01-16 21:04:16.445658: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-01-16 21:04:16.445688: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.445702: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.445710: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2020-01-16 21:04:16.445724: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.445734: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.445743: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LookupTableInsertV2
2020-01-16 21:04:16.445867: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.445878: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-01-16 21:04:16.445885: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.445948: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.445959: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.445968: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.445976: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-01-16 21:04:16.445986: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.445995: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.446013: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.446081: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-01-16 21:04:16.446095: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-01-16 21:04:16.446105: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-01-16 21:04:16.446114: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-01-16 21:04:16.446122: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.446135: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.446143: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-01-16 21:04:16.446151: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.446161: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.446169: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-01-16 21:04:16.446180: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-01-16 21:04:16.446194: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-01-16 21:04:16.446204: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2020-01-16 21:04:16.446221: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-01-16 21:04:16.446310: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-01-16 21:04:16.446323: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-01-16 21:04:16.446345: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: DecodePng
2020-01-16 21:04:16.446388: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-01-16 21:04:16.446430: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2020-01-16 21:04:16.446440: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2
2020-01-16 21:04:16.446553: F tensorflow/lite/toco/import_tensorflow.cc:1585] Check failed: data_type == DT_FLOAT 
Fatal Python error: Aborted

Current thread 0x00007fe7d035f740 (most recent call first):
  File ""venv/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52 in execute
  File ""venv/lib/python3.6/site-packages/absl/app.py"", line 250 in _run_main
  File ""venv/lib/python3.6/site-packages/absl/app.py"", line 299 in run
  File ""venv/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40 in run
  File ""venv/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89 in main
  File ""venv/bin/toco_from_protos"", line 8 in <module>
Aborted (core dumped)
```

**Here you can find a link to the frozen graph**

```
https://drive.google.com/open?id=1fD8PbgszlBscxyV2qVFqyEWFVlMcQvCd
```

**Failure details**
I am attempting to convert this frozen graph to tflite.I tried all the possibilities, a) using a SavedModel for conversion, b) the tflite_convert command, c) using bazel to run the toco file but all produced the same file. I had found the following [issue](https://github.com/emedvedev/attention-ocr/issues/136) referenced on the aocr github repo page but it doesn't seem to take me anywhere.

Any help is much appreciated!
"
35952,Device placement function for placing network variables on specific device,"Hey guys,

I'm having trouble with using a placement function inside `tf.device(...)`. Basically I want that all network variables are placed on a specific device (e.g. cpu). All other tensors and operations should be placed at the current scope. Because a placement function inside `tf.decive(...)` needs always to return a device I implemented the function `pin_network_variables`, which separates the variables, tensors and ops to two devices.
My problem with that is, that now nesting multiple device scopes doesn't work any more. Now all variables, tensors and ops are placed accordingly to `pin_network_variables`.
Here is a minimal example to illustrate the behavior:

```
import tensorflow as tf
from tensorflow.keras.layers import Dense

def pin_network_variables(var_device, op_device):
    """"""
    Pins network variables to the specified device.

    Args:
        var_device: Device to put network variables on.
        op_device: Device for everything except network variables.
    """"""

    VAR_NAME_TYPS = ['kernel', 'bias', 'gamma', 'beta', 'moving_mean', 'moving_variance', 'recurrent_kernel']
    def _is_network_var(op):
        for var_type in VAR_NAME_TYPS:
            if op.name.endswith('/' + var_type):
                return True
        return False

    def _assign(op):
        if _is_network_var(op):
            return var_device
        else:
            return op_device

    return _assign

def main(unused_argv):

    # normally all network varibles whould be placed on cpu and everything else on gpu
    with tf.device(pin_network_variables('/cpu:0', '/gpu:0')):

        input_tensor = tf.ones((1, 5), tf.float32, 'input_tensor')

        # the kernel and bias tensors are placed on cpu correctly
        dense_1 = Dense(units=3,
                        activation='softmax',
                        use_bias=True,
                        kernel_initializer='glorot_uniform',
                        bias_initializer='zeros',
                        name='dense_1')
        dense_2 = Dense(units=3,
                        activation='softmax',
                        use_bias=True,
                        kernel_initializer='glorot_uniform',
                        bias_initializer='zeros',
                        name='dense_2')

        # the call ops of the layers are placed on gpu correctly
        dense_1_out = dense_1(input_tensor)
        dense_2_out = dense_2(input_tensor)

        # in this scope are for example tensors and ops that need to be put on cpu, for computational reason
        with tf.device('/cpu:0'):

            # this op shoud be placed on cpu, but its put on gpu (due to 'pin_network_variables')
            output_tensor = tf.add(dense_1_out, dense_2_out)

    print('--------------------')
    print('input_tensor\t', input_tensor.device)
    print('dense_1.kernel\t', dense_1.kernel.device)
    print('dense_1.bias\t', dense_1.bias.device)
    print('dense_2.kernel\t', dense_2.kernel.device)
    print('dense_2.bias\t', dense_2.bias.device)
    print('dense_1_out\t', dense_1_out.device)
    print('dense_2_out\t', dense_2_out.device)
    print('output_tensor\t', output_tensor.device) # this should be put on cpu
    print('--------------------')

    # create, initialise and session
    sess = tf.compat.v1.Session()
    sess.run(tf.compat.v1.global_variables_initializer())
    output = sess.run(output_tensor)
    print(output)

if __name__ == '__main__':
    tf.compat.v1.app.run()
```

Does anybody now a fix to this?

OS Platform and Distribution: Windows 10
TensorFlow installed from: pip
TensorFlow version: 1.14.0
Python version: 3.6"
35951,how to change model.trainable_weights manually,"I am using TensorFlow 2.0 with Python 3.7.5 to manually change the weights of a neural network model. The code I have to change weights in a layer-wise manner is as follows:

```
def create_nn():
	""""""
	Function to create a toy neural network model
	""""""
	
	model = Sequential()

	model.add(
		Dense(
			units = 4, activation = 'relu',
			kernel_initializer = tf.keras.initializers.GlorotNormal(),
			input_shape = (4,)
			)
		)

	model.add(
		Dense(
			units = 3, activation = 'relu',
			kernel_initializer = tf.keras.initializers.GlorotNormal()
			)
		)

	model.add(
		Dense(
			units = 1, activation = 'sigmoid')
		)

	# Compile the defined NN model above-
	model.compile(
		loss = 'binary_crossentropy',  # loss = 'categorical_crossentropy'
		optimizer = tf.keras.optimizers.Adam(lr = 0.001),
		metrics=['accuracy']
	)

	return model


# Instantiate a neural network model-
orig_model = create_nn()

# modify weights in each layer such that: weights less than 0.5 should become zero,
# while weights greater than or equal to 0.5 should remain as it is-
for layer in orig_model.trainable_weights: 
    layer = tf.where(tf.less(layer, 0.5), 0, layer)

# Check whether the manual weight modifications work-
for layer in orig_model.trainable_weights:
    print(layer.numpy())
```

What can I do to modify/change the weights in a layer-wise manner in the above example?

Thanks"
35950,Error in cuda_dnn.cc(1921) with cudnnRNNBackwardData.  Failed to call ThenRNNBackward,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): [No](https://www.tensorflow.org/tutorials/text/text_classification_rnn)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.1.0-rc1-58-g9837eceb39
- Python version: 3.6.8
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10.1.243 / 7.6.0.64
- GPU model and memory: NVIDIA GeForce GTX 1050, 4.00GiB

**Describe the current behavior**
I'm attempting to learn about Recurrent Neural Networks following [this](https://www.tensorflow.org/tutorials/text/text_classification_rnn) guide from Tensorflow.  For some reason whenever I try to run the network it fails.

Interestingly network only ever seems to get past one epoch when verbose is set to 1, or excluded.  In this case it will typically complete 1-3 epochs before failing.

**Other info / logs**

Potentially similar to [this issue](https://github.com/tensorflow/tensorflow/issues/35791)

```
2020-01-16 10:26:44.374373: E tensorflow/stream_executor/dnn.cc:596] CUDNN_STATUS_INTERNAL_ERROR
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1802): 'cudnnRNNForwardTraining( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.handles(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'
2020-01-16 10:26:44.375949: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 64, 64, 1, 1850, 64, 64] 
2020-01-16 10:26:44.376544: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 64, 64, 1, 1850, 64, 64] 
	 [[{{node CudnnRNN}}]]
2020-01-16 10:26:44.377273: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __forward_cudnn_lstm_with_fallback_4560_specialized_for_sequential_bidirectional_backward_lstm_StatefulPartitionedCall_at___inference_distributed_function_5790}} {{function_node __forward_cudnn_lstm_with_fallback_4560_specialized_for_sequential_bidirectional_backward_lstm_StatefulPartitionedCall_at___inference_distributed_function_5790}} Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 64, 64, 1, 1850, 64, 64] 
	 [[{{node CudnnRNN}}]]
	 [[sequential/bidirectional/backward_lstm/StatefulPartitionedCall]]
	 [[Reshape_11/_38]]
2020-01-16 10:26:44.379170: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __forward_cudnn_lstm_with_fallback_4560_specialized_for_sequential_bidirectional_backward_lstm_StatefulPartitionedCall_at___inference_distributed_function_5790}} {{function_node __forward_cudnn_lstm_with_fallback_4560_specialized_for_sequential_bidirectional_backward_lstm_StatefulPartitionedCall_at___inference_distributed_function_5790}} Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 64, 64, 1, 1850, 64, 64] 
	 [[{{node CudnnRNN}}]]
	 [[sequential/bidirectional/backward_lstm/StatefulPartitionedCall]]
Traceback (most recent call last):
  File ""C:\Users\Cal\Desktop\python\NN\RNN\RNN.py"", line 50, in <module>
    validation_steps=30)
  File ""C:\Users\Cal\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 819, in fit
    use_multiprocessing=use_multiprocessing)
  File ""C:\Users\Cal\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 342, in fit
    total_epochs=epochs)
  File ""C:\Users\Cal\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 128, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""C:\Users\Cal\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 98, in execution_function
    distributed_function(input_fn))
  File ""C:\Users\Cal\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)
  File ""C:\Users\Cal\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 599, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File ""C:\Users\Cal\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2363, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""C:\Users\Cal\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1611, in _filtered_call
    self.captured_inputs)
  File ""C:\Users\Cal\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1692, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""C:\Users\Cal\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\eager\function.py"", line 545, in call
    ctx=ctx)
  File ""C:\Users\Cal\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\eager\execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError:  [_Derived_]  Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 64, 64, 1, 1850, 64, 64] 
	 [[{{node CudnnRNN}}]]
	 [[sequential/bidirectional/backward_lstm/StatefulPartitionedCall]]
	 [[Reshape_11/_38]] [Op:__inference_distributed_function_5790]

Function call stack:
distributed_function -> distributed_function -> distributed_function
```

**Describe the expected behavior**
The network trains without error.

**Code to reproduce the issue**

````
from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow_datasets as tfds
import tensorflow as tf
import matplotlib.pyplot as plt

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string], '')
  plt.xlabel(""Epochs"")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,
                          as_supervised=True)
train_dataset, test_dataset = dataset['train'], dataset['test']
encoder = info.features['text'].encoder

BUFFER_SIZE = 10000
BATCH_SIZE = 64

train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_dataset))

test_dataset = test_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(test_dataset))

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(encoder.vocab_size, 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy',
    optimizer=tf.keras.optimizers.Adam(1e-4),
    metrics=['accuracy'])

history = model.fit(train_dataset, epochs=10, verbose=0,
                    validation_data=test_dataset,
                    validation_steps=30)

test_loss, test_acc = model.evaluate(test_dataset)
print('Test Loss: {}'.format(test_loss))
print('Test Accuracy: {}'.format(test_acc))
````

**Already Tried:**
-Updating tensorflow to 2.1, then to 2.1rc1.
-Updating CUDA
-Updating cudNN to 7.6.5.32
-Allowing GPU memory growth.
"
35949,AttributeError: 'Tensor' object has no attribute '_datatype_enum',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X Catalina (10.15.2)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: 3.7.5
- GPU model and memory: Intel Iris Pro 1536 MB

**Describe the current behavior**

I get the errors

> tensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.

then 

> AttributeError: 'Tensor' object has no attribute '_datatype_enum'

and then

> AttributeError: 'ProgbarLogger' object has no attribute 'log_values'

when I add the following callback to the list of callbacks of `my_model.fit`

```
my_callback = tf.keras.callbacks.LambdaCallback(on_batch_begin=lambda batch, logs: tf.print(my_model.losses))
```

**Describe the expected behavior**

No error.

**Code to reproduce the issue**

```
import tensorflow as tf


def get_model():
    inp = tf.keras.layers.Input(shape=(1,))
    x = tf.keras.layers.Dense(8, activity_regularizer=tf.keras.regularizers.l1(0.01))(inp)
    x = tf.keras.layers.Dense(16, activity_regularizer=tf.keras.regularizers.l1(0.01))(x)
    out = tf.keras.layers.Dense(1)(x)
    model = tf.keras.Model(inputs=inp, outputs=out)
    return model


def train():
    my_model = get_model()
    my_model.compile(optimizer=""adam"", loss=""mse"")
    my_callback = tf.keras.callbacks.LambdaCallback(on_batch_begin=lambda batch, logs: tf.print(my_model.losses))
    my_model.fit([1, 2, 3, 4], [0.1, 0.2, 0.4, 0.2], callbacks=[my_callback])


if __name__ == '__main__':
    train()
```

This issue may be related to https://github.com/tensorflow/tensorflow/issues/28924 and https://github.com/tensorflow/tensorflow/issues/29931. Note that, if I don't use any regulariser, `tf.print` prints an empty list and no error occurs."
35948,Support for saved_model (tf2.*) in tf_compile,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.*
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
TF_Compile doesn't accept saved_model
**Will this change the current api? How?**
it would add a saved_model entry under tf_library macro
**Who will benefit with this feature?**
Anyone using tf2.* and tf-compile. Frozen graphs are deprecated in 2.0
**Any Other info.**
<3"
35944,tf.data.Dataset.from_generator converts input argument types implicitly instead of just forwarding,"**System information**
- custom code
- Ubuntu 18.04.1 LTS
- Thinkpad X240
- TensorFlow installed via pip3
- TensorFlow v2.0.0-rc2-26-g64c3d38 2.0.0
- Python 3.6.8
- no CUDA/cuDNN

**Describe the current behavior**

The code below generates the output `(1, 0)` for the first print when using the generator directly and the exception below when wrapping the generator using `tf.data.Dataset.from_generator`.

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-35-36b15431cf1a> in <module>()
     17 
     18 print( next( movingWindow( data, window_size ) ) )
---> 19 print( next( iter( dataset ) ) )

/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in __next__(self)
    620 
    621   def __next__(self):  # For Python 3 compatibility
--> 622     return self.next()
    623 
    624   def _next_internal(self):

/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in next(self)
    664     """"""Returns a nested structure of `Tensor`s containing the next element.""""""
    665     try:
--> 666       return self._next_internal()
    667     except errors.OutOfRangeError:
    668       raise StopIteration

/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in _next_internal(self)
    649             self._iterator_resource,
    650             output_types=self._flat_output_types,
--> 651             output_shapes=self._flat_output_shapes)
    652 
    653       try:

/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py in iterator_get_next_sync(iterator, output_types, output_shapes, name)
   2671       else:
   2672         message = e.message
-> 2673       _six.raise_from(_core._status_to_exception(e.code, message), None)
   2674   # Add nodes to the TensorFlow graph.
   2675   if not isinstance(output_types, (list, tuple)):

/usr/lib/python3/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: TypeError: an integer is required
Traceback (most recent call last):

  File ""/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/script_ops.py"", line 221, in __call__
    ret = func(*args)

  File ""/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py"", line 585, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File ""<ipython-input-35-36b15431cf1a>"", line 3, in movingWindow
    buffer = collections.deque( data[:window_size-1], maxlen = window_size )

TypeError: an integer is required


	 [[{{node PyFunc}}]] [Op:IteratorGetNextSync]
```

As it turns out this is because the `window_size` given in the `args` argument is unexpectedly converted from `int` to `np.int32`, which can't be used interchangeably for slicing or the deque maxlen parameter.

**Describe the expected behavior**

The example should work without exception. Which means, from_generator should not change the types of any arguments given via the `args` parameter.

**Code to reproduce the issue**


```Python3
import tensorflow as tf
import numpy as np
import collections

def movingWindow( data, window_size ):
    #window_size = int( window_size )
    buffer = collections.deque( data[:window_size-1], maxlen = window_size )
    for i, datum in enumerate( data[window_size-1:] ):
        buffer.append( datum )
        for b in buffer:
            yield datum, b

window_size = 2
data = np.arange( 10 )

dataset = tf.data.Dataset.from_generator( 
    movingWindow,
    args = ( data, window_size ),
    output_types = ( np.int32, np.int32 )
)

print( next( movingWindow( data, window_size ) ) )
print( next( iter( dataset ) ) )
```"
35942,Loading model with tf.keras.models.load_model not working on multi GPU,"**System information**
- Custom code
- TensorFlow version 2.1.0
- Python version: 3.7
- GPU model: 4 V100 GPUs on Kubernetes Engine

**Describe the current behavior**
On multi GPU loading the model from a h5 file is not working. 

**Describe the expected behavior**
Saving and reloading the model from a h5 file using model.save and  keras.models.load_model should work on both single and multi GPU.

**Code to reproduce the issue**
``` python 
import tensorflow as tf 
import os
import contextlib
import numpy as np
import tensorflow.keras as keras  

def get_model():
    model = keras.Sequential([
        keras.layers.Flatten(input_shape=(28, 28)),
        keras.layers.Dense(10, activation=tf.nn.softmax)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(),
                      loss='sparse_categorical_crossentropy')
    return model

def get_model_path():
    model_dir = '/tmp/m' + str(np.random.randint(0, 1000000))
    os.makedirs(model_dir)
    model_path = os.path.join(model_dir, 'model')
    return model_path + "".h5""

def attempt_save_and_reload(model_path, distributed_training=False):
    fashion_mnist = keras.datasets.fashion_mnist
    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
    train_images = train_images / 255.0
    test_images = test_images / 255.0

    with strategy.scope() if distributed_training else contextlib.nullcontext():
        model = get_model()
        model.fit(
            train_images,
            train_labels,
            epochs=1,
        )
        model.save(model_path)
        model = tf.keras.models.load_model(model_path)

if __name__ == '__main__':
    strategy = tf.distribute.MirroredStrategy()
    for distributed_training in [False, True]:
        print('distributed training: ', distributed_training)
        model_path = get_model_path()
        try:
            attempt_save_and_reload(model_path, distributed_training)
        except Exception as e:
            print('Exception raised: \n', e)
        print()
```



**other info/ logs**
I need to use h5 files since saving the optimizer state does not work otherwise (see #33424).  The logs I get are: 
```
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')
distributed training:  False
Train on 60000 samples
60000/60000 [==============================] - 3s 52us/sample - loss: 0.5991

distributed training:  True
Train on 60000 samples
INFO:tensorflow:batch_all_reduce: 2 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:batch_all_reduce: 2 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
60000/60000 [==============================] - 9s 152us/sample - loss: 0.6016
Exception raised: 
 `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.
```

"
35940,Incorrect mention of dataset....,"## URL(s) with the issue:

https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_lite/tflite_c05_exercise_rock_paper_scissors.ipynb
---------------------------------------------------------------
![Screenshot from 2020-01-16 18-37-30](https://user-images.githubusercontent.com/29497701/72527695-5d004900-388f-11ea-84f8-57ed0c12c915.png)
----------------------------------------------------------------

## Description of issue (what needs changing):

I think this exercise doesn't make use of cats_vs_dogs dataset, right??

### Clear description

In place of `cats_vs_dogs` dataset, `rock_paper_scissors` dataset should be mentioned.

### Submit a pull request?

Yes, shortly"
35939,Problems when compiling with Clang,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 19.10
- TensorFlow installed from (source or binary): source (for inference), python (for training)
- Tensorflow version (commit SHA if source): 1768c8f2fa155d4c6406e8ff7addf374c83de7ad for inference, release 2.0 or 2.1 for training.
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): [RIOT](https://github.com/RIOT-OS/RIOT), ARM Cortex-M3/4/7

**Describe the problem**

I recently ported TensorFlow Lite to RIOT, an operating system for microcontrollers, using the package mechanism provided by the RIOT build system: this allows to build TensorFlow-Lite on the fly when generating a RIOT firmware and to use it's API from a RIOT application.
See https://github.com/RIOT-OS/RIOT/pull/12847 for details.

RIOT provides support for many boards including many ARM based boards. It also supports both the GCC and the LLVM (Clang) toolchains.

Our CI showed problems when building with Clang, see below for details and [this issue](https://github.com/RIOT-OS/RIOT/issues/13133). In short, the firmware is crashing when evaluating the FullyConnected operator on ARM Cortex-M but the same thing works fine with GCC.

The [example application in RIOT](https://github.com/RIOT-OS/RIOT/tree/master/tests/pkg_tensorflow-lite) is just running a very basic MLP model (with Dense and Softmax layers) on an image taken from the MNIST dataset. For details of the application, you can have a look at the [main_functions.cc file](https://github.com/RIOT-OS/RIOT/blob/master/tests/pkg_tensorflow-lite/mnist/main_functions.cc) and [the script](https://github.com/RIOT-OS/RIOT/blob/master/tests/pkg_tensorflow-lite/mnist/mnist_mlp.py) used to generate the flatbuffers file containing the model.

Note that `hello_world` example is running fine, even when built with LLVM so I don't know what's wrong here: is the Python script or is it the way the `MicroMutableOpResolver` is built ?

Sorry for the long description but I wanted to make as complete as possible.

**Please provide the exact sequence of commands/steps when you ran into the problem**

- Software required on the host:
  - **clang**: on Ubuntu, can be installed with `apt install clang`
  - **ARM Cortex-M gdb**, from the [GNU ARM toolchain](https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/gnu-rm) (this is for the debug command below).
  - pyocd, can be installed with `pip3 install --user pyocd`
  - socat, on Ubuntu, can be installed with `apt install socat`
- Build/flash/run the default tensorflow-lite example of RIOT on an [nrf52832-mdk](https://wiki.makerdiary.com/nrf52832-mdk/):
```
DEVELHELP=1 RIOT_TERMINAL=socat TOOLCHAIN=llvm make BOARD=nrf52832-mdk -C tests/pkg_tensorflow-lite flash term
```
- You can check where the program crashed using the `debug` target (just follow the instructions reported by the RIOT crash):
```
make BOARD=nrf52832-mdk -C tests/pkg_tensorflow-lite debug
```
Then in gdb:
```gdb
set $pc=0x8006b04
frame 0
bt
```
You get the following output:
```gdb
Reading symbols from /work/riot/RIOT/tests/pkg_tensorflow-lite/bin/stm32f723e-disco/tests_pkg_tensorflow-lite.elf...
Remote debugging using :3333
hard_fault_handler (sp=0x20001bb8 <setup()::static_interpreter+16>, corrupted=1132396544, 
    exc_return=536882079, r4_to_r11_stack=0x310) at vectors_cortexm.c:393
393	    __BKPT(1);
(gdb) set $pc=0x8006bbe
(gdb) frame 0
#0  tflite::GetOptionalInputTensor (context=0x20001bb8 <setup()::static_interpreter+16>, node=0xc46c, 
    index=2)
    at /work/riot/RIOT/tests/pkg_tensorflow-lite/bin/pkg/stm32f723e-disco/tensorflow-lite/tensorflow/lite/kernels/kernel_util.h:80
80	  const bool use_tensor = index < node->inputs->size &&
(gdb) bt
#0  tflite::GetOptionalInputTensor (context=0x20001bb8 <setup()::static_interpreter+16>, node=0xc46c, 
    index=2)
    at /work/riot/RIOT/tests/pkg_tensorflow-lite/bin/pkg/stm32f723e-disco/tensorflow-lite/tensorflow/lite/kernels/kernel_util.h:80
#1  tflite::ops::micro::fully_connected::Eval (context=0x20001bb8 <setup()::static_interpreter+16>, 
    node=0xc46c) at fully_connected.cc:172
#2  0x08002276 in tflite::MicroInterpreter::Invoke (this=0x20001ba8 <setup()::static_interpreter>)
    at micro_interpreter.cc:201
#3  0x080018b0 in setup () at main_functions.cc:100
#4  0x0800164e in main (argc=5, argv=0xc46c) at main.cpp:28
(gdb) quit
```

The board is not very important, this command can be adapted for a lot of other ARM based boards supported by RIOT: STM32 nucleo, kinetis, etc. Note that you'll have to install the right tool for flashing the boards (OpenOCD, JLink, etc) depending on the board configuration.

"
35938,tf.cast on native python float to dtype=tf.float64 leads to loss of precision,"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary (pip install)
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0, v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.7

**Describe the current behavior**

`tf.cast()` on a python float (e.g, a literal float constant, not a numpy float/array type) does an implicit conversion to the TensorFlow default dtype tf.float32, which can result in loss of precision when intending to cast to tf.float64:
```python
>>> tf.cast(0.2, tf.float64)
<tf.Tensor: id=37, shape=(), dtype=float64, numpy=0.20000000298023224>
```

**Describe the expected behavior**

```python
>>> tf.cast(0.2, tf.float64)
<tf.Tensor: id=37, shape=(), dtype=float64, numpy=0.2>
```

**Code to reproduce the issue**
See above.

**Other info / logs**
This was discovered as a bug in GPflow, which we built a work-around for in https://github.com/GPflow/GPflow/pull/1211 - but this is a pervading issue, and it would be good to fix this upstream instead of having to write and use a `gpflow.cast()` everywhere just to work around having potentially passed in a python float."
35936,GPU Support Instructions for CUDA 10 (Ubuntu 16.04) refer to non-existent package,"The current GPU Support instructions for CUDA 10 on Ubuntu 16.04 refers to a package version that does not exist in the [NVIDIA ML repo](https://developer.download.nvidia.cn/compute/machine-learning/repos/ubuntu1604/x86_64/): `libnvinfer5=6.0.1-1+cuda10.1`

#### Ubuntu 16.04 (CUDA 10)

<pre class=""prettyprint lang-bsh"">
...
# Install TensorRT. Requires that libcudnn7 is installed above.
<code class=""devsite-terminal"">sudo apt-get install -y --no-install-recommends libnvinfer5=6.0.1-1+cuda10.1 \
    libnvinfer-dev=6.0.1-1+cuda10.1
</code>
</pre>

This results in an error when trying to install:
```
Reading package lists... Done
Building dependency tree       
Reading state information... Done
E: Version ‘6.0.1-1+cuda10.1’ for ‘libnvinfer5’ was not found
```

Changing this to `libnvinfer6=6.0.1-1+cuda10.1` seems to work and run happily."
35935,conda install tensorflow or any command to install the Tensroflow is not working.,"(base) C:\Users\vishwasnarayan>conda install -c anaconda tensorflow
Collecting package metadata (current_repodata.json): done
Solving environment: done

## Package Plan ##

  environment location: C:\Users\vishwasnarayan\AppData\Local\Continuum\anaconda3

  added / updated specs:
    - tensorflow


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    _tflow_select-2.3.0        |              mkl           3 KB  anaconda
    absl-py-0.8.1              |           py37_0         162 KB  anaconda
    astor-0.8.0                |           py37_0          45 KB  anaconda
    gast-0.2.2                 |           py37_0         138 KB  anaconda
    google-pasta-0.1.8         |             py_0          43 KB  anaconda
    keras-applications-1.0.8   |             py_0          33 KB  anaconda
    keras-preprocessing-1.1.0  |             py_1          36 KB  anaconda
    libmklml-2019.0.5          |                0        21.4 MB  anaconda
    libprotobuf-3.11.2         |       h7bd577a_0         2.3 MB  anaconda
    markdown-3.1.1             |           py37_0         132 KB  anaconda
    opt_einsum-3.1.0           |             py_0          54 KB  anaconda
    protobuf-3.11.2            |   py37h33f27b4_0         597 KB  anaconda
    tensorboard-2.0.0          |     pyhb38c66f_1         3.3 MB  anaconda
    tensorflow-2.0.0           |mkl_py37he1bbcac_0           4 KB  anaconda
    tensorflow-base-2.0.0      |mkl_py37hd1d5974_0        41.9 MB  anaconda
    tensorflow-estimator-2.0.0 |     pyh2649769_0         272 KB  anaconda
    termcolor-1.1.0            |           py37_1           7 KB  anaconda
    ------------------------------------------------------------
                                           Total:        70.4 MB

The following NEW packages will be INSTALLED:

  _tflow_select      anaconda/win-64::_tflow_select-2.3.0-mkl
  absl-py            anaconda/win-64::absl-py-0.8.1-py37_0
  astor              anaconda/win-64::astor-0.8.0-py37_0
  gast               anaconda/win-64::gast-0.2.2-py37_0
  google-pasta       anaconda/noarch::google-pasta-0.1.8-py_0
  grpcio             pkgs/main/win-64::grpcio-1.16.1-py37h351948d_1
  keras-applications anaconda/noarch::keras-applications-1.0.8-py_0
  keras-preprocessi~ anaconda/noarch::keras-preprocessing-1.1.0-py_1
  libmklml           anaconda/win-64::libmklml-2019.0.5-0
  libprotobuf        anaconda/win-64::libprotobuf-3.11.2-h7bd577a_0
  markdown           anaconda/win-64::markdown-3.1.1-py37_0
  opt_einsum         anaconda/noarch::opt_einsum-3.1.0-py_0
  protobuf           anaconda/win-64::protobuf-3.11.2-py37h33f27b4_0
  tensorboard        anaconda/noarch::tensorboard-2.0.0-pyhb38c66f_1
  tensorflow         anaconda/win-64::tensorflow-2.0.0-mkl_py37he1bbcac_0
  tensorflow-base    anaconda/win-64::tensorflow-base-2.0.0-mkl_py37hd1d5974_0
  tensorflow-estima~ anaconda/noarch::tensorflow-estimator-2.0.0-pyh2649769_0
  termcolor          anaconda/win-64::termcolor-1.1.0-py37_1


Proceed ([y]/n)? y


Downloading and Extracting Packages
gast-0.2.2           | 138 KB    | ############################################################################ | 100%
tensorflow-estimator | 272 KB    | ############################################################################ | 100%
tensorflow-base-2.0. | 41.9 MB   | ############################################################################ | 100%
tensorflow-2.0.0     | 4 KB      | ############################################################################ | 100%
astor-0.8.0          | 45 KB     | ############################################################################ | 100%
absl-py-0.8.1        | 162 KB    | ############################################################################ | 100%
_tflow_select-2.3.0  | 3 KB      | ############################################################################ | 100%
libmklml-2019.0.5    | 21.4 MB   | ############################################################################ | 100%
tensorboard-2.0.0    | 3.3 MB    | ############################################################################ | 100%
libprotobuf-3.11.2   | 2.3 MB    | ############################################################################ | 100%
opt_einsum-3.1.0     | 54 KB     | ############################################################################ | 100%
markdown-3.1.1       | 132 KB    | ############################################################################ | 100%
google-pasta-0.1.8   | 43 KB     | ############################################################################ | 100%
keras-applications-1 | 33 KB     | ############################################################################ | 100%
termcolor-1.1.0      | 7 KB      | ############################################################################ | 100%
keras-preprocessing- | 36 KB     | ############################################################################ | 100%
protobuf-3.11.2      | 597 KB    | ############################################################################ | 100%
Preparing transaction: done
Verifying transaction: failed

CondaVerificationError: The package for tensorflow-base located at C:\Users\vishwasnarayan\AppData\Local\Continuum\anaconda3\pkgs\tensorflow-base-2.0.0-mkl_py37hd1d5974_0
appears to be corrupted. The path 'Lib/site-packages/tensorflow-2.0.0.data/purelib/tensorflow_core/include/tensorflow_core/core/grappler/optimizers/generic_layout_optimizer_transposer_factory.h'
specified in the package manifest cannot be found."
35934,Keras Model Errors on Loading - 'list' object has no attribute 'items',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
**No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**
- TensorFlow installed from (source or binary): **pip**
- TensorFlow version (use command below): **2.1.0**
- Python version: **3.5.2**
- CUDA/cuDNN version: **CUDA 10.1 cuDNN 7.6**
- GPU model and memory: **Quadro M2000M 4.00GiB**

**Describe the current behavior**

When trying to load one of my sequential models using `tf.keras.models.load_model` an error is thrown at the following location:
```
tensorflow_core\python\keras\utils\generic_utils.py"", line 254, in class_and_config_for_serialized_keras_object
for key, item in cls_config.items():
```

This code expects `cls_config` to be a dictionary, while for this model it is a list of dictionaries.
I can successfully load and run this model using TensorFlow versions `2.0.0`, `1.14.0` and `1.4.0` (the version is was trained with)

This section of code was introduced when adding support for [passive serialization in Keras](https://github.com/tensorflow/tensorflow/commit/a4fe7d8b6919281e26e2970e530cbff886a778ae#diff-ca2f2579ed6d04b0be9c2bacfa1a4d38)

**Describe the expected behavior**

Can successfully load a model from a hdf5 file when its config is in the list format

**Code to reproduce the issue**

Execute the `class_and_config_for_serialized_keras_object` function with a config of the following form:

```JSON
{
    ""config"": [
        {
            ""config"": 
            {
                ...
            },
            ""class_name"": ""Conv2D""
        },
        ...
    ],
    ""class_name"": ""Sequential""
}
```

Which is different from the form when I create a new model using TF v2.1.0 or 2.0.0 (I haven't verified the form of the config with the other versions):

```JSON
{
    ""config"": {
        ""name"": ""sequential"",
        ""layers"": [
            {
                ""config"": 
                {
                    ...
                },
                ""class_name"": ""Conv2D""
            },
            ...
        ]
    },
    ""class_name"": ""Sequential""
}
```

**Other info / logs**

When loaded with tf.keras in v2.0.0 the layers, model config, inputs, outputs, summary etc. are all parsed correctly, as well as being able to run data through the model."
35933,'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'ravel',"I am using tensorflow 2.1


    `def`` generate_sequence(epoch_num, initial_index, seq_length):
    with open(os.path.join(data_directory, charIndex_json)) as f:
        char_to_index = json.load(f)
    index_to_char = {i:ch for ch, i in char_to_index.items()}
    unique_chars = len(index_to_char)
    
    model = make_model(unique_chars)
    model.load_weights(model_weights_directory + ""Weights_{}.h5"".format(epoch_num))
     
    sequence_index = [initial_index]
    
    for _ in range(seq_length):
        batch = np.zeros((1, 1))
        batch[0, 0] = sequence_index[-1]
        
        predicted_probs = model.predict_on_batch(batch).ravel()
        sample = np.random.choice(range(unique_chars), size = 1, p = predicted_probs)
        
        sequence_index.append(sample[0])
    
    seq = ''.join(index_to_char[c] for c in sequence_index)
    
    cnt = 0
    for i in seq:
        cnt += 1
        if i == ""\n"":
            break
    seq1 = seq[cnt:]
    #above code is for ignoring the starting string of a generated sequence. This is because we are passing any arbitrary 
    #character to the model for generating music. Now, the model start generating sequence from that character itself which we 
    #have passed, so first few characters before ""\n"" contains meaningless word. Model start generating the music rhythm from
    #next line onwards. The correct sequence it start generating from next line onwards which we are considering.
    
    cnt = 0
    for i in seq1:
        cnt += 1
        if i == ""\n"" and seq1[cnt] == ""\n"":
            break
    seq2 = seq1[:cnt]
    #Now our data contains three newline characters after every tune. So, the model has leart that too. So, above code is used for
    #ignoring all the characters that model has generated after three new line characters. So, here we are considering only one
    #tune of music at a time and finally we are returning it..
    
    return seq2








error :

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-48-1c976bcea2e8> in <module>
      3 ln = int(input(""\n3. Enter the length of music sequence you want to generate. Typical number is between 300-600. Too small number will generate hardly generate any sequence: ""))
      4 
----> 5 music = generate_sequence(ep, ar, ln)
      6 
      7 print(""\nMUSIC SEQUENCE GENERATED: \n"")

<ipython-input-47-a1565ba804d1> in generate_sequence(epoch_num, initial_index, seq_length)
     14         batch[0, 0] = sequence_index[-1]
     15 
---> 16         predicted_probs = model.predict_on_batch(batch).ravel()
     17         sample = np.random.choice(range(unique_chars), size = 1, p = predicted_probs)
     18 

AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'ravel'"
35932,Could not find matching function to call loaded from the SavedModel,"My code works in `GPU` based tensorflow environment without any fuss but fails in `CPU` based environments. Some other people also are facing the same issue. `Training` works without any issues but it's the `predict` method that's failing.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- TensorFlow installed from (source or binary): Source
- Other system related information below:

```
Collecting system information...
/tmp/check_os.py:18: DeprecationWarning: dist() and linux_distribution() functions are deprecated in Python 3.5
  platform.linux_distribution(),
/tmp/check_os.py:19: DeprecationWarning: dist() and linux_distribution() functions are deprecated in Python 3.5
  platform.dist(),
cat: /proc/1/cgroup: No such file or directory
2020-01-16 15:58:53.661018: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2020-01-16 15:58:53.661360: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.
')
architecture: ('64bit', '')
machine: x86_64


== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 10.0.1 (clang-1001.0.46.4)
Target: x86_64-apple-darwin18.7.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== check pips ===================================================
numpy                         1.17.4             
protobuf                      3.11.2             
tensorflow                    2.1.0              
tensorflow-estimator          2.1.0              
tensorflow-hub                0.7.0              

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.0.0
tf.version.GIT_VERSION = unknown
tf.version.COMPILER_VERSION = 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_cololect.sh: line 147: nvidia-smi: command not found

== cuda libs  ===================================================

== tensorflow installed from info ==================
Name: tensorflow
Version: 2.1.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /Users/sardarmrinal/anaconda3/lib/python3.7/site-packages
Required-by: 

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 7, 6, 'final', 0)

== bazel version  ===============================================
Build label: 1.2.1
Build time: Tue Nov 26 15:27:31 2019 (1574782051)
Build timestamp: 1574782051
Build timestamp as int: 1574782051
```

**Describe the current behavior**
The code is running successfully in `GPU` based environment and failing in `CPU` based environments.

**Describe the expected behavior**
It should run/fail in the same way in both `GPU` and `CPU` based environments.

**Code to reproduce the issue**
Code in the following link has the same behavior:
https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub

And also people are talking about this issue in the comments.


**Other info / logs**
```
Making predictions
Traceback (most recent call last):
  File ""/Users/sardarmrinal/Egnyte/Private/sardar.mrinal/workspace/competitions/kaggle_nlp_disaster/working/NLP_disaster_bert.py"", line 218, in <module>
    df_sub = model_bert.predict(df_eval)
  File ""/Users/sardarmrinal/Egnyte/Private/sardar.mrinal/workspace/competitions/kaggle_nlp_disaster/working/NLP_disaster_bert.py"", line 186, in predict
    prediction = self.model.predict(x=X)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 909, in predict
    use_multiprocessing=use_multiprocessing)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 462, in predict
    steps=steps, callbacks=callbacks, **kwargs)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 444, in _model_iteration
    total_epochs=1)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 86, in execution_function
    distributed_function(input_fn))
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 503, in _call
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 408, in _initialize
    *args, **kwds))
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1848, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 358, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 73, in distributed_function
    per_replica_function, args=(model, x, y, sample_weights))
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 760, in experimental_run_v2
    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 1787, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 2132, in _call_for_each_replica
    return fn(*args, **kwargs)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 292, in wrapper
    return func(*args, **kwargs)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 162, in _predict_on_batch
    return predict_on_batch(model, x)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 370, in predict_on_batch
    return model(inputs)  # pylint: disable=not-callable
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 847, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 708, in call
    convert_kwargs_to_constants=base_layer_utils.call_context().saving)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 860, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 847, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 292, in wrapper
    return func(*args, **kwargs)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py"", line 218, in call
    lambda: f(training=False))
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/smart_cond.py"", line 56, in smart_cond
    return false_fn()
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py"", line 218, in <lambda>
    lambda: f(training=False))
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py"", line 436, in _call_attribute
    return instance.__call__(*args, **kwargs)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 494, in _call
    results = self._stateful_fn(*args, **kwds)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1822, in __call__
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 358, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/function_deserialization.py"", line 262, in restored_function_body
    ""\n\n"".join(signature_descriptions)))
ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (3 total):
    * [<tf.Tensor 'inputs:0' shape=(None, 3) dtype=int64>, <tf.Tensor 'inputs_1:0' shape=(None, 3) dtype=int64>, <tf.Tensor 'inputs_2:0' shape=(None, 3) dtype=int64>]
    * False
    * None
  Keyword arguments: {}

Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (3 total):
    * [TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids')]
    * True
    * None
  Keyword arguments: {}

Option 2:
  Positional arguments (3 total):
    * [TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids')]
    * False
    * None
  Keyword arguments: {}

Option 3:
  Positional arguments (3 total):
    * [TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/1'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/2')]
    * True
    * None
  Keyword arguments: {}

Option 4:
  Positional arguments (3 total):
    * [TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/1'), TensorSpec(shape=(None, None), dtype=tf.int32, name='inputs/2')]
    * False
    * None
  Keyword arguments: {}
```
"
35931,No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ,macos
- TensorFlow installed from (source or binary): source

I have successfully compiled AAR through bazel and tensorflow source code:
**1.14-tensorflow-lite-with-select-tf-ops.aar** and **1.15-tensorflow-lite-with-select-tf-ops.aar**

**What i want to do:**
Since my tflite model lacks the **RandomUniform** operator, I compiled and generated the AAR.Then I followed the official tips from tensorflow to introduce AAR dependencies in Android Studio.
![image](https://user-images.githubusercontent.com/43409147/72513097-ae0c3f00-3887-11ea-80cd-d0cf4b4d6818.png)
No error was reported during synchronization, but an error occurred when running the .kt file.
**As follows, I get an error when I generate the interpreter：**
![image](https://user-images.githubusercontent.com/43409147/72515495-f5e09580-388a-11ea-8854-aac284518626.png)


**My error message is as follows**

> 01/16 17:41:17: Launching 'MainActivity' on Pixel 2 API 29.
$ adb shell am start -n ""com.example.test_aar2/com.example.test_aar2.MainActivity"" -a android.intent.action.MAIN -c android.intent.category.LAUNCHER
Waiting for process to come online...
Connected to process 4911 on device 'emulator-5554'.
Capturing and displaying logcat messages from application. This behavior can be disabled in the ""Logcat output"" section of the ""Debugger"" settings page.
W/RenderThread: type=1400 audit(0.0:123): avc: denied { write } for name=""property_service"" dev=""tmpfs"" ino=6939 scontext=u:r:untrusted_app:s0:c118,c256,c512,c768 tcontext=u:object_r:property_socket:s0 tclass=sock_file permissive=0 app=com.example.test_aar2
D/libEGL: Emulator has host GPU support, qemu.gles is set to 1.
W/libc: Unable to set property ""qemu.gles"" to ""1"": connection failed; errno=13 (Permission denied)
D/libEGL: loaded /vendor/lib64/egl/libEGL_emulation.so
D/libEGL: loaded /vendor/lib64/egl/libGLESv1_CM_emulation.so
D/libEGL: loaded /vendor/lib64/egl/libGLESv2_emulation.so
W/ample.test_aar: Accessing hidden method Landroid/view/View;->computeFitSystemWindows(Landroid/graphics/Rect;Landroid/graphics/Rect;)Z (greylist, reflection, allowed)
W/ample.test_aar: Accessing hidden method Landroid/view/ViewGroup;->makeOptionalFitsSystemWindows()V (greylist, reflection, allowed)
D/: HostConnection::get() New Host Connection established 0x705aa57140c0, tid 4942
D/: HostComposition ext ANDROID_EMU_CHECKSUM_HELPER_v1 ANDROID_EMU_native_sync_v2 ANDROID_EMU_native_sync_v3 ANDROID_EMU_native_sync_v4 ANDROID_EMU_dma_v1 ANDROID_EMU_direct_mem ANDROID_EMU_host_composition_v1 ANDROID_EMU_host_composition_v2 ANDROID_EMU_vulkan ANDROID_EMU_deferred_vulkan_commands ANDROID_EMU_vulkan_create_resources_with_requirements ANDROID_EMU_async_unmap_buffer GL_OES_EGL_image_external_essl3 GL_OES_vertex_array_object GL_KHR_texture_compression_astc_ldr ANDROID_EMU_gles_max_version_3_1 
W/OpenGLRenderer: Failed to choose config with EGL_SWAP_BEHAVIOR_PRESERVED, retrying without...
D/eglCodecCommon: setVertexArrayObject: set vao to 0 (0) 0 0
D/EGL_emulation: eglCreateContext: 0x705aa5714340: maj 3 min 1 rcv 4
D/EGL_emulation: eglMakeCurrent: 0x705aa5714340: ver 3 1 (tinfo 0x705aa5814400)
E/eglCodecCommon: glUtilsParamSize: unknow param 0x000082da
    glUtilsParamSize: unknow param 0x000082da
W/Gralloc3: mapper 3.x is not supported
D/: createUnique: call
    HostConnection::get() New Host Connection established 0x705aa57145c0, tid 4942
D/: HostComposition ext ANDROID_EMU_CHECKSUM_HELPER_v1 ANDROID_EMU_native_sync_v2 ANDROID_EMU_native_sync_v3 ANDROID_EMU_native_sync_v4 ANDROID_EMU_dma_v1 ANDROID_EMU_direct_mem ANDROID_EMU_host_composition_v1 ANDROID_EMU_host_composition_v2 ANDROID_EMU_vulkan ANDROID_EMU_deferred_vulkan_commands ANDROID_EMU_vulkan_create_resources_with_requirements ANDROID_EMU_async_unmap_buffer GL_OES_EGL_image_external_essl3 GL_OES_vertex_array_object GL_KHR_texture_compression_astc_ldr ANDROID_EMU_gles_max_version_3_1 
D/eglCodecCommon: allocate: Ask for block of size 0x1000
    allocate: ioctl allocate returned offset 0x3ff708000 size 0x2000
D/EGL_emulation: eglMakeCurrent: 0x705aa5714340: ver 3 1 (tinfo 0x705aa5814400)
D/eglCodecCommon: setVertexArrayObject: set vao to 0 (0) 1 0
W/System.err: TensorFlowLite: failed to load native library: dalvik.system.PathClassLoader[DexPathList[[zip file ""/data/app/com.example.test_aar2-GJboUyCuRGe7EMGh-GZalA==/base.apk""],nativeLibraryDirectories=[/data/app/com.example.test_aar2-GJboUyCuRGe7EMGh-GZalA==/lib/x86_64, /system/lib64, /system/product/lib64]]] couldn't find ""libtensorflowlite_jni.so""
W/System.err: TensorFlowLite: failed to load native library: dalvik.system.PathClassLoader[DexPathList[[zip file ""/data/app/com.example.test_aar2-GJboUyCuRGe7EMGh-GZalA==/base.apk""],nativeLibraryDirectories=[/data/app/com.example.test_aar2-GJboUyCuRGe7EMGh-GZalA==/lib/x86_64, /system/lib64, /system/product/lib64]]] couldn't find ""libtensorflowlite_jni.so""
E/ample.test_aar: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)
D/AndroidRuntime: Shutting down VM
E/AndroidRuntime: FATAL EXCEPTION: main
    Process: com.example.test_aar2, PID: 4911
    java.lang.UnsatisfiedLinkError: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)
        at org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:58)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)
        at com.example.test_aar2.MainActivity.action(MainActivity.kt:47)
        at com.example.test_aar2.MainActivity.access$action(MainActivity.kt:23)
        at com.example.test_aar2.MainActivity$onCreate$1.onClick(MainActivity.kt:30)
        at android.view.View.performClick(View.java:7140)
        at android.view.View.performClickInternal(View.java:7117)
        at android.view.View.access$3500(View.java:801)
        at android.view.View$PerformClick.run(View.java:27351)
        at android.os.Handler.handleCallback(Handler.java:883)
        at android.os.Handler.dispatchMessage(Handler.java:100)
        at android.os.Looper.loop(Looper.java:214)
        at android.app.ActivityThread.main(ActivityThread.java:7356)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:492)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:930)
Process 4911 terminated.

**Then I introduced two dependencies at the same time according to the official tips of tensorflow：**
![image](https://user-images.githubusercontent.com/43409147/72513782-7b167b00-3888-11ea-97e2-876c3e1734e8.png)
**As follows：**
![image](https://user-images.githubusercontent.com/43409147/72513984-d8aac780-3888-11ea-88fa-0164d51508a5.png)

**But I encountered the error again：**

> Duplicate class org.tensorflow.lite.DataType found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)
Duplicate class org.tensorflow.lite.DataType$1 found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)
Duplicate class org.tensorflow.lite.Delegate found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)
Duplicate class org.tensorflow.lite.Interpreter found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)
Duplicate class org.tensorflow.lite.Interpreter$Options found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)
Duplicate class org.tensorflow.lite.NativeInterpreterWrapper found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)
Duplicate class org.tensorflow.lite.Tensor found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)
Duplicate class org.tensorflow.lite.TensorFlowLite found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)
Duplicate class org.tensorflow.lite.nnapi.NnApiDelegate found in modules 1.14-tensorflow-lite-with-select-tf-ops-runtime.jar (1.14-tensorflow-lite-with-select-tf-ops.aar) and tensorflow-lite-2.1.0-runtime.jar (org.tensorflow:tensorflow-lite:2.1.0)

I have tried other versions of tensorflow-lite, such as 1.10.0. This error also appears.
I checked a lot of information and made a lot of changes, but still can't use this AAR and tflite that requires RandomUniform operator

So I urgently need your help ~~~"
35930,load_weights does not work when weights are saved in h5 format,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
model can't restore weights from h5 file

**Describe the expected behavior**

**Code to reproduce the issue**
```
class Model(keras.Model):
    def __init__(self, inp1, inp2):
        super(Model, self).__init__()
        self.x1 = self.add_weight('w1',[inp1])
        self.x2 = self.add_weight('w2',[inp2])
    def call(self,x):
        return x
# load_weights method works when save format is 'tf
x = Model(100,200)
x.save_weights('temp.tmp',save_format='tf')
old = x.weights[0][0].numpy()
print(old)
x = Model(100,200)
x.load_weights('temp.tmp')
new = x.weights[0][0].numpy()
print(new)
print(old==new)

# load_weights method does not work when save format is 'h5'
x = Model(100,200)
x.save_weights('temp.h5',save_format='h5')
old = x.weights[0][0].numpy()
print(old)
x = Model(100,200)
x.load_weights('temp.h5')
new = x.weights[0][0].numpy()
print(new)
print(old==new)
```
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35929,`ValueError: Unable to decode config: None` when load_model,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution : Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0.0
- Python version: Python 3.7

**Describe the current behavior**
I save the model using ModelCheckpoint in model.fit(), and after the training, I can load model weights  sucessfully if I set `save_weights_only=True`. but come across the error when save both the weights and model architecture by set `save_weights_only=True` using `tf.keras.models.load_model` 

**Describe the expected behavior**
`ValueError: Unable to decode config: None` when load_model

"
35928,Low performance when using persistent mode GradientTape with LSTM layers,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code: Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Conda
- TensorFlow version (use command below): 2.0.0 and 2.1.0
- Python version: 3.7.5
- CUDA/cuDNN version: 10.0.130 / 7.6.4
- GPU model and memory: GTX 980 Ti, GTX 2080 Ti

**Describe the current behavior**
The performance was very low when using persistent mode tf.GradientTape or create multi-GradientTape objects in one ```with``` block.
This phenomenon only happens when the model includes a LSTM layer.

**Code to reproduce the issue**
Here is a sample code to reproduce the problem.
```python
import timeit
import numpy as np
import tensorflow as tf

model0 = tf.keras.models.Sequential(
    tf.keras.layers.LSTM(128, input_shape=(300, 40))
)
model1 = tf.keras.models.Sequential(
    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(128,))
)
loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)

@tf.function
def train_step_0():
  with tf.GradientTape() as tape0, tf.GradientTape() as tape1:
    f = model0(tf.random.normal([256, 300, 40]))
    y_p0 = model1(f)
    y_p1 = model1(tf.random.normal((256, 128)))
    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)
    loss1 = loss_object(tf.ones_like(y_p1), y_p1)
  grad0 = tape0.gradient(loss0, model0.trainable_variables)
  grad1 = tape1.gradient(loss1, model1.trainable_variables)

train_step_0()
print(timeit.timeit('train_step_0()', globals=globals(), number=100))
```
In this case, the GPU-Util was only about 30 % and the time was 37 seconds.
The following message was shown in screen.
```
2020-01-16 17:16:22.519978: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Input 0 of node sequential/lstm/zeros_like was passed int32 from sequential/lstm/StatefulPartitionedCall:9 incompatible with expected variant.
2020-01-16 17:16:22.667364: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Input 0 of node sequential/lstm/zeros_like was passed int32 from sequential/lstm/StatefulPartitionedCall:9 incompatible with expected variant.
2020-01-16 17:16:22.739393: W tensorflow/core/common_runtime/process_function_library_runtime.cc:675] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node sequential/lstm/zeros_like was passed int32 from sequential/lstm/StatefulPartitionedCall:9 incompatible with expected variant.
```

```python
@tf.function
def train_step_1():
  with tf.GradientTape(persistent=True) as tape:
    f = model0(tf.random.normal([256, 300, 40]))
    y_p0 = model1(f)
    y_p1 = model1(tf.random.normal((256, 128)))
    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)
    loss1 = loss_object(tf.ones_like(y_p1), y_p1)
  grad0 = tape.gradient(loss0, model0.trainable_variables)
  grad1 = tape.gradient(loss1, model1.trainable_variables)

train_step_1()
print(timeit.timeit('train_step_1()', globals=globals(), number=100))
```
In this case, the performance was similar to the previous one.

```python
@tf.function
def train_step_2():
  with tf.GradientTape() as tape0:
    f = model0(tf.random.normal([256, 300, 40]))
    y_p0 = model1(f)
    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)
  with tf.GradientTape() as tape1:
    y_p1 = model1(tf.random.normal((256, 128)))
    loss1 = loss_object(tf.ones_like(y_p1), y_p1)
  grad0 = tape0.gradient(loss0, model0.trainable_variables)
  grad1 = tape1.gradient(loss1, model1.trainable_variables)

train_step_2()
print(timeit.timeit('train_step_2()', globals=globals(), number=100))
```
In this case, the GPU-Util was almost 100 % and the time was only 4 seconds.

In my opinion, ```train_step_0```, ```train_step_1``` and ```train_step_2``` should have similar performance.
I am wandering why the GPU-Util and process time were so that different.
The strangest thing is that this phenomenon only happens when the model includes a LSTM layer.
If we exchange the LSTM layer to Conv or Dense layer, the process time will be all same.
Here is a colaboratory page to reproduce this problem.
https://colab.research.google.com/drive/1sluVFuW1yYtH0Ye4reoOEmLUHYHDSGn7
"
35927,error when set converter.experimental_new_converter = True,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (or github SHA if from source): tf-nightly == 2.1.0-dev20200101


**Command used to run the converter or code if you’re using the Python API**

```
def model():
    input_image = keras.layers.Input(shape=[])
    C3, C4, C5 = resnet_graph(input_image)
    P3, P4, P5, P6, P7 = FPN_graph(C3, C4, C5)
    loc_data, conf_data, mask_data = predict_graph(P3, P4, P5, P6, P7)
    proto_data = protonet(P3)
    anchors = get_priors(config.IMAGE_SHAPE)
    refined_boxes = DecodeBoxes(loc_data, anchors)
    # batch_multiclass_non_max_suppression() is in object_detection.core.post_processing.py
    boxes, scores, class_ids, mask_coef, num_detection =   batch_multiclass_non_max_suppression()
    masks = AssemblyMask(mask_coef, proto_out, boxes)
    model = keras.models.Model([input_image],[boxes, class_ids, scores, masks, num_detections])
   return model

model = model()
model.load_weights('.h5')

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.experimental_new_converter = True
tflite_model = converter.convert()    
```

**The output from the converter invocation**

```

pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-01-16 14:11:43.696917: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-01-16 14:11:43.696927: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-01-16 14:11:43.696936: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-01-16 14:11:43.696944: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-01-16 14:11:43.696952: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-01-16 14:11:43.696960: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-01-16 14:11:43.696968: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-16 14:11:43.697006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:11:43.697323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:11:43.697611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0
2020-01-16 14:11:43.697635: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-01-16 14:11:43.698527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-16 14:11:43.698539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 
2020-01-16 14:11:43.698544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N 
2020-01-16 14:11:43.698621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:11:43.698944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:11:43.699248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8422 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-01-16 14:11:58.365227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:11:58.365704: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-01-16 14:11:58.365842: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-01-16 14:11:58.366283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:11:58.366583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-01-16 14:11:58.366616: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-01-16 14:11:58.366627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-01-16 14:11:58.366636: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-01-16 14:11:58.366645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-01-16 14:11:58.366655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-01-16 14:11:58.366664: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-01-16 14:11:58.366674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-16 14:11:58.366709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:11:58.367016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:11:58.367298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0
2020-01-16 14:11:58.367315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-16 14:11:58.367319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 
2020-01-16 14:11:58.367323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N 
2020-01-16 14:11:58.367372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:11:58.367682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:11:58.367971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8422 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-01-16 14:11:58.687280: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: graph_to_optimize
2020-01-16 14:11:58.687309: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: Graph size after: 1511 nodes (0), 3113 edges (0), time = 51.53ms.
2020-01-16 14:11:58.687313: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: Graph size after: 1511 nodes (0), 3113 edges (0), time = 55.71ms.
2020-01-16 14:11:58.687315: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: yolact_yolact_detection_map_while_cond_24503
2020-01-16 14:11:58.687319: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-01-16 14:11:58.687322: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-16 14:11:58.687324: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: yolact_yolact_detection_map_while_body_24504
2020-01-16 14:11:58.687327: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
2020-01-16 14:11:58.687330: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-16 14:12:00.043387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:12:00.043733: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-01-16 14:12:00.043796: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-01-16 14:12:00.044138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:12:00.044457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-01-16 14:12:00.044485: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-01-16 14:12:00.044496: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-01-16 14:12:00.044505: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-01-16 14:12:00.044515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-01-16 14:12:00.044524: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-01-16 14:12:00.044533: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-01-16 14:12:00.044542: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-16 14:12:00.044572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:12:00.044887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:12:00.045176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0
2020-01-16 14:12:00.045194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-16 14:12:00.045199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 
2020-01-16 14:12:00.045202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N 
2020-01-16 14:12:00.045251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:12:00.045555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:12:00.045994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8422 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-01-16 14:12:06.926724: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: graph_to_optimize
2020-01-16 14:12:06.926752: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 955 nodes (-556), 2636 edges (-445), time = 208.331ms.
2020-01-16 14:12:06.926756: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 955 nodes (0), 2636 edges (0), time = 80.347ms.
2020-01-16 14:12:06.926759: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: yolact_yolact_detection_map_while_cond_24503_frozen
2020-01-16 14:12:06.926762: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 19 nodes (0), 8 edges (0), time = 0.333ms.
2020-01-16 14:12:06.926765: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 19 nodes (0), 8 edges (0), time = 0.201ms.
2020-01-16 14:12:06.926767: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: yolact_yolact_detection_map_while_body_24504_frozen
2020-01-16 14:12:06.926770: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 4210 nodes (-252), 5496 edges (-407), time = 6253.95508ms.
2020-01-16 14:12:06.926773: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 4210 nodes (0), 5496 edges (0), time = 99.047ms.
Traceback (most recent call last):
  File ""/home/chengxu/deeplearning/tensorflow-2.0-study/yolact.py"", line 1567, in <module>
    tflite_model = converter.convert()
  File ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"", line 490, in convert
    **converter_kwargs)
  File ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 476, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 215, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
WARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .
2020-01-16 14:12:08.249581: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:108] Ignored output_format.
2020-01-16 14:12:08.249608: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:114] Ignored drop_control_dependency.
2020-01-16 14:12:08.674483: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-16 14:12:08.698096: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 3600000000 Hz
2020-01-16 14:12:08.698483: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bf05e1f380 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-01-16 14:12:08.698496: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-01-16 14:12:08.700080: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-01-16 14:12:08.752405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:12:08.752789: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bf05e3f110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-01-16 14:12:08.752801: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2020-01-16 14:12:08.752929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:12:08.753249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-01-16 14:12:08.753413: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-01-16 14:12:08.754500: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-01-16 14:12:08.755559: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-01-16 14:12:08.755751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-01-16 14:12:08.756823: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-01-16 14:12:08.757307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-01-16 14:12:08.759489: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-16 14:12:08.759597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:12:08.759997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:12:08.760301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0
2020-01-16 14:12:08.760338: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-01-16 14:12:08.760970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-16 14:12:08.760980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 
2020-01-16 14:12:08.760999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N 
2020-01-16 14:12:08.761074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:12:08.761435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:12:08.761758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8039 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
loc(callsite(""yolact/yolact_detection/map/TensorArrayV2_5""(""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/ops/map_fn.py"":425:0) at callsite(""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py"":574:0 at callsite(""/home/chengxu/deeplearning/tensorflow-2.0-study/utils/shape_utils.py"":228:0 at callsite(""/home/chengxu/deeplearning/tensorflow-2.0-study/core/post_processing.py"":476:0 at callsite(""/home/chengxu/deeplearning/tensorflow-2.0-study/yolact.py"":1118:0 at callsite(""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py"":308:0 at callsite(""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"":785:0 at callsite(""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"":918:0 at callsite(""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"":744:0 at ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"":785:0)))))))))): error: operand type 'tensor<i32>' is not compatible with preceding operands; expected rank: 1
Traceback (most recent call last):
  File ""/home/chengxu/anaconda3/envs/tf-nightly/bin/toco_from_protos"", line 8, in <module>
    sys.exit(main())
  File ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 56, in execute
    enable_mlir_converter)
Exception: /home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/ops/map_fn.py:425:7: error: operand type 'tensor<i32>' is not compatible with preceding operands; expected rank: 1
      name=name)
      ^
/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py:574:7: note: called from
      return func(*args, **kwargs)
      ^
/home/chengxu/deeplearning/tensorflow-2.0-study/utils/shape_utils.py:228:9: note: called from
        return tf.map_fn(fn, elems, dtype, parallel_iterations, back_prop)
        ^
/home/chengxu/deeplearning/tensorflow-2.0-study/core/post_processing.py:476:7: note: called from
      parallel_iterations=parallel_iterations)
      ^
/home/chengxu/deeplearning/tensorflow-2.0-study/yolact.py:1118:98: note: called from
                                                                                                 masks=mask_data)
                                                                                                 ^
/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py:308:7: note: called from
      return func(*args, **kwargs)
      ^
/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:785:19: note: called from
                  outputs = call_fn(cast_inputs, *args, **kwargs)
                  ^
/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py:918:11: note: called from
          output_tensors = layer(computed_tensors, **kwargs)
          ^
/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py:744:9: note: called from
        convert_kwargs_to_constants=base_layer_utils.call_context().saving)
        ^
/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:785:19: note: called from
                  outputs = call_fn(cast_inputs, *args, **kwargs)
                  ^

Process finished with exit code 1







if remove converter.experimental_new_converter = True

WARNING:absl:Please consider switching to use new converter by setting experimental_new_converter to true. Old converter (TOCO) is deprecated and flow will be switched on by default to use new converter soon.
Traceback (most recent call last):
  File ""/home/chengxu/deeplearning/tensorflow-2.0-study/yolact.py"", line 1567, in <module>
    tflite_model = converter.convert()
  File ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"", line 490, in convert
    **converter_kwargs)
  File ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 476, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 215, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
WARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .
2020-01-16 14:54:09.455905: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-16 14:54:09.477996: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 3600000000 Hz
2020-01-16 14:54:09.478678: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55641b1a91a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-01-16 14:54:09.478691: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-01-16 14:54:09.480333: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-01-16 14:54:09.539873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:54:09.540262: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55641b23d750 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-01-16 14:54:09.540278: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2020-01-16 14:54:09.540443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:54:09.540744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-01-16 14:54:09.540885: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-01-16 14:54:09.542169: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-01-16 14:54:09.543344: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-01-16 14:54:09.543537: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-01-16 14:54:09.544789: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-01-16 14:54:09.545460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-01-16 14:54:09.548110: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-16 14:54:09.548224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:54:09.548649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:54:09.548942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0
2020-01-16 14:54:09.548974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-01-16 14:54:09.549580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-16 14:54:09.549591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0 
2020-01-16 14:54:09.549598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N 
2020-01-16 14:54:09.549669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:54:09.550038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-16 14:54:09.550342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8020 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-01-16 14:54:09.621430: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve
2020-01-16 14:54:09.621470: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.621481: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve
2020-01-16 14:54:09.621489: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.621497: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve
2020-01-16 14:54:09.621505: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.621513: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve
2020-01-16 14:54:09.621520: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.621527: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve
2020-01-16 14:54:09.621535: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.621542: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor
2020-01-16 14:54:09.621553: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.622587: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor
2020-01-16 14:54:09.622602: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.622657: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor
2020-01-16 14:54:09.622667: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.622707: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor
2020-01-16 14:54:09.622716: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.622737: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor
2020-01-16 14:54:09.622744: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.622759: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While
2020-01-16 14:54:09.622776: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.622782: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.622788: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.622793: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.622798: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.622804: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.622809: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.622814: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.622819: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.622824: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21
2020-01-16 14:54:09.622833: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack
2020-01-16 14:54:09.622843: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack
2020-01-16 14:54:09.622852: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack
2020-01-16 14:54:09.622861: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack
2020-01-16 14:54:09.622871: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack
2020-01-16 14:54:09.641969: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 655 operators, 1254 arrays (0 quantized)
2020-01-16 14:54:09.652164: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 655 operators, 1254 arrays (0 quantized)
2020-01-16 14:54:09.701043: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 236 operators, 568 arrays (0 quantized)
2020-01-16 14:54:09.704528: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 233 operators, 562 arrays (0 quantized)
2020-01-16 14:54:09.707893: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 233 operators, 562 arrays (0 quantized)
2020-01-16 14:54:09.711235: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 233 operators, 562 arrays (0 quantized)
2020-01-16 14:54:09.713664: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 233 operators, 562 arrays (0 quantized)
2020-01-16 14:54:09.715367: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Identify nearest upsample.: 233 operators, 562 arrays (0 quantized)
2020-01-16 14:54:09.726982: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 18249152 bytes, theoretical optimal value: 11829248 bytes.
2020-01-16 14:54:09.727774: I tensorflow/lite/toco/toco_tooling.cc:471] Number of parameters: 8641389
2020-01-16 14:54:09.729089: E tensorflow/lite/toco/toco_tooling.cc:498] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, DIV, EXP, EXPAND_DIMS, FULLY_CONNECTED, GREATER_EQUAL, LESS, LOGISTIC, MAXIMUM, MINIMUM, MUL, PACK, REDUCE_MAX, REDUCE_MIN, RELU, RESHAPE, RESIZE_BILINEAR, STRIDED_SLICE, SUB, SUM, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.
Traceback (most recent call last):
  File ""/home/chengxu/anaconda3/envs/tf-nightly/bin/toco_from_protos"", line 8, in <module>
    sys.exit(main())
  File ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/chengxu/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 56, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, DIV, EXP, EXPAND_DIMS, FULLY_CONNECTED, GREATER_EQUAL, LESS, LOGISTIC, MAXIMUM, MINIMUM, MUL, PACK, REDUCE_MAX, REDUCE_MIN, RELU, RESHAPE, RESIZE_BILINEAR, STRIDED_SLICE, SUB, SUM, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.

Process finished with exit code 1

"
35926,Install Tensorflow 1.15 on CUDA 10.2 and cudnn 7.6,"**System information**
- OS Platform and Distribution: **Linux Ubuntu 16.04**
- TensorFlow installed from (source or binary): **Binary**
- TensorFlow version: **1.15**
- Python version: **3.6**
- Installed using virtualenv? pip? conda?: **Conda**
- CUDA/cuDNN version: CUDA 10.2 and cuDNN 7.6
- GPU model and memory: Tesla K80 


**Describe the problem**

I have an exisiting code base that uses Tensorflow 1.15. I want to train my model on Azure Instance that uses Tesla K80. I've also installed CUDA and cuDNN versions mentioned above, globally as well as through anaconda. I want to install  the specific version of the tensorflow, is this possible? Can I get prebuilt-binaries for the same?

Initially I tried installing through **conda install tensorflow-gpu**, I get the following error while running my script.

`Loaded runtime CuDNN library: 7605 (compatibility version 7600) but source was compiled with 7102`

**Any other info / logs**
`System info:
--------------------------------------------------------------------------------
__Time Stamp__
2020-01-16 06:18:49.664813

__Hardware Information__
Machine                                       : x86_64
CPU Name                                      : haswell
CPU count                                     : 24
CFS restrictions                              : None
CPU Features                                  :
64bit aes avx avx2 bmi bmi2 cmov cx16 f16c fma fsgsbase invpcid lzcnt mmx movbe
pclmul popcnt rdrnd sahf sse sse2 sse3 sse4.1 sse4.2 ssse3 xsave xsaveopt

__OS Information__
Platform                                      : Linux-4.15.0-1066-azure-x86_64-with-debian-stretch-sid
Release                                       : 4.15.0-1066-azure
System Name                                   : Linux
Version                                       : #71-Ubuntu SMP Thu Dec 12 20:35:32 UTC 2019
OS specific info                              : debianstretch/sid
glibc info                                    : glibc 2.9

__Python Information__
Python Compiler                               : GCC 7.3.0
Python Implementation                         : CPython
Python Version                                : 3.6.10
Python Locale                                 : en_US UTF-8

__LLVM information__
LLVM version                                  : 8.0.0

__CUDA Information__
Found 4 CUDA devices
id 0            b'Tesla K80'                              [SUPPORTED]
                      compute capability: 3.7
                           pci device id: 0
                              pci bus id: 0
id 1            b'Tesla K80'                              [SUPPORTED]
                      compute capability: 3.7
                           pci device id: 0
                              pci bus id: 0
id 2            b'Tesla K80'                              [SUPPORTED]
                      compute capability: 3.7
                           pci device id: 0
                              pci bus id: 0
id 3            b'Tesla K80'                              [SUPPORTED]
                      compute capability: 3.7
                           pci device id: 0
                              pci bus id: 0
Summary:
        4/4 devices are supported
CUDA driver version                           : 10020
CUDA libraries:
Finding cublas from Conda environment
        named  libcublas.so.10.2.2.89
        trying to open library...       ok
Finding cusparse from Conda environment
        named  libcusparse.so.10.3.1.89
        trying to open library...       ok
Finding cufft from Conda environment
        named  libcufft.so.10.1.2.89
        trying to open library...       ok
Finding curand from Conda environment
        named  libcurand.so.10.1.2.89
        trying to open library...       ok
Finding nvvm from Conda environment
        named  libnvvm.so.3.3.0
        trying to open library...       ok
Finding libdevice from Conda environment
        searching for compute_20...     ok
        searching for compute_30...     ok
        searching for compute_35...     ok
        searching for compute_50...     ok

__ROC Information__
ROC available                                 : False
Error initialising ROC due to                 : No ROC toolchains found.
No HSA Agents found, encountered exception when searching:
Error at driver init:
NUMBA_HSA_DRIVER /opt/rocm/lib/libhsa-runtime64.so is not a valid file path.  Note it must be a filepath of the .so/.dll/.dylib or the driver:

__SVML Information__
SVML state, config.USING_SVML                 : False
SVML library found and loaded                 : False
llvmlite using SVML patched LLVM              : True
SVML operational                              : False

__Threading Layer Information__
TBB Threading layer available                 : True
OpenMP Threading layer available              : False
+--> Disabled due to                          : Unknown import problem.
Workqueue Threading layer available           : True

__Numba Environment Variable Information__
None set.

__Conda Information__
conda_build_version                           : 3.18.9
conda_env_version                             : 4.8.1
platform                                      : linux-64
python_version                                : 3.7.4.final.0
root_writable                                 : True

__Current Conda Env__
_libgcc_mutex             0.1                        main
absl-py                   0.9.0                    pypi_0    pypi
astor                     0.8.1                    pypi_0    pypi
ca-certificates           2019.11.27                    0    anaconda
certifi                   2019.11.28               py36_0    anaconda
cudatoolkit               10.2.89              hfd86e86_0    anaconda
cudnn                     7.6.5                cuda10.2_0    anaconda
gast                      0.2.2                    pypi_0    pypi
google-pasta              0.1.8                    pypi_0    pypi
grpcio                    1.26.0                   pypi_0    pypi
h5py                      2.10.0                   pypi_0    pypi
keras-applications        1.0.8                    pypi_0    pypi
keras-preprocessing       1.1.0                    pypi_0    pypi
ld_impl_linux-64          2.33.1               h53a641e_7
libedit                   3.1.20181209         hc058e9b_0
libffi                    3.2.1                hd88cf55_4
libgcc-ng                 9.1.0                hdf63c60_0
libstdcxx-ng              9.1.0                hdf63c60_0
llvmlite                  0.31.0                   pypi_0    pypi
markdown                  3.1.1                    pypi_0    pypi
ncurses                   6.1                  he6710b0_1
numba                     0.47.0                   pypi_0    pypi
numpy                     1.18.1                   pypi_0    pypi
openssl                   1.1.1                h7b6447c_0    anaconda
opt-einsum                3.1.0                    pypi_0    pypi
pip                       19.3.1                   py36_0
protobuf                  3.11.2                   pypi_0    pypi
python                    3.6.10               h0371630_0
readline                  7.0                  h7b6447c_5
setuptools                44.0.0                   py36_0
six                       1.14.0                   pypi_0    pypi
sqlite                    3.30.1               h7b6447c_0
tensorboard               1.15.0                   pypi_0    pypi
tensorflow-estimator      1.15.1                   pypi_0    pypi
termcolor                 1.1.0                    pypi_0    pypi
tk                        8.6.8                hbc83047_0
werkzeug                  0.16.0                   pypi_0    pypi
wheel                     0.33.6                   py36_0
wrapt                     1.11.2                   pypi_0    pypi
xz                        5.2.4                h14c3975_4
zlib                      1.2.11               h7b6447c_3
--------------------------------------------------------------------------------`"
35925,Dataset with Keras Functional Model: tuple index out of range uin steps_per_epoch,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu, Newest
- TensorFlow installed from (source or binary): **Pip (Binary)**
- TensorFlow version (use command below): **2.1**
- Python version: Python **3.7.6**
- CUDA/cuDNN version: **10.2**
- GPU model and memory: **GeForce 1070 8Gb**

**Describe the current behavior**
Passing in a dataset to model.fit from a model generated with tf.Keras layers results in **IndexError: tuple index out of range.** Error both with custom TFRecord dataset and datasets derived from tensorflow-datasets installed via pip. Looks like it is in the standardize_input_data function but since it is an instance of DatasetV2 it should not be hitting that if statement...

**Describe the expected behavior**
Keras models should accept tf DataSets.

**Code to reproduce the issue**
```Python
from tensorflow.keras.layers import Dense, Embedding, Flatten, Lambda, Subtract, Input, Concatenate, Average, Reshape, GlobalAveragePooling1D, Dot, Dropout
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.utils import Sequence
from tensorflow.keras import initializers

import tensorflow_datasets as tfds
tfds.list_builders()
dataset, info = tfds.load(""mnist"", with_info=True)
inputs = Input((28, 28, 1), name=""image"")
First = Dense(128, activation=""relu"")
Second = Dropout(0.2)
Third = Dense(10, activation=""softmax"", name=""label"")

first = First(inputs)
second = Second(first)
third = Third(second)
model = Model(inputs=[inputs], outputs=[third])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(dataset['train'].batch(4096))
```

**Other info / logs**
```
~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing,
**kwargs)
    340                 mode=ModeKeys.TRAIN,
    341                 training_context=training_context,
--> 342                 total_epochs=epochs)
    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    344 

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    126         step=step, mode=mode, size=current_batch_size) as batch_logs:
    127       try:
--> 128         batch_outs = execution_function(iterator)
    129       except (StopIteration, errors.OutOfRangeError):
    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
     96     # `numpy` translates Tensors to values in Eager mode.
     97     return nest.map_structure(_non_none_constant_value,
---> 98                               distributed_function(input_fn))
     99 
    100   return execution_function

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    566         xla_context.Exit()
    567     else:
--> 568       result = self._call(*args, **kwds)
    569 
    570     if tracing_count == self._get_tracing_count():

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    613       # This is the first call of __call__, so we have to initialize.
    614       initializers = []
--> 615       self._initialize(args, kwds, add_initializers_to=initializers)
    616     finally:
    617       # At this point we know that the initialization is complete (or less

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    495     self._concrete_stateful_fn = (
    496         self._stateful_fn._get_concrete_function_internal_garbage_collected( 
# pylint: disable=protected-access
--> 497             *args, **kwds))
    498 
    499     def invalid_creator_scope(*unused_args, **unused_kwds):

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args,
**kwargs)    2387       args, kwargs = None, None    2388     with self._lock:
-> 2389       graph_function, _, _ = self._maybe_define_function(args, kwargs)    2390     return graph_function    2391 

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)    2701     2702       self._function_cache.missed.add(call_context_key)
-> 2703       graph_function = self._create_graph_function(args, kwargs)    2704       self._function_cache.primary[cache_key] = graph_function    2705       return graph_function, args, kwargs

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)    2591             arg_names=arg_names,    2592             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2593             capture_by_value=self._capture_by_value),    2594         self._function_attributes,    2595         # Tell the ConcreteFunction to clean up its graph once it goes out of

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    976                                           converted_func)
    977 
--> 978       func_outputs = python_func(*func_args, **func_kwargs)
    979 
    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    437         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    438         # the function a weak reference to itself to avoid a reference cycle.
--> 439         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    440     weak_wrapped_fn = weakref.ref(wrapped_fn)
    441 

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in distributed_function(input_iterator)
     83     args = _prepare_feed_values(model, input_iterator, mode, strategy)
     84     outputs = strategy.experimental_run_v2(
---> 85         per_replica_function, args=args)
     86     # Out of PerReplica outputs reduce or pick values to return.
     87     all_outputs = dist_utils.unwrap_output_dict(

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)
    761       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),
    762                                 convert_by_default=False)
--> 763       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    764 
    765   def reduce(self, reduce_op, value, axis):

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)    1817       kwargs
= {}    1818     with self._container_strategy().scope():
-> 1819       return self._call_for_each_replica(fn, args, kwargs)    1820     1821   def _call_for_each_replica(self, fn, args, kwargs):

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)    2162         self._container_strategy(),    2163         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):
-> 2164       return fn(*args, **kwargs)    2165     2166   def _reduce_to(self, reduce_op, value, destinations):

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    290   def wrapper(*args, **kwargs):
    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
--> 292       return func(*args, **kwargs)
    293 
    294   if inspect.isfunction(func) or inspect.ismethod(func):

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics, standalone)
    414   x, y, sample_weights = model._standardize_user_data(
    415       x, y, sample_weight=sample_weight, class_weight=class_weight,
--> 416       extract_tensors_from_dataset=True)
    417   batch_size = array_ops.shape(nest.flatten(x, expand_composites=True)[0])[0]
    418   # If `model._distribution_strategy` is True, then we are in a replica context

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)    2381         is_dataset=is_dataset,   2382         class_weight=class_weight,
-> 2383         batch_size=batch_size)    2384     2385   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)    2467           shapes=None,    2468           check_batch_axis=False,  # Don't enforce the batch size.
-> 2469           exception_prefix='target')    2470     2471       # Generate sample-wise weight values given the `sample_weight` and

~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    510                        'for each key in: ' + str(names))
    511   elif isinstance(data, (list, tuple)):
--> 512     if isinstance(data[0], (list, tuple)):
    513       data = [np.asarray(d) for d in data]
    514     elif len(names) == 1 and isinstance(data[0], (float, int)):

IndexError: tuple index out of range
```"
35923,"Here is a list of operators for which you will need custom implementations: BatchMatMul, Erf","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): tf-nightly 1.15


**Command used to run the converter or code if you’re using the Python API**

    input_arrays = [""inputs/input_ids"", ""inputs/segment_ids"", ""inputs/input_mask""]
    output_arrays = [""loss/prob""]
    converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)
    # Weight quantization
    converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
    tflite_quant_model = converter.convert()
    tflite_model = converter.convert()

**The output from the converter invocation**


```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, FULLY_CONNECTED, GATHER, MEAN, MUL, ONE_HOT, PACK, RESHAPE, RSQRT, SLICE, SOFTMAX, SQUARED_DIFFERENCE, SQUEEZE, STRIDED_SLICE, SUB, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: BatchMatMul, Erf.
```

I got the error, then I add  ""converter.allow_custom_ops = True"".  It's success.
But when i use the .lite model to interpreter, I got the new error:

```
RuntimeError: Encountered unresolved custom op: Erf.Node number 191 (Erf) failed to prepare.
```

Please help me! Thanks
"
35922,build failed at master branch,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: master 2.1.0
- Python version: 3.7.6
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 1.2.1
- GCC/Compiler version (if compiling from source):7.4.0
- CUDA/cuDNN version: 10.2 / 7.6.5
- GPU model and memory:
GTX1080Ti GDDR5X 11GB


**Describe the problem**
 
Build error

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
At global scope:
cc1plus: warning: unrecognized command line option ‘-Wno-self-assign’
ERROR: /home/wmind/repo/tensorflow/tensorflow/python/keras/api/BUILD:116:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)
2020-01-15 18:41:58.956645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2
Traceback (most recent call last):
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 776, in <module>
    main()
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 772, in main
    lazy_loading, args.use_relative_imports)
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 629, in create_api_files
    compat_api_versions, lazy_loading, use_relative_imports)
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 503, in get_api_init_text
    _, attr = tf_decorator.unwrap(attr)
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py"", line 219, in unwrap
    elif _has_tf_decorator_attr(cur):
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py"", line 124, in _has_tf_decorator_attr
    hasattr(obj, '_tf_decorator') and
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py"", line 62, in __getattr__
    module = self._load()
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py"", line 45, in _load
    module = importlib.import_module(self.__name__)
  File ""/home/wmind/anaconda3/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py"", line 28, in <module>
    _wrap_py_utils = swig_import_helper()
  File ""/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_wrap_py_utils', fp, pathname, description)
  File ""/home/wmind/anaconda3/lib/python3.7/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/wmind/anaconda3/lib/python3.7/imp.py"", line 342, in load_dynamic
    return _load(spec)
  File ""<frozen importlib._bootstrap>"", line 696, in _load
  File ""<frozen importlib._bootstrap>"", line 670, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 583, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 1043, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: /home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/wmind/repo/tensorflow/tensorflow/tools/pip_package/BUILD:41:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)
INFO: Elapsed time: 3197.242s, Critical Path: 395.57s
INFO: 18184 processes: 18184 local.
FAILED: Build did NOT complete successfully
```

"
35921,Conv2DTranspose shape gets None when exporting SavedModel,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): TF2.1
- Python version: 3.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.2/7.6
- GPU model and memory: V100, 32Gb

**Describe the current behavior**
The shape information after Conv2DTranspose layer is incomplete and the spatial dimensions are missing when exporting the saved model.

**Describe the expected behavior**
The shape information should include the spatial dimensions when exporting the saved model.

**Code to reproduce the issue**
```
import tensorflow as tf

def _crop_and_concat(inputs, residual_input):
  factor = inputs.get_shape().dims[1].value / residual_input.get_shape().dims[1].value
  return tf.concat([inputs, tf.image.central_crop(residual_input, factor)], axis=-1)

class UNet(tf.keras.Model):
  def __init__(self, name):
    super(UNet, self).__init__(name)
    self.conv1 = tf.keras.layers.Conv2D(filters=8,
                                        kernel_size=(3, 3),
                                        activation=tf.nn.relu)
    self.conv2 = tf.keras.layers.Conv2D(filters=8,
                                        kernel_size=(3, 3),
                                        activation=tf.nn.relu)
    self.maxpool = tf.keras.layers.MaxPool2D(pool_size=(2, 2),
                                             strides=2)
    self.deconv = tf.keras.layers.Conv2DTranspose(filters=16,
                                                  kernel_size=(2, 2),
                                                  strides=(2, 2),
                                                  padding='same',
                                                  activation=tf.nn.relu)
    self.conv3 = tf.keras.layers.Conv2D(filters=8,
                                        kernel_size=(3, 3),
                                        activation=tf.nn.relu)

  @tf.function
  def call(self, x):
    print("">>> Input Shape"", x.shape)
    out = self.conv1(x)
    print("">>> conv1 Shape"", out.shape)
    skip = self.conv2(out)
    print("">>> conv2 Shape"", skip.shape)
    out = self.maxpool(skip)
    print("">>> maxpool Shape"", out.shape)
    out = self.deconv(out)
    # the deconv shape will be (None, None, None, 16) when exporting saved model
    print("">>> deconv Shape"", out.shape)
    out = self.conv3(out)
    out = _crop_and_concat(out, skip)

    return out


model = UNet(""dummy"")

res = model.predict(tf.ones((1, 400, 400, 1)))

print(""Finish prediction"")

tf.keras.models.save_model(model, ""/results/SavedModel"",
    save_format=""tf"", overwrite=True, include_optimizer=False)
```

**Other info / logs**
```
>>> Input Shape (None, 400, 400, 1)
>>> conv1 Shape (None, 398, 398, 8)
>>> conv2 Shape (None, 396, 396, 8)
>>> maxpool Shape (None, 198, 198, 8)
>>> deconv Shape (None, 396, 396, 16)
>>> Input Shape (None, 400, 400, 1)
>>> conv1 Shape (None, 398, 398, 8)
>>> conv2 Shape (None, 396, 396, 8)
>>> maxpool Shape (None, 198, 198, 8)
>>> deconv Shape (None, 396, 396, 16)
Finish prediction
>>> Input Shape (None, 400, 400, 1)
>>> conv1 Shape (None, 398, 398, 8)
>>> conv2 Shape (None, 396, 396, 8)
>>> maxpool Shape (None, 198, 198, 8)
>>> deconv Shape (None, **None, None**, 16)
... <Then the TF crashes and complains about the None value since we need it to compute the fraction in _crop_and_concat()> ...
```
The above log shows that in the prediction, all shapes are correctly inferred. But when we are exporting the saved model: the shape becomes incomplete and the spatial info are lost only after the deconv (Conv2DTranspose) and all the other layers still looks fine with correct shape info. So, we have two questions:
(1) Why do we need to calculate the shapes again when exporting the saved model?
(2) Why is the spatial info lost only after deconv? And this one looks like a bug.

FYI @nluehr  

"
35920,TypeError: object of type 'NoneType' has no len(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): from pip (binary?)
- TensorFlow version (use command below): 2.1
- Python version: 3.7
- CUDA/cuDNN version: no
- GPU model and memory: no

**Describe the current behavior and code to reproduce**

I am running this code:

https://gist.github.com/cossio/dc761b2731b1428267b65333dbd3f321

to train an RBM by contrastive divergence. Briefly I am using the Keras sub-classing API and I view the multiple Monte Carlo samples as layers.

However it produces the following error:

```
TypeError: object of type 'NoneType' has no len()
```

Please find the full error message and stack-trace as a comment on the above gist link."
35917,Compilation of TF2.1 with CUDA and TensorRT fails on Linux ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.1.0
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source): GCC 7.4.0
- CUDA/cuDNN version: 10.1,10.2 CuDNN 7
- GPU model and memory: NVidia GTX 1080 Ti
- TensorRT: version 6 or 7.

**Describe the current behavior**
Compiling from source, as indicated in the official tensorflow manual, compilation fails, with the error listed in the log below.
**Describe the expected behavior**
Compilation should successfully complete

**Code to reproduce the issue**
No code needed. After a checkout of the current v.2.1.0, configuration is carried out with no changes from default (other than enabling CUDA). 

**Other info / logs**
Log: see attached log.txt
[log.txt](https://github.com/tensorflow/tensorflow/files/4068010/log.txt)

                                                                                                                                                   
"
35911,Keras model.fit not calling Sequence.on_epoch_end(),"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- TensorFlow installed from (source or binary): binary from pip install tensorflow
- TensorFlow version: v2.1.0-rc2-17-ge5bf8de 2.1.0
- Python version: 3.6.7

**Describe the current behavior**
on_epoch_end() from a tf.keras.utils.Sequence is ***not*** called on epoch end while training using model.fit().

**Describe the expected behavior**
on_epoch_end() from a tf.keras.utils.Sequence should be called on epoch end while training using model.fit().

**Code to reproduce the issue**
```python
import tensorflow as tf
import numpy as np


class ZerosFirstEpochOnesAfter(tf.keras.utils.Sequence):
    def __init__(self):
        self.is_epoch_0 = True

    def __len__(self):
        return 2

    def on_epoch_end(self):
        print('on_epoch_end')
        self.is_epoch_0 = False

    def __getitem__(self, item):
        if self.is_epoch_0:
            print(""First epoch"")
            return np.zeros((16, 1)), np.zeros((16,))
        else:
            return np.ones((16, 1)), np.ones((16,))


if __name__ == '__main__':
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(1, input_dim=1, activation=""softmax""))

    model.compile(
        optimizer='Adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    model.fit(ZerosFirstEpochOnesAfter(), epochs=5,)
```

**Other info / logs**

Note that print('on_epoch_end') is not called anywhere and ""First epoch"" is printed every epoch.

Relevant api docs:
https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence?version=stable

```
WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to
  ['...']
Train for 2 steps
Epoch 1/5
First epoch
First epoch
2020-01-15 17:57:31.979489: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2/2 [==============================] - 1s 260ms/step - loss: 15.2492 - accuracy: 0.0000e+00
Epoch 2/5
First epoch
First epoch
2/2 [==============================] - 0s 6ms/step - loss: 15.2492 - accuracy: 0.0000e+00
Epoch 3/5
First epoch
First epoch
2/2 [==============================] - 0s 6ms/step - loss: 15.2492 - accuracy: 0.0000e+00
Epoch 4/5
First epoch
First epoch
2/2 [==============================] - 0s 6ms/step - loss: 15.2492 - accuracy: 0.0000e+00
Epoch 5/5
First epoch
First epoch
2/2 [==============================] - 0s 6ms/step - loss: 15.2492 - accuracy: 0.0000e+00
```"
35910,Issue with the Interpreter on Android ,"I've to files of the same model that contains the same bytes. I'm trying to initialize the interpreter with both of them, but only with one works. I attach below the two files, only the indentation  seems different but the row bytes at the inside are the same. The first file was created in python saving the converted model, the second was created passing the raw bytes throw the network an storing them in a new file in kotlin
[Archive.zip](https://github.com/tensorflow/tensorflow/files/4067466/Archive.zip)

"
35909,OSError: SavedModel file does not exist at: main.h5/{saved_model.pbtxt|saved_model.pb},"**System information**
- Custom code provided below
- OS Platform and Distribution : Linux Raspbian
- Raspberry pi
- TensorFlow installed from apt repos:
- TensorFlow version (use command below): tensorflow 1.14
- Python version: python 3

**Describe the current behavior**
At running, the following line raise the following error:
``` python
face_recognition_model = tf.keras.models.load_model('face_recognition_model.h5')
```

> 
> OSError: SavedModel file does not exist at: main.h5/{saved_model.pbtxt|saved_model.pb}
> **Describe the expected behavior**
Expected to load the model for later prediction

**Code to reproduce the issue**
``` python

detection_graph = tf.Graph()
with detection_graph.as_default():
    od_graph_def = tf.GraphDef()
    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name='')
label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map,
                                                            max_num_classes=NUM_CLASSES,
                                                            use_display_name=True)
category_index = label_map_util.create_category_index(categories)
face_recognition_model = tf.keras.models.load_model('face_recognition_model.h5')
```

**Other info / logs**
I provided a snippet to reproduce that error. The all code is using a mobilnet trained by transfer learning to detect faces and a classification network is trained and used on keras for facial recognition. The code is working just fine on my computer and i have absolutely no clue why it's not working on my raspberry pi. I can provide the complete code if it's required.
"
35906,global numbering scheme of  tf.keras.layers.BatchNormalization layers seems like a wrong strategy,"<em>I trained a model in Tensorflow and used 36 convolution layers, each holding a batch normalization layer. What I do not understand is why the numbering of batch normalization layer is global instead of starting from 0 for every name scope? 
I am not going to create the layers with the same name_scope in the same order in every model, sometimes I will insert a few layers before a particular layer and would like to restore the batch norm parameters from other model checkpoint, but it's not possible because batch norm layer is numbered globally, and because I created this layer at a step numerically bigger than the step at which I created the same layer in other model, the names will be different and I won't be able to restore parameters in the traditional way. </em>

**System information**
-
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint
- TensorFlow installed from (source or binary): tf-gpu anaconda 
- TensorFlow version : tf.2.0.0
- Python version: 3.7.4


Here is the list of batch norm parameters in my original model, mind the layer: batch_normalization_18 , this name is batch_normalization_18 because this is the 19th (18 is in numbering because index starts from 0) batchnorm layer created.

['Train/vgg_deconv_RGB_LSTM/Block1_conv1/batch_normalization/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block1_conv1/batch_normalization/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block1_conv1/batch_normalization/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block1_conv1/batch_normalization/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block1_conv2/batch_normalization_1/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block1_conv2/batch_normalization_1/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block1_conv2/batch_normalization_1/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block1_conv2/batch_normalization_1/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block2_conv1/batch_normalization_2/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block2_conv1/batch_normalization_2/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block2_conv1/batch_normalization_2/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block2_conv1/batch_normalization_2/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block2_conv2/batch_normalization_3/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block2_conv2/batch_normalization_3/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block2_conv2/batch_normalization_3/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block2_conv2/batch_normalization_3/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block3_conv1/batch_normalization_4/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block3_conv1/batch_normalization_4/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block3_conv1/batch_normalization_4/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block3_conv1/batch_normalization_4/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block3_conv2/batch_normalization_5/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block3_conv2/batch_normalization_5/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block3_conv2/batch_normalization_5/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block3_conv2/batch_normalization_5/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block3_conv3/batch_normalization_6/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block3_conv3/batch_normalization_6/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block3_conv3/batch_normalization_6/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block3_conv3/batch_normalization_6/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block3_conv4/batch_normalization_7/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block3_conv4/batch_normalization_7/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block3_conv4/batch_normalization_7/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block3_conv4/batch_normalization_7/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block4_conv1/batch_normalization_8/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block4_conv1/batch_normalization_8/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block4_conv1/batch_normalization_8/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block4_conv1/batch_normalization_8/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block4_conv2/batch_normalization_9/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block4_conv2/batch_normalization_9/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block4_conv2/batch_normalization_9/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block4_conv2/batch_normalization_9/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block4_conv3/batch_normalization_10/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block4_conv3/batch_normalization_10/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block4_conv3/batch_normalization_10/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block4_conv3/batch_normalization_10/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block4_conv4/batch_normalization_11/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block4_conv4/batch_normalization_11/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block4_conv4/batch_normalization_11/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block4_conv4/batch_normalization_11/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block5_conv1/batch_normalization_12/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block5_conv1/batch_normalization_12/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block5_conv1/batch_normalization_12/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block5_conv1/batch_normalization_12/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block5_conv2/batch_normalization_13/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block5_conv2/batch_normalization_13/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block5_conv2/batch_normalization_13/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block5_conv2/batch_normalization_13/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block5_conv3/batch_normalization_14/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block5_conv3/batch_normalization_14/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block5_conv3/batch_normalization_14/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block5_conv3/batch_normalization_14/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block5_conv4/batch_normalization_15/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block5_conv4/batch_normalization_15/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block5_conv4/batch_normalization_15/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block5_conv4/batch_normalization_15/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block6_conv1/batch_normalization_16/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block6_conv1/batch_normalization_16/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block6_conv1/batch_normalization_16/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block6_conv1/batch_normalization_16/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Block6_conv2/batch_normalization_17/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Block6_conv2/batch_normalization_17/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Block6_conv2/batch_normalization_17/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Block6_conv2/batch_normalization_17/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_18/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_18/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_18/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_18/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_19/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_19/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_19/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_19/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv1/batch_normalization_20/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv1/batch_normalization_20/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv1/batch_normalization_20/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv1/batch_normalization_20/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv2/batch_normalization_21/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv2/batch_normalization_21/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv2/batch_normalization_21/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv2/batch_normalization_21/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv3/batch_normalization_22/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv3/batch_normalization_22/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv3/batch_normalization_22/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock5_conv3/batch_normalization_22/moving_variance:0']


Now, I created another model, where I am creating the same layers, but I am inserting two layers before this 19th layer, and hence, this will be given a different name just because of this reason. 

 ['Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_20/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_20/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_20/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_20/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_21/beta:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_21/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_21/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_21/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/extra_conv1_first/batch_normalization_18/beta:0',
 'Train/vgg_deconv_RGB_LSTM/extra_conv1_first/batch_normalization_18/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/extra_conv1_first/batch_normalization_18/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/extra_conv1_first/batch_normalization_18/moving_variance:0',
 'Train/vgg_deconv_RGB_LSTM/extra_conv2_second/batch_normalization_19/beta:0',
 'Train/vgg_deconv_RGB_LSTM/extra_conv2_second/batch_normalization_19/gamma:0',
 'Train/vgg_deconv_RGB_LSTM/extra_conv2_second/batch_normalization_19/moving_mean:0',
 'Train/vgg_deconv_RGB_LSTM/extra_conv2_second/batch_normalization_19/moving_variance:0']


You can see, now batch_normalization_18 name is given to another layer, and my original layer, which was given batch_normalization_18 in previous model, it is given the name of 
batch_normalization_20, even though all the variable scope and name scope is same. 

Should'nt it be like local numbering for every name_scope? What's the point behind global numbering? 

Please let me know if any more information is needed. 
"
35903,"ImportError: DLL load failed, module not found","Traceback (most recent call last):
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.#module not found#

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\laue\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
35902,Unexpected behavior calling tf.keras.Model.call() with named parameters,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `yes`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux Ubuntu 18.04.3 LTS`
- TensorFlow installed from (source or binary): `binary`
- TensorFlow version (use command below): `2.0.2`
- Python version: `Python 3.7.4 Anaconda`
- CUDA/cuDNN version: `ROCM`:
```
Package: rocm-libs
Version: 3.0.6
Priority: optional
Section: devel
Maintainer: Advanced Micro Devices Inc.
Installed-Size: 13.3 kB
Depends: rocfft, rocrand, rocblas, hipblas, rocsparse, hipsparse, rocalution, rocprim, rocthrust, hipcub
Homepage: https://github.com/RadeonOpenCompute/ROCm
Download-Size: 802 B
APT-Manual-Installed: yes
APT-Sources: http://repo.radeon.com/rocm/apt/debian xenial/main amd64 Packages
Description: Radeon Open Compute (ROCm) Runtime software stack
```
- GPU model and memory: AMD Radeon VII

The full environment script does not work for my machine, but:
```
`python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
```
```
>>> v2.0.0-rocm-3-g0826c3a 2.0.2
```

**Describe the current behavior**

I am getting odd behavior when calling a `tf.keras.Model`'s `call` method when using the names of the method's parameters. The method works as expected when using position only arguments, but breaks when using the names. However, when I call my `model_instance.call()` with the names of the method parameters, things work as expected. It is making me wonder which `__call__` method I am calling when simply running `model_instance()`.

**Describe the expected behavior**

Using the names of the parameters in a `tf.keras.Model`'s `call` method should not be raising an error.

**Code to reproduce the issue**

First, a little bit of setup showing that calling a `tf.keras.layers.Attention` instance from a function works with and without using the names of the positional arguments in a user defined function, `call`:
```python
import tensorflow as tf


def call(q, v, k, mask_q=None, mask_v=None):
    """""" Call attention instance """"""
    return attn(inputs=[q, v, k], mask=[mask_q, mask_v])

x = tf.random.uniform((1, 2, 2))
attn = tf.keras.layers.Attention(use_scale=True)
```
```python
# position arguments work well
call(x, x, x)
```
```
>>> <tf.Tensor: id=89, shape=(1, 2, 2), dtype=float32, numpy=
array([[[0.62968266, 0.6612503 ],
        [0.6235384 , 0.73767066]]], dtype=float32)>
```
```python
# naming the parameters also fine here
call(q=x, v=x, k=x)
```
```
>>> <tf.Tensor: id=89, shape=(1, 2, 2), dtype=float32, numpy=
array([[[0.62968266, 0.6612503 ],
        [0.6235384 , 0.73767066]]], dtype=float32)>
```
Things start getting weird when doing something similar within a `tf.keras.Model`:
```python
class MyAttention(tf.keras.Model):
    
    def __init__(self):
        super(MyAttention, self).__init__()
        self.attention = tf.keras.layers.Attention(use_scale=True)
        
    def call(self, q, v, k, mask_q=None, mask_v=None):
        return self.attention(inputs=[q, v, k], mask=[mask_q, mask_v])


my_attention = MyAttention()
```
```python
# Still works with positional arguments
my_attention(x, x, x)
```
```
>>> <tf.Tensor: id=106, shape=(1, 2, 2), dtype=float32, numpy=
array([[[0.62968266, 0.6612503 ],
        [0.6235384 , 0.73767066]]], dtype=float32)>
```
```python
# Breaks when naming the arguments in my_attention:
my_attention(q=x, v=x, k=x)
```
```
>>> 
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-15-5fa3b47998d9> in <module>
----> 1 needs_attention(q=x, v=x, k=x)

TypeError: __call__() missing 1 required positional argument: 'inputs'
```
Finally, if I explicitly call `my_attention.call()`:
```python
my_attention.call(q=x, v=x, k=x)
```
```
>>> <tf.Tensor: id=106, shape=(1, 2, 2), dtype=float32, numpy=
array([[[0.62968266, 0.6612503 ],
        [0.6235384 , 0.73767066]]], dtype=float32)>
```
**Other info / logs**
Here is a gist to show this behavior:
https://gist.github.com/yngtodd/f3bda25503a9611765ab33c1178db48c
"
35901,FusedBatchNormV3 and AddV2 need custom implementations.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro, Version 1903
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.2.89
- GPU model and memory: GeForce GTX 1050 2.00GiB

**Describe the current behavior**

I have created the simplest case of sequential neural network which only contains one Batch Normalization layer. Then I have tried to convert this model to Tensorflow Lite but then I got following message:

    Here is a list of operators for which you will need custom implementations: FusedBatchNormV3.

When I tried to set parameter `fused=False` I got following message:

    Here is a list of operators for which you will need custom implementations: AddV2.

**Describe the expected behavior**

I expect converted Tensorflow Lite model with optimized Batch Normalization weights.

**Code to reproduce the issue**

    import tensorflow as tf

    tflite_path = 'test.tflite'
    keras_path = 'test.h5'

    bn = tf.keras.layers.BatchNormalization(
        # fused=False,
        input_shape = (128,128,3)
    )

    # Create model
    k_model = tf.keras.models.Sequential()
    k_model.add(bn)
    k_model.save(keras_path)
    
    # Convert keras model to tflite
    converter = tf.lite.TFLiteConverter.from_keras_model(k_model)
    tflite_model = converter.convert()
    open(tflite_path, 'wb').write(tflite_model)

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35900,Dataset padded_batch does not work as documented,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): A little
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (ubuntu-based)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0-rc1
- Python version:  3.7

**Describe the current behavior**

calling `Dataset.padded_batch([batch_size], [output_shape], padding_values=1)`  fails with the following error:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-38-6fccac1ccecf> in <module>()
     20 ds_train = ds_train.padded_batch(BATCH_SIZE, padded_shapes)
     21 
---> 22 ds_test = ds_test.padded_batch(BATCH_SIZE, padded_shapes, padding_values=padded_values)

3 frames
/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/util/nest.py in assert_shallow_structure(shallow_tree, input_tree, check_types)
    297       raise TypeError(
    298           ""If shallow structure is a sequence, input must also be a sequence. ""
--> 299           ""Input has type: %s."" % type(input_tree))
    300 
    301     if check_types and not isinstance(input_tree, type(shallow_tree)):

TypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: <class 'int'>.
```

Note that this does not fail if one uses the default value of `None`

**Describe the expected behavior**

Should pad the data with the value in `padding_values`.  

Also, the error message could be friendly by telling me what type it expects. 

**Code to reproduce the issue**

```
import tensorflow as tf
import tensorflow_datasets as tfds

BATCH_SIZE = 64

ds, ds_info = tfds.load(""imdb_reviews/subwords8k"", with_info=True, as_supervised=True)
ds_train, ds_test = ds[""train""], ds[""test""]

output_shapes_train = tf.compat.v1.data.get_output_shapes(ds_train)
padded_shapes = output_shapes_train  # (TensorShape([None]), TensorShape([]))
padded_values = -1

ds_train = ds_train.padded_batch(BATCH_SIZE, padded_shapes)  # does not fail here
ds_test = ds_test.padded_batch(BATCH_SIZE, padded_shapes, padding_values=padded_values)  # but does fail here
```


**Other info / logs**
https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#padded_batch

Documentation seems pretty clear that the second part should work."
35899,keras and tf.keras has different output,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**


- ubuntu 16.04
I create two conda virtualenv.
first:
- tensorflow2.1  gpu  installed by pip
- python 3.6
- cuda 10.1 installed by conda
- cudnn 7.6 installed by conda 
- gpu Nvidia TitanXp with 12GB memory
second:
- tensorflow1.15 cpu 
- keras 2.2.4 installed by conda 
- python 3.6

**Describe the current behavior**

**In the first virtualenv,I used code as below shows:**
`from sklearn.datasets import make_regression
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import SGD
from matplotlib import pyplot

X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)

n_train = 500
trainX, testX = X[:n_train, :], X[n_train:, :]
trainy, testy = y[:n_train], y[n_train:]

model = Sequential()
model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))
model.add(Dense(1, activation='linear'))

opt = SGD(lr=0.01, momentum=0.9,clipnorm=1)
model.compile(loss='mean_squared_error', optimizer=opt)

history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0)

train_mse = model.evaluate(trainX, trainy, verbose=0)
test_mse = model.evaluate(testX, testy, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))

pyplot.title('Mean Squared Error')
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()`
**In the second virtualenv,I just change the code from tensorflow.keras to keras**

**Describe the expected behavior**
`opt = SGD(lr=0.01, momentum=0.9,clipnorm=1)` this code with and without  parameter clipnorm
will have different performance in second virtualenv(keras) but the first virtualenv(tensorflow2.1 tf.keras) just show same output.So is the parameter clipnorm is useless in tensorflow2.1 ?

"
35897,tensorflow program crashes with `CUDA_ERROR_ILLEGAL_ADDRESS` with `tf.boolean_mask` used.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): docker image `nvcr.io/nvidia/tensorflow:19.09-py3`
- ~Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:~
- TensorFlow installed from (source or binary): see docker image
- TensorFlow version (use command below): 1.14.0+nv
- Python version: 3.6.8
- ~Bazel version (if compiling from source):~
- ~GCC/Compiler version (if compiling from source):~
- CUDA/cuDNN version: 10.1
- GPU model and memory: **Tesla V100-PCIE-16GB**

**Describe the current behavior**
with the following code added in `model_fn`, program crashes with `CUDA_ERROR_ILLEGAL_ADDRESS`:
```python
           def create_mean_metrics(t_logits, t_label, v_label):
                t_filtered = tf.boolean_mask(t_logits, tf.equal(t_map_label, v_label))
                return tf.metrics.mean_tensor(tf.reduce_mean(t_filtered), tf.size(t_filtered))

            eval_metrics = {
                'mean_perfect': create_mean_metrics(t_logits, t_map_label, 0),
                'mean_excellent': create_mean_metrics(t_logits, t_map_label, 1),
                'mean_good': create_mean_metrics(t_logits, t_map_label, 2),
                'mean_fair': create_mean_metrics(t_logits, t_map_label, 3),
                'mean_bad': create_mean_metrics(t_logits, t_map_label, 5),
            }

            return tf.estimator.EstimatorSpec(
                mode=mode,
                loss=t_eval_loss,
                eval_metric_ops=eval_metrics,
            )
```
if the `eval_metrics` part removed, the program will run without crashes.

**Describe the expected behavior**
program run without crashing.

**Other info / logs**
```
2020-01-15 11:07:08.705510: I tensorflow/stream_executor/cuda/ptxas_utils.cc:202] 
2020-01-15 11:07:11.712068: E tensorflow/stream_executor/cuda/cuda_driver.cc:1048] Internal: could not synchronize on CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered :: *** Begin stack trace ***
        tensorflow::CurrentStackTrace()
        stream_executor::gpu::GpuDriver::SynchronizeStream(stream_executor::gpu::GpuContext*, CUstream_st*)
        stream_executor::gpu::GpuExecutor::BlockHostUntilDone(stream_executor::Stream*)
        stream_executor::StreamExecutor::BlockHostUntilDone(stream_executor::Stream*)
        stream_executor::Stream::BlockHostUntilDone()





        tensorflow::XlaRunOp::Compute(tensorflow::OpKernelContext*)
        tensorflow::BaseGPUDevice::ComputeHelper(tensorflow::OpKernel*, tensorflow::OpKernelContext*)
        tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*)


        Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)
        std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)


        clone
*** End stack trace ***

2020-01-15 11:07:11.765522: E tensorflow/stream_executor/cuda/cuda_driver.cc:1032] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered :: *** Begin stack trace ***
        tensorflow::CurrentStackTrace()
        stream_executor::gpu::GpuDriver::SynchronizeContext(stream_executor::gpu::GpuContext*)
        stream_executor::StreamExecutor::SynchronizeAllActivity()
        tensorflow::XlaCompilationCache::~XlaCompilationCache()
        tensorflow::XlaCompilationCache::~XlaCompilationCache()
        tensorflow::ResourceMgr::Clear()
        tensorflow::DirectSession::~DirectSession()
        tensorflow::DirectSession::~DirectSession()
        tensorflow::SessionRef::Close()
        TF_CloseSession


        _PyEval_EvalFrameDefault


        _PyEval_EvalFrameDefault


        _PyEval_EvalFrameDefault


        _PyEval_EvalFrameDefault


        _PyEval_EvalFrameDefault


        _PyEval_EvalFrameDefault



        _PyEval_EvalFrameDefault
        _PyFunction_FastCallDict

        _PyObject_FastCallDict
        PyObject_CallFunctionObjArgs
        _PyEval_EvalFrameDefault



        _PyEval_EvalFrameDefault



        _PyEval_EvalFrameDefault



        _PyEval_EvalFrameDefault



        _PyEval_EvalFrameDefault


        _PyEval_EvalFrameDefault


        _PyEval_EvalFrameDefault
        _PyFunction_FastCallDict



        _PyObject_FastCallKeywords

        _PyEval_EvalFrameDefault



        _PyEval_EvalFrameDefault

        _PyFunction_FastCallDict



        _PyObject_FastCallKeywords

        _PyEval_EvalFrameDefault



        _PyEval_EvalFrameDefault


        _PyEval_EvalFrameDefault


        _PyEval_EvalFrameDefault


        _PyEval_EvalFrameDefault



        _PyEval_EvalFrameDefault


        _PyEval_EvalFrameDefault


        _PyEval_EvalFrameDefault



        _PyEval_EvalFrameDefault


        _PyEval_EvalFrameDefault

        PyEval_EvalCode

        PyRun_FileExFlags
        PyRun_SimpleFileExFlags
        Py_Main
        main
        __libc_start_main
        _start
*** End stack trace ***

2020-01-15 11:07:11.765871: E tensorflow/compiler/jit/xla_compilation_cache.cc:53] Error synchronizing activity while waiting for all programs to complete
2020-01-15 11:07:11.768797: E tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to unload module 0x7f66709c3ec0; leaking: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2020-01-15 11:07:11.776140: E tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to unload module 0x7f66748a71f0; leaking: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal: Failed to complete all kernels launched on stream 0x1f50d730: could not synchronize on CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
         [[{{node cluster_0_1/xla_run}}]]
         [[Identity/_941]]
  (1) Internal: Failed to complete all kernels launched on stream 0x1f50d730: could not synchronize on CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
         [[{{node cluster_0_1/xla_run}}]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""hvd_combine_model_train_fidelity.py"", line 684, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""hvd_combine_model_train_fidelity.py"", line 464, in main
    max_steps=num_train_steps, hooks=training_hooks)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 367, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1158, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1192, in _train_model_default
    saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1480, in _train_with_estimator_spec
    log_step_count_steps=log_step_count_steps) as mon_sess:
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 584, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1007, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 725, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1200, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1205, in _create_session
    return self._sess_creator.create_session()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 878, in create_session
    hook.after_create_session(self.tf_sess, self.coord)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/hooks/hooks.py"", line 180, in after_create_session
    self._evaluate(session)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/hooks/hooks.py"", line 203, in _evaluate
    output_dir=self._eval_dir)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1609, in _evaluate_run
    config=self._session_config)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/evaluation.py"", line 272, in _evaluate_once
    session.run(eval_ops, feed_dict)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 754, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1252, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1353, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/local/lib/python3.6/dist-packages/six.py"", line 693, in reraise
    raise value
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1338, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1411, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1169, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
  (0) Internal: Failed to complete all kernels launched on stream 0x1f50d730: could not synchronize on CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
         [[{{node cluster_0_1/xla_run}}]]
         [[Identity/_941]]
  (1) Internal: Failed to complete all kernels launched on stream 0x1f50d730: could not synchronize on CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
         [[{{node cluster_0_1/xla_run}}]]
0 successful operations.
0 derived errors ignored.
```
"
35896,Python crashed after importing Tensorflow,"**System information**
- OS Platform and Distribution: Windows 10 Enterprise
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version: 2.1.0
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: Virtual environment created with Conda, tensorflow installed with pip
- CUDA/cuDNN version: v10.1 / v7.6.5
- GPU model and memory: Nvidia Titan Xp

**Describe the problem**
Following the instruction provided online, I have successfully installed Tensorflow. To verify my installation, I have tried to import tensorflow in python. However, Python crashed immediately after importing tensorflow library. I have also tried to install through:
`pip install tensorflow-gpu==2.1.0`
Unfortunately, I experienced the same problem. 

**Any other info / logs**
Before the crashing of Python, such message has been printed:
> 2020-01-15 11:51:56.544389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
"
35894,Documentation on Tensorflow 2.1 with TPU,Are you going to update your TPU documentation https://cloud.google.com/tpu/docs/colabs with 2.1 release?
35892,Deprecation notice for collection.Sequence in recurrent.py,"@qlzh727

**Describe the current behavior**

When running unit tests the following deprecation note appears

```
lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py:808: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  if (isinstance(inputs, collections.Sequence)
```

**Describe the expected behavior**

No deprecation notice

Line 808 needs a minor modification
```
...
  if (isinstance(inputs, collections.abc.Sequence)
...
```

See [python 3 docs](https://docs.python.org/3/library/collections.abc.html#collections.abc.Sequence)
"
35889,STM32F7 Hello World example fails for arm_cmplx_mag_squared_q10p6.c,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ""Ubuntu""
VERSION=""18.04.3 LTS (Bionic Beaver)""
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): current
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm Mbed OS - STM32F7

**Describe the problem**
When running make for ""generate_hello_world_mbed_project"", a failure is seen and it stops building. 
Ref: [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world](url)
Ex:
`$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=""CMSIS disco_f746ng"" generate_hello_world_mbed_project`

**Please provide the exact sequence of commands/steps when you ran into the problem**
```
$ git clone --recursive https://github.com/tensorflow/tensorflow.git

$ cd tensorflow/

$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=""CMSIS disco_f746ng"" generate_hello_world_mbed_project
```

Error seen:

```
$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=""CMSIS disco_f746ng"" generate_hello_world_mbed_project
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip"" ""7e8191b24853d75de2af87622ad293ba"" tensorflow/lite/micro/tools/make/downloads/gemmlowp  
downloading https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz"" ""02c64880acb89dbd57eebacfd67200d8"" tensorflow/lite/micro/tools/make/downloads/flatbuffers  
downloading https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://github.com/ARM-software/CMSIS_5/archive/d76d5e3acb87cf089daf50b31f991026149ecb6c.zip"" ""866f79cfb86f7aee29a320aeda530aca"" tensorflow/lite/micro/tools/make/downloads/cmsis  
downloading https://github.com/ARM-software/CMSIS_5/archive/d76d5e3acb87cf089daf50b31f991026149ecb6c.zip
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://developer.arm.com/-/media/Files/downloads/gnu-rm/7-2018q2/gcc-arm-none-eabi-7-2018-q2-update-linux.tar.bz2"" ""299ebd3f1c2c90930d28ab82e5d8d6c0"" tensorflow/lite/micro/tools/make/downloads/gcc_embedded  
downloading https://developer.arm.com/-/media/Files/downloads/gnu-rm/7-2018q2/gcc-arm-none-eabi-7-2018-q2-update-linux.tar.bz2
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_01_13.zip"" ""8a7d2c70325f53136faea6dde517b8cc"" tensorflow/lite/micro/tools/make/downloads/person_model_int8  
downloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_01_13.zip
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://github.com/mborgerding/kissfft/archive/v130.zip"" ""438ba1fef5783cc5f5f201395cc477ca"" tensorflow/lite/micro/tools/make/downloads/kissfft patch_kissfft 
downloading https://github.com/mborgerding/kissfft/archive/v130.zip
Finished patching kissfft
tensorflow/lite/micro/tools/make/download_and_extract.sh ""https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2019_11_21.zip"" ""fe2934bd0788f1dcc7af3f0a954542ab"" tensorflow/lite/micro/tools/make/downloads/person_model_grayscale  
downloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2019_11_21.zip
make: *** No rule to make target 'tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/tensorflow/lite/micro/tools/make/downloads/CMSIS_ext/arm_cmplx_mag_squared_q10p6.c', needed by 'generate_hello_world_mbed_project'.  Stop.

```"
35888,Loaded model fails on inference,"**System information**
- TF version 2.1.0, and I have also has this error on TF 2.2.0-dev20200114
- Python 3.7

**Describe the current behavior**
- Similar to #35527, when I save and then load my model, it fails upon actually using the model, citing the inputs being different to what was expected.
- When I run my code (see below) I get the following error:
```
ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (3 total):
    * (<tf.Tensor 'inputs:0' shape=(1, 10) dtype=int32>, <tf.Tensor 'inputs_1:0' shape=(1, 10) dtype=int32>)
    * False
    * None
  Keyword arguments: {}

Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (3 total):
    * [TensorSpec(shape=(None, 10), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 10), dtype=tf.int32, name='attention_mask')]
    * True
    * None
  Keyword arguments: {}

Option 2:
  Positional arguments (3 total):
    * [TensorSpec(shape=(None, 10), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 10), dtype=tf.int32, name='attention_mask')]
    * False
    * None
  Keyword arguments: {}

Option 3:
  Positional arguments (3 total):
    * [TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/1')]
    * True
    * None
  Keyword arguments: {}

Option 4:
  Positional arguments (3 total):
    * [TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/1')]
    * False
    * None
  Keyword arguments: {}
```
- My reasoning for making this issue when #35527 already exists is that my code to reproduce the issue is more succinct and so should hopefully be easier to troubleshoot.

**Describe the expected behavior**
- The model should load and behave in the exact same manner in which it was saved (I.e. not crash when doing inference on data). This is currently not the case.

**Code to reproduce the issue**
```import tensorflow as tf


############# Create a model using TF and the popular transformers NLP package ###########

class TagModelCreator:

    def __init__(self, language_model):
        self.language_model = language_model

    def create(self, num_classes, max_seq_len, get_token_type_ids=False):

        input_modules = []
        
        input_modules.append(tf.keras.layers.Input(shape=(max_seq_len), dtype='int32', name='input_ids'))
        input_modules.append(tf.keras.layers.Input(shape=(max_seq_len), dtype=""int32"", name='attention_mask'))

        lang_layer = self.language_model(input_modules)
        linear_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(num_classes, name='classifier'))(lang_layer[0])
        model = tf.keras.Model(inputs=input_modules, outputs=linear_layer)

        return model

from transformers import TFAutoModel

model_name = ""bert-base-uncased""
language_model = TFAutoModel.from_pretrained(model_name)
tagging_model_creator = TagModelCreator(language_model)
arbitrary_class_num = 2
arbitrary_sequence_length = 10
tagging_model = tagging_model_creator.create(arbitrary_class_num, arbitrary_sequence_length)




######### Create some spoof data to see how the model handles the data ####################

def data_generator():
    yield (([0]*arbitrary_sequence_length, [1]*arbitrary_sequence_length))

input_types = ((tf.int32, tf.int32))
input_shape = ((tf.TensorShape([None]), tf.TensorShape([None])))

tf_dataset = tf.data.Dataset.from_generator(data_generator, input_types, input_shape).batch(7)






######### Use the spoof data on the model, to confirm that it does inference on the data without errors########

for example_input in tf_dataset:
    test_output = tagging_model(example_input)
    break

print(test_output)
print(""Inference is done correctly BEFORE re-loading the model"")






######## Save and reload the model #################

tf.keras.models.save_model(model=tagging_model,
                                       filepath=""test_model_save.tf"",
                                       save_format=""tf"",
                                       include_optimizer=True
                                      )

reloaded_model = tf.keras.models.load_model(filepath=""test_model_save.tf"")






####### Try to repeat the inference as above #########
for example_input in tf_dataset:
    test_output = reloaded_model(example_input)
    break
```

**Other info / logs**
The full output of the above code, including printouts and the error stack trace is
```
tf.Tensor(
[[[-0.6191008  -0.12756673]
  [-0.89110005  0.06499487]
  [-0.8666591  -0.02111167]
  [-0.8456675  -0.08551306]
  [-0.853022   -0.15643758]
  [-0.8632274  -0.20486367]
  [-0.8571876  -0.24682882]
  [-0.8400811  -0.2774819 ]
  [-0.8864943  -0.32766515]
  [-0.8612056  -0.3529073 ]]], shape=(1, 10, 2), dtype=float32)
Inference is done correctly BEFORE re-loading the model
WARNING:tensorflow:From C:\Users\Peter\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
INFO:tensorflow:Assets written to: test_model_save.tf\assets
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-9d120ebc3322> in <module>
     80 ####### Try to repeat the inference as above #########
     81 for example_input in tf_dataset:
---> 82     test_output = reloaded_model(example_input)
     83     break
     84 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\keras\engine\base_layer.py in __call__(self, inputs, *args, **kwargs)
    820           with base_layer_utils.autocast_context_manager(
    821               self._compute_dtype):
--> 822             outputs = self.call(cast_inputs, *args, **kwargs)
    823           self._handle_activity_regularization(inputs, outputs)
    824           self._set_mask_metadata(inputs, outputs, input_masks)

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\keras\saving\saved_model\utils.py in return_outputs_and_add_losses(*args, **kwargs)
     57     inputs = args[inputs_arg_index]
     58     args = args[inputs_arg_index + 1:]
---> 59     outputs, losses = fn(inputs, *args, **kwargs)
     60     layer.add_loss(losses, inputs)
     61     return outputs

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\keras\saving\saved_model\utils.py in wrap_with_training_arg(*args, **kwargs)
    111         training,
    112         lambda: replace_training_and_call(True),
--> 113         lambda: replace_training_and_call(False))
    114 
    115   # Create arg spec for decorated function. If 'training' is not defined in the

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\keras\utils\tf_utils.py in smart_cond(pred, true_fn, false_fn, name)
     57         pred, true_fn=true_fn, false_fn=false_fn, name=name)
     58   return smart_module.smart_cond(
---> 59       pred, true_fn=true_fn, false_fn=false_fn, name=name)
     60 
     61 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\framework\smart_cond.py in smart_cond(pred, true_fn, false_fn, name)
     54       return true_fn()
     55     else:
---> 56       return false_fn()
     57   else:
     58     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\keras\saving\saved_model\utils.py in <lambda>()
    111         training,
    112         lambda: replace_training_and_call(True),
--> 113         lambda: replace_training_and_call(False))
    114 
    115   # Create arg spec for decorated function. If 'training' is not defined in the

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\keras\saving\saved_model\utils.py in replace_training_and_call(training)
    106     def replace_training_and_call(training):
    107       set_training_arg(training, training_arg_index, args, kwargs)
--> 108       return wrapped_call(*args, **kwargs)
    109 
    110     return tf_utils.smart_cond(

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\eager\def_function.py in __call__(self, *args, **kwds)
    566         xla_context.Exit()
    567     else:
--> 568       result = self._call(*args, **kwds)
    569 
    570     if tracing_count == self._get_tracing_count():

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\eager\def_function.py in _call(self, *args, **kwds)
    604       # In this case we have not created variables on the first call. So we can
    605       # run the first trace but we should fail if variables are created.
--> 606       results = self._stateful_fn(*args, **kwds)
    607       if self._created_variables:
    608         raise ValueError(""Creating variables on a non-first call to a function""

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\eager\function.py in __call__(self, *args, **kwargs)
   2360     """"""Calls a graph function specialized to the inputs.""""""
   2361     with self._lock:
-> 2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
   2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   2364 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\eager\function.py in _maybe_define_function(self, args, kwargs)
   2701 
   2702       self._function_cache.missed.add(call_context_key)
-> 2703       graph_function = self._create_graph_function(args, kwargs)
   2704       self._function_cache.primary[cache_key] = graph_function
   2705       return graph_function, args, kwargs

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\eager\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2591             arg_names=arg_names,
   2592             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2593             capture_by_value=self._capture_by_value),
   2594         self._function_attributes,
   2595         # Tell the ConcreteFunction to clean up its graph once it goes out of

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\framework\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    976                                           converted_func)
    977 
--> 978       func_outputs = python_func(*func_args, **func_kwargs)
    979 
    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\eager\def_function.py in wrapped_fn(*args, **kwds)
    437         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    438         # the function a weak reference to itself to avoid a reference cycle.
--> 439         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    440     weak_wrapped_fn = weakref.ref(wrapped_fn)
    441 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\saved_model\function_deserialization.py in restored_function_body(*args, **kwargs)
    260         .format(_pretty_format_positional(args), kwargs,
    261                 len(saved_function.concrete_functions),
--> 262                 ""\n\n"".join(signature_descriptions)))
    263 
    264   concrete_function_objects = []

ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (3 total):
    * (<tf.Tensor 'inputs:0' shape=(1, 10) dtype=int32>, <tf.Tensor 'inputs_1:0' shape=(1, 10) dtype=int32>)
    * False
    * None
  Keyword arguments: {}

Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (3 total):
    * [TensorSpec(shape=(None, 10), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 10), dtype=tf.int32, name='attention_mask')]
    * True
    * None
  Keyword arguments: {}

Option 2:
  Positional arguments (3 total):
    * [TensorSpec(shape=(None, 10), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 10), dtype=tf.int32, name='attention_mask')]
    * False
    * None
  Keyword arguments: {}

Option 3:
  Positional arguments (3 total):
    * [TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/1')]
    * True
    * None
  Keyword arguments: {}

Option 4:
  Positional arguments (3 total):
    * [TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/1')]
    * False
    * None
  Keyword arguments: {}
```
"
35887,[Windows] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED #7072,"Closed/related issue
https://github.com/tensorflow/tensorflow/issues/7072

Running tf 2.1 on windows 10 installed via pip
Python 3.7.4
Cuda 10.1
dnn 10.1 
GPU: GeForce GTX 1650 

When I run the tf canonical sample code
```
tf.debugging.set_log_device_placement(True)
a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
c = tf.matmul(a, b)
print(c)``
```
I get no issues
```
2020-01-14 21:16:24.262673: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-01-14 21:16:26.104356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-01-14 21:16:26.129817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5
coreClock: 1.245GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.34GiB/s
2020-01-14 21:16:26.136690: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-01-14 21:16:26.149405: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-01-14 21:16:26.155714: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-01-14 21:16:26.160934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-01-14 21:16:26.167186: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-01-14 21:16:26.175237: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-01-14 21:16:26.187187: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-01-14 21:16:26.189723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-01-14 21:16:26.191974: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-01-14 21:16:26.200691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5
coreClock: 1.245GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.34GiB/s
2020-01-14 21:16:26.206054: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-01-14 21:16:26.214181: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-01-14 21:16:26.216951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-01-14 21:16:26.219549: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-01-14 21:16:26.227390: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-01-14 21:16:26.230551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-01-14 21:16:26.233257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-01-14 21:16:26.241289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-01-14 21:16:26.719313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-14 21:16:26.722034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0
2020-01-14 21:16:26.724147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N
2020-01-14 21:16:26.726376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2915 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-01-14 21:16:26.738893: I tensorflow/core/common_runtime/eager/execute.cc:573] Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0
2020-01-14 21:16:26.747089: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32)
```
However, when I run 
```import tensorflow as tf

with tf.device('/gpu:0'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
    c = tf.matmul(a, b)


with tf.Session() as sess:
    print (sess.run(c))
```
I get 
```
2020-01-14 21:14:53.513065: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-01-14 21:14:55.360139: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-01-14 21:14:55.382691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5
coreClock: 1.245GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.34GiB/s
2020-01-14 21:14:55.388200: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-01-14 21:14:55.400965: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-01-14 21:14:55.406969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-01-14 21:14:55.412851: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-01-14 21:14:55.419095: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-01-14 21:14:55.426957: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-01-14 21:14:55.438634: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-01-14 21:14:55.441198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-01-14 21:14:55.443479: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-01-14 21:14:55.452456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5
coreClock: 1.245GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.34GiB/s
2020-01-14 21:14:55.457969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-01-14 21:14:55.466455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-01-14 21:14:55.469159: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-01-14 21:14:55.471860: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-01-14 21:14:55.480462: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-01-14 21:14:55.482601: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-01-14 21:14:55.484608: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-01-14 21:14:55.486769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-01-14 21:14:55.985284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-14 21:14:55.987459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0
2020-01-14 21:14:55.989183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N
2020-01-14 21:14:55.991385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2915 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-01-14 21:14:56.213441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-01-14 21:14:56.468795: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2020-01-14 21:14:56.471524: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2020-01-14 21:14:56.474410: W tensorflow/stream_executor/stream.cc:2041] attempting to perform BLAS operation using StreamExecutor without BLAS support
Traceback (most recent call last):
  File "".\gpu.py"", line 6, in <module>
    c = tf.linalg.matmul(a, b)
  File ""C:\Users\shan\Anaconda3\lib\site-packages\tensorflow_core\python\util\dispatch.py"", line 180, in wrapper
    return target(*args, **kwargs)
  File ""C:\Users\shan\Anaconda3\lib\site-packages\tensorflow_core\python\ops\math_ops.py"", line 2798, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""C:\Users\shan\Anaconda3\lib\site-packages\tensorflow_core\python\ops\gen_math_ops.py"", line 5616, in mat_mul
    _ops.raise_from_not_ok_status(e, name)
  File ""C:\Users\shan\Anaconda3\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(2, 3), b.shape=(3, 2), m=2, n=2, k=3 [Op:MatMul] name: MatMul/
```

"
35885,Problem converting model to tensorflow lite (LSTM model),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**I have used google colab**

- TensorFlow version (or github SHA if from source):
TensorFlow 2.x selected.


**Provide the text output from tflite_convert**

```
---------------------------------------------------------------------------
ConverterError                            Traceback (most recent call last)
<ipython-input-73-6a5821dcb9ed> in <module>()
      1 converter = tf.lite.TFLiteConverter.from_keras_model(model)
----> 2 tflite_model = converter.convert()
      3 
      4 # Save the model to disk
      5 #open(""/content/gdrive/My Drive/detect_car_model.tflite"", ""wb"").write(tflite_model)

2 frames
/tensorflow-2.1.0/python3.6/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    201       stdout = _try_convert_to_unicode(stdout)
    202       stderr = _try_convert_to_unicode(stderr)
--> 203       raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
    204   finally:
    205     # Must manually cleanup files.

ConverterError: See console for info.
2020-01-15 04:01:36.499502: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2020-01-15 04:01:36.510161: E tensorflow/lite/toco/toco_tooling.cc:498] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, FULLY_CONNECTED, RESHAPE, REVERSE_V2, SOFTMAX, STRIDED_SLICE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.
Traceback (most recent call last):
  File ""/tensorflow-2.1.0/python3.6/bin/toco_from_protos"", line 8, in <module>
    sys.exit(main())
  File ""/tensorflow-2.1.0/python3.6/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/tensorflow-2.1.0/python3.6/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/tensorflow-2.1.0/python3.6/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/tensorflow-2.1.0/python3.6/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/tensorflow-2.1.0/python3.6/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 56, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, FULLY_CONNECTED, RESHAPE, REVERSE_V2, SOFTMAX, STRIDED_SLICE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.


```


** info / logs**

I have tried to convert the following model:

```

model = keras.Sequential()
model.add(
    keras.layers.Bidirectional(
      keras.layers.LSTM(
          units=128,
          input_shape=[x_train.shape[1], x_train.shape[2]]
      )
    )
)
model.add(keras.layers.Dropout(rate=0.5))
model.add(keras.layers.Dense(units=128, activation='relu'))
model.add(keras.layers.Dense(y_train.shape[1], activation='softmax'))

model.compile(
  loss='categorical_crossentropy',
  optimizer='adam',
  metrics=['acc']
)
```"
35884,Benchmark test failed on windows,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):tf-nightly
- Python version:3.7.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
File ""\\?\C:\Users\RUNNER~1\AppData\Local\Temp\Bazel.runfiles_e847arzc\runfiles\__main__\tensorflow_addons\activations\rrelu_test.py"", line 88, in benchmarkRreluOp
    self.run_op_benchmark(sess, result.op, min_iters=25)
  File ""C:\hostedtoolcache\windows\Python\3.7.5\x64\lib\site-packages\tensorflow_core\python\platform\benchmark.py"", line 389, in run_op_benchmark
    ""throughput"": mbs / median_delta
ZeroDivisionError: float division by zero

**Describe the expected behavior**
Same behavior as ubuntu
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python 
self.run_op_benchmark(sess, result.op, min_iters=25)
```

**Other info / logs**

https://github.com/tensorflow/addons/issues/839
It seems [```time.time()```](https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/python/platform/benchmark.py#L294) cause the problem and ```timeit.default_timer()``` could fix the problem"
35883,AlphaDropout & mixed_float16 - Op has type float32 that does not match type float16,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 31
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): v2.1.0-1-ga9af83a149 2.1.0
- Python version: 3.7.5
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source): 8.3.1
- CUDA/cuDNN version: 10.2.89 / 7.6.5.33
- GPU model and memory: Nvidia GeForce GTX 1070 TI 8GB

**Describe the current behavior**
```
Traceback (most recent call last):
  File ""/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 468, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1290, in convert_to_tensor
    (dtype.name, value.dtype.name, value))
ValueError: Tensor conversion requested dtype float16 for Tensor with dtype float32: <tf.Tensor 'Cast:0' shape=(None, 1) dtype=float32>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/tmp/test.py"", line 6, in <module>
    dropout = tf.keras.layers.AlphaDropout(0.5)(input)
  File ""/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 773, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/noise.py"", line 202, in call
    return K.in_train_phase(dropped_inputs, inputs, training=training)
  File ""/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py"", line 4303, in in_train_phase
    x = switch(training, x, alt)
  File ""/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py"", line 4236, in switch
    x = control_flow_ops.cond(condition, then_expression_fn, else_expression_fn)
  File ""/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 1174, in cond
    return cond_v2.cond_v2(pred, true_fn, false_fn, name)
  File ""/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/cond_v2.py"", line 83, in cond_v2
    op_return_value=pred)
  File ""/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/noise.py"", line 197, in dropped_inputs
    x = inputs * kept_idx + alpha_p * (1 - kept_idx)
  File ""/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py"", line 902, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py"", line 1201, in _mul_dispatch
    return gen_math_ops.mul(x, y, name=name)
  File ""/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py"", line 6125, in mul
    ""Mul"", x=x, y=y, name=name)
  File ""/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 504, in _apply_op_helper
    inferred_from[input_arg.type_attr]))
TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.
```
**Describe the expected behavior**
No exception

**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.keras.mixed_precision import experimental as mixed_precision
mixed_precision.set_policy(mixed_precision.Policy('mixed_float16'))

input = tf.keras.Input(shape=(1))
dropout = tf.keras.layers.AlphaDropout(0.5)(input)
```"
35880,Unable to use tensorflow gpu ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Windows 10 Home v1903
- TensorFlow installed from (source or binary):Source
- TensorFlow version:2.1.0
- Python version: 3.5.6
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: 7.1.0
- GPU model and memory: RTX 2080 Super



**Describe the problem**
import tensorflow as tf not working

**Provide the exact sequence of commands / steps that you executed before running into the problem**
import tensorflow as tf


**Any other info / logs**

Traceback (most recent call last):
  File ""C:\Users\soumi\.conda\envs\soumil\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\soumi\.conda\envs\soumil\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\soumi\.conda\envs\soumil\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\soumi\.conda\envs\soumil\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\soumi\.conda\envs\soumil\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The file cannot be accessed by the system.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\soumi\.conda\envs\soumil\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\soumi\.conda\envs\soumil\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\soumi\.conda\envs\soumil\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\soumi\.conda\envs\soumil\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\soumi\.conda\envs\soumil\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\soumi\.conda\envs\soumil\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\soumi\.conda\envs\soumil\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\soumi\.conda\envs\soumil\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\soumi\.conda\envs\soumil\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\soumi\.conda\envs\soumil\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\soumi\.conda\envs\soumil\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\soumi\.conda\envs\soumil\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The file cannot be accessed by the system.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
35879,Running fit on an tf.keras.Sequential with a Keras Sequence generator causes deadlock with use_multiprocessing,"> 
> == check python ===================================================
> python version: 3.7.4
> python branch: 
> python build version: ('default', 'Sep  7 2019 18:27:02')
> python compiler version: Clang 10.0.1 (clang-1001.0.46.4)
> python implementation: CPython
> 
> 
> == check os platform ===============================================
> os: Darwin
> os kernel version: Darwin Kernel Version 19.0.0: Wed Sep 25 20:18:50 PDT 2019; root:xnu-6153.11.26~2/RELEASE_X86_64
> os release version: 19.0.0
> os platform: Darwin-19.0.0-x86_64-i386-64bit
> linux distribution: ('', '', '')
> linux os distribution: ('', '', '')
> mac version: ('10.15', ('', '', ''), 'x86_64')
> uname: uname_result(system='Darwin', node='Andrews-MacBook.local', release='19.0.0', version='Darwin Kernel Version 19.0.0: Wed Sep 25 20:18:50 PDT 2019; root:xnu-6153.11.26~2/RELEASE_X86_64', machine='x86_64', processor='i386')
> architecture: ('64bit', '')
> machine: x86_64
> 
> 
> == are we in docker =============================================
> No
> 
> == compiler =====================================================
> xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun
> 
> == check pips ===================================================
> numpy                1.17.4   
> protobuf             3.10.0   
> tensorflow           2.1.0    
> tensorflow-estimator 2.0.1    
> 
> == check for virtualenv =========================================
> True
> 
> == tensorflow import ============================================
> tf.version.VERSION = 2.1.0
> tf.version.GIT_VERSION = v2.0.0-rc2-26-g64c3d382ca
> tf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.11.45.5)
> 
> == env ==========================================================
> LD_LIBRARY_PATH is unset
> DYLD_LIBRARY_PATH is unset
> 
> == nvidia-smi ===================================================
> ./tf_env_collect.sh: line 147: nvidia-smi: command not found
> 
> == cuda libs  ===================================================
> 
> == tensorflow installed from info ==================
> Name: tensorflow
> Version: 2.1.0
> Summary: TensorFlow is an open source machine learning framework for everyone.
> Home-page: https://www.tensorflow.org/
> Author-email: packages@tensorflow.org
> License: Apache 2.0
> Location: /Users/andrew/.local/share/virtualenvs/lib_andrew_scratch--yvJ8pLH/lib/python3.7/site-packages
> Required-by: 
> 
> == python version  ==============================================
> (major, minor, micro, releaselevel, serial)
> (3, 7, 4, 'final', 0)
> 
> == bazel version  ===============================================
> 

**Describe the current behavior**
It seems all threads are waiting in a deadlock behaviour

**Describe the expected behavior**
To train the model

**Code to reproduce the issue**



```
from tensorflow.keras.utils import Sequence
import tensorflow as tf
import numpy as  np

class DataGenerator(Sequence):
    def __init__(self):
        self.batch_size = 32
        self.output_shape = (6, 12)

    def __len__(self):
        return 128

    def __getitem__(self, index):
        X = np.random.uniform(-1, 1, (self.batch_size, *self.output_shape))
        y = np.random.uniform(-1, 1, (self.batch_size, 1))
        return (X, y)

def build_model():
    model = tf.keras.Sequential(name='hello')
    model.add(tf.keras.layers.Flatten(input_shape=(6, 12)))
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
    model.compile(optimizer=tf.keras.optimizers.Adam(),
                  loss=tf.keras.losses.MeanSquaredError())
    return model

if __name__== ""__main__"":

    gen = DataGenerator()
    val_gen = DataGenerator()

    model = build_model()
    model.fit(x = gen, 
        validation_data = val_gen,
        epochs = 8,
        workers = 4,
        use_multiprocessing = True,
        shuffle=True)

```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

"
35878,MultiWorkerMirroredStrategy Keras Example Hangs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.3
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10.0
- GPU model and memory: Tesla K80, 1 GPU per worker, 2 workers

**Describe the current behavior**

When I run the distributed training example for the MultiWorkerMirroredStrategy with Keras documented [here](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras), approximately half the time the training will be successful and otherwise the workers will hang after the first epoch.

**Describe the expected behavior**

The training should successfully complete every time with no hanging.

**Code to reproduce the issue**

```python
import os, json

os.environ['TF_CONFIG'] = json.dumps({'cluster': {'worker': ['X.X.X.X:2000', 'X.X.X.X:2000']}, 'task': {'type': 'worker', 'index': 0}})
import tensorflow_datasets as tfds
import tensorflow as tf
tf.config.optimizer.set_jit(True)
tfds.disable_progress_bar()
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.NCCL) # NCCL vs RING
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
BUFFER_SIZE = 10000
BATCH_SIZE = 64
NUM_WORKERS = 2

def make_datasets_unbatched():
  # Scaling MNIST data from (0, 255] to (0., 1.]
  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label
  datasets, info = tfds.load(name='mnist',
                            with_info=True,
                            as_supervised=True)
  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)

train_datasets = make_datasets_unbatched().batch(BATCH_SIZE)
def build_and_compile_cnn_model():
  model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
  ])
  model.compile(
    loss=tf.keras.losses.sparse_categorical_crossentropy,
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
    metrics=['accuracy'])
  return model
GLOBAL_BATCH_SIZE = 64 * NUM_WORKERS
with strategy.scope():
  train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)
  multi_worker_model = build_and_compile_cnn_model()
multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)
```

**Other info / logs**
```
2020-01-14 20:53:41.329726: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-01-14 20:53:41.393549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.394307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
2020-01-14 20:53:41.394552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-01-14 20:53:41.396280: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-01-14 20:53:41.397495: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-01-14 20:53:41.397794: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-01-14 20:53:41.399430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-01-14 20:53:41.400687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-01-14 20:53:41.404610: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-14 20:53:41.404722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.405478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.406161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-01-14 20:53:41.406769: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-01-14 20:53:41.414319: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300070000 Hz
2020-01-14 20:53:41.414709: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56414dcdbd50 executing computations on platform Host. Devices:
2020-01-14 20:53:41.414747: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
2020-01-14 20:53:41.722508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.723344: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56414dcf9870 executing computations on platform CUDA. Devices:
2020-01-14 20:53:41.723406: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7
2020-01-14 20:53:41.723661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.724362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
2020-01-14 20:53:41.724432: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-01-14 20:53:41.724476: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-01-14 20:53:41.724515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-01-14 20:53:41.724558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-01-14 20:53:41.724594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-01-14 20:53:41.724633: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-01-14 20:53:41.724670: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-14 20:53:41.724757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.725490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.726167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-01-14 20:53:41.726224: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-01-14 20:53:41.727650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-14 20:53:41.727681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2020-01-14 20:53:41.727699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2020-01-14 20:53:41.728328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.729081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.729792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
2020-01-14 20:53:41.731085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.731843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
2020-01-14 20:53:41.731906: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-01-14 20:53:41.731950: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-01-14 20:53:41.731993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-01-14 20:53:41.732036: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-01-14 20:53:41.732078: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-01-14 20:53:41.732119: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-01-14 20:53:41.732161: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-14 20:53:41.732267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.733006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.733681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-01-14 20:53:41.733719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-14 20:53:41.733746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2020-01-14 20:53:41.733768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2020-01-14 20:53:41.734378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.735121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 20:53:41.735829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
D0114 20:53:41.736038696   13433 log.cc:95]                  Warning: insecure environment read function 'getenv' used
D0114 20:53:41.736065073   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
D0114 20:53:41.736085632   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
D0114 20:53:41.736249487   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
D0114 20:53:41.736271507   13433 is_epollexclusive_available.cc:86] epoll_ctl with EPOLLEXCLUSIVE | EPOLLONESHOT succeeded. This is evidence of no EPOLLEXCLUSIVE support. Not using epollex polling engine.
I0114 20:53:41.736291557   13433 ev_epollex_linux.cc:1633]   Skipping epollex because it is not supported.
I0114 20:53:41.736308984   13433 ev_epoll1_linux.cc:116]     grpc epoll fd: 22
D0114 20:53:41.736329223   13433 ev_posix.cc:170]            Using polling engine: epoll1
D0114 20:53:41.736356694   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
D0114 20:53:41.736392859   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
D0114 20:53:41.736408436   13433 dns_resolver.cc:334]        Using native dns resolver
D0114 20:53:41.736428850   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
E0114 20:53:41.737641232   13433 socket_utils_common_posix.cc:198] check for SO_REUSEPORT: {""created"":""@1579035221.737631945"",""description"":""SO_REUSEPORT unavailable on compiling system"",""file"":""external/grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":166}
2020-01-14 20:53:41.737822: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -> {0 -> X.X.X.X:2000, 1 -> localhost:2000}
2020-01-14 20:53:41.738932: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:2000
Number of devices: 2
D0114 20:53:43.049644334   13552 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
D0114 20:53:43.049683824   13552 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
D0114 20:53:43.049689625   13552 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used
D0114 20:53:43.049770214   13552 dns_resolver.cc:275]        Start resolving.
I0114 20:53:43.050377745   13518 subchannel.cc:1025]         New connected subchannel at 0x7f2a04006d60 for subchannel 0x7f2a18008070
WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an ""evaluator"" task exists in the cluster.
WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an ""evaluator"" task exists in the cluster.
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.
WARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.
WARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.
Train for 5 steps
Epoch 1/3
2020-01-14 20:53:46.911991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-01-14 20:53:48.937723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
```
The worker hangs indefinitely after `Successfully opened dynamic library libcudnn.so.7`
"
35877,Bazel build fail: ModuleNotFoundError: No module named 'google.protobuf',"

**System: MacOS Mojave version 10.14.4**

**Description:** 
compiled from master branch and got error `ModuleNotFoundError: No module named 'google.protobuf'`

**Commands followed:** 
```shell
pip install -U --user pip six numpy wheel setuptools mock 'future>=0.17.1'
pip install -U --user keras_applications --no-deps
pip install -U --user keras_preprocessing --no-deps
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow # defaults to master branch -> compiled from master branch. 
./configure 
bazel build //tensorflow/tools/pip_package:build_pip_package
```
**Error:** 
```shell 
ERROR: /Users/johnkarasev/PycharmProjects/untitled/tensorflow/tensorflow/BUILD:866:1: Executing genrule //tensorflow:tf_python_api_gen_v2 failed (Exit 1)
Traceback (most recent call last):
  File ""/private/var/tmp/_bazel_johnkarasev/ecc035b543493d4fee11271827f64626/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.ru
nfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/private/var/tmp/_bazel_johnkarasev/ecc035b543493d4fee11271827f64626/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.ru
nfiles/org_tensorflow/tensorflow/python/__init__.py"", line 64, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/private/var/tmp/_bazel_johnkarasev/ecc035b543493d4fee11271827f64626/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.ru
nfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py"", line 7, in <module>
    from google.protobuf import descriptor as _descriptor
ModuleNotFoundError: No module named 'google.protobuf'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /Users/johnkarasev/PycharmProjects/untitled/tensorflow/tensorflow/tools/pip_package/BUILD:41:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed (Exit
 1)
INFO: Elapsed time: 6.212s, Critical Path: 1.12s
INFO: 12 processes: 12 local.
FAILED: Build did NOT complete successfully
```

"
35876,Keras `predict` with sigmoid output returns probabilities instead of labels,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yee
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 19.10
- TensorFlow installed from (source or binary): PyPI wheel
- TensorFlow version: v2.1.0-rc2-17-ge5bf8de
- Python version: 3.7.5

**Describe the current behavior**

The `predict` method of a Keras model with a sigmoid activiation function for the output returns probabilities.

**Describe the expected behavior**

`predict` should return class indices or class labels, as in the case of softmax activation. The current behavior also seems to be redundant with the method `predict_proba`, which in turn doesn't seem to be documented, at least on https://www.tensorflow.org/api_docs/python/tf/keras/Model

#7287 is perhaps related.

**Code to reproduce the issue**

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation

model = Sequential([
    Dense(10, activation = ""relu""),
    Dense(1, activation = ""sigmoid"")])
model.compile(
    optimizer = ""rmsprop"",
    loss = ""binary_crossentropy"")
model.fit(
    [[1, 2], [1, 3], [1, 1], [2, 2], [2, 3]],
    [True, False, False, True, True])

print(model.predict([[1, 2], [1, 3], [1, 1]]))
```"
35875,Colab crashes due to tcmalloc large allocation,"tcmalloc: large alloc 6645350400 bytes == 0x695ff4000 @ 0x7f6d5f840b6b 0x7f6d5f860379 0x7f6d2b49ec27 0x7f6d2b291a7f 0x7f6d2b15d3cb 0x7f6d2b123526 0x7f6d2b1243b3 0x7f6d2b124583 0x7f6d2f9ad7b5 0x7f6d3012d949 0x7f6d3012e3fe 0x7f6d3010ab82 0x7f6d3010b5bf 0x7f6d3010f44c 0x7f6d301110aa 0x7f6d2df5db25 0x7f6d2def4e1b 0x50a8af 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a067e 0x50d966

This perhaps happens only I load High-quality images....and ofc it needs more RAM but not more than the RAM offered by colab.Any solutions?

WARNING: tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 6645350400 exceeds 10% of system memory.

"
35874,Executing XLA compiled function inside tf.GradientTape context leads to extraneous GPU kernels and D2D copies,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v1.12.1-21967-gd80fda0 2.1.0-dev20200109
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA Version 10.1.243 / cuDNN 7.6.4.38-1
- GPU model and memory: TITAN V, 12GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I am trying to speed my code using tf.function decoration, with `experimental_compile=True`. I observe that my code indeed runs faster using this decoration, but executing the function within a `tf.GradientTape()` context seems to generate two GPU kernels instead of one, even though I am not computing any gradients. Moreover, executing the function inside of the `tf.GradientTape()` context produces two D2D copies, which are not present when executing outside of `tf.GradientTape()`. The following illustrates my point (the first kernel is running outside `tf.GradientTape()`and the second is running inside `tf.GradientTape()`:

![Screen Shot 2020-01-14 at 12 40 12 PM](https://user-images.githubusercontent.com/10225518/72367919-0cd88980-36cb-11ea-9d0c-2fc0937a5db6.png)



**Describe the expected behavior**
I am not sure if this behavior is expected or not. If it is not expected, I would certainly like it if there were less GPU kernel calls, leading to much faster code.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf
from tensorflow.python import eager

def main():

    assert tf.executing_eagerly()

    gpus_p = tf.config.experimental.list_physical_devices('GPU')
    if gpus_p:
        try:
            # Currently, memory growth needs to be the same across GPUs
            for gpu in gpus_p:
                tf.config.experimental.set_memory_growth(gpu, True)
        except RuntimeError as e:
            # Memory growth must be set before GPUs have been initialized
            print(e)


    def tf_forward(x,xmax):
        return tf.quantization.fake_quant_with_min_max_vars(inputs=x,
                                                            min=-xmax,
                                                            max=xmax,
                                                            num_bits=8)


    tf_forward_tf_function = tf.function(func=tf_forward,
                                        experimental_compile=True)

    

    x = tf.random.uniform(shape=[3, 3, 96, 1])
    xmax = tf.Variable(0.5)

    tf_forward_tf_function(x,xmax)

    eager.profiler.start()
    for n in range(1000):
        x = tf_forward_tf_function(x,xmax)
        with tf.GradientTape() as tape:
            tape.watch(x)
            x = tf_forward_tf_function(x,xmax)
    
    profiler_result = eager.profiler.stop()
    eager.profiler.save('/test/log', profiler_result)

if __name__ == '__main__':
    main()
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35873,having print statements in dataset iterator makes it much slower,"I was trying to write the model training by using gradient Tape. Somehow, it is slower than Keras.fit() method. On debugging lot of time. I came to know that the reason was having some other code while iterating through the dataset, I was storing few things in a list and a print statements.

It is what slowing the program. here is the code that is required. I've run it multiple times.
![image](https://user-images.githubusercontent.com/14745146/72365972-55ba2d80-371f-11ea-9524-757d4fb9197d.png)

And after adding a print statement, 
![image](https://user-images.githubusercontent.com/14745146/72366050-7b473700-371f-11ea-88d5-ba93b33d5da5.png)

The time got reduced if I reduce the print statements. 
![image](https://user-images.githubusercontent.com/14745146/72366092-974ad880-371f-11ea-9822-6f84c3190cc4.png)

Can anyone explain this?
"
35872,cudart64_101.dll and nvcuda.dll not found,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows
- TensorFlow installed from (source or binary):source
- TensorFlow version:2
- Python version:3.7
- Installed using :pip
- CUDA/cuDNN version: none
- GPU model and memory:1696



**Describe the problem**

I m not able to install Tensorflow due to a problem with my Graphic Card
here's the error message : 
![CG](https://user-images.githubusercontent.com/48018890/72365775-2a244c80-36f9-11ea-9fe4-5c5d8e97c031.jpg)
![pb CG](https://user-images.githubusercontent.com/48018890/72365807-3c9e8600-36f9-11ea-8df3-bf0c4d5b6055.jpg)
tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
 I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine."
35870,Install Tensorflow C++ API,"**System information**
- OS Platform and Distribution :  Linux Ubuntu 18.04
- TensorFlow installed from : Anaconda
- TensorFlow version: 1.15
- Python version:3.7(anaconda)
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):1.2.1
- GCC/Compiler version (if compiling from source):7.4.0
- CUDA/cuDNN version: cuda10.1 cudnn 7.6
- GPU model and memory:  NVIDIA-SMI 435.21       Driver Version: 435.21 



**Describe the problem**
I want to install tensorflow c api and start training my models in c as in python 
i followed the instruction mentioned here [https://www.tensorflow.org/install/source](url)
but after the making configuration by bazel i cannot get my code running 
my code 
`#include ""tensorflow/cc/ops/const_op.h""
#include ""tensorflow/cc/ops/image_ops.h""
#include ""tensorflow/cc/ops/standard_ops.h""
#include ""tensorflow/core/framework/graph.pb.h""
#include ""tensorflow/core/framework/tensor.h""
#include ""tensorflow/cc/framework/gradients.h""
#include ""tensorflow/core/graph/default_device.h""
#include ""tensorflow/core/graph/graph_def_builder.h""
#include ""tensorflow/core/lib/core/errors.h""
#include ""tensorflow/core/lib/core/stringpiece.h""
#include ""tensorflow/core/lib/core/threadpool.h""
#include ""tensorflow/core/lib/io/path.h""
#include ""tensorflow/core/lib/strings/stringprintf.h""
#include ""tensorflow/core/platform/init_main.h""
#include ""tensorflow/core/platform/logging.h""
#include ""tensorflow/core/platform/types.h""
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/core/util/command_line_flags.h""
#include ""tensorflow/core/platform/load_library.h""`
it gives this dependency errors due to missing files (.h files)
I have two questions
1- where should be the header files placed ?
2- how can i get the missing files which is not here [https://github.com/tensorflow/tensorflow/tree/master/tensorflow](url) ?"
35867,Linking error when building Tensorflow 2.1,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Centos 7 (Docker image)
- TensorFlow installed from (source or binary):
Source
- TensorFlow version:
r2.1
- Python version:
3.6.8
- Bazel version (if compiling from source):
0.29.1 (installed using bazelisk)
- GCC/Compiler version (if compiling from source):
7.3.1 (installed via devtoolset-7)

**Describe the problem**
When building Tensorflow 2.1, the following linking error appears:

```
ERROR: /tmp/bazel/external/swig/BUILD.bazel:5:1: Linking of rule 
'@swig//:swig' failed (Exit 1)
bazel-out/host/bin/external/swig/_objs/swig/allocate.o:allocate.cxx:
function Allocate::~Allocate(): 
error: undefined reference to 'operator delete(void*, unsigned long)'
```

Seems like an incompatibility issue when building some third party components caused when looking for that symbol.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`$ docker run centos:7`

Then inside the container:
```
# yum-config-manager --enable rhel-server-rhscl-7-rpms -y && yum install devtoolset-7 -y
# export PATH=/opt/rh/devtoolset-7/root/usr/bin${PATH:+:${PATH}}
# export PCP_DIR=/opt/rh/devtoolset-7/root
# export PERL5LIB=/opt/rh/devtoolset-7/root//usr/lib64/perl5/vendor_perl:/opt/rh/devtoolset-7/root/usr/lib/perl5:/opt/rh/devtoolset-7/root//usr/share/perl5/vendor_perl${PERL5LIB:+:${PERL5LIB}}
# rpmlibdir=$(rpm --eval ""%{_libdir}"")
# if [ ""$rpmlibdir"" != ""${rpmlibdir/lib64/}"" ]; then
#       rpmlibdir32="":/opt/rh/devtoolset-7/root${rpmlibdir/lib64/lib}""
# fi
# export LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root$rpmlibdir$rpmlibdir32${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
# export LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root$rpmlibdir$rpmlibdir32:/opt/rh/devtoolset-7/root$rpmlibdir/dyninst$rpmlibdir32/dyninst${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
# pythonvers=2.7
# export PYTHONPATH=/opt/rh/devtoolset-7/root/usr/lib64/python$pythonvers/site-packages:/opt/rh/devtoolset-7/root/usr/lib/python$pythonvers/site-packages${PYTHONPATH:+:${PYTHONPATH}}
# export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.222.b10-1.el7_7.x86_64
# git clone https://github.com/tensorflow/ && cd tensorflow && git checkout r2.1
# export OPTM=3
# export PYTHON_BIN_PATH=/usr/bin/python
# export USE_DEFAULT_PYTHON_LIB_PATH=1
# export CC_OPT_FLAGS=""-march=${ARCH} -mtune=$TUNE""
# export TF_NEED_JEMALLOC=1
# export TF_NEED_KAFKA=0
# export TF_NEED_OPENCL_SYCL=0
# export TF_NEED_GCP=0
# export TF_NEED_HDFS=0
# export TF_NEED_S3=0
# export TF_ENABLE_XLA=1
# export TF_NEED_GDR=0
# export TF_NEED_VERBS=0
# export TF_NEED_OPENCL=0
# export TF_NEED_MPI=0
# export TF_NEED_TENSORRT=0
# export TF_SET_ANDROID_WORKSPACE=0
# export TF_DOWNLOAD_CLANG=0
# export TF_NEED_CUDA=0
# ./configure && bazel build --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
Note this error message appears when building different components. Here are some examples:

```
ERROR: /tensorflow/tensorflow/compiler/mlir/tensorflow/BUILD:713:1: Linking of rule '//tensorflow/compiler/mlir/tensorflow:derived_attr_populator_gen' failed (Exit 1)
bazel-out/host/bin/external/llvm/_objs/tablegen/Main.o:Main.cpp:function llvm::cl::list<std::string, bool, llvm::cl::parser<std::string> >::~list(): error: undefined reference to 'operator delete(void*, unsigned long)'
```

```
ERROR: /tmp/bazel/external/com_google_protobuf/BUILD:406:1: Linking of rule '@com_google_protobuf//:protoc' failed (Exit 1)
bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/code_generator.o:code_generator.cc:function google::protobuf::compiler::ParseGeneratorParameter(std::string const&, std::vector<std::pair<std::string, std::string>, std::allocator<std::pair<std::string, std::string> > >*): error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)'
bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function google::protobuf::io::StringOutputStream::~StringOutputStream(): error: undefined reference to 'operator delete(void*, unsigned long)'
```
"
35866,TF 2.1 breaks GPU discovery,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04:
- TensorFlow installed with `pip install tensorflow_gpu`:
- TensorFlow version: `2.1`
- Python version: `3.6.8`
- Installed using virtualenv? `pip`
- CUDA/cuDNN version: `10.2`
- GPU model and memory: `8x V100-SXM2 @ 16GB on GCP`

**Describe the problem**
I have been using `tf 2.0` since some time with no issues on a multi-gpu system (8 GPUs). The newest version of the package I am using ([OpenNMT-tf 2.4](https://github.com/OpenNMT/OpenNMT-tf)) required `tf 2.1` so I had to upgrade. After that I am getting errors related to loading certain CUDA libraries. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
The error is thrown when I run: `devices = tf.config.list_logical_devices(device_type=""GPU"")`. Works fine in ` tf2.0`, fails in `tf 2.1`. When importing `tensorflow` in 2.1 I do get some warnings that are definitely connected.

**Logs when using tf.2.0**
```
>>> import tensorflow as tf
>>> tf.__version__
2.0.0
>>> devices = tf.config.experimental.list_physical_devices(device_type=""GPU"")
>>> devices
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]
```

so everything looks great. But here is the newer version:
```
>>> import tensorflow as tf
2020-01-14 14:34:47.468372: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2020-01-14 14:34:47.468469: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2020-01-14 14:34:47.468485: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.

>>> # Let's ignore all these warnings even though they are relevant.
>>> tf.__version__
'2.0.1'
>>> # Observe that my package changed the call from experimental to just config
>>> devices = tf.config.list_logical_devices(device_type=""GPU"")
2020-01-14 14:35:43.622258: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-01-14 14:37:12.937392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 14:37:12.938977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2020-01-14 14:37:12.939108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 14:37:12.940655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: 
pciBusID: 0000:00:05.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2020-01-14 14:37:12.940720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 14:37:12.942288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 2 with properties: 
pciBusID: 0000:00:06.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2020-01-14 14:37:12.942358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 14:37:12.943868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 3 with properties: 
pciBusID: 0000:00:07.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2020-01-14 14:37:12.943923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 14:37:12.945474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 4 with properties: 
pciBusID: 0000:00:08.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2020-01-14 14:37:12.945535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 14:37:12.947104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 5 with properties: 
pciBusID: 0000:00:09.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2020-01-14 14:37:12.947174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 14:37:12.948703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 6 with properties: 
pciBusID: 0000:00:0a.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2020-01-14 14:37:12.948757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 14:37:12.950260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 7 with properties: 
pciBusID: 0000:00:0b.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2020-01-14 14:37:12.953172: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-01-14 14:37:12.953284: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory
2020-01-14 14:37:12.953360: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2020-01-14 14:37:12.953427: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2020-01-14 14:37:12.953492: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2020-01-14 14:37:12.953556: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory
2020-01-14 14:37:12.957955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-14 14:37:12.958006: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-01-14 14:37:12.958522: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-01-14 14:37:12.967598: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz
2020-01-14 14:37:12.970122: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x439c380 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-01-14 14:37:12.970151: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-01-14 14:37:13.851388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 14:37:13.901617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 14:37:13.993165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 14:37:14.010794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 14:37:14.037557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 14:37:14.069363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 14:37:14.110305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 14:37:14.158077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-14 14:37:14.160296: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x44233a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-01-14 14:37:14.160327: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
2020-01-14 14:37:14.160335: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-SXM2-16GB, Compute Capability 7.0
2020-01-14 14:37:14.160340: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Tesla V100-SXM2-16GB, Compute Capability 7.0
2020-01-14 14:37:14.160346: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Tesla V100-SXM2-16GB, Compute Capability 7.0
2020-01-14 14:37:14.160352: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (4): Tesla V100-SXM2-16GB, Compute Capability 7.0
2020-01-14 14:37:14.160358: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (5): Tesla V100-SXM2-16GB, Compute Capability 7.0
2020-01-14 14:37:14.160372: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (6): Tesla V100-SXM2-16GB, Compute Capability 7.0
2020-01-14 14:37:14.160379: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (7): Tesla V100-SXM2-16GB, Compute Capability 7.0
2020-01-14 14:37:14.163651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-14 14:37:14.163680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] 

>>> devices
[]
>>> # Even if we try the old call to experimental, we don't get the logs but still fails
>>>  tf.config.experimental.list_physical_devices(device_type=""GPU"")
[]
>>>
```

Seems like there is an issue with cuda libraries. Here is the `nvidia-smi` output in case its relevant:

```
Tue Jan 14 14:51:03 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 430.26       Driver Version: 430.26       CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |
| N/A   37C    P0    46W / 300W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  Off  | 00000000:00:05.0 Off |                    0 |
| N/A   38C    P0    44W / 300W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  Off  | 00000000:00:06.0 Off |                    0 |
| N/A   37C    P0    43W / 300W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  Off  | 00000000:00:07.0 Off |                    0 |
| N/A   36C    P0    42W / 300W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-SXM2...  Off  | 00000000:00:08.0 Off |                    0 |
| N/A   35C    P0    41W / 300W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100-SXM2...  Off  | 00000000:00:09.0 Off |                    0 |
| N/A   40C    P0    44W / 300W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100-SXM2...  Off  | 00000000:00:0A.0 Off |                    0 |
| N/A   35C    P0    45W / 300W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100-SXM2...  Off  | 00000000:00:0B.0 Off |                    0 |
| N/A   39C    P0    43W / 300W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```
"
35865,ImportError: DLL load failed: The specified procedure could not be found. appears after an hour of usage,"- OS Platform: Windows 10 Enterprise (10.0.17134)
- TensorFlow installed with: pip install tensorflow-cpu==2.1.0
- CPU: Intel(R) Core(TM) i5-6500
- GPU: Intel(R) HD Graphics 530 (i do not want to use GPU, this is just for completeness)
- Python version: 3.6
- Code used in PyCharm CommunityEdition 2019.3.1
- Anaconda, version 4.7.12

I had the Problem, that after upgrading to tensorflow 2.1.0 the `import tensorflow` statement would not work
After enough search i found, that i needed the Visual C++ Redistributable for 2015, 2017 and 2019, which i now have installed. After that, i was able to run `import tensorflow`, but after about an hour later, the statement failed again. 
The only change to my anaconda-environment was, that i installed opencv via `conda install opencv`, which should not change tensorflow

The Traceback:
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\*\.conda\envs\sirenai\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
            from tensorflow_core import *
  File ""C:\Users\*\.conda\envs\sirenai\lib\site-packages\tensorflow_core\__init__.py"", line 40, in 
            <module>
            from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\*\.conda\envs\sirenai\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
            module = self._load()
  File ""C:\Users\*\.conda\envs\sirenai\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
            module = _importlib.import_module(self.__name__)
  File ""C:\Users\*\.conda\envs\sirenai\lib\importlib\__init__.py"", line 126, in import_module
            return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\*\.conda\envs\sirenai\lib\site-packages\tensorflow_core\python\__init__.py"", line 64, 
            in <module>
            from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Users\*\.conda\envs\sirenai\lib\site-packages\tensorflow_core\core\framework
            \graph_pb2.py"", line 7, in <module>
            from google.protobuf import descriptor as _descriptor
  File ""C:\Users\*\.conda\envs\sirenai\lib\site-packages\google\protobuf\descriptor.py"", line 47, in 
            <module>
            from google.protobuf.pyext import _message
ImportError: DLL load failed: The specified procedure could not be found."
35864,Failed to invoke the interpreter with error: Provided data count 602112 must match the required count 270000.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS 12.0.1
- TensorFlow installed from (source or binary): CocoaPods
- TensorFlow version (or github SHA if from source): 
  - TensorFlowLiteC (2.1.0)
  - TensorFlowLiteSwift (2.1.0):
    - TensorFlowLiteC (= 2.1.0)


**Command used to run the converter or code if you’re using the Python API**

```
import os, cv2, re, random
import numpy as np
import pandas as pd
import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing.image import img_to_array, load_img
from keras import layers, models, optimizers
from keras import backend as K
from sklearn.model_selection import train_test_split
from tensorflow.python.keras import models, layers

img_width = 150
img_height = 150
TRAIN_DIR = './input/train/'
TEST_DIR = './input/test/'
train_images_dogs_cats = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] # use this for full dataset
test_images_dogs_cats = [TEST_DIR+i for i in os.listdir(TEST_DIR)]

def atoi(text):
	return int(text) if text.isdigit() else text

def natural_keys(text):
	return [ atoi(c) for c in re.split('(\d+)', text) ]
	
train_images_dogs_cats.sort(key=natural_keys)
train_images_dogs_cats = train_images_dogs_cats[0:1300] + train_images_dogs_cats[12500:12500 + 1300]

test_images_dogs_cats.sort(key=natural_keys)

def prepare_data(list_of_images):
	""""""
	Returns two arrays: 
		x is an array of resized images
		y is an array of labels
	""""""
	x = [] # images as arrays
	y = [] # labels
	
	for image in list_of_images:
		x.append(cv2.resize(cv2.imread(image), (img_width,img_height), interpolation=cv2.INTER_CUBIC))
	
	for i in list_of_images:
		if 'dog' in i:
			y.append(1)
		elif 'cat' in i:
			y.append(0)
		#else:
			#print('neither cat nor dog name present in images')
			
	return x, y
	
X, Y = prepare_data(train_images_dogs_cats)
print(K.image_data_format())

# First split the data in two sets, 80% for training, 20% for Val/Test)
X_train, X_val, Y_train, Y_val = train_test_split(X,Y, test_size=0.2, random_state=1)

nb_train_samples = len(X_train)
nb_validation_samples = len(X_val)
batch_size = 16

model = models.Sequential()

model.add(layers.Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3)))
model.add(layers.Activation('relu'))
model.add(layers.MaxPooling2D(pool_size=(2, 2)))

model.add(layers.Conv2D(32, (3, 3)))
model.add(layers.Activation('relu'))
model.add(layers.MaxPooling2D(pool_size=(2, 2)))

model.add(layers.Conv2D(64, (3, 3)))
model.add(layers.Activation('relu'))
model.add(layers.MaxPooling2D(pool_size=(2, 2)))

model.add(layers.Flatten())
model.add(layers.Dense(64))
model.add(layers.Activation('relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(1))
model.add(layers.Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
							optimizer='rmsprop',
							metrics=['accuracy'])



model.summary()

train_datagen = ImageDataGenerator(
	rescale=1. / 255,
	shear_range=0.2,
	zoom_range=0.2,
	horizontal_flip=True)

val_datagen = ImageDataGenerator(
	rescale=1. / 255,
	shear_range=0.2,
	zoom_range=0.2,
	horizontal_flip=True)
	
train_generator = train_datagen.flow(np.array(X_train), Y_train, batch_size=batch_size)
validation_generator = val_datagen.flow(np.array(X_val), Y_val, batch_size=batch_size)

history = model.fit_generator(
	train_generator, 
	steps_per_epoch=nb_train_samples // batch_size,
	epochs=30,
	validation_data=validation_generator,
	validation_steps=nb_validation_samples // batch_size
)

model.save_weights('model_wieghts.h5')
model.save('model_keras.h5')

X_test, Y_test = prepare_data(test_images_dogs_cats) #Y_test in this case will be []

test_datagen = ImageDataGenerator(rescale=1. / 255)

test_generator = val_datagen.flow(np.array(X_test), batch_size=batch_size)
prediction_probabilities = model.predict_generator(test_generator, verbose=1)

counter = range(1, len(test_images_dogs_cats) + 1)
solution = pd.DataFrame({""id"": counter, ""label"":list(prediction_probabilities)})
cols = ['label']

for col in cols:
	solution[col] = solution[col].map(lambda x: str(x).lstrip('[').rstrip(']')).astype(float)

solution.to_csv(""dogsVScats.csv"", index = False)

# Convert the model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
open(""./model_from_keras.tflite"", ""wb"").write(tflite_model)
```

**The output from the converter invocation**
Log from console:
```
Failed to invoke the interpreter with error: Provided data count 602112 must match the required count 270000.
```

**Also, please include a link to the saved model or GraphDef**

```
https://gofile.io/?c=nOBHAC
```

**Failure details**
I took an example for iOS.
Then I generated a model using this instruction (the modified script is above):
https://www.kaggle.com/sarvajna/dogs-vs-cats-keras-solution
Then I replaced the old .tflite file with the new one.
Result - the error with data size. Where and how to fix it

P.S. It seems I have the same/similar issue but for iOS:
https://github.com/tensorflow/tensorflow/issues/21602"
35863,Validation and Evaluation Metric Issues in TensorFlow when Transfer Learning,"I have come across some odd behaviours when training CNNs with Tensorflow 2.0 and would appreciate any help in solving them. I am doing transfer learning (just training the classification head) using the pre-trained networks available in 'tensorflow.keras.applications' and have noticed the following:

 1. For the first epoch, validation metrics are always zero, no matter what I do.  
 2. When training after the first epoch, the training metrics improve as you would expect, but the validation metrics essentially are random guesses, even when the EXACT same dataset is used as a training and a validation dataset. It is like it isn't using the model being trained to do its evaluation.

I have tried, VGG16, MobileNetV2, and ResNet50V2, and they all exhibit the same behaviours. 

The configurations I am able to reproduce this on are:
 - Ubuntu 18.04LTS, Nvidia RTX2080ti with driver version 430.50, CUDA10.0, TensorFlow-gpu==2.0.0 
 - MacBook Pro, TensorFlow==2.0.0 (cpu)

Both are running in Conda environments and I have installed TensorFlow with pip. I have put some sample code to show the essence of my workflow down below just in case I am doing anything obviously stupid. Any help would be very appreciated as I am at a loss as to how to fix it.

`
def parse_function(example_proto):
    image_feature_description = {
        'label': tf.io.FixedLenFeature([], tf.int64),
        'image_raw': tf.io.FixedLenFeature([], tf.string)
    }
    parsed_example = tf.io.parse_single_example(example_proto, image_feature_description)
    image = tf.io.decode_image(
                            parsed_example['image_raw'], 
                            channels = 3, 
                            dtype = tf.float32, 
                            expand_animations = False
                            )
    image = tf.image.per_image_standardization(image)
    label = tf.one_hot(parsed_example['label'], 24, dtype=tf.float32) 
    return (image, label)

def load_dataset(TFRecord_dir, record_name):
    record_files = tf.io.matching_files(os.path.join(TFRecord_dir, record_name + '.tfrecords-????'))
    shards = tf.data.TFRecordDataset(record_files)
    shards = shards.shuffle(tf.cast(tf.shape(record_files)[0], tf.int64))
    dataset = shards.map(map_func=parse_function)
    dataset = dataset.batch(batch_size=16, drop_remainder = True)
    dataset = dataset.prefetch(16)
    return dataset


base_model = tf.keras.applications.ResNet50V2(
                                            input_shape=(224,224,3),
                                            weights='imagenet',
                                            include_top = False
                                            )
base_model.trainable = False

model = tf.keras.Sequential([
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(24, activation = 'softmax')
        ])

model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.CategoricalCrossentropy(),
    metrics=[ 
            tf.keras.metrics.CategoricalAccuracy(),
            tf.keras.metrics.TopKCategoricalAccuracy(),
            tf.keras.metrics.Precision(),
            tf.keras.metrics.Recall()
            ])

train_dataset = load_dataset(train_dir, 'train')

model.fit(train_dataset,
                verbose = 1,
                epochs= 5,
                validation_data = train_dataset)
model.evaluate(train_dataset)
`




"
35861,Errors with tf.data.Dataset shape ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: no GPU

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior** ""Error when checking input: expected input1 to have 4 dimensions, but got array with shape (96, 96, 1)"" 

**Describe the expected behavior** tf.data.dataset size its [[90, 96, 96, 1], [90, 6]], Input shape of CNN Model is (None,96,96,1) - so this should not give error

**Code to reproduce the issue**
https://www.kaggle.com/anirbank/bengali-graphemes-starter-eda-basic-cnn-model

**Other info / logs**
Logs given in code: Input shape:
(None, 96, 96, 1)
tf.data.Dataset shape : [[90, 96, 96, 1], [90, 6]]

"
35860,TensorBoard callback without profile_batch setting cause Errors: CUPTI_ERROR_INSUFFICIENT_PRIVILEGES and CUPTI_ERROR_INVALID_PARAMETER,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Stateless LSTM from Keras tutorial using tf backend
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.4
- CUDA/cuDNN version: 10.1
- GPU model and memory: MX150 10GB

**Describe the current behavior**
When using tf.keras.callbacks.TensorBoard() without the profile_batch setting, it gives out errors of CUPTI_ERROR_INSUFFICIENT_PRIVILEGES and CUPTI_ERROR_INVALID_PARAMETER from tensorflow/core/profiler/internal/gpu/cupti_tracer.cc.

**Describe the expected behavior**
With profile_batch = 0, these two errors are gone. 
But comes back when profile_batch = 1, or other non-zero values.

**Code to reproduce the issue**
```

from __future__ import print_function
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM


input_len = 1000
tsteps = 2
lahead = 1
batch_size = 1
epochs = 5

print(""*"" * 33)
if lahead >= tsteps:
    print(""STATELESS LSTM WILL ALSO CONVERGE"")
else:
    print(""STATELESS LSTM WILL NOT CONVERGE"")
print(""*"" * 33)

np.random.seed(1986)

print('Generating Data...')


def gen_uniform_amp(amp=1, xn=10000):

    data_input = np.random.uniform(-1 * amp, +1 * amp, xn)
    data_input = pd.DataFrame(data_input)
    return data_input


to_drop = max(tsteps - 1, lahead - 1)
data_input = gen_uniform_amp(amp=0.1, xn=input_len + to_drop)

expected_output = data_input.rolling(window=tsteps, center=False).mean()

if lahead > 1:
    data_input = np.repeat(data_input.values, repeats=lahead, axis=1)
    data_input = pd.DataFrame(data_input)
    for i, c in enumerate(data_input.columns):
        data_input[c] = data_input[c].shift(i)

expected_output = expected_output[to_drop:]
data_input = data_input[to_drop:]


def create_model(stateful):
    model = Sequential()
    model.add(LSTM(20,
              input_shape=(lahead, 1),
              batch_size=batch_size,
              stateful=stateful))
    model.add(Dense(1))
    model.compile(loss='mse', optimizer='adam')
    return model

print('Creating Stateful Model...')
model_stateful = create_model(stateful=True)


def split_data(x, y, ratio=0.8):
    to_train = int(input_len * ratio)
    to_train -= to_train % batch_size
    x_train = x[:to_train]
    y_train = y[:to_train]
    x_test = x[to_train:]
    y_test = y[to_train:]

    # tweak to match with batch_size
    to_drop = x.shape[0] % batch_size
    if to_drop > 0:
        x_test = x_test[:-1 * to_drop]
        y_test = y_test[:-1 * to_drop]

    # some reshaping
    reshape_3 = lambda x: x.values.reshape((x.shape[0], x.shape[1], 1))
    x_train = reshape_3(x_train)
    x_test = reshape_3(x_test)

    reshape_2 = lambda x: x.values.reshape((x.shape[0], 1))
    y_train = reshape_2(y_train)
    y_test = reshape_2(y_test)

    return (x_train, y_train), (x_test, y_test)


(x_train, y_train), (x_test, y_test) = split_data(data_input, expected_output)
print('x_train.shape: ', x_train.shape)
print('y_train.shape: ', y_train.shape)
print('x_test.shape: ', x_test.shape)
print('y_test.shape: ', y_test.shape)

print('Creating Stateless Model...')
model_stateless = create_model(stateful=False)

import os
import datetime
ROOT_DIR = os.getcwd()
log_dir = os.path.join('callback_tests')
if not os.path.exists(log_dir):
    os.makedirs(log_dir)
print(log_dir)
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)
                                       
print('Training')
history = model_stateless.fit(x_train,
                    y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_data=(x_test, y_test),
                    shuffle=False,
                    callbacks=[tensorboard_callback]
                    )


```

**Other info / logs**
Train on 800 samples, validate on 200 samples
2020-01-14 21:30:27.591905: I tensorflow/core/profiler/lib/profiler_session.cc:225] Profiler session started.
2020-01-14 21:30:27.594743: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1259] Profiler found 1 GPUs
2020-01-14 21:30:27.599172: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cupti64_101.dll
2020-01-14 21:30:27.704083: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES
2020-01-14 21:30:27.716790: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1346] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES
Epoch 1/5
2020-01-14 21:30:28.370429: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-01-14 21:30:28.651767: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-01-14 21:30:29.662864: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1329] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER
2020-01-14 21:30:29.670282: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:88]  GpuTracer has collected 0 callback api events and 0 activity events.
800/800 [==============================] - 5s 6ms/sample - loss: 0.0011 - val_loss: 0.0011
Epoch 2/5
800/800 [==============================] - 3s 4ms/sample - loss: 8.5921e-04 - val_loss: 0.0010
Epoch 3/5
800/800 [==============================] - 3s 3ms/sample - loss: 8.5613e-04 - val_loss: 0.0010
Epoch 4/5
800/800 [==============================] - 3s 4ms/sample - loss: 8.5458e-04 - val_loss: 9.9713e-04
Epoch 5/5
800/800 [==============================] - 3s 4ms/sample - loss: 8.5345e-04 - val_loss: 9.8825e-04
"
35858,Memory leak in LSTM with dropout when executed in eager mode,"**System information**
- Have I written custom code: Simple LSTM layer with dropouts in eager loop
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: pip
- TensorFlow version: v2.0.0 and v2.1.0
- Python version: 3.6.8

**Describe the current behavior**
An LSTM layer with dropout enabled leaks memory in each eager execution. This is likely due to the dropout mask in the RNN layer not being properly released (related to #19671). 

If the dropout arguments are set to 0 then the memory remains constant.

**Describe the expected behavior**
The memory usage should remain constant among batches with dropouts enabled.

**Code to reproduce the issue**
```python
import tensorflow as tf

layer = tf.keras.layers.LSTM(64, return_sequences=True, 
                             dropout=0.4, 
                             recurrent_dropout=0.2)

while True:
    X = tf.random.uniform((500, 100, 64))
    pred = layer(X, training=True)
```

**Other info / logs**
Memory usage when dropouts are enabled taken with memory_profiler
![memory_leak](https://user-images.githubusercontent.com/5730052/72333667-70a78600-36bc-11ea-9ac7-7bc71ec2468d.png)

Memory usage when dropouts are disabled
![no_memory_leak](https://user-images.githubusercontent.com/5730052/72333733-95036280-36bc-11ea-982c-4621ec5776da.png)

**Workaround**
A working workaround is to set `tf.random.set_seed(seed)` in each iteration, which seems to clear the cached masks, but it can seriously affect the experiments."
35857,How to do Early stopping with monitoring weighted metric of multi-output model?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow version (use command below): 2.1
- Python version: 3

**Describe the current behavior**
Is there any way to do early stopping with monitoring weighted metric of multi-output model?
I want to have a ""global"" metric like weighted accuracy to control my training process.
But I cant find any example or solution on the internet.  

**Code to reproduce the issue**
https://colab.research.google.com/drive/1mcqYo0OFs91uxo3mN82UpLCqCymSKTAI#scrollTo=vqTq1JH5U7Fj
```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

image_input = keras.Input(shape=(32, 32, 3), name='img_input')
timeseries_input = keras.Input(shape=(None, 10), name='ts_input')

x1 = layers.Conv2D(3, 3)(image_input)
x1 = layers.GlobalMaxPooling2D()(x1)

x2 = layers.Conv1D(3, 3)(timeseries_input)
x2 = layers.GlobalMaxPooling1D()(x2)

x = layers.concatenate([x1, x2])

score_output = layers.Dense(1, name='score_output')(x)
class_output = layers.Dense(5, activation='softmax', name='class_output')(x)

model = keras.Model(inputs=[image_input, timeseries_input],
                    outputs=[score_output, class_output])
model.compile(
    optimizer=keras.optimizers.RMSprop(1e-3),
    loss={'score_output': keras.losses.MeanSquaredError(),
          'class_output': keras.losses.CategoricalCrossentropy()},
    metrics={'score_output': [keras.metrics.CategoricalAccuracy()],
             'class_output': [keras.metrics.CategoricalAccuracy()]},
    loss_weights={'score_output': 2., 'class_output': 1.})
callbacks = [
    # keras.callbacks.EarlyStopping(""score_output_categorical_accuracy"", patience=2, restore_best_weights=True, mode=""auto"")
    keras.callbacks.EarlyStopping(""weighted_categorical_accuracy"", patience=2, restore_best_weights=True, mode=""auto"")
]
import numpy as np
# Generate dummy Numpy data
img_data = np.random.random_sample(size=(100, 32, 32, 3))
ts_data = np.random.random_sample(size=(100, 20, 10))
score_targets = np.random.random_sample(size=(100, 1))
class_targets = np.random.random_sample(size=(100, 5))

# Fit on lists
model.fit([img_data, ts_data], [score_targets, class_targets],
          callbacks=callbacks,
          batch_size=32,
          epochs=10)
```

**Other info / logs**
`WARNING:tensorflow:Early stopping conditioned on metric `weighted_categorical_accuracy` which is not available.`
"
35855,"build failed on Centos 6.7 with Error: no such instruction: `mulxq %r9,%r8,%r9'","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS release 6.7 (Final)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12.0
- Python version: 2.7.9
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 4.8.2
- CUDA/cuDNN version: no
- GPU model and memory:


**Describe the problem**
ERROR: /home/xiaoju/.cache/bazel/_bazel_xiaoju/be98a53b27ec7c052c1f167e0f93692d/external/boringssl/BUILD:130:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1)
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S: Assembler messages:
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:758: Error: no such instruction: `mulxq %r9,%r8,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:759: Error: no such instruction: `mulxq %r10,%rcx,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:760: Error: no such instruction: `mulxq %r11,%rbp,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:762: Error: no such instruction: `mulxq %r12,%rcx,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:764: Error: no such instruction: `mulxq %r15,%rdx,%rax'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:771: Error: no such instruction: `mulxq 0+128(%r14),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:772: Error: no such instruction: `adcxq %rcx,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:773: Error: no such instruction: `adoxq %rbp,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:775: Error: no such instruction: `mulxq 8+128(%r14),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:776: Error: no such instruction: `adcxq %rcx,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:777: Error: no such instruction: `adoxq %rbp,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:779: Error: no such instruction: `mulxq 16+128(%r14),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:780: Error: no such instruction: `adcxq %rcx,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:781: Error: no such instruction: `adoxq %rbp,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:783: Error: no such instruction: `mulxq 24+128(%r14),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:785: Error: no such instruction: `adcxq %rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:786: Error: no such instruction: `adoxq %rbp,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:787: Error: no such instruction: `adcxq %r8,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:788: Error: no such instruction: `adoxq %r8,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:792: Error: no such instruction: `mulxq 0+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:793: Error: no such instruction: `adcxq %rcx,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:794: Error: no such instruction: `adoxq %rbp,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:796: Error: no such instruction: `mulxq 8+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:797: Error: no such instruction: `adcxq %rcx,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:798: Error: no such instruction: `adoxq %rbp,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:800: Error: no such instruction: `mulxq 16+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:801: Error: no such instruction: `adcxq %rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:802: Error: no such instruction: `adoxq %rbp,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:804: Error: no such instruction: `mulxq 24+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:806: Error: no such instruction: `mulxq %r15,%rdx,%rax'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:807: Error: no such instruction: `adcxq %rcx,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:808: Error: no such instruction: `adoxq %rbp,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:810: Error: no such instruction: `adcxq %r8,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:811: Error: no such instruction: `adoxq %r8,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:815: Error: no such instruction: `mulxq 0+128(%r14),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:816: Error: no such instruction: `adcxq %rcx,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:817: Error: no such instruction: `adoxq %rbp,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:819: Error: no such instruction: `mulxq 8+128(%r14),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:820: Error: no such instruction: `adcxq %rcx,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:821: Error: no such instruction: `adoxq %rbp,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:823: Error: no such instruction: `mulxq 16+128(%r14),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:824: Error: no such instruction: `adcxq %rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:825: Error: no such instruction: `adoxq %rbp,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:827: Error: no such instruction: `mulxq 24+128(%r14),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:829: Error: no such instruction: `adcxq %rcx,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:830: Error: no such instruction: `adoxq %rbp,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:831: Error: no such instruction: `adcxq %r9,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:832: Error: no such instruction: `adoxq %r9,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:836: Error: no such instruction: `mulxq 0+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:837: Error: no such instruction: `adcxq %rcx,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:838: Error: no such instruction: `adoxq %rbp,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:840: Error: no such instruction: `mulxq 8+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:841: Error: no such instruction: `adcxq %rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:842: Error: no such instruction: `adoxq %rbp,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:844: Error: no such instruction: `mulxq 16+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:845: Error: no such instruction: `adcxq %rcx,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:846: Error: no such instruction: `adoxq %rbp,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:848: Error: no such instruction: `mulxq 24+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:850: Error: no such instruction: `mulxq %r15,%rdx,%rax'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:851: Error: no such instruction: `adcxq %rcx,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:852: Error: no such instruction: `adoxq %rbp,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:854: Error: no such instruction: `adcxq %r9,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:855: Error: no such instruction: `adoxq %r9,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:859: Error: no such instruction: `mulxq 0+128(%r14),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:860: Error: no such instruction: `adcxq %rcx,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:861: Error: no such instruction: `adoxq %rbp,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:863: Error: no such instruction: `mulxq 8+128(%r14),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:864: Error: no such instruction: `adcxq %rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:865: Error: no such instruction: `adoxq %rbp,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:867: Error: no such instruction: `mulxq 16+128(%r14),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:868: Error: no such instruction: `adcxq %rcx,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:869: Error: no such instruction: `adoxq %rbp,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:871: Error: no such instruction: `mulxq 24+128(%r14),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:873: Error: no such instruction: `adcxq %rcx,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:874: Error: no such instruction: `adoxq %rbp,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:875: Error: no such instruction: `adcxq %r10,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:876: Error: no such instruction: `adoxq %r10,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:880: Error: no such instruction: `mulxq 0+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:881: Error: no such instruction: `adcxq %rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:882: Error: no such instruction: `adoxq %rbp,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:884: Error: no such instruction: `mulxq 8+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:885: Error: no such instruction: `adcxq %rcx,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:886: Error: no such instruction: `adoxq %rbp,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:888: Error: no such instruction: `mulxq 16+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:889: Error: no such instruction: `adcxq %rcx,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:890: Error: no such instruction: `adoxq %rbp,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:892: Error: no such instruction: `mulxq 24+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:894: Error: no such instruction: `mulxq %r15,%rdx,%rax'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:895: Error: no such instruction: `adcxq %rcx,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:896: Error: no such instruction: `adoxq %rbp,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:898: Error: no such instruction: `adcxq %r10,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:899: Error: no such instruction: `adoxq %r10,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:903: Error: no such instruction: `mulxq 0+128(%r14),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:904: Error: no such instruction: `adcxq %rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:905: Error: no such instruction: `adoxq %rbp,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:907: Error: no such instruction: `mulxq 8+128(%r14),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:908: Error: no such instruction: `adcxq %rcx,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:909: Error: no such instruction: `adoxq %rbp,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:911: Error: no such instruction: `mulxq 16+128(%r14),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:912: Error: no such instruction: `adcxq %rcx,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:913: Error: no such instruction: `adoxq %rbp,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:915: Error: no such instruction: `mulxq 24+128(%r14),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:918: Error: no such instruction: `adcxq %rcx,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:919: Error: no such instruction: `adoxq %rbp,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:921: Error: no such instruction: `adcxq %r11,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:922: Error: no such instruction: `adoxq %r11,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:999: Error: no such instruction: `mulxq %r14,%r9,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1000: Error: no such instruction: `mulxq %r15,%rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1003: Error: no such instruction: `mulxq %r8,%rbp,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1011: Error: no such instruction: `mulxq %r15,%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1012: Error: no such instruction: `adcxq %rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1013: Error: no such instruction: `adoxq %rbp,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1015: Error: no such instruction: `mulxq %r8,%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1017: Error: no such instruction: `adcxq %rcx,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1018: Error: no such instruction: `adoxq %rbp,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1021: Error: no such instruction: `mulxq %r8,%rcx,%r14'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1025: Error: no such instruction: `adcxq %r9,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1026: Error: no such instruction: `adoxq %rcx,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1027: Error: no such instruction: `adcxq %r10,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1028: Error: no such instruction: `adoxq %r15,%r14'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1031: Error: no such instruction: `mulxq %rdx,%r8,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1033: Error: no such instruction: `adcxq %r11,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1034: Error: no such instruction: `adoxq %rbp,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1035: Error: no such instruction: `adcxq %r12,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1036: Error: no such instruction: `mulxq %rdx,%rcx,%rax'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1038: Error: no such instruction: `adcxq %r13,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1039: Error: no such instruction: `adoxq %rcx,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1040: Error: no such instruction: `adcxq %r14,%r14'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1041: Error: no such instruction: `mulxq %rdx,%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1044: Error: no such instruction: `adoxq %rax,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1045: Error: no such instruction: `adcxq %r15,%r15'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1046: Error: no such instruction: `adoxq %rcx,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1047: Error: no such instruction: `adoxq %rbp,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1048: Error: no such instruction: `mulxq %rdx,%rcx,%rax'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1049: Error: no such instruction: `adoxq %rcx,%r14'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1050: Error: no such instruction: `adoxq %rax,%r15'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1054: Error: no such instruction: `mulxq 32(%rsi),%rdx,%rcx'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1057: Error: no such instruction: `mulxq 0(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1058: Error: no such instruction: `adcxq %rcx,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1059: Error: no such instruction: `adoxq %rbp,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1060: Error: no such instruction: `mulxq 8(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1061: Error: no such instruction: `adcxq %rcx,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1062: Error: no such instruction: `adoxq %rbp,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1063: Error: no such instruction: `mulxq 16(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1064: Error: no such instruction: `adcxq %rcx,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1065: Error: no such instruction: `adoxq %rbp,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1066: Error: no such instruction: `mulxq 24(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1067: Error: no such instruction: `adcxq %rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1068: Error: no such instruction: `adoxq %rbp,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1069: Error: no such instruction: `adcxq %rax,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1073: Error: no such instruction: `mulxq 32(%rsi),%rdx,%rcx'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1075: Error: no such instruction: `mulxq 0(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1076: Error: no such instruction: `adoxq %rcx,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1077: Error: no such instruction: `adcxq %rbp,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1078: Error: no such instruction: `mulxq 8(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1079: Error: no such instruction: `adoxq %rcx,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1080: Error: no such instruction: `adcxq %rbp,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1081: Error: no such instruction: `mulxq 16(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1082: Error: no such instruction: `adoxq %rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1083: Error: no such instruction: `adcxq %rbp,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1084: Error: no such instruction: `mulxq 24(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1085: Error: no such instruction: `adoxq %rcx,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1086: Error: no such instruction: `adcxq %rbp,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1087: Error: no such instruction: `adoxq %rax,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1091: Error: no such instruction: `mulxq 32(%rsi),%rdx,%rcx'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1093: Error: no such instruction: `mulxq 0(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1094: Error: no such instruction: `adcxq %rcx,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1095: Error: no such instruction: `adoxq %rbp,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1096: Error: no such instruction: `mulxq 8(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1097: Error: no such instruction: `adcxq %rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1098: Error: no such instruction: `adoxq %rbp,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1099: Error: no such instruction: `mulxq 16(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1100: Error: no such instruction: `adcxq %rcx,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1101: Error: no such instruction: `adoxq %rbp,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1102: Error: no such instruction: `mulxq 24(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1103: Error: no such instruction: `adcxq %rcx,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1104: Error: no such instruction: `adoxq %rbp,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1105: Error: no such instruction: `adcxq %rax,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1109: Error: no such instruction: `mulxq 32(%rsi),%rdx,%rcx'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1111: Error: no such instruction: `mulxq 0(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1112: Error: no such instruction: `adoxq %rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1113: Error: no such instruction: `adcxq %rbp,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1114: Error: no such instruction: `mulxq 8(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1115: Error: no such instruction: `adoxq %rcx,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1116: Error: no such instruction: `adcxq %rbp,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1117: Error: no such instruction: `mulxq 16(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1118: Error: no such instruction: `adoxq %rcx,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1119: Error: no such instruction: `adcxq %rbp,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1120: Error: no such instruction: `mulxq 24(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1121: Error: no such instruction: `adoxq %rcx,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1122: Error: no such instruction: `adcxq %rbp,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1123: Error: no such instruction: `adoxq %rax,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1711: Error: no such instruction: `mulxq %r9,%r8,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1712: Error: no such instruction: `mulxq %r10,%rcx,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1715: Error: no such instruction: `mulxq %r11,%rbp,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1718: Error: no such instruction: `mulxq %r12,%rcx,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1721: Error: no such instruction: `shlxq %r14,%r8,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1723: Error: no such instruction: `shrxq %r14,%r8,%rcx'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1731: Error: no such instruction: `mulxq %r15,%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1740: Error: no such instruction: `mulxq 0+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1741: Error: no such instruction: `adcxq %rcx,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1742: Error: no such instruction: `adoxq %rbp,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1744: Error: no such instruction: `mulxq 8+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1745: Error: no such instruction: `adcxq %rcx,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1746: Error: no such instruction: `adoxq %rbp,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1748: Error: no such instruction: `mulxq 16+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1749: Error: no such instruction: `adcxq %rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1750: Error: no such instruction: `adoxq %rbp,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1752: Error: no such instruction: `mulxq 24+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1754: Error: no such instruction: `adcxq %rcx,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1755: Error: no such instruction: `shlxq %r14,%r9,%rcx'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1756: Error: no such instruction: `adoxq %rbp,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1757: Error: no such instruction: `shrxq %r14,%r9,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1759: Error: no such instruction: `adcxq %r8,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1760: Error: no such instruction: `adoxq %r8,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1768: Error: no such instruction: `mulxq %r15,%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1777: Error: no such instruction: `mulxq 0+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1778: Error: no such instruction: `adcxq %rcx,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1779: Error: no such instruction: `adoxq %rbp,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1781: Error: no such instruction: `mulxq 8+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1782: Error: no such instruction: `adcxq %rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1783: Error: no such instruction: `adoxq %rbp,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1785: Error: no such instruction: `mulxq 16+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1786: Error: no such instruction: `adcxq %rcx,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1787: Error: no such instruction: `adoxq %rbp,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1789: Error: no such instruction: `mulxq 24+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1791: Error: no such instruction: `adcxq %rcx,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1792: Error: no such instruction: `shlxq %r14,%r10,%rcx'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1793: Error: no such instruction: `adoxq %rbp,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1794: Error: no such instruction: `shrxq %r14,%r10,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1796: Error: no such instruction: `adcxq %r9,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1797: Error: no such instruction: `adoxq %r9,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1805: Error: no such instruction: `mulxq %r15,%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1814: Error: no such instruction: `mulxq 0+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1815: Error: no such instruction: `adcxq %rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1816: Error: no such instruction: `adoxq %rbp,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1818: Error: no such instruction: `mulxq 8+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1819: Error: no such instruction: `adcxq %rcx,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1820: Error: no such instruction: `adoxq %rbp,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1822: Error: no such instruction: `mulxq 16+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1823: Error: no such instruction: `adcxq %rcx,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1824: Error: no such instruction: `adoxq %rbp,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1826: Error: no such instruction: `mulxq 24+128(%rsi),%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1828: Error: no such instruction: `adcxq %rcx,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1829: Error: no such instruction: `shlxq %r14,%r11,%rcx'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1830: Error: no such instruction: `adoxq %rbp,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1831: Error: no such instruction: `shrxq %r14,%r11,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1833: Error: no such instruction: `adcxq %r10,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1834: Error: no such instruction: `adoxq %r10,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1842: Error: no such instruction: `mulxq %r15,%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1876: Error: no such instruction: `mulxq %r14,%r9,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1877: Error: no such instruction: `mulxq %r15,%rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1880: Error: no such instruction: `mulxq %r8,%rbp,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1887: Error: no such instruction: `mulxq %r15,%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1888: Error: no such instruction: `adcxq %rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1889: Error: no such instruction: `adoxq %rbp,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1891: Error: no such instruction: `mulxq %r8,%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1893: Error: no such instruction: `adcxq %rcx,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1894: Error: no such instruction: `adoxq %rbp,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1898: Error: no such instruction: `mulxq %r8,%rcx,%r14'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1901: Error: no such instruction: `adcxq %r9,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1902: Error: no such instruction: `adoxq %rcx,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1903: Error: no such instruction: `adcxq %r10,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1904: Error: no such instruction: `adoxq %r15,%r14'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1906: Error: no such instruction: `mulxq %rdx,%r8,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1908: Error: no such instruction: `adcxq %r11,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1909: Error: no such instruction: `adoxq %rbp,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1910: Error: no such instruction: `adcxq %r12,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1911: Error: no such instruction: `mulxq %rdx,%rcx,%rax'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1913: Error: no such instruction: `adcxq %r13,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1914: Error: no such instruction: `adoxq %rcx,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1915: Error: no such instruction: `adcxq %r14,%r14'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1917: Error: no such instruction: `mulxq %rdx,%rcx,%rbp'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1919: Error: no such instruction: `adoxq %rax,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1920: Error: no such instruction: `adcxq %r15,%r15'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1921: Error: no such instruction: `adoxq %rcx,%r12'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1923: Error: no such instruction: `adoxq %rbp,%r13'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1925: Error: no such instruction: `mulxq %rdx,%rcx,%rax'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1927: Error: no such instruction: `adoxq %rcx,%r14'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1928: Error: no such instruction: `shlxq %rsi,%r8,%rcx'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1929: Error: no such instruction: `adoxq %rax,%r15'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1930: Error: no such instruction: `shrxq %rsi,%r8,%rax'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1937: Error: no such instruction: `mulxq %r8,%rcx,%r8'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1939: Error: no such instruction: `shlxq %rsi,%r9,%rcx'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1941: Error: no such instruction: `shrxq %rsi,%r9,%rax'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1947: Error: no such instruction: `mulxq %r9,%rcx,%r9'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1949: Error: no such instruction: `shlxq %rsi,%r10,%rcx'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1951: Error: no such instruction: `shrxq %rsi,%r10,%rax'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1957: Error: no such instruction: `mulxq %r10,%rcx,%r10'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1959: Error: no such instruction: `shlxq %rsi,%r11,%rcx'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1961: Error: no such instruction: `shrxq %rsi,%r11,%rax'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1967: Error: no such instruction: `mulxq %r11,%rcx,%r11'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2127: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2128: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2129: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2135: Error: no such instruction: `vpermd %ymm1,%ymm2,%ymm1'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2148: Error: suffix or operands invalid for `vpcmpeqd'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2149: Error: suffix or operands invalid for `vpcmpeqd'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2151: Error: suffix or operands invalid for `vpaddd'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2152: Error: suffix or operands invalid for `vpaddd'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2155: Error: suffix or operands invalid for `vpand'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2156: Error: suffix or operands invalid for `vpand'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2157: Error: suffix or operands invalid for `vpand'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2158: Error: suffix or operands invalid for `vpand'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2159: Error: suffix or operands invalid for `vpand'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2160: Error: suffix or operands invalid for `vpand'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2162: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2163: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2164: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2165: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2166: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2167: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2191: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2192: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2199: Error: no such instruction: `vpermd %ymm1,%ymm2,%ymm1'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2214: Error: suffix or operands invalid for `vpcmpeqd'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2215: Error: suffix or operands invalid for `vpcmpeqd'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2216: Error: suffix or operands invalid for `vpcmpeqd'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2218: Error: suffix or operands invalid for `vpaddd'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2219: Error: suffix or operands invalid for `vpaddd'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2220: Error: suffix or operands invalid for `vpaddd'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2223: Error: suffix or operands invalid for `vpand'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2224: Error: suffix or operands invalid for `vpand'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2225: Error: suffix or operands invalid for `vpand'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2226: Error: suffix or operands invalid for `vpand'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2227: Error: suffix or operands invalid for `vpand'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2228: Error: suffix or operands invalid for `vpand'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2230: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2231: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2232: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2233: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2234: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2235: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2244: Error: suffix or operands invalid for `vpcmpeqd'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2246: Error: suffix or operands invalid for `vpand'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2247: Error: suffix or operands invalid for `vpand'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2249: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2250: Error: suffix or operands invalid for `vpxor'
Target //tensorflow:libtensorflow_cc.so failed to build

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build -c opt //tensorflow:libtensorflow_cc.so
"
35853,Custom Metrics The tensor cannot be accessed here.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 2.1.0 
- **Python version**: 3.7
- **CUDA/cuDNN version**: 
- **GPU model and memory**:
- **Exact command to reproduce**:


### Describe the problem

The Custom Metric I use cannot be used when compiling the following error is thrown.
The Custom Metric is:

`

class CaseAccuracy(tf.keras.metrics.Metric):
    def __init__(self, name='case_accuracy', thresholds: float = 0.5, num_classes: int = 24,
                 dtype=tf.float32, **kwargs):
        super(CaseAccuracy, self).__init__(name=name, dtype=dtype, **kwargs)
        self.thresholds = thresholds
        self.num_classes = num_classes
        self.correct_samples = self.add_weight(name='correct_samples', shape=(1,), initializer='zeros', dtype=self.dtype)
        self.num_samples = self.add_weight(name='num_samples', shape=(1,), initializer='zeros', dtype=self.dtype)
        self.accuracy = self.add_weight(name='case_accuracy', shape=(1,), initializer='zeros', dtype=self.dtype)

    def update_state(self, y_true, y_pred, **kwargs):
        """"""
        Updates the state of the Metric.

        :param y_true:
        :param y_pred:
        :return:
        """"""
        # Get binary results for predictions
        y_pred = tf.where(y_pred >= self.thresholds, 1., 0.)

        # Get binary result if prediction is correct
        corr = tf.where(y_pred == y_true, 1., 0.)

        # Get binary result if all predictions for a sample are correct
        all_corr = tf.where(tf.reduce_sum(corr, axis=-1) == y_pred.shape[-1], 1., 0.,)
        self.correct_samples = self.correct_samples + tf.reduce_sum(all_corr, axis=-1)
        self.num_samples = tf.add(self.num_samples, self.num_classes, name='num_samples_add')
        self.accuracy = tf.divide(self.correct_samples, self.num_samples, name='case_accuracy_div')
        return self.accuracy

    def result(self):
        return tf.divide(self.correct_samples, self.num_samples, name='case_accuracy_result')

    def reset_states(self):
        # The state of the metric will be reset at the start of each epoch.
        self.correct_samples.assign(0.)
        self.num_samples.assign(0.)
        self.accuracy.assign(0.)
`

### Source code / logs
`
from aggregation.model import get_model, CaseAccuracy
input_shapes = [(20, 24), (10, 24)]
num_classes = 24
model = get_model(input_shapes, num_classes)
try:
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(),
                 CaseAccuracy()]
    )
    model.summary()
except Exception as e:
    print(e)

    
2020-01-14 09:05:44.297580: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-01-14 09:05:48.583880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-01-14 09:05:48.606855: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2020-01-14 09:05:48.609872: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ...
2020-01-14 09:05:48.610491: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 
2020-01-14 09:05:48.610973: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
The tensor 'Tensor(""add:0"", dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=update_state, id=1923611490848); accessed from: FuncGraph(name=keras_graph, id=1923588214736).
`"
35852,ValueError when using AUC metric with multi label flag,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from pip
- TensorFlow version 2.1.0
- Python version: 3.7
- CUDA/cuDNN version: CUDA10.1 and cnDNN7.6
- GPU model and memory: Tesla K80

**Describe the current behavior**
During the compilation of a model the below error occurs. The error is duo to the use of the `tf.keras.metrics.AUC` class with the `multi_label=True` option used. When `multi_label=False`, the model compiles without errors.

**Describe the expected behavior**
Model should compiles without errors.

**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import SGD

inputs = Input(shape=(10,))
output = Dense(3, activation=""sigmoid"")(inputs)

model = Model(
    inputs=inputs, 
    outputs=output
)

model.compile(
    loss='binary_crossentropy',
    optimizer=SGD(lr=1e-3, momentum=0.9), 
    metrics=[tf.keras.metrics.AUC(multi_label=True)]
)
```

**Other info / logs**
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py in zeros(shape, dtype, name)
   2439         shape = constant_op._tensor_shape_tensor_conversion_function(
-> 2440             tensor_shape.TensorShape(shape))
   2441       except (TypeError, ValueError):

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _tensor_shape_tensor_conversion_function(s, dtype, name, as_ref)
    333     raise ValueError(
--> 334         ""Cannot convert a partially known TensorShape to a Tensor: %s"" % s)
    335   s_list = s.as_list()

ValueError: Cannot convert a partially known TensorShape to a Tensor: (200, None)

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-1-dc556804b7cd> in <module>
     15     loss='binary_crossentropy',
     16     optimizer=SGD(lr=1e-3, momentum=0.9),
---> 17     metrics=[tf.keras.metrics.AUC(multi_label=True)]
     18 )

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)
    437           targets=self._targets,
    438           skip_target_masks=self._prepare_skip_target_masks(),
--> 439           masks=self._prepare_output_masks())
    440 
    441       # Prepare sample weight modes. List with the same length as model outputs.

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _handle_metrics(self, outputs, targets, skip_target_masks, sample_weights, masks, return_weighted_metrics, return_weighted_and_unweighted_metrics)
   2002           metric_results.extend(
   2003               self._handle_per_output_metrics(self._per_output_metrics[i],
-> 2004                                               target, output, output_mask))
   2005         if return_weighted_and_unweighted_metrics or return_weighted_metrics:
   2006           metric_results.extend(

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _handle_per_output_metrics(self, metrics_dict, y_true, y_pred, mask, weights)
   1953       with K.name_scope(metric_name):
   1954         metric_result = training_utils.call_metric_function(
-> 1955             metric_fn, y_true, y_pred, weights=weights, mask=mask)
   1956         metric_results.append(metric_result)
   1957     return metric_results

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py in call_metric_function(metric_fn, y_true, y_pred, weights, mask)
   1153 
   1154   if y_pred is not None:
-> 1155     return metric_fn(y_true, y_pred, sample_weight=weights)
   1156   # `Mean` metric only takes a single value.
   1157   return metric_fn(y_true, sample_weight=weights)

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py in __call__(self, *args, **kwargs)
    194     from tensorflow.python.keras.distribute import distributed_training_utils  # pylint:disable=g-import-not-at-top
    195     return distributed_training_utils.call_replica_local_fn(
--> 196         replica_local_fn, *args, **kwargs)
    197 
    198   @property

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py in call_replica_local_fn(fn, *args, **kwargs)
   1133     with strategy.scope():
   1134       return strategy.extended.call_for_each_replica(fn, args, kwargs)
-> 1135   return fn(*args, **kwargs)
   1136 
   1137 

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py in replica_local_fn(*args, **kwargs)
    177     def replica_local_fn(*args, **kwargs):
    178       """"""Updates the state of the metric in a replica-local context.""""""
--> 179       update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable
    180       with ops.control_dependencies([update_op]):
    181         result_t = self.result()  # pylint: disable=not-callable

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/metrics_utils.py in decorated(metric_obj, *args, **kwargs)
     74 
     75     with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):
---> 76       update_op = update_state_fn(*args, **kwargs)
     77     if update_op is not None:  # update_op will be None in eager execution.
     78       metric_obj.add_update(update_op)

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py in update_state(self, y_true, y_pred, sample_weight)
   1883     deps = []
   1884     if not self._built:
-> 1885       self._build(y_true.shape)
   1886 
   1887     if self.multi_label or (self.label_weights is not None):

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py in _build(self, shape)
   1844         'true_positives',
   1845         shape=variable_shape,
-> 1846         initializer=init_ops.zeros_initializer)
   1847     self.true_negatives = self.add_weight(
   1848         'true_negatives',

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py in add_weight(self, name, shape, aggregation, synchronization, initializer, dtype)
    274         collections=[],
    275         synchronization=synchronization,
--> 276         aggregation=aggregation)
    277 
    278   ### End: For use by subclasses ###

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)
    444         synchronization=synchronization,
    445         aggregation=aggregation,
--> 446         caching_device=caching_device)
    447     backend.track_variable(variable)
    448 

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)
    742         dtype=dtype,
    743         initializer=initializer,
--> 744         **kwargs_for_getter)
    745 
    746     # If we set an initializer and the variable processed it, tracking will not

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py in make_variable(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)
    140       synchronization=synchronization,
    141       aggregation=aggregation,
--> 142       shape=variable_shape if variable_shape else None)
    143 
    144 

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py in __call__(cls, *args, **kwargs)
    256   def __call__(cls, *args, **kwargs):
    257     if cls is VariableV1:
--> 258       return cls._variable_v1_call(*args, **kwargs)
    259     elif cls is Variable:
    260       return cls._variable_v2_call(*args, **kwargs)

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py in _variable_v1_call(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)
    217         synchronization=synchronization,
    218         aggregation=aggregation,
--> 219         shape=shape)
    220 
    221   def _variable_v2_call(cls,

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py in <lambda>(**kwargs)
    195                         shape=None):
    196     """"""Call on Variable class. Useful to force the signature.""""""
--> 197     previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
    198     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access
    199       previous_getter = _make_getter(getter, previous_getter)

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)
   2594         synchronization=synchronization,
   2595         aggregation=aggregation,
-> 2596         shape=shape)
   2597   else:
   2598     return variables.RefVariable(

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py in __call__(cls, *args, **kwargs)
    260       return cls._variable_v2_call(*args, **kwargs)
    261     else:
--> 262       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    263 
    264 

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)
   1409           aggregation=aggregation,
   1410           shape=shape,
-> 1411           distribute_strategy=distribute_strategy)
   1412 
   1413   def _init_from_args(self,

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)
   1540           with ops.name_scope(""Initializer""), device_context_manager(None):
   1541             initial_value = ops.convert_to_tensor(
-> 1542                 initial_value() if init_from_fn else initial_value,
   1543                 name=""initial_value"", dtype=dtype)
   1544           if shape is not None:

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py in <lambda>()
    120           (type(init_ops.Initializer), type(init_ops_v2.Initializer))):
    121         initializer = initializer()
--> 122       init_val = lambda: initializer(shape, dtype=dtype)
    123       variable_dtype = dtype.base_dtype
    124   if use_resource is None:

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py in __call__(self, shape, dtype, partition_info)
    112     if dtype is None:
    113       dtype = self.dtype
--> 114     return array_ops.zeros(shape, dtype)
    115 
    116   def get_config(self):

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py in zeros(shape, dtype, name)
   2441       except (TypeError, ValueError):
   2442         # Happens when shape is a list with tensor elements
-> 2443         shape = ops.convert_to_tensor(shape, dtype=dtypes.int32)
   2444     if not shape._shape_tuple():
   2445       shape = reshape(shape, [-1])  # Ensure it's a vector

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
   1312 
   1313     if ret is None:
-> 1314       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1315 
   1316     if ret is NotImplemented:

~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _tensor_shape_tensor_conversion_function(s, dtype, name, as_ref)
    332   if not s.is_fully_defined():
    333     raise ValueError(
--> 334         ""Cannot convert a partially known TensorShape to a Tensor: %s"" % s)
    335   s_list = s.as_list()
    336   int64_value = 0

ValueError: Cannot convert a partially known TensorShape to a Tensor: (200, None)
```"
35851,TensorFlow Lite with Python,"Hi,

is there a way (documented procedure) for preparing ""Python wheel"" to just install the interpreter to run inferences with TensorFlow Lite.  On the platform I use (arm64) I have option to run an inference with CPU or GPU. How this will be evaluated via Interpreter, is there an option to have more possible IP blocks to run inference on?
I'm using the 1.13.2 version."
35850,My TFLite model works with 32-bit ARM-based architecture but doesn't work with 64-bit AArch64 architecture,"Hi,

1. I've created a TFLite model and ran it on a device with 32-bit ARM-based CPUs and it worked fine.
2. I've done the same thing for 64-bit architecture and there seems to be a problem:
I built the libtensorflowlite.so files using Bazel build (used --cpu=arm64-v8a for 64bit) as your guides explains. I pushed the complied files to my Android platform and tried to run the model.
I don't get any errors, the model is allegedly terminated successfully but it reports 0 [ms] runtimes and no output.

**Do you have any idea what can be the problem?** 
"
35849,Unexpected device usage in optimizer apply_gradients(zip()),"Hi,
I have two GPUs and test a liner model on TF 2.1.0. This is the main part of the code:

```
DEVICE = ""/gpu:1""

with tf.device(DEVICE):
    grads = tf.Variable(0.0)
    opt = tf.optimizers.Adam(1e-6)

@tf.function
def train_one_batch(model,train_data,train_label):
    print(""tracing batch"")
    with tf.device(DEVICE):
        with tf.GradientTape() as tape:
            predicts = model(train_data)
            loss = tf.nn.l2_loss(predicts - train_label)
            grads = tape.gradient(loss, [model.W, model.b])
            opt.apply_gradients(zip(grads, [model.W, model.b])) 
```

I make sure that all other tensors/ops are on GPU:1,so this program is surposed to run on GPU:1 only without GPU:0. But check the nvidia-smi console while running:

```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce RTX 208...  Off  | 00000000:02:00.0 Off |                  N/A |
| 31%   43C    P2    54W / 250W |  10571MiB / 10989MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce RTX 208...  Off  | 00000000:82:00.0 Off |                  N/A |
| 29%   40C    P2    53W / 250W |  10635MiB / 10989MiB |      7%      Default |
+-------------------------------+----------------------+----------------------+
```

According to the memory-usage of GPU:0, this GPU is already in use. When I delete the last line ""opt.apply_gradients(zip(grads, [model.W, model.b])) "" and run it again, nvidia-smi shows:

```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce RTX 208...  Off  | 00000000:02:00.0 Off |                  N/A |
| 31%   43C    P2    54W / 250W |    165MiB / 10989MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce RTX 208...  Off  | 00000000:82:00.0 Off |                  N/A |
| 30%   40C    P2    53W / 250W |  10571MiB / 10989MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
```

It shows that GPU:0 is not in use as expected.

So,this is the problem,why the line ""opt.apply_gradients(zip(grads, [model.W, model.b])) ""
use an unexperted device despite I have alrealy specified one. Are there any other APIs to avoid this problem.

And whats more, that line also causes another tracing behavior(retrace) in the tf.function after first trace.

Thanks"
35848,Feature request: RecallAtPrecision Metric,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: pip install
- **TensorFlow version (use command below)**: 2.1.0
- **Python version**: (3, 6, 8, 'final', 0)
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

Feature request:
tf.keras.metrics.RecallAtPrecision

Thank you all for including PrecisionAtRecall, would it be possible to also add in the inverse, Recall At Precision? I tried adding a custom metric, but its rather difficult to replicate the pattern, since I cannot inherit the `SensitivitySpecificityBase` class defined in the source code.

Thank you!"
35847,The result of NNAPI hardswish quantization implementation is different with tensorflow hardswish,"Recently,  I ran AI benchmark v4 model(Mobilenet-v3-quant.tflite) by label_image. But I got the different result by tflite and NNAPI implementation quantization. The detailed difference is just 1 per quant HardSwish layer. And HardSwish is splited by NNAPI delegate as below: 

> // Lower hardswish according to the following equation:
> // hard_swish[x] = x (ReLU6(x + 3)) / 6 == x * (Relu_N1_to_1(x/3) * 3 + 3) / 6
> // = 0.5x * Relu_N1_to_1(x/3) + 0.5x

in nnpai_delegate.cc at the line around 510.
BTW, NNAPI uses int32 quantization but tflite uses int16."
35846,tf.image.random_brightness do not reproducible with multiprocessing on tf.data.Dataset.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.1.0-rc1
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: V10.1.243
- GPU model and memory:


**Describe the current behavior**
I use tf.image.random_brightness api for augmentation. With dataset api I put those augmentation functions in parser method and use Dataset.map api for applying the method to the dataset and _Transformed dataset_ is not reproducible when I put non negative integer > 0 in _num_parallel_calls_ argument of Dataset.map api. It is also the same if I use tf.data.experimental.AUTOTUNE for num_parallel_calls argument.

**Code to reproduce the issue**
[here is the colab link to reproduce the issue](https://colab.research.google.com/drive/1uNpn1Rf1_WvG2lnAS41g36IDWOB2IW7-)

**Other info / logs**
It looks like tf.random.uniform method is the reason of happening this. The implementation of tf.image.random_brightness use tensorflow random uniform method and the method is not reproducible with multiprocessing. I have tested tf.image.random_contrast and tf.image.random_saturation. They are not reproducible also.
"
35844,Aggregate lists of trainable variables from different Keras models in Tensorflow v2,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.1.0-rc1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

I have two neural networks (e.g., f and g) implemented using `tensorflow.keras.Model`. I simply want to compute the gradient of f(g(x)). However, I was not able to aggregate the lists of trainable variables of both models. 

According to Tensorflow V2 documentations: `If you need to aggregate lists of variables (like tf.Graph.get_collection(tf.GraphKeys.VARIABLES)), use the .variables and .trainable_variables attributes of the Layer and Model objects.`

I tried to do so but I am getting this weird error:

    /content/drive/My Drive/Colab/IIC/IIC.py in train_step(self, x, gx, head)
        197 
        198                 gradients = tape.gradient(loss, trainable_variables)
    --> 199                 self.optimizer.apply_gradients(zip(gradients, trainable_variables))
        200                 self.train_loss(loss)
        201 
    
    /tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name)
        432         _ = self.iterations
        433         self._create_hypers()
    --> 434         self._create_slots(var_list)
        435 
        436       if not grads_and_vars:
    
    /tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/optimizer_v2/adam.py in _create_slots(self, var_list)
        147     # Separate for-loops to respect the ordering of slot variables from v1.
        148     for var in var_list:
    --> 149       self.add_slot(var, 'm')
        150     for var in var_list:
        151       self.add_slot(var, 'v')
    
    /tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in add_slot(self, var, slot_name, initializer)
        572     if slot_name not in self._slot_names:
        573       self._slot_names.append(slot_name)
    --> 574     var_key = _var_key(var)
        575     slot_dict = self._slots.setdefault(var_key, {})
        576     weight = slot_dict.get(slot_name, None)
    
    /tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in _var_key(var)
       1063   if hasattr(var, ""_distributed_container""):
       1064     var = var._distributed_container()
    -> 1065   if var._in_graph_mode:
       1066     return var._shared_name
       1067   return var._unique_id
    
    AttributeError: 'list' object has no attribute '_in_graph_mode'

The `trainable_variables` is just a list of trainable variables from both models f and g.


    trainable_variables = [f.trainable_variables, g.trainable_variables]



"
35843,Keras does not allow using custom iterator from a Python generator,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**
I am trying to run model.fit() on a tf.keras model using a custom data generator on my database. While this generator derives from tf.data.Dataset, it is not a subclass because there some preprocessing involved. However, it supports an iterator that can generate batches of input-output pairs and is compatible with the Keras guidelines. However, tf.keras does not let me use this iterator and throws the following ValueError:
![image](https://user-images.githubusercontent.com/14254187/72313857-9774a680-3641-11ea-9ab2-af8deefd7307.png)

`ValueError: For performance reasons Keras `fit`, `evaluate` and`predict` accept tf.data `Datasets` as input but not iterators that have been manually generated from Datasets by users. Please directly pass in the original `Dataset` object instead of passing in iter(dataset).`

Is there a flag to overrule this behavior, which seems like a performance enhancement issue and not a fundamental inability of the package?"
35842,tensorflow/python/eager/BUILD:14:1: in cc_library rule //tensorflow/python/eager:pywrap_tfe_lib: cycle in dependency graph,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS: Windows 10 Pro x64 18362
- TensorFlow installed from (source or binary): source (master, 8782b7679c12780fd914827abf4e79ceb51d6b41)
- TensorFlow version: 2.1.0
- Python version: 3.5.3/3.6.8/3.7.6/3.8.1
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 1.2.1
- GCC/Compiler version (if compiling from source): VS2017 14.16.27023
- CUDA/cuDNN/tensorrt version: 10.2.89, 7.6.5.32, 7.0.0.11
- GPU model and memory: RTX 2080Ti, 11GB



**Describe the problem**

```
build --action_env PYTHON_BIN_PATH=""C:/tensorflow/venv36/Scripts/python.exe""
build --action_env PYTHON_LIB_PATH=""C:/tensorflow/venv36/lib/site-packages""
build --python_path=""C:/tensorflow/venv36/Scripts/python.exe""
build:xla --define with_xla_support=true
build --action_env CUDA_TOOLKIT_PATH=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1,7.5""
build --config=cuda
# build --config=tensorrt
build:opt --copt=/arch:AVX
build:opt --define with_default_optimizations=true
build --define=override_eigen_strong_inline=true
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-v1only
build --action_env TF_CONFIGURE_IOS=""0""

```

If I enable tensorrt and compile using the method described in [this article](https://tensorflow.google.cn/install/source_windows#gpu_support), I get the following error:


```
ERROR: C:/tensorflow/tensorflow/python/eager/BUILD:14:1: in cc_library rule //tensorflow/python/eager:pywrap_tfe_lib: cycle in dependency graph:
    //tensorflow/tools/pip_package:build_pip_package
    //tensorflow/python/keras/distribute:distribute_strategy_test_lib
    //tensorflow/python/distribute:tpu_strategy
    //tensorflow/python/distribute:values
    //tensorflow/python:framework_ops
    //tensorflow/python:tensor_conversion_registry
    //tensorflow/python/eager:context
    //tensorflow/python/eager:executor
    //tensorflow/python:pywrap_tfe
    //tensorflow/python:pywrap_tensorflow
    //tensorflow/python:pywrap_tensorflow_internal
    //tensorflow/python:pywrap_tensorflow_internal.py
    //tensorflow/python:pywrap_tensorflow_internal_py_wrap
.-> //tensorflow/python/eager:pywrap_tfe_lib
|   //tensorflow/python:ndarray_tensor_bridge
|   //tensorflow/python:bfloat16_lib
|   //tensorflow/python:safe_ptr
|   //tensorflow/c/eager:c_api
|   //tensorflow/core/distributed_runtime/rpc:grpc_server_lib
|   //tensorflow/core/distributed_runtime/rpc:grpc_master_service
|   //tensorflow/core/distributed_runtime:master
|   //tensorflow/core/distributed_runtime:master_session
|   //tensorflow/core/distributed_runtime:scheduler
|   //tensorflow/core:tensorflow_opensource
|   //tensorflow/core:all_kernels
|   //tensorflow/core:all_kernels_impl
|   //tensorflow/compiler/tf2tensorrt:trt_op_kernels
|   //tensorflow/python:pywrap_tensorflow_import_lib
|   //tensorflow/python:pywrap_tensorflow_import_lib_file
|   //tensorflow/python:get_pywrap_tensorflow_import_lib_file
|   //tensorflow/python:_pywrap_tensorflow_internal.so
`-- //tensorflow/python/eager:pywrap_tfe_lib
This cycle occurred because of a configuration option
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted
```

"
35841,ValueError: Input array not provided for operation 'reshape'.,"**System information**
- MacOS 10.12 (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from source: pip
- TensorFlow version (or github SHA if from source): 1.15.0


**Provide the text output from tflite_convert**

```
ValueError                                Traceback (most recent call last)
<ipython-input-10-d6e20568d7d5> in <module>()
      5 
      6 # Load TFLite model and allocate tensors.
----> 7 interpreter = tf.lite.Interpreter(model_path=""/Users/zhuxinyu/Downloads/logits_model(2).tflite"")
      8 interpreter.allocate_tensors()
      9 

~/anaconda3/lib/python3.6/site-packages/tensorflow_core/lite/python/interpreter.py in __init__(self, model_path, model_content, experimental_delegates)
    204       self._interpreter = (
    205           _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(
--> 206               model_path))
    207       if not self._interpreter:
    208         raise ValueError('Failed to open {}'.format(model_path))

ValueError: Input array not provided for operation 'reshape'.
```

```
converter = tf.lite.TFLiteConverter.from_session(infer_sess, [infer_graph.get_tensor_by_name('Infer/DKT/input_buf:0'), infer_graph.get_tensor_by_name('Infer/DKT/input_len:0')], [infer_graph.get_tensor_by_name('Infer/DKT/before_output:0')])
converter.post_training_quantize = True
```
I get the quantized model but when I load the mode using `interpreter = tf.lite.Interpreter(model_path=""/Users/zhuxinyu/Downloads/logits_model(2).tflite"")`，it says it does not support reshape operation，but it seems to be inevitable in my network(it happens in a dense layer i think)，how can I make the tf.layer.dense not using reshape inside?
"
35840,Eager code with tf.function decoration is >2x slower than without it.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- TensorFlow version: 
    TF 2.0.0
- Python version:
   Python 3.7.6
- CUDA/cuDNN version:
   10.0.130
- GPU model and memory:
   GTX2080

**Describe the current behavior**
I walk through the ```@tf.function``` usage examples from [Tensorflow Tutorial](https://www.tensorflow.org/guide/function). There is a example shows that the eager code version of  ```conv_layer``` with ```@tf.function``` decorator could run faster than without it. However, when I imitate it and write eager code version  of ```embedding_layer```, the decorator one is >2x slower than the not decorator one. See the code below

**Code to reproduce the issue**
```
# check time it for embedding layer
from tensorflow.keras.initializers import RandomUniform
import numpy as np
import tensorflow as tf

input_dim = 1000
output_dim = 300
emb_layer = tf.keras.layers.Embedding(input_dim, output_dim, RandomUniform(-1,1))

@tf.function
def emb_fn(indices):
    return emb_layer(indices)


indices = np.random.randint(0, input_dim-1, size=64)
indices = tf.constant(indices)
# warm up
emb_layer(indices); emb_fn(indices)

print(""Eager embed: "", timeit.timeit(lambda: emb_layer(indices), number=10000))
print(""Function embed: "", timeit.timeit(lambda: emb_fn(indices), number=10000))
```
**The result**
Eager embed:  1.895560858771205
Function embed:  4.15901411511004


"
35837,TF2.0 Error checkpointing custom map,"I am subclassing a keras model with custom layers. Each layer wraps a dictionary of parameters that is used when generating they layers. It seems these param dictionaries are not set before the training checkpoint is made in Tensorflow, they are set after, which causes an error. I am not sure how to fix this, as the `ValueError` being raised also gives outdated information (`tf.contrib` no longer exists). 

> ValueError: Unable to save the object {'units': 32, 'activation':
> 'tanh', 'recurrent_initializer': 'glorot_uniform', 'dropout': 0,
> 'return_sequences': True} (a dictionary wrapper constructed
> automatically on attribute assignment). The wrapped dictionary was
> modified outside the wrapper (its final value was {'units': 32,
> 'activation': 'tanh', 'recurrent_initializer': 'glorot_uniform',
> 'dropout': 0, 'return_sequences': True}, its value when a checkpoint
> dependency was added was None), which breaks restoration on object
> creation.
> 
> If you don't need this dictionary checkpointed, wrap it in a
> tf.contrib.checkpoint.NoDependency object; it will be automatically
> un-wrapped and subsequently ignored.


Here's an example of the Layer that is throwing this issue:


    class RecurrentConfig(BaseLayer):
        '''Basic configurable recurrent layer'''
        def __init__(self, params: Dict[Any, Any], mode: ModeKeys, layer_name: str = '', **kwargs):
            self.layer_name = layer_name
            self.cell_name = params.pop('cell', 'GRU')
            self.num_layers = params.pop('num_layers', 1)
            kwargs['name'] = layer_name
            super().__init__(params, mode, **kwargs)
            if layer_name == '':
                self.layer_name = self.cell_name
            self.layers: List[layers.Layer] = stack_layers(self.params,
                                                           self.num_layers,
                                                           self.cell_name)
    
        def call(self, inputs: np.ndarray) -> layers.Layer:
            '''This function is a sequential/functional call to this layers logic
            Args:
                inputs: Array to be processed within this layer
            Returns:
                inputs processed through this layer'''
            processed = inputs
            for layer in self.layers:
                processed = layer(processed)
            return processed
    
        @staticmethod
        def default_params() -> Dict[Any, Any]:
            return{
                'units': 32,
                'recurrent_initializer': 'glorot_uniform',
                'dropout': 0,
                'recurrent_dropout': 0,
                'activation': 'tanh',
                'return_sequences': True
            }

BaseLayer.py

    '''Basic ABC for a keras style layer'''
    
    from typing import Dict, Any
    
    from tensorflow.keras import layers
    from mosaix_py.mosaix_learn.configurable import Configurable
    
    class BaseLayer(Configurable, layers.Layer):
        '''Base configurable Keras layer'''
        def get_config(self) -> Dict[str, Any]:
            '''Return configuration dictionary as part of keras serialization'''
            config = super().get_config()
            config.update(self.params)
            return config
    
        @staticmethod
        def default_params() -> Dict[Any, Any]:
            raise NotImplementedError('Layer does not implement default params')

"
35835,model.predict leaks memory,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. We are using our own custom layer to build a keras model.


- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac & Ubuntu 16.04. Python 3.7 
- TensorFlow installed from (source or binary):
Using the stable version of tensorflow 2.0.0, but tf 2.1 appears to exhibit the same issue
- TensorFlow version (use command below):
v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version:
3.7.6


**Describe the current behavior**
When calling model.predict in a loop memory grows to many gigs. Below is a very simple example of what we're doing that experiences severe memory growth. Granted there are many layers involved, some custom. Each call to predict is growing in memory size.

```
model = tf.keras.Model(sent_input, preds)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])
model.load_weights(...)
while True:
    inputs = prepare_inputs(x)
    model.predict(inputs)
```

I have seen some other reports that suggest predict_on_batch might be used, but this also exhibits the same issue.


**Describe the expected behavior**
I would expect to be able to call predict as many times as necessary and for memory consumption to remain mostly constant.

**Other info / logs**
In my debugging I was able to find this using pympler. I was able to attach an image of what I believe is the culprit  object being leaked, but I could be mistaken on this. It looks like EagerTensors are being leaked. It appears the TimeDistributedLayer is keeping references to them.

```
                                                 types |   # objects |   total size
====================================================== | =========== | ============
                                                  dict |          20 |      2.07 KB
    tensorflow.python.framework.tensor_shape.Dimension |          13 |    832     B
           tensorflow.python.framework.ops.EagerTensor |           3 |    528     B
                                                  list |           5 |    520     B
  tensorflow.python.framework.tensor_shape.TensorShape |           5 |    320     B
```

![objgraph-qwasabxu](https://user-images.githubusercontent.com/5382738/72302369-73a86500-362f-11ea-9044-93cd2b1605e2.png)

"
35833,Keras Estimator fails on regression task while underlying model works,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Windows 10
- TensorFlow installed from: binary
- TensorFlow version: Same issue using tf2.0.0-beta1-cpu
 and tf1.14.0-gpu
- Python version: 3.6.9
- CUDA/cuDNN version: CUDA 10.1.168/cuDNN 7.6.2
- GPU model and memory: NVIDIA GeForce RTX 2060, 6GB dedicated memory

**Describe the current behavior**
A convolutional reggression (last layer has linear activation and one neuron) network built with tf.keras is shown to fit the MNIST dataset (I know that MNIST is a classification task; this is an example) when converted to a dataset.

When the same model is packaged into an estimator using  `tf.keras.estimator.model_to_estimator` no error messages occur, however the model no longer fits. The loss does not decrease.

I had made a Stackoverflow question about this (https://stackoverflow.com/q/59631744/9988487) with no traction whatsoever. After some more trying to get it to work, I believe it is a bug.

**Describe the expected behavior**
The keras estimator should have the same behaviour as the underlying model. Change the USE_ESTIMATOR variable to see that the underlying model works.
**Code to reproduce the issue**
```
# python 3.6. Tested with tensorflow-gpu-1.14 and tensorflow-cpu-2.0
import tensorflow as tf
import numpy as np


def get_model(IM_WIDTH=28, num_color_channels=1):
    """"""Create a very simple convolutional neural network using a tf.keras Functional Model.""""""
    input = tf.keras.Input(shape=(IM_WIDTH, IM_WIDTH, num_color_channels))
    x = tf.keras.layers.Conv2D(32, 3, activation='relu')(input)
    x = tf.keras.layers.MaxPooling2D(3)(x)
    x = tf.keras.layers.Conv2D(64, 3, activation='relu')(x)
    x = tf.keras.layers.MaxPooling2D(3)(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(64, activation='relu')(x)
    output = tf.keras.layers.Dense(1, activation='linear')(x)
    model = tf.keras.Model(inputs=[input], outputs=[output])
    model.compile(optimizer='adam', loss=""mae"",
                  metrics=['mae'])
    model.summary()
    return model


def input_fun(train=True):
    """"""Load MNIST and return the training or test set as a tf.data.Dataset; Valid input function for tf.estimator""""""
    (train_images, train_labels), (eval_images, eval_labels) = tf.keras.datasets.mnist.load_data()
    train_images = train_images.reshape((60_000, 28, 28, 1)).astype(np.float32) / 255.
    eval_images = eval_images.reshape((10_000, 28, 28, 1)).astype(np.float32) / 255.
    # train_labels = train_labels.astype(np.float32)  # these two lines don't affect behaviour.
    # eval_labels = eval_labels.astype(np.float32)
    # For a neural network with one neuron in the final layer, it doesn't seem to matter if target data is float or int.

    if train:
        dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))
        dataset = dataset.shuffle(buffer_size=100).repeat(None).batch(32).prefetch(1)
    else:
        dataset = tf.data.Dataset.from_tensor_slices((eval_images, eval_labels))
        dataset = dataset.batch(32).prefetch(1)  # note: prefetching does not affect behaviour

    return dataset


model = get_model()
train_input_fn = lambda: input_fun(train=True)
eval_input_fn = lambda: input_fun(train=False)

NUM_EPOCHS, STEPS_PER_EPOCH = 4, 1875  # 1875 = number_of_train_images(=60.000)  /  batch_size(=32)
USE_ESTIMATOR = False  # change this to compare model/estimator. Estimator performs much worse for no apparent reason
if USE_ESTIMATOR:
    estimator = tf.keras.estimator.model_to_estimator(
        keras_model=model, model_dir=""model_directory"",
        config=tf.estimator.RunConfig(save_checkpoints_steps=200, save_summary_steps=200))

    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=STEPS_PER_EPOCH * NUM_EPOCHS)
    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, throttle_secs=0)

    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
    print(""Training complete. Evaluating Estimator:"")
    print(estimator.evaluate(eval_input_fn))
    # final train loss with estimator: ~2.5 (mean abs. error).
else:
    dataset = train_input_fn()
    model.fit(dataset, steps_per_epoch=STEPS_PER_EPOCH, epochs=NUM_EPOCHS)
    print(""Training complete. Evaluating Keras model:"")
    print(model.evaluate(eval_input_fn()))
    # final train loss with Keras model: ~0.4 (mean abs. error).
```
"
35830,Why error occurs but programme is not affacted.,"I use python to train a model and export the model. Then I use c++ to import the exported model and use it to predict data. However, during the prediction process, error message is printed during the screen, but the programme run normally with no exception. Below is the error message.

`2020-01-14 03:29:49.794822: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
Error running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
         [[{{node ParseExample/ParseExample}}]]
2020-01-14 03:29:51.689021: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
2020-01-14 03:29:51.689069: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
Error running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
         [[{{node ParseExample/ParseExample}}]]
Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
         [[{{node ParseExample/ParseExample}}]]
2020-01-14 03:29:51.702364: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
Error running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
         [[{{node ParseExample/ParseExample}}]]
2020-01-14 03:29:53.728503: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
Error running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
         [[{{node ParseExample/ParseExample}}]]
2020-01-14 03:29:55.683826: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
Error running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
         [[{{node ParseExample/ParseExample}}]]
2020-01-14 03:29:55.686735: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
2020-01-14 03:29:55.686879: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
2020-01-14 03:29:55.686918: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
Error running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
         [[{{node ParseExample/ParseExample}}]]
2020-01-14 03:29:55.687032: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
2020-01-14 03:29:55.687120: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
Error running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
         [[{{node ParseExample/ParseExample}}]]
Error running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
         [[{{node ParseExample/ParseExample}}]]
Error running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
         [[{{node ParseExample/ParseExample}}]]
Error running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.
         [[{{node ParseExample/ParseExample}}]]`

Below is the code that use the model in c++.
`float NodeEVEngine::getmlev(int curturn,vector<float> &inputfeature,bool allin,bool turnover){
//    MyTimer::GetInstance()->start(""prepare"");
    Example example;
    Features features;
    int featurelen = inputfeature.size();
    for (int i = 0; i < featurelen; i++) {
        FloatList floatList;
        floatList.add_value(inputfeature[i]);
        Feature feature;
        *feature.mutable_float_list() = floatList;
        features.mutable_feature()->operator[](to_string(i)) = feature;
    }
    *example.mutable_features()=features;
    std::string test_str;
    example.SerializeToString(&test_str);
    tensorflow::Input input({test_str});
    vector<string> outputname({""dnn/logits/BiasAdd:0""});
    std::vector<Tensor> outputs;
    Status run_status;
//    MyTimer::GetInstance()->stopandprintandclear(""prepare"");
//    MyTimer::GetInstance()->start(""mlev"");
    if (allin)run_status = m_allinbundle[curturn-PREFLOPNO]->session->Run({{""input_example_tensor"",input.tensor()}}, outputname, {}, &outputs);
    else if (turnover)run_status = m_turnoverbundle[curturn-PREFLOPNO]->session->Run({{""input_example_tensor"",input.tensor()}}, outputname, {}, &outputs);
    else run_status = m_bundle[curturn-PREFLOPNO]->session->Run({{""input_example_tensor"",input.tensor()}}, outputname, {}, &outputs);
    if (!run_status.ok()) {
        cout << ""Error running session: "" << run_status << std::endl;
        return -1;
    }
//    MyTimer::GetInstance()->stopandprintandclear(""mlev"");
//    PrintVector(inputfeature,""inputfeature:"");
    return outputs[0].tensor<float,2>()(0,0);
}`"
35828,tf.contrib.cudnn_rnn.CudnnGRU runtime error: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Google Colab
- TensorFlow version (use command below):
1.15.0
- Python version:
3.6.9
- CUDA/cuDNN version:
Cuda compilation tools, release 10.0, V10.0.130
- GPU model and memory:

**Describe the current behavior**
I am trying to convert my CPU GRU code to a CudnnGRU implementation. When I run the code [here](https://colab.research.google.com/drive/1c64kUiCs8K17I5YygWaf14DKWhObdKA9), my script runs for a seemingly random of number of training epochs (normally between 0 and 20) before the session crashes, often with a CUDA_ERROR_ILLEGAL_ADDRESS error. I see similar behaviour when I run the script on my institution's hardware (TF version 1.14, CUDA Version: 10.1).

**Code to reproduce the issue**
See Google colab sheet [here](https://colab.research.google.com/drive/1c64kUiCs8K17I5YygWaf14DKWhObdKA9). I select the GPU as the hardware accelerator in the notebook settings.

**Other info / logs**
```
Learning Rate: 0.001 
Total number of portions: 10 
batch_length: 500 
n_mini_batches 25 
mini_batch_length 20 
number of MEG channels/PCs: 5 

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/cudnn_rnn/python/layers/cudnn_rnn.py:342: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/cudnn_rnn/python/layers/cudnn_rnn.py:345: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
Alpha coefficients will be soft-plus transformed
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/slot_creator.py:193: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Device mapping:
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5

Beginning training...
0
1
2
3
4
```

Please see the colab-jupyter.log file attached.

Apologies for my ignorance and thank you in advance for any kind help.
[colab-jupyter.log](https://github.com/tensorflow/tensorflow/files/4055241/colab-jupyter.log)
"
35827,NullPointerException when trying to access Classifier.Recognition / result.getLocation(),"**Setup:** 
I've used Google's Cloud AutoML Vision Object Detection platform to create a custom model for object recognition using this tutorial:

https://cloud.google.com/vision/automl/object-detection/docs/edge-quickstart

Then added the model to the following app template:

https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android

Then I overcame the following two errors: 

1) `Cannot convert between a TensorFlowLite buffer with 1080000 bytes and a ByteBuffer with 270000 bytes.` I modified TF_OD_API_INPUT_SIZE accordingly. 

2) ` tflite ml google [1, 20, 4] and a Java object with shape [1, 10, 4]. ` I modified NUM_DETECTIONS according to my custom model.

Finally got the model working. 

Now I want to send the box locations using Bluetooth to a HC-05. I added two public classes: BluetoothArduinoBridge, and LocationSender. Both of them attached as txt files.

**Problem**
App crashes when objects are recognized. 

**Code to reproduce the issue**
I modified the code at DetectorActivity.java, lines 203-218 to this: 

```
            for (final Classifier.Recognition result : results) {
              final RectF location = result.getLocation();
              if (location != null && result.getConfidence() >= minimumConfidence) {
                canvas.drawRect(location, paint);

                cropToFrameTransform.mapRect(location);

                result.setLocation(location);
                mappedRecognitions.add(result);

                //----------------------------------------
                mLocationSender.send_location((location.bottom),(location.left),(location.right)
                ,(location.top));
                //----------------------------------------
              }
            }

```
**Other info / logs**
```
E/AndroidRuntime: FATAL EXCEPTION: inference
    Process: org.tensorflow.lite.examples.detection, PID: 16690
    java.lang.NullPointerException: Attempt to invoke virtual method 'void org.tensorflow.lite.examples.detection.LocationSender.send_location(float, float, float, float)' on a null object reference
        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:213)
```

**Question**
How can I send the box locations using mLocationSender.send_location???

**Note**
I'm quite new to Android so this might probably be a dumb question. Please be gentle. 
[LocationSender.txt](https://github.com/tensorflow/tensorflow/files/4055214/LocationSender.txt)
[BluetoothArduinoBridge.txt](https://github.com/tensorflow/tensorflow/files/4055215/BluetoothArduinoBridge.txt)
"
35826,Bug in Transfer learning + Distributed Training,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 2.0
- **Python version**: 3.7
- **CUDA/cuDNN version**: cuda 10.0, cudnn 7
- **GPU model and memory**: Quadro P6000, 24GB

### Describe the problem
Under the distributed environment, if the model is updated, the trainable_weights of the model is not updated in the distributed training loop. Please see following code for reproducing the bug.

I first created 2 Conv2D layers. I created the first model using only 1 Conv2D layer, and it works fine. Then, I update the mode to create a 2-Conv2D model, then there's the bug. Outside the training loop, there are trainable_weights (2 kernel + 2 bias), but inside the training loop, there's only 2 trainable_weights

### Source code / logs
```
import numpy as np
import tensorflow as tf
from tensorflow import keras

tf.config.set_soft_device_placement(True)
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
  tf.config.experimental.set_memory_growth(gpu, True)

# Begin
x_in = np.random.randn(2, 64, 64, 3).astype(np.float32)
gt = np.random.randn(2, 64, 64, 3).astype(np.float32)

layer1 = keras.layers.Conv2D(
        input_shape=(None, None, None, 3), filters=3,
        kernel_size=3, strides=1, padding='same',
        name='conv1')
layer2 = keras.layers.Conv2D(
        input_shape=(None, None, None, 3), filters=3,
        kernel_size=3, strides=1, padding='same',
        name='conv2')

strategy = tf.distribute.MirroredStrategy()
@tf.function
def train():
  def train_step():
    with tf.GradientTape() as tape:
      loss = tf.reduce_mean((model(x_in) - gt) ** 2)
    grads = tape.gradient(loss, model.trainable_weights)
    tf.print(""Length of trainable_weights: "", len(model.trainable_weights), ""Length of grads: "", len(grads))
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
  strategy.experimental_run_v2(train_step)

print('------ First model ------')
with strategy.scope():
  x = keras.Input((64, 64, 3))
  model = keras.Model(inputs=x, outputs=layer1(x))
  optimizer = keras.optimizers.Adam(0.1, amsgrad=True)
print(""Length of trainable_weights: "", len(model.trainable_weights))
print(model.trainable_weights[-1].values[0].name, model.trainable_weights[-1].values[0].numpy())
# Print: 
# Length of trainable_weights:  2
# conv1/bias:0 [0. 0. 0.]

for i in range(2):
  train()
# Print:
# Length of trainable_weights:  2 Length of grads:  2
# Length of trainable_weights:  2 Length of grads:  2

print(""Length of trainable_weights: "", len(model.trainable_weights))
print(model.trainable_weights[-1].values[0].name, model.trainable_weights[-1].values[0].numpy())
# Print:
# Length of trainable_weights:  2
# conv1/bias:0 [0.03003622 0.03190297 0.02856238]


print('------ Change model ------')
with strategy.scope():
  x = keras.Input((64, 64, 3))
  model = keras.Model(inputs=x, outputs=layer2(layer1(x)))
print(""Length of trainable_weights: "", len(model.trainable_weights))
print(model.trainable_weights[-1].values[0].name, model.trainable_weights[-1].values[0].numpy())
# Print: 
# Length of trainable_weights:  4
# conv2/bias:0 [0. 0. 0.]

for i in range(2):
  train()
# Print:
# Length of trainable_weights:  2 Length of grads:  2
# Length of trainable_weights:  2 Length of grads:  2

print(""Length of trainable_weights: "", len(model.trainable_weights))
print(model.trainable_weights[-1].values[0].name, model.trainable_weights[-1].values[0].numpy())
# Print: 
# Length of trainable_weights:  4
# conv2/bias:0 [0. 0. 0.]



```
"
35825,Extend `tf.keras.Model.evaluate` with `class_weight`,"**System information**
- TensorFlow version (you are using): 
TF 2.0.0
- Are you willing to contribute it (Yes/No):
Yes

**Describe the feature and the current behaviour/state.**
Currently `tf.keras.models.Model.fit` method allows the user to pass either 'sample_weight' and 'class_weight' parameters. These are used to compute at [some point](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/engine/training.py#L2527 ) a standardised 'sample_weights' and used later on while calculating the loss.

This feature request is about extending the 'tf.keras.Model.evaluate' API so that is permits using `class_weight` directly. The `evaluate` function already permits for `sample_weight`.


**Will this change the current api? How?**
current API
```
evaluate(
    x=None,
    y=None,
    batch_size=None,
    verbose=1,
    sample_weight=None,
    steps=None,
    callbacks=None,
    max_queue_size=10,
    workers=1,
    use_multiprocessing=False
)
```
new API
```
evaluate(
    x=None,
    y=None,
    batch_size=None,
    verbose=1,
    sample_weight=None,
>>>    class_weight=None,
    steps=None,
    callbacks=None,
    max_queue_size=10,
    workers=1,
    use_multiprocessing=False
)
```
**Who will benefit with this feature?**

Those users of the API who would like to perform the evaluation of a model that was trained with bespoke class weights.

**Any Other info.**
"
35824,TimeDistributed Layer Does Not Support Multiple Inputs,"Python Version: 3.76
TensorFlow Version: 2.1
OS: Windows 10

Issue:
It does not seem the TimeDistributed layer supports multiple inputs. See example code:

```
import tensorflow as tf
from tensorflow.keras.layers import Dense, Concatenate, RepeatVector, Activation, Dot, Bidirectional, Embedding, Input, SpatialDropout1D, LSTM, Dropout, Lambda, Conv1D, Attention, AdditiveAttention, GlobalAveragePooling1D, TimeDistributed, AveragePooling1D
from tensorflow.keras.models import Model
import numpy as np

def example_2():
    # Encode each timestep
    input_1 = Input(shape=(None,), dtype='int64', name=""Input1"")
    input_2 = Input(shape=(None,), dtype='int64', name=""Input2"")

    output = Concatenate([input_1, input_2])
    output = TimeDistributed(output)([input_1, input_2])

    model = Model([input_1, input_2], output)
    model.compile(loss='categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['accuracy'])

    return model


input_1 = np.array([[1, 2, 3, 4, 5, 6, 7]])
input_2 = np.array([[1, 1, 1, 1, 1, 1, 1]])
y = np.array([1, 0, 1, 1, 0, 0, 1])

example_2().fit(input=[input_1, input_2], output=y)
```
I get the following issue:

```
Traceback (most recent call last):
  File ""C:/Development/Projects/TensorFlow_2.0_Hierarchical_Attention/TimeDistributed_Multiple_Inputs.py"", line 26, in <module>
    example_2().fit(input=[input_1, input_2], output=y)
  File ""C:/Development/Projects/TensorFlow_2.0_Hierarchical_Attention/TimeDistributed_Multiple_Inputs.py"", line 12, in example_2
    output = TimeDistributed(output)([input_1, input_2])
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 748, in __call__
    self._maybe_build(inputs)
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 2116, in _maybe_build
    self.build(input_shapes)
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\keras\layers\wrappers.py"", line 197, in build
    input_shape = tensor_shape.TensorShape(input_shape).as_list()
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py"", line 771, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py"", line 771, in <listcomp>
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py"", line 716, in as_dimension
    return Dimension(value)
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py"", line 200, in __init__
    None)
  File ""<string>"", line 3, in raise_from
TypeError: Dimension value must be integer or None or have an __index__ method, got TensorShape([None, None])
```

<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35823,"keras.models.Model.fit_generator returns  ""RuntimeError: You must compile your model before using it."" even specifying all the input shapes","**System information**
- Python 3.7
- Keras 2.2.4

** Issue **

When using `fit_generator` in `keras.models.Model` returns `RuntimeError: You must compile your model before using it.` 

Following the reply [here](https://stackoverflow.com/questions/52721018/keras-fit-generator-raise-you-must-compile-your-model-before-using-it-error), I specified `input_shape` (and also `batch_shape`) both in `Input` layer and first `Conv1D` layer. But this didn't solve the error.

I don't know if it is a related issue, but manually generating one batch of data and using `model.fit` raise `AttributeError: 'Model' object has no attribute '_output_tensor_cache'` which looks even more suspicious

** Minimal working example **
```
from keras.utils import Sequence

from keras.models import Model, Sequential
from keras.layers import Input, Conv1D, Dense, Flatten

import numpy.random

def create_model():

    input_layer = Input(shape=(60,1), batch_shape=(50,60,1))
    out = Conv1D(64, 3, activation='relu', input_shape=(60, 1))(input_layer)
    out = Flatten()(out)
    out = Dense(1)(out)

    #create model
    model = Model()
    model.compile(inputs    = input_layer,
                  outputs   = out,
                  loss      = 'mse',
                  optimizer = 'adam',
                  )

    return model

class examples_generator(Sequence):

    def __getitem__(self, _):
        batch  = numpy.random.rand(50, 60, 1)
        target = numpy.random.rand(50, 1)
        return (batch, target)

    def __len__(self):
        return 2

if __name__ == '__main__':
    model = create_model()
    training_batch_generator = examples_generator()

    # This will generate RuntimeError: You must compile your model before using it.
    model.fit_generator(training_batch_generator,
                        steps_per_epoch=1,
                        epochs=1,
                        verbose=2)

    one_batch = training_batch_generator[None]
     # This will generate AttributeError: 'Model' object has no attribute '_output_tensor_cache'
    model.fit(one_batch[0], one_batch[1])
```

Using an identical model created using `keras.models.Sequential` works perfectly instead. Defining:

```
def create_sequential_model():

    #create model
    model = Sequential()
    model.add(Conv1D(64, 3, activation='relu', input_shape=(60, 1)))
    model.add(Flatten())
    model.add(Dense(1))
    model.compile(loss      = 'mse',
                  optimizer = 'adam',
                  )

    return model

model = create_sequential_model()

# Works
model.fit_generator(training_batch_generator,
                                steps_per_epoch=100,
                                epochs=3,
                                verbose=2)

# Works
one_batch = training_batch_generator[None]
model.fit(one_batch[0], one_batch[1])
```
and replacing `create_model` with `create_sequential_model` in the example above.


"
35822, tf.py_function eager execution causes memory leak,"## Description:
The eager execution is a python function that loads a wave file

```python
    def safe_load(self, path, offset, duration, sample_rate, dtype):
        get_logger().info(
            f'Loading audio {path} from {offset} to {offset + duration}')
        try:
            (data, _) = self.load(
                path.numpy(),
                offset.numpy(),
                duration.numpy(),
                sample_rate.numpy(),
                dtype=dtype.numpy())
            return (data, False)
        except Exception as e:
            get_logger().warning(e)
        return (np.float32(-1.0), True)
```

and

```python
results = tf.py_function(
                    func=self.safe_load,
                    inp=[audio_descriptor, offset, duration, sample_rate, dtype],
                    Tout=(tf.float32, tf.bool)),
                waveform, error = results[0]
````

The memory leak happens at every call to `estimator.predict`:

```python
with session.as_default():
        with session.graph.as_default():
            prediction = estimator.predict(
                lambda: get_dataset(
                    audio_adapter,
                    filenames_and_crops,
                    sample_rate,
                    n_channels, session),
                yield_single_examples=False)
```

## System information 
- Custom library: https://github.com/deezer/spleeter/issues/229
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow version: 1.14
- Python version: 3.7.4

## Stacktrace
```
{
	""message"": {
		""traceback"": [{
				""memory"": 2925,
				""blocks"": 29192,
				""stack"": [
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\"", line 246"",
					""    allow_broadcast=True)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\"", line 290"",
					""    name=name).outputs[0]"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\"", line 507"",
					""    return func(*args, **kwargs)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\"", line 3616"",
					""    op_def=op_def)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\"", line 2005"",
					""    self._traceback = tf_stack.extract_stack()"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_stack.py\"", line 64"",
					""    ret.append((filename, lineno, name, frame_globals, func_start_lineno))""
				]
			},
			{
				""memory"": 2756,
				""blocks"": 12,
				""stack"": [
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\"", line 1145"",
					""    as_ref=False)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\"", line 1224"",
					""    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\"", line 305"",
					""    return constant(v, dtype=dtype, name=name)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\"", line 246"",
					""    allow_broadcast=True)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\"", line 254"",
					""    t = convert_to_eager_tensor(value, ctx, dtype)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\"", line 115"",
					""    return ops.EagerTensor(value, handle, device, dtype)""
				]
			},
			{
				""memory"": 2188,
				""blocks"": 21506,
				""stack"": [
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/error_interpolation.py\"", line 319"",
					""    for frame in op.traceback:"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\"", line 2568"",
					""    return tf_stack.convert_stack(self._traceback)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_stack.py\"", line 123"",
					""    line = linecache.getline(filename, lineno, frame_globals)"",
					""  File \""/usr/local/lib/python3.7/linecache.py\"", line 16"",
					""    lines = getlines(filename, module_globals)"",
					""  File \""/usr/local/lib/python3.7/linecache.py\"", line 47"",
					""    return updatecache(filename, module_globals)"",
					""  File \""/usr/local/lib/python3.7/linecache.py\"", line 137"",
					""    lines = fp.readlines()""
				]
			},
			{
				""memory"": 1715,
				""blocks"": 19490,
				""stack"": [
					""  File \""<frozen importlib._bootstrap>\"", line 983"",
					""  File \""<frozen importlib._bootstrap>\"", line 967"",
					""  File \""<frozen importlib._bootstrap>\"", line 677"",
					""  File \""<frozen importlib._bootstrap_external>\"", line 724"",
					""  File \""<frozen importlib._bootstrap_external>\"", line 857"",
					""  File \""<frozen importlib._bootstrap_external>\"", line 525""
				]
			},
			{
				""memory"": 1620,
				""blocks"": 16132,
				""stack"": [
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\"", line 587"",
					""    \""ReadVariableOp\"", resource=resource, dtype=dtype, name=name)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\"", line 788"",
					""    op_def=op_def)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\"", line 507"",
					""    return func(*args, **kwargs)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\"", line 3616"",
					""    op_def=op_def)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\"", line 2005"",
					""    self._traceback = tf_stack.extract_stack()"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_stack.py\"", line 64"",
					""    ret.append((filename, lineno, name, frame_globals, func_start_lineno))""
				]
			},
			{
				""memory"": 1370,
				""blocks"": 13764,
				""stack"": [
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\"", line 1503"",
					""    \""VarIsInitializedOp\"", resource=resource, name=name)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\"", line 788"",
					""    op_def=op_def)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\"", line 507"",
					""    return func(*args, **kwargs)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\"", line 3616"",
					""    op_def=op_def)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\"", line 2005"",
					""    self._traceback = tf_stack.extract_stack()"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_stack.py\"", line 64"",
					""    ret.append((filename, lineno, name, frame_globals, func_start_lineno))""
				]
			},
			{
				""memory"": 1148,
				""blocks"": 11418,
				""stack"": [
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_control_flow_ops.py\"", line 935"",
					""    \""Switch\"", data=data, pred=pred, name=name)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\"", line 788"",
					""    op_def=op_def)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\"", line 507"",
					""    return func(*args, **kwargs)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\"", line 3616"",
					""    op_def=op_def)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\"", line 2005"",
					""    self._traceback = tf_stack.extract_stack()"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_stack.py\"", line 64"",
					""    ret.append((filename, lineno, name, frame_globals, func_start_lineno))""
				]
			},
			{
				""memory"": 1140,
				""blocks"": 5830,
				""stack"": [
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\"", line 788"",
					""    op_def=op_def)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\"", line 507"",
					""    return func(*args, **kwargs)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\"", line 3616"",
					""    op_def=op_def)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\"", line 2037"",
					""    for i, output_type in enumerate(output_types)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\"", line 2037"",
					""    for i, output_type in enumerate(output_types)"",
					""  File \""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\"", line 357"",
					""    self._consumers = []""
				]
			}
		]
	}
}
```

## Discussion:
Using as a workaround:

```python
tf.reset_default_graph()
tf.keras.backend.clear_session()
```

we had memory leak decreasing from the whole variables retained in the python function (a `data` holding the wave file ~24MB), to a smaller fraction of ~600-700KB, but formally the leak is still there.

Possibile duplicate of https://github.com/tensorflow/tensorflow/issues/35010 
but related to TF 1.14

"
35820,Same op sequence is computed multiple times,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Not applicable
- TensorFlow version (use command below): 2.1.0
- Python version: Not applicable
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Not applicable
- GPU model and memory: Not applicable

**Describe the current behavior**
When applying a layer multiple times to the same input, it seems TF is computing it for every call, even if it's unnecessary, see colab:
https://colab.research.google.com/drive/1PTvolVt1xQJvgp4MP0ZvgbemKBrqWy0U

**Describe the expected behavior**
The duplicate parts of the graph should be squashed

**Code to reproduce the issue**
https://colab.research.google.com/drive/1PTvolVt1xQJvgp4MP0ZvgbemKBrqWy0U
"
35819,"AutoGraph could not transform ... (Bad argument number for Name: 3, expecting 4)","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows, Linux
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.6

**Describe the current behavior**
Warning said to report to TensorFlow team, see below.

**Code to reproduce the issue**
```
import tensorflow as tf

def test(data):
    tf.cond(
        tf.less(tf.random.uniform((), maxval=2), 1),
        lambda: 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0,
        lambda: 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0,
    )
    return data

tf.data.Dataset.from_tensor_slices([[[0]]]).map(test)
```
**Other info / logs**
```
2020-01-13 16:01:26.037936: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-01-13 16:01:28.948415: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-01-13 16:01:29.117581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:02:00.0 name: Quadro M5000 computeCapability: 5.2
coreClock: 1.038GHz coreCount: 16 deviceMemorySize: 7.50GiB deviceMemoryBandwidth: 196.99GiB/s
2020-01-13 16:01:29.132706: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-01-13 16:01:29.150465: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-01-13 16:01:29.169042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-01-13 16:01:29.177916: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-01-13 16:01:29.197289: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-01-13 16:01:29.210804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-01-13 16:01:29.230095: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-01-13 16:01:29.238015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-01-13 16:01:29.243127: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-01-13 16:01:29.256487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:02:00.0 name: Quadro M5000 computeCapability: 5.2
coreClock: 1.038GHz coreCount: 16 deviceMemorySize: 7.50GiB deviceMemoryBandwidth: 196.99GiB/s
2020-01-13 16:01:29.269781: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-01-13 16:01:29.275906: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-01-13 16:01:29.285202: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-01-13 16:01:29.291266: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-01-13 16:01:29.299261: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-01-13 16:01:29.304025: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-01-13 16:01:29.311939: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-01-13 16:01:29.316594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-01-13 16:01:30.029865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-13 16:01:30.036285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0
2020-01-13 16:01:30.040429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N
2020-01-13 16:01:30.045089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6016 MB memory) -> physical GPU (device: 0, name: Quadro M5000, pci bus id: 0000:02:00.0, compute capability: 5.2)
WARNING:tensorflow:AutoGraph could not transform <function test at 0x000001977DE52438> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Bad argument number for Name: 3, expecting 4
```"
35818,Creating outputs from the final_state returned by dynamic_rnn causes conversion to fail,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): r1.14


**Command used to run the converter or code if you’re using the Python API**
import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_saved_model('./savedmodel')
tflite_model = converter.convert()
open(""lstm.tflite"", ""wb"").write(tflite_model)

**The output from the converter invocation**

Traceback (most recent call last):
  File ""saved2lite.py"", line 4, in <module>
    tflite_model = converter.convert()
  File ""/workdir/env-cpu/lib/python2.7/site-packages/tensorflow/lite/python/lite.py"", line 898, in convert
    **converter_kwargs)
  File ""/workdir/env-cpu/lib/python2.7/site-packages/tensorflow/lite/python/convert.py"", line 404, in toco_convert_impl
    input_data.SerializeToString())
  File ""/workdir/env-cpu/lib/python2.7/site-packages/tensorflow/lite/python/convert.py"", line 172, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2020-01-13 14:56:27.199876: F tensorflow/lite/toco/tooling_util.cc:918] Check failed: GetOpWithOutput(model, output_array) Specified output array ""lstm/body/encoder/rnn/while/Identity_4"" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.


**Failure details**

Model conversion fails when final_state is returned as an output


**Any other info / logs**

relevant code in model definition:

output, next_state = tf.compat.v1.lite.experimental.nn.dynamic_rnn(...)
c_out = next_state[0].c
h_out = next_state[0].h

relevant code in conversion to SavedModel (which does work):

tensor_info_c_out = tf.saved_model.utils.build_tensor_info(c_out)
tensor_info_h_out = tf.saved_model.utils.build_tensor_info(h_out)

prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs = {'x': x}, outputs = {'y': output, 'c_out': c_out, 'h_out':h_out), method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)


The TFLite conversion works if I don't return the next state variables, c_out and h_out, but I need those to run the next step of the prediction.

If I run the conversion with tfnightly (using the same SavedModel file saved using 1.14) I get a different, seemingly unrelated, error:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CAST, FULLY_CONNECTED, GATHER, MUL, NOT_EQUAL, RESHAPE, SOFTMAX, SQUEEZE, TOPK_V2. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While."
35817,LayerNormalization dtype issues with mixed_precision.Policy('mixed_float16'),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab notebook
- TensorFlow version (use command below): tf-nightly (2.2.0-dev20200113)
- GPU model and memory: GPU 0: Tesla T4

I'm making small edits to the colab notebook here: https://www.tensorflow.org/guide/keras/mixed_precision

When using mixed precision computation via 
```python
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_policy(policy)
```

LayerNormalization doesn't appear to work
```python
inputs = keras.Input(shape=(784,), name='digits')
if tf.config.list_physical_devices('GPU'):
  print('The model will run with 4096 units on a GPU')
  num_units = 4096
else:
  # Use fewer units on CPUs so the model finishes in a reasonable amount of time
  print('The model will run with 64 units on a CPU')
  num_units = 64
dense1 = layers.Dense(num_units, activation='relu', name='dense_1')
x = dense1(inputs)
dense2 = layers.Dense(num_units, activation='relu', name='dense_2')
x = dense2(x)
layer_norm = layers.LayerNormalization()
x = layer_norm(x)
```

```
The model will run with 4096 units on a GPU
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-7-7e586313e764> in <module>()
     12 x = dense2(x)
     13 layer_norm = layers.LayerNormalization()
---> 14 x = layer_norm(x)

5 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)
     59           ""allowed values: %s"" %
     60           (param_name, dtypes.as_dtype(dtype).name,
---> 61            "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
     62 
     63 

TypeError: Value passed to parameter 'scale' has DataType float16 not in list of allowed values: float32
```

Replacing LayerNormalization with BatchNormalization works without issue.



"
35816,Batching images first or formatting them first?,"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_lite/tflite_c04_exercise_convert_model_to_tflite_solution.ipynb

Under ""Create a Dataset from Images and Labels"", we have this code

`train_batches=train_examples.cache().shuffle(num_examples//4).batch(BATCH_SIZE).map(format_example).prefetch(1)`

and similar for the validation and test examples...

Is this a right practise?? 
Shouldn't we first format the raw images and then batch them together rather than opposite way?"
35815,"model.fit outputs an extra ""Epoch 1/x"" for each epoch","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): see below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Linux
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.6

**Describe the current behavior**
```
Epoch 1/2
Epoch 1/2
1/1 [==============================] - 0s 13ms/step - loss: 0.0000e+00
1/1 [==============================] - 0s 481ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00
Epoch 2/2
Epoch 1/2
1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00
1/1 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - val_loss: 0.0000e+00
```

**Describe the expected behavior**
""Epoch 1/2"" should not be in each epoch.

**Code to reproduce the issue**
```
import tensorflow as tf

data = [[1]]
dataset = tf.data.Dataset.from_tensor_slices(data)
input_target = tf.data.Dataset.zip((dataset, dataset))
layer = tf.keras.layers.Input(shape=())
model = tf.keras.models.Model(inputs=layer, outputs=layer)
model.compile(optimizer=""adam"", loss=""mse"", experimental_run_tf_function=False)
model.fit(x=input_target, validation_data=input_target, epochs=2)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35814,Failed to load the native TensorFlow runtime. even though I have specified all paths and everything,"**System information**
- OS: Windows 10
- TensorFlow version: 2.1
- Python version: 3.7.0
- Installed using pip
- CUDA version: 10.1
- cuDNN version: v7.6.5.32
- GPU model and memory: GTX 1080 Ti

**Describe the problem**
Hello I tried installing tensorflow using the following command - 
```
pip --no-cache-dir install tensorflow
```
I am using no cache dir as I tried installing tensorflow many times and all of them failed. So i reinstalled python 4 times and then finally I used this command. It installed tensorflow.
But now  when I do - 
```
>>> import tensorflow
```
It gives me the following error - 

https://pastebin.com/T9niAwFt

So now I have installed everything and even added python to path and also the cuda and cudnn. But please note last time I installed python I didn't add it to path so I added it so it made 2 paths when I reinstalled python and they got duplicated and then I deleted the duplicated ones. And now here are my path variables - 
![image](https://user-images.githubusercontent.com/30827615/72258042-12829080-3633-11ea-8f3f-057141dbbceb.png)
And by the way I am using vs code 2017 and I have everything installed and also by the way I am not using any venv as I want to acess tensorflow without having to activate any environment and writing lines to activate every time I wanna acess tensorflow."
35813,Threading + tf.keras.layers.Input produces TypeError: 'NoneType' object is not iterable,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- No custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
- TensorFlow installed from (source or binary): from pip
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0
- Python version: Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) 
[GCC 7.3.0] on linux

- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When I build a `tf.keras` model with the functional API in a `threading.Thread`, I often (not always) see an error in a thread:
```python
TypeError: 'NoneType' object is not iterable
```
After the exception has appeared once, it does not appear again, until the python kernel is restarted.

In fact, it is not even necessary to build a whole model. Just instantiating a `tf.keras.layers.Input` layer is sufficient to get the error. I tested a few other layers (Dense, Conv1D) and they do not lead to the error, so am suspecting that there is something happening with Input)

**Describe the expected behavior**
I expect that no such error should appear in any thread when building a `tf.keras` model.

**Code to reproduce the issue**
```python
from threading import Thread
import tensorflow as tf

def make_model():
    tf.keras.layers.Input(10)
    
[Thread(target=make_model).start() for _ in range(10)]
```

**Other info / logs**
The error does not appear if i instantiate a `tf.keras.models.Model` subclass.
for example:

```python
from threading import Thread
import tensorflow as tf

class Model(tf.keras.models.Model):
    def __init__(self):
        super().__init__()
        self.dense = tf.keras.layers.Dense(10)
    
    def call(self, inputs):
        return self.dense(inputs)
    
def make_model():
    Model()
    
[Thread(target=make_model).start() for _ in range(10)]
```

Full traceback:
```python
Exception in thread Thread-3:
Traceback (most recent call last):
  File ""/home/dani/miniconda3/envs/testtf/lib/python3.7/threading.py"", line 926, in _bootstrap_inner
    self.run()
  File ""/home/dani/miniconda3/envs/testtf/lib/python3.7/threading.py"", line 870, in run
    self._target(*self._args, **self._kwargs)
  File ""<ipython-input-1-3fe088eac4c0>"", line 5, in make_model
    tf.keras.layers.Input(10)
  File ""/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_layer.py"", line 274, in Input
    input_layer = InputLayer(**input_layer_config)
  File ""/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_layer.py"", line 127, in __init__
    ragged=ragged)
  File ""/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py"", line 1054, in placeholder
    x = array_ops.placeholder(dtype, shape=shape, name=name)
  File ""/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py"", line 2990, in placeholder
    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
  File ""/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py"", line 6676, in placeholder
    ""Placeholder"", dtype=dtype, shape=shape, name=name)
  File ""/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py"", line 759, in _apply_op_helper
    return output_structure, op_def.is_stateful, op, outputs
  File ""/home/dani/miniconda3/envs/testtf/lib/python3.7/contextlib.py"", line 119, in __exit__
    next(self.gen)
  File ""/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 418, in inner_cm
    for fn in self._scope_exit_callbacks:
TypeError: 'NoneType' object is not iterable
```
Notes: the error does not appear in tf 2.0."
35811,[Feature Request] Deformable Convolution v1 for tf-1.1.0 slim,"**System information**
- TensorFlow version (you are using): 1.1.0
TensorFlow installed from conda command
- Are you willing to contribute it (Yes/No): yes,if I have the ability

**Describe the feature and the current behavior/state.**
Deformable Convolutional [https://arxiv.org/pdf/1703.06211.pdf,https://arxiv.org/pdf/1703.06211.pdf](url), I used an old project with TF1.1.0 slim. Now I want to add Deformable Convolutional layer to train my network.
But, I can't find a way to achieve it.

**Will this change the current api? How?**
yes,add a slim layer
**Who will benefit with this feature?**
anyone who want to use deformable conv by tf slim"
35810,Autograph transformation warning,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: 7.6
- GPU model and memory: 2080ti


**Describe the current behavior**
I'm seeing the message ""AutoGraph could not transform and will run it as-is""
I found no performance issue, because a model shows almost the same losses compared to the exact code implemented with pytorch. I'm using gast version 0.2.2, so I doubt this is caused by gast. 
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import os
import tensorflow as tf
from tensorflow.keras import layers
import tensorflow.keras as keras


os.environ['AUTOGRAPH_VERBOSITY'] = '10'

def shape_list(x):
    static = x.shape.as_list()
    dynamic = tf.shape(x)
    return [dynamic[i] if s is None else s for i, s in enumerate(static)]


class TFConv1D(layers.Layer):
    def __init__(self, input_dim, output_dim, init_std=0.02, use_bias=True, **kwargs):
        """""" TFConv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)
            Basically works like a Linear layer but the weights are transposed
        """"""
        super(TFConv1D, self).__init__(**kwargs)
        self.nf = output_dim
        self.nx = input_dim
        self.initializer_range = init_std
        self.use_bias = use_bias
        self.weight = self.add_weight(
            ""{}_weight"".format(self.name),
            shape=[self.nx, self.nf],
            initializer=keras.initializers.TruncatedNormal(stddev=init_std))
        if self.use_bias:
            self.bias = self.add_weight(
                ""{}_bias"".format(self.name),
                shape=[1, self.nf],
                initializer=tf.zeros_initializer())

    def call(self, x):
        x = tf.matmul(x, self.weight)
        if self.use_bias:
            x += self.bias
        return x


class Adaptive_Softmax(layers.Layer):
    def __init__(self, vocab_size: int, hidden_dim: int, cutoffs: list, padding_index: int, init_std=0.02):
        super(Adaptive_Softmax, self).__init__()
        self.padding_index = padding_index
        self.vocab_size = vocab_size
        self.hidden_dim = hidden_dim
        self.n_clusters = len(cutoffs) + 1
        self.cutoffs = [0] + cutoffs + [vocab_size]
        self.cluster_logit = TFConv1D(hidden_dim, self.n_clusters)
        self.logits = self.add_weight(
            ""{}_weight"".format(self.name),
            shape=[hidden_dim, vocab_size],
            initializer=keras.initializers.TruncatedNormal(stddev=init_std))

        self.bias = self.add_weight(
            ""{}_bias"".format(self.name),
            shape=[1, vocab_size],
            initializer=tf.zeros_initializer())

    def call(self, x, y):
        x = x[:, :-1]
        b, l, h = shape_list(x)
        x = tf.reshape(x, [b * l, -1])
        y = tf.reshape(y, [-1])
        cl = self.cluster_logit(x)
        cluster_ll = tf.nn.log_softmax(cl, axis=1)
        nll = tf.zeros_like(y, dtype=x.dtype)
        tail_weight = self.logits

        for i in range(self.n_clusters):
            l, r = self.cutoffs[i], self.cutoffs[i + 1]
            mask = (y >= l) & (y < r)
            indices = tf.where(mask)
            target_i = tf.boolean_mask(y, mask) - l
            tail_logit = tf.matmul(tf.boolean_mask(x, mask), tail_weight[:, l:r]) + self.bias[:, l:r]
            tail_logprob_i = tf.nn.log_softmax(tail_logit, axis=1)  # [b,vocab]
            # word_nll[indices] = -logprob_i
            cur_ll = tf.gather_nd(cluster_ll, tf.concat([indices, tf.ones_like(indices) * i], 1)) + \
                     tf.gather_nd(tail_logprob_i,
                                  tf.stack([tf.range(tf.size(target_i, out_type=target_i.dtype)), target_i], 1))
            nll = tf.tensor_scatter_nd_update(nll, indices, -cur_ll)
        return nll

vocab_size = 51
hidden_dim = 100
cutoffs = [5,20]
padding_index = 50
x = tf.random.normal((800,51,100),dtype=tf.float32)
y = tf.random.uniform((800,50),maxval=50,dtype=tf.int64)

dataset = tf.data.Dataset.from_tensor_slices((x,y))
batchfier = dataset.batch(4)

model = Adaptive_Softmax(vocab_size,hidden_dim,cutoffs,padding_index)
optimizer = keras.optimizers.Adam()

@tf.function
def update_step(x, y):
    with tf.GradientTape() as tape:
        batch_loss = model(x,y)
    step_grad = tape.gradient(batch_loss, model.trainable_variables)
    optimizer.apply_gradients(zip(step_grad, model.trainable_variables))
    return batch_loss

for x,y in batchfier:
    update_step(x,y)

```
**Other info / logs**

> 2020-01-13 16:53:44.063623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-01-13 16:53:44.064637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
2020-01-13 16:53:44.604399: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-01-13 16:53:44.613753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-13 16:53:44.614201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s
2020-01-13 16:53:44.614220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-01-13 16:53:44.614238: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-01-13 16:53:44.615203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-01-13 16:53:44.615364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-01-13 16:53:44.616463: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-01-13 16:53:44.617020: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-01-13 16:53:44.617041: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-13 16:53:44.617092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-13 16:53:44.617634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-13 16:53:44.618056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-01-13 16:53:44.618258: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-13 16:53:44.641610: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz
2020-01-13 16:53:44.642154: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ad9a2a6710 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-01-13 16:53:44.642165: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-01-13 16:53:44.686305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-13 16:53:44.686789: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ad9a925960 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-01-13 16:53:44.686818: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2020-01-13 16:53:44.686923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-13 16:53:44.687358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.73GiB deviceMemoryBandwidth: 573.69GiB/s
2020-01-13 16:53:44.687376: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-01-13 16:53:44.687383: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-01-13 16:53:44.687395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-01-13 16:53:44.687403: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-01-13 16:53:44.687411: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-01-13 16:53:44.687419: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-01-13 16:53:44.687425: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-01-13 16:53:44.687455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-13 16:53:44.687898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-13 16:53:44.688316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-01-13 16:53:44.688332: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-01-13 16:53:44.910219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-13 16:53:44.910242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-01-13 16:53:44.910247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-01-13 16:53:44.910389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-13 16:53:44.910886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-13 16:53:44.911318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9064 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
WARNING:tensorflow:AutoGraph could not transform <bound method Adaptive_Softmax.call of <__main__.Adaptive_Softmax object at 0x7ff7a6a7f320>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7ff79c3853c8>, <gast.gast.Return object at 0x7ff79c385c50>]
WARNING:tensorflow:AutoGraph could not transform <bound method Adaptive_Softmax.call of <__main__.Adaptive_Softmax object at 0x7ff7a6a7f320>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7ff79c212198>, <gast.gast.Return object at 0x7ff79c2121d0>]
2020-01-13 16:53:45.777560: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10

Process finished with exit code 0


Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35831,Building the PoseNet example on iOS is failed,"**System information**
- OS Platform and Distribution: macOS 10.15 and iPadOS 13.3
- Mobile device: iPad 2018
- TensorFlow installed from (source or binary): pod

**Describe the problem**

I did all the steps from the readme of https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/ios 

but got next error:

> [!] CocoaPods could not find compatible versions for pod ""TensorFlowLiteSwift"":
>  In snapshot (Podfile.lock):
>    TensorFlowLiteSwift (= 0.0.1-nightly)

> In Podfile:
>    TensorFlowLiteSwift (= 0.0.1-nightly)

> None of your spec sources contain a spec satisfying the dependencies: `TensorFlowLiteSwift (= 0.0.1-nightly), TensorFlowLiteSwift (= 0.0.1-nightly)`.

>You have either:
> * out-of-date source repos which you can update with `pod repo update` or with `pod install --repo-update`.
> * mistyped the name or version.
> * not added the source repo that hosts the Podspec to your Podfile.

**The solution**

To make it work I just removed the Podfile.lock file and run again the command
`pod install`

Maybe the source repo should also be updated.

Thanks!

P.S.: Initially it was posted there https://github.com/tensorflow/tensorflow/issues/35803 "
35809,Typo Error in `tflite_c02_transfer_learning.ipynb`,"## URL(s) with the issue:

https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_lite/tflite_c02_transfer_learning.ipynb

## Description of issue (what needs changing):

![Screenshot from 2020-01-13 12-53-58](https://user-images.githubusercontent.com/29497701/72238480-d7b53400-3603-11ea-847d-0eb7c0eb0716.png)

In description, it should be 'cats_vs_dogs'

### Submit a pull request?

Yes"
35808,Update Pull Request Template,"When I read the pull request, it is hard to read, so I think updating a Pull Request Template  that similar to the Issue Template will make pull requests more readable. Such as:
######Description

######Check List

######My PR Type is:

Thank you!!
"
35807,concat axis!=-1 and reshape change data layout don't support quant on dsp or NPU,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):pip
- TensorFlow version (use command below):1.14.0
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
nnapi log said:concat axis!=-1 and reshape change data layout don't support accelator
**Describe the expected behavior**
it should run successful on dsp or hta
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
./benmakrk --graph=myquantized.tflite --use_nnapi=true
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
New Comment: ""01-08 20:47:35.790 14384 14384 I TypeManager: Failed to read /vendor/etc/nnapi_extensions_app_allowlist ; No app allowlisted for vendor extensions use.
01-08 20:47:35.799 919 1023 E OperationsUtils: NN_CHECK failed: inputShapes[0].scale == inputShapes[i].scale'
01-08 20:47:35.799 919 1023 E OperationsUtils:
01-08 20:47:35.799 919 1023 E OperationsUtils: NN_CHECK failed: inputShapes[0].offset == inputShapes[i].offset'
01-08 20:47:35.799 919 1023 E OperationsUtils:
01-08 20:47:35.801 919 1023 E hta-unnhal: RESHAPE doesn't support changing data layout
01-08 20:47:35.801 919 1023 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.
01-08 20:47:35.801 919 1023 E hta-unnhal: RESHAPE doesn't support changing data layout
01-08 20:47:35.801 919 1023 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.
01-08 20:47:35.801 919 1023 E hta-unnhal: RESHAPE doesn't support changing data layout
01-08 20:47:35.801 919 1023 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.
01-08 20:47:35.801 919 1023 E hta-unnhal: Only supports channel dimension for CONCATENATION
01-08 20:47:35.801 919 1023 E hta-unnhal: {CONCATENATION, TENSOR_QUANT8} is not supported.
01-08 20:47:35.801 919 1023 E hta-unnhal: RESHAPE doesn't support changing data layout
01-08 20:47:35.801 919 1023 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.
01-08 20:47:35.801 919 1023 E hta-unnhal: Only supports channel dimension for CONCATENATION
01-08 20:47:35.801 919 1023 E hta-unnhal: {CONCATENATION, TENSOR_QUANT8} is not supported.
01-08 20:47:35.805 919 1023 E OperationsUtils: NN_CHECK failed: inputShapes[0].scale == inputShapes[i].scale'
01-08 20:47:35.805 919 1023 E OperationsUtils:
01-08 20:47:35.806 919 1023 E OperationsUtils: NN_CHECK failed: inputShapes[0].offset == inputShapes[i].offset'
01-08 20:47:35.806 919 1023 E OperationsUtils:
01-08 20:47:35.810 919 1023 W android.hardware.neuralnetworks@1.2-service: pickAcceleratorByName cannot find the accelerator:adreno
01-08 20:47:35.810 919 1023 W android.hardware.neuralnetworks@1.2-service:
01-08 20:47:35.810 919 1023 W android.hardware.neuralnetworks@1.2-service: getSupportedOperations() cannot pick an accelerator
01-08 20:47:35.810 919 1023 W android.hardware.neuralnetworks@1.2-service:
01-08 20:47:35.814 919 919 E hta-unnhal: RESHAPE doesn't support changing data layout
01-08 20:47:35.814 919 919 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.
01-08 20:47:35.814 919 919 E hta-unnhal: RESHAPE doesn't support changing data layout
01-08 20:47:35.814 919 919 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.
01-08 20:47:35.814 919 919 E hta-unnhal: RESHAPE doesn't support changing data layout
01-08 20:47:35.814 919 919 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.
01-08 20:47:35.814 919 919 E hta-unnhal: Only supports channel dimension for CONCATENATION
01-08 20:47:35.814 919 919 E hta-unnhal: {CONCATENATION, TENSOR_QUANT8} is not supported.
01-08 20:47:35.814 919 919 E hta-unnhal: RESHAPE doesn't support changing data layout
01-08 20:47:35.814 919 919 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} is not supported.
01-08 20:47:35.814 919 919 E hta-unnhal: Only supports channel dimension for CONCATENATION
01-08 20:47:35.814 919 919 E hta-unnhal: {CONCATENATION, TENSOR_QUANT8} is not supported.""
"
35806,"save_model_builder saved, then estimator load fail!","This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

tensorflow: 1.12.0
environment: win10
code:
```python3
import tensorflow as tf
saved_model_dir = ""./model_tst""

def builder_and_save_model(saved_model_dir):
    with tf.Session(graph=tf.Graph()) as sess:
        x = tf.placeholder(tf.int32, name='input_x')
        y = tf.placeholder(tf.int32, name='input_y')
        b = tf.Variable(1, name='b')
        xy = tf.multiply(x, y)
        # 这里的输出需要加上name属性
        op = tf.add(xy, b, name='output')

        sess.run(tf.global_variables_initializer())

        builder = tf.saved_model.builder.SavedModelBuilder(saved_model_dir)
        # x 为输入tensor, keep_prob为dropout的prob tensor
        inputs = {'input_x': tf.saved_model.utils.build_tensor_info(x),
                  'input_y': tf.saved_model.utils.build_tensor_info(y)}

        # y 为最终需要的输出结果tensor
        outputs = {'output': tf.saved_model.utils.build_tensor_info(op)}

        signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs,
                                                                           outputs=outputs,
                                                                           method_name='variables')
        builder.add_meta_graph_and_variables(sess=sess,
                                             tags=[tf.saved_model.tag_constants.SERVING],
                                             signature_def_map={'variables': signature},
                                             assets_collection=None,)
        builder.save()

builder_and_save_model(saved_model_dir)
predict_fn = tf.contrib.predictor.from_saved_model(saved_model_dir)
prediction = predict_fn( {""input_x"": 5, ""input_y"": 6})
print(prediction[""output""])
```

error:
```python3
ValueError: Got unexpected keys in input_dict: {'input_y', 'input_x'}
expected: set()
```
"
35805,Win10: ImportError: DLL load failed: The specified module could not be found,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 home
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: PC
- TensorFlow installed from (source or binary): 1.13.0
- TensorFlow version:
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0 7.4.2.24
- GPU model and memory: RTX 2080 Ti
- CPU model and make: AMD Ryzen 7 3800X 8-Core Processor 3.89GHz
- Anaconda Python Command Prompt

**Describe the problem**

Trying to build TF 1.13 from source. Issue at the creation of wheel file with bazel. Attached exact sequence of commands issued. Trying to conver a *.pb file into a TFLite file that was trained in tensorflow 1.13

**Exact sequence of commands / steps executed before running into the problem**

conda update -n base -c defaults conda
conda update --all
conda create -n tensorflow-build pip python=3.6
conda activate tensorflow-build
python -m pip install --upgrade pip
conda install -c anaconda git
set PATH=%PATH%;C:\msys64\usr\bin
pip install six numpy wheel
pip install keras_applications==1.0.6 --no-deps
pip install keras_preprocessing==1.0.5 --no-deps
conda install -c conda-forge bazel=0.21.0
mkdir C:\tensorflow-build
cd C:\tensorflow-build
git clone https://github.com/tensorflow/tensorflow.git 
cd tensorflow
git checkout r1.13
python ./configure.py

You have bazel 0.21.0- (@non-git) installed. 

Please specify the location of python. [Default is C:\ProgramData\Anaconda3\envs\tensorflow-build\python.exe]: 
  
Found possible Python library paths: 

  C:\ProgramData\Anaconda3\envs\tensorflow-build\lib\site-packages 

Please input the desired Python library path to use.  Default is [C:\ProgramData\Anaconda3\envs\tensorflow-build\lib\site-packages] 

Do you wish to build TensorFlow with XLA JIT support? [y/N]: n 
No XLA JIT support will be enabled for TensorFlow. 

Do you wish to build TensorFlow with ROCm support? [y/N]: n 
No ROCm support will be enabled for TensorFlow. 
  
Do you wish to build TensorFlow with CUDA support? [y/N]: y 

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]: 10.0

Please specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:

Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7.4

Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:

Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 7.5

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:

Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y
Eigen strong inline overridden.

bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package


**info / logs**

////////////////////////////////////////////////////////LOG/////////////////////////////////////////////////////

ERROR: C:/tensorflow-build/tensorflow/tensorflow/BUILD:579:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1): bash.exe failed: error executing command
  cd C:/users/eduar/_bazel_eduar/j7bi4x5j/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin
    SET PYTHON_BIN_PATH=C:/Users/eduar/.conda/envs/tensorflow-build/python.exe
    SET PYTHON_LIB_PATH=C:/Users/eduar/.conda/envs/tensorflow-build/lib/site-packages
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
  C:/msys64/usr/bin/bash.exe bazel-out/x64_windows-opt/genfiles/tensorflow/tf_python_api_gen_v1.genrule_script.sh
Execution platform: @bazel_tools//platforms:host_platform
Traceback (most recent call last):
  File ""\\?\C:\Users\eduar\AppData\Local\Temp\Bazel.runfiles_fxnhn1zo\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""\\?\C:\Users\eduar\AppData\Local\Temp\Bazel.runfiles_fxnhn1zo\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""\\?\C:\Users\eduar\AppData\Local\Temp\Bazel.runfiles_fxnhn1zo\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\eduar\.conda\envs\tensorflow-build\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\eduar\.conda\envs\tensorflow-build\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""\\?\C:\Users\eduar\AppData\Local\Temp\Bazel.runfiles_fxnhn1zo\runfiles\org_tensorflow\tensorflow\python\tools\api\generator\create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""\\?\C:\Users\eduar\AppData\Local\Temp\Bazel.runfiles_fxnhn1zo\runfiles\org_tensorflow\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""\\?\C:\Users\eduar\AppData\Local\Temp\Bazel.runfiles_fxnhn1zo\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""\\?\C:\Users\eduar\AppData\Local\Temp\Bazel.runfiles_fxnhn1zo\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""\\?\C:\Users\eduar\AppData\Local\Temp\Bazel.runfiles_fxnhn1zo\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""\\?\C:\Users\eduar\AppData\Local\Temp\Bazel.runfiles_fxnhn1zo\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\eduar\.conda\envs\tensorflow-build\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\eduar\.conda\envs\tensorflow-build\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1319.085s, Critical Path: 291.62s
INFO: 4651 processes: 4651 local.
FAILED: Build did NOT complete successfully

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35804,Tensorboard 2.1 broken on Windows 10 (won't display anything on webpage),"I've ran the same code both before and after upgrading from 2.0 to 2.1 on two different machines.  What worked in 2.0 now in 2.1 serves up a blank web page instead of the usual graphs and stuff (page is completely blanked out).  Again, it seems to ""run"" as it would usually (no errors or anything, gives the typical localhost link, etc in the terminal -- just totally blank page served up).  

I've upgraded many times and this is the first TB issue I've every run across.  Any ideas?

In case you want example code to test out....

```
import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

def create_model():
  return tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax')
  ])

model = create_model()
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

log_dir=r""C:\Users\justjo\PycharmProjects\tensorboardTest\logs\fit""
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch = 100000000)

model.fit(x=x_train,
          y=y_train,
          epochs=5,
          validation_data=(x_test, y_test),
          callbacks=[tensorboard_callback])
```

Then use 
tensorboard --logdir=C:\Users\justjo\PycharmProjects\tensorboardTest\logs\fit
"
35803,Building the PoseNet example on iOS is failed,"**System information**
- OS Platform and Distribution: macOS 10.15 and iPadOS 13.3
- Mobile device: iPad 2018
- TensorFlow installed from (source or binary): pod

**Describe the problem**

I did all the steps from the readme of https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/ios 

but got next error:

> [!] CocoaPods could not find compatible versions for pod ""TensorFlowLiteSwift"":
>  In snapshot (Podfile.lock):
>    TensorFlowLiteSwift (= 0.0.1-nightly)

> In Podfile:
>    TensorFlowLiteSwift (= 0.0.1-nightly)

> None of your spec sources contain a spec satisfying the dependencies: `TensorFlowLiteSwift (= 0.0.1-nightly), TensorFlowLiteSwift (= 0.0.1-nightly)`.

>You have either:
> * out-of-date source repos which you can update with `pod repo update` or with `pod install --repo-update`.
> * mistyped the name or version.
> * not added the source repo that hosts the Podspec to your Podfile.

**The solution**

To make it work I just removed the Podfile.lock file and run again the command
`pod install`

Maybe the source repo should also be updated.

Thanks!"
35802,recude_max on RaggedTensor fails to propagate gradients,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary(pip)
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0/7.3.1
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Currently, passing a ragged tensor into a reduce_max an computing the gradient on the result fails. The following code is a minimal example of this:
```python
import numpy as np
import tensorflow as tf

with tf.GradientTape() as tape:
    ragged_t = tf.RaggedTensor.from_row_splits(
        [0.0, 1.0, 2.0, 3.0, 4.0, 5.0], [0, 1, 4, 6]
    )
    tape.watch(ragged_t.values)
    max_t = tf.reduce_max(ragged_t, axis=-1)
    # max_t = tf.reduce_max(ragged_t.to_tensor(default_value=np.nan), axis=-1)
    gradients = tape.gradient(max_t, ragged_t.values)
```

**Describe the expected behavior**
As tensorflows main purpose is to compute gradients and reduce operations on ragged tensors is the most basic usage of ragged tensor representations I consider this behavior a bug. The above code shows a workaround in the commented line, but I expect the above example to work without this workaround and without the need of allocating in some cases a substantial amount of memory through the usage of `to_tensor`.

**Other info / logs**
The full error trace of the above code example on my system is:
```
Traceback (most recent call last):
  File ""./misc/reduce_max_ragged_gradients.py"", line 13, in <module>
    gradients = tape.gradient(max_t, ragged_t.values)
  File ""/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py"", line 1014, in gradient
    unconnected_gradients=unconnected_gradients)
  File ""/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py"", line 76, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File ""/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py"", line 138, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File ""/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py"", line 455, in _UnsortedSegmentMaxGrad
    return _UnsortedSegmentMinOrMaxGrad(op, grad)
  File ""/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py"", line 432, in _UnsortedSegmentMinOrMaxGrad
    _GatherDropNegatives(op.outputs[0], op.inputs[1])
TypeError: 'NoneType' object is not subscriptable
```
"
35801,[Feature Request] MultiHeadAttention ops and Layer,"Multi Head Self Attention is reaching SOTA results in a wide variety of fields:
* NLP (Attention is all you need, GPT, Bert, etc)
* [Graphs](https://arxiv.org/abs/1710.10903)
* Computer Vision: [1](https://arxiv.org/abs/1906.05909) and [2](https://arxiv.org/abs/1911.03584)
* [Time Series](https://arxiv.org/pdf/1909.07369.pdf)
* [Audio/Speech Recognition](https://arxiv.org/pdf/1910.12977.pdf)

Nvidia already added this operation to [cudnn](https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_750.html) and pytorch already has an official [MultiHeadAttention](https://pytorch.org/docs/master/nn.html#multiheadattention) module. Its about time Tensorflow gets this operation too as it would enable researches and developers to get started quicker.

Currently there is an `Attention` layer in tf.keras but it only covers the main single head equation, has no learnable parameters, and can't be used as a basis for an efficient `MultiHeadAttention` implementation.
"
35799,Incompatible shapes when using tf.keras.backend.ctc_decode,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian Buster
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When using a `tf.keras.backend.ctc_decode` with a batch size <  the size of the model input, a ValueError is raised related to failure to broadcast input shapes.

**Describe the expected behavior**
I expect shapes to be consistent and therefore no `ValueError` to be raised.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import tensorflow as tf
import numpy as np

def CTCDecoder():
    def decoder(y_pred):
        input_shape = tf.keras.backend.shape(y_pred)
        input_length = tf.ones(shape=input_shape[0]) * tf.keras.backend.cast(input_shape[1], 'float32')
        return tf.keras.backend.ctc_decode(y_pred, input_length)[0][0]
    return tf.keras.layers.Lambda(decoder, name='decode')

input_layer = tf.keras.layers.Input((48, 37))
x = CTCDecoder()(input_layer)
model = tf.keras.models.Model(inputs=input_layer, outputs=x)

# This never raises a ValueError. The batch size is equal to the length
# of the input.
y = model.predict(np.random.uniform(size=(100, 48, 37)), batch_size=100)

# This usually raises a ValueError.
y = model.predict(np.random.uniform(size=(100, 48, 37)), batch_size=32)

# This always raises a ValueError.
y = model.predict(np.random.uniform(size=(100, 48, 37)), batch_size=1)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Here is the full traceback for an example exception.

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-145-9e000cf7055c> in <module>
----> 1 y = model.predict(np.random.uniform(size=(100, 48, 37)), batch_size=1)

/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)
   1011         max_queue_size=max_queue_size,
   1012         workers=workers,
-> 1013         use_multiprocessing=use_multiprocessing)
   1014 
   1015   def reset_metrics(self):

/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in predict(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)
    496         model, ModeKeys.PREDICT, x=x, batch_size=batch_size, verbose=verbose,
    497         steps=steps, callbacks=callbacks, max_queue_size=max_queue_size,
--> 498         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)
    499 
    500 

/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in _model_iteration(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)
    473               mode=mode,
    474               training_context=training_context,
--> 475               total_epochs=1)
    476           cbks.make_logs(model, epoch_logs, result, mode)
    477 

/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    177             batch_outs,
    178             batch_start=step * batch_size,
--> 179             batch_end=step * batch_size + current_batch_size)
    180       cbks.make_logs(model, batch_logs, batch_outs, mode)
    181       step += 1

/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py in aggregate(self, batch_outs, batch_start, batch_end)
    345     batch_outs = nest.flatten_up_to(self._structure, batch_outs)
    346     for batch_element, result in zip(batch_outs, self.results):
--> 347       result.aggregate(batch_element, batch_start, batch_end)
    348 
    349   def finalize(self):

/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py in aggregate(self, batch_element, batch_start, batch_end)
    278     num_elements = np.prod(batch_element.shape)
    279     if num_elements < self._BINARY_SIZE_THRESHOLD:
--> 280       self.results[batch_start:batch_end] = batch_element
    281     else:
    282       is_finished = threading.Event()

ValueError: could not broadcast input array from shape (1,46) into shape (1,48)
```"
35798,[TF2]     tf.TensorArray Error with Keras,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0
- Python version: 3.6

I took a `tf.TensorArray` [sample](https://www.tensorflow.org/guide/function#batching) from the documentation and wrapt it in a custom Keras Layer. The Layer works fine when I directly call the layer or the model, but when I user `model.predict` I get an error.

Code sample:
```
import tensorflow as tf

class Test_Layer(tf.keras.layers.Layer):

    def __init__(self):
        super(Test_Layer, self).__init__()

    @tf.function
    def call(self, x):
        result = tf.TensorArray(tf.int32, size=x.shape[0])
        for i in tf.range(x.shape[0]):
            if x[i] > 0:
                result = result.write(i, x[i] ** 2)
            else:
                result = result.write(i, x[i])
        return result.stack()



test_layer = Test_Layer()

out = test_layer(tf.range(-5, 5))
print(out) #works fine:= tf.Tensor([-5 -4 -3 -2 -1  0  1  4  9 16], shape=(10,), dtype=int32)


test_model = tf.keras.models.Sequential([test_layer])
test_model.compile(loss=tf.losses.mse)


out = test_model(tf.range(-5, 5))
print(out) #works fine:= tf.Tensor([-5 -4 -3 -2 -1  0  1  4  9 16], shape=(10,), dtype=int32)


out = test_model.predict(tf.range(-5, 5))
print(out) #ERROR
```

Traceback:
```
Traceback (most recent call last):
  File ""/media/jan/buffer/RNN/test_2.py"", line 34, in <module>
    out = test_model.predict(tf.range(-5, 5))
  File ""lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1013, in predict
    use_multiprocessing=use_multiprocessing)
  File ""lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 498, in predict
    workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)
  File ""lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 426, in _model_iteration
    use_multiprocessing=use_multiprocessing)
  File ""lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 646, in _process_inputs
    x, y, sample_weight=sample_weights)
  File ""lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 2346, in _standardize_user_data
    all_inputs, y_input, dict_inputs = self._build_model_with_inputs(x, y)
  File ""lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 2572, in _build_model_with_inputs
    self._set_inputs(cast_inputs)
  File ""lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 2659, in _set_inputs
    outputs = self(inputs, **kwargs)
  File ""lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 773, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""lib/python3.6/site-packages/tensorflow_core/python/keras/engine/sequential.py"", line 281, in call
    outputs = layer(inputs, **kwargs)
  File ""lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 773, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)
  File ""lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 606, in _call
    results = self._stateful_fn(*args, **kwds)
  File ""lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2362, in __call__
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File ""lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 439, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 3211, in bound_method_wrapper
    return wrapped_fn(*args, **kwargs)
  File ""lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in converted code:

    test_2.py:10 call  *
        result = tf.TensorArray(tf.int32, size=x.shape[0])
    lib/python3.6/site-packages/tensorflow_core/python/ops/tensor_array_ops.py:1078 __init__
        name=name)
    lib/python3.6/site-packages/tensorflow_core/python/ops/tensor_array_ops.py:444 __init__
        raise ValueError(""Size must be provided if flow is not provided"")

    ValueError: Size must be provided if flow is not provided

```"
35796,hlo_algorithm_blacklist.cc fails compilation,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : Windows 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.1
- Python version: 3.7
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source): VS2019
- CUDA/cuDNN version: 10.1 / 7.6
- GPU model and memory: 2070 Max Q

I setup it using configure with the following options selected : 
- XLA JIT
- Cuda
- /arch:AVX
- Eigen strong inline overridden

When attempting to build from source using the following command. 

bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow:libtensorflow.so

I get the following error.

ERROR: C:/sdks/tensorflow/tensorflow/compiler/xla/service/gpu/BUILD:1616:1: C++ compilation of rule '//tensorflow/compiler/xla/service/gpu:hlo_algorithm_blacklist' failed (Exit 2)
cl : Command line warning D9002 : ignoring unknown option '-std=c++14'
tensorflow/compiler/xla/service/gpu/hlo_algorithm_blacklist.cc(28): error C2131: expression did not evaluate to a constant
external/com_google_absl\absl/strings/string_view.h(186): note: a non-constant (sub-)expression was encountered
"
35795,"Saved model, KeyError: 'serving_default'","Have I written custom code: N/A
OS Platform and Distribution: Ubuntu 18.04
TensorFlow installed from : pip3
TensorFlow version 1.14.0
Bazel version 1.1.0
CUDA/cuDNN version: 10.1 / N/A
GPU model and memory GeForce GTX 1050
Exact command to reproduce : Describe below

I try to use that generated model:
http://eugen-lange.de/download/ssd-4-traffic-sign-detection-frozen_inpherence_graph-pb/

Here is my program:


```
class TFEngine:
    def __init__(self, model_name):
        self.model = None

        if ""ssd4tsd_full"" in model_name:
            self.load_model_local(""/home/xavier/Downloads/""+model_name + ""/saved_model"")
        else:
            self.load_model(model_name)

    def load_model(self, model_name):
        base_url = 'http://download.tensorflow.org/models/object_detection/'
        model_file = model_name + '.tar.gz'
        model_dir = tf.keras.utils.get_file(
            fname=model_name,
            origin=base_url + model_file,
            untar=True)
        model_dir = pathlib.Path(model_dir) / ""saved_model""
        model = tf.saved_model.load(str(model_dir))
        self.model = model.signatures['serving_default']
        return model

    def load_model_local(self, model_dir):
        model = tf.saved_model.load(str(model_dir))
        self.model = model.signatures['serving_default']
        return model

    def DetectWithImage(self, image):
        image = np.array(image)
        image = np.asarray(image)
        # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.
        input_tensor = tf.convert_to_tensor(image)
        # The model expects a batch of images, so add an axis with `tf.newaxis`.
        input_tensor = input_tensor[tf.newaxis, ...]

        # Run inference
        output_dict = self.model(input_tensor)

        # All outputs are batches tensors.
        # Convert to numpy arrays, and take index [0] to remove the batch dimension.
        # We're only interested in the first num_detections.
        num_detections = int(output_dict.pop('num_detections'))
        output_dict = {key: value[0, :num_detections].numpy()
                       for key, value in output_dict.items()}
        output_dict['num_detections'] = num_detections

        # detection_classes should be ints.
        output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)

        # Handle models with masks:
        if 'detection_masks' in output_dict:
            # Reframe the the bbox mask to the image size.
            detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
                output_dict['detection_masks'], output_dict['detection_boxes'],
                image.shape[0], image.shape[1])
            detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,
                                               tf.uint8)
            output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()

        return output_dict

.....

image_size = list([720, 1280])
tf_engine = TFEngine(model_name)
output_dict = tf_engine.DetectWithImage(frame)
obj_img = append_objs_to_img(frame, output_dict, category_index)
```

I get the error:
`    return self._signatures[key]
KeyError: 'serving_default'
`

I think the model is not in savedModel format so I try to convert it into savedModel :

`python ~/dev/tensorflow/tensorflow/python/tools/saved_model_cli.py convert --dir /home/xavier/Downloads/ssd4tsd_full/ --output_dir /home/xavier/Downloads/ssd4tsd_full/saved_model/ --tag_set serve --signature_def serving_default
usage: saved_model_cli.py convert [-h] --dir DIR --output_dir OUTPUT_DIR
                                  --tag_set TAG_SET
                                  {tensorrt} ...
saved_model_cli.py convert: error: invalid choice: 'serving_default' (choose from 'tensorrt')
`

I don't know how to solve it.

"
35794,"Failed to allocate memory for tensor_info, 1320 bytes required","@tensorflow/micro

Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
NAME=""Ubuntu""
VERSION=""18.04.3 LTS""
ID=ubuntu
PRETTY_NAME=""Ubuntu 18.04.3 LTS""
VERSION_ID=""18.04""
Python versionL 2.7.15+
Target platform : K64F

**Describe the problem**
I have build a GRU model in python and converted to a bin format for ARM K64F board. When I flash the binary I get the following error message in the serial console.  How can I fix it? Any help is appreciated.

`Failed to allocate memory for tensor_info, 1320 bytes required
AllocateTensors() failed

++ MbedOS Fault Handler ++

FaultType: HardFault

Context:
R0   : 00000000
R1   : 00000000
R2   : 00000000
R3   : 20013FE8
R4   : 00000000
R5   : 00000000
R6   : 00000000
R7   : 20002368
R8   : 20002364
R9   : 00000000
R10  : 00000000
R11  : 00000000
R12  : 000000B0
SP   : 20013F10
LR   : 00008E89
PC   : 000010E4
xPSR : 61070000
PSP  : 20013EA8
MSP  : 2002FFC0
CPUID: 410FC241
HFSR : 40000000
MMFSR: 00000000
BFSR : 00000004
UFSR : 00000000
DFSR : 00000008
AFSR : 00000000
Mode : Thread
Priv : Privileged
Stack: PSP

-- MbedOS Fault Handler --



++ MbedOS Error Info ++
Error Status: 0x80FF013D Code: 317 Module: 255
Error Message: Fault exception
Location: 0x10E4
Error Value: 0x1FFF0400
Current Thread: main Id: 0x200013E8 Entry: 0x3F07 StackSize: 0x10000 StackMem: 0x20004010 SP: 0x20013F10
For more info, visit: https://mbed.com/s/error?error=0x80FF013D&tgt=K64F
-- MbedOS Error Info --

= System will be rebooted due to a fatal error =
= Reboot count(=7) reached maximum, system will halt after rebooting
`



"
35791,Errors from `cuda_dnn.cc(1802): 'cudnnRNNForwardTraining...` or `cuda/cuda_dnn.cc(1959): 'cudnnRNNBackwardWeights...`,"**System information**
- Have I written custom code: no
- OS Platform: Windows 10
- TensorFlow installed from: binary
- TensorFlow version: v2.1.0-rc2-17-ge5bf8de410 2.1.0
- Python 3.6.8
- CUDA/cuDNN version: cuda_10.2.89_441.22_win10/cudnn-10.2-windows10-x64-v7.6.5.32
- GPU model and memory: Asus GeForxe GTX 980 Strix 4GB, Compute Capability 5.2
- nVidia driver: 441.87

![image](https://user-images.githubusercontent.com/3266706/72216802-14aaf900-3526-11ea-9cbd-a78f74b9e0d5.png)

**Describe the current behavior**

I'm just learning to use tensorflow. I've adapted some code from tutorials but during training getting any of 2 following errors in random epoch. Rarely it finishes without error...

**Other info / logs**
1. Either:

```
2020-01-11 22:24:39.258245: E tensorflow/stream_executor/dnn.cc:596] CUDNN_STATUS_INTERNAL_ERROR
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1959): 'cudnnRNNBackwardWeights
( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), output_desc.handles(), output_data.opaque(), workspace.opaque(), workspace.size(), rnn_desc.params_handle(), params_backprop_data->opaque(), reserve_space_data->opaque(), reserve_space_data->size())'
2020-01-11 22:24:39.259004: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 256, 256, 1, 256, 32, 256] 
2020-01-11 22:24:39.259641: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 256, 256, 1, 256, 32, 256] 
	 [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
2020-01-11 22:24:39.260191: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_6848_7024_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_9522}} {{function_node __inference___backward_cudnn_lstm_with_fallback_6848_7024_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_9522}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 256, 256, 1, 256, 32, 256] 
	 [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]
	 [[StatefulPartitionedCall_1]]
2020-01-11 22:24:39.261943: I tensorflow/stream_executor/stream.cc:1990] [stream=000001EB81C23220,impl=000001EBF7EBD920] did not wait for [stream=000001EB81C231A0,impl=000001EBF7EBE3A0]
2020-01-11 22:24:39.262108: I tensorflow/stream_executor/stream.cc:4938] [stream=000001EB81C23220,impl=000001EBF7EBD920] did not memcpy host-to-device; source: 000001EB834B4180
2020-01-11 22:24:39.262270: E tensorflow/stream_executor/stream.cc:332] Error recording event in stream: Error recording CUDA event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.
2020-01-11 22:24:39.263052: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2020-01-11 22:24:39.263276: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1
```

2. Or:

```
2020-01-12 08:21:48.672516: E tensorflow/stream_executor/dnn.cc:596] CUDNN_STATUS_INTERNAL_ERROR
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1802): 'cudnnRNNForwardTraining( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.handles(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'
2020-01-12 08:21:48.674257: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2020-01-12 08:21:48.674423: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1
2020-01-12 08:
Process finished with exit code -1073740791 (0xC0000409)
```

**Already tried**
- updating nvidia drivers, reinstalling cuda SDK, python, tensorflow
- downgrading to tensorflow 1.7 + cuda_10.0.130_411.31_win10 + cudnn-10.0-windows10-x64-v7.4.1.5
- burning GPU for a while to check stability

**Similar errors for**
cudnn-10.1-windows10-x64-v7.6.5.32 / cuda_10.1.243_426.00_win10
```
2020-01-12 15:56:23.613001: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2020-01-12 15:56:23.613167: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1
2020
Process finished with exit code -1073740791 (0xC0000409)
```

**Describe the expected behavior**
tensorflow finishes training

**Code to reproduce the issue**
```
import datetime
import os

import pandas as pd
from numpy import reshape

import tensorflow as tf

EPOCHS = 500
BATCH_SIZE = 1000
TEST_SET_RATIO = 0.2

LEARNING_RATE = 0.001
DECAY = 5e-5
LOSS_FUNC = 'categorical_crossentropy'
DROPOUT = 0.2

L_AMOUNT = 2  # number of labels
RNN_SEQ_AMOUNT = 128  # number of RNN/LSTM sequence features
DNN_F_AMOUNT = 4  # number of DNN branch features

MIN_ACC_TO_SAVE_MODEL = 0.6


def create_model():
    # RNN
    rnn_input = tf.keras.layers.Input(shape=(RNN_SEQ_AMOUNT, 1))

    rnn = tf.keras.layers.LSTM(RNN_SEQ_AMOUNT, return_sequences=True)(rnn_input)
    rnn = tf.keras.layers.Dropout(DROPOUT)(rnn)
    rnn = tf.keras.layers.BatchNormalization()(rnn)

    rnn = tf.keras.layers.LSTM(RNN_SEQ_AMOUNT, return_sequences=True)(rnn)
    rnn = tf.keras.layers.Dropout(DROPOUT / 2)(rnn)
    rnn = tf.keras.layers.BatchNormalization()(rnn)

    rnn = tf.keras.layers.LSTM(RNN_SEQ_AMOUNT)(rnn)
    rnn = tf.keras.layers.Dropout(DROPOUT)(rnn)
    rnn = tf.keras.layers.BatchNormalization()(rnn)

    rnn_output = tf.keras.layers.Flatten()(rnn)

    # DNN
    dnn_input = tf.keras.layers.Input(shape=(DNN_F_AMOUNT,))

    dnn = tf.keras.layers.Dense(16, activation=tf.keras.activations.relu)(dnn_input)
    dnn = tf.keras.layers.Dense(8, activation=tf.keras.activations.relu)(dnn)

    dnn_output = tf.keras.layers.Dense(2, activation=tf.keras.activations.softmax)(dnn)

    # Concatenate above outputs
    combined_input = tf.keras.layers.Concatenate()([rnn_output, dnn_output])

    combined_nn = tf.keras.layers.Dense(128, activation=tf.keras.activations.relu)(combined_input)
    combined_nn = tf.keras.layers.Dense(64, activation=tf.keras.activations.relu)(combined_nn)

    combined_output = tf.keras.layers.Dense(units=L_AMOUNT, activation=tf.keras.activations.softmax)(combined_nn)

    # Model setup
    new_model = tf.keras.Model(inputs=[rnn_input, dnn_input], outputs=[combined_output])
    opt = tf.keras.optimizers.Adam(LEARNING_RATE,
                                   decay=DECAY)
    new_model.compile(optimizer=opt,
                      loss=LOSS_FUNC,
                      metrics=['accuracy'])

    print(new_model.summary())
    return new_model


class CustomModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):
    def __init__(self, fp, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch', **kwargs):
        super().__init__(fp, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, **kwargs)

    def on_epoch_end(self, epoch, logs=None):
        print(""\n-------------------------------------------------------------------------------------------------------"")
        print(f""epoch: {epoch}, training_acc: {round(float(logs['accuracy']), 4)}, validation_acc: {round(float(logs['val_accuracy']), 4)}"")
        print(""-------------------------------------------------------------------------------------------------------\n"")

        if MIN_ACC_TO_SAVE_MODEL <= logs['accuracy']:
            super().on_epoch_end(epoch, logs)


if __name__ == '__main__':
    data_filename = 'train_2020-01-05_data.csv'
    print(""Loading data file: %s"" % data_filename)

    dataset = pd.read_csv('e:\\ml\\data\\training\\faulty\\%s' % data_filename, delimiter=',', header=None)
    dataset = dataset.drop(columns=[0, 1, 2, 3, 4, 5, 6]).values  # drop columns with additional information

    test_set_size = int(len(dataset) * TEST_SET_RATIO)
    print(""Test set split at: %d"" % test_set_size)

    train_data = dataset[:-test_set_size]
    test_data = dataset[-test_set_size:]  # use most recent data for testing

    rnn_tr_f = train_data[:, 0:RNN_SEQ_AMOUNT]
    rnn_ts_f = test_data[:, 0:RNN_SEQ_AMOUNT]

    combined_tr_l = train_data[:, RNN_SEQ_AMOUNT:RNN_SEQ_AMOUNT + L_AMOUNT]
    combined_ts_l = test_data[:, RNN_SEQ_AMOUNT:RNN_SEQ_AMOUNT + L_AMOUNT]

    dnn_tr_f = train_data[:, RNN_SEQ_AMOUNT + L_AMOUNT:]  # enhancement data is added in column after labels
    dnn_ts_f = test_data[:, RNN_SEQ_AMOUNT + L_AMOUNT:]

    rnn_tr_features = reshape(rnn_tr_f, (len(rnn_tr_f), RNN_SEQ_AMOUNT, 1))
    rnn_ts_features = reshape(rnn_ts_f, (len(rnn_ts_f), RNN_SEQ_AMOUNT, 1))

    # create model for combined RNN and DNN
    model = create_model()

    TRAINING_TIMESTAMP = datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
    model_name = ""faulty_%s"" % TRAINING_TIMESTAMP
    os.mkdir(""e:\\ml\\models\\%s"" % model_name)
    filepath = ""e:\\ml\\models\\%s\\%s--{epoch:02d}-{val_accuracy:.3f}.model"" % (model_name, model_name)
    checkpoint = CustomModelCheckpoint(filepath,
                                       monitor='val_accuracy',
                                       verbose=1,
                                       save_best_only=True,
                                       mode='max')

    log_dir = ""e:\\ml\\logs\\fit\\"" + model_name + "".model""
    tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)

    model.fit(x=[rnn_tr_features, dnn_tr_f],
              y=combined_tr_l,
              epochs=EPOCHS,
              shuffle=True,
              batch_size=BATCH_SIZE,
              validation_data=([rnn_ts_features, dnn_ts_f], combined_ts_l),
              callbacks=[checkpoint, tensorboard])
```

**DATA**

[train_2020-01-05_data.zip](https://github.com/tensorflow/tensorflow/files/4050437/train_2020-01-05_data.zip)

**Installed packages**

```
C:\WINDOWS\system32>pip list
Package              Version
-------------------- ----------
absl-py              0.9.0
astor                0.8.1
attrs                19.3.0
backcall             0.1.0
bleach               3.1.0
cachetools           4.0.0
certifi              2019.11.28
chardet              3.0.4
colorama             0.4.3
cycler               0.10.0
decorator            4.4.1
defusedxml           0.6.0
entrypoints          0.3
gast                 0.2.2
google-auth          1.10.0
google-auth-oauthlib 0.4.1
google-pasta         0.1.8
grpcio               1.26.0
h5py                 2.10.0
idna                 2.8
importlib-metadata   1.3.0
ipykernel            5.1.3
ipython              7.11.1
ipython-genutils     0.2.0
jedi                 0.15.2
Jinja2               2.10.3
json5                0.8.5
jsonschema           3.2.0
jupyter-client       5.3.4
jupyter-core         4.6.1
jupyterlab           1.2.4
jupyterlab-server    1.0.6
Keras-Applications   1.0.8
Keras-Preprocessing  1.1.0
kiwisolver           1.1.0
Markdown             3.1.1
MarkupSafe           1.1.1
matplotlib           3.1.2
mistune              0.8.4
more-itertools       8.0.2
nbconvert            5.6.1
nbformat             4.4.0
notebook             6.0.2
numpy                1.18.0
oauthlib             3.1.0
opt-einsum           3.1.0
pandas               0.25.3
pandocfilters        1.4.2
parso                0.5.2
patsy                0.5.1
pickleshare          0.7.5
pip                  19.3.1
prometheus-client    0.7.1
prompt-toolkit       3.0.2
protobuf             3.11.2
pyasn1               0.4.8
pyasn1-modules       0.2.8
Pygments             2.5.2
pyparsing            2.4.6
pyrsistent           0.15.6
python-dateutil      2.8.1
pytz                 2019.3
pywin32              227
pywinpty             0.5.7
pyzmq                18.1.1
requests             2.22.0
requests-oauthlib    1.3.0
rsa                  4.0
scipy                1.4.1
Send2Trash           1.5.0
setuptools           41.0.0
six                  1.13.0
statsmodels          0.10.2
tensorboard          2.1.0
tensorflow           2.1.0
tensorflow-estimator 2.1.0
termcolor            1.1.0
terminado            0.8.3
testpath             0.4.4
tornado              6.0.3
traitlets            4.3.3
urllib3              1.25.7
wcwidth              0.1.7
webencodings         0.5.1
Werkzeug             0.16.0
wheel                0.33.6
wrapt                1.11.2
zipp                 0.6.0
```"
35789,DLL load failed even though I have everything installed and I only upgraded tensorflow to 2.10,"Hello I am using tensorflow gpu version. I had tf 2.0 gpu version but tensorboard was giving me some trouble. It was using tf1.4 tensorboard. So I uninstalled tf1.4 and tensorboard1.4 and then upgraded tf2.0 gpu version to tf2.1 gpu version but when I do it it gives me some problems. Here is my system information.
**System information**
- OS - Windows 10
- TensorFlow version: 2.1(after upgrading)
- Python version:3.6.9
- I am using conda environment which I have named tensorflow_gpuenv
- CUDA version: 10.0.130
-CUDNN version: 7.6.0
- GPU model: NVIDIA 1080 TI
So now I have been using gpu version of tensorflow since a long time. I have everything installed and I also have vs if you are wondering I do not have that and vs 2017 more specifically. And by the way my tf 1 was installed using conda and tf 2 using pip and then tf 2.1 also using pip. So know when I 
```py
import tensorflow
```
it gives me the following error - 

```py
Traceback (most recent call last):
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Tejas\Anaconda3\envs\tensorflow_gpuenv\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.
```
"
35788,Tensors are getting passes to metric functions when they want an array,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Through Pycharm PIP
- TensorFlow version (use command below): 2.0
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.2
- GPU model and memory:  GTX 1060 6GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

When i pass my numpy array data to fit() i see that at some point tensors are pass to update_state() in the metrics class when they need to be arrays.

**Describe the expected behavior**

Maybe tensorflow is not converting these to arrays first?

**Code to reproduce the issue**
Again, not sure

**Other info / logs**
types being passed to fit() <class 'numpy.ndarray'>
types being passed to update_state() <class 'tensorflow.python.framework.ops.Tensor'>
Traceback (most recent call last):
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\sklearn\utils\validation.py"", line 159, in _num_samples
    return len(x)
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\framework\ops.py"", line 741, in __len__
    ""shape information."".format(self.name))
TypeError: len is not well defined for symbolic Tensors. (output_1_target:0) Please call `x.shape` rather than `len(x)` for shape information.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/Ben/PycharmProjects/trader/trainer-crypto.py"", line 17, in <module>
    tune(executions_per_trial, data_params, 'test2')
  File ""C:\Users\Ben\PycharmProjects\trader\HPtuner.py"", line 134, in tune
    tuner.search(data, num_executions, val_baseline)
  File ""C:\Users\Ben\AppData\Local\Programs\Python\Python36\lib\site-packages\kerastuner\engine\base_tuner.py"", line 78, in search
    self.run_trial(trial, *fit_args, **fit_kwargs)
  File ""C:\Users\Ben\PycharmProjects\trader\HPtuner.py"", line 100, in run_trial
    history = train(model, data['X_train'], data['y_train'], min_delta, patience, batch_size, X_test=data['X_test'], y_test=data['y_test'])[1]
  File ""C:\Users\Ben\PycharmProjects\trader\HPtuner.py"", line 87, in train
    verbose=0)])
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\keras\engine\training.py"", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 224, in fit
    distribution_strategy=strategy)
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 547, in _process_training_inputs
    use_multiprocessing=use_multiprocessing)
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 594, in _process_inputs
    steps=steps)
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2433, in _standardize_user_data
    self._compile_from_inputs(all_inputs, y_input, x, y)
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2668, in _compile_from_inputs
    experimental_run_tf_function=self._experimental_run_tf_function)
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\training\tracking\base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\keras\engine\training.py"", line 366, in compile
    masks=self._prepare_output_masks())
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2063, in _handle_metrics
    target, output, output_mask))
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2014, in _handle_per_output_metrics
    metric_fn, y_true, y_pred, weights=weights, mask=mask)
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\keras\engine\training_utils.py"", line 1067, in call_metric_function
    return metric_fn(y_true, y_pred, sample_weight=weights)
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\keras\metrics.py"", line 193, in __call__
    replica_local_fn, *args, **kwargs)
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\keras\distribute\distributed_training_utils.py"", line 1135, in call_replica_local_fn
    return fn(*args, **kwargs)
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\keras\metrics.py"", line 176, in replica_local_fn
    update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\keras\utils\metrics_utils.py"", line 75, in decorated
    update_op = update_state_fn(*args, **kwargs)
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\python\keras\metrics.py"", line 582, in update_state
    matches = self._fn(y_true, y_pred, **self._fn_kwargs)
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\sklearn\metrics\_regression.py"", line 178, in mean_absolute_error
    y_true, y_pred, multioutput)
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\sklearn\metrics\_regression.py"", line 84, in _check_reg_targets
    check_consistent_length(y_true, y_pred)
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\sklearn\utils\validation.py"", line 208, in check_consistent_length
    lengths = [_num_samples(X) for X in arrays if X is not None]
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\sklearn\utils\validation.py"", line 208, in <listcomp>
    lengths = [_num_samples(X) for X in arrays if X is not None]
  File ""C:\Users\Ben\AppData\Roaming\Python\Python36\site-packages\sklearn\utils\validation.py"", line 161, in _num_samples
    raise TypeError(message)
TypeError: Expected sequence or array-like, got <class 'tensorflow.python.framework.ops.Tensor'>
"
35787,[GPU Backend] Tensorflow Task Graph,"I am wondering about the tensorflow task/op graph. Is scheduling done ahead of time? If I am using an accelerator like a GPU using CUDA, how does the tensorflow runtime synchronise data dependencies? Does it use CUDA streams or something more sophisticated?"
35785,TPU InternalError with TF 2.1.0 on Google Colab (Assigned device does not have registered OpKernel support for _Arg node iteratorgetnext_iterator),"**System information**
- Custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow installed from (source or binary): use of `%tensorflow_version 2.x`
- TensorFlow version (use command below): 2.1.0-rc1
- Python version: 3.6.9

**Describe the current behavior**

I tried to run @huan's Google Colab Notebook mentioned [here](https://stackoverflow.com/questions/55541881/how-to-convert-tf-keras-model-to-tpu-using-tensorflow-2-0-in-google-colab/55686370#55686370) and available [here](https://colab.research.google.com/github/huan/tensorflow-handbook-tpu/blob/master/tensorflow-handbook-tpu-example.ipynb). It raises the following error:

```
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-7-79d308ea228d> in <module>()
      4   steps_per_epoch=60,
      5   validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)),
----> 6   validation_freq=5
      7 )
      8 

11 frames
/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InternalError: Assigned device '/job:worker/replica:0/task:0/device:TPU:0' does not have registered OpKernel support for _Arg
	 [[{{node iteratorgetnext_iterator}}]] [Op:__inference_distributed_function_2822]
```

I tried `tf.compat.v1.disable_eager_execution()` as discussed [here](https://github.com/huan/tensorflow-handbook-tpu/issues/1) but it doesn't work:

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-15-5118e7c1b79a> in <module>()
      6   steps_per_epoch=60,
      7   validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)),
----> 8   validation_freq=5
      9 )
     10 

25 frames
/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
   1278       graph = get_default_graph()
   1279       if not graph.building_function:
-> 1280         raise RuntimeError(""Attempting to capture an EagerTensor without ""
   1281                            ""building a function."")
   1282       return graph.capture(value, name=name)

RuntimeError: Attempting to capture an EagerTensor without building a function.
```

**Describe the expected behavior**

Not sure if @huan's example should be working given what he explained on [stackoverflow](https://stackoverflow.com/questions/55541881/how-to-convert-tf-keras-model-to-tpu-using-tensorflow-2-0-in-google-colab/55686370#55686370), but I think it should with TF 2.1.0 version.

**Code to reproduce the issue**
Again, @huan's work available [here](https://colab.research.google.com/github/huan/tensorflow-handbook-tpu/blob/master/tensorflow-handbook-tpu-example.ipynb).

**Other info / logs**
I had the same issue with personal code and thought first it was due to `ImageDataGenerator` (see my last comment [here](https://github.com/tensorflow/tensorflow/issues/34346)), however it seems not to be as @huan's example doesn't use it.
I have the same issue on @huan's code with TF 2.1.0 installed with `!pip install tensorflow==2.1.0` in his notebook."
35784,The tflite only utilize single core of CPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu18.04 and Raspbian 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
Raspberry Pi 4
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
for Ubuntu            v1.14.0-rc1-22-gaf24dc9 1.14.0
for Raspberry Pi     v1.12.1-14948-g43dcb71 1.14.0
- Python version:
3.7

**Describe the current behavior**
The tflite only utilize single core of CPU. I test it on PC and raspberry Pi 4.

**Code to reproduce the issue**
`interpreter = tf.lite.Interpreter(model_content=tflite_model) 

input_details = interpreter.get_input_details() 

output_details = interpreter.get_output_details() 

interpreter.allocate_tensors() 

interpreter.set_tensor(input_details[0]['index'], inp) 

interpreter.invoke() 

result = interpreter.get_tensor(output_details[0]['index'])`

The model I use is here:
[1578756226.tar.gz](https://github.com/tensorflow/tensorflow/files/4050550/1578756226.tar.gz)
"
35783,model._function_kwargs is silently ignored,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): see below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows, Linux
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.6

**Describe the current behavior**
In this example, no error is raised although session kwargs are not supported in eager mode. kwargs are simply silently ignored.
```
import tensorflow as tf
from tensorflow import keras

fetches = [lambda: whatever_I_write_here_is_ignored]

var = tf.Variable([[3.0]])
model = keras.models.Sequential([keras.layers.Dense(1, input_shape=(1,))])
model.compile(loss=""mse"", optimizer=""adam"")
model._function_kwargs = {""fetches"": fetches, ""should_fail"": ""ignored_as_well""}

model.fit([[7.0]], [[9.0]], epochs=2)
```

**Describe the expected behavior**
An error should be raised. I am aware that `model._function_kwargs` is not part of the public API, but `keras` (as opposed to `tf.keras`) does raise an error here:
```
import keras
import tensorflow as tf

fetches = [lambda: whatever_I_write_here_is_ignored]

var = tf.Variable([[3.0]])
model = keras.models.Sequential([keras.layers.Dense(1, input_shape=(1,))])
model.compile(loss=""mse"", optimizer=""adam"")
model._function_kwargs = {""fetches"": fetches, ""should_fail"": ""ignored_as_well""}

model.fit([[7.0]], [[9.0]], epochs=2)
```

outputs
```
Exception has occurred: ValueError
Session keyword arguments are not support during eager execution. You passed: {'fetches': [<function <lambda> at 0x7f94681be3b0>], 'should_fail': 'ignored_as_well'}
  File ""/home/bersbersbers/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py"", line 3759, in function
```
Somewhat related: https://github.com/tensorflow/tensorflow/issues/34448"
35781,tf.Variable.assign fails silently,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): see below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Linux
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.6


**Describe the current behavior**
A special assign operation does nothing (not even raising an error).

**Describe the expected behavior**
At least an error should be raised.

**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow import keras

# tf.config.experimental_run_functions_eagerly(True)
var = tf.Variable([[3.0]])
model = keras.Sequential([keras.layers.Dense(1, input_shape=(1,))])
model.compile(loss=""mse"", optimizer=""adam"")

tf.print(var)  # should print 3, OK
var.assign(model.inputs[0])
tf.print(var)  # should print anything else but 3, or raise an error - but prints 3
```

**Other info / logs**
```
>>> tf.print(var)  # should print 3, OK
[[3]]
>>> var.assign(model.inputs[0])
<tf.Variable 'UnreadVariable' shape=(1, 1) dtype=float32, numpy=array([[3.]], dtype=float32)>
>>> tf.print(var)  # should print anything else but 3, or raise an error - fail.
[[3]]
```"
35777, Windows fatal exception: access violation ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows10
- TensorFlow installed from (source or binary):pip
- TensorFlow version (use command below):2.1
- Python version:3.7.6
- CUDA/cuDNN version:10.1,7.6
- GPU model and memory:GTX1060,6g

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
2020-01-12 02:31:50.290624: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
Windows fatal exception: access violation

Current thread 0x00002d50 (most recent call first):
  File ""D:\Anaconda3\envs\tf21\lib\site-packages\tensorflow_core\python\lib\io\file_io.py"", line 84 in _preread_check
  File ""D:\Anaconda3\envs\tf21\lib\site-packages\tensorflow_core\python\lib\io\file_io.py"", line 122 in read
  File ""D:\Anaconda3\envs\tf21\lib\json\__init__.py"", line 293 in load
  File ""E:\tensorflow_natural_question\official\nlp\bert\squad_lib.py"", line 146 in read_squad_examples
  File ""E:\tensorflow_natural_question\official\nlp\bert\squad_lib.py"", line 836 in generate_tf_record_from_json_file
  File ""create_finetuning_data.py"", line 152 in generate_squad_dataset
  File ""create_finetuning_data.py"", line 175 in main
  File ""D:\Anaconda3\envs\tf21\lib\site-packages\absl\app.py"", line 250 in _run_main
  File ""D:\Anaconda3\envs\tf21\lib\site-packages\absl\app.py"", line 299 in run
  File ""create_finetuning_data.py"", line 184 in <module>

**Describe the expected behavior**
no bug
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35774,Installation from Docker instructions have wrong argument in workspace flag,"I'm trying to [build tensorflow from source](https://www.tensorflow.org/install/source) following the Docker instructions:
>docker pull tensorflow/tensorflow:devel

>docker run -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS=""$(id -u):$(id -g)"" \
    tensorflow/tensorflow:devel bash
>
>git pull  # within the container, download the latest source code

Here are the commands I run in the terminal (on Ubuntu), along with their output:
```
$ docker --version
Docker version 19.03.2, build 6a30dfc

$ docker pull tensorflow/tensorflow:devel
devel: Pulling from tensorflow/tensorflow
Digest: sha256:0ee065743f0001f922561bcba914013929a88263ec2a5af21ba35899c3ac85a7
Status: Image is up to date for tensorflow/tensorflow:devel
docker.io/tensorflow/tensorflow:devel

$ docker run -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS=""$(id -u):$(id -g)"" \
>     tensorflow/tensorflow:devel bash

________                               _______________                
___  __/__________________________________  ____/__  /________      __
__  /  _  _ \_  __ \_  ___/  __ \_  ___/_  /_   __  /_  __ \_ | /| / /
_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / 
/_/    \___//_/ /_//____/ \____//_/    /_/      /_/  \____/____/|__/


WARNING: You are running this container as root, which can cause new files in
mounted volumes to be created as the root user on your host machine.

To avoid this, run the container by specifying your user's userid:

$ docker run -u $(id -u):$(id -g) args...

root@4746a002f18e:/tensorflow# 

```

But now, if I run `git pull` as instructed, I get
```
fatal: not a git repository (or any of the parent directories): .git
```"
35773,ImportError: DLL load failed: The specified module could not be found,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information** 64 bit operating system
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home Single Language
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):not sure
- TensorFlow version:
- Python version:3.7.6
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

conda create -n tensorflow python=3.7
activate tensorflow
pip install --ignore-installed --upgrade tensorflow
python
import tensorflow as tf

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Traceback (most recent call last):
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Asus Pc\AppData\Local\conda\conda\envs\tensorflow_cpu\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors
"
35772,Tensorflow import issue,"error code

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\USER\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\USER\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Users\USER\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\core\framework\graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""C:\Users\USER\AppData\Local\Programs\Python\Python36\lib\site-packages\google\protobuf\descriptor.py"", line 47, in <module>
    from google.protobuf.pyext import _message
ImportError: DLL load failed: The specified procedure could not be found.


python version 3.6
tensorflow version 1.5(installed via pip)
visual studio c++ installed
using a 64 bit machine"
35771,"An exception has occurred, use %tb to see the full traceback.  SystemExit: 2","This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
35770,ERROR: ~/....../tensorflow/python/keras/api/BUILD:130:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed,"
bazel 1.1.0

```console
➜  tensorflow git:(master) ✗ bazel version
Build label: 1.1.0- (@non-git)
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Wed. Jan. 8 01:20:59 2020 (1578446459)
Build timestamp: 1578446459
Build timestamp as int: 1578446459
```

failed to build tensorflow master.
```console
INFO: From Compiling tensorflow/lite/toco/graph_transformations/propagate_array_data_types.cc [for host]:
tensorflow/lite/toco/graph_transformations/propagate_array_data_types.cc: In member function 'virtual tensorflow::Status toco::PropagateArrayDataTypes::Run(toco::Model*, std::size_t, bool*)':
tensorflow/lite/toco/graph_transformations/propagate_array_data_types.cc:173:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
       for (int i = 0; i < op->outputs.size(); ++i) {
                       ~~^~~~~~~~~~~~~~~~~~~~
ERROR: ~/....../tensorflow/python/keras/api/BUILD:130:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)
Traceback (most recent call last):
  File ""~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 776, in <module>
    main()
  File ""~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 772, in main
    lazy_loading, args.use_relative_imports)
  File ""~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 629, in create_api_files
    compat_api_versions, lazy_loading, use_relative_imports)
  File ""~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 503, in get_api_init_text
    _, attr = tf_decorator.unwrap(attr)
  File ""~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py"", line 219, in unwrap
    elif _has_tf_decorator_attr(cur):
  File ""~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py"", line 124, in _has_tf_decorator_attr
    hasattr(obj, '_tf_decorator') and
  File ""~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py"", line 62, in __getattr__
    module = self._load()
  File ""~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py"", line 45, in _load
    module = importlib.import_module(self.__name__)
  File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py"", line 28, in <module>
    _wrap_py_utils = swig_import_helper()
  File ""~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_wrap_py_utils', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
  File ""<frozen importlib._bootstrap>"", line 684, in _load
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: ~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: ~/....../tensorflow/python/tools/BUILD:81:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)
INFO: Elapsed time: 10740.244s, Critical Path: 307.06s
INFO: 15793 processes: 15793 local.
FAILED: Build did NOT complete successfully
```"
35769,No module named 'tensorflow.python.keras.applications.ResNet50',
35767,tensorflow-gpu not working,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, 64 bit
- TensorFlow installed from (source or binary): source
- TensorFlow version: tensorflow-gpu==2.1.0
- Python version: Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)] on win32
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: CUDA: 10.1
- GPU model and memory: GeForce GTX 1060 with Max-Q, Memory: 8GB



**Describe the problem**


Tensorflow is not able to get imported properly.
the program is simple:
```
import tensorflow as tf

hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello))
```



Tensorflow returns following error:
```
""D:\Multiverse_Projects\Python Environment\Scripts\python.exe"" ""D:\Apps\JetBrains\PyCharm Community Edition 2019.1.3\helpers\pydev\pydevd.py"" --multiproc --qt-support=auto --client 127.0.0.1 --port 50936 --file ""D:/Multiverse_Projects/Final year project- Fall Detection/fall-detection-two-stream-cnn-master/tensorflow_test.py""
pydev debugger: process 4236 is connecting

Connected to pydev debugger (build 191.7479.30)
Traceback (most recent call last):
  File ""D:\Multiverse_Projects\Python Environment\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Multiverse_Projects\Python Environment\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Multiverse_Projects\Python Environment\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\prahu\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\prahu\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Apps\JetBrains\PyCharm Community Edition 2019.1.3\helpers\pydev\pydevd.py"", line 1758, in <module>
    main()
  File ""D:\Apps\JetBrains\PyCharm Community Edition 2019.1.3\helpers\pydev\pydevd.py"", line 1752, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""D:\Apps\JetBrains\PyCharm Community Edition 2019.1.3\helpers\pydev\pydevd.py"", line 1147, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""D:\Apps\JetBrains\PyCharm Community Edition 2019.1.3\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""D:/Multiverse_Projects/Final year project- Fall Detection/fall-detection-two-stream-cnn-master/tensorflow_test.py"", line 1, in <module>
    import tensorflow as tf
  File ""D:\Multiverse_Projects\Python Environment\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""D:\Multiverse_Projects\Python Environment\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""D:\Multiverse_Projects\Python Environment\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""D:\Multiverse_Projects\Python Environment\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\prahu\AppData\Local\Programs\Python\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""D:\Multiverse_Projects\Python Environment\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Multiverse_Projects\Python Environment\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\Multiverse_Projects\Python Environment\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Multiverse_Projects\Python Environment\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\Multiverse_Projects\Python Environment\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\prahu\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\prahu\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Process finished with exit code -1

```


I have set all the environment variables.
One issue I faces during the installation was that the cudnn-10.1 zip downloaded from the [Nvidia website](https://developer.nvidia.com/cudnn) seems to be corrupt, no matter what zip software I use.

Although, I was able to unzip the required files (not sure if they are not broken), and follow this process: https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#install-windows

I've tried multiple TensorFlow versions including the CPU only ones. Same issue in each.

"
35766,tensorflow-gpu Importerror: DLL load failed: DLL initialization routine failed.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 pro 1909
- TensorFlow installed from (source or binary): https://files.pythonhosted.org/packages/
- TensorFlow version:1.8.0
- Python version:3.6.8
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version:CUDA 9.0/cuDNN7.0(and cuDNN7.1)
- GPU model and memory:GTX1050 (4GB)



**Describe the problem**

Import tensorflow error ,it reads  DLL initialization routine failed,i have changed  vc++ 2015  runtime libs(both x64 and x86) and using Nvidia display driver version 385.54(CUDA9.0 default)

If I use tensorflow-gpu 2.1.0 and import tensorflow, python will crash immediately, but if it is 1.8.0 or 1.7.0, python will run for a few seconds or so, and then the above error occurs

**Any other info / logs**


**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
Traceback (most recent call last):
File ""C:\Users\Hasee\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
from tensorflow.python.pywrap_tensorflow_internal import *
File ""C:\Users\Hasee\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in <module>  
_pywrap_tensorflow_internal = swig_import_helper()
File ""C:\Users\Hasee\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in swig_import_helper
return importlib.import_module(mname)
File ""C:\Users\Hasee\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
Importerror: DLL load failed: DLL initialization routine failed.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File ""f:/DL/1 hello world/index.py"", line 1, in <module>
import tensorflow as tf
File ""C:\Users\Hasee\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
File ""C:\Users\Hasee\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
from tensorflow.python import pywrap_tensorflow
File ""C:\Users\Hasee\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
raise ImportError(msg)
ImportError: Traceback (most recent call last):
File ""C:\Users\Hasee\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
from tensorflow.python.pywrap_tensorflow_internal import *
File ""C:\Users\Hasee\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in <module>  
_pywrap_tensorflow_internal = swig_import_helper()
File ""C:\Users\Hasee\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in swig_import_helper
return importlib.import_module(mname)
File ""C:\Users\Hasee\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
Importerror: DLL load failed: DLL initialization routine failed.
Failed to load the native TensorFlow runtime.
 DLL initialization routine failed
```"
35765,Autograph failure with `\`,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Nvidia RTX 2080

**Describe the current behavior**
Tensorflow shows warning about  failure of autograph
```
WARNING:tensorflow:AutoGraph could not transform <bound method C.f of <__main__.C object at 0x7f4a83904668>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7f4a47e0d5c0>, <gast.gast.Return object at 0x7f4a47e0df28>]
```
The warning seems to be caused by the backslash `""\""`. 

**Describe the expected behavior**
There should be no such warning

**Code to reproduce the issue**
```python
import tensorflow as tf

class C(object):
    def f(self):
        # error disappear if \ in the following line is removed
        a = \
            1
        return a

obj = C()

@tf.function
def func():
    mem =  obj.f()
    return mem

def main():
    print(func())

if __name__ == ""__main__"":
    main()
```
"
35763,A Subclass of Custom Loss Function Is not Trackable as an Argument in Checkpoint,"Tf Version: 2.0.0
Python: 3.7.5
IDE: Spyder
OS: Windows

I wrote a subclass of custom loss function and tried to list it in the arguments of a checkpoint as follows. The model is an arbitrary one
```
class SoftDiceLoss(tf.keras.losses.Loss):
    '''
    SoftDiceLoss calculates multi-class soft dice loss
    loss = avg_batch(1-(sum(W_k*sum(yPred.*yTrue)))/(sum(W_ksum(yPred^2+yTrue^2))))
    where W_k = 1/(number of voxels in class k)^wPow
    Class number of segmented regions includes background
    Args:
        yPred/yTrue: prediced and desired outputs shaped as [mbSize, classNum, tensor dimensions]. Also, both must be float-point
    	wPow, power of weiight. A higher one favours classes with a smaller number of voxels
    Return:
        loss: a scalar tensor
    '''
    def __init__(self, wPow=2.0, name='SoftDiceLoss'):
        super().__init__(name=name)
        self.epsilon = 1e-16 
        self.wPow = wPow

    def call(self, yPred, yTrue):
        yTrue =tf.dtypes.cast(yTrue, dtype=yPred.dtype)
		# Dot product yPred and yTrue and sum them up for each datum and class
        crossProd=tf.multiply(yPred, yTrue)
		# As a symbolic tensor, dimensions and shapes etc. cannot be extracted from data, nor can it be used in subroutines.
        crossProdSum=tf.math.reduce_sum(crossProd, axis=np.arange(2, 5)) #tf.rank(yTrue)))
		# Calculate weight for each datum and class 
        weight = tf.math.reduce_sum(yTrue, axis=np.arange(2, 5))#tf.rank(yTrue)))
		#weight = tf.math.divide(1, tf.math.square(weight)+self.epsilon)
        weight = tf.math.divide(1, tf.math.pow(weight, self.wPow)+self.epsilon)
		# Weighted sum over classes
        numerator = 2*tf.math.reduce_sum(tf.multiply(crossProdSum, weight), axis=1)
		# Saquared summation 
        yySum = tf.math.reduce_sum(tf.math.square(yPred) + tf.math.square(yTrue), axis=np.arange(2, 5))#tf.rank(yTrue)))
		# Weighted sum over classes
        denominator = tf.math.reduce_sum(tf.multiply(weight, yySum), axis=1)
		# Get individual loss and average over minibatch
        loss = tf.math.reduce_mean(1 - tf.math.divide(numerator, denominator+self.epsilon))
			
        return loss
    
    def get_config(self):
        config = super(SoftDiceLoss, self).get_config()
        return config

curOpt = tf.keras.optimizers.Adam(learning_rate=1e-4)	
lossFunc=SoftDiceLoss(2.0)
ckpt = tf.train.Checkpoint(model=myModel(...), optimizer=curOpt, lossFunc=lossFunc, accFunc=accFunc)

```

I got the following error
```
ckpt = tf.train.Checkpoint(model=myModel(...), optimizer=curOpt, lossFunc=lossFunc, accFunc=accFunc)
Traceback (most recent call last):

  File ""<ipython-input-17-a4d9163bdda3>"", line 2, in <module>
    optimizer=curOpt, lossFunc=lossFunc, accFunc=accFunc)

  File ""D:\TProgramFiles\Anaconda3\envs\keras-gpu\lib\site-packages\tensorflow_core\python\training\tracking\util.py"", line 1779, in __init__
    % (v,))

ValueError: `Checkpoint` was expecting a trackable object (an object derived from `TrackableBase`), got <SoftDiceLoss object at 0x0000000011F6F7C8>. If you believe this object should be trackable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue.
```
I am not sure if a subclass is trackable or not. Is that an intention of design? Or shall such a feature be added? 
By the way, if I change the subclass into a normal function, it works fine. Actually, my accFunc(...) is just a normal function."
35762,_ = dataset.cache() accelerates data pipeline,"**System information**
- Have I written custom code
- Linux Ubuntu 18.04
- TensorFlow ROCm installed from PyPI
- TensorFlow version: v2.0.0-rocm-3-g0826c3a 2.0.2
- Python version: Python 3.7.5
- CUDA/cuDNN version: None
- GPU model and memory: 2 x Radeon Vega 64

**Describe the current behavior**

TF.Data is quicker when one caches the whole dataset to an unused name.

-    With `dataset_*.cache(...)`:
    -   With `_= dataset.cache(...)`: 10870 samples/s stdev 27 samples/s
    -   Without `_= dataset.cache(...)`: 10563 samples/s, stdev 50 samples/s
    -   (10870 - 27) / (10563 + 50) = 1.02167153491
-    Without `dataset_*.cache(...)`:
    -   With `_= dataset.cache(...)`: 2902 samples/s stdev 14 samples/s
    -   Without `_= dataset.cache(...)`: 2732 samples/s, stdev 9 samples/s
    -   (10870 - 27) / (10563 + 50) = 1.05363006202

**Describe the expected behavior**

TF.Data is as quick as possible by default.

**Code to reproduce the issue**

My input data pipeline looks something like this: 

```python
data_frame_valid = pd.read_csv(...)
data_frame_invalid = pd.read_csv(...)
dataset_valid: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(...)
dataset_invalid: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(...)
dataset: tf.data.Dataset = dataset_valid.concatenate(dataset_invalid)
dataset = dataset.shuffle(...)
_ = dataset.cache()  # This speeds up iterating the data by 2 - 5 %
dataset_training = dataset.take(data_set_size_training)
dataset_testing = dataset.skip(data_set_size_training)
dataset_validation = dataset_testing.skip(data_set_size_validation)
dataset_testing = dataset_testing.take(data_set_size_testing)
# dataset_training = dataset_training.cache()  # ""_ speed up"" can be observed with and without
# dataset_validation = dataset_validation.cache()  # ""_ speed up"" can be observed with and without
# dataset_testing = dataset_testing.cache()  # ""_ speed up"" can be observed with and without
```

My benchmark:

```python
import statistics

import time
import tensorflow as tf

from pfasdr.neural.ze_discriminate_pd_np.get_data_sets_module import \
    get_data_sets
from pfasdr.neural.ze_discriminate_pd_np.path_templates_module import \
    valid_file_path_template, invalid_file_path_template


def benchmark_dataset(dataset, num_epochs=2):
    tf.print('Iterating data set ...')

    throughput_history = []
    for index_epoch in tf.data.Dataset.range(num_epochs):
        tf.print(f'Iterating for epoch {index_epoch}')
        index = 0

        # The actual benchmark
        tine_start = time.perf_counter()
        for index, _ in enumerate(dataset):
            pass
            # Uncomment for progress reporting
            # if not index % 100:
            #     print('\r' + str(index), end='')
        time_end = time.perf_counter()

        print('\r' + str(index))
        duration = time_end - tine_start
        throughput = index / duration

        tf.print(
            f'Iterating data set took: '
            f'{round(duration, 2)} s in epoch number {index_epoch}. '
            f'This makes for a throughput of {round(throughput)} 1/s'
        )

        if not index_epoch:
            # First round uses cold caches.
            # So do not record it.
            continue
        throughput_history.append(throughput)

    throughput_average = statistics.mean(throughput_history)
    throughput_deviation = statistics.stdev(throughput_history)
    throughput_upper = throughput_average + throughput_deviation
    throughput_lower = throughput_average - throughput_deviation
    tf.print(f'Average dataset entry throughput {round(throughput_average)} '
             f'with a variation of +/- {round(throughput_deviation)}, '
             f'which means {round(throughput_upper)} (+), '
             f'or {round(throughput_lower)} (-).')


def main():
    batch_size_training = 32
    node_count = 15
    valid_file_path = valid_file_path_template.substitute(
        length=node_count,
    )
    invalid_file_path = invalid_file_path_template.substitute(
        length=node_count,
    )

    dataset_training, dataset_validation, dataset_testing, \
        batch_size_training, dataset_size_validation, dataset_size_evaluate,\
        dataset_size_training \
        = get_data_sets(
            batch_size=batch_size_training,
            names=list(range(node_count)),
            invalid_file_path=invalid_file_path,
            valid_file_path=valid_file_path,
        )

    benchmark_dataset(dataset=dataset_training, num_epochs=5)


if __name__ == '__main__':
    main()
```"
35761,[TF2.1] k parameter is ignored in tf.linalg.diag_part,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit (also happens in WSL)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1
- Python version: 3.7
- CUDA/cuDNN version: 10.1 (also happens in tensorflow-cpu)
- GPU model and memory: GTX 1050 2GB

**Describe the current behavior**
Using the `k` parameter of `tf.linalg.diag_part` does not affect anything. The result is still the same.

**Describe the expected behavior**
It should select the super or subdiagonal dependent on `k`.

**Code to reproduce the issue**
```
import tensorflow as tf
import numpy as np
input = np.array([[[1, 2, 3, 4],  # Input shape: (2, 3, 4)
                   [5, 6, 7, 8],
                   [9, 8, 7, 6]],
                  [[5, 4, 3, 2],
                   [1, 2, 3, 4],
                   [5, 6, 7, 8]]])
# this works as expected
tf.linalg.diag_part(input) ==> [[1, 6, 7],  # Output shape: (2, 3)
                                [5, 2, 7]]
# this does not
tf.linalg.diag_part(input, k = 1) ==> [[1, 6, 7], [5, 2, 7]] # still returns the same output
  # [[2, 7, 6], [4, 3, 8]]  Is the expected output
```
This example is taken from the [documentation](https://www.tensorflow.org/api_docs/python/tf/linalg/diag_part?version=nightly), which is also incorrect see #35760."
35760,diag_part documentation outdated,"## URL(s) with the issue: 
https://www.tensorflow.org/api_docs/python/tf/linalg/diag_part?version=nightly

## Description of issue (what needs changing):
The `diag_part` documentation contains old examples of non-existing APIs, it uses `tf.matrix_diag_part` in the examples which does not exist.
"
35759,Error when running gmake for Tensorflow Lite Micro,"Hello. 
I have an issue similar to #29524.

I'm trying to generate a Keil project for the microcontroller I'm developing.
When I was following procedures in the following link, I got the error:

https://www.tensorflow.org/lite/microcontrollers/library

System informationOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.2 (19C57)
TensorFlow installed from (source or binary): source
TensorFlow version: e689e24
GCC/Compiler version (if compiling from source): Apple clang version 11.0.0 (clang-1100.0.33.16)

Describe the problem
When running$ gmake -f tensorflow/lite/experimental/micro/tools/Makefile generate_projects results in the following error:

gmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/gen/osx_x86_64/prj/micro_speech/make/tensorflow/lite/experimental/micro/examples/micro_speech/simple_features/simple_model_settings.h', needed by 'generate_micro_speech_make_project'.  Stop.
"
35756,Build error when building //tensorflow/compiler/aot:tfcompile,"when building the aot compiler with:
`bazel build --config=opt  -j 24 //tensorflow/compiler/aot:tfcompile`
the following error is triggered:
`ERROR: /root/.cache/bazel/_bazel_root/e7f9511d00737e63bb3d33a961e2b58a/external/llvm-project/llvm/BUILD:741:1: C++ compilation of rule '@llvm-project//llvm:aarch64_info' failed (Exit 1)
external/llvm-project/llvm/lib/Target/AArch64/TargetInfo/AArch64TargetInfo.cpp:9:10: fatal error: TargetInfo/AArch64TargetInfo.h: No such file or directory
 #include ""TargetInfo/AArch64TargetInfo.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
Target //tensorflow/compiler/aot:tfcompile failed to build`
the failing compilation is:
` (cd /root/.cache/bazel/_bazel_root/e7f9511d00737e63bb3d33a961e2b58a/execroot/org_tensorflow &&   exec env -     CUDA_TOOLKIT_PATH=/usr/local/cuda     GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7     LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64     PATH=/usr/local/nvm/versions/node/v13.3.0/bin:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/tensorrt/bin     PWD=/proc/self/cwd     PYTHON_BIN_PATH=/usr/bin/python     PYTHON_LIB_PATH=/usr/lib/python3.6/dist-packages     TF2_BEHAVIOR=1     TF_CONFIGURE_IOS=0     TF_CUDA_COMPUTE_CAPABILITIES=7.0     TF_ENABLE_XLA=1     TF_NEED_CUDA=1   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/aarch64_info/AArch64TargetInfo.d '-frandom-seed=bazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/aarch64_info/AArch64TargetInfo.o' -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -iquote external/llvm-project -iquote bazel-out/k8-opt/bin/external/llvm-project -iquote external/zlib_archive -iquote bazel-out/k8-opt/bin/external/zlib_archive -isystem external/llvm-project/llvm/include -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include -isystem external/zlib_archive -isystem bazel-out/k8-opt/bin/external/zlib_archive -isystem external/llvm-project/llvm/include/llvm/IR -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include/llvm/IR -isystem external/llvm-project/llvm/lib/IR -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/lib/IR -isystem external/llvm-project/llvm/lib/Transforms/InstCombine -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/lib/Transforms/InstCombine '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS '-march=native' -Wno-sign-compare '-std=c++14' -Iexternal/llvm/lib/Target/AArch64 -c external/llvm-project/llvm/lib/Target/AArch64/TargetInfo/AArch64TargetInfo.cpp -o bazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/aarch64_info/AArch64TargetInfo.o)`

This is because an incorrect include path: `-Iexternal/llvm/lib/Target/AArch64`
The include comes from the file `third_party/llvm/llvm.autogenerated.BUILD`
along with 90 other incorrect include paths to a non-existing directory. All paths of the form:
`    copts = llvm_copts + [""-Iexternal/llvm/lib/Target/<SomeTarget>""],`
should be:
`    copts = llvm_copts + [""-Iexternal/llvm-project/llvm/lib/Target/<SomeTarget>""],`
Since this is a generated file, I can not submit a PR with the proposed fix.

"
35754,Make Tensorflow 2.1 available through conda,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): conda
- Python version: 3.7

**Describe the problem**

`conda install tensorflow` installs version 2.0 instead of 2.1.
Please, make tensorflow 2.1 available through conda."
35750,TensorFlow 2.0 Unknown entries in loss dictionary when recovering saved model with tf.keras.models.load_model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): based on an official example, but modified
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): docker image `tensorflow/tensorflow:2.0.0-gpu-py3-jupyter`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): docker
- TensorFlow version (use command below): 2.0
- Python version: 3.6
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I have a `tf.keras` model initialized via the ""functional"" API, e.g.

```python
input = tf.keras.Input(...)
y = tf.keras.layers.Dense(...)(input)
...
logits = tf.keras.layers.Dense(..., name=""logit_layer"")(y)

model = tf.keras.models.Model(inputs, logits)
```

I then compile it with a custom loss:

```python
def my_loss(labels, logits):
    # here just wrapping a known loss to remove errors that could come from me
    # however there may be additional functionality here
    loss = tf.nn.softmax_cross_entropy_with_logits(
        labels=labels,
        logits=logits
    )
    return loss

model.compile(
    ...,
    loss ={
        'logit_layer': my_loss
    },
)

```

I then proceed to train this model with `ModelCheckpoint` callbacks, and after training save it in a myriad of ways:


```python
model.save(...)
tf.keras.experimental.export_saved_model(model, ...)
tf.keras.models.save_model(model, ...)
model.save_weights(...)
```

Now I try to load my saved model:

```
custom = {
    'my_loss': my_loss
}
model_file = # one of the files from above
model = tf.keras.models.load_model(model_file, custom)
```

Each and every one of these models gives me the following error:

```

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-7-52082d68b682> in <module>
      1 model_file = os.path.join(model_to_use)
----> 2 model = tf.keras.models.load_model(model_file, custom)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py in load_model(filepath, custom_objects, compile)
    145   if isinstance(filepath, six.string_types):
    146     loader_impl.parse_saved_model(filepath)
--> 147     return saved_model_load.load(filepath, compile)
    148 
    149   raise IOError(

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saved_model/load.py in load(path, compile)
     91     if model._training_config is not None:  # pylint: disable=protected-access
     92       model.compile(**saving_utils.compile_args_from_training_config(
---> 93           model._training_config))  # pylint: disable=protected-access
     94 
     95   return model

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    455     self._self_setattr_tracking = False  # pylint: disable=protected-access
    456     try:
--> 457       result = method(self, *args, **kwargs)
    458     finally:
    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)
    334     # Prepare list of loss functions, same size of model outputs.
    335     self.loss_functions = training_utils.prepare_loss_functions(
--> 336         self.loss, self.output_names)
    337 
    338     target_tensors = self._process_target_tensor_for_compile(target_tensors)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py in prepare_loss_functions(loss, output_names)
   1337   """"""
   1338   if isinstance(loss, collections_abc.Mapping):
-> 1339     generic_utils.check_for_unexpected_keys('loss', loss, output_names)
   1340     loss_functions = []
   1341     for name in output_names:

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/generic_utils.py in check_for_unexpected_keys(name, input_dict, expected_values)
    589     raise ValueError('Unknown entries in {} dictionary: {}. Only expected '
    590                      'following keys: {}'.format(name, list(unknown),
--> 591                                                  expected_values))
    592 
    593 

ValueError: Unknown entries in loss dictionary: ['logit_layer']. Only expected following keys: ['output_1']
```


I have found several github issues related to this:

1. https://github.com/qqwweee/keras-yolo3/issues/481
2. https://github.com/tensorflow/tensorflow/issues/28059
3. https://github.com/tensorflow/tensorflow/issues/25938

The second issue claims to be duplicate of the third and seems different from what I do above.
The first issue, uses an actual layer to be the loss, and passes a permissive lambda function as the loss:

```python
lambda y_pred, y_true: y_pred
``` 
This does not seem the same as what I am doing.


**Describe the expected behavior**
That providing the `customs` object is sufficient. 


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

A MWE can be found here: https://gitlab.com/SumNeuron/neumf
Clone it then run
```
python docker.py -w ai -c build
python docker.py -w ai -c up
```

The notebook to run is: https://gitlab.com/SumNeuron/neumf/blob/master/notebooks/NeuMF.ipynb

This is based off of the official recommendations model

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35749,TensorFlow 2.1: ImportError: DLL load failed: The specified module could not be found.,"I am using 

Python 3.76
TensorFlow 2.1
Installed using: pip install tensorflow
Processor:  Intel(R) Core(TM) i7-6500U CPU @ 2.50GHz, 2601 Mhz, 2 Core(s), 4 Logical Processor(s)
Laptop System Model:  HP Spectre x360 Convertible
Reproduce: All I have to type is  ""import tensorflow as tf"" and it fails.

**Note:** I also tried using tensorflow-cpu and still got the same issue.

Stack Trace:
```
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Development\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Development\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Development\Python\Python37\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Development\Python\Python37\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Development\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Development\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

<class 'ImportError'>, ImportError('Traceback (most recent call last):\n  File 
""C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py"", line 58, in <module>\n    from 
tensorflow.python.pywrap_tensorflow_internal import *\n  File 
""C:\\Development\\Python\\Python37\\lib\\site-
packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py"", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File 
""C:\\Development\\Python\\Python37\\lib\\site-
packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py"", line 24, in
 swig_import_helper\n    _mod = imp.load_module(\'_pywrap_tensorflow_internal\', fp, pathname,
 description)\n  File ""C:\\Development\\Python\\Python37\\lib\\imp.py"", line 242, in load_module\n
    return load_dynamic(name, filename, file)\n  File 
""C:\\Development\\Python\\Python37\\lib\\imp.py"", line 342, in load_dynamic\n    return 
_load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to
 load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some
 common reasons and solutions.  Include the entire stack trace\nabove this error message when
 asking for help.'), <traceback object at 0x000001E0E43DCA48>



<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

```
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35748,tflite micro softmax op is still version 1,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from: Source
- Tensorflow version: 4b3c1199a97cb36b8866d98e7036f4ec3e70abd6
- Target platform: Apollo3

**Describe the problem**

The tflite micro softmax op in [tensorflow/lite/micro/kernels/softmax.cc](https://github.com/tensorflow/tensorflow/blob/4b3c1199a97cb36b8866d98e7036f4ec3e70abd6/tensorflow/lite/micro/kernels/softmax.cc) already has int8 input support.
From what I understand this should be version 2 in [tensorflow/lite/micro/kernels/all_ops_resolver.cc](https://github.com/tensorflow/tensorflow/blob/4b3c1199a97cb36b8866d98e7036f4ec3e70abd6/tensorflow/lite/micro/kernels/all_ops_resolver.cc#L26)

"
35747,Tensorflow Lite download_dependencies.sh doesn't work,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10 (buster)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not a mobile device, it's a FreeWave ZumIQ edge computer. CPU is the same CPU as in a BeagleBone Black.
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: See description
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): gcc (Debian 8.3.0-6) 8.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

I'm trying to build a custom TF Lite wheel for this hardware. As a shortcut, I tried using the Raspberry Pi wheel, but the processor is different and so I got an ""Illegal instruction"" error. Therefore, I have to build my own.

I did this successfully last summer, with r1.14. It's really easy to do: I run the **download_dependencies.sh** script, then I do a couple of minor edits to the **build_pip_package.sh** script and run that script. 

Now it doesn't work anymore. It's the **download_dependencies.sh** that is causing the problem.

By the way, my shell is **/bin/bash** and my **uname-a** output is

`Linux fwt1311tp-min-jail-agent 4.4.0-170-generic #199-Ubuntu SMP Thu Nov 14 01:45:04 UTC 2019 armv7l GNU/Linux`


**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
$ ./tensorflow/lite/tools/make/download_dependencies.sh
downloading https://gitlab.com/libeigen/eigen/-/archive/e6fcee995b0083e5652c79957090684a47a727c3/eigen-e6fcee995b0083e5652c79957090684a47a727c3.tar.gz

gzip: stdin: unexpected end of file
/bin/tar: Child returned status 1
/bin/tar: Error is not recoverable: exiting now
```
 
So I edit **download_dependencies.sh**. Lines 63 and 65 are:

```
63   if [[ ""${url}"" == *gz ]]; then
64  ...
55   elif [[ ""${url}"" == *zip ]]; then
```

I end up wrapping the **star gz** and **star zip** in double quotes.

This time, I get this far:

```
$ ./tensorflow/lite/tools/make/download_dependencies.sh
downloading https://gitlab.com/libeigen/eigen/-/archive/e6fcee995b0083e5652c79957090684a47a727c3/eigen-e6fcee995b0083e5652c79957090684a47a727c3.tar.gz
downloading https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/gemmlowp/archive/12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3.zip
downloading https://github.com/google/googletest/archive/release-1.8.0.tar.gz
downloading https://github.com/abseil/abseil-cpp/archive/43ef2148c0936ebf7cb4be6b19927a9d9d145b8f.tar.gz
downloading https://github.com/intel/ARM_NEON_2_x86_SSE/archive/master.zip
downloading https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/farmhash/archive/816a4ae622e964763ca0862d9dbd19324a1eaf45.tar.gz
downloading https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz
downloading https://storage.googleapis.com/mirror.tensorflow.org/www.kurims.kyoto-u.ac.jp/~ooura/fft2d.tgz
/bin/sed: can't read tensorflow/lite/tools/make/downloads/eigen/Eigen/src/Core/arch/NEON/Complex.h: No such file or directory
```
And, in fact, the directory **./tensorflow/lite/tools/make/downloads/eigen** is empty. All of the **downloads** directories are empty. Nothing was downloaded.

I `git checkout r1.14` instead, to see if it makes any difference. It doesn't. I get the same failures.

**Any other info / logs**

Rather than repeat the source code, I will point you to **tensorflow/lite/tools/make/download_dependencies.sh**.
"
35746,Cannot train the person detection test on Tensorflow 2.0,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**

**Please provide the exact sequence of commands/steps when you ran into the problem**

"
35745,Add a warning that tfds.load can not be used for own Datasets,"## URL(s) with the issue:
https://www.tensorflow.org/datasets/api_docs/python/tfds/load

## Description of issue (what needs changing):
Add a warning that `tfds.load()` can not be used for the users own Datasets, i.e. that he creates himself. To a new user trying to to load a Dataset from a set of files it is not obvious that this method is only for pre-made, immutable Datasets.

Although it does say
> Loads the named dataset into a tf.data.Dataset.

i initially interpreted it such that my own Dataset can be assigned a name. 

I was looking for a way to split a Dataset into train and validation subsets and stumbled upon this documentation. I was redirected from https://www.tensorflow.org/datasets/splits which comes up as one of the most prominent search results when searching for ""tensorflow Dataset splits"" .

## Result
A user who visits https://www.tensorflow.org/datasets/api_docs/python/tfds/load will not spend 1 h of trying to understand all the documentation but will immediately realize that this is only for immutable pre-made Datasets."
35744,tf.shape shape mismatched in custom loss function,"Running on tf2.1.0-rc and python 3.6.8/ I wrote a custom loss function, but somehow I always get tf.shape error message

```
import tensorflow as tf
from scipy.stats import rankdata
from tensorflow.python.framework import ops
from tensorflow import keras

BATCH_SIZE = 4
num_targets =9

# Define custom py_func which takes also a grad op as argument:
def py_func(func, inp, Tout, name=None, grad=None):
    
    rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))
    
    tf.RegisterGradient(rnd_name)(grad)  
    g = tf.compat.v1.get_default_graph()
    with g.gradient_override_map({""PyFunc"": rnd_name}):
        return tf.py_function(func, inp, Tout, name=name)

# Def custom square function using np.square instead of tf.square:
def myrankdata(x, name=None):
    
    with ops.op_scope([x], name, ""MyGrad"") as name:
        rank = py_func(rankdata,
                        [x],
                        [tf.float32],
                        name=name,
                        grad=_MyGrad)  
        return rank

# Actual gradient:
def _MyGrad(op, grad):
    return grad * 20 * op.inputs[0]  

def custom_loss(num_targets):
    def custom_rhos(y_true, y_pred):
        rhos = []
        y_true, y_pred = tf.reshape(y_true, [-1, num_targets]), tf.reshape(y_pred, [-1, num_targets])
        for ind in range(num_targets):
            a = tf.slice(y_true, [0, ind], [-1, 1])
            a = tf.reshape(a, [-1])
            b = tf.slice(y_pred, [0, ind], [-1, 1])
            b = tf.reshape(b, [-1])
            rank_a, rank_b = myrankdata(a)[0], myrankdata(b)[0]
            rho = 1 - 6 * tf.reduce_sum((rank_a - rank_b) ** 2) / (BATCH_SIZE ** 3 - BATCH_SIZE)
            rhos.append(rho)
        return -tf.reduce_sum(rhos)
    return custom_rhos

```

My model summary is
```
Model: ""model_3""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_category (InputLayer)     [(None, 1)]          0                                            
__________________________________________________________________________________________________
input_host (InputLayer)         [(None, 1)]          0                                            
__________________________________________________________________________________________________
q_input_word_ids (InputLayer)   [(None, None)]       0                                            
__________________________________________________________________________________________________
q_input_masks (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
q_segment_ids (InputLayer)      [(None, None)]       0                                            
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 1, 32)        192         input_category[0][0]             
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 1, 32)        2080        input_host[0][0]                 
__________________________________________________________________________________________________
keras_layer_3 (KerasLayer)      [(None, 768), (None, 109482241   q_input_word_ids[0][0]           
                                                                 q_input_masks[0][0]              
                                                                 q_segment_ids[0][0]              
__________________________________________________________________________________________________
spatial_dropout1d_6 (SpatialDro (None, 1, 32)        0           embedding_6[0][0]                
__________________________________________________________________________________________________
spatial_dropout1d_7 (SpatialDro (None, 1, 32)        0           embedding_7[0][0]                
__________________________________________________________________________________________________
global_average_pooling1d_3 (Glo (None, 768)          0           keras_layer_3[0][1]              
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 2, 32)        0           spatial_dropout1d_6[0][0]        
                                                                 spatial_dropout1d_7[0][0]        
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 768)          0           global_average_pooling1d_3[0][0] 
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 64)           0           concatenate_6[0][0]              
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 832)          0           dropout_3[0][0]                  
                                                                 flatten_3[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 9)            7497        concatenate_7[0][0]              
==================================================================================================
Total params: 109,492,010
Trainable params: 109,492,009
Non-trainable params: 1
__________________________________________________________________________________________________
```

Here is my error message

```
/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Input to reshape is a tensor with 1 values, but the requested shape has 4
	 [[node Reshape_41 (defined at <ipython-input-147-9b8e1e62f3d2>:10) ]]
	 [[Reshape_74/_626]]
  (1) Invalid argument:  Input to reshape is a tensor with 1 values, but the requested shape has 4
	 [[node Reshape_41 (defined at <ipython-input-147-9b8e1e62f3d2>:10) ]]
0 successful operations.
0 derived errors ignored. [Op:__inference_distributed_function_868236]

Function call stack:
distributed_function -> distributed_function
```"
35743,"Multi-GPU training issue, tensorflow 2.0.0","I am trying to train my code on multiple-GPUs and have followed the tutorials online on using MirroredStrategy using MNIST dataset. Below is the error... 

ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.

I have a feeling that the issue is due to the fact that when I call on my GPUs. They are named XLA and not :/device:GPU. This leads to me to believe it is a bug? Below is my full code and following that the full error. I am currently using tensorflow 2.0.0, CUDA 10.1, and CentOS 7.6.1810

My code is below...
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import Xception
from tensorflow.keras.utils import multi_gpu_model
import numpy as np

num_samples = 1000
height = 224
width = 224
num_classes = 1000

#this puts the models weights on the CPU 
with tf.device(device_names[1]):
    model = Xception(weights = None, 
                     input_shape = (height, width, 3), 
                     classes = num_classes)

#this splits up the training amongst multiple GPUs
mirrored_strategy = tf.distribute.MirroredStrategy(devices = [device_names[3], device_names[4]])

from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow_datasets as tfds 
import tensorflow as tf
tfds.disable_progress_bar()
import os
print(tf.__version__)

2.0.0

datasets, info = tfds.load(name = 'mnist', with_info = True, as_supervised = True)
mnist_train, mnist_test = datasets['train'], datasets['test']
#this splits up the training amongst multiple specific GPUs
strategy = tf.distribute.MirroredStrategy(devices = [device_names[2], device_names[3]])
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
print('Name of devices are: ' + device_names[2] + ' and ' + device_names[3])

Number of devices: 2
Name of devices are: /device:XLA_GPU:0 and /device:XLA_GPU:1

#The benefit of using multiple GPUs is that you can train with the largest batchsize
#then you can just tweak the learning_rate accordingly

num_train_examples = info.splits['train'].num_examples
num_test_examples = info.splits['test'].num_examples

BUFFER_SIZE = 10000

BATCH_SIZE_PER_REPLICA = 64
def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /=255
    
    return image, label

train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)

with strategy.scope(): #the scope portion indicates which parts of the code will be distributed
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, 3, activation = 'relu', input_shape = (28,28,1)),
        tf.keras.layers.MaxPooling2D(), 
        tf.keras.layers.Flatten(), 
        tf.keras.layers.Dense(64, activation = 'relu'),
        tf.keras.layers.Dense(10, activation = 'softmax')
    ])

import datetime
log_dir = ""logs/fit/"" + datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1)
model.compile(loss = 'sparse_categorical_crossentropy', 
              optimizer = tf.keras.optimizers.Adam(), 
              metrics = ['accuracy'])

def decay(epoch):
    if epoch < 3:
        return 1e-3
    elif epoch >= 3 and epoch < 7:
        return 1e-4
    else:
        return 1e-5
checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, ""ckpt_{epoch}"")

class PrintLR(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs = None):
        print('\nLearning rate for epoch {} is {}'.format(epoch + 1, model.optimizer.lr.numpy()))

callbacks = [tensorboard_callback, 
             tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix, 
                                                save_weights_only = True), 
             tf.keras.callbacks.LearningRateScheduler(decay), 
             PrintLR()
            ]


model.fit(train_dataset, epochs = 30, callbacks = callbacks)

FULL ERROR: 
-------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-12-1c5af51f2d91> in <module>
----> 1 model.fit(train_dataset, epochs = 30, callbacks = callbacks)

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--> 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    322                 mode=ModeKeys.TRAIN,
    323                 training_context=training_context,
--> 324                 total_epochs=epochs)
    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    326 

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    121         step=step, mode=mode, size=current_batch_size) as batch_logs:
    122       try:
--> 123         batch_outs = execution_function(iterator)
    124       except (StopIteration, errors.OutOfRangeError):
    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
     84     # `numpy` translates Tensors to values in Eager mode.
     85     return nest.map_structure(_non_none_constant_value,
---> 86                               distributed_function(input_fn))
     87 
     88   return execution_function

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--> 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    501       # This is the first call of __call__, so we have to initialize.
    502       initializer_map = object_identity.ObjectIdentityDictionary()
--> 503       self._initialize(args, kwds, add_initializers_to=initializer_map)
    504     finally:
    505       # At this point we know that the initialization is complete (or less

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    406     self._concrete_stateful_fn = (
    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 408             *args, **kwds))
    409 
    410     def invalid_creator_scope(*unused_args, **unused_kwds):

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1846     if self.input_signature:
   1847       args, kwargs = None, None
-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1849     return graph_function
   1850 

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2148         graph_function = self._function_cache.primary.get(cache_key, None)
   2149         if graph_function is None:
-> 2150           graph_function = self._create_graph_function(args, kwargs)
   2151           self._function_cache.primary[cache_key] = graph_function
   2152         return graph_function, args, kwargs

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2039             arg_names=arg_names,
   2040             override_flat_arg_shapes=override_flat_arg_shapes,
-> 2041             capture_by_value=self._capture_by_value),
   2042         self._function_attributes,
   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    913                                           converted_func)
    914 
--> 915       func_outputs = python_func(*func_args, **func_kwargs)
    916 
    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    357         # the function a weak reference to itself to avoid a reference cycle.
--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    359     weak_wrapped_fn = weakref.ref(wrapped_fn)
    360 

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in distributed_function(input_iterator)
     71     strategy = distribution_strategy_context.get_strategy()
     72     outputs = strategy.experimental_run_v2(
---> 73         per_replica_function, args=(model, x, y, sample_weights))
     74     # Out of PerReplica outputs reduce or pick values to return.
     75     all_outputs = dist_utils.unwrap_output_dict(

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)
    758       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),
    759                                 convert_by_default=False)
--> 760       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    761 
    762   def reduce(self, reduce_op, value, axis):

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)
   1785       kwargs = {}
   1786     with self._container_strategy().scope():
-> 1787       return self._call_for_each_replica(fn, args, kwargs)
   1788 
   1789   def _call_for_each_replica(self, fn, args, kwargs):

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)
   2130         self._container_strategy(),
   2131         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):
-> 2132       return fn(*args, **kwargs)
   2133 
   2134   def _reduce_to(self, reduce_op, value, destinations):

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    290   def wrapper(*args, **kwargs):
    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
--> 292       return func(*args, **kwargs)
    293 
    294   if inspect.isfunction(func) or inspect.ismethod(func):

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)
    262       y,
    263       sample_weights=sample_weights,
--> 264       output_loss_metrics=model._output_loss_metrics)
    265 
    266   if reset_metrics:

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorfl<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>
ow_core/python/keras/engine/training_eager.py in train_on_batch(model, inputs, targets, sample_weights, output_loss_metrics)
    309           sample_weights=sample_weights,
    310           training=True,
--> 311           output_loss_metrics=output_loss_metrics))
    312   if not isinstance(outs, list):
    313     outs = [outs]

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, output_loss_metrics, sample_weights, training)
    270                         loss_scale_optimizer.LossScaleOptimizer):
    271             grads = model.optimizer.get_unscaled_gradients(grads)
--> 272           model.optimizer.apply_gradients(zip(grads, trainable_weights))
    273       else:
    274         logging.warning('The list of trainable weights is empty. Make sure that'

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name)
    439           functools.partial(self._distributed_apply, apply_state=apply_state),
    440           args=(grads_and_vars,),
--> 441           kwargs={""name"": name})
    442 
    443   def _distributed_apply(self, distribution, grads_and_vars, name, apply_state):

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in merge_call(self, merge_fn, args, kwargs)
   1915     if kwargs is None:
   1916       kwargs = {}
-> 1917     return self._merge_call(merge_fn, args, kwargs)
   1918 
   1919   def _merge_call(self, merge_fn, args, kwargs):

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py in _merge_call(self, merge_fn, args, kwargs)
   1922         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access
   1923     try:
-> 1924       return merge_fn(self._strategy, *args, **kwargs)
   1925     finally:
   1926       _pop_per_thread_mode()

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in _distributed_apply(self, distribution, grads_and_vars, name, apply_state)
    480         # delays. See b/136304694.
    481         with backend.name_scope(
--> 482             scope_name), distribution.extended.colocate_vars_with(var):
    483           update_ops.extend(
    484               distribution.extended.update(

~/anaconda3/envs/tf/lib/python3.6/contextlib.py in __enter__(self)
     79     def __enter__(self):
     80         try:
---> 81             return next(self.gen)
     82         except StopIteration:
     83             raise RuntimeError(""generator didn't yield"") from None

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in _colocate_with_for_gradient(self, op, gradient_uid, ignore_existing)
   4218   def _colocate_with_for_gradient(self, op, gradient_uid,
   4219                                   ignore_existing=False):
-> 4220     with self.colocate_with(op, ignore_existing):
   4221       if gradient_uid is not None and self._control_flow_context is not None:
   4222         self._control_flow_context.EnterGradientColocation(op, gradient_uid)

~/anaconda3/envs/tf/lib/python3.6/contextlib.py in __enter__(self)
     79     def __enter__(self):
     80         try:
---> 81             return next(self.gen)
     82         except StopIteration:
     83             raise RuntimeError(""generator didn't yield"") from None

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in colocate_with(self, op, ignore_existing)
   4267       raise ValueError(""Trying to reset colocation (op is None) but ""
   4268                        ""ignore_existing is not True"")
-> 4269     op = _op_to_colocate_with(op, self)
   4270 
   4271     # By default, colocate_with resets the device function stack,

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in _op_to_colocate_with(v, graph)
   6601   # happen soon, perhaps this hack to work around the circular
   6602   # import dependency is acceptable.
-> 6603   if hasattr(v, ""handle"") and hasattr(v.handle, ""op"") and isinstance(
   6604       v.handle.op, Operation):
   6605     if graph.building_function:

~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow_core/python/distribute/values.py in handle(self)
    715       device = distribute_lib.get_update_device()
    716       if device is None:
--> 717         raise ValueError(""`handle` is not available outside the replica context""
    718                          "" or a `tf.distribute.Strategy.update()` call."")
    719     return self.get(device=device).handle

ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call."
35742,How to add a new op to a pb model,"This problem is rather embarassing: I have a pb model at hand but not ready for deployment, because of a missing op. So, I doubt if I could add the required op to the pb and export a new pb for deployment?

Could anyone shed some light please?"
35741,fake_quant_with_min_max_vars innefficiencies,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
TensorFlow installed from (source or binary): Binary
TensorFlow version (use command below): v1.12.1-21401-gd908b50 2.1.0-dev20191230
Python version: 3.6.9
Bazel version (if compiling from source): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: CUDA Version 10.1.243 / cuDNN 7.6.4.38-1
GPU model and memory: TITAN V, 12GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
A call to fake_quant_with_min_max_vars consistently results in a couple of D2H transfers before the quantization kernel is executed. I believe these transfers are part of ValidateInputTypeAndPlacement and largely dominate the operation cost. This is slowing down to unbearable levels the training of large NNs with fake quantization nodes.

An image of a profile resulting from back to back dependent quantization calls:

![image](https://user-images.githubusercontent.com/24900898/72160140-d7aaff00-338b-11ea-9215-d44440bb65fa.png)




**Describe the expected behavior**

I would expect little to zero overhead before the actual quantization kernel.

**Code to reproduce the issue**

`import tensorflow as tf
import numpy as np
import time
from tensorflow.python import eager
import os

x = tf.random.uniform(shape=[10000,1000])
xmax = tf.Variable(0.5)

eager.profiler.start()

xmax_val = xmax.value()

for n in range(10):
    x = tf.quantization.fake_quant_with_min_max_vars(inputs=x,
                                                    min=xmax_val,
                                                    max=xmax_val,
                                                    num_bits=8)

profiler_result = eager.profiler.stop()
eager.profiler.save(os.path.join('quant','log'), profiler_result)

`

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35739,disable_eager_execution resets random seeds set before,"This is on TF2.1 from pip on Windows 10.

**Describe the current behavior**
```
import tensorflow.compat.v1 as tf1

tf1.random.set_random_seed(0)
tf1.disable_eager_execution()

print(tf1.keras.backend.get_session().run(tf1.random.uniform((), 0, 1)))
```

This prints a different number every time. I have a similar example with more TF2-relevant code, where the same thing happens:
```
import tensorflow as tf
import tensorflow.compat.v1 as tf1

tf.random.set_seed(0)
tf1.disable_eager_execution()

print(tf1.keras.backend.get_session().run(tf.random.uniform((), 0, 1)))
```

A workaround is to set the seed after `disable_eager_execution`."
35738,build aborted analysis of target build_tbb failed,"I am trying to build `tensorflow` from source for my `Mac` as it supports `SSE4.1`, `SSE4.2`, `AVX` and `FMA` with the following the official documentation (https://www.tensorflow.org/install/source). 
I am using the `master` branch and the following command:

    bazel build -c opt --copt=-mavx --copt=-mfma --copt=-msse4.2 --copt=-msse4.1 --copt=-mfpmath=sse --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --config=mkl --config=ngraph --config=numa --config=noaws --config=nogcp --config=nohdfs --config=nonccl -k //tensorflow/tools/pip_package:build_pip_package


Here goes my `bazel` configuration:

    (base) IKA-XK0X8JG5J:tensorflow_build sardarmrinal$ ./configure 
    WARNING: Running Bazel server needs to be killed, because the startup options are different.
    WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
    You have bazel 1.2.1 installed.
    Please specify the location of python. [Default is /opt/anaconda3/bin/python]: 


    Found possible Python library paths:
      /opt/anaconda3/lib/python3.7/site-packages
    Please input the desired Python library path to use.  Default is [/opt/anaconda3/lib/python3.7/site-packages]

    Do you wish to build TensorFlow with XLA JIT support? [Y/n]: Y
    XLA JIT support will be enabled for TensorFlow.

    Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N
    No OpenCL SYCL support will be enabled for TensorFlow.

    Do you wish to build TensorFlow with ROCm support? [y/N]: N
    No ROCm support will be enabled for TensorFlow.

    Do you wish to build TensorFlow with CUDA support? [y/N]: N
    No CUDA support will be enabled for TensorFlow.

    Do you wish to download a fresh release of clang? (Experimental) [y/N]: N
    Clang will not be downloaded.

    Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 


    Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
    Not configuring the WORKSPACE for Android builds.

    Do you wish to build TensorFlow with iOS support? [y/N]: N
    No iOS support will be enabled for TensorFlow.

    Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
    	--config=mkl         	# Build with MKL support.
    	--config=monolithic  	# Config for mostly static monolithic build.
    	--config=ngraph      	# Build with Intel nGraph support.
    	--config=numa        	# Build with NUMA support.
    	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
    	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
    Preconfigured Bazel build configs to DISABLE default on features:
    	--config=noaws       	# Disable AWS S3 filesystem support.
    	--config=nogcp       	# Disable GCP support.
    	--config=nohdfs      	# Disable HDFS support.
    	--config=nonccl      	# Disable NVIDIA NCCL support.
    Configuration finished

And I am getting the following error:

    ERROR: /private/var/tmp/_bazel_sardarmrinal/841c8c9a203505f6ff7b50ea63697e9e/external/tbb/BUILD.bazel:12:1: in cmd attribute of genrule rule @tbb//:build_tbb: $(AR) not defined
    ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@tbb//:build_tbb' failed; build aborted

Any idea how I can get past this?

FYI: I built one successfully yesterday wihtout `--cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""` and I wanted to use this option this time. All I did was pulling the latest `master` branch.



Output from `tf_env_collect.sh`:

    == check python ===================================================
        python version: 3.7.4
        python branch: 
        python build version: ('default', 'Aug 13 2019 15:17:50')
        python compiler version: Clang 4.0.1 (tags/RELEASE_401/final)
        python implementation: CPython


    == check os platform ===============================================
    os: Darwin
    os kernel version: Darwin Kernel Version 18.7.0: Sun Dec  1 18:59:03 PST 2019; root:xnu-4903.278.19~1/RELEASE_X86_64
    os release version: 18.7.0
    os platform: Darwin-18.7.0-x86_64-i386-64bit
    linux distribution: ('', '', '')
    linux os distribution: ('', '', '')
    mac version: ('10.14.6', ('', '', ''), 'x86_64')
    uname: uname_result(system='Darwin', node='IKA-XK0X8JG5J', release='18.7.0', version='Darwin Kernel Version 18.7.0: Sun Dec  1 18:59:03 PST 2019; root:xnu-4903.278.19~1/RELEASE_X86_64', machine='x86_64', processor='i386')
    architecture: ('64bit', '')
    machine: x86_64


    == are we in docker =============================================
    No

    == compiler =====================================================
    Apple LLVM version 10.0.1 (clang-1001.0.46.4)
    Target: x86_64-apple-darwin18.7.0
    Thread model: posix
    InstalledDir: /Library/Developer/CommandLineTools/usr/bin

    == check pips ===================================================
    numpy                              1.17.2   
    numpydoc                           0.9.1    
    protobuf                           3.11.2   
    tensorflow                         2.0.0    
    tensorflow-estimator               2.0.1    

    == check for virtualenv =========================================
    False

    == tensorflow import ============================================
    tf.version.VERSION = 2.0.0
    tf.version.GIT_VERSION = unknown
    tf.version.COMPILER_VERSION = 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)

    == env ==========================================================
    LD_LIBRARY_PATH is unset
    DYLD_LIBRARY_PATH is unset

    == nvidia-smi ===================================================
    tf_env_cololect.sh: line 147: nvidia-smi: command not found

    == cuda libs  ===================================================

    == tensorflow installed from info ==================
    Name: tensorflow
    Version: 2.0.0
    Summary: TensorFlow is an open source machine learning framework for everyone.
    Home-page: https://www.tensorflow.org/
    Author-email: packages@tensorflow.org
    License: Apache 2.0
    Location: /opt/anaconda3/lib/python3.7/site-packages
    Required-by: 

    == python version  ==============================================
    (major, minor, micro, releaselevel, serial)
    (3, 7, 4, 'final', 0)

    == bazel version  ===============================================
    Build label: 1.2.1
    Build time: Tue Nov 26 15:27:31 2019 (1574782051)
    Build timestamp: 1574782051
    Build timestamp as int: 1574782051

"
35737,TFlite compilation failing for tf 2.1.0,"<em>
I am trying to compile tflite library for x86 machine, I have tried it using the following script
https://github.com/sourcecode369/tensorflow-1/blob/master/tensorflow/lite/tools/make/build_lib.sh

before this, I have also installed the required dependencies using
https://github.com/sourcecode369/tensorflow-1/blob/master/tensorflow/lite/tools/make/download_dependencies.sh
</em>

**System information**
- OS Platform and Distribution : Linux Ubuntu 18.04
- TensorFlow installed from : source
- TensorFlow version : 2.1.0
- GCC/Compiler version :7.4.0
- Bazel version : 2.0.0

**Describe the current behavior**
/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/tools/benchmark/benchmark_performance_options.o /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/tools/benchmark/benchmark_utils.o /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/tools/benchmark/benchmark_params.o /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/profiling/profile_summarizer.o /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/core/util/stats_calculator.o /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/tools/command_line_flags.o /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/tools/evaluation/utils.o
ar: creating /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/benchmark-lib.a
g++ -O3 -DNDEBUG -fPIC  --std=c++11 -fPIC -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -pthread -I. -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include \
-o /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/bin/benchmark_model /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/tools/benchmark/benchmark_main.o \
 /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/benchmark-lib.a  -lstdc++ -lpthread -lm -lz -ldl
g++ -O3 -DNDEBUG -fPIC  --std=c++11 -fPIC -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -pthread -I. -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include \
-o /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/bin/benchmark_model_performance_options /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/tools/benchmark/benchmark_tflite_performance_options_main.o \
 /home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/benchmark-lib.a  -lstdc++ -lpthread -lm -lz -ldl
/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/benchmark-lib.a(command_line_flags.o): In function `tflite::Flags::Parse(int*, char const**, std::vector<tflite::Flag, std::allocator<tflite::Flag> > const&)':
command_line_flags.cc:(.text+0x57c2): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'
command_line_flags.cc:(.text+0x57ed): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'
command_line_flags.cc:(.text+0x5997): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'
command_line_flags.cc:(.text+0x59c8): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'
command_line_flags.cc:(.text+0x5b1d): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'
command_line_flags.cc:(.text+0x5b48): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'
command_line_flags.cc:(.text+0x5db0): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'
command_line_flags.cc:(.text+0x5e24): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'
command_line_flags.cc:(.text+0x5ea5): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'
collect2: error: ld returned 1 exit status
tensorflow/lite/tools/make/Makefile:295: recipe for target '/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/bin/benchmark_model' failed
make: *** [/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/bin/benchmark_model] Error 1
make: *** Waiting for unfinished jobs....
/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/benchmark-lib.a(command_line_flags.o): In function `tflite::Flags::Parse(int*, char const**, std::vector<tflite::Flag, std::allocator<tflite::Flag> > const&)':
command_line_flags.cc:(.text+0x57c2): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'
command_line_flags.cc:(.text+0x57ed): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'
command_line_flags.cc:(.text+0x5997): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'
command_line_flags.cc:(.text+0x59c8): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'
command_line_flags.cc:(.text+0x5b1d): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'
command_line_flags.cc:(.text+0x5b48): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'
command_line_flags.cc:(.text+0x5db0): undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'
command_line_flags.cc:(.text+0x5e24): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'
command_line_flags.cc:(.text+0x5ea5): undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'
collect2: error: ld returned 1 exit status
tensorflow/lite/tools/make/Makefile:301: recipe for target '/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/bin/benchmark_model_performance_options' failed
make: *** [/home/swati/git_workspace/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/bin/benchmark_model_performance_options] Error 1
make: Leaving directory '/home/swati/git_workspace/tensorflow'


**Describe the expected behavior**
It should successfully compile and build the library


**Code to reproduce the issue**
git clone https://github.com/tensorflow/tensorflow
cd tensorflow
./tensorflow/lite/tools/download_dependencies.sh 
./tensorflow/lite/tools/make/build_lib.sh

I am new to source compilation, currently unable to understand why this is not working.

"
35736,Converting saved_model to TFLite model using TF 2.0,"**System information**
- Google colab:
- TensorFlow 2.0.0

I am working on converting custom object detection model (trained using SSD and inception network) to quantized TFLite model. I can able to convert custom object detection model from frozen graph to quantized TFLite model using the following code snippet (using **Tensorflow 1.4**):
```
converter = tf.lite.TFLiteConverter.from_frozen_graph(args[""model""],input_shapes = {'normalized_input_image_tensor':[1,300,300,3]},
input_arrays = ['normalized_input_image_tensor'],output_arrays = ['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1',
'TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'])

converter.allow_custom_ops=True
converter.post_training_quantize=True 
tflite_model = converter.convert()
open(args[""output""], ""wb"").write(tflite_model)
```
However ```tf.lite.TFLiteConverter.from_frozen_graph``` class method is not available for **Tensorflow 2.0** (refer [this link](https://www.tensorflow.org/lite/convert/python_api#exporting_a_savedmodel_)). So I tried to convert the model using ```tf.lite.TFLiteConverter.from_saved_model``` class method. The code snippet is shown below:


```
converter = tf.lite.TFLiteConverter.from_saved_model(""/content/"") # Path to saved_model directory
converter.optimizations =  [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
```

**The above code snippet throws the following error:**

```
ValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'.
```

**I tried to pass ```input_shapes``` as argument**

```
converter = tf.lite.TFLiteConverter.from_saved_model(""/content/"",input_shapes={""image_tensor"" : [1,300,300,3]})
```

**but it throws the following error:**

```
TypeError: from_saved_model() got an unexpected keyword argument 'input_shapes'
```

Am I missing something? Please feel free to correct me!"
35734,Hexagon Delegate requirements needs to be clarified?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: SD 820 device
- TensorFlow installed from (source or binary): source, 4b7d5117de4a193bd895ff357dc5286de847c632
- TensorFlow version (use command below): -
- Python version: -
- Bazel version (if compiling from source): 1.2.1
- GCC/Compiler version (if compiling from source): Android NDK clang
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**

When using Hexagon delegate on 32bit ARM device:

```
CANNOT LINK EXECUTABLE: cannot locate symbol ""remote_handle64_open"" referenced by ""/data/package/lib/libhexagon_interface.so""...
```

As far as I know 820 supports Hexagon acceleration, but it seems libhexagon_interface.so requires extra symbol that is not present on my device system libraries (As far as I know it is part of qualcomm code)

On our 64bit device I see that this function is properly used, there are even some logs from `vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c`

So it is more of a question: what is required to run hexagon delegate?

I know that this is pre-built library from vendor, but I wasn't able to find it in Hexagon SDK itself.
So I'm not sure where exactly it is coming from.
If possible I'd appreciate some guidance here

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35733,Docker images with tags '2.1.0-gpu-py3' and '2.1.0-gpu-py3-jupyter' are missing on DockerHub,"Probably the CI has failed to upload those images, because all other combinations of tensorflow:2.1.0[-gpu][-py3][-jupyter] are available.

"
35732,A strange numerical computation bug for the simple dense layer,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory:  N/A

**Describe the current behavior**

See the code:

```
import numpy as np
import tensorflow as tf

m = 3
n = 3
x = tf.cast(np.random.randn(1, m, 32), tf.float32)
z = tf.tile(x, [n, 1, 1])

layer = tf.keras.layers.Dense(32)
w = layer(z)

tf.print(tf.norm(z[0, :, :] - z[1, :, :]), tf.norm(z[1, :, :] - z[n-1, :, :]))
tf.print(tf.norm(w[0, :, :] - w[1, :, :]), tf.norm(w[1, :, :] - w[n-1, :, :]))
```
In the code we replicate the input `x` 3 times and apply a dense layer upon it. We expect to get the same results for the 3 replicates. In fact the 1st and 2nd results are indeed same, while the 3rd result is different. Here is the results of the script above:

```
0 0
0 1.0617149e-06 # this error is not fixed for each run
```
where we expect all results to be 0.

Strangely enough, this bug only appears for some `(m,n)` pairs (in the example above `m=n=3`). I ran the code for all `m` and `n` from 1 to 100 and found that there are ~40% combinations that will cause a bug, but I didn't find any obvious pattern..."
35731,RuntimeError when saving Keras model with stacked RNN cells in HDF5 format,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.6
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0
- Python version: 3.7.4

**Describe the current behavior**
When I try to save a Keras model in HDF5 format with `model.save()`, if a model includes RNN layer with stacked cells, a `RuntimeError` occurs because of non-unique trainable weight `tf.Variable` names.

**Describe the expected behavior**
I should be able to save Keras model to HDF5 file without issues.

**Code to reproduce the issue**
```python
import tensorflow as tf

X = tf.keras.Input([10, 91], name=""train_input"")
rnn_layers = [
    tf.keras.layers.LSTMCell(size, recurrent_dropout=0, name=""rnn_cell%d"" % i)
    for i, size in enumerate([512, 512])
]
rnn_output = tf.keras.layers.RNN(rnn_layers, return_sequences=True, name=""rnn_layer"")(X)
pred_feat = tf.keras.layers.Dense(91, name=""prediction_features"")(rnn_output)
pred = tf.keras.layers.Softmax()(pred_feat)
model = tf.keras.Model(inputs=[X], outputs=[pred, pred_feat])
model.save(""test.h5"")
```

**Other info / logs**
Traceback:
```
  File ""/Users/dimitrijer/git/mlai/footpy/footpy/train_keras.py"", line 41, in train
    model.save()
  File ""/Users/dimitrijer/git/mlai/footpy/footpy/model_keras.py"", line 246, in save
    self.model.save(os.path.join(model_path, filename), overwrite=True)
  File ""/Users/dimitrijer/.pyenv/versions/footpy/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1035, in save
    signatures, options)
  File ""/Users/dimitrijer/.pyenv/versions/footpy/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py"", line 118, in save_model
    model, filepath, overwrite, include_optimizer)
  File ""/Users/dimitrijer/.pyenv/versions/footpy/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 109, in save_model_to_hdf5
    save_weights_to_hdf5_group(model_weights_group, model_layers)
  File ""/Users/dimitrijer/.pyenv/versions/footpy/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 631, in save_weights_to_hdf5_group
    param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)
  File ""/Users/dimitrijer/.pyenv/versions/footpy/lib/python3.7/site-packages/h5py/_hl/group.py"", line 139, in create_dataset
    self[name] = dset
  File ""/Users/dimitrijer/.pyenv/versions/footpy/lib/python3.7/site-packages/h5py/_hl/group.py"", line 373, in __setitem__
    h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)
  File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper
  File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper
  File ""h5py/h5o.pyx"", line 202, in h5py.h5o.link
RuntimeError: Unable to create link (name already exists)
```
This occurs because there is a repeating set of TF variables for each RNN cell:
- `<tf.Variable 'rnn_layer/kernel:0'...>`
- `<tf.Variable 'rnn_layer/recurrent_kernel:0'...>`
- `<tf.Variable 'rnn_layer/bias:0'...>`

These three variables have the same name across stacked RNN cells. This causes conflict when HDF5 model is saved - when a group is formed for RNN layer, list at `hdf5_format.py:628` contains triplets with repeating names, which causes the error when dataset is created for each weight at line 635."
35729,"tensorflow.python.framework.errors_impl.InvalidArgumentError: indices = -1 is not in [0, 4) [Op:GatherV2]","I am getting InvalidArgumentError while running below code:

![Code](https://user-images.githubusercontent.com/59674741/72132628-b47a5280-337f-11ea-9dfb-f60609156f4f.jpeg)

### System information
- **OS Platform:Ubuntu 18.04
- **TensorFlow version: v2.0.0-rc2-26-g64c3d38 and tensorflow2.0.0
- **Python version**: Python 3.7.4

This code is failing while running for first epoch. It has to run for 400 steps in first epoch but getting failed at 256, 300 or sometimes any no of iteration.
Below is the error i am getting:
256/400 [==================>...........] - ETA: 3:40 - loss: 0.6932 - accuracy: 0.4968Traceback (most recent call last):
  File ""train_model_latest.py"", line 196, in <module>
    train_model()
  File ""train_model_latest.py"", line 168, in train_model
    callbacks=callbacks)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1297, in fit_generator
    steps_name='steps_per_epoch')
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py"", line 265, in model_iteration
    batch_outs = batch_function(*batch_data)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 973, in train_on_batch
    class_weight=class_weight, reset_metrics=reset_metrics)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 264, in train_on_batch
    output_loss_metrics=model._output_loss_metrics)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py"", line 311, in train_on_batch
    output_loss_metrics=output_loss_metrics))
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py"", line 252, in _process_single_batch
    training=training))
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py"", line 127, in _model_loss
    outs = model(inputs, **kwargs)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py"", line 256, in call
    return super(Sequential, self).call(inputs, training=training, mask=mask)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 708, in call
    convert_kwargs_to_constants=base_layer_utils.call_context().saving)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py"", line 860, in _run_internal_graph
    output_tensors = layer(computed_tensors, **kwargs)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 891, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/u/s/shalinis/.local/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py"", line 233, in call
    update_op = tf_utils.smart_cond(training, add_update, no_op)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py"", line 59, in smart_cond
    pred, true_fn=true_fn, false_fn=false_fn, name=name)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/smart_cond.py"", line 54, in smart_cond
    return true_fn()
  File ""/u/s/shalinis/.local/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py"", line 227, in add_update
    [self.pruning_obj.conditional_mask_update()]):
  File ""/u/s/shalinis/.local/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py"", line 245, in conditional_mask_update
    mask_update_distributed)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 1917, in merge_call
    return self._merge_call(merge_fn, args, kwargs)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 1924, in _merge_call
    return merge_fn(self._strategy, *args, **kwargs)
  File ""/u/s/shalinis/.local/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py"", line 241, in mask_update_distributed
    no_update)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 1202, in cond
    result = true_fn()
  File ""/u/s/shalinis/.local/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py"", line 232, in update_distributed
    new_threshold, new_mask = self._maybe_update_block_mask(weight)
  File ""/u/s/shalinis/.local/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py"", line 128, in _maybe_update_block_mask
    return self._update_mask(weights)
  File ""/u/s/shalinis/.local/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_impl.py"", line 102, in _update_mask
    current_threshold = array_ops.gather(values, k - 1)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py"", line 180, in wrapper
    return target(*args, **kwargs)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py"", line 3967, in gather
    return gen_array_ops.gather_v2(params, indices, axis, name=name)
  File ""/u/s/shalinis/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py"", line 4075, in gather_v2
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: indices = -1 is not in [0, 4) [Op:GatherV2]
"
35728,Wrong Window Size while training the model,"## URL(s) with the issue:

https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l08c05_forecasting_with_machine_learning.ipynb

## Description of issue (what needs changing):

Window size should be 30 instead of 20 in the description under ""Forecasting With Machine Learning"".

### Clear description

![Screenshot from 2020-01-10 12-05-13](https://user-images.githubusercontent.com/29497701/72131252-b1905980-33a1-11ea-8cf5-11d089d5316e.png)

--------------------
As we can see under ""Linear Model"", `window_size=30` while in description it is mentioned as model forecasts, given previous 20 steps.

### Submit a pull request?

Yes..."
35726,ERROR:root:Internal Python error in the inspect module.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-5-df9a78134369>"", line 1, in <module>
    from keras.models import Model
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\backend\load_backend.py"", line 90, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2040, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1101, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 319, in wrapped
    return f(*args, **kwargs)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 353, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\inspect.py"", line 1502, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\inspect.py"", line 1460, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\inspect.py"", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-5-df9a78134369>"", line 1, in <module>
    from keras.models import Model
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\backend\load_backend.py"", line 90, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2040, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-5-df9a78134369>"", line 1, in <module>
    from keras.models import Model
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\backend\load_backend.py"", line 90, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2040, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3249, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3343, in run_code
    self.showtraceback(running_compiled_code=True)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2043, in showtraceback
    value, tb, tb_offset=tb_offset)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1385, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1288, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1150, in structured_traceback
    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)
TypeError: can only concatenate str (not ""list"") to str

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2040, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'TypeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1101, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 319, in wrapped
    return f(*args, **kwargs)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 353, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\inspect.py"", line 1502, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\inspect.py"", line 1460, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\inspect.py"", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-5-df9a78134369>"", line 1, in <module>
    from keras.models import Model
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\backend\load_backend.py"", line 90, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2040, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3249, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3343, in run_code
    self.showtraceback(running_compiled_code=True)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2043, in showtraceback
    value, tb, tb_offset=tb_offset)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1385, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1288, in structured_traceback
    self, etype, value, tb, tb_offset, number_of_lines_of_context
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\ultratb.py"", line 1150, in structured_traceback
    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)
TypeError: can only concatenate str (not ""list"") to str

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2040, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'TypeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\KRISHNA PRASAD P\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\Anaconda3\lib\imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

~\Anaconda3\lib\imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:


During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)
~\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py in showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)
   2039                         # in the engines. This should return a list of strings.
-> 2040                         stb = value._render_traceback_()
   2041                     except Exception:

AttributeError: 'ImportError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
~\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py in run_code(self, code_obj, result, async_)
   3341             if result is not None:
   3342                 result.error_in_exec = sys.exc_info()[1]
-> 3343             self.showtraceback(running_compiled_code=True)
   3344         else:
   3345             outflag = False

~\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py in showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)
   2041                     except Exception:
   2042                         stb = self.InteractiveTB.structured_traceback(etype,
-> 2043                                             value, tb, tb_offset=tb_offset)
   2044 
   2045                     self._showtraceback(etype, value, stb)

~\Anaconda3\lib\site-packages\IPython\core\ultratb.py in structured_traceback(self, etype, value, tb, tb_offset, number_of_lines_of_context)
   1383         self.tb = tb
   1384         return FormattedTB.structured_traceback(
-> 1385             self, etype, value, tb, tb_offset, number_of_lines_of_context)
   1386 
   1387 

~\Anaconda3\lib\site-packages\IPython\core\ultratb.py in structured_traceback(self, etype, value, tb, tb_offset, number_of_lines_of_context)
   1286             # Verbose modes need a full traceback
   1287             return VerboseTB.structured_traceback(
-> 1288                 self, etype, value, tb, tb_offset, number_of_lines_of_context
   1289             )
   1290         elif mode == 'Minimal':

~\Anaconda3\lib\site-packages\IPython\core\ultratb.py in structured_traceback(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)
   1148         exception = self.get_parts_of_chained_exception(evalue)
   1149         if exception:
-> 1150             formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)
   1151             etype, evalue, etb = exception
   1152         else:

TypeError: can only concatenate str (not ""list"") to str
"
35725,How to print tensor within GradientTape scope in TF2?,"How to print tensor within GradientTape scope in TF2? i.e.

```
with tf.GradientTape() as tape: 
    ...
    print(x)

```

Currently it gives me output like this:

> Tensor(""Identity_1:0"", shape=(), dtype=float32)

This is very needed for debugging. Shouldn't the concept of computational graph already be discarded in TF2, such that it should work very much like PyTorch? Why is it still behaving like TF1 in GradientTape?"
35724,Recurrent Dropout is Wrong,"I've reviewed one design in-depth, and two others superficially, but Keras/TF's `recurrent_dropout` does not implement _any_ of them; publication links below.

 1. I see some potentially severe problems with TF's implementation in light of the papers I've read, which explicitly advocate against the used scheme. This said - what is TensorFlow / Keras's justification / rationale  of its own implementation?

 2. The implementation is inconsistent - see below; the docstring only mentions a performance difference, but there's also a _reproducibility_ and _design_ difference; `==1` uses _different masks_ per gate, whereas `==2` uses a _shared mask_. The two are neither theoretically nor practically identical. 

Second's fixable via a docstring, but first involves significant changes to recurrent dropout logic for `LSTM`, `GRU`, and maybe other RNNs. This said: **is TensorFlow / Keras open to changing its base implementations of recurrent dropout?** If so, I can go ahead and clarify **(1)** in detail, and maybe even do the re-implementing myself in a PR, per paper 1.

<hr>

**Inconsistency**: `implementation==1` vs. `implementation==2`

```python
if 0. < self.recurrent_dropout < 1.:  # implementation==1
    h_tm1_i = h_tm1 * rec_dp_mask[0]
    h_tm1_f = h_tm1 * rec_dp_mask[1]
    h_tm1_c = h_tm1 * rec_dp_mask[2]
    h_tm1_o = h_tm1 * rec_dp_mask[3]
```
```python
if 0. < self.recurrent_dropout < 1.:  # implementation==2
    h_tm1 *= rec_dp_mask[0]
```
Source codes: [keras](https://github.com/keras-team/keras/blob/master/keras/layers/recurrent.py#L2014) -- [tf.keras](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent.py#L2391)

<hr>

**Publications**:

 1. [Recurrent Dropout without Memory Loss](https://arxiv.org/abs/1603.05118)
 2. [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)
 3. [RNNDrop: A novel dropout for RNNs](https://sci-hub.se/10.1109/ASRU.2015.7404775)"
35721,import tensorflow as tf fail,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 2.1 (with pip)
- **Python version**: 3.7 (anaconda)
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: cuda 10.1 cudnn 7.6
- **GPU model and memory**: RTX 2080
- **Exact command to reproduce**: import tensorflow as tf 


### Describe the problem
Hi, I install tf 2.1 with pip.
When I write inside Spyder :
import tensorflow as tf

I get the error 


Error in callback <bound method AutoreloadMagics.post_execute_hook of <autoreload.AutoreloadMagics object at 0x000002944E042978>> (for post_execute):


Traceback (most recent call last):

  File ""C:\Users\user\Anaconda3\lib\site-packages\IPython\extensions\autoreload.py"", line 578, in post_execute_hook
    _, pymtime = self._reloader.filename_and_mtime(sys.modules[modname])

  File ""C:\Users\user\Anaconda3\lib\site-packages\IPython\extensions\autoreload.py"", line 184, in filename_and_mtime
    if not hasattr(module, '__file__') or module.__file__ is None:

  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()

  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)

  File ""C:\Users\user\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)

  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import

  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load

  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked

  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed

  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import

  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load

  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked

  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked

  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module

  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed

  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio

  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav

  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow

  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()

  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)

  File ""C:\Users\user\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)

  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\user\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\user\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 


Failed to load the native TensorFlow runtime.

Thanks for your help !"
35720,Decorating a function with tf.function leads to slower execution,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): None
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v1.12.1-21967-gd80fda0 2.1.0-dev20200109
- Python version: 3.6.9
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: CUDA Version 10.1.243 / cuDNN 7.6.4.38-1
- GPU model and memory: TITAN V, 12GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When decorating a function with tf.function leads to slower execution, even though the function accepts only tf Tensors.

**Describe the expected behavior**
I expect the function to execute as fast or faster with tf.function decoration.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
`
import tensorflow as tf
import numpy as np
import time

assert tf.executing_eagerly()

gpus_p = tf.config.experimental.list_physical_devices('GPU')
if gpus_p:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus_p:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)

@tf.function
def sony_forward_tf_function(x,d,xmax):
    return d * tf.round(tf.clip_by_value(x , -xmax , xmax ) / d)


def sony_forward(x,d,xmax):
    return d * tf.round(tf.clip_by_value(x , -xmax , xmax ) / d)


x = tf.random.uniform(shape=[10000,1000])
times = {'tf':[],'tf_function':[],'sony':[],'sony_function':[]}
xmax = tf.Variable(0.5)
step_size = xmax / (tf.pow(2.0, tf.cast(8,dtype=tf.float32) - 1.0) - 1.0)

sony_forward(x,-xmax,xmax)
sony_forward_tf_function(x,-xmax,xmax)

for n in range(1000):
    start = time.time()
    x = sony_forward(x,step_size,xmax)
    stop = time.time()
    times['sony'].append(stop-start)

for n in range(1000):
    start = time.time()
    x = sony_forward_tf_function(x,step_size,xmax)
    stop = time.time()
    times['sony_function'].append(stop-start)
    
print('Sony forward ' + str(1000 * np.mean(times['sony'])) + ' ms')
print('Sony forward w/ tf.function ' + str(1000 * np.mean(times['sony_function'])) + ' ms')
`

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35719,parallel_for: No converter defined for MatrixSolve,"I need to calculate the Jacobian of a function, where `tf.linalg.solve` is part of the function. I can usually use `parallel_for` but `pfor` does not support the `MatrixSolve` op, requiring a fallback to a slow while loop. It would be great to add a converter for `MatrixSolve` to `parallel_for.` Note that below, my example can be easily mathematically simplified, but in my actually use case it cannot be.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0
- Python version: 3.7.3

**Code to reproduce the issue**

```python
import tensorflow as tf
from tensorflow.python.ops.parallel_for.gradients import jacobian
import numpy as np

sess = tf.InteractiveSession()
print(tf.__version__)

x = tf.compat.v1.placeholder(tf.float64, shape = [3])
y = tf.compat.v1.placeholder(tf.float64, shape = [3])
z = tf.reshape(tf.linalg.solve(tf.linalg.diag(x), tf.reshape(y, [-1, 1])), [-1])

jac = jacobian(z, x)

print(sess.run(jac, feed_dict = {x: np.array([1., 1., 1.]), y: np.array([1., 2., 3.])}))
```

**Describe the current behavior**

```python
ValueError: No converter defined for MatrixSolve
name: ""loop_body/gradients/MatrixSolve_grad/MatrixSolve""
op: ""MatrixSolve""
input: ""MatrixDiag""
input: ""loop_body/gradients/Reshape_1_grad/Reshape""
attr {
  key: ""T""
  value {
    type: DT_DOUBLE
  }
}
attr {
  key: ""adjoint""
  value {
    b: true
  }
}

inputs: [WrappedTensor(t=<tf.Tensor 'MatrixDiag:0' shape=(3, 3) dtype=float64>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'loop_body/gradients/Reshape_1_grad/Reshape/pfor/Reshape:0' shape=(3, 3, 1) dtype=float64>, is_stacked=True, is_sparse_stacked=False)]. 
Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower
```

**Describe the expected behavior**

I should get
```
array([[-1.,  0.,  0.],
       [ 0., -2.,  0.],
       [ 0.,  0., -3.]])
```
"
35718,Fake quantization is much slower when decorated with tf.function,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v1.12.1-21967-gd80fda0 2.1.0-dev20200109
- Python version: 3.6.9
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: CUDA Version 10.1.243 / cuDNN 7.6.4.38-1
- GPU model and memory: TITAN V, 12GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Executing `tf.quantization.fake_quant_with_min_max_vars` within a `tf.function` decorator is much slower than executing without the decorator. On my system, the code below prints:

TF forward 0.27578210830688477 ms
TF forward w/ tf.function 0.4100463390350342 ms

**Describe the expected behavior**
I expect the speed to be the same.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
`
import tensorflow as tf
import numpy as np
import time

assert tf.executing_eagerly()

gpus_p = tf.config.experimental.list_physical_devices('GPU')
if gpus_p:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus_p:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)

@tf.function
def tf_forward_tf_function(x,xmin,xmax):
    return tf.quantization.fake_quant_with_min_max_vars(inputs=x,
                                                        min=xmin,
                                                        max=xmax,
                                                        num_bits=8)

def tf_forward(x,xmin,xmax):
    return tf.quantization.fake_quant_with_min_max_vars(inputs=x,
                                                        min=xmin,
                                                        max=xmax,
                                                        num_bits=8)

x = tf.random.uniform(shape=[10000,1000])
times = {'tf':[],'tf_function':[]}
xmax = tf.Variable(0.5)

tf_forward(x,-xmax,xmax)
tf_forward_tf_function(x,-xmax,xmax)

for n in range(1000):
    start = time.time()
    tf_forward(x,-xmax,xmax)
    stop = time.time()
    times['tf'].append(stop-start)

    start = time.time()
    tf_forward_tf_function(x,-xmax,xmax)
    stop = time.time()
    times['tf_function'].append(stop-start)
    
print('TF forward ' + str(1000 * np.mean(times['tf'])) + ' ms')
print('TF forward w/ tf.function ' + str(1000 * np.mean(times['tf_function'])) + ' ms')   

`

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35717,TensorFlow<1.15 documentation redirects to GitHub,"## URL(s) with the issue:

For example,
https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/train
https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/estimator/Estimator
https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/Model

## Description of issue (what needs changing):

The links above currently redirect to GitHub. The 1.15 links work:

https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train
https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/estimator/Estimator
https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/Model

I have projects using TensorFlow 1.14, so I would like to use the 1.14 docs for reference.

Will the 1.14 docs be back up?
"
35713,No gradient in custom loss function,"I tried to create a custom loss function using spearman correlation in tensorflow 2.1.0-rc1. However, I've encountered ""no gradient"" error. I have multiple targets. Here is my code:
```
import tensorflow as tf
from scipy.stats import rankdata
BATCH_SIZE = 4

def custom_loss(num_targets):
    def custom_rhos(y_true, y_pred):
        rhos = []
        for ind in range(num_targets):
            a = tf.squeeze(tf.slice(y_true, [0, ind], [-1, 1]))
            b = tf.squeeze(tf.slice(y_pred, [0, ind], [-1, 1]))
            rank_a, rank_b = tf.numpy_function(rankdata, [a],  Tout=tf.float32), tf.numpy_function(rankdata, [b],  Tout=tf.float32)
            rho = 1 - 6 * tf.reduce_sum((rank_a - rank_b) ** 2) / (BATCH_SIZE ** 3 - BATCH_SIZE)
            rhos.append(rho)
        return -tf.reduce_sum(rhos)
    return custom_rhos
```

It keeps giving the erros as following:

```
/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in _filter_grads(grads_and_vars)
   1037   if not filtered:
   1038     raise ValueError(""No gradients provided for any variable: %s."" %
-> 1039                      ([v.name for _, v in grads_and_vars],))
   1040   if vars_with_empty_grads:
   1041     logging.warning(

ValueError: No gradients provided for any variable:
```
"
35712,tf.distribute.experimental.TPUStrategy doesn't render Stable correctly,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/TPUStrategy?version=stable

## Description of issue (what needs changing):

On the documentation page for tf.distribute.experimental.TPUStrategy, the Stable documentation is shown as the raw text of the documentation (looks like it's a combination of HTML and Markdown?).

Example below:

<img width=""1664"" alt=""Screen Shot 2020-01-09 at 1 22 11 PM"" src=""https://user-images.githubusercontent.com/11432284/72094036-87786200-32e3-11ea-89ca-253b45a5ad7b.png"">

Clicking ""See Nightly"" it renders correctly, but clicking ""See Stable"" again it still shows the raw text again.
"
35711,unknown option '--proto-format' when make a TensorFlow Lite AAR,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I nearly finished the build ！but finally met this error

```
ERROR: /Users/norly/Documents/github/android/tensorflow-r1.15/tensorflow/lite/java/BUILD:28:1: Processing Android resources for //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops_dummy_app_for_so failed (Exit 1): ResourceProcessorBusyBox failed: error executing command 
  (cd /private/var/tmp/_bazel_root/c472abc5e7a3fc0cf3c1a120192310f1/execroot/org_tensorflow && \
  exec env - \
    ANDROID_BUILD_TOOLS_VERSION=26.0.2 \
    ANDROID_NDK_API_LEVEL=18 \
    ANDROID_NDK_HOME=/Users/norly/Documents/github/android/android-ndk-r17c \
    ANDROID_SDK_API_LEVEL=26 \
    ANDROID_SDK_HOME=/Users/norly/library/Android/Sdk \
    PATH='/Applications/anaconda3/envs/android/bin:/Users/norly/opt/anaconda3/condabin:/anaconda3/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Applications/VMware Fusion.app/Contents/Public:/Library/TeX/texbin:usr/bin:/Library/Java/JavaVirtualMachines/jdk1.8.0_171.jdk/Contents/Home/bin:Users/daredos/Swig/bin:/Users/norly/bin' \
    PYTHON_BIN_PATH=/Applications/anaconda3/envs/android/bin/python \
    PYTHON_LIB_PATH=/Applications/anaconda3/envs/android/lib/python3.6/site-packages \
    TF_CONFIGURE_IOS=0 \
  bazel-out/host/bin/external/bazel_tools/src/tools/android/java/com/google/devtools/build/android/ResourceProcessorBusyBox --tool AAPT2_PACKAGE -- --aapt2 bazel-out/host/bin/external/androidsdk/aapt2_binary --directData ::bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/java/tensorflowlite_flex_processed_manifest/AndroidManifest.xml:bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/java/tensorflowlite_flex_symbols/R.aapt2.txt:bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/java/tensorflowlite_flex_symbols/symbols.zip --useCompiledResourcesForMerge --primaryData ::bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/java/_merged/tensorflow-lite-with-select-tf-ops_dummy_app_for_so/AndroidManifest.xml --buildToolsVersion 26.0.2 --androidJar external/androidsdk/platforms/android-26/android.jar --rOutput bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/java/tensorflow-lite-with-select-tf-ops_dummy_app_for_so_symbols/R.txt --symbolsOut bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/java/tensorflow-lite-with-select-tf-ops_dummy_app_for_so_symbols/merged.bin --srcJarOutput bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/java/tensorflow-lite-with-select-tf-ops_dummy_app_for_so.srcjar --proguardOutput bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/java/proguard/tensorflow-lite-with-select-tf-ops_dummy_app_for_so/_tensorflow-lite-with-select-tf-ops_dummy_app_for_so_proguard.cfg --mainDexProguardOutput bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/java/proguard/tensorflow-lite-with-select-tf-ops_dummy_app_for_so/main_dex_tensorflow-lite-with-select-tf-ops_dummy_app_for_so_proguard.cfg --manifestOutput bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/java/tensorflow-lite-with-select-tf-ops_dummy_app_for_so_processed_manifest/AndroidManifest.xml --resourcesOutput bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/java/tensorflow-lite-with-select-tf-ops_dummy_app_for_so_files/resource_files.zip --packagePath bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/java/tensorflow-lite-with-select-tf-ops_dummy_app_for_so.ap_ --packageForR dummy.package.for.so)
Execution platform: @bazel_tools//platforms:host_platform
1月 10, 2020 2:04:41 上午 com.google.devtools.build.android.ResourceProcessorBusyBox processRequest
严重: Error during processing
java.lang.RuntimeException: Error during Linking bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/java/_merged/tensorflow-lite-with-select-tf-ops_dummy_app_for_so/AndroidManifest.xml:
Command: bazel-out/host/bin/external/androidsdk/aapt2_binary\
	link\
	--no-version-vectors\
	--no-static-lib-packages\
	--manifest\
	bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/java/_merged/tensorflow-lite-with-select-tf-ops_dummy_app_for_so/AndroidManifest.xml\
	--auto-add-overlay\
	--proto-format\
	--custom-package\
	dummy.package.for.so\
	-I\
	external/androidsdk/platforms/android-26/android.jar\
	-R\
	/var/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T/android_resources_tmp17238626937439459235/linked/filtered/bazel-out/android-armeabi-v7a-opt/bin/tensorflow/lite/java/tensorflowlite_flex_symbols/symbols.zip\
	-R\
	/var/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T/android_resources_tmp17238626937439459235/linked/filtered/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T/android_resources_tmp17238626937439459235/compiled/compiled.zip\
	-0\
	.apk\
	--output-text-symbols\
	/var/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T/android_resources_tmp17238626937439459235/linked/R.txt\
	--emit-ids\
	/var/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T/android_resources_tmp17238626937439459235/linked/ids.txt\
	--java\
	/var/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T/android_resources_tmp17238626937439459235/linked/java\
	--proguard\
	/var/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T/android_resources_tmp17238626937439459235/linked/proguard.cfg\
	--proguard-main-dex\
	/var/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T/android_resources_tmp17238626937439459235/linked/proguard.maindex.cfg\
	-o\
	/var/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T/android_resources_tmp17238626937439459235/linked/bin.-pb.apk
Output:
unknown option '--proto-format'.

aapt2 link [options] -o arg --manifest arg files...

Options:
 -o arg                                            Output path.
 --manifest arg                                    Path to the Android manifest to build.
 -I arg                                            Adds an Android APK to link against.
 -A arg                                            An assets directory to include in the APK. These are unprocessed.
 -R arg                                            Compilation unit to link, using `overlay` semantics.
                                                   The last conflicting resource given takes precedence.
 --package-id arg                                  Specify the package ID to use for this app. Must be greater or equal to
                                                   0x7f and can't be used with --static-lib or --shared-lib.
 --java arg                                        Directory in which to generate R.java.
 --proguard arg                                    Output file for generated Proguard rules.
 --proguard-main-dex arg                           Output file for generated Proguard rules for the main dex.
 --no-auto-version                                 Disables automatic style and layout SDK versioning.
 --no-version-vectors                              Disables automatic versioning of vector drawables. Use this only
                                                   when building with vector drawable support library.
 --no-version-transitions                          Disables automatic versioning of transition resources. Use this only
                                                   when building with transition support library.
 --no-resource-deduping                            Disables automatic deduping of resources with
                                                   identical values across compatible configurations.
 --enable-sparse-encoding                          Enables encoding sparse entries using a binary search tree.
                                                   This decreases APK size at the cost of resource retrieval performance.
 -x                                                Legacy flag that specifies to use the package identifier 0x01.
 -z                                                Require localization of strings marked 'suggested'.
 -c arg                                            Comma separated list of configurations to include. The default
                                                   is all configurations.
 --preferred-density arg                           Selects the closest matching density and strips out all others.
 --product arg                                     Comma separated list of product names to keep
 --output-to-dir                                   Outputs the APK contents to a directory specified by -o.
 --no-xml-namespaces                               Removes XML namespace prefix and URI information from
                                                   AndroidManifest.xml and XML binaries in res/*.
 --min-sdk-version arg                             Default minimum SDK version to use for AndroidManifest.xml.
 --target-sdk-version arg                          Default target SDK version to use for AndroidManifest.xml.
 --version-code arg                                Version code (integer) to inject into the AndroidManifest.xml if none is
                                                   present.
 --version-name arg                                Version name to inject into the AndroidManifest.xml if none is present.
 --shared-lib                                      Generates a shared Android runtime library.
 --static-lib                                      Generate a static Android library.
 --no-static-lib-packages                          Merge all library resources under the app's package.
 --non-final-ids                                   Generates R.java without the final modifier. This is implied when
                                                   --static-lib is specified.
 --stable-ids arg                                  File containing a list of name to ID mapping.
 --emit-ids arg                                    Emit a file at the given path with a list of name to ID mappings,
                                                   suitable for use with --stable-ids.
 --private-symbols arg                             Package name to use when generating R.java for private symbols.
                                                   If not specified, public and private symbols will use the application's
                                                   package name.
 --custom-package arg                              Custom Java package under which to generate R.java.
 --extra-packages arg                              Generate the same R.java but with different package names.
 --add-javadoc-annotation arg                      Adds a JavaDoc annotation to all generated Java classes.
 --output-text-symbols arg                         Generates a text file containing the resource symbols of the R class in
                                                   the specified folder.
 --auto-add-overlay                                Allows the addition of new resources in overlays without
                                                   <add-resource> tags.
 --rename-manifest-package arg                     Renames the package in AndroidManifest.xml.
 --rename-instrumentation-target-package arg       Changes the name of the target package for instrumentation. Most useful
                                                   when used in conjunction with --rename-manifest-package.
 -0 arg                                            File extensions not to compress.
 --split arg                                       Split resources matching a set of configs out to a Split APK.
                                                   Syntax: path/to/output.apk:<config>[,<config>[...]].
                                                   On Windows, use a semicolon ';' separator instead.
 -v                                                Enables verbose logging.
 -h                                                Displays this help menu

	at com.google.devtools.build.android.AaptCommandBuilder.execute(AaptCommandBuilder.java:318)
	at com.google.devtools.build.android.aapt2.ResourceLinker.linkProtoApk(ResourceLinker.java:423)
	at com.google.devtools.build.android.aapt2.ResourceLinker.link(ResourceLinker.java:531)
	at com.google.devtools.build.android.Aapt2ResourcePackagingAction.main(Aapt2ResourcePackagingAction.java:185)
	at com.google.devtools.build.android.ResourceProcessorBusyBox$Tool$14.call(ResourceProcessorBusyBox.java:144)
	at com.google.devtools.build.android.ResourceProcessorBusyBox.processRequest(ResourceProcessorBusyBox.java:240)
	at com.google.devtools.build.android.ResourceProcessorBusyBox.main(ResourceProcessorBusyBox.java:203)
```


**Provide the exact sequence of commands / steps that you executed before running into the problem**

.tf_configure.bazelrc
```
build --action_env PYTHON_BIN_PATH=""/Applications/anaconda3/envs/android/bin/python""
build --action_env PYTHON_LIB_PATH=""/Applications/anaconda3/envs/android/lib/python3.6/site-packages""
build --python_path=""/Applications/anaconda3/envs/android/bin/python""
build:xla --define with_xla_support=true
build:opt --copt=-march=native
build:opt --copt=-Wno-sign-compare
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build:v2 --define=tf_api_version=2
build --action_env ANDROID_NDK_HOME=""/Users/norly/Documents/github/android/android-ndk-r17c""
build --action_env ANDROID_NDK_API_LEVEL=""18""
build --action_env ANDROID_BUILD_TOOLS_VERSION=""26.0.2""
build --action_env ANDROID_SDK_API_LEVEL=""26""
build --action_env ANDROID_SDK_HOME=""/Users/norly/library/Android/Sdk""
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_tag_filters=-benchmark-test,-no_oss,-oss_serial
test --build_tag_filters=-benchmark-test,-no_oss
test --test_tag_filters=-gpu,-nomac,-no_mac
test --build_tag_filters=-gpu,-nomac,-no_mac
build --action_env TF_CONFIGURE_IOS=""0""
```

`sudo bazel build --cxxopt='--std=c++11' -c opt --config=android_arm --config=monolithic //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35710,tf.range fails when `limit` is type of `tf.int32` and `dtype` is `tf.int64`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos Catalina
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `2.1.0`
- Python version: `3.7`
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
The behavior of `tf.range` changed between `2.0.0` and `2.1.0`, such that `tf.range(limit, dtype=dtype)` fails when `limit` is type of `tf.int32` and `dtype` is `tf.int64`. Not sure if this is a bug or a feature but I would expect this to still work.

The documentation nor the `2.1.0` release notes don't explicitly mention anything about this.

**Describe the expected behavior**
The behavior as it was in `2.0.0`, i.e. no exception is raised.

**Code to reproduce the issue**
```python
import tensorflow as tf
tf.range(tf.constant(4, dtype=tf.int32), dtype=tf.int64)
```

**Other info / logs**

With `tensorflow == 2.1.0`:
```bash
$ python -c ""import tensorflow as tf; print(tf.__version__); print(tf.range(tf.constant(4, dtype=tf.int32), dtype=tf.int64))""
2.1.0
2020-01-09 16:45:39.137901: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-09 16:45:39.151651: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa652c190b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-01-09 16:45:39.151667: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/Users/hartikainen/conda/envs/bae/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py"", line 1430, in range
    limit = ops.convert_to_tensor(limit, dtype=dtype, name=""limit"")
  File ""/Users/hartikainen/conda/envs/bae/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1290, in convert_to_tensor
    (dtype.name, value.dtype.name, value))
ValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: <tf.Tensor: shape=(), dtype=int32, numpy=4>
```

With `tensorflow==2.0.0`
```bash
$ python -c ""import tensorflow as tf; print(tf.__version__); print(tf.range(tf.constant(4, dtype=tf.int32), dtype=tf.int64))""
2.0.0
2020-01-09 16:40:11.425955: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-09 16:40:11.439063: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8c6ccdfd00 executing computations on platform Host. Devices:
2020-01-09 16:40:11.439079: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)
```
"
35709,TensorFlow 2.1.0 requires scipy,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.1.0
- Python version: 3.6.10
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

TensorFlow 2.1.0 added `scipy` as a required dependency.  However, as far as I can tell TensorFlow doesn't actually need scipy. Judging from this PR https://github.com/tensorflow/tensorflow/pull/35278 (which introduced the requirement), the intention was just to avoid a bug with scipy==1.4.0. But it seems that simply not having scipy installed would be another way to avoid that bug, and adding scipy as a requirement to every tensorflow installation seems like kind of a drastic solution. It also wasn't mentioned in the release notes at all, so I'm wondering whether this was an intentional change or not.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`pip install tensorflow`
"
35708,tf2.0 & 2.1 in colab tpu get 'Compilation failure: Asked to propagate a dynamic dimension from..' error,"def simple_residual_block(input, weight_decay=1e-4):

    input_channels = input.get_shape().as_list()[-1]

    x = BatchNormalization()(input)
    x = Activation('relu')(x)
    x = Convolution2D(input_channels// 4, 1, 1, kernel_regularizer=l2(weight_decay))(x)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Convolution2D(input_channels // 4, 3, 1, padding='same', kernel_regularizer=l2(weight_decay))(x)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Convolution2D(input_channels, 1, 1, kernel_regularizer=l2(weight_decay))(x)

    x = add([x, input])
    return x

def attention_block(input, encoder_depth, weight_decay=1e-4):

    assert input.get_shape().as_list()[-2]>=math.pow(2,encoder_depth)*8, 'input width should >= pow(2,encoder_depth)*8'

    input_channels = input.get_shape().as_list()[-1]
    input = simple_residual_block(input, weight_decay)
    output_trunk = simple_residual_block(input, weight_decay)
    output_trunk = simple_residual_block(output_trunk, weight_decay)
    output_soft_mask = MaxPooling2D(padding='same')(input)
    output_soft_mask = simple_residual_block(output_soft_mask, weight_decay)

    skip_connections = []

    for i in range(encoder_depth):
        output_skip_connection = simple_residual_block(output_soft_mask, weight_decay)
        skip_connections.append(output_skip_connection)
        output_soft_mask = MaxPooling2D(padding='same')(output_soft_mask)
        output_soft_mask = simple_residual_block(output_soft_mask, weight_decay)

    skip_connections = list(reversed(skip_connections))

    for i in range(encoder_depth):
        output_soft_mask = simple_residual_block(output_soft_mask, weight_decay)
        output_soft_mask = UpSampling2D()(output_soft_mask)
        output_soft_mask = add([output_soft_mask, skip_connections[i]])

    output_soft_mask = simple_residual_block(output_soft_mask, weight_decay)
    output_soft_mask = UpSampling2D()(output_soft_mask)

    output_soft_mask = Convolution2D(input_channels, 1, 1)(output_soft_mask)
    output_soft_mask = Convolution2D(input_channels, 1, 1)(output_soft_mask)
    output_soft_mask = Activation('sigmoid')(output_soft_mask)

    output = Lambda(lambda x: x + 1)(output_soft_mask)
    output = Multiply()([output, output_trunk])
    output = simple_residual_block(output, weight_decay)

    return output


 call attention_block with encoder_depth >1 get this err, 

and setting 'strategy.experimental_enable_dynamic_batch_size = False' do not work





Epoch 1/200
496/497 [============================>.] - ETA: 0s - loss: 4.5875 - accuracy: 0.2602
---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
<ipython-input-1-db2f1c8a2097> in <module>()
    188                                 validation_data=(x_validation, y_validation),
    189                                 validation_steps=x_validation.shape[0] // batch_size,
--> 190                                 shuffle=True)
    191             show_train_history(history, 'accuracy', 'val_accuracy')
    192             show_train_history(history, 'loss', 'val_los')

11 frames
/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

UnimplementedError:  Compilation failure: Asked to propagate a dynamic dimension from hlo %add.5706 = f32[128,8,8,64]{3,2,1,0} add(f32[128,8,8,64]{3,2,1,0} %add.5705, f32[128,8,8,64]{3,2,1,0} %add.5678), metadata={op_type=""AddV2"" op_name=""model/add_8/add""}@{}@0 to hlo %custom-call.5725 = f32[128,16,16,64]{3,2,1,0} custom-call(f32[128,8,8,64]{3,2,1,0} %add.5706), custom_call_target=""ResizeNearest"", metadata={op_type=""ResizeNearestNeighbor"" op_name=""model/up_sampling2d/resize/ResizeNearestNeighbor""}, backend_config=""\""01\"""", which is not implemented.
	TPU compilation failed
	 [[{{node tpu_compile_succeeded_assert/_8555495338583930365/_5}}]]
Additional GRPC error information:
{""created"":""@1578578190.016458329"",""description"":""Error received from peer"",""file"":""external/grpc/src/core/lib/surface/call.cc"",""file_line"":1039,""grpc_message"":"" Compilation failure: Asked to propagate a dynamic dimension from hlo %add.5706 = f32[128,8,8,64]{3,2,1,0} add(f32[128,8,8,64]{3,2,1,0} %add.5705, f32[128,8,8,64]{3,2,1,0} %add.5678), metadata={op_type=""AddV2"" op_name=""model/add_8/add""}@{}@0 to hlo %custom-call.5725 = f32[128,16,16,64]{3,2,1,0} custom-call(f32[128,8,8,64]{3,2,1,0} %add.5706), custom_call_target=""ResizeNearest"", metadata={op_type=""ResizeNearestNeighbor"" op_name=""model/up_sampling2d/resize/ResizeNearestNeighbor""}, backend_config=""\""01\"""", which is not implemented.\n\tTPU compilation failed\n\t [[{{node tpu_compile_succeeded_assert/_8555495338583930365/_5}}]]"",""grpc_status"":12} [Op:__inference_distributed_function_203353]

Function call stack:
distributed_function -> distributed_function
"
35707,Failed to load model from file:///android_asset/frozen_inference_graph.pb',"Hi guys, 

I am trying to run the trained model on my android device, but it's crashing on opening. 

When i checked my logcat, i have the following screenshot:
<img width=""1364"" alt=""Screen Shot 2020-01-09 at 5 47 51 PM"" src=""https://user-images.githubusercontent.com/22390818/72073182-d7b5eb00-3308-11ea-996d-b508710f75c4.png"">
Process: org.tensorflow.demo, PID: 3797 java.lang.RuntimeException: Failed to load model from 'file:///android_asset/frozen_inference_graph.pb'.

I have tried so many things like changing my TF version to match the tensorflow android build version but all to no avail. 

And also, sometimes the logcat will log an error message relating to something like `Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary`. 

I would have matched my android build.gradle dependencies with my current TF version but the latest tensorflow-android is 1.13.1.

My model was trained on TF 1.15.0
macOS Mojave 10.14
TF Version: 1.15.0
Model used: ssd_mobilenet_v2_quantized_300*300_coco and ssd_inception_v2_coco

Please help, I have used more than 24 hours in looking for the solution. 
"
35706,Init operations did not make model ready. When import form .pbtxt,"**System information**
- Ubuntu 16.04:
- TensorFlow installed from (source or binary): pip binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.5.2
- CUDA/cuDNN version: 10.0 / 7.5
- GPU model and memory: NVIDIA RTX 2080TI

I am trying the tf.io.write_graph() and tf.import_graph_def API to implement model saving and re-building. And I encountered ""Init operations did not make model ready"" when importing the graph_def into MonitoredSession()

**Here is the importing related source code:**

```
# Some code to load graph_def from .pbtxt file
tf.reset_default_graph()
tf.import_graph_def(graph_def, name='')
with tf.train.MonitoredTrainingSession() as sess:
```

**The bug information is:**
RuntimeError: Init operations did not make model ready.  Init op: group_deps, init fn: None, local_init_op: name: ""group_deps_1""
op: ""NoOp""
input: ""^init_2""
input: ""^init_all_tables""
input: ""^init_3""
, error: Variables not initialized: global_step, ......"
35705,AttributeError: _ckpt_saved_epoch when using MultiWorkerMirroredStrategy TF2.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
pip
- TensorFlow version (use command below):
v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: 
3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
10.0, 7.6.2
- GPU model and memory:
GeForce GTX 1070 and GeForce GTX 1080

**Describe the current behavior**
Crash when writing Checkpoint (or stuck when writing log for tensorboard)

**Describe the expected behavior**
Write a checkpoint and a Tensorboardlog

**Code to reproduce the issue**
```
os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        ""worker"": [""10.10.1.168:1234""],
        'chief': [""10.10.1.60:2345""]
    },
    'task': {'type': 'chief', 'index': 0}
})
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

def get_label(file_path, class_names):
  parts = tf.strings.split(file_path, os.path.sep)
  return parts[-2] == class_names

def parse_image(filename):
    parts = tf.strings.split(filename, ""\\"")
    label = get_label(filename, CLASS_NAMES)
    image = tf.io.read_file(filename)
    image = tf.image.decode_png(image, channels=3)
    image = tf.image.convert_image_dtype(image, tf.float32)
    image = tf.image.resize(image, [299,299])
    return image, label

def make_dataset_unbatched():
    images_ds = list_ds.map(parse_image, num_parallel_calls=AUTOTUNE)
    images_ds = images_ds.shuffle(BATCH_SIZE)
    images_ds = images_ds.repeat(epochs)
    images_ds = images_ds.prefetch(BUFFER_SIZE)
    return images_ds

datasetFilePath = ""D:\TrainData\BalancedData""
IMAGESIZE = 299
AUTOTUNE = tf.data.experimental.AUTOTUNE
datasetPath = pathlib.Path(datasetFilePath)
list_ds = tf.data.Dataset.list_files(str(datasetPath/""*/*""))
num_elements = tf.data.experimental.cardinality(list_ds).numpy()

CLASS_NAMES = np.array([item.name for item in datasetPath.glob('*')])

epochs = 2
def build_and_compile_model():
    base_model =tf.keras.applications.InceptionV3(include_top=False, weights = ""imagenet"", input_shape=(299,299,3))

    base_model.trainable = True
    x = base_model.output
    x = tf.keras.layers.GlobalAveragePooling2D(name=""avg_pool"")(x)
    x = tf.keras.layers.Dense(256, activation=""relu"")(x)
    predictions = tf.keras.layers.Dense(2, activation=""softmax"")(x)
    model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)

    base_learning_rate = 0.00001
    model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),
                 loss=""categorical_crossentropy"",
                 metrics=[""accuracy""])
    return model
logdir = os.path.join(""Z:\Tensorflow\TensorboardLogs"", datetime.datetime.now().strftime(""%Y%m%d-%H%M%S""))
callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=""Z:\Tensorflow\Checkpoints""), 
             tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)]

with strategy.scope():
    dataset = make_dataset_unbatched().batch(BATCH_SIZE, drop_remainder=True)
    multi_worker_model = build_and_compile_model()

history = multi_worker_model.fit(dataset, epochs=epochs, steps_per_epoch=50, callbacks=callbacks)
``` 

**Other info / logs**
[ckpt_error.txt](https://github.com/tensorflow/tensorflow/files/4040557/ckpt_error.txt)

This log happens with the checkpoint in the code, and with only the tensorboard-log as checkpoint the chief stops right at the end of the first epoch and nothing else happens.

I hope someone can help me with this.
"
35703,Problem on building tensorflow optimized for AVX2  CPU,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : **Debian GNU/Linux 10 (buster)**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): **source**
- TensorFlow version: **2.1**
- Python version: **3.6.9**
- Installed using virtualenv? pip? conda?: **N/A**
- Bazel version (if compiling from source): **1.2.1**
- GCC/Compiler version (if compiling from source): **8.3.0**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**

**Describe the problem**

Hi everybody,

I tried to build Tensorflow from the source as the way to deal with problem **Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2** since the existing installation of tensorflow by pip install does not allow the optimization for using in my laptop that use CPU that use AVX2.

However, the bazel compiler keeps giving me a headache of Fatal Failed to sync ... Permission denied. as shown in the command and  jpeg file I attached

**Provide the exact sequence of commands / steps that you executed before running into the problem**

> FATAL: failed to sync '/root/.cache/bazel/_bazel_root/install/84defa6eb1e9416bf92d6f89ab2d4f31.tmp.10122/A-server.jar': (error: 13): Permission denied

How can I deal with this kind of troubles on building and installing tensorflow optimized for my CPU? Please help me out. 

BTW, I run tensorflow through docker container since my Laptop OS is Windows 10 though. 
![ProblemonCompilingTensorFlowfromSource](https://user-images.githubusercontent.com/58510558/72069742-6d0dab00-331a-11ea-8a5f-6e0b02b09572.jpg)





"
35702,Tensorflow 2.1.0 failed to list GPU devices and detect GPU devices automatically,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04.3 LTS
- TensorFlow installed from (source or binary): pip3 install tensorflow --upgrade
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.5
- CUDA/cuDNN version: 10.2
- GPU model and memory: GTX2080Ti 11GB

**Describe the current behavior**
I try to implement the official distributed training example by using `strategy = tf.distribute.MirroredStrategy()` to detect the GPU devices. It is supposed to detect all gpu devices automatically. However, when I use this API under my environment, it can only find the CPU devices. 
**Describe the expected behavior**

It should have the same output like the Tensorflow 2.0.0b1.
**Code to reproduce the issue**

```
import tensorflow as tf 
strategy = tf.distribute.MirroredStrategy()
print(""Number of devices: {}"".format(strategy.num_replicas_in_sync))
```

**Other info / logs**
I also find that the API `tf.config.experimental.list_physical_devices('GPU')` cannot list the GPU devices. After I check, my gpu device type is XLA_GPU. But I can use the same API to list all my GPU devices.
"
35701,[TF2.0] Build from sources to support CUDA9,"As we all know ，the tf2-cuda9.0 haven't the pkg by official.
But in some server， we can't update the GPU driver to support CUDA10. 
So we need to bazel the pkg by ourself.
I bazeled the  tensorflow2.0 pkgs that support cuda9.0 for py3.5 3.6 3.7 . 
I hope it could help more people.

But ，sometimes ，it will have this error.

```shell
W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.9.0';.....undefined symbol: GOMP_critical_end;

```

and I find if you add the two lines of code, it will work.

```python
import ctypes
ctypes.CDLL(""libgomp.so.1"", mode=ctypes.RTLD_GLOBAL)
```

I'm not sure if these two lines of code have some side effects.
So I need the official to explain how to avoid this error，and the  side effects will  bring by the two lines of code.

And the pkg and installation method in this link.
https://github.com/SmileTM/Tensorflow2.X-GPU-CUDA9.0"
35700,tensorflow2.1 can not initialize colab TPU ,"    if '2.1' in tf.__version__:
        if 'COLAB_TPU_ADDR' in os.environ:
            resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
            tf.config.experimental_connect_to_cluster(resolver)
            tf.tpu.experimental.initialize_tpu_system(resolver)
            strategy = tf.distribute.experimental.TPUStrategy(resolver)
            print('Running on TPU ', resolver.cluster_spec().as_dict())




NotFoundError                             Traceback (most recent call last)
<ipython-input-1-53f26737ac5e> in <module>()
    160             resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
    161             tf.config.experimental_connect_to_cluster(resolver)
--> 162             tf.tpu.experimental.initialize_tpu_system(resolver)
    163             strategy = tf.distribute.experimental.TPUStrategy(resolver)
    164             print('Running on TPU ', resolver.cluster_spec().as_dict())

3 frames
/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

NotFoundError: '__inference__tpu_init_fn_4' is neither a type of a primitive operation nor a name of a function registered in binary running on n-3ea5ef93-w-0. Make sure the operation or function is registered in the binary running in this process."
35699,Dangling pointer  through IntArrayFromInitializer causing Segfault in Tests,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 19.10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): source

- TensorFlow version (use command below): 2.1
- Python version: 3.6
- Bazel version (if compiling from source): 1.2.1
- GCC/Compiler version (if compiling from source): 9.2.1
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**
The IntArrayFromInitializer routine maintains a reference to the initializer_list's data.
The initializer_list is destroyed at the end of the function's scope as it is not a reference, and there is no guarantee that the initializer_list will exist beyond the function's scope.
This causes Segfaults and Corruption of the memory being referenced as revealed from testing ""tensorflow/lite/micro/kernels:elementwise_test"". This is illustrated in the screenshot attached below.

**Describe the expected behavior**
There should be an established certainty that the data being pointed to will be valid beyond the function scope. This can be achieved by using C++ constructs; concrete types instead of the error-prone C-style pointer constructs.

**Code to reproduce the issue**
test cases available under:
tensorflow/lite/micro/kernels/elementwise_test.cc

**Other info / logs**
See screenshots attached below:

The culprit (tensorflow/lite/micro/testing/test_utils.h):
![Screenshot from 2020-01-09 11-06-37](https://user-images.githubusercontent.com/26050398/72058282-24310480-32d0-11ea-940e-a1faf1fbc17e.png)

The test log (tensorflow/lite/micro/kernels/elementwise_test.cc):
![Screenshot from 2020-01-09 11-02-22](https://user-images.githubusercontent.com/26050398/72058451-66f2dc80-32d0-11ea-9059-d85ae50b6fa5.png)"
35697,"tensorflow.python.framework.errors_impl.CancelledError: Cancelled, When do distributed training with parameter-server","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 14.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.13.2
- Python version: 2.7
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source): 4.8.4
- CUDA/cuDNN version: no CUDA
- GPU model and memory: no GPU

**Describe the current behavior**
When do **distribute training** using **parameter-server strategy**, there are some workers failed with the information `tensorflow.python.framework.errors_impl.CancelledError: Cancelled` **occasionally**.

There is no more information, so I have no idea how to debug the error. Is there anyone has seen this error or how to debug?  appreciate your kind help.

**Code to reproduce the issue**
It's an occasional problem, so it's not easy to reproduce.

**Other info / logs**
```
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 676, in run 
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1171, in run 
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1270, in run 
    raise six.reraise(*original_exc_info)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1255, in run 
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1327, in run 
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1091, in run 
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 929, in run 
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.CancelledError: Cancelled
```"
35696,About symbols' visibility in shared library,"**System information**
- TensorFlow version (you are using):  v2.0.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
As we can see from [tf_version_script.lds in v2.0.0](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/tf_version_script.lds), none of `Eager*` symbols have been set to `global visibility`. But in newest master codes, this has been [updated](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tf_version_script.lds#L7).

In our usage scenario, we need transfer `EagerTensor_Handle` in our python apis to read or update the tensors. It's important that no copy happens in Tensor transfering. However, `EagerTensor*` like symbols depend on `_pywrap_tensorflow_internal.so` are `Local` visibility in current stable release.

**Will this change the current api? How?**
No. Only version script file: tf_version_script.lds 

**Who will benefit with this feature?**
The one who need read or write Tensors outside tensorflow.

**And more**
Maybe it's not good to update symbols' stripping rule in `v2.0.0` release (or other tags). But this have changed in master branch. When will you release this feature in next edition? Or is this a good way to interchange data between outerside and tf (**with zero copy**)?"
35695,2020-01-09 12:25:17.491189: F tensorflow/lite/toco/graph_transformations/quantize.cc:611] Check failed: is_rnn_state_array ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary):binary
- TensorFlow version:1.14
- Python version:3.7.4
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):7.4
- CUDA/cuDNN version:10.2
- GPU model and memory:GeForce GTX 960M/PCIe/SSE2, 16GB



**Describe the problem**
2020-01-09 12:25:17.491189: F tensorflow/lite/toco/graph_transformations/quantize.cc:611] Check failed: is_rnn_state_array 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I am converting a quantized graph def (.pb ) to a quantized tflite (.tflite) using the dummy quantization and encounter error as follows
```
(tf_gpu_clone) ridlr@ridlr107:~/TensorFlow/exported_model_12k_quantized$ tflite_convert --output_file tflite_graph.tflite --graph_def_file tflite_graph.pb --input_arrays image_tensor --output_arrays TFLite_Detection_PostProcess --input_shapes 1,576,720,3 --allow_custom_ops --inference_type QUANTIZED_UINT8 --std_dev_values 127 --mean_values 128 --default_ranges_min 0 --default_ranges_max 6
2020-01-09 12:25:15.452049: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-09 12:25:15.474575: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz
2020-01-09 12:25:15.475004: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561bb6736540 executing computations on platform Host. Devices:
2020-01-09 12:25:15.475031: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
Traceback (most recent call last):
  File ""/home/ridlr/anaconda3/bin/tflite_convert"", line 10, in <module>
    sys.exit(main())
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 503, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 499, in run_main
    _convert_tf1_model(tflite_flags)
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 193, in _convert_tf1_model
    output_data = converter.convert()
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 904, in convert
    **converter_kwargs)
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 373, in toco_convert_graph_def
    input_data.SerializeToString())
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 172, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2020-01-09 12:25:16.861669: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TFLite_Detection_PostProcess
2020-01-09 12:25:16.957738: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1537 operators, 2264 arrays (0 quantized)
2020-01-09 12:25:17.017901: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1537 operators, 2264 arrays (0 quantized)
2020-01-09 12:25:17.482076: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 181 operators, 341 arrays (0 quantized)
2020-01-09 12:25:17.485583: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 181 operators, 341 arrays (0 quantized)
2020-01-09 12:25:17.486877: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 99 operators, 259 arrays (0 quantized)
2020-01-09 12:25:17.488034: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 99 operators, 259 arrays (0 quantized)
2020-01-09 12:25:17.489088: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before default min-max range propagation graph transformations: 99 operators, 259 arrays (0 quantized)
2020-01-09 12:25:17.489972: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After default min-max range propagation graph transformations pass 1: 99 operators, 259 arrays (0 quantized)
2020-01-09 12:25:17.491160: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 99 operators, 259 arrays (0 quantized)
2020-01-09 12:25:17.491189: F tensorflow/lite/toco/graph_transformations/quantize.cc:611] Check failed: is_rnn_state_array 
Fatal Python error: Aborted

Current thread 0x00007fb839eed740 (most recent call first):
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33 in execute
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 250 in _run_main
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 299 in run
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40 in run
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59 in main
  File ""/home/ridlr/anaconda3/bin/toco_from_protos"", line 10 in <module>
Aborted (core dumped)

```

However if I do not include the following specifiers a *.tflite is created.
`--inference_type QUANTIZED_UINT8 --std_dev_values 127 --mean_values 128 --default_ranges_min 0 --default_ranges_max 6 `

This *.tflite file when used to convert to *_edgetpu.tflite (this model is used to run inference on Google coral) gives the following error
```
(tf_gpu_clone) ridlr@ridlr107:~/TensorFlow/exported_model_12k_quantized$ edgetpu_compiler tflite_graph.tflite 
Edge TPU Compiler version 2.0.267685300
Invalid model: tflite_graph.tflite
Model not quantized
```

Hence it is necessary to include the specifiers for quantization.

"
35694,MultiWorkerMirroredStrategy stuck,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tensorflow 1.15-2.1
- Python version:3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
MultiWorkerMirroredStrategy stucks at start server when I run cluster on different computers.
**Describe the expected behavior**
Works same as on a computer
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
import tensorflow as tf
import json
import os
import tensorflow_datasets as tfds

BUFFER_SIZE = 10000
BATCH_SIZE = 4

def make_datasets_unbatched():
  # Scaling MNIST data from (0, 255] to (0., 1.]
  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label

  datasets, info = tfds.load(name='mnist',
                            with_info=True,
                            as_supervised=True)

  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)


def build_and_compile_cnn_model():
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
  ])
  model.compile(
      loss=tf.keras.losses.sparse_categorical_crossentropy,
      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
      metrics=['accuracy'])
  return model

def train_task(index):
    os.environ['TF_CONFIG'] = json.dumps({
        'cluster': {
            'worker': [""ip1:9901"",""ip2:9902""],
        },
        'task': {'type': 'worker', 'index': index},
    })
    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
    # Here the batch size scales up by number of workers since
    # `tf.data.Dataset.batch` expects the global batch size. Previously we used 64,
    # and now this becomes 128.
    GLOBAL_BATCH_SIZE = 12
    train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)
    with strategy.scope():
        multi_worker_model = build_and_compile_cnn_model()
    multi_worker_model.fit(x=train_datasets, epochs=3)

# runs on ip1
train_task(0)
# runs on ip2
# train_task(1)
```
**Other info / logs**
Above code works on a single computer.
Ports are accessible between computers.
I have searched on the internet and read serveal books, but I still can't find solution."
35693,Correction in calculation of error while Naive Forecasting,"## URL(s) with the issue:

https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l08c03_moving_average.ipynb

## Description of issue (what needs changing):

It should be 'mean absolute error' instead of squared error while Naive Forecasting

![Screenshot from 2020-01-09 12-10-09](https://user-images.githubusercontent.com/29497701/72044240-4bd89a80-32d9-11ea-937f-a189784b83b0.png)

### Submit a pull request?

Yes, I'll be submitting one shortly"
35692,tflite_convert failed,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.14


**Provide the text output from tflite_convert**

```
(tf_gpu_clone) ridlr@ridlr107:~/TensorFlow/exported_model_12k_quantized$ tflite_convert --output_file tflite_graph.tflite --graph_def_file tflite_graph.pb --input_arrays image_tensor --output_arrays TFLite_Detection_PostProcess --input_shapes 1,576,720,3
2020-01-09 12:10:44.239300: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-09 12:10:44.262441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz
2020-01-09 12:10:44.262923: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558c8fa667e0 executing computations on platform Host. Devices:
2020-01-09 12:10:44.262939: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
Traceback (most recent call last):
  File ""/home/ridlr/anaconda3/bin/tflite_convert"", line 10, in <module>
    sys.exit(main())
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 503, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 499, in run_main
    _convert_tf1_model(tflite_flags)
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 193, in _convert_tf1_model
    output_data = converter.convert()
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 904, in convert
    **converter_kwargs)
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 373, in toco_convert_graph_def
    input_data.SerializeToString())
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 172, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2020-01-09 12:10:45.667362: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TFLite_Detection_PostProcess
2020-01-09 12:10:45.763812: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1537 operators, 2264 arrays (0 quantized)
2020-01-09 12:10:45.824420: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1537 operators, 2264 arrays (0 quantized)
2020-01-09 12:10:46.292215: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 181 operators, 341 arrays (0 quantized)
2020-01-09 12:10:46.295908: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 181 operators, 341 arrays (0 quantized)
2020-01-09 12:10:46.298914: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 181 operators, 341 arrays (0 quantized)
2020-01-09 12:10:46.304648: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 20160000 bytes, theoretical optimal value: 17280000 bytes.
2020-01-09 12:10:46.305189: I tensorflow/lite/toco/toco_tooling.cc:433] Estimated count of arithmetic ops: 1.29335 billion (note that a multiply-add is counted as 2 ops).
2020-01-09 12:10:46.305598: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/Conv/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305607: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305629: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305633: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_1/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305636: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305641: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_1/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305645: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_2/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305650: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305654: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_2/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305658: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_2/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305662: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_3/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305665: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305669: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_3/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305674: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_4/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305678: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305681: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_4/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305684: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_4/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305688: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_5/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305692: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305696: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_5/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305700: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_5/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305703: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_6/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305706: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305709: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_6/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305713: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_7/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305717: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305721: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_7/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305725: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_7/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305729: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_8/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305733: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305737: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_8/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305741: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_8/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305745: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_9/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305749: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305753: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_9/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305758: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_9/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305762: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_10/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305766: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305770: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_10/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305774: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_11/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305778: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305782: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_11/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305786: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_11/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305790: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_12/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305793: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305796: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_12/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305800: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_12/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305803: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_13/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305807: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305811: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_13/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305815: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_14/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305819: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305823: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_14/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305827: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_14/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305831: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_15/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305835: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305839: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_15/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305843: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_15/post_activation_bypass_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305847: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_16/expand/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305851: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305855: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/expanded_conv_16/project/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305859: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/Conv_1/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305863: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305867: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305871: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305875: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305879: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305883: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305887: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305891: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305896: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_0/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305900: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_0/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305904: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_1/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305908: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_1/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305912: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_2/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305916: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_2/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305920: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_3/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305924: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_3/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305928: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_4/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305932: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_4/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305936: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_5/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305940: W tensorflow/lite/toco/tflite/export.cc:456] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_5/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.
2020-01-09 12:10:46.305998: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FAKE_QUANT, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.
Traceback (most recent call last):
  File ""/home/ridlr/anaconda3/bin/toco_from_protos"", line 10, in <module>
    sys.exit(main())
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FAKE_QUANT, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.

```

Also, please include a link to a GraphDef or the model if possible.

"
35691,A model results inconsistent output values depending on input batch size,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): python:3.7.5-slim docker image, macOS Catalina 10.15.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip 19.3.1
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.5(docker), 3.7.4(macOs)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
The `predict` method of `tf.keras.applications.mobilenet.MobileNet`, gives different output values when the batch sizes are different even though all input value are the same.
This problem does not occur with Tensorflow 2.0.0 and occurs with 2.1.0

**Describe the expected behavior**
The output value of an input does not change as it is in another batch with different size.

**Code to reproduce the issue**
``` python
import numpy as np
import tensorflow as tf


def test_mobile_net():
    four_black_images = np.zeros((4, 224, 224, 3), dtype='uint8')
    one_black_image = np.zeros((1, 224, 224, 3), dtype='uint8')

    four_input = tf.keras.applications.mobilenet.preprocess_input(four_black_images)
    one_input = tf.keras.applications.mobilenet.preprocess_input(one_black_image)

    # below assert statement passes, meaning all input batches have the same input values
    for i in range(4):
        assert (four_input[i] == one_input[0]).all()

    model = tf.keras.applications.mobilenet.MobileNet(input_shape=(224, 224, 3), include_top=False, pooling='avg')

    one_image_result = model.predict(one_input)
    four_images_result = model.predict(four_input)

    # bellow assert statement passes
    # There is no inter-differences between  output values within the same batch
    for i in range(4):
        for j in range(4):
            assert (four_images_result[i] == four_images_result[j]).all()

    # Bellow assert statement fails with Tensorflow 2.1.0 but passes with 2.0.0
    assert (four_images_result[0] == one_image_result[0]).all()
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I also have tested with batch size with 4 and 7, wondering if the output result is only different when the batch has size 1, but it also gave inconsistent results. 
So the output value changes as batch size changes with version 2.1.0."
35690,Determine input_arrays and output_arrays values for tflite_convert,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version:1.14
- Python version:3.7.4
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):7.4
- CUDA/cuDNN version:10.2
- GPU model and memory:GeForce GTX 960M/PCIe/SSE2, 16GB



**Describe the problem**
tflite_convert : need to know the values for inout_arrays and --output_arrays
**Provide the exact sequence of commands / steps that you executed before running into the problem**
I have created a tflite_graph.pb from export_tflite_ssd_graph.py, quantized checkpoint and config files succesfully.

My task is to generate a .tflite file using the generated graph_def_file using the tflite_convert command. And then use this to generate a edgetpu.tflite file to run on Google coral.
following is the log of the command

```
(tf_gpu_clone) ridlr@ridlr107:~/TensorFlow/exported_model_12k_quantized$ tflite_convert --output_file tflite_graph.tflite --graph_def_file tflite_graph.pb --input_arrays image_tensor --output_arrays detection_boxes --input_shapes 1,576,720,3
2020-01-09 11:05:56.913487: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-09 11:05:56.934582: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz
2020-01-09 11:05:56.935469: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5569f40357d0 executing computations on platform Host. Devices:
2020-01-09 11:05:56.935514: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
Traceback (most recent call last):
  File ""/home/ridlr/anaconda3/bin/tflite_convert"", line 10, in <module>
    sys.exit(main())
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 503, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 499, in run_main
    _convert_tf1_model(tflite_flags)
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py"", line 193, in _convert_tf1_model
    output_data = converter.convert()
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 904, in convert
    **converter_kwargs)
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 373, in toco_convert_graph_def
    input_data.SerializeToString())
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py"", line 172, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2020-01-09 11:05:58.375534: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TFLite_Detection_PostProcess
2020-01-09 11:05:58.456167: F tensorflow/lite/toco/tooling_util.cc:918] Check failed: GetOpWithOutput(model, output_array) Specified output array ""detection_boxes"" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.
Fatal Python error: Aborted

Current thread 0x00007f347d2e6740 (most recent call first):
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 33 in execute
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 250 in _run_main
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 299 in run
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40 in run
  File ""/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 59 in main
  File ""/home/ridlr/anaconda3/bin/toco_from_protos"", line 10 in <module>
Aborted (core dumped)
```

How do I determine the correct value of --output_array.


**Any other info / logs**
If I use the specifier --inference_type=QUANTIZED_UINT8
How do I determine the values of following specifiers?
--std_dev_values 
--mean_values 
--default_ranges_min 
--default_ranges_max
"
35689,TFLite C/C++ library header file installation,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0

**Describe the problem**
I need just a TFLite header file without the whole TensorFlow source code. Is there any method to install the TFLite header?

I found a similar issue at #3536. But, `//tensorflow:install_headers` Bazel target does not install a TFLite header file.
"
35688,RuntimeError: Encountered unresolved custom op: Enter.Node number 8 (Enter) failed to prepare.,"I have converted tf.keras model to tf.lite successfully. However, when I use it for inference, I get an error. Is there anyone who can resolve it? Thanks!

code:
`interpreter = tf.lite.Interpreter(model_path=""E:/object_detection/EfficientDet-region_anchor_opt_mbconv-head-ckpts/tflites/ckpts_B0_image-size-768/mbconv-se-head_1e-5_unfreeze-backbone_freeze-bn/csv_04_0.6736_0.7484_opts.tflite"")

interpreter.allocate_tensors()`

error:
`RuntimeError                              Traceback (most recent call last)
<ipython-input-12-ca8eb7ec6089> in <module>()
      1 # interpreter = tf.lite.Interpreter(model_content=tflite_model)
----> 2 interpreter.allocate_tensors()
      3 # help(tf.lite.Interpreter)

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\lite\python\interpreter.py in allocate_tensors(self)
    242   def allocate_tensors(self):
    243     self._ensure_safe()
--> 244     return self._interpreter.AllocateTensors()
    245 
    246   def _safe_to_run(self):

~\AppData\Roaming\Python\Python36\site-packages\tensorflow_core\lite\python\interpreter_wrapper\tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)
    104 
    105     def AllocateTensors(self):
--> 106         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
    107 
    108     def Invoke(self):

RuntimeError: Encountered unresolved custom op: Enter.Node number 8 (Enter) failed to prepare.`"
35687,multi_gpu_model is slower than single gpu model.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): 1.14
- Python version: Python 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0
- GPU model and memory: 2*Titan XP - 12GB memory per card


**Describe the current behavior**

The performance of a multi_gpu_model is slower than using a single GPU. When the model is running on a single GPU, the compute performance of the GPU tops at 91% to 95%, but when model is running on a multiple GPUs, the performance varies from 9% to 21%.

**Describe the expected behavior**

Expect the model to train faster on multiple GPUs compared to single GPU.

**Code to reproduce the issue**

All the Convolution networks I've tried.

"
35685,"Define variable shape at restore/load, allow direct restoring of variables prior to calling __build__ (non-lazy variable loading from checkpoint)","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): Yes (if necessary)



**Describe the feature and the current behavior/state.**

Typically for RL, I use tensorflow in combination with Ray, where a remote agent collects data using a policy model, and the episodes are fed to a tf-agents EpisodicReplayBuffer in a second train-loop process. A tf.dataset is used to feed the main model training on this thread, which is then serialized via get_weights and sent back to the remote actor via a ray remote call to set_weights.

Currently I use tf.Checkpoint.restore.expect_partial() to restore my subclassed tf.keras.Model. In practice, this is extremely convoluted on initialization:
* I make a forward pass through my remote Agent on random policy (since I have not initialized the weights), retrieve this forward pass, and send it to the train loop
* I make a second forward pass through my train loop (again, randomly initialized network still), this has the side effect of initializing the network, allowing my checkpoint to restore the variables in this model
* I call get_weights on the train_loop model, and pass the weights to my remote model via set_weights.
* I can now collect a full replay buffer with my Agent on policy
* I can now run the train loop

Maybe I'm missing something, but most of my labmates are also confused by what the best practice for this currently is.

**Will this change the current api? How?**
I'm not sure what the best approach is. Some ideas for discussion? 

* Allow set_weights to define the shapes and names of tf.Variables, such that calls to __build__ in the tf.keras.Model on first run will align with the initialized tf.Variables.
* Allow tf.keras.model.load_weights() to set the weights immediately rather than waiting for first call to build()

**Who will benefit with this feature?**
RL community 
**Any Other info.**
"
35683,Wrapper.from_config mutates its input,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): platform-independent
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1
- Python version: 3.7

**Describe the current behavior**

`tf.keras.layers.Wrapper.from_config` modifies its `config` parameter, which can cause unexpected side effects in calling code.

https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/layers/wrappers.py#L83-L87

Specifically, `config.pop` in line 86 above mutates the `config` dict in a way that persists outside the `from_config` function call.

Elsewhere (e.g., in `tf.keras.layers.Bidirectional.from_config`) this is avoided by copying the `config` dict:

https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/layers/wrappers.py#L743-L745

**Describe the expected behavior**

Being able to call `tf.keras.layers.Wrapper.from_config(config)` without `config` changing.

I have a use case where I am subclassing the `Wrapper` class and relying on its `from_config` method. My workaround is to call `from_config(config.copy())`, but I don't think this should be required.

**Code to reproduce the issue**

```python
import tensorflow as tf


class MyWrapper(tf.keras.layers.Wrapper):
    def call(self, inputs, *args, **kwargs):
        return self.layer(inputs, *args, **kwargs)


wrapper = MyWrapper(tf.keras.layers.Dense(1))
config = wrapper.get_config()
config_copy = config.copy()
assert config == config_copy

wrapper_from_config = MyWrapper.from_config(config)
new_config = wrapper.get_config()
assert new_config == config_copy
assert config == config_copy  # Fails! The 'layer' key has been popped from config
```"
35682,tf.data.Dataset.map repeats random numbers in each epoch  of a Keras training loop when a graph-level seed is set,"Thanks for staying with me after this long title. There are many issues on non-determinism of `Dataset.map` - this one is contrary. With random data augmentation, I would expect `Dataset.map` to behave differently between epochs, but it does not if a graph-level seed is set.

**System information**
- Have I written custom code: yes, below.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 & Linux
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.1.0rc2, also 2.0.0 and tf-nightly
- Python version: 3.7.6
- CUDA/cuDNN version: 10.1/7.x
- GPU model and memory: various

**Describe the current behavior**
The code below outputs the same random numbers in each epoch:
```
Train for 3 steps
Epoch 1/3
WARNING:tensorflow:The list of trainable weights is empty. Make sure that you are not setting model.trainable to False before compiling the model.
0.926393032
0.0866344
0.783794165
3/3 [==============================] - 1s 186ms/step - loss: 0.0000e+00
Epoch 2/3
0.926393032
0.0866344
0.783794165
3/3 [==============================] - 0s 7ms/step - loss: 0.0000e+00
Epoch 3/3
0.926393032
0.0866344
0.783794165
3/3 [==============================] - 0s 6ms/step - loss: 0.0000e+00

```

**Describe the expected behavior**
It should output different random numbers in each epoch.

**Code to reproduce the issue**
```
import tensorflow as tf

tf.random.set_seed(0)


def do_something_random(*args):
    """"""It does the same things in each epoch. That's not random!""""""
    tf.print(tf.random.uniform((), 0, 1))
    return args


data = [[1], [2], [3]]
data = tf.data.Dataset.from_tensor_slices((data, data)).batch(1)
data = data.map(do_something_random)

layer = tf.keras.layers.Input(shape=(1,))
model = tf.keras.models.Model(inputs=layer, outputs=layer)
model.compile(optimizer=""adam"", loss=""mse"")
model.fit(x=data, epochs=3)
```

**Other info / logs**
Initially, I thought, well, the `Dataset.map` command is maybe compiled once and run identically in each epoch. But
- I am using `tf.random.uniform` explicitly,
- the problem disappears when commenting out `tf.random.set_seed(0)`, implying that the `Dataset.map` command *is* able to yield different results in each epoch:
```
Train for 3 steps
Epoch 1/3
WARNING:tensorflow:The list of trainable weights is empty. Make sure that you are not setting model.trainable to False before compiling the model.
0.632945657
0.70087862
0.662360072
3/3 [==============================] - 1s 186ms/step - loss: 0.0000e+00
Epoch 2/3
0.536124825
0.0930280685
0.26403141
3/3 [==============================] - 0s 7ms/step - loss: 0.0000e+00
Epoch 3/3
0.323968768
0.376766324
0.693181396
3/3 [==============================] - 0s 6ms/step - loss: 0.0000e+00
```

The issue is why the graph-level seed has an influence on this effect. Are the `Dataset.map` calls in each epoch maybe run in separate graphs, each of which receives the same graph-level seed?"
35681,OOM error when running ops on large tensors in TF2.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0.130 / 7.6.0
- GPU model and memory: Nvidia GeForce GTX 1060

**Describe the current behavior**

I want to allocate a large tensor (in the 100Ms of elements) and perform an operation on it such as `tf.exp`. However, when I apply an operation to my tensor it appears that TF is reallocating the entire thing, causing an OOM. This behavior is not limited to `tf.exp`, I believe it is common across all ops. Is there any way to force TF not to do such a massive reallocation?

**Describe the expected behavior**

TF should return the result of `tf.exp` without error.

**Code to reproduce the issue**

Running:
```python
x = tf.ones([1, 25088, 25088])
tf.exp(x)
```
Gives me the following error:
```
2020-01-08 15:06:32.805387: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cwise_ops_common.h:259 : Resource exhausted: OOM when allocating tensor with shape[1,25088,25088] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/evan/opt/miniconda3/envs/tf-gpu-2.0.0/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py"", line 3970, in exp
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1,25088,25088] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Exp]
```

In PyTorch, I could run the following identical code without error:
```python
x = torch.ones((1,25088,25088))
torch.exp(x)
```
"
35680,Redundant functions between `Tensorflow Features Columns` API and `TF Keras Preprocessing Layers`,"In working with TF 2.0, I noticed that the TF Feature Columns API seems to overlap with the Keras Preprocessing layers (or Keras Utils). For example, you can create an `tf.feature_column.indicator_column()` which creates a bunch of dummy variables or a one-hot encoded matrix based upon a categorical variable. With Keras preprocessing or Utils, you can use the `tf.keras.backend.one_hot()` function to perform the same operation. I think there are similar overlaps between TF Feature Columns like the embedding columns and similar Keras functions for embedding columns.

I was just wondering if the steering committees for Tensorflow have any direction on whether they plan to promote one set of functions versus the other? Are there any plans to deprecate one set versus the other. For me, it is just a question of where to invest time and planning for code that might have to change in the future. Seems like maintaining the redundancy in the package will potentially lead to performance differences between similar functions, or confusion in setting up the code, etc. 

**NB** 
Oh yes, I actually asked this question in the Tensorflow Discussion forum, but no one answered it. @dynamicwebpaige even forwarded the message to @karmel and Mark Omernick, but no one responded. Hence, I posted here. 
"
35678,TensorFlowLite_LSTM_Keras_Tutorial.ipynb update ,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/TensorFlowLite_LSTM_Keras_Tutorial.ipynb

## Description of issue (what needs changing):
The example script on how to make lstm layers ready for tf lite is outdated and not working anymore, because the requested tf-nightly package causes issues. 


I would like to get an updated tutorial or a better alternative. To use the TFLite converter with the experimental_flag set to True works with lstm layers, but does not allow post training quantization. As a general question I would like to know, if this would be possible with the the model that is build in the example script?

"
35677,Explanation regarding `seed` parameter,"## URL(s) with the issue:

https://github.com/tensorflow/examples/blob/d631c0545dac90c6390da76ed8df7c4f6a2a25bc/courses/udacity_intro_to_tensorflow_for_deep_learning/l08c01_common_patterns.ipynb#L307

`def white_noise(time, noise_level=1, seed=None):`

## Description of issue (what needs changing):

I think, we should add explanation of `seed` parameter here since it's quite an important one.

### Clear description

Some explanation about how `seed` affects generation of random numbers every time along with links for reference can be added.

### Submit a pull request?

Yes"
35675,tflite crash with segmentfault when I use set_tensor to set input tensor.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu16.04 windows and Raspbian 10

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
I test it both on x64 and armv7l (Raspberry Pi4 which is running 32bit OS)

- TensorFlow installed from (source or binary):
from binary

- TensorFlow version (use command below):
I test with 2.0 1.12

- Python version:
3.7

**Describe the current behavior**
As I use tf1.12 produce a saved_model.pb, I use tf2.0 to convert it to tflite file. But when I try to inference with tflite. it alway crash with segment fault when I try to set_tensor. I tried it on windows and linux and raspberry pi, all doesn't work.

I have test the saved_model.pb as it work well, so I think the it's not the model's problem.

**Code to reproduce the issue**
I use the following code to convert the model to tflite:
`import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_saved_model('./')

tflite_model = converter.convert() 

open('model_network.tflite', 'wb').write(tflite_model)`

and following code to inference:
`
interpreter = tf.lite.Interpreter(model_path=model_path)

input_details = interpreter.get_input_details()

output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']

input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)

interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

result = interpreter.get_tensor(output_details[0]['index'])
`
it crash when interpreter.set_tensor
I attach my model files here.
https://drive.google.com/file/d/1NflTBZ2iB4hptDozODdaSke4o7pzPBNW/view?usp=sharing
The original code is to long, if needed I can upload it later."
35674,"SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'input_1:0' shape=(None, 30, 78) dtype=float32>]","------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Version 1909
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow version (use command below)**: 2.0.0
- **Python version**: 3.6.9
- **CUDA/cuDNN version**:10.0.130
- **GPU model and memory**:GTX 1660 Ti 6GB


[Jazz improvisation with LSTM.zip](https://github.com/tensorflow/tensorflow/files/4035452/Jazz.improvisation.with.LSTM.zip)

I uploaded a zip file in which the .ipynb file when runs gets that error. Please you can run and check what's the problem. How to fix this issuse?"
35673,ValueError: too many values to unpack (expected 2),"while performing language translation it is rising an issue value error.
i followed the code from the tensorflow guide

from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
#from sklearn.model_selection import train_test_split

import unicodedata
import re
import numpy as np
import os
import io
import time
# Download the file

path_to_file = ""spanish.txt""
# Converts the unicode file to ascii
def unicode_to_ascii(s):
    return ''.join(c for c in unicodedata.normalize('NFD', s)
                   if unicodedata.category(c) != 'Mn')


def preprocess_sentence(w):
    w = unicode_to_ascii(w.lower().strip())
    # creating a space between a word and the punctuation following it
    # eg: ""he is a boy."" => ""he is a boy .""
    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation
    w = re.sub(r""([?.!,¿])"", r"" \1 "", w)
    w = re.sub(r'["" ""]+', "" "", w)

    # replacing everything with space except (a-z, A-Z, ""."", ""?"", ""!"", "","")
    w = re.sub(r""[^a-zA-Z?.!,¿]+"", "" "", w)
    w = w.rstrip().strip()
    # adding a start and an end token to the sentence
    # so that the model know when to start and stop predicting.
    w = '<start> ' + w + ' <end>'
    return w
en_sentence = u""Wow!""
sp_sentence = u""¡Órale!""
print(preprocess_sentence(en_sentence))
print(preprocess_sentence(sp_sentence).encode('utf-8'))

# 1. Remove the accents
# 2. Clean the sentences
# 3. Return word pairs in the format: [ENGLISH, SPANISH]
def create_dataset(path, num_examples):
    lines = io.open(path, encoding='UTF-8').read().strip().split(""\t"")
    word_pairs = [[preprocess_sentence(w) for w in l.split(""\t"")] for l in lines[:num_examples]]
    return word_pairs
en, sp = create_dataset(path_to_file, None)
print(en[-1])
print(sp[-1])


Raising an error in line : en, sp = create_dataset(path_to_file, None)"
35672,tf.keras.estimator.model_to_estimator InvalidArgumentError: while setting up XLA_GPU_JIT device number 2,"I get an error similar to the issue described in https://github.com/tensorflow/tensorflow/issues/31451
When using tf.keras.estimator.model_to_estimator in TF 1.14 and 1.15 I get the InvalidArgumentError described below. The problem doesn't occur in TF 1.13.2. 

I use Linux Mint with Python 3.6 with 2 GPUs (Nvidia GTX1080Ti). To be complete, there are 3 NVidia video cards in the machine (that seems relevant if I understand the error correctly).

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-6-3ce5a1d7f637> in <module>
      2     keras_model=model_f,
      3     custom_objects={'Merge': Merge},
----> 4     model_dir='./data/estimator')

~/.../python3.6/site-packages/tensorflow_core/python/keras/estimator/__init__.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config, checkpoint_format)
    105       config=config,
    106       checkpoint_format=checkpoint_format,
--> 107       use_v2_estimator=False)
    108 
    109 

~/.../python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config, checkpoint_format, use_v2_estimator)
    574   if keras_model._is_graph_network:
    575     warm_start_path = _save_first_checkpoint(keras_model, custom_objects,
--> 576                                              config, save_object_ckpt)
    577   elif keras_model.built:
    578     logging.warning('You are creating an Estimator from a Keras model manually '

~/.../python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py in _save_first_checkpoint(keras_model, custom_objects, config, save_object_ckpt)
    390 
    391       # save to checkpoint
--> 392       with session.Session(config=config.session_config) as sess:
    393         if keras_weights:
    394           model.set_weights(keras_weights)

~/.../python3.6/site-packages/tensorflow_core/python/client/session.py in __init__(self, target, graph, config)
   1583           protocol buffer with configuration options for the session.
   1584     """"""
-> 1585     super(Session, self).__init__(target, graph, config=config)
   1586     # NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.
   1587     self._default_graph_context_manager = None

~/.../python3.6/site-packages/tensorflow_core/python/client/session.py in __init__(self, target, graph, config)
    697     try:
    698       # pylint: disable=protected-access
--> 699       self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)
    700       # pylint: enable=protected-access
    701     finally:

InvalidArgumentError: Invalid device ordinal value (2). Valid range is [0, 1].
	while setting up XLA_GPU_JIT device number 2
```"
35671,"Where is self.activity_regularizer used in SimpleRNN, GRU and LSTM? Is it a dangling orphan attribute?","In the `__init__()` of keras' `SimpleRNN`, `GRU` and `LSTM` the input parameter `activity_regularizer` is stored

```
...
self.activity_regularizer = regularizers.get(activity_regularizer)
...
```

However, it seems that `self.activity_regularizer` never used anywhere in `SimpleRNN`, `GRU` and `LSTM` afterwards. "
35668,the network is not learning anything - problem with Adam optimizer?,"I customized this repository of Recurrent Models of Visual Attention (minimal) implementation on MNIST dataset to work with my custom RGB dataset, here is the forked repository: https://github.com/dusa2/RAM

Basically I made minimal changes to make it work with RGB data, and I read in the data using the datagenerator, I have checked data input and shapes and it all seems okay.

However, the network is not learning anything at all! I have a %50 %50 2 label dataset, and the network reward only moves around 0.5, the evaluation is around %50 and at first very little improvement is made (as it reads on the first epoch), but after that the network accuracy does not get better at all. I tried to lower the learning rate and nothing has changed. I suspect that the Adam optimizer does not update the learning rate either.

It all seems straightforward but I can not figure out why. I also tried another dataset (again custom) and it has the same behavior. How can I make sure that the optimizer is working as intended? What else might be the problem?

"
35667,assign() got an unexpected keyword argument 'validate_shape',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
```
import tensorflow as tf

a = tf.Variable(2)
a.assign(5)
assert a.numpy() == 5

# ValueError: Shapes () and (2,) are incompatible
a.assign([1,2])  

# TypeError: assign() got an unexpected keyword argument 'validate_shape'
a.assign([1,2], validate_shape=False)

# ValueError: Shapes () and (2,) are incompatible
tf.compat.v1.assign(a, [1,2], validate_shape=False)  

```
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Linux
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0, 2.1.0
- Python version: 3.7

**Describe the current behavior**

`tf.assign` had a `validate_shape` parameter that `Variable.assign` seems to be missing.

In addition, the docs say:
> If you want to change the shape of a variable later you have to use an `assign` Op with `validate_shape=False`.

https://www.tensorflow.org/api_docs/python/tf/Variable

How should one change the shape of a variable?

**Code to reproduce the issue**
See above.
"
35665,tf.keras.models.load_model unable to use output layer names specified for loss_weights,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64-Bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2 via conda install tensorflow-gpu cudnn=7.3.1=cuda9.0_0
- Python version: 3.7.4
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: cudnn=7.3.1=cuda9.0_0
- GPU model and memory: NVIDIA Quadro M1200

**Describe the current behavior**
I am using tf.keras functional APIs to create a multi-output model and to my compile call, I provide loss weights specific to my output names:

```
my_model.compile(optimizer=optimizer, metrics=['accuracy']
                      ,loss='categorical_crossentropy'
                      ,loss_weights={'sec': 0.25, 'cls': 0.35, 'subcls': 0.4}
                     )
```

Model compilation, fitment all work and I'm able to use model object to predict, evaluate etc. I'm also able to save model in 'tf' format. The save for 'hdf5' format doesn't work and gives OOM error, but that's ok for me now. The code to save is:

`model.save(r'./models/hf_s100/')`

The problem I face is when I try to load it via:

`hf_model = keras.models.load_model(r'./models/hf_s100/')`

I get an error that loss weights found to be 'sec', 'cls' and 'subcls' and was expecting 'output_1', 'output_2', 'output_3'

**Describe the expected behavior**
Given that we are allowed to name our output layers and use the names in loss_weights option, the load_model should also be able to pick the name specified as part of model save.

**Code to reproduce the issue**
See code snippets provided above"
35664,"Register a custom tensorflow operation that could be used in tensorflow C++, which itself uses some existing tensorflow operation","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Professional
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14.0
- Python version: 3.6.6
- Installed using virtualenv? pip? conda?: -
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: - 



**Describe the problem**
I’m trying to register a custom tensorflow operation that I could later use in C++.
The operation is required to use an already existing tensorflow operation as-

`Output output1 = tensorflow::ops::DeepCopy(scope, tensor);`

I’ve created custom_symbolic_gradients.cc file that contains following as shown in official documentation of tensorflow to create a custom op (https://www.tensorflow.org/guide/create_op )-
1. Defining the op interface 
2. Implementing the kernel for the op
3. Registering it with tensorflow kernel

I’ve placed `custom_symbolic_gradients.cc` in following directory:
`//tensorflow/core/user_ops/custom_symbolic_gradients.cc`

When I try to build this tensorflow package with bazel, the following problem arises-
`custom_symbolic_gradients.cc` includes a header file `tensorflow/cc/ops/standard_ops.h` (I had to include it because `DeepCopy `is declared in this header file), which further contains more headers (`tensorlfow/cc/ops/array_ops.h`, `tensorflow/cc/ops/candidate_sampling_ops.h`, and so on...) but these headers are not pre existing in tensorflow package and are generated while building tensorflow with bazel.

So, while building tensorflow with bazel with the following command-
`bazel build -c opt //tensorflow:libtensorflow_cc.so` 

I’m getting the following error-
`.\tensorflow/cc/ops/standard_ops.h(19): fatal error C1083: Cannot open include file: 'tensorflow/cc/ops/array_ops.h': No such file or directory
`
I’ve also tried it with different targets but still get the same error.

Later, I tried it building as incremental build, I first built tensorflow using bazel successfully without adding any custom tensorflow operation, and then tried to build it again after adding custom_symbolic_gradients.cc file in the `tensorflow/core/user_ops/` directory, then I get the following error - 
```
undeclared inclusion(s) in rule '//tensorflow/core:user_ops_op_lib':
this rule is missing dependency declarations for the following files included by 'tensorflow/core/user_ops/custom_symbolic_gradients.cc':
  'bazel-out/x64_windows-opt/genfiles/tensorflow/cc/ops/array_ops.h'
Target //tensorflow:libtensorflow_cc.so failed to build

```
Any idea that I can use to resolve this issue?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build -c opt //tensorflow:libtensorflow_cc.so 

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[command.log](https://github.com/tensorflow/tensorflow/files/4034281/command.log)
"
35663,ModuleNotFoundError: No module named 'tflite_runtime',"Hi,

I am working a x86 Laptop and have installed tensorflow using https://www.tensorflow.org/lite/guide/python
Following is the list of tflite  installed
```

ankit@HP:~$ pip3 list | grep tflite
tflite                        1.15.0                       
tflite-runtime                1.14.0 
```    

my aim is to get a Google coral example running from this link https://coral.ai/docs/accelerator/get-started/#3-run-a-model-using-the-tensorflow-lite-api

When I execute the command for inference I get the folllowing error

```
Traceback (most recent call last):
  File ""classify_image.py"", line 36, in <module>
    import tflite_runtime.interpreter as tflite
ModuleNotFoundError: No module named 'tflite_runtime'

```


Is there anything else that I need to install. I already have installed the libedgetpu1-std.
"
35662,"Cannot build from source, get error: command succeeded, but there were errors parsing the target pattern","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version:CUDA 10.0/cuDNN7.4
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

 I want to get rid of the message like 

>Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA

Therefore, I build TF2.0 following the [guide](https://www.tensorflow.org/install/source) on the official site. Here's my configuration

```
Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.

Found CUDA 10.0 in:
    /usr/local/cuda/lib64
    /usr/local/cuda/include
Found cuDNN 7 in:
    /usr/local/cuda/lib64
    /usr/local/cuda/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 7.5]:


Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:


Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apache Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished
```
I use the following command to build
```
bazel build -c opt — copt=-mavx — copt=-mavx2 — copt=-mfma — copt=-mfpmath=both — copt=-msse4.2 -k //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:308:16:   required from ‘void EigenForTFLite::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMappe
r>::packRhs(RhsScalar**, const typename RhsMapper::SubMapper&, StorageIndex, StorageIndex) [with ResScalar = float; LhsScalar = float; RhsScalar = float; StorageIndex = long int; OutputMapper = EigenForTFLite::internal::blas_data_mapper<f
loat, long int, 0, 0>; LhsMapper = EigenForTFLite::internal::TensorContractionInputMapper<float, long int, 1, EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenFo
rTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> >, EigenForTFLite::ThreadPoolDevice>, std::array<long int, 1>, std::array<long int, 1>, 4, true, false, 0, EigenForTFLite::MakePointer>; RhsMapper = EigenForTFLit
e::internal::TensorContractionInputMapper<float, long int, 0, EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenF
orTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> > >, EigenForTFLite::ThreadPoolDevice>, std::array<long int, 1>, std::array<long int, 1>, 4, true, false, 0, EigenForTFLite::MakePointer>; EigenForTFLite::intern
al::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::RhsBlock = float*; typename RhsMapper::SubMapper = EigenForTFLite::internal::TensorContractionSubMapper<float, long int, 0, Ei
genForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, lo
ng int>, 16> > >, EigenForTFLite::ThreadPoolDevice>, std::array<long int, 1>, std::array<long int, 1>, 4, true, false, 0, EigenForTFLite::MakePointer>]’
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:898:11:   required from ‘void EigenForTFLite::TensorContractionEvaluatorBase<Derived>::evalGemmPartial(EigenForTFLite::TensorContractionEvaluatorBase<Derived>::
Scalar*, EigenForTFLite::TensorContractionEvaluatorBase<Derived>::Index, EigenForTFLite::TensorContractionEvaluatorBase<Derived>::Index, int) const [with bool lhs_inner_dim_contiguous = true; bool rhs_inner_dim_contiguous = true; bool rhs
_inner_dim_reordered = false; int Alignment = 0; bool use_output_kernel = false; Derived = EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<long int>, 1>, const EigenForT
FLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> > >, const EigenForTFLite::Tenso
rReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> >, const EigenForTFLite::NoOpOutputKernel>, EigenForTFLite::ThreadPoolDevice>; EigenForTFLite
::TensorContractionEvaluatorBase<Derived>::Scalar = float; EigenForTFLite::TensorContractionEvaluatorBase<Derived>::Index = long int]’
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:813:37:   [ skipping 7 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:175:44:   required from ‘bool EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<NewDimensions, XprType>, Device>::evalSubExprsIfNeeded(EigenF
orTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType) [with NewDimensions = const EigenForTFLite::DSizes<long int, 4>; ArgType = const EigenForTFLite::TensorContractionOp
<const std::array<EigenForTFLite::IndexPair<long int>, 1>, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<EigenForTFLite:
:Tensor<const float, 4, 1, long int>, 16> > >, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> >, const EigenForTF
Lite::NoOpOutputKernel>; Device = EigenForTFLite::ThreadPoolDevice; EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<NewDimensions, XprType>, Device>::EvaluatorPointerType = float*]’
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:148:62:   required from ‘bool EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorAssignOp<LhsXprType, RhsXprType>, Device>::evalSubExprsIfNeeded(EigenForTFL
ite::TensorEvaluator<const EigenForTFLite::TensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType) [with LeftArgType = EigenForTFLite::TensorMap<EigenForTFLite::Tensor<float, 4, 1, long int>, 16>; RightArgType = const Eigen
ForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 4>, const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<long int>, 1>, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSiz
es<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> > >, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int,
 2>, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> >, const EigenForTFLite::NoOpOutputKernel> >; Device = EigenForTFLite::ThreadPoolDevice; EigenForTFLite::TensorEvaluator<const EigenForTFLite::T
ensorAssignOp<LhsXprType, RhsXprType>, Device>::EvaluatorPointerType = float*]’
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:277:16:   required from ‘static void EigenForTFLite::internal::TensorExecutor<Expression, EigenForTFLite::ThreadPoolDevice, Vectorizable, Tileable>::run(const Expr
ession&, const EigenForTFLite::ThreadPoolDevice&) [with Expression = const EigenForTFLite::TensorAssignOp<EigenForTFLite::TensorMap<EigenForTFLite::Tensor<float, 4, 1, long int>, 16>, const EigenForTFLite::TensorReshapingOp<const EigenFor
TFLite::DSizes<long int, 4>, const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<long int>, 1>, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::Tens
orImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> > >, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<EigenF
orTFLite::Tensor<const float, 4, 1, long int>, 16> >, const EigenForTFLite::NoOpOutputKernel> > >; bool Vectorizable = true; bool Tileable = false]’
external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:62:   required from ‘EigenForTFLite::TensorDevice<ExpressionType, DeviceType>& EigenForTFLite::TensorDevice<ExpressionType, DeviceType>::operator=(const OtherDeri
ved&) [with OtherDerived = EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 4>, const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<long int>, 1>, const EigenForTFLite::TensorReshap
ingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> > >, const EigenForTFLite::TensorReshapingOp<const
EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long int>, 16> >, const EigenForTFLite::NoOpOutputKernel> >; ExpressionType = EigenForTFLite::TensorMap<EigenForTFLite::Tensor<
float, 4, 1, long int>, 16>; DeviceType = EigenForTFLite::ThreadPoolDevice]’
./tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h:128:29:   required from ‘void tflite::multithreaded_ops::EigenTensorConvFunctor<T>::operator()(const EigenForTFLite::ThreadPoolDevice&, const T*, int, int, int, int, const
T*, int, int, int, int, int, int, int, tflite::PaddingType, T*, int, int) [with T = float]’
./tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h:169:65:   required from here
./tensorflow/core/kernels/eigen_spatial_convolutions-inl.h:600:15: warning: ignoring attributes on template argument ‘EigenForTFLite::internal::packet_traits<float>::type {aka __vector(4) float}’ [-Wignored-attributes]
     const int packetSize = internal::unpacket_traits<Packet>::size;
               ^~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package up-to-date:
  bazel-bin/tensorflow/tools/pip_package/build_pip_package
ERROR: command succeeded, but there were errors parsing the target pattern
INFO: Elapsed time: 6524.563s, Critical Path: 252.51s
INFO: 18226 processes: 18226 local.
FAILED: Build did NOT complete successfully
```
"
35661,Bug when convert to tflite models.,"**System information**

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04, GPU GTX 1660
TensorFlow installed from (source or binary):pip
TensorFlow version (use command below):2.1.0
Python version:3.7

**Describe the current behavior**
Convert to tflite failed

**Code to reproduce the issue:**

```
from tensorflow.python.ops import gen_math_ops
from tensorflow.python.framework import ops
from tensorflow.python.framework import sparse_tensor
from tensorflow.python.framework import constant_op
import tensorflow as tf
import numpy as np

class RepeatLayers(tf.keras.layers.Layer):
    def __init__(self, axis=0):
        super(RepeatLayers, self).__init__()
        self.axis = axis

    def _all_dimensions(self, x):
        if isinstance(x, ops.Tensor) and x.get_shape().ndims is not None:
          return constant_op.constant(
              np.arange(x.get_shape().ndims), dtype=tf.int32)
        if (isinstance(x, sparse_tensor.SparseTensor) and x.dense_shape.get_shape().is_fully_defined()):
          r = x.dense_shape.get_shape().dims[0].value
          return constant_op.constant(tf.arange(r), dtype=tf.int32)

        return gen_math_ops._range(0, rank(x), 1)
    
    def _tile_one_dimension(self, data, axis, multiple):
        if data.shape.ndims is not None:
          multiples = [1] * data.shape.ndims
          multiples[axis] = multiple
        else:
          ones_value = tf.ones(tf.rank(data), tf.int32)
          multiples = tf.concat([ones_value[:axis], [multiple], ones_value[axis + 1:]],
                           axis=0)
      
        return tf.tile(data, multiples)

    def repeat_with_axis(self, data, repeats, axis):
        data = tf.convert_to_tensor(data, name='data') # [B, max_len, d]
        repeats = tf.cast(tf.convert_to_tensor(repeats, name='repeats'), tf.int32) # [B, max_len]

        data_shape = tf.shape(data)

        max_repeat = gen_math_ops.maximum(0, gen_math_ops._max(repeats, self._all_dimensions(repeats)))
        mask = tf.sequence_mask(repeats, max_repeat) # [B, max_len, max_value_of_repeat]

        expanded = tf.expand_dims(data, axis+1) # [B, max_len, 1, d]
        tiled = self._tile_one_dimension(expanded, axis+1, max_repeat) # [B, max_len, max_value_of_repeat, d]

        masked = tf.boolean_mask(tiled, mask) 
        result_shape = tf.concat([data_shape[:axis], [-1], data_shape[axis + 1:]], axis=0)
        result = tf.reshape(masked, result_shape)

        return result

    def call(self, encoder_h, repeats):
        return self.repeat_with_axis(data=encoder_h, repeats=repeats, axis=self.axis)


repeat = RepeatLayers(axis=1)

a = tf.keras.Input(shape=[35, 384], dtype=tf.float32)
b = tf.keras.Input(shape=[35], dtype=tf.int32)

output = repeat(a, b)

model = tf.keras.models.Model([a,b], outputs=output)


model.summary()

converter = tf.lite.TFLiteConverter.from_keras_model(model)

tflite_model = converter.convert()



interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()


input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()


input_shape_1 = input_details[0]['shape']
input_shape_2 = input_details[1]['shape']
input_data_1 = np.array(np.random.random_sample(input_shape_1), dtype=np.float32)
input_data_2 = np.array(np.random.random_sample(input_shape_2), dtype=np.int32)


interpreter.set_tensor(input_details[0]['index'], input_data_1)
interpreter.set_tensor(input_details[1]['index'], input_data_2)



interpreter.invoke()

interpreter.invoke()
```

**Other info / logs:**
```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-13-7d35ed1dfe14> in <module>
----> 1 interpreter.invoke()

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py in invoke(self)
    491     """"""
    492     self._ensure_safe()
--> 493     self._interpreter.Invoke()
    494 
    495   def reset_all_variables(self):

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in Invoke(self)
    111 
    112     def Invoke(self):
--> 113         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)
    114 
    115     def InputIndices(self):

RuntimeError: tensorflow/lite/kernels/range.cc:39 (start > limit && delta < 0) || (start < limit && delta > 0) was not true.Node number 6 (RANGE) failed to invoke.
```"
35660,adamax on tf.keras gpu 1.14.0 frequently crashes with this error,"optimizer='adamax' with tf.keras 1.14.0 frequently crashes with this error (training on gpu), it also frequently produces NaNs in training when not producing this error. Other optimizers are fine (no crashes).

  File "".../.conda/envs/tensorflow-gpu_1.14.0/lib/python3.7/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py"", line 166, in fit
    history = self.model.fit(x, y, **fit_args)
  File "".../.conda/envs/tensorflow-gpu_1.14.0/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 780, in fit
    steps_name='steps_per_epoch')
  File "".../.conda/envs/tensorflow-gpu_1.14.0/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 419, in model_iteration
    callbacks.on_epoch_end(epoch, epoch_logs)
  File "".../.conda/envs/tensorflow-gpu_1.14.0/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py"", line 311, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File "".../.conda/envs/tensorflow-gpu_1.14.0/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py"", line 1247, in on_epoch_end
    self.model.set_weights(self.best_weights)
  File "".../.conda/envs/tensorflow-gpu_1.14.0/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1105, in set_weights
    if len(params) != len(weights):
TypeError: object of type 'NoneType' has no len()"
35657,'AttentionWrapper' object has no attribute 'zero_state' ,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
35656,Custom bidirectional LSTM cell throws unexpected keyword argument 'name',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
N/A - tested on colab only
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
2.1.0-rc1
- Python version:
3.6.9
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
GCC 8.3.0
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
throws unexpected keyword argument 'name' error

**Describe the expected behavior**
should not throw error

**Code to reproduce the issue**
https://colab.research.google.com/drive/17B_MLss_dWjJCBvyMRCWFga31Im2eesc

**Other info / logs**
```
from __future__ import absolute_import, division, print_function, unicode_literals

%tensorflow_version 2.x

import tensorflow as tf
import sys

print(tf.version.GIT_VERSION, tf.version.VERSION)
print(sys.version)

class TestLSTMCell(tf.keras.layers.LSTMCell):
    def __init__(self, units):
        # what's missing here to prevent ""TypeError: __init__() got an unexpected keyword argument 'name'""
        super(TestLSTMCell, self).__init__(units)

inputs = tf.keras.Input((128, 256))
cell = TestLSTMCell(512)
forward_layer = tf.keras.layers.RNN(cell, return_state=True)

outputs = forward_layer(inputs)
print(""works !!!"")

bidirectional_rnn = tf.keras.layers.Bidirectional(forward_layer, merge_mode=""concat"")
print(""nope"")
```

See colab link for stack trace."
35655,tensorflow master window build fail,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version:master
- Python version:3.6
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):1.1.0
- GCC/Compiler version (if compiling from source):vs 2015
- CUDA/cuDNN version:no cuda 
- GPU model and memory:no



**Describe the problem**

use below compile command:
python ./configure.py
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

build fail

**Any other info / logs**

master window compile fail，ERROR: E:/work/work/gitlab/github/tensorflow/tensorflow/core/framework/BUILD:450:1: C++ compilation of rule '//tensorflow/core/framework:allocator_registry_impl' failed (Exit 2)
c:\users\yinxungong\_bazel_yinxungong\4pfkvgbi\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\cxx11\src/Tensor/TensorBlock.h(950): error C2061: 语法错误: 标识符“Kind”
c:\users\yinxungong\_bazel_yinxungong\4pfkvgbi\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\cxx11\src/Tensor/TensorBlock.h(1056): note: 参见对正在编译的类 模板 实例化“Eigen::internal::StridedLinearBufferCopy<Scalar,IndexType>”的引用
c:\users\yinxungong\_bazel_yinxungong\4pfkvgbi\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\cxx11\src/Tensor/TensorBlock.h(959): error C2061: 语法错误: 标识符“Kind”
"
35654,LSTM & LSTMCell with dropout are not working in Subclassed Keras Model.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave, no GPU
- TensorFlow installed from (source or binary):pip
- TensorFlow version (use command below):2.0.0
- Python version:3.7

**Describe the current behavior**

LSTM with dropout are not working in customized keras model. If the dropout is set to 0 the codes work.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf
from tensorflow.keras.layers import *
import numpy

assert tf.__version__==""2.0.0"", f""Expect TF-2.0.0 but get {tf.__version__}""

class multiplicative_attention_model(tf.keras.Model):
	""""""
	A Tensorflow 2.0 implementation of encoder-decoder attention model as illustrated in Effective Approaches to Attention-based Neural Machine Translation.
	The following types of attentions are implemented:
	• Global attention with dot alignment
	• Local attention with monotonic alignment
	
	# Arguments:

		attn_mode: `global` or `local`
		input_shape: shape of input data, should be a two-dimensional tuple (batch, n_step)
		input_vocab_size: size of input vocabulary size excluding <EOS>
		output_vocab_size: size of output vocabulary size excluding <EOS>
		input_embed_dim: word embedding dimension for input
		hidden_state_dim: hidden state dimension for encoder and decoder LSTM
		local_attn_window: the context vector will be extracted from window [current_position ± local_attn_window]
		name: name of the model

	# Paper:

		https://www-nlp.stanford.edu/pubs/emnlp15_attn.pdf

	# Examples

	```python
		# Expected shape for input data and output data
		attn_mode = ""global""
		input_shape = (None,40)
		input_vocab_size = 200
		output_vocab_size = 300

		# Initialize Model
		model = multiplicative_attention_model(attn_mode, input_shape, input_vocab_size, output_vocab_size)

		# Compile Model
		learning_rate = 1e-3
		model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,
					  optimizer=tf.keras.optimizers.Adam(learning_rate),
					  metrics=[tf.keras.metrics.sparse_categorical_accuracy]
					  )

		# Call Model once to build the model
		_ = model(tf.ones(shape=(1,40)))

		# Take a look at model
		model.summary()
	```
	""""""
	def __init__(self, attn_mode, input_shape, input_vocab_size, output_vocab_size, name=""Multiplicative-Attention"", input_embed_dim=200, hidden_state_dim=200, local_attn_window=None, **kwargs):
		super(multiplicative_attention_model, self).__init__(name=name, **kwargs)

		# Expect attention model to be either global or local
		assert attn_mode == ""global"" or attn_mode == ""local"", f""Expect 'global' or 'local' for attn_mode, get {attn_mode} instead""
		self.attn_mode = attn_mode

		# Expect input shape to be two-dimensional
		assert isinstance(input_shape, tuple), ""Expect tuple for input_shape, get {type(input_shape)} instead""
		assert len(input_shape) == 2, ""Expect 2-dim tuple for input_shape, get {len(input_shape)}-dim instead""
		_, self._n_step = input_shape

		# Embedding layer for input
		self._embedding_layer = Embedding(input_dim=input_vocab_size+1, output_dim=input_embed_dim, input_length=self._n_step)

		# LSTM encoder
		self._lstm_encoder_layer_1 = LSTM(units=hidden_state_dim, dropout=0.2, recurrent_dropout=0.2, activation=""tanh"", recurrent_activation=""sigmoid"", use_bias=True, return_sequences=True, return_state=True)
		self._lstm_encoder_layer_2 = LSTM(units=hidden_state_dim, dropout=0.2, recurrent_dropout=0.2, activation=""tanh"", recurrent_activation=""sigmoid"", use_bias=True, return_sequences=True, return_state=True)

		# LSTM decoder
		self._lstm_decoder_cell_1 = LSTMCell(units=hidden_state_dim, dropout=0.2, recurrent_dropout=0.2, activation=""tanh"", recurrent_activation=""sigmoid"", use_bias=True)
		self._lstm_decoder_cell_2 = LSTMCell(units=hidden_state_dim, dropout=0.2, recurrent_dropout=0.2, activation=""tanh"", recurrent_activation=""sigmoid"", use_bias=True)
		self._decoder_state_array = tf.TensorArray(dtype=tf.float32, size=self._n_step, clear_after_read=True)

		# Attention utility
		if self.attn_mode == ""global"":
			pass
		if self.attn_mode == ""local"":
			assert isinstance(local_attn_window, int), f""Expect integer for local_attn_window, get {type(local_attn_window)} instead""
			assert local_attn_window > 0, ""Expect positive value for local_attn_window""
			self.D = local_attn_window

		# Utility layer
		self._concat_layer = Concatenate()

		# Output layer
		self._ffnn_decoder_sequence_layer = TimeDistributed(Dense(output_vocab_size+1))
	
	def call(self, inputs, training=False):
		# Embedding
		inputs_embed = self._embedding_layer(inputs)

		# Encoder
		encoder_1_sequence, encoder_1_hidden_state, encoder_1_cell_state = self._lstm_encoder_layer_1(inputs=inputs_embed, training=training)
		encoder_2_sequence, encoder_2_hidden_state, encoder_2_cell_state = self._lstm_encoder_layer_2(inputs=encoder_1_sequence, initial_state=[encoder_1_hidden_state, encoder_1_cell_state], training=training)
		decoder_1_input = tf.transpose(encoder_2_sequence, [1, 0, 2])
		decoder_1_states = [encoder_1_hidden_state, encoder_1_cell_state]
		decoder_2_states = [encoder_2_hidden_state, encoder_2_cell_state]

		# Decoder
		for step in range(self._n_step):
			decoder_1_hidden_state, decoder_1_states = self._lstm_decoder_cell_1(inputs=decoder_1_input[step],states=decoder_1_states, training=training)
			decoder_2_hidden_state, decoder_2_states = self._lstm_decoder_cell_2(inputs=decoder_1_hidden_state, states=decoder_2_states, training=training)
			if self.attn_mode == ""global"":
				attention_score = tf.nn.softmax(tf.einsum(""ijk,ik->ij"", encoder_2_sequence, decoder_2_hidden_state))
				context_vector = tf.einsum(""ijk,ij->ik"", encoder_2_sequence, attention_score)
			if self.attn_mode == ""local"":
				lb, ub = max(0, step-self.D), min(self._n_step-1,step+self.D)
				encoder_2_sequence_part = encoder_2_sequence[:,lb:ub+1,:]
				attention_score = tf.nn.softmax(tf.einsum(""ijk,ik->ij"", encoder_2_sequence_part, decoder_2_hidden_state))
				context_vector = tf.einsum(""ijk,ij->ik"", encoder_2_sequence_part, attention_score)
			output_vector = self._concat_layer([decoder_2_hidden_state, context_vector])
			self._decoder_state_array = self._decoder_state_array.write(step,output_vector)
		decoder_sequence = tf.transpose(self._decoder_state_array.stack(), [1, 0, 2])

		# Softmax outputs
		outputs = tf.nn.softmax(self._ffnn_decoder_sequence_layer(decoder_sequence))
		return outputs

attn_mode = ""local""
input_shape = (None,20)
input_vocab_size = 200
output_vocab_size = 300

# Initialize Model
model = multiplicative_attention_model(attn_mode, input_shape, input_vocab_size, output_vocab_size, local_attn_window=4)

# Compile Model
learning_rate = 1e-2
model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,
              optimizer=tf.keras.optimizers.Adam(learning_rate),
              metrics=[tf.keras.metrics.sparse_categorical_accuracy]
              )

x = tf.constant(np.random.choice(range(201),size=(10000,20)))
y = tf.constant(np.random.choice(range(301),size=(10000,20)))

model.fit(x,y,batch_size=1024)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     60                                                op_name, inputs, attrs,
---> 61                                                num_outputs)
     62   except core._NotOkStatusException as e:

TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: Multiplicative-Attention_6/lstm_cell_34/cond/Identity:0

During handling of the above exception, another exception occurred:

_SymbolicException                        Traceback (most recent call last)
<ipython-input-31-fab8afc8ed12> in <module>
----> 1 model.fit(x,y,batch_size=64)

~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--> 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    322                 mode=ModeKeys.TRAIN,
    323                 training_context=training_context,
--> 324                 total_epochs=epochs)
    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    326 

~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    121         step=step, mode=mode, size=current_batch_size) as batch_logs:
    122       try:
--> 123         batch_outs = execution_function(iterator)
    124       except (StopIteration, errors.OutOfRangeError):
    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
     84     # `numpy` translates Tensors to values in Eager mode.
     85     return nest.map_structure(_non_none_constant_value,
---> 86                               distributed_function(input_fn))
     87 
     88   return execution_function

~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--> 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    518         # Lifting succeeded, so variables are initialized and we can run the
    519         # stateless function.
--> 520         return self._stateless_fn(*args, **kwds)
    521     else:
    522       canon_args, canon_kwds = \

~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   1821     """"""Calls a graph function specialized to the inputs.""""""
   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1824 
   1825   @property

~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
   1139          if isinstance(t, (ops.Tensor,
   1140                            resource_variable_ops.BaseResourceVariable))),
-> 1141         self.captured_inputs)
   1142 
   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1222     if executing_eagerly:
   1223       flat_outputs = forward_function.call(
-> 1224           ctx, args, cancellation_manager=cancellation_manager)
   1225     else:
   1226       gradient_name = self._delayed_rewrite_functions.register()

~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    509               inputs=args,
    510               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 511               ctx=ctx)
    512         else:
    513           outputs = execute.execute_with_cancellation(

~/.virtualenvs/python3env/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     73       raise core._SymbolicException(
     74           ""Inputs to eager execution function cannot be Keras symbolic ""
---> 75           ""tensors, but found {}"".format(keras_symbolic_tensors))
     76     raise e
     77   # pylint: enable=protected-access

_SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'Multiplicative-Attention_6/lstm_cell_34/cond/Identity:0' shape=(None, 200) dtype=float32>, <tf.Tensor 'Multiplicative-Attention_6/lstm_cell_34/cond_4/Identity:0' shape=(None, 200) dtype=float32>, <tf.Tensor 'Multiplicative-Attention_6/lstm_cell_35/cond/Identity:0' shape=(None, 200) dtype=float32>, <tf.Tensor 'Multiplicative-Attention_6/lstm_cell_35/cond_4/Identity:0' shape=(None, 200) dtype=float32>]
```"
35653,tensorflow 2.1 build error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary):  source 2.1
- TensorFlow version: 2.1.0
- Python version: 3.7.6
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version:  10.2/ 7.6.5
- GPU model and memory: RTX2080Ti GDDR6 11GB



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel build //tensorflow/tools/pip_package:build_pip_packagee


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
ERROR: /home/wmind/repo/tensorflow/tensorflow/python/BUILD:1270:1: C++ compilation of rule '//tensorflow/python:_op_def_registry.so' failed (Exit 1)
In file included from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/pytypes.h:12:0,
                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/cast.h:13,
                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/attr.h:13,
                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/pybind11.h:44,
                 from ./tensorflow/python/lib/core/pybind11_status.h:21,
                 from tensorflow/python/framework/op_def_registry.cc:20:
bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/detail/common.h:302:12: error: multiple definition of ‘enum class pybind11::return_value_policy’
 enum class return_value_policy : uint8_t {
            ^~~~~~~~~~~~~~~~~~~
In file included from external/pybind11/include/pybind11/pytypes.h:12:0,
                 from external/pybind11/include/pybind11/cast.h:13,
                 from external/pybind11/include/pybind11/attr.h:13,
                 from external/pybind11/include/pybind11/pybind11.h:49,
                 from tensorflow/python/framework/op_def_registry.cc:16:
external/pybind11/include/pybind11/detail/common.h:302:12: note: previous definition here
 enum class return_value_policy : uint8_t {
            ^~~~~~~~~~~~~~~~~~~
In file included from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/pytypes.h:12:0,
                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/cast.h:13,
                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/attr.h:13,
                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/pybind11.h:44,
                 from ./tensorflow/python/lib/core/pybind11_status.h:21,
                 from tensorflow/python/framework/op_def_registry.cc:20:
bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/detail/common.h: In function ‘constexpr int pybind11::detail::log2(pybind11::size_t, int)’:
bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/detail/common.h:355:29: error: redefinition of ‘constexpr int pybind11::detail::log2(pybind11::size_t, int)’
 inline static constexpr int log2(size_t n, int k = 0) { return (n <= 1) ? k : log2(n >> 1, k + 1); }
                             ^~~~
In file included from external/pybind11/include/pybind11/pytypes.h:12:0,
                 from external/pybind11/include/pybind11/cast.h:13,
                 from external/pybind11/include/pybind11/attr.h:13,
                 from external/pybind11/include/pybind11/pybind11.h:49,
                 from tensorflow/python/framework/op_def_registry.cc:16:
external/pybind11/include/pybind11/detail/common.h:355:29: note: ‘constexpr int pybind11::detail::log2(pybind11::size_t, int)’ previously defined here
 inline static constexpr int log2(size_t n, int k = 0) { return (n <= 1) ? k : log2(n >> 1, k + 1); }
                             ^~~~
In file included from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/pytypes.h:12:0,
                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/cast.h:13,
                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/attr.h:13,
                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/pybind11.h:44,
                 from ./tensorflow/python/lib/core/pybind11_status.h:21,
                 from tensorflow/python/framework/op_def_registry.cc:20:
bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/detail/common.h: In function ‘constexpr pybind11::size_t pybind11::detail::size_in_ptrs(pybind11::size_t)’:
bazel-out/k8-opt/bin/external/local_config_python/python_include/pybind11/detail/common.h:358:32: error: redefinition of ‘constexpr pybind11::size_t pybind11::detail::size_in_ptrs(pybind11::size_t)’
 inline static constexpr size_t size_in_ptrs(size_t s) { return 1 + ((s - 1) >> log2(sizeof(void *))); }
                                ^~~~~~~~~~~~
```"
35651,"Tensorflow model.fit ""use_multiprocessing"" ""distribution_strategy"" ""adapter_cls"" ""failed to find data adapter that can handle""","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- I have written custom code:
- Windows 10 - Anaconda):
-Tensor flow installed from Anaconda Repo, 
- TensorFlow version (use command below): unknown 2.0.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: GTX 1070 4 gb


**Describe the current behavior**

I have written two different files for an MRI image dataset i prepared (i am a doctor, not a software engineer)

```
import numpy as np
import matplotlib.pyplot as plt
import os
import cv2
DATADIR = ""F:/MRI_data_final""
CATEGORIES = [""DarKanal"", ""Disk""]

for category in CATEGORIES:
    path = os.path.join(DATADIR, category)
    for img in os.listdir(path):
        img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)
        plt.imshow(img_array, cmap=""gray"")
        plt.show()
        break
    break
        
print(img_array)
print(img_array.shape)

IMG_SIZE = 200

new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
plt.imshow(new_array, cmap = 'gray')
plt.show()

training_data =[]

def create_training_data():
    for category in CATEGORIES:
        path = os.path.join(DATADIR, category)
        class_num = CATEGORIES.index(category)
        for img in os.listdir(path):
            try:
                img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)
                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
                training_data.append([new_array, class_num])
            except Exception as e:
                print(e)
                
create_training_data()

    
import random
random.shuffle(training_data)

X = []
y = []

for features, label in training_data:
    X.append(features)
    y.append(label)
    
X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)

import pickle

pickle_out = open(""X.pickle"",""wb"")
pickle.dump(X, pickle_out)
pickle_out.close()

pickle_out = open(""y.pickle"", ""wb"")
pickle.dump(y, pickle_out)
pickle_out.close()
```


I load data to X.pickle and y.pickle files, and when i read data from them, it reads. 

Then I create the new file for Neural Network Model,
```
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
import pickle

X = pickle.load(open(""X.pickle"", ""rb""))
y = pickle.load(open(""y.pickle"", ""rb""))

model = Sequential()
model.add( Conv2D(64, (3,3), input_shape = X.shape[1:])   )
model.add(Activation(""relu""))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(64, (3,3)))
model.add(Activation(""relu""))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Flatten())
model.add(Dense(64))

model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss=""binary_crossentropy"",
              optimizer=""adam"",
              metrics=['accuracy'])

model.fit(X, y, batch_size=32, validation_split=0.1)
```

I am using a Jupyter Notebook and the program works until the last line (model.fit) but after the last line it returns this:

Also note that i ran the code in both CPU and GPU environments... 

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-6-170db6ee9b5d> in <module>
----> 1 model.fit(X, y, batch_size=32, validation_split=0.1)
      2 time.sleep(0.1)

~\Anaconda3\envs\PythonGPU-MRI\lib\site-packages\tensorflow_core\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--> 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

~\Anaconda3\envs\PythonGPU-MRI\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    222           validation_data=validation_data,
    223           validation_steps=validation_steps,
--> 224           distribution_strategy=strategy)
    225 
    226       total_samples = _get_total_number_of_samples(training_data_adapter)

~\Anaconda3\envs\PythonGPU-MRI\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in _process_training_inputs(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)
    495                      'at same time.')
    496 
--> 497   adapter_cls = data_adapter.select_data_adapter(x, y)
    498 
    499   # Handle validation_split, we want to split the data and get the training

~\Anaconda3\envs\PythonGPU-MRI\lib\site-packages\tensorflow_core\python\keras\engine\data_adapter.py in select_data_adapter(x, y)
    651         ""Failed to find data adapter that can handle ""
    652         ""input: {}, {}"".format(
--> 653             _type_name(x), _type_name(y)))
    654   elif len(adapter_cls) > 1:
    655     raise RuntimeError(

ValueError: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {""<class 'int'>""})
```

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35650,InvalidArgumentError if np.ndarray is registered as Sequence type,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: CPU

**Describe the current behavior**

For reference, numpy is planning to register numpy ndarray as a Sequence:
https://github.com/numpy/numpy/issues/2776

The sample ResNet50 code from https://keras.io/applications/ runs fine. But, if ndarray is registered as a sequence, then TF2 throws an InvalidArgumentError.

**Code to reproduce the issue**

```
from keras.applications.resnet50 import ResNet50
from keras.preprocessing import image
from keras.applications.resnet50 import preprocess_input, decode_predictions
import numpy as np
import typing
typing.Sequence.register(np.ndarray)

model = ResNet50(weights='imagenet')

img_path = 'elephant.jpg'
img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

preds = model.predict(x)
# decode the results into a list of tuples (class, description, probability)
# (one such list for each sample in the batch)
print('Predicted:', decode_predictions(preds, top=3)[0])
# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]
```

Two extra lines added to the sample ResNet50 code are:

```
import typing
typing.Sequence.register(np.ndarray)
```

The error is:

```
2020-01-07 13:48:16.421816: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: The first dimension of padding\s must be the rank of inputs[4,2] []
         [[{{node conv1_pad/Pad}}]]
Traceback (most recent call last):
  File ""test_d3m_imports.py"", line 16, in <module>
    preds = model.predict(x)
  File ""/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/keras/engine/training.py"", line 1462, in predict
    callbacks=callbacks)
  File ""/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/keras/engine/training_arrays.py"", line 324, in predict_loop
    batch_outs = f(ins_batch)
  File ""/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py"", line 3740, in __call__
    outputs = self._graph_fn(*converted_inputs)
  File ""/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1081, in __call__
    return self._call_impl(args, kwargs)
  File ""/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1121, in _call_impl
    return self._call_flat(args, self.captured_inputs, cancellation_manager)
  File ""/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File ""/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 511, in call
    ctx=ctx)
  File ""/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError:  The first dimension of paddings must be the rank of inputs[4,2] []
         [[node conv1_pad/Pad (defined at /data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [O\p:__inference_keras_scratch_graph_10370]

Function call stack:
keras_scratch_graph

```
"
35649,Incorrect name_scope with tf.function decoration,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
If a function is decorated with tf.function, the name_scope is lost

**Describe the expected behavior**
The name_scope should be same with or without tf.function decoration

**Code to reproduce the issue**
```python
import tensorflow as tf

@tf.function
def f():
    with tf.name_scope(""f"") as scope:
        tf.print(scope)

def g():
    with tf.name_scope(""g"") as scope:
        tf.print(scope)

def main():
    with tf.name_scope(""main""):
        f()   # expect to print ""main/f/"", actually get ""f/""
        g()

if __name__ == ""__main__"":
    main()
```
The output is
```
f/
main/g/
```

"
35648,Malformed `TRAINABLE_ATTRIBUTE_NOTE` in batch normalization documentation,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization?version=stable

## Description of issue (what needs changing):

After the references list there is a stray malformed tag:

{ {TRAINABLE_ATTRIBUTE_NOTE}}

I suspect that this is supposed to resolve to the note that `moving_mean` and `moving_variance` are placed in `UPDATE_OPS` and need to be executed alongside the training op.  (This note is present in the `tf.layers` doc: https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers/batch_normalization)  But without knowing what this tag refers to I can't really say for sure.  (The tag is present in only three places in the Tensorflow codebase and shows up malformed on the website in each case.)
"
35647,"tf.strings.lower and tf.strings.upper have ""TODO"" docstrings","## URL(s) with the issue:

* lower: https://www.tensorflow.org/api_docs/python/tf/strings/lower?version=stable
* upper: https://www.tensorflow.org/api_docs/python/tf/strings/upper?version=stable

## Description of issue (what needs changing):

### Clear description

The first line of the docstrings for these functions is the auto-generated ""TODO: add doc."" instead of an actual summary.

### Submit a pull request?

Yes, I'll be submitting one shortly."
35646,ERROR: Could not find a version that satisfies the requirement tensorflow,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac Air, Moave (10.14.5 )

- TensorFlow installed from (source or binary): installed using: pip3 install tensorflow
- TensorFlow version: 2.0
- Python version: 3.6.5 64Bit
- Installed using: pip


**Describe the problem**
Using in RStudio, I installed R packages: 

devtools::install_github('rstudio/tensorflow')

devtools::install_github('rstudio/keras')

tried:

install_tensorflow()
tensorflow::install_tensorflow()

but both gave me an error message:

ERROR: Could not find a version that satisfies the requirement tensorflow==2.0.0 (from versions: none)
ERROR: No matching distribution found for tensorflow==2.0.0
Error: Error installing package(s): 'tensorflow==2.0.0'

I checked page after page and found that I needed 3.6.x version of python, so I reverted and it did nothing. I re-installed tensorflow and nothing changed. 

When I checked on python tensorflow worled but for some reason I can't get it to work on RStudio.

Any help please?

Thanks
Mark"
35645,Error trying to build for AVX2 on CPU,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.0.0
- Python version: 3.6.4
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10/7
- GPU model and memory: NVIDIA GeForce MX150


**Describe the problem**
I am trying to build tensorflow with AVX2 instructions for my CPU to increase speeds, but bazel keeps failing when loading tensorflow

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Cloned the tensorflow source from github and checked out the r2.0 branch.
2. Ran python ./configure.py
3. Proceded through steps, selecting no for all other builds and set the bazel config option to /arch:AVC2
4. Ran bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
The error text is attached here:
[error.txt](https://github.com/tensorflow/tensorflow/files/4032375/error.txt)
"
35642,mysql-client can not be installed FROM official tensorflow/tensorflow image,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): official tensorflow image from docker hub
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): n/a
- TensorFlow version: 1.15.0
- Python version: 3
- Installed using virtualenv? pip? conda?: Docker
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10
- GPU model and memory: n/a



**Describe the problem**
Simple image built `FROM tensorflow/tensorflow:tag` fails to build when including `mysql-client`

```dockerfile
FROM tensorflow/tensorflow:1.15.0-gpu-py3-jupyter

# Avoid ERROR: invoke-rc.d: policy-rc.d denied execution of start.
# RUN sed -i ""s/^exit 101$/exit 0/"" /usr/sbin/policy-rc.d

# --- Install any needed packages specified in requirements.apt
COPY requirements.apt .
RUN apt-get update && xargs apt-get install -y < requirements.apt

# --- Install any needed packages specified in requirements.pip
COPY requirements.pip .
RUN pip install -U --trusted-host pypi.python.org -r requirements.pip

# activate jupyter extensions
RUN jupyter contrib nbextension install \
  && jupyter nbextension enable codefolding/main \
  && jupyter nbextension enable collapsible_headings/main
```


**Provide the exact sequence of commands / steps that you executed before running into the problem**

A MWE [repo](https://gitlab.com/SumNeuron/mytf) contains the following files:

- `Dockerfile.ai`: custom image built on top of official tensorflow/tensorflow
- `docker-compose.ai.development.yml`: specifies `Dockerfile.ai` as build file and mounts `notebooks` directory
- `requirements.pip`: pip requirements that may be used in images other than `Dockerfile.ai`
- `requirements.apt`: packages needed to be installed via `apt-get`

For convenience a python script `docker.py` is provided, e.g. 
```
python docker.py -c {build | up | down}
```
instead of 
```
docker-compose -f docker-compose.ai.development.yml {build | up | down}
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35639,Pipeline & TypeError: can't pickle _thread.RLock objects,"
**System information**
- colab script
# Install the latest Tensorflow version.
!pip3 install --upgrade tensorflow-gpu
# Install TF-Hub.
!pip3 install tensorflow-hub
!pip3 install seaborn
- Python version:
Python 3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0] on linux

Using google universal sentence encoder ( see:[https://tfhub.dev/google/universal-sentence-encoder/4]( https://tfhub.dev/google/universal-sentence-encoder/4) )
in a scikit Pipeline return following error:
TypeError: can't pickle _thread.RLock objects
I suppose the point is that it is not possible to clone the object with:
from sklearn.base import clone

Here you can find code to reproduce error:

```

!pip3 install --upgrade tensorflow-gpu
!pip3 install tensorflow-hub
!pip3 install seaborn

module = hub.Module(href)
module_url = ""https://tfhub.dev/google/universal-sentence-encoder/4"" #@param [""https://tfhub.dev/google/universal-sentence-encoder/4"", ""https://tfhub.dev/google/universal-sentence-encoder-large/5""]
model = hub.load(module_url)
print (""module %s loaded"" % module_url)
def embed(input):
  return model(input)

class UnivEmbedding( BaseEstimator, TransformerMixin ):
  #Class Constructor 
  def __init__( self, module, use_light=True, verbose=False):
       self.module = module
       self.use_light= use_light
       self.verbose= verbose 

  #Return self nothing else to do here   
  def fit( self, X, y = None ):
    return self 

  #Method that describes what we need this transformer to do
  def transform( self, X, y = None ):
    return embed(X) # universal_embedding (self.module, X, self.use_light, self.verbose)

  def fit_transform( self, X, y = None ):  
    if self.verbose: print(self.module)
    return embed(X) #universal_embedding (self.module, X, self.use_light, self.verbose)

  def get_params(self, deep=True):
    return {""module"": self.module, ""use_light"": self.use_light, ""verbose"": self.verbose}

  def set_params(self, **parameters):
    for parameter, value in parameters.items():
        setattr(self, parameter, value)
    return self


univ_emb= UnivEmbedding(module, use_light=False, verbose=False)

clone (univ_emb)
```

Is it a bug? Can you help me?"
35637,tensorflow for arm64 issue,"Hi,

I have successfully built the tensorflow for arm64 and indeed it created libtensorflow-lite.a However I'm trying to import the tesorflow in one of my python script like

$ cat test.py
import numpy
import tensorflow

It is showing No named modules something like.

$ python3 test.py 
Traceback (most recent call last):
  File ""test_imports/test_ai.py"", line 2, in <module>
    import tensorflow
ModuleNotFoundError: No module named 'tensorflow'

Do we need to do any setup to python to picking the tensorflow here? or did I miss any step.

Any help?
"
35636,Failed to train with keras model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): 
source
- TensorFlow version (use command below): 
tf2.0.0
- Python version:
 3.5
- CUDA/cuDNN version: 
CUDA10
- GPU model and memory: 
nvidia V100  8*GPU 450G



**Describe the current behavior**
I try to train a tf.keras.applications.MobileNetV2 on the dataset ""tf_flowers"". However, I have two styles of training loo: 1) keras style  2) tensorflow style.
As a result, keras style works but tensorflow style doesn't learn anything.

result:
keras style:
![image](https://user-images.githubusercontent.com/33815430/71894611-b975b100-3189-11ea-8f9b-896069c057d2.png)


tensorflow style:
![image](https://user-images.githubusercontent.com/33815430/71894296-cc3bb600-3188-11ea-8ad4-e65d5b139ca1.png)


**Describe the expected behavior**
I expect Tensorflow style training loop could work. I am pretty sure my training loop is corrcet, 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

Tensorflow style training loop:
```python
import os, math, json
import numpy as np
from matplotlib import pyplot as plt
import tensorflow as tf

print(""Tensorflow version "" + tf.__version__)
# tf.enable_eager_execution()
AUTO = tf.data.experimental.AUTOTUNE

HAS_COLAB_TPU = 'COLAB_TPU_ADDR' in os.environ
assert not HAS_COLAB_TPU, ""Please select a GPU backend for this notebook. Pre-trained models in tf.keras.applications.* are not yet TPU-compatible""

GCS_PATTERN = 'gs://flowers-public/tfrecords-jpeg-192x192-2/*.tfrec'
IMAGE_SIZE = [192, 192]

BATCH_SIZE = 64 # 128 works on GPU too but comes very close to the memory limit of the Colab GPU
EPOCHS = 5

VALIDATION_SPLIT = 0.19
CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips'] # do not change, maps to the labels in the data (folder names)

# splitting data files between training and validation
filenames = tf.io.gfile.glob(GCS_PATTERN)
split = int(len(filenames) * VALIDATION_SPLIT)
training_filenames = filenames[split:]
validation_filenames = filenames[:split]

print(""Pattern matches {} data files. Splitting dataset into {} training files and {} validation files"".format(len(filenames), len(training_filenames), len(validation_filenames)))
validation_steps = int(3670 // len(filenames) * len(validation_filenames)) // BATCH_SIZE
steps_per_epoch = int(3670 // len(filenames) * len(training_filenames)) // BATCH_SIZE
print(""With a batch size of {}, there will be {} batches per training epoch and {} batch(es) per validation run."".format(BATCH_SIZE, steps_per_epoch, validation_steps))

def read_tfrecord(example):
    features = {
        ""image"": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring
        ""class"": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar
    }
    example = tf.io.parse_single_example(serialized=example, features=features)
    image = tf.image.decode_jpeg(example['image'], channels=3)
    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range
    # image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size will be needed for TPU
    image = tf.image.resize(image, (192, 192))
    class_label = example['class']
    return image, class_label

def load_dataset(filenames):
  # read from TFRecords. For optimal performance, use ""interleave(tf.data.TFRecordDataset, ...)""
  # to read from multiple TFRecord files at once and set the option experimental_deterministic = False
  # to allow order-altering optimizations.

  option_no_order = tf.data.Options()
  option_no_order.experimental_deterministic = False

  dataset = tf.data.Dataset.from_tensor_slices(filenames)
  dataset = dataset.with_options(option_no_order)
  #dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=16)
  dataset = dataset.interleave(tf.data.TFRecordDataset, cycle_length=16, num_parallel_calls=AUTO) # faster
  dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)
  return dataset


def get_batched_dataset(filenames):
  dataset = load_dataset(filenames)
  # dataset = dataset.cache() # This dataset fits in RAM
  # dataset = dataset.repeat()
  dataset = dataset.batch(BATCH_SIZE) # drop_remainder will be needed on TPU
  dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)
  # should shuffle too but this dataset was well shuffled on disk already
  # For proper ordering of map/batch/repeat/prefetch, see Dataset performance guide: https://www.tensorflow.org/guide/performance/datasets
  return dataset

# instantiate the datasets
train_ds = get_batched_dataset(training_filenames)
test_ds = get_batched_dataset(validation_filenames)



pretrained_model = tf.keras.applications.MobileNetV2(input_shape=[*IMAGE_SIZE, 3], include_top=False, weights='imagenet')
pretrained_model.trainable = True

model = tf.keras.Sequential([
  pretrained_model,
  tf.keras.layers.GlobalAveragePooling2D(),
  tf.keras.layers.Dense(5, activation=""softmax"")
])




loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = tf.keras.optimizers.SGD()

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')


## ----- training loop -----##
@tf.function
def train_step(images, labels):
  # tf.print(images)
  with tf.GradientTape() as tape:
    predictions = model(images)
    loss = loss_object(labels, predictions)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

  train_loss(loss)
  train_accuracy(labels, predictions)
  template = 'Epoch {}, Loss: {}, Accuracy: {}'

@tf.function
def test_step(images, labels):

  predictions = model(images)
  t_loss = loss_object(labels, predictions)

  test_loss(t_loss)
  test_accuracy(labels, predictions)
  print(test_loss.result())

EPOCHS = 20

for epoch in range(EPOCHS):
  # Reset the metrics at the start of the next epoch
  train_loss.reset_states()
  train_accuracy.reset_states()
  test_loss.reset_states()
  test_accuracy.reset_states()
  for images, labels in train_ds:
    # tf.print(images)
    train_step(images, labels)

  for test_images, test_labels in test_ds:
    test_step(test_images, test_labels)

  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
  print(template.format(epoch+1,
                        train_loss.result(),
                        train_accuracy.result()*100,
                        test_loss.result(),
                        test_accuracy.result()*100))
```
keras style:

```python
import os, math, json
import numpy as np
from matplotlib import pyplot as plt
import tensorflow as tf


print(""Tensorflow version "" + tf.__version__)
# tf.enable_eager_execution()
AUTO = tf.data.experimental.AUTOTUNE

HAS_COLAB_TPU = 'COLAB_TPU_ADDR' in os.environ
assert not HAS_COLAB_TPU, ""Please select a GPU backend for this notebook. Pre-trained models in tf.keras.applications.* are not yet TPU-compatible""

GCS_PATTERN = 'gs://flowers-public/tfrecords-jpeg-192x192-2/*.tfrec'
IMAGE_SIZE = [192, 192]

BATCH_SIZE = 64 # 128 works on GPU too but comes very close to the memory limit of the Colab GPU
EPOCHS = 5

VALIDATION_SPLIT = 0.19
CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips'] # do not change, maps to the labels in the data (folder names)

# splitting data files between training and validation
filenames = tf.io.gfile.glob(GCS_PATTERN)
split = int(len(filenames) * VALIDATION_SPLIT)
training_filenames = filenames[split:]
validation_filenames = filenames[:split]

print(""Pattern matches {} data files. Splitting dataset into {} training files and {} validation files"".format(len(filenames), len(training_filenames), len(validation_filenames)))
validation_steps = int(3670 // len(filenames) * len(validation_filenames)) // BATCH_SIZE
steps_per_epoch = int(3670 // len(filenames) * len(training_filenames)) // BATCH_SIZE
print(""With a batch size of {}, there will be {} batches per training epoch and {} batch(es) per validation run."".format(BATCH_SIZE, steps_per_epoch, validation_steps))

def read_tfrecord(example):
    features = {
        ""image"": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring
        ""class"": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar
    }
    example = tf.io.parse_single_example(serialized=example, features=features)
    image = tf.image.decode_jpeg(example['image'], channels=3)
    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range
    # image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size will be needed for TPU
    image = tf.image.resize(image, (192, 192))
    class_label = example['class']
    return image, class_label

def load_dataset(filenames):
  # read from TFRecords. For optimal performance, use ""interleave(tf.data.TFRecordDataset, ...)""
  # to read from multiple TFRecord files at once and set the option experimental_deterministic = False
  # to allow order-altering optimizations.

  option_no_order = tf.data.Options()
  option_no_order.experimental_deterministic = False

  dataset = tf.data.Dataset.from_tensor_slices(filenames)
  dataset = dataset.with_options(option_no_order)
  #dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=16)
  dataset = dataset.interleave(tf.data.TFRecordDataset, cycle_length=16, num_parallel_calls=AUTO) # faster
  dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)
  return dataset


def get_batched_dataset(filenames):
  dataset = load_dataset(filenames)
  # dataset = dataset.cache() # This dataset fits in RAM
  dataset = dataset.repeat()
  dataset = dataset.batch(BATCH_SIZE) # drop_remainder will be needed on TPU
  dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)
  # should shuffle too but this dataset was well shuffled on disk already
  # For proper ordering of map/batch/repeat/prefetch, see Dataset performance guide: https://www.tensorflow.org/guide/performance/datasets
  return dataset

# instantiate the datasets
train_ds = get_batched_dataset(training_filenames)
test_ds = get_batched_dataset(validation_filenames)



pretrained_model = tf.keras.applications.MobileNetV2(input_shape=[*IMAGE_SIZE, 3], include_top=False, weights='imagenet')
pretrained_model.trainable = True

model = tf.keras.Sequential([
  pretrained_model,
  tf.keras.layers.GlobalAveragePooling2D(),
  tf.keras.layers.Dense(5, activation=""softmax"")
])

model.compile(
  optimizer=tf.keras.optimizers.SGD(lr=0.01),
              loss='sparse_categorical_crossentropy',
              metrics=['sparse_categorical_accuracy']
)

model.summary()

history = model.fit(train_ds, steps_per_epoch=steps_per_epoch, epochs=EPOCHS,
                    validation_data=test_ds, validation_steps=validation_steps)

```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35634,"EarlyStopping should restore weights on end of training, not end of epoch","## System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin-19.2.0-x86_64-i386-64bit
- TensorFlow installed from (source or binary): Pipenv
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

## Current behavior

The EarlyStopping callback will restore the best weights only if it itself stops training. For example, if a model is set to train for 40 epochs, uses early stopping with a patience of 5, and the best weights occur in epoch 37, then the best weights will not be restored, and the model will have the weights from epoch 40.

## Expected behavior

EarlyStopping should, when initialized with `restore_best_weights=True`, always restore the best weights when training stops, regardless of the reason why training stopped.

## Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
import tensorflow as tf
from numpy.random import RandomState
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential

N_CLASSES, N_SAMPLES = 5, 500

for seed in [1, 2]:
    print(f""=== Random seed is {seed} ==="")

    tf.random.set_seed(seed)
    rng = RandomState(seed)

    x_train = rng.standard_normal(size=(N_SAMPLES, 10))
    x_test = rng.standard_normal(size=(N_SAMPLES, 10))
    y_train = rng.random_integers(N_CLASSES, size=N_SAMPLES)
    y_test = rng.random_integers(N_CLASSES, size=N_SAMPLES)

    model = Sequential([Dense(32), Dense(N_CLASSES)])
    model.compile(""adam"", ""categorical_crossentropy"", [""accuracy""])
    early_stopping = EarlyStopping(
        ""val_accuracy"", patience=10, verbose=1, restore_best_weights=True
    )
    history = model.fit(
        x_train,
        y_train,
        epochs=20,
        callbacks=[early_stopping],
        verbose=0,
        validation_data=(x_test, y_test),
    )
    best_acc = max(history.history[""val_accuracy""])
    _, eval_acc = model.evaluate(x_test, y_test, verbose=0)
    print(f""Best accuracy in training: {best_acc}. In evaluation: {eval_acc}\n"")
```

### Output from example:

```txt
=== Random seed is 1 ===
Restoring model weights from the end of the best epoch.
Epoch 00011: early stopping
Best accuracy in training: 0.19200000166893005. In evaluation: 0.19200000166893005

=== Random seed is 2 ===
Best accuracy in training: 0.15199999511241913. In evaluation: 0.14000000059604645

```

## Other info / logs

- Numpy version: 1.18.1"
35633,i want to do distributed model training in tensorflow.keras  with multiple ram and one main memory ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
35632,TFLite GPU execution failed,"I trying to run one model on TFite mobile GPU using opencl and getting below error:

> INFO: Initialized TensorFlow Lite runtime.                                                                                           
> INFO: Created TensorFlow Lite delegate for GPU.                                                                                      
> ERROR: Next operations are not supported by GPU delegate:                                                                            
> SPLIT_V: Operation is not supported.                                                                                                 
> First 294 operations will run on the GPU, and the remaining 40 on the CPU.                                                           
> INFO: Initialized OpenCL-based API.                                                                                                  
> Applied GPU delegate.                                                                                                                
> Initialized session in 107237ms.                                                                                                     
> Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.                         
> ERROR: TfLiteGpuDelegate Invoke: Failed to read data from GPU (clEnqueueReadBuffer) - Execution status error for events in wait list 
> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.                                                                       
>                                                                                                                                      
> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list
> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.                                                                                                            
>                                                                                                                                      
> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list
> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.
> 
> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list
> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.
> 
> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list
> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.
> 
> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list
> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.
> 
> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list
> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.
> 
> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list
> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.
> 
> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list
> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.
> 
> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list
> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.
> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU (clEnqueueWriteBuffer) - Execution status error for events in wait list
> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.
> 
> count=138 first=73906 curr=1840 min=1414 max=73906 avg=3142.52 std=6057
> 

Please help me to resolve this issue
"
35631,"ValueError: Expected scalar shape for softmax_cross_entropy_with_logits_sg/Reshape_2:0, saw shape: (?,).","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): pip install tensorflow-gpu==1.12.0
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0
- GPU model and memory: 2G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Traceback (most recent call last):
  File ""D:/workspace/nlp_plat/nlp-platform/algorithm/text_classification/base/workflow_dl.py"", line 769, in <module>
    _tet_fit()
  File ""D:/workspace/nlp_plat/nlp-platform/algorithm/text_classification/base/workflow_dl.py"", line 726, in _tet_fit
    wf_dl.fit()
  File ""D:/workspace/nlp_plat/nlp-platform/algorithm/text_classification/base/workflow_dl.py"", line 183, in fit
    self._fit()
  File ""D:/workspace/nlp_plat/nlp-platform/algorithm/text_classification/base/workflow_dl.py"", line 271, in _fit
    model = self.Graph(self.params)
  File ""D:\workspace\nlp_plat\nlp-platform\algorithm\text_classification\text_cnn\graph.py"", line 15, in __init__
    super().__init__(params)
  File ""D:\workspace\nlp_plat\nlp-platform\algorithm\text_classification\base\graph_dl.py"", line 43, in __init__
    self.build()
  File ""D:\workspace\nlp_plat\nlp-platform\algorithm\text_classification\base\graph_dl.py"", line 55, in build
    self.build_optimize()
  File ""D:\workspace\nlp_plat\nlp-platform\algorithm\text_classification\base\graph_dl.py"", line 202, in build_optimize
    self.optimize = tf.contrib.layers.optimize_loss(self.loss, global_step=self.global_step, learning_rate=self.learning_rate, optimizer=""Adam"")
  File ""D:\code\anaconda3\envs\tensorflow13\lib\site-packages\tensorflow\contrib\layers\python\layers\optimizers.py"", line 155, in optimize_loss
    contrib_framework.assert_scalar(loss)
  File ""D:\code\anaconda3\envs\tensorflow13\lib\site-packages\tensorflow\python\ops\check_ops.py"", line 1264, in assert_scalar
    % (tensor.name, shape))
ValueError: Expected scalar shape for softmax_cross_entropy_with_logits_sg/Reshape_2:0, saw shape: (?,).


"
35630,OutOfRangeError Unknown Error when extracting particular zip file,"Please be aware this issue was originally posted in tensorflow/datasets but I got directed here as it seems that the issue is related to the GFile implementation:
https://github.com/tensorflow/datasets/issues/1337


**Short description**
When the download of imagenet_resized finished and tfds starts extracting/writing records, the program crashes.

You can reproduce this error by downloading the particular zip file manually and extracting it with tensorflow:

http://www.image-net.org/image/downsample/Imagenet32_train_npz.zip

**Environment information**
* Operating System: Windows 10
* Python version: 3.7
* tensorflow-datasets version: 1.3.2
* tensorflow-gpu version: 2.0.0

**Reproduction instructions**
Without TFDS:
```
import zipfile
import tensorflow.compat.v2 as tf

path = 'path/to/file.zip'
with tf.io.gfile.GFile(path, 'rb') as fobj:
  z = zipfile.ZipFile(fobj)
  for member in z.infolist():
    extract_file = z.open(member)
    print(member.filename)
```


With TFDS:

```
import tensorflow_datasets as tfds

imagenet_data, info = tfds.load(name=""imagenet_resized/32x32"", with_info=True, as_supervised=True)
```

**Link to logs**
```
Dl Size...: 100%|██████████| 3414/3414 [22:47<00:00,  2.60 MiB/s]



0 examples [00:00, ? examples/s]Traceback (most recent call last):
  File ""C:\Program Files\Python37\lib\contextlib.py"", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File ""C:\Users\[username]\AppData\Roaming\Python\Python37\site-packages\tensorflow_datasets\core\file_format_adapter.py"", line 199, in incomplete_dir
    yield tmp_dir
  File ""C:\Users\[username]\AppData\Roaming\Python\Python37\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 333, in download_and_prepare
    download_config=download_config)
  File ""C:\Users\[username]\AppData\Roaming\Python\Python37\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 1008, in _download_and_prepare
    max_examples_per_split=download_config.max_examples_per_split,
  File ""C:\Users\[username]\AppData\Roaming\Python\Python37\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 871, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File ""C:\Users\[username]\AppData\Roaming\Python\Python37\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 1033, in _prepare_split
    total=split_info.num_examples, leave=False):
  File ""C:\Users\[username]\AppData\Roaming\Python\Python37\site-packages\tqdm\_tqdm.py"", line 1005, in __iter__
    for obj in iterable:
  File ""C:\Users\[username]\AppData\Roaming\Python\Python37\site-packages\tensorflow_datasets\image\imagenet_resized.py"", line 141, in _generate_examples
    for fname, fobj in archive:
  File ""C:\Users\[username]\AppData\Roaming\Python\Python37\site-packages\tensorflow_datasets\core\download\extractor.py"", line 179, in iter_zip
    z = zipfile.ZipFile(fobj)
  File ""C:\Program Files\Python37\lib\zipfile.py"", line 1225, in __init__
    self._RealGetContents()
  File ""C:\Program Files\Python37\lib\zipfile.py"", line 1288, in _RealGetContents
    endrec = _EndRecData(fp)
  File ""C:\Program Files\Python37\lib\zipfile.py"", line 259, in _EndRecData
    fpin.seek(0, 2)
  File ""C:\Users\[username]\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\[username]\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\lib\io\file_io.py"", line 167, in seek
    offset += self.size()
  File ""C:\Users\[username]\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\lib\io\file_io.py"", line 102, in size
    return stat(self.__name).length
  File ""C:\Users\[username]\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\lib\io\file_io.py"", line 727, in stat
    return stat_v2(filename)
  File ""C:\Users\[username]\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\lib\io\file_io.py"", line 744, in stat_v2
    pywrap_tensorflow.Stat(compat.as_bytes(path), file_statistics)
tensorflow.python.framework.errors_impl.OutOfRangeError: C:\Users\[username]\tensorflow_datasets\downloads\image-net.org_image_downs_Image_train_npzlCJjN-zBsDCdn80BZxJ6qtyTFYcDX7y1OSUjXtuuxPw.zip; Unknown error

Process finished with exit code 1
```
**Expected behavior**
No error"
35629,TPU support in TF2.1 release candidate 2,"I'm trying to run prototype of a code for training a model on TPU, with bfloat16 precision. I'm doing it in google colab notebook.

To do it I install tensorflow-2.1.0rc2 and run the following code:
```
import tensorflow as tf
import os
from tensorflow.keras.mixed_precision import experimental as mixed_precision

def create_model():
    model = tf.keras.models.Sequential()

    model.add(tf.keras.layers.Conv2D(128, (3, 3), input_shape=[32,32,3]))
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))
    model.add(tf.keras.layers.Activation('elu'))

    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(10))
    model.add(tf.keras.layers.Activation('softmax', dtype='float32'))

    return model

#this is for bfloat16 precision
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_policy(policy)

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.config.experimental_connect_to_host(resolver.master())
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)

with strategy.scope():
    model = create_model()
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss=tf.keras.losses.sparse_categorical_crossentropy,
        metrics=[tf.keras.metrics.sparse_categorical_accuracy])
```

After running this code I receive an error:

```
---------------------------------------------------------------------------

NotFoundError                             Traceback (most recent call last)

<ipython-input-4-4b29fabfb5e2> in <module>()
     18 resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
     19 tf.config.experimental_connect_to_host(resolver.master())
---> 20 tf.tpu.experimental.initialize_tpu_system(resolver)
     21 strategy = tf.distribute.experimental.TPUStrategy(resolver)
     22 

3 frames

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

NotFoundError: '__inference__tpu_init_fn_14' is neither a type of a primitive operation nor a name of a function registered in binary running on n-88be52b9-w-0. Make sure the operation or function is registered in the binary running in this process.
```


Here is notebook with code:
https://colab.research.google.com/drive/1SuCZ7AsoT7SZSQBgxQUf_L41ibYpFT0a

This code worked on TF2.0, but ONLY without bfloat16 support(see block with comment in code). What can i do, if I need a TPU on TF2 with bfloat16 support?"
35628,How many items does tf.gradients return?,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/gradients?version=stable

## Description of issue (what needs changing):
It's unclear how many list items are returned from `tf.gradients`.

The second paragraph states that ""It returns a list of Tensor of length `len(xs)` where each tensor is the `sum(dy/dx)` for y in `ys`."" The ""Returns"" section says, ""A list of `sum(dy/dx)` for each x in `xs`.""

So... which one is it? `sum(dy/dx)` for x in `xs` or `sum(dy/dx)` for y in `ys`? Besides the inconsistency, the summation notation in this documentation is ambiguous. When it says ""`sum(dy/dx)` for x in `xs`"" does that mean `dy/dx` is summed over the `ys` axis and there is one element produced for each `xs` or the other way around?

A clarifying example would help and a statement along the lines of ""returns a list of <whatever> with as many elements as `xs`"" (or `ys` -- I don't know).

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
35627,Tensorflow thirdparty libraries are clashing with other same libraries in the project,"**System information**
Linux
tensorflow-2.0.0 
Bazel 0.26.1
Built using virtual environment, python 3

**Describe the problem**
Built **libtensorflow_cc.so & libtensorflow_framework.so** using bazel 0.26.1.
And when linked our existing c++ binary to these libraries, libcurl or opencv calls are clashing with the symbols in tensorflow libraries.

How exactly I can built & include these tenforflow libraries in a larger project without exporting third party symbols. 

Is --config=monolithic the right way ? 
"
35626,TFLite 2.0.0 compilation errors in block_map.cc,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Linux Debian buster
  uname -a output: Linux freewave-ib 4.14.79-g3438de3474 #22 PREEMPT Mon Nov 4 20:16:15 UTC 2019 armv7l GNU/Linux
- Mobile device: N/A
- TensorFlow version: 2.0
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: none of the above
- Bazel version (if compiling from source): Don't know
- GCC/Compiler version (if compiling from source): g++ (Debian 8.3.0-6) 8.3.0, 
  GNU Make 4.2.1
  Built for arm-unknown-linux-gnueabihf
- CUDA/cuDNN version: Don't know
- GPU model and memory: none



**Describe the problem**

When building the Pip package for TFLite, I ran across two bugs in the file `tensorflow/lite/experimental/ruy/block_map.cc` that caused the compilation to exit with an error. 

- The first bug was a missing header file,` profiling/instrumentation.h`.
- The second bug was that the object `gemmlowp` was not declared before it was used.
- There may be additional bugs as well. I stopped looking after I found these two, since I don't know 
  the code well enough to know what to look for.

(Peripheral to this issue, the two relevant scripts needed minor edits before they would work. I've included this information here for completeness, but the C++ source file is the larger, more important issue.
1. I edited the curl commands in `download_dependencies.sh`. The curl commands in lines 63 and 65 were edited to `curl -Lsk`, and the star-zip in line 65 was wrapped in double quotes, `""*zip""` .
2. I inserted the line `TENSORFLOW_TARGET=rpi` in `build_pip_package.sh`, just before line 42, to ensure that it compiled for an armv7l.
)

**Commands used to replicate the issue**

```
git clone -b master --depth=1 http://github.com/tensorflow/tensorflow
cd tensorflow
./tensorflow/lite/tools/make/download_dependencies.sh
./tensorflow/lite/tools/pip_package/build_pip_package.sh
```

**Any other info / logs**

I'm only including the relevant part of stdout/stderr. I can include the entire output upon request.

1. The missing header file: output from build_pip_package.sh
```
[...]
python3 setup.py bdist --plat-name=linux-armv7l bdist_wheel --plat-name=linux-armv7l
running bdist
running bdist_dumb
running build
running build_py
running build_ext
make: Entering directory '/ptp/tensorflow'
g++ -O3 -DNDEBUG -fPIC  --std=c++11 -fPIC -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -pthread -I. -I/ptp/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/ptp/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/ -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/experimental/ruy/block_map.cc -o /ptp/tensorflow/tensorflow/lite/tools/make/gen/linux_armv7l/obj/tensorflow/lite/experimental/ruy/block_map.o
tensorflow/lite/experimental/ruy/block_map.cc:27:10: fatal error: profiling/instrumentation.h: No such file or directory
 #include ""profiling/instrumentation.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
make: *** [tensorflow/lite/tools/make/Makefile:254: /ptp/tensorflow/tensorflow/lite/tools/make/gen/linux_armv7l/obj/tensorflow/lite/experimental/ruy/block_map.o] Error 1
make: Leaving directory '/ptp/tensorflow'
Traceback (most recent call last):
  File ""setup.py"", line 188, in <module>
    'build_py': CustomBuildPy,
  File ""/usr/lib/python3/dist-packages/setuptools/__init__.py"", line 145, in setup
    return distutils.core.setup(**attrs)
  File ""/usr/lib/python3.7/distutils/core.py"", line 148, in setup
    dist.run_commands()
  File ""/usr/lib/python3.7/distutils/dist.py"", line 966, in run_commands
    self.run_command(cmd)
  File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""/usr/lib/python3.7/distutils/command/bdist.py"", line 143, in run
    self.run_command(cmd_name)
  File ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""/usr/lib/python3.7/distutils/command/bdist_dumb.py"", line 81, in run
    self.run_command('build')
  File ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""/usr/lib/python3.7/distutils/command/build.py"", line 135, in run
    self.run_command(cmd_name)
  File ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""setup.py"", line 123, in run
    self.run_command('build_ext')
  File ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""setup.py"", line 115, in run
    make()
  File ""setup.py"", line 95, in make
    subprocess.check_call(make_args(quiet=False))
  File ""/usr/lib/python3.7/subprocess.py"", line 347, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['make', 'SHELL=/bin/bash', 'BUILD_WITH_NNAPI=false', '-C', '/ptp/tensorflow/tensorflow/lite/tools/pip_package/../../../..', '-f', 'tensorflow/lite/tools/make/Makefile', '-j', '1']' returned non-zero exit status 2.
```

2. The undeclared object: output from build_pip_package.sh
```
[...]
+ python3 setup.py bdist --plat-name=linux-armv7l bdist_wheel --plat-name=linux-armv7l
running bdist
running bdist_dumb
running build
running build_py
running build_ext
make: Entering directory '/ptp/tensorflow'
g++ -O3 -DNDEBUG -fPIC  --std=c++11 -fPIC -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -pthread -I. -I/ptp/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/ptp/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/ -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/ptp/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/experimental/ruy/block_map.cc -o /ptp/tensorflow/tensorflow/lite/tools/make/gen/linux_armv7l/obj/tensorflow/lite/experimental/ruy/block_map.o
tensorflow/lite/experimental/ruy/block_map.cc: In function ‘void ruy::GetBlockByIndex(const ruy::BlockMap&, int, ruy::SidePair<int>*)’:
tensorflow/lite/experimental/ruy/block_map.cc:36:3: error: ‘gemmlowp’ has not been declared
   gemmlowp::ScopedProfilingLabel label(""GetBlockByIndex"");
   ^~~~~~~~
tensorflow/lite/experimental/ruy/block_map.cc: In function ‘void ruy::MakeBlockMap(int, int, int, int, int, int, int, int, ruy::Path, int, ruy::BlockMap*)’:
tensorflow/lite/experimental/ruy/block_map.cc:273:3: error: ‘gemmlowp’ has not been declared
   gemmlowp::ScopedProfilingLabel label(""MakeBlockMap"");
   ^~~~~~~~
tensorflow/lite/experimental/ruy/block_map.cc: In function ‘void ruy::GetBlockMatrixCoords(ruy::Side, const ruy::BlockMap&, int, int*, int*)’:
tensorflow/lite/experimental/ruy/block_map.cc:412:3: error: ‘gemmlowp’ has not been declared
   gemmlowp::ScopedProfilingLabel label(""GetBlockMatrixCoords"");
   ^~~~~~~~
make: *** [tensorflow/lite/tools/make/Makefile:254: /ptp/tensorflow/tensorflow/lite/tools/make/gen/linux_armv7l/obj/tensorflow/lite/experimental/ruy/block_map.o] Error 1
make: Leaving directory '/ptp/tensorflow'
Traceback (most recent call last):
  File ""setup.py"", line 188, in <module>
    'build_py': CustomBuildPy,
  File ""/usr/lib/python3/dist-packages/setuptools/__init__.py"", line 145, in setup
    return distutils.core.setup(**attrs)
  File ""/usr/lib/python3.7/distutils/core.py"", line 148, in setup
    dist.run_commands()
  File ""/usr/lib/python3.7/distutils/dist.py"", line 966, in run_commands
    self.run_command(cmd)
  File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""/usr/lib/python3.7/distutils/command/bdist.py"", line 143, in run
    self.run_command(cmd_name)
  File ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""/usr/lib/python3.7/distutils/command/bdist_dumb.py"", line 81, in run
    self.run_command('build')
  File ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""/usr/lib/python3.7/distutils/command/build.py"", line 135, in run
    self.run_command(cmd_name)
  File ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""setup.py"", line 123, in run
    self.run_command('build_ext')
  File ""/usr/lib/python3.7/distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/usr/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""setup.py"", line 115, in run
    make()
  File ""setup.py"", line 95, in make
    subprocess.check_call(make_args(quiet=False))
  File ""/usr/lib/python3.7/subprocess.py"", line 347, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['make', 'SHELL=/bin/bash', 'BUILD_WITH_NNAPI=false', '-C', '/ptp/tensorflow/tensorflow/lite/tools/pip_package/../../../..', '-f', 'tensorflow/lite/tools/make/Makefile', '-j', '1']' returned non-zero exit status 2.

```
"
35625,RuntimeError: Invalid quantization params for op RESHAPE at index 36 in subgraph 0,"I download source code of SSD from github.com. I want to convert the model to run in edgetpu.
When I convert saved model to tflite, then run 'edgetpu_compiler XXX.tflite' for running in tpu. But it tell me that not quantization, I change parameter for quantization.Such as 
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8], 
but occur error: RuntimeError: Invalid quantization params for op RESHAPE at index 36 in subgraph 0

**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.6 LTS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source):1.15


**Command used to run the converter or code if you’re using the Python API**

```
import tensorflow as tf
from PIL import Image
import numpy as np

ckpt_filename = '/home/lanjunc/python_project/SSD-Tensorflow/convert/model'

img = Image.open('./dog.jpg')
img = img.resize((300, 300))
input_data = np.array(img, dtype=np.float32)
def representative_dataset_gen():
    for i in range(1):
        yield [input_data]


converter = tf.lite.TFLiteConverter.from_saved_model(ckpt_filename,
                                                     # tag_set=['test_saved_model1'], signature_key='signature_test1',
                                                     input_shapes={""Placeholder"":[300,300,3]})

converter.optimizations = [tf.lite.Optimize.DEFAULT]  # DEFAULT, OPTIMIZE_FOR_SIZE, OPTIMIZE_FOR_LATENCY

converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
# converter.target_spec.supported_ops = [
#                                        tf.lite.OpsSet.SELECT_TF_OPS]
#
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.representative_dataset = representative_dataset_gen

tflite_model = converter.convert()

open(""converted_model.tflite"", ""wb"").write(tflite_model)
```

**The output from the converter invocation**

```
RuntimeError: Invalid quantization params for op RESHAPE at index 36 in subgraph 0
```

"
35623,Linking error when building libtensorflow_cc.so,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: `v2.1.0-rc2` branch
- Python version: 3.7
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.5
- GPU model and memory: RTX 2070 8GB



**Describe the problem**

My project uses TensorFlow in C++, so I'm trying to get C++ related `.so` compiled. The problem now is that in the linking stage, there are lots of undefined references, see the following for example and the attached log for more.

```
mlir_passthrough_op.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.29+0x2f8): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(std::function<tensorflow::Status (tensorflow::shape_inference::InferenceContext*)>)'
mlir_passthrough_op.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.29+0x324): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'
bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/libcuda_stub.pic.a(cuda_stub.pic.o): In function `cudaError_enum (*(anonymous namespace)::LoadSymbol<cudaError_enum (*)(cudaError_enum, char const**)>(char const*))(cudaError_enum, char const**)':
cuda_stub.cc:(.text._ZN12_GLOBAL__N_110LoadSymbolIPF14cudaError_enumS1_PPKcEEET_S3_+0x2a): undefined reference to `tensorflow::Env::Default()'
bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/libcuda_stub.pic.a(cuda_stub.pic.o): In function `cudaError_enum (*(anonymous namespace)::LoadSymbol<cudaError_enum (*)(int*)>(char const*))(int*)':
cuda_stub.cc:(.text._ZN12_GLOBAL__N_110LoadSymbolIPF14cudaError_enumPiEEET_PKc+0x2a): undefined reference to `tensorflow::Env::Default()'
bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/libcuda_stub.pic.a(cuda_stub.pic.o): In function `cudaError_enum (*(anonymous namespace)::LoadSymbol<cudaError_enum (*)(char*, int, int)>(char const*))(char*, int, int)':
cuda_stub.cc:(.text._ZN12_GLOBAL__N_110LoadSymbolIPF14cudaError_enumPciiEEET_PKc+0x2a): undefined reference to `tensorflow::Env::Default()'
bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/libcuda_stub.pic.a(cuda_stub.pic.o): In function `cudaError_enum (*(anonymous namespace)::LoadSymbol<cudaError_enum (*)(int)>(char const*))(int)':
cuda_stub.cc:(.text._ZN12_GLOBAL__N_110LoadSymbolIPF14cudaError_enumiEEET_PKc+0x2a): undefined reference to `tensorflow::Env::Default()'
bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/libcuda_stub.pic.a(cuda_stub.pic.o): In function `cudaError_enum (*(anonymous namespace)::LoadSymbol<cudaError_enum (*)(CUctx_st*)>(char const*))(CUctx_st*)':
cuda_stub.cc:(.text._ZN12_GLOBAL__N_110LoadSymbolIPF14cudaError_enumP8CUctx_stEEET_PKc+0x2a): undefined reference to `tensorflow::Env::Default()'
bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/libcuda_stub.pic.a(cuda_stub.pic.o):cuda_stub.cc:(.text._ZN12_GLOBAL__N_110LoadSymbolIPF14cudaError_enumPP8CUctx_stEEET_PKc+0x2a): more undefined references to `tensorflow::Env::Default()' follow
bazel-out/k8-opt/bin/tensorflow/core/distributed_runtime/librequest_id.pic.a(request_id.pic.o): In function `tensorflow::GetUniqueRequestId()':
request_id.cc:(.text._ZN10tensorflow18GetUniqueRequestIdEv+0x9): undefined reference to `tensorflow::random::New64()'
collect2: error: ld returned 1 exit status
INFO: Elapsed time: 33.036s, Critical Path: 32.21s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
bazel build --config=opt \
                //tensorflow:libtensorflow_cc.so \
                //tensorflow:libtensorflow_framework.so \
                //tensorflow:install_headers
```

.tf_configure.bazelrc:

```
build --action_env PYTHON_BIN_PATH=""/home/abcdabcd987/.pyenv/versions/3.7.2/bin/python""
build --action_env PYTHON_LIB_PATH=""/home/abcdabcd987/.pyenv/versions/3.7.2/lib/python3.7/site-packages""
build --python_path=""/home/abcdabcd987/.pyenv/versions/3.7.2/bin/python""
build:xla --define with_xla_support=true
build --action_env TF_CUDA_VERSION=""10.1""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TF_NCCL_VERSION=""""
build --action_env TF_CUDA_PATHS=""/usr/local/cuda-10.1""
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda-10.1""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""7.5""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/x86_64-linux-gnu-gcc-7""
build --config=cuda
build:opt --copt=-march=native
build:opt --copt=-Wno-sign-compare
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_tag_filters=-benchmark-test,-no_oss,-oss_serial
test --build_tag_filters=-benchmark-test,-no_oss
test --test_tag_filters=-gpu
test --build_tag_filters=-gpu
build --action_env TF_CONFIGURE_IOS=""0""
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[tensorflow_cc-link.txt.gz](https://github.com/tensorflow/tensorflow/files/4028250/tensorflow_cc-link.txt.gz)

"
35618,Tensorflow 2.1.0 - DLL load failed,"### System information
- **OS Platform and Distribution**: Windows 10
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu==2.1.0rc2
- **TensorFlow version (use command below)**: 2.1.0rc2
- **Python version**: 3.7.4
- **CUDA/cuDNN version**: CUDA 10.1, cuDNN 7.6.5 associated with CUDA 10.1
- **GPU model and memory**: GTX 1060 6GB

### Describe the problem
I installed the pre-release using `pip install tensorflow-gpu==2.1.0rc2`. I installedCUDA 10.1 with cuDNN 7.6.5. When running `import tensorflow`, I get a 

> ImportError: DLL load failed: The specified module could not be found.

It probably means there is an issue with the CUDA installation, or a bug somewhere. Tensorflow 2.1.0 is supposed to be compatible with CUDA 10.1. 

In my system environment variables, path contains:

> C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\bin
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\extras\CUPTI\lib64
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\include


### Source code / logs
  ```
File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.2.3\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Users\johndoe\AppData\Local\Continuum\anaconda3\envs\dummy_env_name\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\johndoe\AppData\Local\Continuum\anaconda3\envs\dummy_env_name\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\johndoe\AppData\Local\Continuum\anaconda3\envs\dummy_env_name\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\johndoe\AppData\Local\Continuum\anaconda3\envs\dummy_env_name\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

```"
35617,erfcx special function,"**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): -

**Describe the feature and the current behavior/state.**

I would like to see the `erfcx(x) = exp(x**2) * erfc(x)` function (e.g., `scipy.special.erfcx`) implemented in Tensorflow. The naive implementation `exp(x**2) * erfc(x)` fails for large `x`.

This is useful when one needs to work with truncated normal distributions. 

`erfcx` is available in many numerical packages, such as [Matlab](https://www.mathworks.com/help/matlab/ref/erfcx.html), [Julia](https://juliamath.github.io/SpecialFunctions.jl/latest/functions_list/#SpecialFunctions.erfcx), [SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.erfcx.html) [R](https://www.rdocumentation.org/packages/pracma/versions/1.9.9/topics/erf), and others.

An alternative is to implement `log_erfc(x) = log(erfc(x))`, from which one can obtain `erfcx(x)` accurately. The function `log_erfc(x)` is implemented in [GSL](https://github.com/ampl/gsl/blob/644e768630841bd085cb7121085a688c4ff424d0/specfunc/erfc.c#L441).

**Who will benefit with this feature?**

Anyone working with truncated normal distributions.
"
35616,Typo Error in `l07c01_saving_and_loading_models.ipynb`,"## URL(s) with the issue:

https://github.com/ManishAradwad/examples/blob/9f7d80aff8214b358e4aea0b83f2648748990c4b/courses/udacity_intro_to_tensorflow_for_deep_learning/l07c01_saving_and_loading_models.ipynb#L579

`The differnece in output should be zero:`

## Description of issue (what needs changing):

differnece should be difference

### Submit a pull request?

Yes
"
35615,"tf.argmax(x, axis=-1) breaks TFLite exporter","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0
- Python version: 3.7.5
- CUDA/cuDNN version: none
- GPU model and memory: none

**Code to reproduce the issue**
```python
def build_model():
    from tensorflow.keras.layers import Lambda, Input
    from tensorflow.keras.models import Model

    a = Input(shape=(16, 16, 4))
    b = Input(shape=(4,))

    def f(inputs):
        a, b = inputs
        print('b.shape', b.shape)
        b = tf.one_hot(tf.argmax(b, axis=-1), 4)
        return tf.einsum('bhwc,bc->bhw', a, b)

    x = Lambda(f)([a, b])
    model = Model(inputs=[a, b], outputs=x)
    return model

_model = build_model()
print(_model.predict([np.ones((1, 16, 16, 4)), np.ones((1, 4))]))

tf.saved_model.save(_model, 'argmax_model')
converter = tf.lite.TFLiteConverter.from_saved_model('argmax_model')
graph = converter.convert()
```

**Describe the current behavior**
_model.predict` returns the correct result.

The TFLite converter falls over with the following error:

```
Traceback (most recent call last):
  ...
  File ""/Users/stefan/Documents/dc-form-model/src/scratch.py"", line 29, in <module>
    graph = converter.convert()
  File ""/Users/stefan/.virtualenvs/dc-form-model/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"", line 446, in convert
    **converter_kwargs)
  File ""/Users/stefan/.virtualenvs/dc-form-model/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 449, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""/Users/stefan/.virtualenvs/dc-form-model/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 200, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2020-01-06 18:33:19.342565: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 18 operators, 36 arrays (0 quantized)
2020-01-06 18:33:19.343757: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 18 operators, 36 arrays (0 quantized)
2020-01-06 18:33:19.344782: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:460] Check failed: input_flat_size == RequiredBufferSizeForShape(output_shape) (16 vs. 4)Input cannot be reshaped to requested dimensions for Reshape op with output ""PartitionedCall/model/lambda/einsum/Reshape_1"". Are your input shapes correct?
Fatal Python error: Aborted

Current thread 0x00000001151fa5c0 (most recent call first):
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52 in execute
  File ""/usr/local/lib/python3.7/site-packages/absl/app.py"", line 250 in _run_main
  File ""/usr/local/lib/python3.7/site-packages/absl/app.py"", line 299 in run
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40 in run
  File ""/usr/local/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89 in main
  File ""/usr/local/bin/toco_from_protos"", line 8 in <module>
```

**Describe the expected behavior**
Expected for it to work correctly.

If I replace `tf.argmax(b, axis=-1)` with `tf.argmax(b, axis=1)`, which indexes exactly the same axis but with a non-negative index (`b` is a rank-2 tensor), then the export finishes successfully.

"
35614,Segmentation fault when inferencing in C++,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
/
- TensorFlow installed from (source or binary):
source
- TensorFlow version (use command below):
2.1
- Python version:
3.6.9
- Bazel version (if compiling from source):
bazel 0.27.2
- GCC/Compiler version (if compiling from source):
gcc 8.3.0
- CUDA/cuDNN version:
10.1
- GPU model and memory:
gtx 950m


**Describe the current behavior**
I have trained a model (sub-classed keras.Model) and saved it via the SavedModel-API in Python.
I want to include this model in a c++ program. To this end, I have compiled the tensorflow_cc library and linked it to my project by using:

`bazel build --config=opt --config=cuda //tensorflow:libtensorflow_cc.so`

In c++, I can load the model, but upon running the session for inference, the program suffers a segmentation fault. I have done this in a similar manner before, but not since moving to eager execution. My model's call method is decorated with @tf.function. Unfortunately, I am unable to provide further details since I can't build tensorflow with debug symbols to get a proper backtrace of the issue.
 
**Describe the expected behavior**
The model should run a forward pass in C++. 
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```c++
#ifdef __GNUC__
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored ""-Winvalid-partial-specialization""
#include <tensorflow/core/public/session.h>
#include <tensorflow/core/protobuf/meta_graph.pb.h>
#include <tensorflow/core/public/session_options.h>
#include <tensorflow/cc/saved_model/loader.h>
#include <tensorflow/cc/saved_model/tag_constants.h>
#pragma GCC diagnostic pop
#else
#include <tensorflow/core/public/session.h>
#include <tensorflow/core/protobuf/meta_graph.pb.h>
#include <tensorflow/core/public/session_options.h>
#include <tensorflow/cc/saved_model/loader.h>
#include <tensorflow/cc/saved_model/tag_constants.h>
#endif

#include <string>
#include <Eigen/Dense>

using namespace tensorflow;

int main(){

    const std::string pathToGraph = ""/path/to/saved/model/"";

    Status status;
    SavedModelBundle bundle;
    status = LoadSavedModel(SessionOptions(), RunOptions(), pathToGraph, {tensorflow::kSavedModelTagServe}, &bundle);
    Session* session = bundle.GetSession();

    if (!status.ok())
    {
        throw std::runtime_error(""Could not create Tensorflow session: "" + status.ToString());
    }

    if (!status.ok())
    {
        throw std::runtime_error(""Error reading graph definition from "" + pathToGraph + "": "" + status.ToString());
    }

    GraphDef def = bundle.meta_graph_def.graph_def();
    std::string outputlayer = ""StatefulPartitionedCall:0"";

    tensorflow::Tensor input_tensor_state(tensorflow::DT_FLOAT, tensorflow::TensorShape({1, 6}));
    tensorflow::Tensor input_tensor_action(tensorflow::DT_FLOAT, tensorflow::TensorShape({1, 3}));

    auto input_tensor_state_mapped = input_tensor_state.flat<float>().data();
    auto input_tensor_action_mapped = input_tensor_action.flat<float>().data();

    Eigen::Matrix<float, 6, 1> X = Eigen::Matrix<float, 6, 1>::Zero();
    Eigen::Matrix<float, 3, 1> U = Eigen::Matrix<float, 3, 1>::Zero();

    memcpy(input_tensor_state_mapped, X.data(), X.size()*sizeof(float));
    memcpy(input_tensor_action_mapped, U.data(), U.size()*sizeof(float));

    std::vector<tensorflow::Tensor> outputs;
    
    Status run_status = session->Run({{""serving_default_input_1:0"", input_tensor_state},
                                      {""serving_default_input_2:0"", input_tensor_action}}, {outputlayer}, {},
                                     &outputs); // The program segfaults here


}

```

**Other info / logs**
I am also including the saved model in question.
[NeuralModel.zip](https://github.com/tensorflow/tensorflow/files/4027127/NeuralModel.zip)

"
35612,lite/micro/kernels/cmis-nn ,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 2274eacd794d7e501849567811637b7921e52820
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ARM cmis-nn

**Describe the problem**

Issue reported by ""On-Device AI Co., Ltd. "" on tensorflow/sig-micro gitter
Lite/micro examples using CMIS-NN kernels no longer compile.

Root cause:
The PR: Lite: Kernel_util refactored #27019 did not refactor the cmis-nn specific add and mul kernels. Will submit PR request with the missing changes.

**Please provide the exact sequence of commands/steps when you ran into the problem**
```make -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge TAGS=""cmsis-nn"" micro_speech_bin

......

arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -fno-rtti -DPART_apollo3 -DAM_PACKAGE_BGA -DAM_PART_APOLLO3 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DNDEBUG -DTF_LITE_MCU_DEBUG_LOG -D __FPU_PRESENT=1 -DARM_MATH_CM4 -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m4 -mthumb -mfpu=fpv4-sp-d16 -mfloat-abi=hard -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -ggdb -O3 -DARM_MATH_DSP -DARM_MATH_LOOPUNROLL -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -isystemtensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include/ -Itensorflow/lite/micro/tools/make/downloads/CMSIS_ext/ -Itensorflow/lite/micro/tools/make/downloads/gcc_embedded//arm-none-eabi/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/mcu/apollo3/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/CMSIS/AmbiqMicro/Include/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/bsp -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/devices/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/utils/ -Itensorflow/lite/micro/tools/make/downloads/cmsis//CMSIS/Core/Include -Itensorflow/lite/micro/tools/make/downloads/cmsis//CMSIS/NN/Include -Itensorflow/lite/micro/tools/make/downloads/cmsis//CMSIS/DSP/Include -Itensorflow/lite/micro/tools/make/downloads/kissfft -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/examples/example1_edge_test/src/tf_accelerometer/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/examples/example1_edge_test/src/tf_adc/ -c tensorflow/lite/micro/kernels/cmsis-nn/add.cc -o tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/kernels/cmsis-nn/add.o
tensorflow/lite/micro/kernels/cmsis-nn/add.cc: In function 'TfLiteStatus tflite::ops::micro::add::CalculateOpData(TfLiteContext*, TfLiteAddParams*, const TfLiteTensor*, const TfLiteTensor*, TfLiteTensor*, tflite::ops::micro::add::OpData*)':
tensorflow/lite/micro/kernels/cmsis-nn/add.cc:89:7: error: 'CalculateActivationRangeUint8' was not declared in this scope
       CalculateActivationRangeUint8(params->activation, output,
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/lite/micro/kernels/cmsis-nn/add.cc:89:7: note: suggested alternative: 'CalculateActivationRange'
       CalculateActivationRangeUint8(params->activation, output,
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
       CalculateActivationRange
tensorflow/lite/micro/kernels/cmsis-nn/add.cc:93:7: error: 'CalculateActivationRangeInt8' was not declared in this scope
       CalculateActivationRangeInt8(params->activation, output,
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/lite/micro/kernels/cmsis-nn/add.cc:93:7: note: suggested alternative: 'CalculateActivationRange'
       CalculateActivationRangeInt8(params->activation, output,
       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
       CalculateActivationRange
.....
```"
35609,Anyone knows this error? cuda_dnn.cc:86] Check failed: narrow == wide (-1236407296 vs. 3058560000)checked narrowing failed; values not equal post-conversion,"2020-01-06 13:21:24.480792: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-01-06 13:21:24.742605: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-01-06 13:21:28.135955: F tensorflow/stream_executor/cuda/cuda_dnn.cc:86] Check failed: narrow == wide (-1236407296 vs. 3058560000)checked narrowing failed; values not equal post-conversion
"
35608,Beginner models,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):2.0
- Are you willing to contribute it (Yes/No):Yes



**Describe the feature and the current behavior/state.**
perfectly normal
**Will this change the current api? How?**
Yes. The beginner models can be a different api in tensorflow itself. Like `tf.BeginnerModel`.
**Who will benefit with this feature?**
Beginners in machine learning who want to write less code and acheive a lot
**Any Other info.**
This will make it easier to use tensorflow as a beginner"
35606,log1pexp,"**System information**
- TensorFlow version (you are using): 2.0
- Are you willing to contribute it (Yes/No): -


**Describe the feature and the current behavior/state.**

Please add `log1pexp(x)` function to math. This computes `log(1 + exp(x)` in a numerically stable way. Here is a Python implementation that I am currently using:

```
@tf.function
def log1pexp(x):
    return tf.where(tf.less(x, 9),
                    tf.math.log1p(tf.exp(x)),
                    tf.where(tf.less(x, 16),
                             x + tf.math.exp(-x),
                             x))
```

But a native C version will probably be faster.

**Who will benefit with this feature?**

This function is useful when programming RBM models, for instance. But it has many other uses."
35605,Request Adding Ops in Tensorflow Lite ：RandomUniform,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DIV, FLOOR, MAX_POOL_2D, MUL, RESHAPE, SPLIT, SUB, TRANSPOSE_CONV. Here is a list of operators for which you will need custom implementations: RandomUniform.
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35604,module 'tensorflow' has no attribute 'function' .,"Name: tensorflow
Version: 2.0.0 in Anaconda with Python v. 3.7.3"
35603,[XLA] [TPU] It should not be possible to run out of vmem - please file a bug against XLA.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

```v1.12.1-21643-g03f1214 2.1.0-dev20200105```

**Describe the current behavior**
I modified this script: https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_squad.py.

```
2020-01-06 08:30:26.371423: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:75]Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: {{function_node __inference_tpu_function_106662}} Compilation failure: Ran out of memory in memory space vmem. It should not be possible to run out of vmem - please file a bug against XLA.

Largest program allocations in vmem:

  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742
  Allocation type: scoped

  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742
  Allocation type: scoped

  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742
  Allocation type: scoped

        TPU compilation failed
         [[{{node tpu_compile_succeeded_assert/_7693574632605057830/_9}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

Traceback (most recent call last):
  File ""train_eval.py"", line 482, in <module>
    main()
  File ""train_eval.py"", line 458, in main
    global_step, tr_loss = train(args, model_class, tokenizer, config, strategy)
  File ""train_eval.py"", line 129, in train
    running_loss = smooth * running_loss + (1. - smooth) * float(loss)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 867, in __float__
    return float(self._numpy())
  File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py"", line 918, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.ResourceExhaustedError: {{function_node __inference_tpu_function_106662}} Compilation failure: Ran out of memory in memory space vmem. It should not be possible to run out of vmem - please file a bug against XLA.

Largest program allocations in vmem:

  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742
  Allocation type: scoped

  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742
  Allocation type: scoped

  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742
  Allocation type: scoped

        TPU compilation failed
         [[{{node tpu_compile_succeeded_assert/_7693574632605057830/_9}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
```

I debugged this error a bit and found that using the custom optimizer causes the error. That is

```
optimizer = optimization.create_optimizer(args.learning_rate,
            num_steps_per_epoch * args.num_train_epochs, warmup_steps)
```

With a default optimizer the program runs fine:
```
optimizer = tf.keras.optimizers.Adam(learning_rate=args.learning_rate)
```

**Describe the expected behavior**

Custom optimizers should work and the error message should be more specific.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

I was not able to create a minimal example to reproduce a minimal example, but the two lines make the difference between a failed and successful compilation.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35602,"Census tutorial documentation refers to Python 2, but code uses Python 3","## URL(s) with the issue:

https://www.tensorflow.org/tfx/tutorials/transform/census#python_check_imports_and_globals

## Description of issue (what needs changing):

The text says ""First we'll make sure that we're using Python 2, and then go ahead and install and import the stuff we need."" but the code below indicates we need to use Python 3.

### Clear description

The text states:

> First we'll make sure that we're using Python 2, and then go ahead and install and import the stuff we need.

The code example says:

```python
import sys

# Confirm that we're using Python 3
assert sys.version_info.major is 3, 'Oops, not running Python 3. Use Runtime > Change runtime type'
```
"
35601,RuntimeError: Mixing different tf.distribute.Strategy objects: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategyV1 object at 0x7f50aed5d160> is not <tensorflow.python.distribute.mirrored_strategy.MirroredStrategyV1 object at 0x7f5230440748>,"`def train_net():
        mirror_strategy = tf.distribute.MirroredStrategy(
                     cross_device_ops=config.cross_device_ops
          )
         with mirror_strategy.scope():

for i in range(1, config.num_step+1):
            train_net(config, stage_cfg, args.fast_mode, merge_num=i)```"
35598,dll load failed with errorcode 3221225501,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version: 2.0
- Python version: 3.7.4
- Installed using virtualenv? pip? conda?:  installed using pip also tried wheel
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: No Gpu, only cpu, 



**Describe the problem**
When i import tensorflow it raise error
`dll load failed with errorcode 3221225501`
I tried several methods like
1) Installing whl from [Tensorflow wheel](https://github.com/fo40225/tensorflow-windows-wheel/blob/master/2.0.0/py37/CPU/avx2/tensorflow-2.0.0-cp37-cp37m-win_amd64.whl )
2) Installed visual c++ 2015 redistributable

But all failed,

**Provide the exact sequence of commands / steps that you executed before running into the problem**
>>>import tensorflow 

**Any other info / logs**

>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\SAMAN\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap
_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\SAMAN\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap
_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\SAMAN\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap
_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\SAMAN\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\SAMAN\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code 3221225501

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\SAMAN\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line
 98, in <module>
    from tensorflow_core import *
  File ""C:\Users\SAMAN\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"",
 line 42, in <module>
    from . _api.v2 import audio
  File ""C:\Users\SAMAN\Anaconda3\lib\site-packages\tensorflow_core\_api\v2\audio
\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\SAMAN\Anaconda3\lib\site-packages\tensorflow_core\python\ops\ge
n_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\SAMAN\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line
 50, in __getattr__
    module = self._load()
  File ""C:\Users\SAMAN\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line
 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\SAMAN\Anaconda3\lib\importlib\__init__.py"", line 127, in import
_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\SAMAN\Anaconda3\lib\site-packages\tensorflow_core\python\__init
__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\SAMAN\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap
_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\SAMAN\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap
_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\SAMAN\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap
_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\SAMAN\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap
_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\SAMAN\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\SAMAN\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code 3221225501


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
35597,"""EIGEN_ALWAYS_INLINE"" is not a class or function template in the current scope","**Configuration**
Windows 10, Visual Studio 2017
Tensorflow: 1.14
Python: 3.6.3
CUDA: No
Language: C++
CPU: Yes (arch/AVX)
Rcom: No
JIT: No
Override eigen inline for less compile time: Yes

My tensorflow cpu build is successful. I set the paths in C++ ""Include Path Directories"" at project properties in VS2017. While trying to execute the demo program I am getting int32, EIGEN_ALWAYS_INLINE errors. Below find the code below and the errors.

**Error Screenshot**
[https://drive.google.com/open?id=1xQfZVwQAmi30OMmrxl4BI-7pYvgW3BGi](url)

**Sample code**
```
// tf demo 1.cpp : This file contains the 'main' function. Program execution begins and ends there.
//

#include <iostream>
#include <Windows.h>
#include <stdio.h>
//#include <vector>

//#include <eigen/Dense>

// #include ""matmul.h""
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/cc/ops/standard_ops.h""

#include ""tensorflow/core/public/session.h""
#include ""tensorflow/core/platform/env.h""

#include ""tensorflow/cc/client/client_session.h""
#include ""tensorflow/cc/ops/standard_ops.h""
#include ""tensorflow/cc/ops/math_ops.h""
#include ""tensorflow/core/framework/tensor.h""
#include ""tensorflow/core/protobuf/config.pb.h""

using namespace tensorflow;

int main() {
	using namespace tensorflow;
	using namespace tensorflow::ops;
	Scope root = Scope::NewRootScope();
	//Matrix A = [3 2; -1 0]
	auto A = Const(root, { {3.f, 2.f}, {-1.f, 0.f} });
	// Vector b = [3 5]
	auto b = Const(root, { {3.f, 5.f} });
	// v = Ab^T
	auto v = MatMul(root.WithOpName(""v""), A, b, MatMul::TransposeB(true));
	std::vector<Tensor> outputs;
	ClientSession session(root);
	// Run and fetch v
	TF_CHECK_OK(session.Run({ v }, &outputs));
	// Expect outputs[0] == [19; -3]
	LOG(INFO) << outputs[0].matrix<float>();
	return 0;
}
```"
35594,[sparkfun-tensorflow-codelab] - Update Code,"The codelab still links to the experimental folder for the makefile, which is incorrect.

Visual:
![image](https://user-images.githubusercontent.com/997157/71787651-3bc27180-2fe0-11ea-8318-424dc887efc5.png)

Update the codelab at this link https://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#3

Code should read:
```
make -f tensorflow/lite/micro/tools/make/Makefile \                                    
TARGET=sparkfun_edge micro_speech_bin
```

## URL(s) with the issue:
https://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#3
"
35593,The report a mistake link does not work,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#3

## Description of issue (what needs changing):

### Clear description

The ""Report a Mistake"" link at the bottom left does not work, and goes to a GitHub 404 instead.
![image](https://user-images.githubusercontent.com/997157/71787589-711a8f80-2fdf-11ea-8c54-03f8fc68b8d0.png)

Currently it links to: https://github.com/tensorflow/tensorflow/issues/new/title=[sparkfun-tensorflow-codelab]:"
35592,Unable to build hello_world for sparkfun_edge,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: a2cf331a2073d6f5e9a52802cbc2809f9e2667b8
- Python version: Python 3.6.8
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source): arm-none-eabi-gcc (15:6.3.1+svn253039-1build1) 6.3.1 20170620
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I am unable to build hello_world for sparkfun_edge.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I followed the instructions on https://www.tensorflow.org/lite/microcontrollers/library
and issued the command:
`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge hello_world_bin`

This results in the following build error:
```
arm-none-eabi-ar: creating tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/lib/libtensorflow-microlite.a
arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -fno-rtti -DPART_apollo3 -DAM_PACKAGE_BGA -DAM_PART_APOLLO3 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DNDEBUG -DTF_LITE_MCU_DEBUG_LOG -D __FPU_PRESENT=1 -DARM_MATH_CM4 -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m4 -mthumb -mfpu=fpv4-sp-d16 -mfloat-abi=hard -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -ggdb -O3 -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -isystemtensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include/ -Itensorflow/lite/micro/tools/make/downloads/CMSIS_ext/ -Itensorflow/lite/micro/tools/make/downloads/gcc_embedded//arm-none-eabi/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/mcu/apollo3/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/CMSIS/AmbiqMicro/Include/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/bsp -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/devices/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/utils/ -Itensorflow/lite/micro/tools/make/downloads/kissfft -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/examples/example1_edge_test/src/tf_accelerometer/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/examples/example1_edge_test/src/tf_adc/ -o tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/hello_world tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world/main.o tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world/main_functions.o tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world/sine_model_data.o tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world/sparkfun_edge/output_handler.o tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world/sparkfun_edge/constants.o  tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/lib/libtensorflow-microlite.a -mthumb -mcpu=cortex-m4 -mfpu=fpv4-sp-d16 -mfloat-abi=hard -nostartfiles -static -Wl,--gc-sections -Wl,--entry,Reset_Handler -Wl,--start-group -lm -lc -lgcc -Wl,--end-group -fno-exceptions -nostdlib --specs=nano.specs -t -lstdc++ -lc -lnosys -lm -Wl,-T,tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/apollo3_evb/examples/hello_world/gcc_patched/apollo3evb.ld -Wl,-Map=tensorflow/lite/micro/tools/make/gen/sparkfun_edge.map,--cref tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/bsp/gcc/bin/libam_bsp.a tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/mcu/apollo3/hal/gcc/bin/libam_hal.a tensorflow/lite/micro/tools/make/downloads/gcc_embedded//lib/gcc/arm-none-eabi/7.3.1/thumb/v7e-m/fpv4-sp/hard/crtbegin.o -lm
arm-none-eabi-g++: error: tensorflow/lite/micro/tools/make/downloads/gcc_embedded//lib/gcc/arm-none-eabi/7.3.1/thumb/v7e-m/fpv4-sp/hard/crtbegin.o: No such file or directory
tensorflow/lite/micro/examples/hello_world/Makefile.inc:41: recipe for target 'tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/hello_world' failed
make: *** [tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/hello_world] Error 1
```

"
35591,GPU memory on Bahdanau Attention,"**System information**
Google Colab system (GPU mode) with TensorFlow 2.1-rc2 and Python 3.

**Describe the current behavior**
Bahdanau attention get a lot gpu memory, it's right? Even bahdanau attention using concatenation approach, Luong attention get 2.5gb of memory while bahdanau over 15gb. (using scale=False, batch_size=128, units=512 both)

**Describe the expected behavior**
To use less memory

**Code to reproduce the issue**
The model detail is [here](https://drive.google.com/open?id=170KpjIGFmNNt-SS-6oXgVbRnZR4-7ah_) (the same of this issue https://github.com/tensorflow/tensorflow/issues/35553)"
35590,experimental_new_converter / error: 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab, python 3 runtime
- TensorFlow installed from (source or binary): pip3 install tf-nightly
- TensorFlow version (or github SHA if from source): '2.1.0-dev20200104'


**Command used to run the converter or code if you’re using the Python API**

converter = tensorflow.lite.TFLiteConverter.from_keras_model(loaded_model)
converter.optimizations = [tensorflow.lite.Optimize.DEFAULT]
converter.experimental_new_converter=True
tflite_quant_model = converter.convert()

**The output from the converter invocation**

---------------------------------------------------------------------------
ConverterError                            Traceback (most recent call last)
<ipython-input-27-934d73e7faa7> in <module>()
      3 converter.optimizations = [tensorflow.lite.Optimize.DEFAULT]
      4 converter.experimental_new_converter=True
----> 5 tflite_quant_model = converter.convert()

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    213       stdout = _try_convert_to_unicode(stdout)
    214       stderr = _try_convert_to_unicode(stderr)
--> 215       raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
    216   finally:
    217     # Must manually cleanup files.

ConverterError: See console for info.
2020-01-05 08:26:27.512446: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:108] Ignored output_format.
2020-01-05 08:26:27.512515: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:114] Ignored drop_control_dependency.
2020-01-05 08:26:27.711202: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
loc(callsite(""model_1/up_sampling2d_1/resize/ResizeNearestNeighbor""(""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py"":853:0) at callsite(""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py"":947:0 at callsite(""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py"":409:0 at callsite(""<ipython-input-27-934d73e7faa7>"":2:0 at callsite(""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"":2882:0 at callsite(""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"":2822:0 at callsite(""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"":2718:0 at callsite(""/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py"":533:0 at callsite(""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"":196:0 at ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"":399:0)))))))))): error: 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op
error: failed while converting: 'main'
Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag): ResizeNearestNeighbor.
Traceback (most recent call last):
  File ""/usr/local/bin/toco_from_protos"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 56, in execute
    enable_mlir_converter)
Exception: /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py:853:9: error: 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op
        self._initialize(args, kwargs, add_initializers_to=initializers)
        ^
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py:947:5: note: called from
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
    ^
/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py:409:5: note: called from
    concrete_func = func.get_concrete_function()
    ^
<ipython-input-27-934d73e7faa7>: note: called from
/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882:17: note: called from
                exec(code_obj, self.user_global_ns, self.user_ns)
                ^
/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822:17: note: called from
                if self.run_code(code, result):
                ^
/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718:20: note: called from
                   interactivity=interactivity, compiler=compiler, result=result)
                   ^
/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py:533:9: note: called from
        return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
        ^
/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py:196:13: note: called from
            res = shell.run_cell(code, store_history=store_history, silent=silent)
            ^
/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py:399:41: note: called from
                                        user_expressions, allow_stdin)
                                        ^
<unknown>:0: error: failed while converting: 'main'
Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag): ResizeNearestNeighbor.



**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)

the old toco based converter works as expected,
when testing: converter.experimental_new_converter=True
the above error appears

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35589,Config value monolithic is not defined ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.15.0
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0.326
- GPU model and memory: 1080ti 11MB



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Trying to create a standalone C++ installation.

Successfully configured and built the Python package.

Since I didn't find any help in the official tensorflow documentation, tried the solution here:

[https://stackoverflow.com/questions/33620794/how-to-build-and-use-google-tensorflow-c-api](url)

Since I'm adding TF to an existing OpenCV application, the recommended solution from [14267](https://github.com/tensorflow/tensorflow/issues/14267) would be 

`bazel build //tensorflow:libtensorflow_cc.so --config=monolithic`

But that results in error:
 
`Config value monolithic is not defined in any .rc file`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The original build command:

`bazel build -c opt --jobs 32 --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.1 --copt=-msse4.2 -k //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --define=grpc_no_ares=true
`

Here is the configure session:


`./configure

You have bazel 0.26.1 installed.
Please specify the location of python. [Default is /home/david/TF/bin/python]: 


Found possible Python library paths:
  /home/david/TF/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/home/david/TF/lib/python3.6/site-packages]`

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: 
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: y
TensorRT support will be enabled for TensorFlow.

Found CUDA 10.1 in:
    /usr/local/cuda/lib64
    /usr/local/cuda/include
Found cuDNN 7 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include
Found TensorRT 6 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include/x86_64-linux-gnu`


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]: 


Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.`

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=gdr         	# Build with GDR support.
	--config=verbs       	# Build with libverbs support.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=noignite    	# Disable Apache Ignite support.
	--config=nokafka     	# Disable Apache Kafka support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
a`
"
35588,Tried installing tensorflow packages in Conda for Ubuntu but kept facing import errors,"
### Describe the problem
Issues while running Handwritten Digit classifier

### Source code / logs
ImportError                               Traceback (most recent call last)
~/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~/yes/envs/tensorflow_env/lib/python3.6/imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

~/yes/envs/tensorflow_env/lib/python3.6/imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: /home/ponaravind/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so: undefined symbol: cuDevicePrimaryCtxGetState

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-b3fe95775323> in <module>
      1 import cv2
      2 import numpy as np
----> 3 from keras.datasets import mnist
      4 from keras.models import load_model
      5 

~/yes/envs/tensorflow_env/lib/python3.6/site-packages/keras/__init__.py in <module>
      1 from __future__ import absolute_import
      2 
----> 3 from . import utils
      4 from . import activations
      5 from . import applications

~/yes/envs/tensorflow_env/lib/python3.6/site-packages/keras/utils/__init__.py in <module>
      4 from . import data_utils
      5 from . import io_utils
----> 6 from . import conv_utils
      7 
      8 # Globally-importable utils.

~/yes/envs/tensorflow_env/lib/python3.6/site-packages/keras/utils/conv_utils.py in <module>
      7 from six.moves import range
      8 import numpy as np
----> 9 from .. import backend as K
     10 
     11 

~/yes/envs/tensorflow_env/lib/python3.6/site-packages/keras/backend/__init__.py in <module>
     87 elif _BACKEND == 'tensorflow':
     88     sys.stderr.write('Using TensorFlow backend.\n')
---> 89     from .tensorflow_backend import *
     90 else:
     91     # Try and load external backend.

~/yes/envs/tensorflow_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in <module>
      3 from __future__ import print_function
      4 
----> 5 import tensorflow as tf
      6 from tensorflow.python.framework import ops as tf_ops
      7 from tensorflow.python.training import moving_averages

~/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/__init__.py in <module>
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 try:

~/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 from tensorflow.python.tools import component_api_helper

~/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""/home/ponaravind/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/ponaravind/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/ponaravind/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/ponaravind/yes/envs/tensorflow_env/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/ponaravind/yes/envs/tensorflow_env/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /home/ponaravind/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so: undefined symbol: cuDevicePrimaryCtxGetState


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
35587,Looking to install 1.15 C++ ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.15.0
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.4.0
- CUDA/cuDNN version: 10.0.326
- GPU model and memory: 1080ti 11MB

**Describe the problem**

Build command:

`bazel build -c opt --jobs 32 --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.1 --copt=-msse4.2 -k //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --define=grpc_no_ares=true
`

Completed the build with bazel, and want to produce the libraries and includes to install in my system, but I'm unable to find anywhere ()many years experience with CMAKE, 0 with bazel).

I found [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md](url) for C (NOT what I'm looking for but wanted to get started with something..), and ran the command instructed there

`bazel test --config opt //tensorflow/tools/lib_package:libtensorflow_test`

But returns 

`ERROR: Config value opt is not defined in any .rc file`

But the real question is how do I install tensorflow 1.15 C++ 

Many thanks!!
"
35586,Tensorflow-GPU processing fit on CPU,"**System information**
- Haven't written custom code.
- OS Platform and Distribution: Windows 10
- TensorFlow installed from: Anaconda
- TensorFlow version: 2.0.0
- Python version: 3.7
- CUDA/cuDNN version: CUDA: 10.0.130, cuDNN: 7.6.5
- GPU model and memory: GTX 1060 3Gb

**Describe the current behavior**
Tensorflow GPU is using the CPU instead of the GPU to fit. If I use tf.compat.v1.debugging.set_log_device_placement(True) when trying to fit the model I get:

2020-01-04 21:00:20.616161: I tensorflow/core/common_runtime/eager/execute.cc:574] Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0
2020-01-04 21:00:20.616622: I tensorflow/core/common_runtime/eager/execute.cc:574] Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0
2020-01-04 21:00:20.637944: I tensorflow/core/common_runtime/eager/execute.cc:574] Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0 

But for example on other ocations (not fitting):

2020-01-04 21:00:19.201662: I tensorflow/core/common_runtime/eager/execute.cc:574] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0


**Code to reproduce the issue**
```
import gym
import numpy as np
from tensorflow import keras
from collections import deque
import random
import tensorflow as tf
import gc
import time
print(tf.test.is_gpu_available())
tf.compat.v1.debugging.set_log_device_placement(True)


class Agent:
    def __init__(self, state_size, action_size):
        self.LEARNING_RATE = 0.001
        self.EPSILON = 1.0
        self.EPSILON_DECAY = 0.97
        self.MIN_EPSILON = 0.01
        self.DISCOUNT = 0.95

        self.model = keras.Sequential()
        self.model.add(keras.layers.Dense(state_size, activation=""relu""))
        self.model.add(keras.layers.Dense(24, activation=""relu""))
        self.model.add(keras.layers.Dense(48, activation=""relu""))
        self.model.add(keras.layers.Dense(action_size, activation=""linear""))
        self.model.compile(optimizer=keras.optimizers.Adam(learning_rate=self.LEARNING_RATE),
                                                           loss=""mse"")

        self.memory = deque(maxlen=2000)

    def act(self, state):
        action = 0

        if state is None:
            action = 1
        else:
            if np.random.rand() <= self.EPSILON:
                action = random.randint(0, 2)
            else:
                action = self.model.predict(state)
                keras.backend.clear_session()
                gc.collect()
                action = np.argmax(action)

        return action

    def remember(self, state, reward, action, next_state, done):
        self.memory.append((state, reward, action, next_state, done))

    def retrain(self, minibatch_size):
        if minibatch_size < len(self.memory):
            minibatch = random.sample(self.memory, minibatch_size)
            now = time.time()
            for i in range(100):
                print(""Start time:"", str(now))
            for state, reward, action, next_state, done in minibatch:
                target = reward

                if not done:
                    target = reward + self.DISCOUNT*np.amax(self.model.predict(next_state)[0])
                    keras.backend.clear_session()
                    gc.collect()

                if not (state is None):
                    target_f = self.model.predict(state)
                    keras.backend.clear_session()
                    gc.collect()
                    target_f[0][action] = target
                    self.model.fit(state, target_f, epochs=1, verbose=0)

            print(""Elapsed:"", str(time.time() - now))

        if self.EPSILON > self.MIN_EPSILON:
            self.EPSILON *= self.EPSILON_DECAY

    def load(self, name):
        self.model.build(input_shape=(1, 2))
        self.model.load_weights(name)

    def save(self):
        self.model.save_weights(str(episode + 6700) + "".h5"")


class Environment:
    def __init__(self):
        self.env = gym.make(""MountainCar-v0"")
        self.reward = 0
        self.max_height = -999
        self.record_pos = -9

    def make_move(self, action):
        state, reward, done, info = self.env.step(action)

        if state[0] > self.max_height:
            if state[0] > self.record_pos:
                self.record_pos = state[0]
                self.max_height = self.record_pos
                self.reward = 50
            else:
                self.max_height = state[0]
                self.reward = 10

        if state[0] == 0.5:
            self.reward = 100

        return state, reward, done

    def get_state_size(self):
        return self.env.observation_space.shape[0]

    def get_action_size(self):
        return self.env.action_space.n

    def reset(self):
        self.env.reset()
        self.reward = 0
        self.max_height = -999

    def close(self):
        self.env.close()

    def render(self):
        self.env.render()

    def get_max_position(self):
        return self.record_pos


environment = Environment()
agent = Agent(environment.get_state_size(), environment.get_action_size())
agent.load(""6700.h5"")
for episode in range(1000):
    i = 0
    state = None
    environment.reset()
    done = False
    previous_height = 0
    while not done:
        if episode % 100 == 0:
            environment.render()

        action = agent.act(state)
        next_state, reward, done = environment.make_move(action)

        next_state = np.array(next_state)
        next_state = np.reshape(next_state, [1, environment.get_state_size()])

        if state is not None:
            agent.remember(state, reward, action, next_state, done)

        state = next_state

    print(""Max height:"", environment.get_max_position(), ""episode:"", episode)
    if episode % 100 == 0 and episode != 0:
        agent.save()
    agent.retrain(32)

environment.close()
```

**Other log that might be useful is:**
print(tf.test.is_gpu_available()) ----> True

Also, the GPU load is 0%.
"
35585,Difference in training accuracy and loss using gradientTape vs model.fit with binary_accuracy: A bug?,"Hi all, 

I am running a training loop using gradientTape which works well, however I am getting different training accuracy metrics when training using the gradientTape loop vs a straight model.fit method. I apologise if this should be a question for stack overflow, however, to the best of my knowledge the parameters are the same and therefore should be producing exactly the same results (or very close at least).. I therefore think there may be a bug and if any one can help me elucidate this i would really appreciate it!

I have prepared a sequential model as follows:

```
model=tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(units=64, input_dim=5078, activation=""relu""))
model.add(tf.keras.layers.Dense(units=32, activation=""relu""))
model.add(tf.keras.layers.Dense(units=100, activation=""relu""))
model.add(tf.keras.layers.Dense(units=24, activation=""sigmoid""))
```
and for the ` model.fit` method, fit as follows:

```
model.compile(optimizer=""Adam"", loss=""binary_crossentropy"", metrics=[""acc""])

model.fit(X_train, y_train,
 batch_size=32,
 epochs=100, verbose=1,
 validation_split=0.15,
 shuffle=True)
```
This works well and produces the following results (please note 100 epochs is overkill and the model overfits, however this is just to keep the same epochs as the as the gradientTape loop, otherwise there would be an early-stopping callback normally...

The model metrics are as follows:

```
 32/119 [=======>......................] - ETA: 0s - loss: 0.0699 - acc: 0.9753
119/119 [==============================] - 0s 168us/sample - **loss: 0.0668** - acc: **0.9779** - val_loss: **0.2350** - val_acc: **0.9048**
```

This is the expected behaviour (minus the overfitting)... Now when I create the gradientTape loop as follows, the accuracy metrics are of by about ~4-5% during the same 100 epochs, and the reason i suspect a bug is because i believe i am using the appropriate metrics:

```
def random_batch(X,y, batch_size=32):
    idx= np.random.randint(len(X), size=batch_size)
    return X[idx], y[idx]

##Further split train data to training set and validation set

X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.15, random_state=1)

```

```
##Run autodiff on model

n_epochs=100
batch_size=32
n_steps=len(X_train)//batch_size

optimizer=tf.keras.optimizers.Adam()
loss=tf.keras.losses.BinaryCrossentropy()

metricLoss=tf.keras.metrics.BinaryCrossentropy()
metricsAcc=tf.keras.metrics.BinaryAccuracy()

val_acc_metric=tf.keras.metrics.BinaryAccuracy()
val_acc_loss=tf.keras.metrics.BinaryCrossentropy()


train_loss_results = []
train_accuracy_results = []

validation_loss_results = []
validation_accuracy_results = []

# for loop iterate over epochs
for epoch in range(n_epochs):

    print(""Epoch {}/{}"".format(epoch, n_epochs))

    # for loop iterate over batches
    for step in range(1, n_steps + 1):
        X_batch, y_batch=random_batch(X_train.values, y_train)

        # gradientTape autodiff
        with tf.GradientTape() as tape:
            y_pred=model(X_batch, training=True)
            loss_values=loss(y_batch, y_pred)
        gradients=tape.gradient(loss_values, model.trainable_weights)
        optimizer.apply_gradients(zip(gradients, model.trainable_weights))

        metricLoss(y_batch, y_pred)
        metricsAcc.update_state(y_batch, y_pred)

        # Loss and accuracy
        train_loss_results.append(loss_values)
        train_accuracy_results.append(metricsAcc.result())

        # Read out training results
        readout = 'Epoch {}, Training loss: {}, Training accuracy: {}'
        print(readout.format(epoch + 1, loss_values,
                              metricsAcc.result() * 100))

        metricsAcc.reset_states

        # Run a validation loop at the end of each epoch

    for valbatch in range(1+ n_steps +1):
        X_batchVal, y_batchVal = random_batch(X_val.values, y_val)

        val_logits = model(X_batchVal)
        # Update val metrics
        val_acc_metric(y_batchVal, val_logits)
        val_acc = val_acc_metric.result()

        val_acc_metric.update_state(y_batchVal, val_logits)

        val_loss=val_acc_loss(y_batchVal, val_logits)

        validation_loss_results.append(val_loss)
        validation_accuracy_results.append(val_acc_metric.result())

        # Read out validation results
        print( 'Validation loss: ' , float(val_loss),'Validation acc: %s' % (float(val_acc * 100),) )

        val_acc_metric.reset_states()
```

When i run this code, it works fine, and the iterations update the states of the accuracy and loss: however, the training accuracy is much lower than the model.fit method, after running also for 100 epochs: showing final epoch result that is printed (each same epoch is iterating over each batch):

Epoch 100, Training loss: 0.027735430747270584, Training accuracy: 93.6534423828125
Epoch 100, Training loss: 0.03832387551665306, Training accuracy: 93.67249298095703
**Epoch 100, Training loss: 0.035500235855579376, Training accuracy: 93.69097900390625**
Validation loss:  0.3204055726528168 Validation acc: 90.36458587646484
Validation loss:  0.32066160440444946 Validation acc: 89.71354675292969
Validation loss:  0.32083287835121155 Validation acc: 90.49479675292969
Validation loss:  0.3209479749202728 Validation acc: 90.10416412353516
**Validation loss:  0.32088229060173035 Validation acc: 90.625**

As you can see,  the training accuracy is ~4-5% lower compared to the model.fit method. The loss records fine, and also, the validation data looks pretty much just like the validation data in the model.fit method. 

Additionally, when i plot accuracy and loss in both model.fit and geadientTape methods, the shape of the curves look pretty much the same, and they both begin to overfit at similar points! but again, there is a huge discrepancy in the training accuracy. 

I have specified the adam optimizer as well binary_crossentropy loss in model.fit and gradientTape. For model.fit, when I specific 'accuracy' or 'acc' for metrics, my understanding is that it will call on the binary_accuracy for calculating the accuracy. So as far as I am aware the parameters are similar that results should be fairly similar. 

Additionally, when i call` model.compile` after training the model with `gradientTape` just to confirm evaluation, the results are slightly different again and look more like the model.fit method:

```
**Training**
model.compile(optimizer=optimizer, loss=tf.keras.losses.binary_crossentropy, metrics=['acc'])
print('\n', model.evaluate(X_train, y_train, verbose=1)[1])

32/101 [========>.....................] - ETA: 0s - loss: 0.0336 - acc: 0.9948
101/101 [==============================] - 0s 307us/sample - **loss: 0.0330 - acc: 0.9942**

**Validation**
model.compile(optimizer=optimizer, loss=tf.keras.losses.binary_crossentropy, metrics=['acc'])
print('\n', model.evaluate(X_val, y_val, verbose=1)[1])

18/18 [==============================] - 0s 111us/sample - **loss: 0.3879 - acc: 0.9028**
```

Now model.evaluate shows a loss and accuracy that are very similar to the model.fit method when i call evaluate on X_train and y_train. This is why i am suspect of a bug? Interestingly, the model.evaluate on validation data look similar to the gradientTape loop which leaves me really confused as i am therefore unsure of the true training accuracy and loss!

If anyone can help i would really appreciate this... I am happy to provide further code upstream of the model etc.. Again, apologies if this is not a bug but this seems really confusing to me like an incorrect behaviour...




"
35584,TF 1.14 build failed on Ubuntu 18.04: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Source
- TensorFlow version:1.14
- Python version:3.6.9
- Installed using virtualenv? pip? conda?: Virtualenv
- Bazel version (if compiling from source):1.1.0
- GCC/Compiler version (if compiling from source):gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
- CUDA/cuDNN version: Cuda=10.2/cuDNN=7.6.5
- GPU model and memory:Geforce GTX 1070/8G/Driver Version: 440.33.01    


**Describe the problem**
Undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_

Build fails with error: ERROR: /data/vkurien/tensorflow-build/tensorflow/tensorflow/python/keras/api/BUILD:115:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)
Traceback (most recent call last):
  File ""/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 776, in <module>
    main()
  File ""/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 772, in main
    lazy_loading, args.use_relative_imports)
  File ""/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 629, in create_api_files
    compat_api_versions, lazy_loading, use_relative_imports)
  File ""/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 503, in get_api_init_text
    _, attr = tf_decorator.unwrap(attr)
  File ""/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py"", line 219, in unwrap
    elif _has_tf_decorator_attr(cur):
  File ""/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py"", line 124, in _has_tf_decorator_attr
    hasattr(obj, '_tf_decorator') and
  File ""/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py"", line 62, in __getattr__
    module = self._load()
  File ""/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py"", line 45, in _load
    module = importlib.import_module(self.__name__)
  File ""/home/vkurien/.virtualenvs/tensorflow-build-1.14/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py"", line 28, in <module>
    _wrap_py_utils = swig_import_helper()
  File ""/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_wrap_py_utils', fp, pathname, description)
  File ""/home/vkurien/.virtualenvs/tensorflow-build-1.14/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/vkurien/.virtualenvs/tensorflow-build-1.14/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
  File ""<frozen importlib._bootstrap>"", line 684, in _load
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: /home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /data/vkurien/tensorflow-build/tensorflow/tensorflow/python/tools/BUILD:98:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)
INFO: Elapsed time: 6732.030s, Critical Path: 456.19s
INFO: 4238 processes: 4238 local.
FAILED: Build did NOT complete successfully

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Ran ./configure and accepted CUDA and TensorRT option. 
Then ran: bazel-1.1.0 build --config=opt --config=v1 --verbose_failures //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35583,TensorRT 6.0.1 performs worse than TensorRT 5.1.6 on Jetson AGX Xavier,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetpack 4.3 (L4T 32.3.1 Ubuntu 18.04)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Nvidia Official Wheel
- TensorFlow version (use command below): 1.13.1 & 1.15.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0.326 / 7.5.0 & 7.6.3
- GPU model and memory: Tegra Xavier 16 GB

**Describe the current behavior**
I am currently trying some examples within from docker containers on new release of Jetpack (4.3). I wanted to do some benchmarkings and compare TensorRT 6.0.1 and cuDNN 7.6.3 versions with the old ones (5.1.6 and 7.5.0 respectively). I have some issues regarding to this as follows;

1. **24 FPS** with --> protobuf=3.6.1, tensorflow-gpu=1.13.1+nv19.3, tensorrt=5.1.6, cudnn=7.5.0 opencv=3.3.1
2. **9.6 FPS** with --> protobuf=3.8.0, tensorflow-gpu=1.15.0+nv19.11, tensorrt=6.0.1, cudnn=7.6.3, opencv=4.1.1
3. **8.5 FPS** with --> protobuf=3.6.1, tensorflow-gpu=1.15.0+nv19.11, tensorrt=6.0.1, cudnn=7.6.3, opencv=4.1.1
4. **6.5 FPS** with --> protobuf=3.6.1, tensorflow-gpu=1.15.0+nv19.11, tensorrt=5.1.6, cudnn=7.5.0, opencv=3.4.6

**Describe the expected behavior**
Improved inference performance (above 24 FPS) with the new versions of Tensorflow, cuDNN and TensorRT

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
https://github.com/jkjung-avt/tf_trt_models --> `python3 camera_tf_trt.py --usb --model ssd_mobilenet_v1_coco --build`

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I've created a [topic](https://devtalk.nvidia.com/default/topic/1069238/jetson-agx-xavier/tensorrt-6-0-1-performs-worse-than-tensorrt-5-1-6-on-jetson-agx-xavier/) on Nvidia DevTalk forum and here is the answer I got from Nvidia people: [DevTalk forum comment](https://devtalk.nvidia.com/default/topic/1069238/jetson-agx-xavier/tensorrt-6-0-1-performs-worse-than-tensorrt-5-1-6-on-jetson-agx-xavier/post/5416335/#5416335)"
35582,Tensorflow 1.14.0 can not be built from source on Nvidia Jetson,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: L4T 32.3.1 Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.14.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.24.1
- **GCC/Compiler version (if compiling from source)**: GCC 7.4
- **CUDA/cuDNN version**: 10.0.326 / 7.6.3
- **GPU model and memory**: Tegra Xavier / 16 GB
- **Exact command to reproduce**: 
```
export TMP=/tmp
PYTHON_BIN_PATH=$(which python3) \
PYTHON_LIB_PATH=$(python3 -c 'import site; print(site.getsitepackages()[0])') \
TF_CUDA_COMPUTE_CAPABILITIES=${cuda_compute} \
TF_CUDA_VERSION=10.0 \
TF_CUDA_CLANG=0 \
TF_CUDNN_VERSION=7 \
TF_TENSORRT_VERSION=${trt_version} \
CUDA_TOOLKIT_PATH=/usr/local/cuda \
CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu \
TENSORRT_INSTALL_PATH=/usr/lib/aarch64-linux-gnu \
TF_NEED_IGNITE=0 \
TF_ENABLE_XLA=0 \
TF_NEED_OPENCL_SYCL=0 \
TF_NEED_COMPUTECPP=0 \
TF_NEED_ROCM=0 \
TF_NEED_CUDA=1 \
TF_NEED_TENSORRT=1 \
TF_NEED_OPENCL=0 \
TF_NEED_MPI=0 \
GCC_HOST_COMPILER_PATH=$(which gcc) \
CC_OPT_FLAGS=""-march=native"" \
TF_SET_ANDROID_WORKSPACE=0 \
    ./configure
```
```
bazel build --config=opt \
	    --config=cuda \
	    --local_resources=${local_resources} \
            //tensorflow/tools/pip_package:build_pip_package
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Hi, I am currently trying to build tensorflow 1.14.0 on Nvidia Jetson platform for TensorRT 6.0.1. But it is too painful to do so because it keep throwing errors and fails. I have applied some patches to the source code as described in below links:

- https://github.com/tensorflow/tensorflow/issues/19956#issuecomment-397212193
- https://github.com/tensorflow/tensorflow/issues/28277#issuecomment-517469318
- https://github.com/tensorflow/tensorflow/issues/32925#issuecomment-537593098

However, after all of the above patches, I still get the following errors shown on the logs section below. It is not the full log but if it is required, I can provide the full log too. Thanks in advance.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
In file included from ./tensorflow/compiler/tf2tensorrt/utils/trt_allocator.h:25:0,
                 from ./tensorflow/compiler/tf2tensorrt/convert/convert_nodes.h:26,
                 from tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:16:
bazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:5611:33: note: declared here
     TRT_DEPRECATED virtual void setInt8Calibrator(IInt8Calibrator* calibrator) TRTNOEXCEPT = 0;
                                 ^~~~~~~~~~~~~~~~~
tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:4939:77: warning: 'virtual nvinfer1::INetworkDefinition* nvinfer1::IBuilder::createNetwork()' is deprecated [-Wdeprecated-declarations]
       TrtUniquePtrType<nvinfer1::INetworkDefinition>(builder->createNetwork());
                                                                             ^
In file included from ./tensorflow/compiler/tf2tensorrt/utils/trt_allocator.h:25:0,
                 from ./tensorflow/compiler/tf2tensorrt/convert/convert_nodes.h:26,
                 from tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:16:
bazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:5431:58: note: declared here
     TRT_DEPRECATED virtual nvinfer1::INetworkDefinition* createNetwork() TRTNOEXCEPT = 0;
                                                          ^~~~~~~~~~~~~
tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:5011:62: warning: 'virtual nvinfer1::ICudaEngine* nvinfer1::IBuilder::buildCudaEngine(nvinfer1::INetworkDefinition&)' is deprecated [-Wdeprecated-declarations]
   engine->reset(builder->buildCudaEngine(*converter.network()));
                                                              ^
In file included from ./tensorflow/compiler/tf2tensorrt/utils/trt_allocator.h:25:0,
                 from ./tensorflow/compiler/tf2tensorrt/convert/convert_nodes.h:26,
                 from tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:16:
bazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:5566:51: note: declared here
     TRT_DEPRECATED virtual nvinfer1::ICudaEngine* buildCudaEngine(
                                                   ^~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 6315.216s, Critical Path: 450.27s
INFO: 5761 processes: 5761 local.
FAILED: Build did NOT complete successfully
```

"
35579,tf.linalg.expm is incompatible with vectorized_map,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.2
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0-dev20191221
- Python version: 3.7

**Describe the current behavior**
Vectorizing `tf.linalg.expm` throws the error: `UnrecognizedFlagError: Unknown command line flag 'f'`

**Describe the expected behavior**
No error

**Code to reproduce the issue**
`tf.vectorized_map(lambda x: tf.linalg.expm(x), tf.reshape(tf.range(8.0),[2,2,2]))`

**Other info / logs**
This may not be related specifically to expm, as it occurs with [other functions](https://github.com/tensorflow/tensorflow/issues/34734) as well
"
35577,Is there any way to convert tflite file to .pb?,"I have done a terrible mistake(accidentally deleting the dataset). The dataset was my many month's research. But fortunately, I have a tflite file is there any features available to convert it back to .pb. file?


**System information**
- I'm using the latest version of Tensorflow

"
35576,fatal error: tensorflow/core/common_runtime/gpu/gpu_process_state.h: No such file or directory,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux (Kernel 5.3.18, glibc 2.30-3)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: `bd56e040ba4a8272163357a2f8786a128deb4aaf` from `r2.1` branch.
- Python version: Not Applicable
- Installed using virtualenv? pip? conda?: Not Applicable
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source): Host GCC 9.2.0; Cuda GCC 6.5.0
- CUDA/cuDNN version: CUDA 10.0; cuDNN 7
- GPU model and memory: RTX 2070 8GB



**Describe the problem**

I'm trying to upgrade the TensorFlow dependency in our project from `42c4f4ab6b53bce8639c203d7839d27eac11bd2f` (from `r1.13` branch) to `bd56e040ba4a8272163357a2f8786a128deb4aaf` (from `r2.1` branch). The problem is that `gpu_process_state.h` is missing. The error line in our project is [here](https://github.com/uwsampl/nexus/blob/bd1a079ebea4cbb076bd3b4b2b8d010b49794afa/src/nexus/backend/tensorflow_model.cpp#L6).

```
/home/abcdabcd987/work/nexus/src/nexus/backend/tensorflow_model.cpp:6:10: fatal error: tensorflow/core/common_runtime/gpu/gpu_process_state.h: No such file or directory
    6 | #include ""tensorflow/core/common_runtime/gpu/gpu_process_state.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
```

I checked the cloned TensorFlow source code. I can see that [`gpu_process_state.h`](https://github.com/tensorflow/tensorflow/blob/bd56e040ba4a8272163357a2f8786a128deb4aaf/tensorflow/core/common_runtime/gpu/gpu_process_state.h) exists. However, when I checked that build output that bazel produces, it was not there:

```
$ pwd
/home/abcdabcd987/work/nexus/build/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/include

$ ls -la tensorflow/core/common_runtime/gpu/
total 28
drwxr-xr-x 2 abcdabcd987 abcdabcd987 4096 Jan  3 20:37 ./
drwxr-xr-x 3 abcdabcd987 abcdabcd987 4096 Jan  3 20:37 ../
-rw-r--r-- 1 abcdabcd987 abcdabcd987 7639 Jan  3 20:37 gpu_event_mgr.h
-rw-r--r-- 1 abcdabcd987 abcdabcd987 4091 Jan  3 20:37 gpu_id.h
-rw-r--r-- 1 abcdabcd987 abcdabcd987 1617 Jan  3 20:37 gpu_id_manager.h
-rw-r--r-- 1 abcdabcd987 abcdabcd987 1665 Jan  3 20:37 gpu_init.h
```

I tried to figure out the difference of the BUILD files between the old and the new version, but since the `//tensorflow:install_headers` target is quite complicated, I cannot get it straight. Any help is much appreciated! Thanks!

**Provide the exact sequence of commands / steps that you executed before running into the problem**

TensorFlow in our project is built with the following command:

```
bazel --output_base=../build/tensorflow build \
  --config=opt \
  --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \
  --action_env CUDNN_INSTALL_PATH=/usr/local/cuda-10.0 \
  --action_env GCC_HOST_COMPILER_PATH=/usr/bin/gcc-6 \
  //tensorflow:libtensorflow_cc.so \
  //tensorflow:libtensorflow_framework.so \
  //tensorflow:install_headers
```

The `.tf_configure.bazelrc` is the following:

```
build:xla --define with_xla_support=false
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_ROCM=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env TF_CUDA_VERSION=""10.0""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TF_NCCL_VERSION=""""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.0,7.5""
build --action_env TF_CUDA_CLANG=""0""
build --config=cuda
build:opt --copt=-march=native
build:opt --copt=-Wno-sign-compare
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_tag_filters=-benchmark-test,-no_oss,-oss_serial
test --build_tag_filters=-benchmark-test,-no_oss
test --test_tag_filters=-gpu
test --build_tag_filters=-gpu
build --action_env TF_CONFIGURE_IOS=""0""
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35574,OP_REQUIRES failed due to missing tempstate or invalid argument,"**System information**
- Custom implementation of ALBERT for TF2.0
- Training on DataBricks
- TensorFlow installed from (source or binary): Binary pypi
- TensorFlow version (use command below): 2.0-gpu
- Python version: 3.7
- CUDA/cuDNN version: 10.1
- GPU model and memory: Databricks 4xGPU cluster X8

**Describe the current behavior**

Current behavior: Training proceeds through 3 epochs and barfs on Train Step 399/1330 in the _save_checkpoint routine. We are using ADLSgen2 to store data, and we are mounting to that filesystem. Filesystem works transparently for all tasks so far & we have DataBricks team support. There are many read/writes to this mount point so I am not quick to blame the filesystem - but it's still worth noting.

The actual error is:
2020-01-03 20:51:34.688654: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at save_restore_v2_ops.cc:137 : Invalid argument: /dbfs/mnt/devscoutprototype/eval_out/checkpoint/ctl_step_399.ckpt-3_temp_08f0418651184fbd97263e13fc11b45c/part-00001-of-00002.data-00000-of-00001.tempstate9461307533243821894; Invalid argument

When we navigate to this location we find the folder is genuinely there, however inside that folder there is not this temp file. Instead there are two files:
1)part-00000-of-00002.data-00000-of-00001
2)part-00000-of-00002.index

**Describe the expected behavior**

I would expect to see a new checkpoint at this step.

**Code to reproduce the issue**
This is made following the examples of https://github.com/kamalkraj/ALBERT-TF2.0
If you simply run this example top to bottom on DataBricks you will encounter this error.

**Other info / logs**

loss = 0.2804690897464752
I0103 20:48:47.790678 140033029736192 model_training_utils.py:346] Train Step: 396/1330  / loss = 0.3561434745788574
I0103 20:48:50.499201 140033029736192 model_training_utils.py:346] Train Step: 397/1330  / loss = 0.26206889748573303
I0103 20:48:53.168039 140033029736192 model_training_utils.py:346] Train Step: 398/1330  / loss = 0.3312344551086426
I0103 20:48:55.838387 140033029736192 model_training_utils.py:346] Train Step: 399/1330  / loss = 0.41013699769973755
2020-01-03 20:51:34.688654: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at save_restore_v2_ops.cc:137 : Invalid argument: /dbfs/mnt/devscoutprototype/eval_out/checkpoint/ctl_step_399.ckpt-3_temp_08f0418651184fbd97263e13fc11b45c/part-00001-of-00002.data-00000-of-00001.tempstate9461307533243821894; Invalid argument
Traceback (most recent call last):
  File ""/dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/run_classifer.py"", line 455, in <module>
    app.run(main)
  File ""/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/run_classifer.py"", line 356, in main
    custom_callbacks = custom_callbacks)
  File ""/dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/model_training_utils.py"", line 354, in run_customized_training_loop
    checkpoint_name.format(step=current_step))
  File ""/dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/model_training_utils.py"", line 33, in _save_checkpoint
    saved_path = checkpoint.save(checkpoint_path)
  File ""/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py"", line 1889, in save
    file_path = self.write(""%s-%d"" % (file_prefix, checkpoint_number))
  File ""/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py"", line 1819, in write
    output = self._saver.save(file_prefix=file_prefix)
  File ""/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py"", line 1155, in save
    file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)
  File ""/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py"", line 1103, in _save_cached_when_graph_building
    save_op = saver.save(file_prefix)
  File ""/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py"", line 230, in save
    sharded_saves.append(saver.save(shard_prefix))
  File ""/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py"", line 72, in save
    return io_ops.save_v2(file_prefix, tensor_names, tensor_slices, tensors)
  File ""/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py"", line 1933, in save_v2
    ctx=_ctx)
  File ""/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py"", line 1970, in save_v2_eager_fallback
    ctx=_ctx, name=name)
  File ""/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: /dbfs/mnt/devscoutprototype/eval_out/checkpoint/ctl_step_399.ckpt-3_temp_08f0418651184fbd97263e13fc11b45c/part-00001-of-00002.data-00000-of-00001.tempstate9461307533243821894; Invalid argument [Op:SaveV2]



---------------------------------------------------------------------------
CalledProcessError                        Traceback (most recent call last)
<command-1805221645764486> in <module>
----> 1 get_ipython().run_cell_magic('sh', '', ""\n# Running classifier:\n\nexport GLUE_DIR='/dbfs/mnt/devscoutprototype/dataset/'\nexport ALBERT_DIR='/dbfs/mnt/devscoutprototype/models/base_2/'\nexport TASK_NAME='CoLA'\nexport OUTPUT_DIR='/dbfs/mnt/devscoutprototype/train_out'\nexport MODEL_DIR='/dbfs/mnt/devscoutprototype/eval_out/'\n\npython /dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/run_classifer.py --train_data_path=${OUTPUT_DIR}/${TASK_NAME}_train.tf_record --eval_data_path=${OUTPUT_DIR}/${TASK_NAME}_eval.tf_record --input_meta_data_path=${OUTPUT_DIR}/${TASK_NAME}_meta_data --init_checkpoint=${ALBERT_DIR}/tf2_model.h5 --spm_model_file=${ALBERT_DIR}/vocab/30k-clean.model --albert_config_file=${ALBERT_DIR}/config.json --output_dir=${MODEL_DIR} --do_train --task_name=${TASK_NAME} --do_eval --custom_training_loop --train_batch_size=64 --learning_rate=1e-5 --num_train_epochs=10\n"")

/databricks/python/lib/python3.7/site-packages/IPython/core/interactiveshell.py in run_cell_magic(self, magic_name, line, cell)
   2350             with self.builtin_trap:
   2351                 args = (magic_arg_s, cell)
-> 2352                 result = fn(*args, **kwargs)
   2353             return result
   2354 

/databricks/python/lib/python3.7/site-packages/IPython/core/magics/script.py in named_script_magic(line, cell)
    140             else:
    141                 line = script
--> 142             return self.shebang(line, cell)
    143 
    144         # write a basic docstring:

</databricks/python/lib/python3.7/site-packages/decorator.py:decorator-gen-110> in shebang(self, line, cell)

/databricks/python/lib/python3.7/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k)
    185     # but it's overkill for just that one bit of state.
    186     def magic_deco(arg):
--> 187         call = lambda f, *a, **k: f(*a, **k)
    188 
    189         if callable(arg):

/databricks/python/lib/python3.7/site-packages/IPython/core/magics/script.py in shebang(self, line, cell)
    243             sys.stderr.flush()
    244         if args.raise_error and p.returncode!=0:
--> 245             raise CalledProcessError(p.returncode, cell, output=out, stderr=err)
    246 
    247     def _run_script(self, p, cell, to_close):

CalledProcessError: Command 'b""\n# Running classifier:\n\nexport GLUE_DIR='/dbfs/mnt/devscoutprototype/dataset/'\nexport ALBERT_DIR='/dbfs/mnt/devscoutprototype/models/base_2/'\nexport TASK_NAME='CoLA'\nexport OUTPUT_DIR='/dbfs/mnt/devscoutprototype/train_out'\nexport MODEL_DIR='/dbfs/mnt/devscoutprototype/eval_out/'\n\npython /dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/run_classifer.py "
35573,tensorflow 2.0.0 crashes with protobuf 3.10.1 on macOS,"Using MacPorts with the latest up-to-date versions (tensorflow 2.0.0, protobuf 3.10.1),

> `python3 -c 'import tensorflow'`
```
[libprotobuf ERROR google/protobuf/descriptor_database.cc:394] Invalid file descriptor data passed to EncodedDescriptorDatabase::Add().
[libprotobuf FATAL google/protobuf/descriptor.cc:1359] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): 
libc++abi.dylib: terminating with uncaught exception of type google::protobuf::FatalException: CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): 
Abort trap: 6
```

This is reported and closed in https://github.com/tensorflow/tensorboard/issues/2985, but the issue persists.

A workaround is `pip-3.7 install protobuf==3.8`, but this doesn't work with system-level, up-to-date library installs like with MacPorts. Also see: https://trac.macports.org/ticket/59826

Is this an issue with TF, or with protobuf?"
35571,"Autograph in Tensorflow 1.15  has unexpected behavior with ""=="" conditional statement","**System information**
- Custom code: Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow version: 1.15.0 (git version v1.15.0-rc3-22-g590d6eef7e)
- Python version: 3.6

**Current behavior**
Using autograph to translate python code with ""=="" conditional statement leads to unexpected behavior, where the value of the variable in conditional statement does not affect the output. The translation does not give an error, but leads to wrong implementation. This behavior is unique to ""=="" and does not occur with ""<, >, <=, >="" conditionals. 

**Expected behavior**
The code execution should be affected by truth state of ""=="" conditional statement. Although, one can get desired behavior by replacing ""if z == 0:"" with ""if tf.math.equal(z, 0):"" as a workaround, but one might not have access to replace this statement. This issue can lead to unintended consequences. 

**Code to reproduce the issue**
This is a modification of square_if_positive code from TensorFlow documentation.

```
import tensorflow as tf
from tensorflow import autograph as ag

#minimal code for method to demonstrate issue
def foo(x, opt):
    if opt == 0:
        y = x * x
    else:
        y = -x
    return y

#graph construction
mdl = tf.Graph()
with mdl.as_default():
    converted_foo = ag.to_graph(foo)
    print(ag.to_code(foo))
    x = tf.placeholder(tf.float64, name='x')
    opt = tf.placeholder(tf.float64, name='opt')
    y = converted_foo(x, opt)

#graph execution
with tf.Session(graph=mdl) as sess:
    aval = sess.run(y, feed_dict={x:4, opt:0})
    print('output for opt = 0 is ' + str(aval))
    aval = sess.run(y, feed_dict={x:4, opt:1})
    print('output for opt = 1 is ' + str(aval))
```
Current Response:
```
output for opt = 0 is -4.0
output for opt = 1 is -4.0
```
Desired Response:
```
output for opt = 0 is 16.0
output for opt = 1 is -4.0
```
We can get desired response by modifying python source code of ```foo``` method, but that is not the ideal solution to this issue:
```
#minimal code for method to demonstrate issue
def foo(x, opt):
    if tf.math.equal(opt,0):
        y = x * x
    else:
        y = -x
    return y
```
**Other info / logs**

In Tensorboard, we can see that with ""=="" the truth state of ""opt == 0"" is completely ignored
![expected_with_equals](https://user-images.githubusercontent.com/8063769/71745435-1eeb3a00-2e38-11ea-9b23-c5a8ae257b70.PNG)

However, with ""tf.math.equal(opt,0)"", we get desired graph:
![expected_with_tf_equal](https://user-images.githubusercontent.com/8063769/71745450-290d3880-2e38-11ea-843c-0446ae20461f.PNG)


"
35569,colocation_graph started logging so much in TF 1.15.0 from TF 1.13.1,"Hi,

We have a TF code to create a graph which later on it's being loaded into our Scala/Java application. 
[Code to generate the graph](https://github.com/JohnSnowLabs/spark-nlp/blob/9407da076eced850ec840cfc61e665887400de12/python/tensorflow/lib/ner/ner_model.py)

In TF `1.12.0` and TF `1.13.1` we used to only see the device placement logs in the console, but in TF `1.15.0` it logs over thousands of lines complaining about:

```
Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [
  /job:localhost/replica:0/task:0/device:CPU:0].
```

The full log is here due to the length:
https://gist.github.com/maziyarpanahi/83f179e01634db2de12cb82501177c8d

I understand what the log is saying and I know there is no way to control the logging level through Java API, but I would like to understand what has changed in TF 1.15.0 that we have now all these logs without any change in our process.  

Issue: It is really hard to debug with that many logs in the console and also the Travis fails due to exceeding logs limit since we have many unit tests using this graph and every time there will be 1000 lines of logs saying something we really don't care.

Any help to manage to suppress these logs would be highly appreciated.



"
35568,A bug with inaccurate metric accuracy when using closure loss,"I just wanted to report a bug with closure loss in TF that gives us inaccurate accuracy result. Let me first by providing a simple code that build image classification on MNIST dataset successfully in [1]. I trained the model with 1 epoch and I got a very decent result:

Result:
`60000/60000 [==============================] - 10s 168us/sample - loss: 0.2097 - acc: 0.9362 - val_loss: 0.0459 - val_acc: 0.9850`

However, when I change the loss to using a ""closure"" loss as in [2], I got a different result regarding to the accuracy.

`60000/60000 [==============================] - 10s 165us/sample - loss: 0.1980 - acc: 0.0989 - val_loss: 0.0433 - val_acc: 0.0995`

Indeed, I found the reported accuracy (acc and val_acc) is very inaccurate, which I believe is a bug.  You can check the prediction output for some examples, which is highly accurate.

Meanwhile the loss is computed correctly I think.


Code for [1]:

```
from __future__ import print_function
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras import backend as K
import numpy as np


num_classes = 10
epochs = 1

# input image dimensions
img_rows, img_cols = 28, 28

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

if K.image_data_format() == 'channels_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255

def create_model():
  input_sequence = tf.keras.layers.Input(dtype='float32', shape=input_shape, name='input_sequence') 
  conv1 = Conv2D(32, kernel_size=(3, 3),
                   activation='relu')(input_sequence)
  conv2d = Conv2D(64, (3, 3), activation='relu')(conv1)
  conv2d = Dropout(0.25)(MaxPooling2D(pool_size=(2, 2))(conv2d))
  conv2d = Flatten()(conv2d)
  conv2d = Dropout(0.5)(Dense(128, activation='relu')(conv2d))
  output = Dense(num_classes, activation='softmax')(conv2d)
  model = tf.keras.models.Model(inputs=[input_sequence], outputs=output)
  model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,
                optimizer=keras.optimizers.Adam(),
                metrics=['accuracy'])
  return model
  
model = create_model()
batch_size = 64
model.fit([np.array(x_train)], np.array(y_train),
  verbose=1,
  batch_size = batch_size,
  epochs=epochs,
  validation_data=([x_test], np.array(y_test)))
a = model.predict([x_test])
for x, y in zip(a[:20], y_test[:20]):
  print(x, np.argmax(x), y)
```

Code for [2]:
```

from __future__ import print_function
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras import backend as K
import numpy as np

num_classes = 10
epochs = 1

# input image dimensions
img_rows, img_cols = 28, 28

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

if K.image_data_format() == 'channels_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255

def create_model():
  input_sequence = tf.keras.layers.Input(dtype='float32', shape=input_shape, name='input_sequence') 
  conv1 = Conv2D(32, kernel_size=(3, 3),
                   activation='relu')(input_sequence)
  conv2d = Conv2D(64, (3, 3), activation='relu')(conv1)
  conv2d = Dropout(0.25)(MaxPooling2D(pool_size=(2, 2))(conv2d))
  conv2d = Flatten()(conv2d)
  conv2d = Dropout(0.5)(Dense(128, activation='relu')(conv2d))
  output = Dense(num_classes, activation='softmax')(conv2d)
  model = tf.keras.models.Model(inputs=[input_sequence], outputs=output)
  def custom_loss():
    def loss(y_true, y_predict):
      return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_predict)
    return loss
  model.compile(loss=custom_loss(),
                optimizer=keras.optimizers.Adam(),
                metrics=['accuracy'])
  return model
  
model = create_model()
batch_size = 64
model.fit([np.array(x_train)], np.array(y_train),
  verbose=1,
  batch_size = batch_size,
  epochs=epochs,
  validation_data=([x_test], np.array(y_test)))
a = model.predict([x_test])
for x, y in zip(a[:20], y_test[:20]):
  print(x, np.argmax(x), y)
```"
35565,Typo Errors in `l05c03_exercise_flowers_with_data_augmentation.ipynb`,"## URL(s) with the issue:

https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l05c03_exercise_flowers_with_data_augmentation.ipynb

## Description of issue (what needs changing):

In the directory structure, it should be ""daisy"" instead of ""diasy""

![Screenshot from 2020-01-03 18-39-11](https://user-images.githubusercontent.com/29497701/71725075-a8aaff80-2e58-11ea-9ac8-076078500026.png)

### Submit a pull request?

Yes
"
35564,tflite_runtime interpreter.py NOT ImplementedError,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Buster
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) Raspberry Pi 4B
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.14
- Python version:3.7
- Installed using virtualenv? pip? conda?: Pip within virtual environment
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

Installed Tesnorflow Lite Runtime from the instructions in Python Quickstart using tflite_runtime-1.14.0-cp37-cp37m-linux_armv7l.whl.

On executing the following command:

python label_image.py   --model_file /home/pi/mobilenet_v1_1.0_224.tflite   --label_file /home/pi/labels.txt   --image /tmp/grace_hopper.bmp

I get:

2020-01-03 11:20:05.640880: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory
Traceback (most recent call last):
  File ""label_image.py"", line 63, in <module>
    interpreter = tflite.Interpreter(model_path=args.model_file)
  File ""/home/pi/.virtualenvs/HOLLY/lib/python3.7/site-packages/tflite_runtime/interpreter.py"", line 206, in __init__
    model_path))
NotImplementedError: Wrong number or type of arguments for overloaded function 'InterpreterWrapper_CreateWrapperCPPFromFile'.
  Possible C/C++ prototypes are:
    tflite::interpreter_wrapper::InterpreterWrapper::CreateWrapperCPPFromFile(char const *,std::vector< std::string > const &,std::string *)
    tflite::interpreter_wrapper::InterpreterWrapper::tflite_interpreter_wrapper_InterpreterWrapper_CreateWrapperCPPFromFile__SWIG_1(char const *,PyObject *)

Very new to GitHub and Tensorflow so appologies if im miss posting or not providing enough info but i'm a bit stuck on how to resolve this.

Many thanks

Eifion

"
35563,copy_to_device + prefetch not working,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v1.12.1-21401-gd908b50 2.1.0-dev20191230
- Python version: 3.6.9
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: CUDA Version 10.1.243 / cuDNN 7.6.4.38-1
- GPU model and memory: TITAN V, 12GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I have tried both combinations of copy_to_device(""/gpu:0"") followed by prefetch and prefetch_to_device(""/gpu:0""). In both cases the resulting tensors are placed in ""/cpu:0"" after consuming them from an iterator.

**Describe the expected behavior**

My expectation is that the resulting tensors should still sit in the device where they have been copied (""/gpu:0"" in this case). Executing simply a copy_to_device(""/gpu:0"") does return tensors placed in the GPU.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

>>> import tensorflow as tf
>>> dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])
>>> ds = dataset.batch(2).prefetch(1)
>>> for x in ds: print(x.device)
... 
/job:localhost/replica:0/task:0/device:CPU:0
/job:localhost/replica:0/task:0/device:CPU:0
/job:localhost/replica:0/task:0/device:CPU:0
>>> ds = dataset.batch(2).apply(tf.data.experimental.copy_to_device(""/gpu:0""))
>>> for x in ds: print(x.device)
... 
/job:localhost/replica:0/task:0/device:GPU:0
/job:localhost/replica:0/task:0/device:GPU:0
/job:localhost/replica:0/task:0/device:GPU:0
>>> ds = dataset.batch(2).apply(tf.data.experimental.copy_to_device(""/gpu:0"")).prefetch(1)
>>> for x in ds: print(x.device)
... 
/job:localhost/replica:0/task:0/device:CPU:0
/job:localhost/replica:0/task:0/device:CPU:0
/job:localhost/replica:0/task:0/device:CPU:0
>>> ds = dataset.batch(2).apply(tf.data.experimental.prefetch_to_device(""/gpu:0"", 1))
>>> for x in ds: print(x.device)
... 
/job:localhost/replica:0/task:0/device:CPU:0
/job:localhost/replica:0/task:0/device:CPU:0
/job:localhost/replica:0/task:0/device:CPU:0
>>> 


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35562,PEM routines:OPENSSL_internal:NO_START_LINE,"Actually  this is not a tensorflow problem. I had a project my own, it run well. But it comes to an runtime error when I link it to the the tensorflow_framework.so, even I didn't change any codes of my project(it doesn't include any of tensoflow headers).  It seems related to openssl certification, but it's hard to trace where the collapse occurred. Is there anybody can help me? Thanks very much!

**System information**
...
terminate called after throwing an instance of 'std::runtime_error'
  what():  Unable to parse cert 1: error:0900006e:PEM routines:OPENSSL_internal:NO_START_LINE
*** Aborted at 1578043256 (unix time) try ""date -d @1578043256"" if you are using GNU date ***
PC: @     0x7ffb9e4b8e97 gsignal
*** SIGABRT (@0x3e800006475) received by PID 25717 (TID 0x7ffbaa404500) from PID 25717; stack trace: ***
    @     0x7ffba03c7890 (unknown)
    @     0x7ffb9e4b8e97 gsignal
    @     0x7ffb9e4ba801 abort
    @     0x7ffb9eead957 (unknown)
    @     0x7ffb9eeb3ab6 (unknown)
    @     0x7ffb9eeb3af1 std::terminate()
    @     0x7ffb9eeb3d24 __cxa_throw
    @     0x555e5286eea7 (unknown)
    @     0x555e5272d044 (unknown)
    @     0x555e5272d342 (unknown)
    @     0x555e5245ec1c (unknown)
    @     0x555e52479c7e (unknown)
    @     0x555e52479e98 (unknown)
    @     0x555e5244452e (unknown)
    @     0x7ffb9e49bb97 __libc_start_main
    @     0x555e52449dea (unknown)


version: tensorflow 1.14.0 
"
35558,Your Session has crashed due to unknown reason  ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom Code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.1.0-rc1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory: Colab Notebook default GPU/CPU

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I am trying to train a model using two separate sub-models, each using audio and image data respectively. I have created an input pipeline having (audio data,image data and binary labels). According to <a href=""https://www.tensorflow.org/guide/keras/train_and_evaluate#training_evaluation_from_tfdata_datasets"">this</a> guide, I normally passed the tf.data.Dataset to the model.fit() method and soon the colab notebook crashed, giving an ""unknown error"". 

**Describe the expected behavior**
Starting of the training process

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
filenames = [data_path]
raw_dataset = tf.data.TFRecordDataset(filenames)
raw_dataset

feature = {'train/image': tf.io.FixedLenFeature([], tf.string),
            'train/label': tf.io.FixedLenFeature([], tf.int64),
           'train/audio': tf.io.FixedLenFeature([], tf.string),}

def _parse_function(example_proto):
  features = tf.io.parse_single_example(example_proto, feature)
  audio_raw = tf.expand_dims(tf.io.decode_raw(features['train/audio'],tf.float32),axis=-1)
  image_raw = tf.io.decode_raw(features['train/image'],tf.float32)
  image = tf.reshape(image_raw,[5,500,500,3])
  labels =features['train/label']
  labels = tf.one_hot(labels,depth=2)
  return audio_raw,image,labels

dataset = raw_dataset.map(_parse_function)
dataset

BATCH_SIZE = 4
BUFFER_SIZE = 10000

dataset = dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()
dataset
```
<br><br>

```
am = action_model((5,500,500,3))
afm = audio_and_final_model((105000,1))

combined = tf.keras.layers.Concatenate(axis=-1)([am.output,afm.output])
z = tf.keras.layers.Dense(32,activation='relu')(combined)
dropOut = tf.keras.layers.Dropout(0.3)(z)
out = tf.keras.layers.Dense(2,activation='tanh')(dropOut)
model = tf.keras.models.Model(inputs=[am.input,afm.input],outputs=out)
model.compile(loss='hinge', optimizer='adam', metrics=['accuracy'])

# print(model.summary())
EPOCHS=30
callbacks = [tf.keras.callbacks.ReduceLROnPlateau(verbose=1,monitor='loss',patience=2),tf.keras.callbacks.ModelCheckpoint('/content/drive/My Drive/Model.h5',monitor='loss',save_best_only=True,save_freq='epoch',save_weights_only=True),tf.keras.callbacks.TensorBoard()]
print(""Starting training"")
model.fit(dataset,epochs=EPOCHS,callbacks=callbacks)
print(""Saving Model"")
model.save(""Model.h5"")
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Logs- https://pastebin.com/BdXCRdvV"
35557,Line break missing in tf.keras.Model.fit() documentation,"## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=stable

## Description of issue (what needs changing):
In the `validation_data` part of `Model.fit()`, the third alternative reads
> dataset For the first two cases, `batch_size` must be provided. For the last case, `validation_steps` must be provided.

I feel a link break should be inserted after ""dataset""."
35556,tensorflow lite become slower after quantization,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.15 and 2.0
- Python version: using C++
- Bazel version (if compiling from source): 0.26 for tf1.5/ 1.10 for tf2.0
- GCC/Compiler version (if compiling from source): 4.9.4
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I'm using tensorflow lite for quantizing mobileNetV1 to int8. And then, I do inference using label_image.cc in tensorflow github source code. All the work is done on my Ubuntu16.04, on CPU, not GPU. Before quantization, doing inference takes 85ms. However, the quantized model takes 1200ms. I do the experiments using tf1.15/tf2.0, both of them have such issue. Really wired, quantized model should be faster."
35553,set_floatx('float16') slow to build/compile,"**System information**
Google Colab system (GPU mode) with TensorFlow 2.1-rc2 and Python 3.

**Describe the current behavior**
Take a long time to build/compile a seq2seq model when use ``set_floatx('float16')`` instead float32 (default)

**Describe the expected behavior**
I don't know if is normal, but default value ``float32`` takes 1-2 second

**Code to reproduce the issue**
I coded an example [here](https://drive.google.com/open?id=170KpjIGFmNNt-SS-6oXgVbRnZR4-7ah_)"
35551,TFLite expermimental_new_converter error with tf.keras Bidirectional Wrapper or attribute go_backwards=True,"**System information**
- OS Platform and Distribution: Linux Ubuntu  18.04
- TensorFlow installed from (source or binary): 
- TensorFlow version (or github SHA if from source):
Used to build model: 2.0.0
Used to run converter : 2.1.0-dev20191227

**Command used to run the converter or code if you’re using the Python API**

```
import tensorflow as tf
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

print(tf.__version__)

model = tf.keras.models.load_model(""/home/amish/PycharmProjects/myproject/scripts/temp.h5"")
model.summary()
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
converter.experimental_new_converter = True  # Add this line

tflite_model = converter.convert()
```

**The output from the converter invocation**

```
WARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .
2.1.0-dev20191227
2020-01-03 02:45:22.187710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-01-03 02:45:22.193838: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-01-03 02:45:22.193908: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: enigma
2020-01-03 02:45:22.193928: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: enigma
2020-01-03 02:45:22.207575: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.50.0
2020-01-03 02:45:22.207706: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.50.0
2020-01-03 02:45:22.207730: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.50.0
2020-01-03 02:45:22.208264: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-03 02:45:22.384583: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 2496000000 Hz
2020-01-03 02:45:22.391769: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cbc700ede0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-01-03 02:45:22.391868: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Model: ""sequential_14""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
masking_14 (Masking)         (None, 50, 36)            0         
_________________________________________________________________
bidirectional_6 (Bidirection (None, 50, 256)           168960    
_________________________________________________________________
dropout_16 (Dropout)         (None, 50, 256)           0         
_________________________________________________________________
dense_16 (Dense)             (None, 50, 10)            2570      
=================================================================
Total params: 171,530
Trainable params: 171,530
Non-trainable params: 0
_________________________________________________________________
2020-01-03 02:45:24.787867: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-01-03 02:45:24.788386: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-01-03 02:45:25.204179: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: graph_to_optimize
2020-01-03 02:45:25.204249: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: Graph size after: 255 nodes (0), 310 edges (0), time = 121.027ms.
2020-01-03 02:45:25.204268: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: Graph size after: 255 nodes (0), 310 edges (0), time = 9.261ms.
2020-01-03 02:45:25.204281: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: sequential_14_bidirectional_6_forward_lstm_56_while_body_3164
2020-01-03 02:45:25.204300: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.004ms.
2020-01-03 02:45:25.204316: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-01-03 02:45:25.204332: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: sequential_14_bidirectional_6_forward_lstm_56_while_cond_3163
2020-01-03 02:45:25.204345: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
2020-01-03 02:45:25.204360: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-01-03 02:45:25.204376: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: sequential_14_bidirectional_6_backward_lstm_56_while_body_3360
2020-01-03 02:45:25.204391: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
2020-01-03 02:45:25.204406: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-01-03 02:45:25.204422: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: sequential_14_bidirectional_6_backward_lstm_56_while_cond_3359
2020-01-03 02:45:25.204440: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
2020-01-03 02:45:25.204460: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-01-03 02:45:25.373374: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-01-03 02:45:25.373537: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-01-03 02:45:25.447623: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: graph_to_optimize
2020-01-03 02:45:25.447671: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 177 nodes (-78), 225 edges (-85), time = 50.718ms.
2020-01-03 02:45:25.447678: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 177 nodes (0), 225 edges (0), time = 3.89ms.
2020-01-03 02:45:25.447702: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: sequential_14_bidirectional_6_backward_lstm_56_while_body_3360_frozen
2020-01-03 02:45:25.447707: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 71 nodes (-1), 96 edges (0), time = 2.225ms.
2020-01-03 02:45:25.447730: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 71 nodes (0), 96 edges (0), time = 1.116ms.
2020-01-03 02:45:25.447752: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: sequential_14_bidirectional_6_forward_lstm_56_while_body_3164_frozen
2020-01-03 02:45:25.447757: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 71 nodes (-1), 96 edges (0), time = 2.118ms.
2020-01-03 02:45:25.447761: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 71 nodes (0), 96 edges (0), time = 1.179ms.
2020-01-03 02:45:25.447766: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: sequential_14_bidirectional_6_backward_lstm_56_while_cond_3359_frozen
2020-01-03 02:45:25.447771: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 17 nodes (0), 4 edges (0), time = 0.339ms.
2020-01-03 02:45:25.447795: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 17 nodes (0), 4 edges (0), time = 0.214ms.
2020-01-03 02:45:25.447800: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: sequential_14_bidirectional_6_forward_lstm_56_while_cond_3163_frozen
2020-01-03 02:45:25.447817: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 17 nodes (0), 4 edges (0), time = 0.311ms.
2020-01-03 02:45:25.447822: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 17 nodes (0), 4 edges (0), time = 0.213ms.
Traceback (most recent call last):
  File ""convert.py"", line 15, in <module>
    tflite_model = converter.convert()
  File ""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"", line 490, in convert
    **converter_kwargs)
  File ""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 476, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 215, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
WARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .
2020-01-03 02:45:27.257206: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:108] Ignored output_format.
2020-01-03 02:45:27.257267: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:114] Ignored drop_control_dependency.
2020-01-03 02:45:27.478616: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-03 02:45:27.500564: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 2496000000 Hz
2020-01-03 02:45:27.500991: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555ae8caec30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-01-03 02:45:27.501033: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-01-03 02:45:27.502780: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-01-03 02:45:27.505700: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-01-03 02:45:27.505725: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: enigma
2020-01-03 02:45:27.505732: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: enigma
2020-01-03 02:45:27.505775: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.50.0
2020-01-03 02:45:27.505796: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.50.0
2020-01-03 02:45:27.505801: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.50.0
loc(callsite(""sequential_14/bidirectional_6/backward_lstm_56/ReverseV2_1""(""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"":853:0) at callsite(""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"":947:0 at callsite(""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"":409:0 at ""convert.py"":9:0)))): error: 'tfl.reverse_v2' op operand #0 must be tensor of 32-bit float or 16-bit integer or 32-bit integer or 64-bit integer values, but got 'tensor<50x1x1xi1>'
Traceback (most recent call last):
  File ""/home/amish/anaconda3/bin/toco_from_protos"", line 8, in <module>
    sys.exit(main())
  File ""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/amish/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/amish/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 56, in execute
    enable_mlir_converter)
Exception: /home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py:853:9: error: 'tfl.reverse_v2' op operand #0 must be tensor of 32-bit float or 16-bit integer or 32-bit integer or 64-bit integer values, but got 'tensor<50x1x1xi1>'
        self._initialize(args, kwargs, add_initializers_to=initializers)
        ^
/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py:947:5: note: called from
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
    ^
/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py:409:5: note: called from
    concrete_func = func.get_concrete_function()
    ^
convert.py:9:1: note: called from
converter = tf.lite.TFLiteConverter.from_keras_model(model)

```

**This issue caused due to Bidirectional Wrapper. Also same error occurs when ```go_backwards=True``` in the LSTM layer**.

Please tell a workaround(if any) for this so that I can temporarily fix this. "
35550,Deprecation Warnings just from trying to see packages in IPython,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Redhat 7.6**
- TensorFlow installed from (source or binary): from `conda install keras`
- TensorFlow version (use command below): `1.15.0` 
- Python version: `3.7.5`
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: Using CPU version, so N/A

**Describe the current behavior**
While I was in the IPython, I did a standard `import tensorflow.` and then pressed TAB, to see what packages were there. 

After a second I was given an immediate deluge of warning messages...before I even pressed ENTER or tried to import anything. After the crazy deluge I got prompted to press ENTER 7 times for something...after which it finally gave me my IPython line back and I was able to run away from whatever terrible thing I had just done.

Funnily enough, if I'm in the same IPython session and try and do this again, I get no warnings, and I can browse package names like normal. If I restart the session I have to go through it all over again.

**Describe the expected behavior**
I expected none of that to happen. The only thing I expected to happen is a list to appear of top-level modules in `tensorflow`, like what happens for every other library. I hadn't even imported anything!

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

`import tensorflow.` and then press TAB in an IPython terminal.

**Other info / logs**
If it helps, I'm using an Anaconda environment. I know this isn't the latest tensorflow, but I'm using the version that came with my Keras install. 

Here's the full dump it gave me. It's a doozy...
```
WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.COMPILER_VERSION is deprecated. Please use tf.version.COMPILER_VERSION instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.CXX11_ABI_FLAG is deprecated. Please use tf.sysconfig.CXX11_ABI_FLAG instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.ConditionalAccumulator is deprecated. Please use tf.compat.v1.ConditionalAccumulator instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.ConditionalAccumulatorBase is deprecated. Please use tf.compat.v1.ConditionalAccumulatorBase instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.Dimension is deprecated. Please use tf.compat.v1.Dimension instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.Event is deprecated. Please use tf.compat.v1.Event instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.FIFOQueue is deprecated. Please use tf.queue.FIFOQueue instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.FixedLenSequenceFeature is deprecated. Please use tf.io.FixedLenSequenceFeature instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.GIT_VERSION is deprecated. Please use tf.version.GIT_VERSION instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.GRAPH_DEF_VERSION is deprecated. Please use tf.version.GRAPH_DEF_VERSION instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.GRAPH_DEF_VERSION_MIN_CONSUMER is deprecated. Please use tf.version.GRAPH_DEF_VERSION_MIN_CONSUMER instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.GRAPH_DEF_VERSION_MIN_PRODUCER is deprecated. Please use tf.version.GRAPH_DEF_VERSION_MIN_PRODUCER instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.GraphOptions is deprecated. Please use tf.compat.v1.GraphOptions instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.HistogramProto is deprecated. Please use tf.compat.v1.HistogramProto instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.LogMessage is deprecated. Please use tf.compat.v1.LogMessage instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.MONOLITHIC_BUILD is deprecated. Please use tf.sysconfig.MONOLITHIC_BUILD instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.MetaGraphDef is deprecated. Please use tf.compat.v1.MetaGraphDef instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.NameAttrList is deprecated. Please use tf.compat.v1.NameAttrList instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.NoGradient is deprecated. Please use tf.no_gradient instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.NodeDef is deprecated. Please use tf.compat.v1.NodeDef instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.NotDifferentiable is deprecated. Please use tf.no_gradient instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.OpError is deprecated. Please use tf.errors.OpError instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.PaddingFIFOQueue is deprecated. Please use tf.queue.PaddingFIFOQueue instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.PriorityQueue is deprecated. Please use tf.queue.PriorityQueue instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.QUANTIZED_DTYPES is deprecated. Please use tf.dtypes.QUANTIZED_DTYPES instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.QueueBase is deprecated. Please use tf.queue.QueueBase instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.RandomShuffleQueue is deprecated. Please use tf.queue.RandomShuffleQueue instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.ReaderBase is deprecated. Please use tf.compat.v1.ReaderBase instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.RunMetadata is deprecated. Please use tf.compat.v1.RunMetadata instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.RunOptions is deprecated. Please use tf.compat.v1.RunOptions instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.SessionLog is deprecated. Please use tf.compat.v1.SessionLog instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.SparseConditionalAccumulator is deprecated. Please use tf.compat.v1.SparseConditionalAccumulator instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.SparseFeature is deprecated. Please use tf.io.SparseFeature instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.SparseTensorValue is deprecated. Please use tf.compat.v1.SparseTensorValue instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.SummaryMetadata is deprecated. Please use tf.compat.v1.SummaryMetadata instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.TensorInfo is deprecated. Please use tf.compat.v1.TensorInfo instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.VERSION is deprecated. Please use tf.version.VERSION instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.VariableScope is deprecated. Please use tf.compat.v1.VariableScope instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.accumulate_n is deprecated. Please use tf.math.accumulate_n instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.add_check_numerics_ops is deprecated. Please use tf.compat.v1.add_check_numerics_ops instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.add_to_collections is deprecated. Please use tf.compat.v1.add_to_collections instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.angle is deprecated. Please use tf.math.angle instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.arg_max is deprecated. Please use tf.argmax instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.arg_min is deprecated. Please use tf.argmin instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_greater is deprecated. Please use tf.compat.v1.assert_greater instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_greater_equal is deprecated. Please use tf.compat.v1.assert_greater_equal instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_integer is deprecated. Please use tf.compat.v1.assert_integer instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_less is deprecated. Please use tf.compat.v1.assert_less instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_near is deprecated. Please use tf.compat.v1.assert_near instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_negative is deprecated. Please use tf.compat.v1.assert_negative instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_non_negative is deprecated. Please use tf.compat.v1.assert_non_negative instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_non_positive is deprecated. Please use tf.compat.v1.assert_non_positive instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_none_equal is deprecated. Please use tf.compat.v1.assert_none_equal instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_positive is deprecated. Please use tf.compat.v1.assert_positive instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_proper_iterable is deprecated. Please use tf.debugging.assert_proper_iterable instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_rank is deprecated. Please use tf.compat.v1.assert_rank instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_rank_at_least is deprecated. Please use tf.compat.v1.assert_rank_at_least instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_rank_in is deprecated. Please use tf.compat.v1.assert_rank_in instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_same_float_dtype is deprecated. Please use tf.debugging.assert_same_float_dtype instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_scalar is deprecated. Please use tf.compat.v1.assert_scalar instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_type is deprecated. Please use tf.compat.v1.assert_type instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assert_variables_initialized is deprecated. Please use tf.compat.v1.assert_variables_initialized instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.assign_sub is deprecated. Please use tf.compat.v1.assign_sub instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.batch_to_space_nd is deprecated. Please use tf.batch_to_space instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.betainc is deprecated. Please use tf.math.betainc instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.bincount is deprecated. Please use tf.math.bincount instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.ceil is deprecated. Please use tf.math.ceil instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.check_numerics is deprecated. Please use tf.debugging.check_numerics instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.cholesky is deprecated. Please use tf.linalg.cholesky instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.cholesky_solve is deprecated. Please use tf.linalg.cholesky_solve instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.confusion_matrix is deprecated. Please use tf.math.confusion_matrix instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.conj is deprecated. Please use tf.math.conj instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.container is deprecated. Please use tf.compat.v1.container instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.convert_to_tensor_or_indexed_slices is deprecated. Please use tf.compat.v1.convert_to_tensor_or_indexed_slices instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.convert_to_tensor_or_sparse_tensor is deprecated. Please use tf.compat.v1.convert_to_tensor_or_sparse_tensor instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.cross is deprecated. Please use tf.linalg.cross instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.cumprod is deprecated. Please use tf.math.cumprod instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.decode_base64 is deprecated. Please use tf.io.decode_base64 instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.decode_compressed is deprecated. Please use tf.io.decode_compressed instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.decode_csv is deprecated. Please use tf.io.decode_csv instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.decode_json_example is deprecated. Please use tf.io.decode_json_example instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.delete_session_tensor is deprecated. Please use tf.compat.v1.delete_session_tensor instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.depth_to_space is deprecated. Please use tf.compat.v1.depth_to_space instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.dequantize is deprecated. Please use tf.quantization.dequantize instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.deserialize_many_sparse is deprecated. Please use tf.io.deserialize_many_sparse instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.diag is deprecated. Please use tf.linalg.tensor_diag instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.digamma is deprecated. Please use tf.math.digamma instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.dimension_at_index is deprecated. Please use tf.compat.dimension_at_index instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.dimension_value is deprecated. Please use tf.compat.dimension_value instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.disable_control_flow_v2 is deprecated. Please use tf.compat.v1.disable_control_flow_v2 instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.disable_tensor_equality is deprecated. Please use tf.compat.v1.disable_tensor_equality instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.disable_v2_tensorshape is deprecated. Please use tf.compat.v1.disable_v2_tensorshape instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.div_no_nan is deprecated. Please use tf.math.divide_no_nan instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.enable_control_flow_v2 is deprecated. Please use tf.compat.v1.enable_control_flow_v2 instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.enable_resource_variables is deprecated. Please use tf.compat.v1.enable_resource_variables instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.enable_tensor_equality is deprecated. Please use tf.compat.v1.enable_tensor_equality instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.enable_v2_behavior is deprecated. Please use tf.compat.v1.enable_v2_behavior instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.enable_v2_tensorshape is deprecated. Please use tf.compat.v1.enable_v2_tensorshape instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.encode_base64 is deprecated. Please use tf.io.encode_base64 instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.erf is deprecated. Please use tf.math.erf instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.erfc is deprecated. Please use tf.math.erfc instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.expm1 is deprecated. Please use tf.math.expm1 instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.fake_quant_with_min_max_args is deprecated. Please use tf.quantization.fake_quant_with_min_max_args instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.fake_quant_with_min_max_args_gradient is deprecated. Please use tf.quantization.fake_quant_with_min_max_args_gradient instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.fake_quant_with_min_max_vars is deprecated. Please use tf.quantization.fake_quant_with_min_max_vars instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.fake_quant_with_min_max_vars_gradient is deprecated. Please use tf.quantization.fake_quant_with_min_max_vars_gradient instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.fake_quant_with_min_max_vars_per_channel is deprecated. Please use tf.quantization.fake_quant_with_min_max_vars_per_channel instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.fake_quant_with_min_max_vars_per_channel_gradient is deprecated. Please use tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.fft is deprecated. Please use tf.signal.fft instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.fft2d is deprecated. Please use tf.signal.fft2d instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.fft3d is deprecated. Please use tf.signal.fft3d instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.fixed_size_partitioner is deprecated. Please use tf.compat.v1.fixed_size_partitioner instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.floor_div is deprecated. Please use tf.math.floordiv instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.floordiv is deprecated. Please use tf.math.floordiv instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.floormod is deprecated. Please use tf.math.floormod instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.get_collection_ref is deprecated. Please use tf.compat.v1.get_collection_ref instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.get_local_variable is deprecated. Please use tf.compat.v1.get_local_variable instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.get_seed is deprecated. Please use tf.compat.v1.get_seed instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.get_session_handle is deprecated. Please use tf.compat.v1.get_session_handle instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.get_session_tensor is deprecated. Please use tf.compat.v1.get_session_tensor instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.global_norm is deprecated. Please use tf.linalg.global_norm instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.ifft is deprecated. Please use tf.signal.ifft instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.ifft2d is deprecated. Please use tf.signal.ifft2d instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.ifft3d is deprecated. Please use tf.signal.ifft3d instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.igamma is deprecated. Please use tf.math.igamma instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.igammac is deprecated. Please use tf.math.igammac instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.imag is deprecated. Please use tf.math.imag instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.invert_permutation is deprecated. Please use tf.math.invert_permutation instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.is_finite is deprecated. Please use tf.math.is_finite instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.is_inf is deprecated. Please use tf.math.is_inf instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.is_non_decreasing is deprecated. Please use tf.math.is_non_decreasing instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.is_numeric_tensor is deprecated. Please use tf.debugging.is_numeric_tensor instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.is_strictly_increasing is deprecated. Please use tf.math.is_strictly_increasing instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.lbeta is deprecated. Please use tf.math.lbeta instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.lgamma is deprecated. Please use tf.math.lgamma instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.lin_space is deprecated. Please use tf.linspace instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.local_variables is deprecated. Please use tf.compat.v1.local_variables instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.log1p is deprecated. Please use tf.math.log1p instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.log_sigmoid is deprecated. Please use tf.math.log_sigmoid instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.logical_xor is deprecated. Please use tf.math.logical_xor instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.make_template is deprecated. Please use tf.compat.v1.make_template instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.make_tensor_proto is deprecated. Please use tf.compat.v1.make_tensor_proto instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.matching_files is deprecated. Please use tf.io.matching_files instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.matrix_band_part is deprecated. Please use tf.linalg.band_part instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.matrix_determinant is deprecated. Please use tf.linalg.det instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.matrix_diag is deprecated. Please use tf.linalg.diag instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.matrix_diag_part is deprecated. Please use tf.linalg.diag_part instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.matrix_inverse is deprecated. Please use tf.linalg.inv instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.matrix_set_diag is deprecated. Please use tf.linalg.set_diag instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.matrix_solve is deprecated. Please use tf.linalg.solve instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.matrix_solve_ls is deprecated. Please use tf.linalg.lstsq instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.matrix_transpose is deprecated. Please use tf.linalg.matrix_transpose instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.matrix_triangular_solve is deprecated. Please use tf.linalg.triangular_solve instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.min_max_variable_partitioner is deprecated. Please use tf.compat.v1.min_max_variable_partitioner instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.mod is deprecated. Please use tf.math.mod instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.model_variables is deprecated. Please use tf.compat.v1.model_variables instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.moving_average_variables is deprecated. Please use tf.compat.v1.moving_average_variables instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.no_regularizer is deprecated. Please use tf.compat.v1.no_regularizer instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.op_scope is deprecated. Please use tf.compat.v1.op_scope instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.parse_example is deprecated. Please use tf.io.parse_example instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.parse_single_sequence_example is deprecated. Please use tf.io.parse_single_sequence_example instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.parse_tensor is deprecated. Please use tf.io.parse_tensor instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.polygamma is deprecated. Please use tf.math.polygamma instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.qr is deprecated. Please use tf.linalg.qr instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.quantize is deprecated. Please use tf.quantization.quantize instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.quantized_concat is deprecated. Please use tf.quantization.quantized_concat instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.random_gamma is deprecated. Please use tf.random.gamma instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.random_normal is deprecated. Please use tf.random.normal instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.random_poisson is deprecated. Please use tf.random.poisson instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.read_file is deprecated. Please use tf.io.read_file instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.real is deprecated. Please use tf.math.real instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.reciprocal is deprecated. Please use tf.math.reciprocal instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.regex_replace is deprecated. Please use tf.strings.regex_replace instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.report_uninitialized_variables is deprecated. Please use tf.compat.v1.report_uninitialized_variables instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.resource_variables_enabled is deprecated. Please use tf.compat.v1.resource_variables_enabled instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.reverse_v2 is deprecated. Please use tf.reverse instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.rint is deprecated. Please use tf.math.rint instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.scatter_add is deprecated. Please use tf.compat.v1.scatter_add instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.scatter_div is deprecated. Please use tf.compat.v1.scatter_div instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.scatter_max is deprecated. Please use tf.compat.v1.scatter_max instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.scatter_min is deprecated. Please use tf.compat.v1.scatter_min instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.scatter_mul is deprecated. Please use tf.compat.v1.scatter_mul instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.scatter_nd_add is deprecated. Please use tf.compat.v1.scatter_nd_add instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.scatter_nd_sub is deprecated. Please use tf.compat.v1.scatter_nd_sub instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.scatter_nd_update is deprecated. Please use tf.compat.v1.scatter_nd_update instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.scatter_sub is deprecated. Please use tf.compat.v1.scatter_sub instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.scatter_update is deprecated. Please use tf.compat.v1.scatter_update instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.segment_max is deprecated. Please use tf.math.segment_max instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.segment_mean is deprecated. Please use tf.math.segment_mean instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.segment_min is deprecated. Please use tf.math.segment_min instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.segment_prod is deprecated. Please use tf.math.segment_prod instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.segment_sum is deprecated. Please use tf.math.segment_sum instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.self_adjoint_eig is deprecated. Please use tf.linalg.eigh instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.self_adjoint_eigvals is deprecated. Please use tf.linalg.eigvalsh instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.serialize_many_sparse is deprecated. Please use tf.compat.v1.serialize_many_sparse instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.serialize_sparse is deprecated. Please use tf.compat.v1.serialize_sparse instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.serialize_tensor is deprecated. Please use tf.io.serialize_tensor instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.space_to_batch_nd is deprecated. Please use tf.space_to_batch instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.space_to_depth is deprecated. Please use tf.compat.v1.space_to_depth instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_fill_empty_rows is deprecated. Please use tf.sparse.fill_empty_rows instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_mask is deprecated. Please use tf.sparse.mask instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_matmul is deprecated. Please use tf.linalg.matmul instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_maximum is deprecated. Please use tf.sparse.maximum instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_minimum is deprecated. Please use tf.sparse.minimum instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_reorder is deprecated. Please use tf.sparse.reorder instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_reset_shape is deprecated. Please use tf.sparse.reset_shape instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_reshape is deprecated. Please use tf.sparse.reshape instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_segment_mean is deprecated. Please use tf.compat.v1.sparse_segment_mean instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_segment_sqrt_n is deprecated. Please use tf.compat.v1.sparse_segment_sqrt_n instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_segment_sum is deprecated. Please use tf.compat.v1.sparse_segment_sum instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_slice is deprecated. Please use tf.sparse.slice instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_softmax is deprecated. Please use tf.sparse.softmax instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_to_indicator is deprecated. Please use tf.sparse.to_indicator instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.sparse_transpose is deprecated. Please use tf.sparse.transpose instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.squared_difference is deprecated. Please use tf.math.squared_difference instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.string_join is deprecated. Please use tf.strings.join instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.string_strip is deprecated. Please use tf.strings.strip instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.string_to_hash_bucket is deprecated. Please use tf.strings.to_hash_bucket instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.string_to_hash_bucket_fast is deprecated. Please use tf.strings.to_hash_bucket_fast instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.string_to_hash_bucket_strong is deprecated. Please use tf.strings.to_hash_bucket_strong instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.string_to_number is deprecated. Please use tf.strings.to_number instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.svd is deprecated. Please use tf.linalg.svd instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.tensor_scatter_add is deprecated. Please use tf.tensor_scatter_nd_add instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.tensor_scatter_sub is deprecated. Please use tf.tensor_scatter_nd_sub instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.tensor_scatter_update is deprecated. Please use tf.tensor_scatter_nd_update instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.trace is deprecated. Please use tf.linalg.trace instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.unsorted_segment_max is deprecated. Please use tf.math.unsorted_segment_max instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.unsorted_segment_mean is deprecated. Please use tf.math.unsorted_segment_mean instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.unsorted_segment_min is deprecated. Please use tf.math.unsorted_segment_min instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.unsorted_segment_prod is deprecated. Please use tf.math.unsorted_segment_prod instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.unsorted_segment_sqrt_n is deprecated. Please use tf.math.unsorted_segment_sqrt_n instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.unsorted_segment_sum is deprecated. Please use tf.math.unsorted_segment_sum instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.variable_axis_size_partitioner is deprecated. Please use tf.compat.v1.variable_axis_size_partitioner instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.variable_op_scope is deprecated. Please use tf.compat.v1.variable_op_scope instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.verify_tensor_all_finite is deprecated. Please use tf.compat.v1.verify_tensor_all_finite instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.where_v2 is deprecated. Please use tf.compat.v2.where instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.wrap_function is deprecated. Please use tf.compat.v1.wrap_function instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.write_file is deprecated. Please use tf.io.write_file instead.

WARNING:tensorflow:From [/path/to/]anaconda3/envs/thermo/lib/python3.7/site-packages/IPython/core/completerlib.py:153: The name tf.zeta is deprecated. Please use tf.math.zeta instead.

Press ENTER to continue...                                                                               
Press ENTER to continue...                                                                               
Press ENTER to continue...                                                                               
Press ENTER to continue...                                                                               
Press ENTER to continue...                                                                               
Press ENTER to continue...                                                                               
Press ENTER to continue...                                                                               
Press ENTER to continue... 
```
"
35547,“Cloud TPU” console spam on every TensorFlow import,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux (like Debian)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `v1.12.1-21487-g2e8d5e5 2.1.0-dev20200102`
- Python version: Python 3.7.5rc1
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Importing TensorFlow prints an unnecessary and unhelpful warning:

> WARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .

**Describe the expected behavior**

Importing TensorFlow should not print any messages about Cloud TPUs.
This is a normal desktop installation that doesn’t have anything to do
with TPUs, and doesn’t need them.

**Code to reproduce the issue**

```shell
python -c 'import tensorflow' 2>&1 | diff -u /dev/null -
```

**Other info / logs**

Likely introduced by 5364121e858b.
"
35545,Signal module import,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.8

**Describe the current behavior**
The `signal` module is not found  when doing `from tensorflow.signal`.
I have this error:
```
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-1-89a7b2fbc883> in <module>
----> 1 from tensorflow.signal import fft2d

ModuleNotFoundError: No module named 'tensorflow.signal'
```

**Describe the expected behavior**
I would like `from tensorflow.signal` to work.

**Code to reproduce the issue**
```python
from tensorflow.signal import fft2d, ifft2d
```

**Other info / logs**
In version 1.14 this was working.
Also I can still do the following:
```python
import tensorflow as tf
fft2d = tf.signal.fft2d
ifft2d = tf.signal.ifft2d
```
 but it's obviously not very handy.

I also had opened an [SO question](https://stackoverflow.com/questions/59116928/how-do-i-import-the-fft2d-in-tensorflow-2-0-0), but it didn't get a lot of attention."
35543,Can anyone build a optimized tensorflow-gpu-v2.0 based on cuda10.0 and cudnn7.6(centos7) for me? ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Users in China can not download third-party tools from GitHub. It's so frustrating. Can some kind person help me compile an optimized version of TensorFlow-GPU-v2.0. THKS!

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35542,Tf 2.0 training performance issue,"**System information**
- Custom code for training ResNet50
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): pip install (precompiled binary, stable version)
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.0.130
- GPU model and memory: RTX 2060 (8GB)
- CPU: AMD Ryzen 3200

I am seeing slow training in order of 41.4%  in TF2.0 because of ~50% CPU and GPU utilization. Most likely the issue seems to be in memory transfer between CPU and GPU.  Other tf performance issues are closed without any satisfactory answer hence I have written standalone code in TF and pytorch for comparison. Please let me know how I can improve performance of my tf training code given below from 224ms/step to 133ms/step. This difference is almost doubling days of training time for the large model that I am training. 

-------TF code Start ------------
```python
import tensorflow as tf
import numpy as np
physical_devices = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)
#tf.compat.v1.disable_eager_execution()

class Lossfunc(tf.keras.losses.Loss):
    def __init__(self,
                 reduction=tf.keras.losses.Reduction.AUTO,
                    name = ""Lossfunc""):
        super(Lossfunc, self).__init__(reduction=reduction,
                                                           name=name)

    def call(self, target, output):
        #target = NoneCHW
        loss = tf.keras.losses.MSE(target, output)
        return loss

#channel = 'channels_last'
#IMG_SHAPE = tuple((256,256,3))
#output_shape = tuple((8,8,2048))
channel = 'channels_first'
IMG_SHAPE = tuple((3, 256,256))
output_shape = tuple((2048,8,8))
tf.keras.backend.set_image_data_format(channel)

def data_gen():
    input = np.random.normal(0, 0.1, IMG_SHAPE)
    y = np.random.normal(0.5, 0.1, output_shape)
    yield input, y

def ResNet():
      backbone = tf.keras.applications.ResNet50(
                                              input_shape=IMG_SHAPE,
                                              include_top=False,
                                              #weights='imagenet')
                                              weights=None)

      final_output = backbone.output
      model = tf.keras.Model(inputs=backbone.input, outputs=final_output)
      return model

def main():
    # cudnn related setting ???
    model = ResNet()
    model.summary()

    train_loader = tf.data.Dataset.from_generator(
                        data_gen,
                        output_types=(tf.float32, tf.float32),
                        output_shapes=((IMG_SHAPE),
                                       (output_shape)
                        )
                    )
    train_loader = train_loader.repeat().batch(16).prefetch(tf.data.experimental.AUTOTUNE)
    criterion = Lossfunc()
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    model.compile(loss=criterion, optimizer=optimizer)
    history = model.fit(train_loader,
                                  initial_epoch = 0,
                                  epochs=10,
                                  shuffle=False,
                                  steps_per_epoch=100,
                                  use_multiprocessing=True,
                                  workers=4)

if __name__ == '__main__':
        main()
```
Output:
```
100/100 [==============================] - 31s 306ms/step - loss: 0.1300
Epoch 2/10
100/100 [==============================] - 23s 226ms/step - loss: 0.0171
Epoch 3/10
100/100 [==============================] - 22s 224ms/step - loss: 0.0138
Epoch 4/10
100/100 [==============================] - 23s 227ms/step - loss: 0.0123
Epoch 5/10
100/100 [==============================] - 23s 227ms/step - loss: 0.0121
```
CPU utilization: ~50% (None of the CPU thread is more than 60% utilized at anytime)
GPU utilization: 56%

---Pytorch code Start---
```python
import torch
import torchvision.models as models
import torch.backends.cudnn as cudnn
import torch.nn as nn
from torch.utils.data import Dataset
import numpy as np
import time

# cudnn related setting
cudnn.benchmark = True
torch.backends.cudnn.deterministic = False
torch.backends.cudnn.enabled = True

class Lossfunc(nn.Module):
    def __init__(self):
        super(Lossfunc, self).__init__()
        self.criterion = nn.MSELoss(reduction='mean')
    def forward(self, output, target):
        loss = self.criterion(output, target)
        return loss

IMG_SHAPE = tuple((3, 256,256))
output_shape = tuple((2048,8,8))

class GenData(Dataset):
    def __init__(self ):
        self.db = None

    def __getitem__(self, idx):
        input = np.float32(np.random.normal(0, 0.1, IMG_SHAPE))
        y = np.float32(np.random.normal(0.5, 0.1, output_shape))
        return input, y

    def __len__(self,):
        return 1600

class ResNet(nn.Module):
    def  __init__(self):
        super(ResNet, self).__init__()
        backbone = models.resnet50(pretrained=True)
        self.backbone = nn.Sequential(*list(backbone.children())[:-2])

    def forward(self, input):
        x = self.backbone(input)
        return x

def main():
    # cudnn related setting

    model = ResNet()
    model = model.cuda()
    print(model)

    train_dataset = GenData()
    train_loader = torch.utils.data.DataLoader(
                        train_dataset,
                        batch_size=16,
                        shuffle=False,
                        num_workers=4,
                        pin_memory=True
                    )

    criterion = Lossfunc()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    model.train()
    for epoch in range(10):
        start = time.time()
        for i, (input, y) in enumerate(train_loader):
            input = input.cuda()
            output = model(input)
            y = y.cuda(non_blocking=True)
            loss = criterion(output, y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        i +=1
        end = time.time() - start
        print(""{} Steps in {} sec @ {} msec/step"".format(i,end, (end/i)*1000  ))


if __name__ == '__main__':
        main()
```
OUtput:
```
100 iterations in 14.790956020355225 sec @ 147.90956020355225 msec/step
100 iterations in 13.290162563323975 sec @ 132.90162563323975 msec/step
100 iterations in 13.326005935668945 sec @ 133.26005935668945 msec/step
100 iterations in 13.390087127685547 sec @ 133.90087127685547 msec/step
```
CPU Utilization: 60%
GPU Utilization: 98%

The difference in GPU % utilization is same as % time difference.


"
35540,Deadlock on recursive tf.function-decorated function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.10
- TensorFlow installed from (source or binary): binary, conda
- TensorFlow version (use command below): unknown 2.0.0
- Python version: 3.7

**Describe the current behavior**

Recursive calls with a `tf.function` decorated python function results in a deadlock. A minimal example is provided below.

Where it gets stuck: by recursively calling `_maybe_define_function`, which internally requires a lock (line 2118 in `tensorflow_core/python/eager.function.py`). This seems to deadlock when trying to create a graph function again invoking itself.

About use-cases: yes, there are. But yes, in principle, there are ""workarounds"". But I assume this is not in general intended to produce a deadlock anyway, should it? If so, I would rather propose a loud failure.

**Code to reproduce the issue**
```
@tf.function(autograph=False)
def func1(depth=0):
    if depth > 1:
        return depth
    else:
        return func1(depth + 1)

func1(0)
```"
35539,CUDA libraries in a self created docker container,"** Base System information**
- Linux Ubuntu 18.04
- nvidia-docker: Docker version 19.03.5, build 633a0ea838
- nvidia driver via: `sudo apt-get install nvidia-driver-418`
- CUDA/cuDNN version: 10.1
- GPU model and memory:
```
$ nvidia-smi 
Thu Jan  2 12:08:27 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 430.64       Driver Version: 430.64       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GT 710      Off  | 00000000:AF:00.0 N/A |                  N/A |
| 40%   46C    P0    N/A /  N/A |      0MiB /  2002MiB |     N/A      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce RTX 207...  Off  | 00000000:D8:00.0 Off |                  N/A |
| 34%   39C    P0     1W / 215W |      0MiB /  7982MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0                    Not Supported                                       |
+-----------------------------------------------------------------------------+
```


** Docker container information**
- Linux Ubuntu 18.04
- TensorFlow installed from: pip3
- TensorFlow version: 1.14.0
- Python version: 3.6.9
- CUDA/cuDNN version:
- GPU model and memory: nvidia-smi output (see above)
- my Dockerfile:
```
#https://www.tensorflow.org/install/gpu
FROM nvidia/cuda:10.0-base-ubuntu18.04

ENV PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/conda/bin:$PATH
RUN apt-get update && apt-get upgrade -y && apt-get install -y \
        nodejs \
        npm \
        python3-pip \
        wget \
        libmysqlclient-dev \
        python-dev
#RUN wget -nv https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && bash Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda && rm Miniconda3-latest-Lin$
RUN mkdir /etc/skel/notebooks
RUN npm install -g configurable-http-proxy && pip3 install \
        jupyterhub \
        jupyterhub-ldapauthenticator \
        jupyterlab \
        notebook \
        tensorflow-gpu
RUN pip3 install \
        folium \
        keras \
        matplotlib \
        mysql \
        mysql-connector \
        pandas \
        pymysql \
        seaborn \
        sklearn
VOLUME [""/home""]
RUN useradd -ms /bin/bash user -p ""$(openssl passwd -1 test)""
COPY ./jupyterhub_config.py /etc/jupyterhub/jupyterhub_config.py
COPY ./jupyterhub_cookie_secret /etc/jupyterhub/jupyterhub_cookie_secret
COPY ./jupyterhub.sqlite /etc/jupyterhub/jupyterhub.sqlite
#CMD [""/usr/local/bin/jupyterhub"", ""upgrade-db"", ""--db=sqlite:////etc/jupyterhub/jupyterhub.sqlite""]
EXPOSE 8000
CMD [""/usr/local/bin/jupyterhub"", ""-f"", ""/etc/jupyterhub/jupyterhub_config.py"", ""--debug""]
```


**Describe the problem**

I created a Docker container based on the Dockerfile (see above) for a multiuser jupyterhub (with notebook and lab). The basics works fine, I can log in and use the desired python packages and running my projects. However, the container lacks the GPU capabiltites.

- container will be started by: `$ nvidia-docker run -d -p 8000:8000 --runtime=nvidia --restart unless-stopped --gpus all -v ~/myDocker/home:/home jupyterhub`
- `nvidia-smi` works within the container
- `device_lib.list_local_devices()` prints:
```[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 9710542890831123693
, name: ""/device:XLA_GPU:0""
device_type: ""XLA_GPU""
memory_limit: 17179869184
locality {
}
incarnation: 12724683280898329209
physical_device_desc: ""device: XLA_GPU device""
, name: ""/device:XLA_GPU:1""
device_type: ""XLA_GPU""
memory_limit: 17179869184
locality {
}
incarnation: 12608392157467121046
physical_device_desc: ""device: XLA_GPU device""
, name: ""/device:XLA_CPU:0""
device_type: ""XLA_CPU""
memory_limit: 17179869184
locality {
}
incarnation: 16188673672643731433
physical_device_desc: ""device: XLA_CPU device""
]
```

- `from keras import backend as K
K.tensorflow_backend._get_available_gpus()` prints an empty field
- `tf.test.is_gpu_available()` says `False`

I can see that tensorflow is looking for the following ones and cannot find them:
- libcublas.so.10.0
- libcufft.so.10.0
- libcurand.so.10.0
- libcusolver.so.10.0
- libcusparse.so.10.0
- libcudnn.so.7

It is strange, I never installed nvidia-smi within the container, I seems to be deployed by docker, but necessary parts are missing for tensorflow. However, if I install CUDA or/and nvidia-toolkits and other software parts in the container, tensorflow is reporting about incorpatible versions:

```
2020-01-01 20:50:11.429852: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-01-01 20:50:11.466847: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2020-01-01 20:50:11.468037: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination
2020-01-01 20:50:11.468085: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 9f9f93453aee
2020-01-01 20:50:11.468093: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 9f9f93453aee
2020-01-01 20:50:11.468236: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 440.33.1
2020-01-01 20:50:11.468265: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.64.0
2020-01-01 20:50:11.468285: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 430.64.0 does not match DSO version 440.33.1 -- cannot find working devices in this configuration
2020-01-01 20:50:11.489100: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz
2020-01-01 20:50:11.493082: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4c12f30 executing computations on platform Host. Devices:
2020-01-01 20:50:11.493143: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
```
I do not think that it is an tensorflow issue. It is moreover the question, how to create an own docker container with tensorflow-gpu with CUDA support. Is the docker base image the right one?"
35538,TFLite model output different probabilities and thus different classes in Android compared to the original h5 model..,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab / Android Studio
- TensorFlow installed from (source or binary): Source
- TensorFlow version (or github SHA if from source):  1.14


**Provide the text output from tflite_convert**
TFLite Model output probabilities on same input text:
```
[0.07323484, 0.021794094, 0.013985702, 0.013658127, 0.010268052]
```

H5 Model file output on same input text: 
```
[0.03437233, 0.02771266, 0.01885756, 0.01123235, 0.01093488]
```
However, in both cases the first probabilities belong to the same classes. The predicted classes in both cases are not meaningless but also they are not same, which should be the case.
"
35537,Confusion between layers.Activation('softmax'...) and activations.softmax(...),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.0.0
- Python version: 3.7.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0.1
- GPU model and memory: QuadroK620M

I am building a custom model, and trying to use mixed precision. Please take a look at the following two custom models
```
from __future__ import absolute_import, division, print_function, unicode_literals
import functools

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, activations


class SFM1(tf.keras.Model):
    def __init__(self):
        super(SFM1, self).__init__()
        self.output_layer = layers.Activation('softmax', dtype='float32')
    
    def call(self, inputs):
        return self.output_layer(inputs)

class SFM2(tf.keras.Model):
    def __init__(self):
        super(SFM2, self).__init__()
        
    
    def call(self, inputs):
        return activations.softmax(inputs, axis=1)
    
x = tf.random.uniform((2, 4, 64, 64, 64), dtype=tf.float32)
sfm1 = SFM1()
y1 = sfm1(x)

sfm2 = SFM2()
y2= sfm2(x)

tf.math.equal(y1, y2)
```
y1 and y2 differ, because layers.Activation(...) assumes axis=-1 by default.
The issue is, that in mixed presion doc [here](https://www.tensorflow.org/guide/keras/mixed_precision#training_the_model_with_a_custom_training_loop)
The dtype of the output layer shall be specified as tf.float32, but activations.softmax(...) cannot do it. If I use layers.Activation(...), the axis cannot be specified as 1 ( I am using channel_first data, and I am afraid that cannot change source data). So what can I do?
Shall I have to work around by 
``` tf.dtypes.cast(activations.softmax(inputs, axis=1), dtype=tf.float32)```"
35536,"there are some high level vulnerability in Tensorflow 1.15.0, do we have plan to upgrade it?","curl: 7.65.3
sqlite3: 3.28.0

"
35535,lite/micro: Incorrectly copy quantization information from serialized data,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): gcc/c++ 5.4.0
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
When micro allocator copies quantization information from serialized data in InitializeRuntimeTensor(), it copies 8 bytes for 'zero_point'. However, the size of 'zero_point' is 4 bytes, so the next member data('allocation_type') will be modified accidentally.

**Describe the expected behavior**
I think it should just copy 4 bytes for 'zero_point'.

**Code to reproduce the issue**
Any test case in tensorflow/lite/micro/examples/ can reproduce the issue.

**Other info / logs**
This is the fragment of bug code
```
  // This magic handles issues with little-endianness.
  for (unsigned int b = 0; b < sizeof(int64_t); ++b)
    *(reinterpret_cast<char*>(&result->params.zero_point) + b) =
        *(reinterpret_cast<const char*>(
          src_quantization->zero_point()->Data()) +
            b);
    result->params.zero_point =
      flatbuffers::EndianScalar(result->params.zero_point);
```
Screenshot below is a tensor before the code
![image](https://user-images.githubusercontent.com/11289375/71656049-b5562780-2d74-11ea-84ee-48cbe5b424b0.png)

Screenshot below is a tensor after the code
![image](https://user-images.githubusercontent.com/11289375/71656203-652b9500-2d75-11ea-8e35-8ff7810da4d1.png)

The quantization information should not modify the allocation_type. I think `sizeof(int64_t)` should be changed into `sizeof(result->params.zero_point)`
"
35534,Include file when building tensorflow 2.0 from source missing.,"When compiled, tensorflow2.0 outputs the libtensorflow_cc.so and libtensorflow_framework.so
Used: bazel build --config=opt  //tensorflow:libtensorflow_cc.so 
However there are no more genfiles where I was getting the include files to link to . In the tensorflow/cc/ops directory e.g. I am missing array_ops. There are many others missing. Before I could those from genfiles directory, now this folder is gone. Where can I get all the include files to compile my file with?
The alternative I think would be to use bazel build <my file>, however this takes too long to compile my file. I just want to use g++ to link with the above lib and the include files.
------------------------
OS: Linux Ubuntu 14.04 
Tensorflow 2.0 (master) installed from source
No Cuda
C++ API

"
35533,Bug: Model.fit VS GradientTape in tf2.0: GradientTape can't work,"### 1. Network define
```
import pathlib
from concurrent.futures import ProcessPoolExecutor

import gensim
import numpy as np
import tensorflow as tf
from absl import app
from absl import flags
from absl import logging
from tensorflow import keras as tfk
from tqdm import tqdm

from src.preprocessing.sequence import create_dataset
from src.preprocessing.text import JiebaTokenizer


class TextCNN(tfk.Model):

    LABEL_IDX_DICT = {""constellation"": 0, ""education"": 1, ""entertainment"": 2, ""fashion"": 3, ""finance"": 4, ""game"": 5,
                      ""house"": 6, ""land"": 7, ""lottery"": 8, ""political"": 9, ""social"": 10, ""sports"": 11, ""stock"": 12,
                      ""technology"": 13}
    IDX_LABEL_DICT = {idx: label for label, idx in LABEL_IDX_DICT.items()}

    def __init__(self, embeddings):
        super(TextCNN, self).__init__()
        self.embedding_layer = tfk.layers.Embedding(embeddings.shape[0], embeddings.shape[1], trainable=False,
                                                    embeddings_initializer=tf.initializers.Constant(embeddings))
        self.conv_layers = [tfk.layers.Conv1D(FLAGS.filters, FLAGS.kernel_size + idx * FLAGS.kernel_distance,
                                              strides=FLAGS.conv_strides, padding=FLAGS.conv_padding)
                            for idx in range(FLAGS.conv_pool_num)]
        self.pool_layers = [tfk.layers.MaxPool1D(pool_size=FLAGS.pool_size, strides=FLAGS.pool_strides,
                                                 padding=FLAGS.pool_padding)
                            for idx in range(FLAGS.conv_pool_num)]
        self.dense1_layer = tfk.layers.Dense(FLAGS.dense_units, activation=FLAGS.activation)
        self.batchnorm_layer = tfk.layers.BatchNormalization()
        self.dense2_layer = tfk.layers.Dense(len(self.LABEL_IDX_DICT), activation=tfk.activations.softmax)
        self.dropout_layer = tfk.layers.Dropout(FLAGS.dropout_rate)

    def call(self, inputs, training=None):
        x = inputs
        x_embed = self.embedding_layer(x)
        assert FLAGS.conv_pool_num > 0, ValueError(""conv_pool_num must > 0"")
        encodings = [self.conv_layers[idx](x_embed) for idx in range(FLAGS.conv_pool_num)]
        encodings_pool = [self.pool_layers[idx](encodings[idx]) for idx in range(FLAGS.conv_pool_num)]
        encodings_concat = tf.concat(encodings_pool, axis=-1)
        encodings_concat = self.batchnorm_layer(encodings_concat, training=training)
        encodings_flatten = tfk.layers.Flatten()(encodings_concat)
        encodings_flatten = self.dropout_layer(encodings_flatten, training=training)
        hidden = self.dense1_layer(encodings_flatten)
        y_pred = self.dense2_layer(hidden)
        return y_pred
```
### 2. Train with model.fit
```
w2v = gensim.models.KeyedVectors.load_word2vec_format(FLAGS.embeddings_path)
words = list(w2v.vocab.keys())[:FLAGS.vocab_size]
tokenizer = JiebaTokenizer(words)
gpus = tf.config.experimental.list_physical_devices(""GPU"")
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
embeddings = np.zeros(shape=(FLAGS.vocab_size + 2, w2v.vector_size), dtype=np.float32)
for word, idx in tokenizer.word2idx_dict.items():
    if word in w2v.vocab:
        embeddings[idx] = w2v.word_vec(word)
x, y = TextCNN.preprocess_dataset(tokenizer, FLAGS.trainset_path)
trainset = create_dataset([x, y])
trainset = trainset.batch(FLAGS.batch_size).shuffle(FLAGS.buffer_size)
val_x, val_y = TextCNN.preprocess_dataset(tokenizer, FLAGS.valset_path)
valset = create_dataset([val_x, val_y])
valset = valset.batch(FLAGS.batch_size).shuffle(FLAGS.buffer_size)
model = TextCNN(embeddings)
optimizer = tf.optimizers.Adam(FLAGS.lr)
model.compile(optimizer=optimizer, loss=tf.losses.sparse_categorical_crossentropy, metrics=[""acc""])
model.fit(x=x, y=y, batch_size=FLAGS.batch_size, epochs=FLAGS.epochs, validation_data=(val_x, val_y))
```
### 3. Train with GradientTape
```
w2v = gensim.models.KeyedVectors.load_word2vec_format(FLAGS.embeddings_path)
words = list(w2v.vocab.keys())[:FLAGS.vocab_size]
tokenizer = JiebaTokenizer(words)
gpus = tf.config.experimental.list_physical_devices(""GPU"")
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
embeddings = np.zeros(shape=(FLAGS.vocab_size + 2, w2v.vector_size), dtype=np.float32)
for word, idx in tokenizer.word2idx_dict.items():
    if word in w2v.vocab:
        embeddings[idx] = w2v.word_vec(word)
x, y = TextCNN.preprocess_dataset(tokenizer, FLAGS.trainset_path)
trainset = create_dataset([x, y])
trainset = trainset.batch(FLAGS.batch_size).shuffle(FLAGS.buffer_size)
val_x, val_y = TextCNN.preprocess_dataset(tokenizer, FLAGS.valset_path)
valset = create_dataset([val_x, val_y])
valset = valset.batch(FLAGS.batch_size).shuffle(FLAGS.buffer_size)
model = TextCNN(embeddings)
optimizer = tf.optimizers.Adam(FLAGS.lr)
loss_object = tf.losses.SparseCategoricalCrossentropy()
train_acc = tf.metrics.SparseCategoricalAccuracy()
val_acc = tf.metrics.SparseCategoricalAccuracy()
train_loss = tf.metrics.Mean()
val_loss = tf.metrics.Mean()

@tf.function
def train_op(x, y):
    with tf.GradientTape() as tape:
        y_pred = model(x, training=True)
        loss = loss_object(y, y_pred)
        train_acc.update_state(y, y_pred)
        train_loss.update_state(loss)
        grads = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))

@tf.function
def val_op(x, y):
    y_pred = model(x, training=False)
    loss = loss_object(y, y_pred)
    val_acc.update_state(y, y_pred)
    val_loss.update_state(loss)

for epoch in range(FLAGS.epochs):
    tf.print(""Epoch {}/{}"".format(epoch + 1, FLAGS.epochs))
    bar = tfk.utils.Progbar(target=len(x), unit_name=""sample"")
    train_acc.reset_states()
    val_acc.reset_states()
    train_loss.reset_states()
    val_loss.reset_states()
    for batch_x, batch_y in trainset:
        train_op(batch_x, batch_y)
        bar.add(len(batch_y))
    for batch_x, batch_y in valset:
        val_op(batch_x, batch_y)
    template = ""loss: {:.4f}\nacc: {:.4f}\nval_loss: {:.4f}\nval_acc: {:.4f}""
    message = template.format(train_loss.result().numpy(), train_acc.result().numpy(),
                              val_loss.result().numpy(), val_acc.result().numpy())
    tf.print(message)
```
### 4. model.fit result
![image](https://user-images.githubusercontent.com/22722147/71655214-d452ba80-2d70-11ea-9ef8-294d90944ea1.png)
### 5. GradientTape result
![image](https://user-images.githubusercontent.com/22722147/71654550-3e696080-2d6d-11ea-896e-e72ade58974a.png)
![image](https://user-images.githubusercontent.com/22722147/71654563-5214c700-2d6d-11ea-880b-10744b899d6f.png)
![image](https://user-images.githubusercontent.com/22722147/71654570-5b059880-2d6d-11ea-896c-a132b383ee32.png)
### 6. Question
Like the result above, I use same parameter, but get very different result. Who can help me to figure out it. I have read some relevant tensorflow2.0 source code, but can't solve it.
"
35532,Fail to invoke tflite_runtime.interpreter,"I'm trying to run the tflite_runtime.interpreter

However when I go to run the interpreter I get the following error:
`RuntimeError: tensorflow/lite/kernels/mfcc.cc:131 params->dct_coefficient_count != mfcc_output.size() (40 != 0)Node number 1 (Mfcc) failed to invoke.`
"
35531,How to remove future warnings in tensorflow 2.0?,"Hello I am using tensorflow 2.0 and whenever I import tensorflow it gives me a future warning which is very long and annoying and does hides a big part of the output. I have tried many solutions from other issues but none of them work and I think almost all of them are for tf1.0. I have tried - 

```py
import logging
tf.get_logger().setLevel(logging.ERROR)
```
```py
import sys
original_stdout = sys.stdout
sys.stdout = None
```
```py
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
import tensorflow as tf
```
Also I have tried solutions from these issues - 
https://github.com/tensorflow/tensorflow/issues/8340
https://github.com/tensorflow/tensorflow/issues/7652
None of them work."
35529,model crashes in distributed strategy in tf2.1,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0-rc2
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1 / 7.6
- GPU model and memory: RTX titan 24GB


**Describe the current behavior**
when the model is to be trained on distributed setting, model crashes with the message of Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled. When it is set on single gpu everything works fine
**Describe the expected behavior**

**Code to reproduce the issue**
```
import tensorflow as tf
import tensorflow.keras as keras
import random
import os
os.environ[""CUDA_VISIBLE_DEVICES""]=""0,1""

class Model(keras.Model):
    def __init__(self):
        super(Model, self).__init__()
        self.emb = keras.layers.Embedding(51,100)
        self.layer = keras.layers.Dense(51)

    def call(self,x):
        x = self.emb(x)
        x = self.layer(x)
        return x


strategy = tf.distribute.MirroredStrategy()

data = [[i for i in range(random.randint(10,50))] for j in range(400)]


def iterator():
    for i in range(len(data)):
        yield data[i], data[i]


with strategy.scope():
    model = Model()
    optimizer = keras.optimizers.Adam()

dataset = tf.data.Dataset.from_generator(iterator, output_types=(tf.int64, tf.int64))
batchfier = dataset.padded_batch(4, padded_shapes=([None], [None]))
batchfier = strategy.experimental_distribute_dataset(batchfier)


@tf.function(input_signature=batchfier.element_spec)
def multi_gpu_step(x,y):
    def example_update_step(x, y):
        with tf.GradientTape() as tape:
            y_ = model(x)
            batch_loss = keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_, from_logits=True)
            losses = batch_loss / strategy.num_replicas_in_sync
        step_grad = tape.gradient(losses, model.trainable_variables)
        optimizer.apply_gradients(zip(step_grad, model.trainable_variables))
        return tf.reduce_mean(batch_loss,1)
    example_loss = strategy.experimental_run_v2(
        example_update_step, args=(x, y))
    losses_sum = strategy.reduce(
        tf.distribute.ReduceOp.SUM, example_loss, axis=0)
    return losses_sum


for x,y in batchfier:
    multi_gpu_step(x,y)
```
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**

> 2020-01-02 13:56:10.710246: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
2020-01-02 13:56:10.711123: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
Traceback (most recent call last):
  File ""/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2240, in _convert_inputs_to_signature
    value, dtype_hint=spec.dtype)
  File ""/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1314, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 317, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 258, in constant
    allow_broadcast=True)
  File ""/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 266, in _constant_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py"", line 96, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: Attempt to convert a value (PerReplica:{
  0 /job:localhost/replica:0/task:0/device:GPU:0: <tf.Tensor: shape=(2, 48), dtype=int64, numpy=
array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,
        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,
        32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42,  0,  0,  0,  0,  0],
       [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,
        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,
        32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])>,
  1 /job:localhost/replica:0/task:0/device:GPU:1: <tf.Tensor: shape=(2, 48), dtype=int64, numpy=
array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,
        16, 17, 18, 19, 20, 21, 22, 23, 24,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],
       [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,
        16, 17, 18, 19, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>
}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""multi_test_variable.py"", line 56, in <module>
    multi_gpu_step(x,y)
  File ""/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)
  File ""/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 632, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2362, in __call__
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File ""/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2661, in _maybe_define_function
    *args, **kwargs)
  File ""/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2185, in canonicalize_function_inputs
    self._flat_input_signature)
  File ""/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2246, in _convert_inputs_to_signature
    format_error_message(inputs, input_signature))
ValueError: When input_signature is provided, all inputs to the Python function must be convertible to tensors:
  inputs: (
    PerReplica:{
  0 /job:localhost/replica:0/task:0/device:GPU:0: tf.Tensor(
[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
  24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  0  0  0  0  0]
 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
  24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47]], shape=(2, 48), dtype=int64),
  1 /job:localhost/replica:0/task:0/device:GPU:1: tf.Tensor(
[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
  24  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20  0  0  0
   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]], shape=(2, 48), dtype=int64)
},
    PerReplica:{
  0 /job:localhost/replica:0/task:0/device:GPU:0: tf.Tensor(
[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
  24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  0  0  0  0  0]
 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
  24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47]], shape=(2, 48), dtype=int64),
  1 /job:localhost/replica:0/task:0/device:GPU:1: tf.Tensor(
[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
  24  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20  0  0  0
   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]], shape=(2, 48), dtype=int64)
})
  input_signature: (
    TensorSpec(shape=(None, None), dtype=tf.int64, name=None),
    TensorSpec(shape=(None, None), dtype=tf.int64, name=None))
"
35528,Why Tf Warns 'Model's Complie Policy is not the same as the dtype policy's loss scale',"I built up a custom model, then tried to adopt mixed precision as instructed by [here](https://www.tensorflow.org/guide/keras/mixed_precision#training_the_model_with_a_custom_training_loop)

```
from tensorflow.keras.mixed_precision import experimental as mixed_precision
curOpt = tf.keras.optimizers.Adam(learning_rate=1e-4)
curOpt = mixed_precision.LossScaleOptimizer(curOpt, loss_scale='dynamic')
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_policy(policy)
# query if it has been well set
print('Compute dtype: %s' % policy.compute_dtype)
print('Variable dtype: %s' % policy.variable_dtype)
modelInDim = (4, 64, 64, 64)
classNum = 2
mbSize = 2
TUNet = SimpleUNet(modelInDim, classNum)
TUNet.build_model(input_shape=(mbSize,)+modelInDim)
TUNet.summary()
```
Note that I set all Then as I call
```
TUNet.compile(loss=softDiceLoss, optimizer=curOpt)
```
The following warning pops out
```
TUNet.compile(loss=softDiceLoss, optimizer=curOpt)
WARNING:tensorflow:LossScale of LossScaleOptimizer passed to compile (DynamicLossScale(current_loss_scale=32768.0, num_good_steps=0, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0)) is not the same as the dtype policy's loss scale (DynamicLossScale(current_loss_scale=32768.0, num_good_steps=0, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0)). Because the dtype policy has a loss scale, you should pass an optimizer that is not wrapped with a LossScaleOptimizer,
```
What does it mean? The guide requires the optimizer to be wrapped in the way above. I am quite confused... 
FYI, the following is the model I built up.
```
class C3BR(tf.keras.Model):
    ''' 3D Convolution + Batch Normalisation + Relu '''
    def __init__(self, filterNum, kSize, strSize, padMode):
        super(C3BR, self).__init__()
        self.conv = layers.Conv3D(filters=filterNum, kernel_size=kSize, strides=strSize, padding=padMode, data_format='channels_first')
        self.BN = layers.BatchNormalization(axis=1)
    
    def call(self, inputs, ifTrain=False):
        x = self.conv(inputs)
        if ifTrain == True:
            x = self.BN(x)
        return activations.relu(x)

    def build_model(self, input_shape):
        ''' A work-around to define dimensions of signals through the NN'''
        self.build(input_shape)
        inputs = tf.keras.Input(shape=input_shape[1:])
        _ = self.call(inputs) 


class SimpleUNet(tf.keras.Model):
    """"""
    Input:
        inDim: (for initialisation) [modaility/channel, tensor dimensions]
        classNum: background included
        name: name for the net
        inputs: 5D tf tensor of [mbSize, modaility/channel, tensor dimensions]. Inputs must be organised into channel first order
        input_shape: a 1X5 tuple (mbSize, modaility/channel, tensor dimensions)
        ifTrain: True for training, and False for validation and testing
    Returns:
        outputs: 5D tf tensor of [mbSize, classNum, tensor dimensions]
    """"""
    def __init__(self, inDim, classNum):
        super(SimpleUNet, self).__init__()
        self.inDim = inDim
        self.classNum = classNum
        dimEnSt1End = np.array(inDim)[1:]-2-2
        dimEnSt2Ed = dimEnSt1End/2-2-2
        dimBridgeEnd = (dimEnSt2Ed/2-2-2)*2
        dimDEStd1End = (dimBridgeEnd-2-2)*2
        self.outDim = dimDEStd1End-2-2-2
        temp = ((dimEnSt2Ed - dimBridgeEnd)/2).astype('int32')
        crop3d1 = tuple(np.tile(temp, (2, 1)).T)
        temp = ((dimEnSt1End - dimDEStd1End)/2).astype('int32')
        crop3d2 = tuple(np.tile(temp, (2, 1)).T)

        self.en_st1_cbr1 = C3BR(32, 3, 1, 'valid')
        self.en_st1_cbr2 = C3BR(64, 3, 1, 'valid')
        self.en_st2_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')
        self.en_st2_cbr1 = C3BR(128, 3, 1, 'valid')
        self.en_st2_cbr2 = C3BR(128, 3, 1, 'valid')
        self.bridge_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')
        self.bridge_cbr1 = C3BR(256, 3, 1, 'valid')
        self.bridge_cbr2 = C3BR(256, 3, 1, 'valid')    
        self.bridge_tconv1 = layers.Conv3DTranspose(512, 2, strides=2, padding='valid', data_format='channels_first')
        self.de_3dcrop1 = layers.Cropping3D(crop3d1, data_format='channels_first')
        self.de_st1_concat = layers.Concatenate(axis=1)
        self.de_st1_cbr1 = C3BR(256, 3, 1, 'valid')
        self.de_st1_cbr2 = C3BR(128, 3, 1, 'valid')    
        self.de_st1_tconv1 = layers.Conv3DTranspose(128, 2, strides=2, padding='valid', data_format='channels_first')
        self.de_3dcrop2 = layers.Cropping3D(crop3d2, data_format='channels_first')
        self.de_st2_concat = layers.Concatenate(axis=1)
        self.de_st2_cbr1 = C3BR(64, 3, 1, 'valid')
        self.de_st2_cbr2 = C3BR(64, 3, 1, 'valid') 
        self.final_conv3D = layers.Conv3D(filters=self.classNum, kernel_size=3, strides=1, padding='valid', data_format='channels_first')                
        self.output_layer = layers.Activation('softmax', dtype='float32')
    
    #@tf.function
    # In fact, decorating it does not bring much benefit as it primarily contains large ops. that have been optimised by tf.
    def call(self, inputs, ifTrain=False):
        x0 = self.en_st1_cbr1(inputs, ifTrain)
        xEnSt1End = self.en_st1_cbr2(x0, ifTrain)
        x1 = self.en_st2_mp(xEnSt1End)
        x2 = self.en_st2_cbr1(x1, ifTrain)
        xEnSt2Ed = self.en_st2_cbr2(x2, ifTrain)
        x3 = self.bridge_mp(xEnSt2Ed)  
        x4 = self.bridge_cbr1(x3, ifTrain)
        x5 = self.bridge_cbr2(x4, ifTrain)    
        xBridgeEnd = self.bridge_tconv1(x5)
        xCrop1 = self.de_3dcrop1(xEnSt2Ed)
        x6 = self.de_st1_concat([xBridgeEnd, xCrop1])
        x7 = self.de_st1_cbr1(x6, ifTrain)
        x8 = self.de_st1_cbr2(x7, ifTrain)
        xDeSt1End = self.de_st1_tconv1(x8)
        xCrop2 = self.de_3dcrop2(xEnSt1End)
        x9 = self.de_st2_concat([xDeSt1End, xCrop2])
        x10 = self.de_st2_cbr1(x9, ifTrain)
        x11 = self.de_st2_cbr2(x10, ifTrain)
        x12 = self.final_conv3D(x11)
        outputs = self.output_layer(x12)
        
        return outputs
        
    def build_model(self, input_shape):
        ''' A work-around to permit one to see dimensions of signals through the NN. An imperative API does not support it
            by its own right
        '''
        self.build(input_shape)
        inputs = tf.keras.Input(shape=input_shape[1:])
        _ = self.call(inputs)
        
    def compute_output_shape(self):
        ''''Override this function if one expects to use the subclassed model in Kera's fit() method; Otherwise, it is optional.
        '''
        return tf.TensorShape(np.append(self.classNum, self.outDim))    

```"
35527,Python Crashes When Trying to Reload Weights from a Subclassed Model,"OS: Windows 7
Python Version: 3.7.5
IDE: Spyder 4.0.0

I built a subclassed model, which can run custom training loops properly, and I saved weights by
```
ep=0  
curTime = strftime(""%H-%M-%S-%d-%m-%Y"", gmtime())
fullPath =  os.path.join(e:, 'Weight_' + curTime + '_Ep' + str(ep).zfill(5))
os.makedirs(fullPath)
model.save_weights(os.path.join(fullPath, 'Weight'), save_format='tf')
```
Then I tried to load weights as per instructions of tf's in the last section of
(https://www.tensorflow.org/guide/keras/save_and_serialize#saving_subclassed_models)

by firstly rebuilding the model's architecture, compiling it, training one sample, then
```
path = r'...\TUNet_Test\Weight_03-01-46-02-01-2020_Ep00000'
model.load_weights(path)
```
I tried several times, but at this moment Python crashes by reporting
```
  Event Name:	APPCRASH
  Application Name:	pythonw.exe
  Application Version:	3.7.5150.1013
  Application Time Stamp:	5dbb41e3
  Error Module Name:	_pywrap_tensorflow_internal.pyd
```
What is saved in the folder is as follows:
![捕获](https://user-images.githubusercontent.com/51472988/71650720-fa7f4880-2ce5-11ea-9928-3451bf6c5b3f.JPG)

Did I make any mistake or is it an internal issue of tensorflow?"
35526,iOS Demo Pod install error,"i can search ""Tensorflow"", but can not install example : https://github.com/tensorflow/examples/tree/master/lite/examples/gesture_classification/ios

Error info:
![image](https://user-images.githubusercontent.com/3898789/71649968-f6871100-2d4d-11ea-9225-f2b90fb8991b.png)
![image](https://user-images.githubusercontent.com/3898789/71649972-fab32e80-2d4d-11ea-9878-d6fbca7548b5.png)
"
35525,Correction in the course material,"## Description of issue (what needs changing):

Do we still need the `steps_per_epochs` parameter while fitting the model to training set?
In the tensorflow tutorial(which is very similar to the MNIST tutorial of Intro to Deep Learning course ), there is no such parameter...

## URL(s) with the issue:

Udacity Course Notebook : https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l03c01_classifying_images_of_clothing.ipynb#scrollTo=S5Uhzt6vVIB2

-----------------------------------------------------------------
![Screenshot from 2020-01-01 23-27-10](https://user-images.githubusercontent.com/29497701/71644446-78dfe880-2cee-11ea-9da7-5c033d8a0592.png)
-----------------------------------------------------------------

Tensorflow Tutorial : 
https://www.tensorflow.org/tutorials/keras/classification/

-----------------------------------------------------------------
![Screenshot from 2020-01-01 23-26-50](https://user-images.githubusercontent.com/29497701/71644452-93b25d00-2cee-11ea-97af-70d0ece074d9.png)
-----------------------------------------------------------------

### Parameters defined

`steps_per_epoch` parameter in `model.fit` should be removed??

### Submit a pull request?

I'll submit a PR right away if this issue is relevant..."
35524,Suspected memory leak - when loading multiple models with tf.keras.models.load_model(),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
NA
- TensorFlow installed from (source or binary): binary wheel via PyPI
- TensorFlow version (use command below):
2.1.0-dev20191231 (v1.12.1-21412-g3a094e6 2.1.0-dev20191231)
- Python version:
- Bazel version (if compiling from source):
NA
- GCC/Compiler version (if compiling from source):
NA
- CUDA/cuDNN version:
CUDA 10.0 
- GPU model and memory:
V100 32 GB
You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I'm suspecting a CPU memory leak when loading multiple models.
When im running infinite loop that keeps loading the same model while using the same variable the memory (private bytes and working set) of the process keep increasing. At some points the working set seems to free some memory, but the trend is that the memory keeps on rising.
I used a simple model (attached).

This trend happens even though I call gc.collect() on every iteration and tf.keras.backend.clear_session().

the issue also happens in TF 2.0 (v2.0.0-rc2-26-g64c3d38 2.0.0).
for a specific model:
    running in TF 2.0 each iteration adds 16 MiB
    running in TF 2.1 each iteration adds 2 MiB

**Describe the expected behavior**

The memory shouldnt increase on each interation

**Code to reproduce the issue**
`
```
import os
import tensorflow as tf
import gc # garbage collector
import objgraph
from memory_profiler import profile

def mem_stat():
  objs = gc.get_objects()
  print(""total objects count"", len(objs))

@profile
def profile_own_model():
    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    # model.save('my_model')
    tf.keras.backend.clear_session()
    del model
    gc.collect()

@profile
def profile_load_model(path):
    model = tf.keras.models.load_model(model_path, compile=False)
    tf.keras.backend.clear_session()
    del model
    gc.collect()



model_path = f'/my_model.hd5'
print(""load model in loops:"")

c = 1
while True:
    print(""----------- iter"", c)
    profile_load_model(model_path)

    print(""mem stat after model creation:"")
    mem_stat()
    objgraph.show_growth(limit=30)
    c += 1
```
`

**Other info / logs**
![memory tf 2 1](https://user-images.githubusercontent.com/27951762/71644038-d9f5c500-2cca-11ea-96e3-b8aedd2e4efb.png)


Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

"
35523,Tensorflow 2.0 (GPU) throws CancelledErrors while fitting models ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04 LTS**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **No**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version (use command below): **r2.0**
- Python version: **3.6.9**
- Bazel version (if compiling from source): **0.26.0**
- GCC/Compiler version (if compiling from source): **7.4.0**
- CUDA/cuDNN version: **v10.1**
- GPU model and memory: **GeForce RTX 2070 Super (8GB)**

**Describe the current behavior**
The script throws a `CancelledError` when fitting the model (very 1st epoch)

**Describe the expected behavior**
`$ model.fit( ... )` is executed without throwing any errors.

**Code to reproduce the issue**
```
model = Sequential()
model.add(Embedding(vocab_size, dimensions,weights=[embedding_matrix], input_length=pad2, trainable=True))
model.add(GRU(128, return_sequences=True))
model.add(MaxPooling1D())
model.add(AveragePooling1D())
model.add(GRU(128))
model.add(Dense(len(set(outputs)), activation='sigmoid'))
checkpoint = ModelCheckpoint('model-%s-%s-%s-%s' %(setting, indicators, case, str(window_size)), verbose=1, monitor='val_acc', save_best_only=True, mode='auto')
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
print(model.summary())
# The summary is as expected
train, dev = padded_docs[:split], padded_docs[split:]
y_train, y_dev = y[:split], y[split:]
model.fit(train, y_train, epochs=epochs, verbose=1, callbacks=[checkpoint], batch_size=batch_size, validation_data=(dev, y_dev))
# This is where the script fails
```

**Other info / logs**
```
CancelledError                            Traceback (most recent call last)
<ipython-input-18-ba796bf236a0> in <module>
     70         train, dev = padded_docs[:split], padded_docs[split:]
     71         y_train, y_dev = y[:split], y[split:]
---> 72         model.fit(train, y_train, epochs=epochs, verbose=1, callbacks=[checkpoint], batch_size=batch_size, validation_data=(dev, y_dev))
     73         pred = model.predict(dev, batch_size=batch_size)
     74         model.save('./cross-fold-models/model-%s-%s-%s-%s-%s' %(setting, indicators, case, str(window_size), str(number)))

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    726         max_queue_size=max_queue_size,
    727         workers=workers,
--> 728         use_multiprocessing=use_multiprocessing)
    729 
    730   def evaluate(self,

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    322                 mode=ModeKeys.TRAIN,
    323                 training_context=training_context,
--> 324                 total_epochs=epochs)
    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    326 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    121         step=step, mode=mode, size=current_batch_size) as batch_logs:
    122       try:
--> 123         batch_outs = execution_function(iterator)
    124       except (StopIteration, errors.OutOfRangeError):
    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
     84     # `numpy` translates Tensors to values in Eager mode.
     85     return nest.map_structure(_non_none_constant_value,
---> 86                               distributed_function(input_fn))
     87 
     88   return execution_function

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--> 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    518         # Lifting succeeded, so variables are initialized and we can run the
    519         # stateless function.
--> 520         return self._stateless_fn(*args, **kwds)
    521     else:
    522       canon_args, canon_kwds = \

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
   1821     """"""Calls a graph function specialized to the inputs.""""""
   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1824 
   1825   @property

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
   1139          if isinstance(t, (ops.Tensor,
   1140                            resource_variable_ops.BaseResourceVariable))),
-> 1141         self.captured_inputs)
   1142 
   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1222     if executing_eagerly:
   1223       flat_outputs = forward_function.call(
-> 1224           ctx, args, cancellation_manager=cancellation_manager)
   1225     else:
   1226       gradient_name = self._delayed_rewrite_functions.register()

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    509               inputs=args,
    510               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 511               ctx=ctx)
    512         else:
    513           outputs = execute.execute_with_cancellation(

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

CancelledError:  [_Derived_]RecvAsync is cancelled.
	 [[{{node Adam/Adam/update/AssignSubVariableOp/_45}}]]
	 [[Reshape_14/_42]] [Op:__inference_distributed_function_18141]

Function call stack:
distributed_function
```"
35520,Operation 'EagerPyFunc' has no attr named '_XlaCompile'.,"I'm using tensorflow-gpu 2.1.0-rc1 on python 3.6.8. And I tried to write a loss function using spearman correlation and binary crossentropy. The final loss is the average of 30 targets. The basic idea is, if the target has only one unique value, binary crosessentropy will be used (to avoid nan in spearman), otherwise, spearman will be used. Here is the code:
```
import tensorflow as tf
from scipy.stats import spearmanr
from tensorflow.kears import backend as K

def custom_loss(y_true, y_pred):
    rhos = tf.constant(0, dtype='float32')
    for ind in range(30):
        a = tf.slice(y_true, [0, ind], [-1, 1])
        a = tf.reshape(a, [-1])
        b = tf.slice(y_pred, [0, ind], [-1, 1])
        b = tf.reshape(b, [-1])
        rhos = tf.cond(tf.equal(tf.argmax(a), tf.argmin(a)),
                lambda: tf.add(rhos, tf.metrics.binary_crossentropy(a, b)),
                lambda: tf.subtract(rhos,
                    tf.py_function(spearmanr, [a, b], Tout=tf.float32)))
    return tf.divide(rhos, tf.constant(30, 'float32'))
```
It gives the following error:
```
InvalidArgumentError                      Traceback (most recent call last)
/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py in get_attr(self, name)
   2325       with c_api_util.tf_buffer() as buf:
-> 2326         c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)
   2327         data = c_api.TF_GetBuffer(buf)

InvalidArgumentError: Operation 'EagerPyFunc' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
39 frames
ValueError: Operation 'EagerPyFunc' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py in __hash__(self)
    703     if (Tensor._USE_EQUALITY and executing_eagerly_outside_functions() and
    704         (g is None or g._building_function)):  # pylint: disable=protected-access
--> 705       raise TypeError(""Tensor is unhashable if Tensor equality is enabled. ""
    706                       ""Instead, use tensor.experimental_ref() as the key."")
    707     else:

TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.
```

Even if I decorate the function with ""@tf.function"". It still gives the same error.

Thanks for any suggestions in advance!
"
35517,"Building with OMP support compiles, but causes runtime error ""undefined symbol: omp_get_thread_num""","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.6 LTS (Xenial Xerus)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.15
- Python version: 3.5
- Installed using virtualenv? pip? conda?: from source, within a virtualenv. Bazel creates a pip wheels which is then installed.
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.12) 
- CUDA/cuDNN version: not used, CPU only
- GPU model and memory: not used, CPU only



**Describe the problem**
I am trying to build TF from source with support for `omp simd`. Compiling, creating and installing the pip wheel works fine using bazel. However, as soon as I try to import tensorflow, I get the error `ImportError: /work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: undefined symbol: omp_get_thread_num` 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
As described in https://www.tensorflow.org/install/source, I first run `bazel clean`. Then I run `./configure` and always select the default values:

```
(t2t) brix@cluster:/work/tensorflow$ ./configure 
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: Waiting for server process to terminate (waited 5 seconds, waiting at most 60)
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.26.1 installed.
Please specify the location of python. [Default is /work/venv/t2t/bin/python]: 

Found possible Python library paths:
  /work/venv/t2t/lib/python3.5/site-packages
Please input the desired Python library path to use.  Default is [/work/venv/t2t/lib/python3.5/site-packages]
Do you wish to build TensorFlow with XLA JIT support? [Y/n]: XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded.

Do you wish to build TensorFlow with MPI support? [y/N]: No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apache Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished

```

Afterwards, I run `bazel build -s --config=opt --copt=""-fopenmp"" --copt=""-lgomp"" //tensorflo
w/tools/pip_package:build_pip_package`. Note that I've added `--copt=""-fopenmp"" --copt=""-lgomp""` to ensure that `omp` is linked. This was suggested by the comment in this SO question: https://stackoverflow.com/questions/45667374/import-tensorlfow-failed-with-errors-undefined-symbol-omp-get-num-threads

Finally, I create the pip package using `./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg` and install it.

Unfortunately, when I then run `python -c ""import tensorflow""`, I get the following stack trace:

```
Traceback (most recent call last):
  File ""/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: undefined symbol: omp_get_thread_num

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/work/venv/t2t/lib/python3.5/site-packages/tensorflow/__init__.py"", line 99, in <module>
    from tensorflow_core import *
  File ""/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/work/venv/t2t/lib/python3.5/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/work/venv/t2t/lib/python3.5/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/usr/lib/python3.5/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: undefined symbol: omp_get_thread_num


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```
"
35516,request to tag the latest fully qualified commit on the master branch,"It would be very helpful to know the ""latest fully qualified"" commit on the master branch (i.e. the commit for which all internal TF checks have passed). The tip of the master branch is not that (i.e. the ""latest fully qualified"") commit, because we sometimes see commits/PRs getting rolled back because they failed the internal TF checks.

Having a tag/pointer to the ""latest fully qualified"" commit would helpful to us (and I suspect othesr in a similar boat to us), in at least a couple of ways                                                                                                         

1. We current have a nightly job for the ROCm Community Supported Build (CSB), which uses the tip of the master. If there was a ""bad"" commit (i.e. a commit that will, but has not yet been, rolled back due to internal check failure) since the last CSB run, then it has the potential to fail the ROCm CSB as well. When the ROCm CSB fails, we need to go through the process of triaging the failures, and ensuring that the failures were not regressions caused by ROCm.                                                 
                                                                                                                                                                                                                                                                 
  - If the nightly job uses the ""latest fully qualified"" commit, instead of the tip of master, then we could immediately rule out, ""bad"" upstream commits as the cause, saving us time and avoiding unnecessary ROCm CSB build failures.                             
                                                                                                                                                                                                                                                                 
   - In addition to the nightly job for ROCm CSB, we will soon be adding nightly jobs for running broader suite of tests for the ROCm build, and running them on the ""latest full qualified"" commit instead of the tip, would better serve our purpose (of detecting regressions in ROCm functionality)                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                 
                                                                                                                                                                                                                                                                 
2. We maintain a fork on the TF repo, and periodically (weekly) sync with the `master` branch in the upstream repo. We use the tip of `master` to sync, but using the ""latest fully qualified"" commit instead, would be a better option.                         

thanks"
35515,layers.Inputlayer does not show up in model.summary(),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
Tensorflow 2.1.0 on gpu on colab

**Describe the current behavior**
In model.summary(), layers.Inputlayer() does not show up as a layer.

**Code to reproduce the issue**
`model1 = tf.keras.models.Sequential([layers.InputLayer(input_shape=(784,)),
                                     layers.Dense(1500, activation=""relu""),
                                     layers.Dense(1000, activation=""relu""),
                                     layers.Dense(500, activation=""relu""),
                                     layers.Dense(1, activation=""sigmoid""),
                                     layers.Dense(500, activation=""relu""),
                                     layers.Dense(1000, activation=""relu""),
                                     layers.Dense(1500, activation=""relu""),
                                     layers.Dense(784),
])`
![Screenshot from 2019-12-31 22-39-50_modified](https://user-images.githubusercontent.com/43641071/71628625-d147b500-2c1e-11ea-873a-b8adf4259cfd.png)



**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

"
35514,pip installation .whl for aarch64?,"
Is there a release version for **aarch64**?
It seems there are only version of **x86_64**, **raspberry pi(armv7)**, etc. ?
"
35513,model.fit_generator() always uses eager execution instead of a compiled graph,"**System information**
OS: Debian 10, x64
GPU: GeForce GTX 1060 6GB
Python: 3.7.3
TensorFlow: 2.0.0 installed from source (git tag v2.0.0)
Bazel: 0.26.1
GCC: 6.5.0
CUDA: 10.1
cuDNN: 7.6.4

**Code to reproduce the issue**
```python
import numpy
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Layer

BATCH_SIZE = 32
N_BATCHES = 8
FEATURE_SIZE = 32

class SimpleDense(Layer):
    def __init__(self, units, **kwargs):
        super().__init__(**kwargs)
        self.units = units

    def build(self, input_shape):
        self.kernel = self.add_weight('kernel', shape=[input_shape[-1], self.units])

    def call(self, inputs, **kwargs):
        print('call() has been called with', inputs.__class__.__name__)
        return tf.matmul(inputs, self.kernel)

i = Input(shape=(FEATURE_SIZE,))
o = SimpleDense(units=FEATURE_SIZE)(i)
m = Model(i, o)
m.compile('sgd', 'mse')

print('*** Using fit()')
xy = numpy.zeros((N_BATCHES * BATCH_SIZE, FEATURE_SIZE))
m.fit(xy, xy, batch_size=BATCH_SIZE, verbose=0)

print('*** Using fit_generator()')

def gen():
    zeros = numpy.zeros((BATCH_SIZE, FEATURE_SIZE))
    for _ in range(N_BATCHES):
        yield zeros, zeros

m.fit_generator(gen(), steps_per_epoch=N_BATCHES, epochs=1, verbose=0)
```

**Describe the current behavior**
Output of the above code is:
```
call() has been called with Tensor
*** Using fit()
call() has been called with Tensor
call() has been called with Tensor
*** Using fit_generator()
call() has been called with EagerTensor
call() has been called with EagerTensor
call() has been called with EagerTensor
call() has been called with EagerTensor
call() has been called with EagerTensor
call() has been called with EagerTensor
call() has been called with EagerTensor
call() has been called with EagerTensor
```
When using `fit()` the model runs as a graph, as it should. When using `fit_generator()` the model is run with eager execution, causing `call()` to be invoked on every single batch.

**Describe the expected behavior**
`fit_generator()` should also run the model as a graph, for increased performance.

**Other observations**
* Using `tf.compat.v1.disable_eager_execution()` is an obvious workaround
* It's also possible to use `m.compile(..., experimental_run_tf_function=False)`, however that is very inconvenient because this flag does not persist across saving/reloading (and it's not possible to pass it again, since `compile()` is called by the loading code)
* Setting `m._experimental_run_tf_function = False` is an ugly hack, but works also when reloading the model.
* Setting `m.run_eagerly = False` does not help since it's already `False`.
* Decorating `SimpleDense.call()` with `@tf.function` also works, but that's inconvenient to do for all layers and it's not optimal since only a part of the entire model will run as graph. Also I'm not sure how to manually wrap the entire model with `@tf.function`."
35512,C++ Tensorflow is very slower than Python Tensorflow,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.14 and 1.13
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): VC 14 (VS 2015)
- CUDA/cuDNN version: 10.0/7.4
- GPU model and memory: RTX 2080

**Describe the current behavior**
I exported a .pb graph file. I use it in python and it took 55 milliseconds to process. I also use it in C++ (both pre-build versions and self-build from source) it took 750 milliseconds. Everything is same!

**Describe the expected behavior**
both python and C++ take 55 ms to process.

 
**Other info / logs**
I build C++ API with below command:
`bazel --output_user_root=""C:/bazel/our"" build -c opt --copt=""-mavx"" --config=opt --config=cuda //tensorflow:libtensorflow_cc.so` then rename .so to .dll and .so.ifso to .lib

in #3471 (https://github.com/tensorflow/tensorflow/issues/3471#issuecomment-234671300) mentioned to add `-c opt` flag with `--copt=-mavx`. but it didn't make any difference. Also I found tons of warning as: 
`cl : Command line warning D9002 : ignoring unknown option '-mavx'
`
"
35511,where can I find “hexagon_nn_skel.run” ?,"from ""tensorflow/lite/g3doc/performance/hexagon_delegate.md"" :
Run “hexagon_nn_skel.run” - Note: you will need to accept the license agreement. It should provide 3 different shared libraries “libhexagon_nn_skel.so”, “libhexagon_nn_skel_v65.so”, “libhexagon_nn_skel_v66.so”

but I can not find hexagon_nn_skel.run, where can I download  “libhexagon_nn_skel.so”, “libhexagon_nn_skel_v65.so”, “libhexagon_nn_skel_v66.so”  ??"
35509,TFLite GPU Delegate on iOS: failed assertion `Cannot create a buffer of zero length.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iPAD OS 13.2.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPAD Pro 2018
- TensorFlow installed from (source or binary): Binary (TfLiteGPUExperimental)
- TensorFlow version (use command below):
- Python version: NA
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: iPAD GPU

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When running the model on iPAD using CPU, we are able to get the output. But when doing GPU delegate, we get the error the following error:
_failed assertion `Cannot create a buffer of zero length._
**Describe the expected behavior**
No error
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
delegate = NewGpuDelegate(nullptr);
      interpreter->ModifyGraphWithDelegate(delegate);

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35508,Tensorflow gets stuck in sess.run() while making a prediction,"I'm using an **ssd** model to make prediction on images using **tensorflow**. I'm currently on **tensorflow** **1.14.0**.

I'm running the code in two different environments. I'm making a request to Cherrypy in one environment and it is working perfectly fine.

In the other environment, the request is made to **Cherrypy** through **Kafka** and the `tf.session.run()` gets stuck infinitely and doesn't return any output nor does it raise an exception.

`results = self._do_run(handle, final_targets, final_fetches,
                             feed_dict_tensor, options, run_metadata)`

This line is found in `session.py` package in `TensorFlow`. This is the specific point in code where the code gets stuck

I have not been able to figure out a reason for this anomaly and I didn't find anything useful on the internet regarding this problem. Does sending a request through **Kafka** is causing such an abnormal behavior?

Any help in this regard would be very appreciated. Thank You.

The details of my environment:

> CLUTTER_IM_MODULE=xim LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:.tar=01;31:.tgz=01;31:.arc=01;31:.arj=01;31:.taz=01;31:.lha=01;31:.lz4=01;31:.lzh=01;31:.lzma=01;31:.tlz=01;31:.txz=01;31:.tzo=01;31:.t7z=01;31:.zip=01;31:.z=01;31:.Z=01;31:.dz=01;31:.gz=01;31:.lrz=01;31:.lz=01;31:.lzo=01;31:.xz=01;31:.zst=01;31:.tzst=01;31:.bz2=01;31:.bz=01;31:.tbz=01;31:.tbz2=01;31:.tz=01;31:.deb=01;31:.rpm=01;31:.jar=01;31:.war=01;31:.ear=01;31:.sar=01;31:.rar=01;31:.alz=01;31:.ace=01;31:.zoo=01;31:.cpio=01;31:.7z=01;31:.rz=01;31:.cab=01;31:.wim=01;31:.swm=01;31:.dwm=01;31:.esd=01;31:.jpg=01;35:.jpeg=01;35:.mjpg=01;35:.mjpeg=01;35:.gif=01;35:.bmp=01;35:.pbm=01;35:.pgm=01;35:.ppm=01;35:.tga=01;35:.xbm=01;35:.xpm=01;35:.tif=01;35:.tiff=01;35:.png=01;35:.svg=01;35:.svgz=01;35:.mng=01;35:.pcx=01;35:.mov=01;35:.mpg=01;35:.mpeg=01;35:.m2v=01;35:.mkv=01;35:.webm=01;35:.ogm=01;35:.mp4=01;35:.m4v=01;35:.mp4v=01;35:.vob=01;35:.qt=01;35:.nuv=01;35:.wmv=01;35:.asf=01;35:.rm=01;35:.rmvb=01;35:.flc=01;35:.avi=01;35:.fli=01;35:.flv=01;35:.gl=01;35:.dl=01;35:.xcf=01;35:.xwd=01;35:.yuv=01;35:.cgm=01;35:.emf=01;35:.ogv=01;35:.ogx=01;35:.aac=00;36:.au=00;36:.flac=00;36:.m4a=00;36:.mid=00;36:.midi=00;36:.mka=00;36:.mp3=00;36:.mpc=00;36:.ogg=00;36:.ra=00;36:.wav=00;36:.oga=00;36:.opus=00;36:.spx=00;36:.xspf=00;36: LESSCLOSE=/usr/bin/lesspipe %s %s XDG_MENU_PREFIX=gnome- LANG=en_US.UTF-8 MANAGERPID=3651 DISPLAY=:0 INVOCATION_ID=56bfbab67ced4d1496b1e75a14fb4af7 GNOME_SHELL_SESSION_MODE=ubuntu COLORTERM=truecolor ZEITGEIST_DATA_PATH=/home/pandas/.local/share/zeitgeist USERNAME=pandas XDG_VTNR=2 SSH_AUTH_SOCK=/run/user/1000/keyring/ssh XDG_SESSION_ID=2 USER=pandas DESKTOP_SESSION=ubuntu QT4_IM_MODULE=xim TEXTDOMAINDIR=/usr/share/locale/ GNOME_TERMINAL_SCREEN=/org/gnome/Terminal/screen/a438ce7f_2ce0_4925_a81f_ef228f7d5a94 JOURNAL_STREAM=9:71038 TEXTDOMAIN=im-config SSH_AGENT_PID=3796 QT_ACCESSIBILITY=1 XDG_SESSION_TYPE=x11 XDG_DATA_DIRS=/usr/share/ubuntu:/usr/local/share/:/usr/share/:/var/lib/snapd/desktop XDG_SESSION_DESKTOP=ubuntu DBUS_STARTER_ADDRESS=unix:path=/run/user/1000/bus,guid=d47913508d685ac43652dcc65e059d87 GTK_MODULES=gail:atk-bridge WINDOWPATH=2 TERM=xterm-256color SHELL=/bin/bash VTE_VERSION=5202 QT_IM_MODULE=xim XMODIFIERS=@im=ibus IM_CONFIG_PHASE=2 DBUS_STARTER_BUS_TYPE=session XDG_CURRENT_DESKTOP=ubuntu:GNOME GPG_AGENT_INFO=/run/user/1000/gnupg/S.gpg-agent:0:1 GNOME_TERMINAL_SERVICE=:1.98 XDG_SEAT=seat0 SHLVL=1 GDMSESSION=ubuntu GNOME_DESKTOP_SESSION_ID=this-is-deprecated LOGNAME=pandas DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus,guid=d47913508d685ac43652dcc65e059d87 XDG_RUNTIME_DIR=/run/user/1000 XAUTHORITY=/run/user/1000/gdm/Xauthority XDG_CONFIG_DIRS=/etc/xdg/xdg-ubuntu:/etc/xdg PATH=/home/pandas/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin SESSION_MANAGER=local/ubuntu:@/tmp/.ICE-unix/3691,unix/ubuntu:/tmp/.ICE-unix/3691 LESSOPEN=| /usr/bin/lesspipe %s GTK_IM_MODULE=ibus

> "
35506,code for libhexagon_interface.so,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.15
- Python version: 3.4
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 1.2.1
- GCC/Compiler version (if compiling from source): 5.4
- CUDA/cuDNN version: 10.1
- GPU model and memory: 1080TI, 12G

**Describe the problem**
@karimnosseir, saw your answers to the https://github.com/tensorflow/tensorflow/issues/35221, 
Just interested in where is the source code to build libhexagon_interface.so for the hexagon delegate? 
Found that libhexagon_nn_skel.so had the source code in https://source.codeaurora.org/quic/hexagon_nn/nnlib/, but no code for libhexagon_interface.so? Do you know if libhexagon_interface.so will be open source as well?

"
35504,"AutoGraph: ""Entity could not be transformed and will be executed as-is""","Got the following error output from Databricks; exported the notebook to the attached file; error occurs in cell 10.

INFO:tensorflow:Converted call: <function read.<locals>.<lambda> at 0x7fbde97eae18>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=string>,)
    kwargs: {}

INFO:tensorflow:Not whitelisted: <method-wrapper '__call__' of function object at 0x7fbde97eae18>: default rule
INFO:tensorflow:Not whitelisted: <function read.<locals>.<lambda> at 0x7fbde97eae18>: default rule
INFO:tensorflow:Entity <function read.<locals>.<lambda> at 0x7fbde97eae18> is not cached for key <code object <lambda> at 0x7fbde9888f60, file ""<command-608347>"", line 138> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7fbdddf7c0b8>, frozenset({'mean', 'var'}))
INFO:tensorflow:Converting <function read.<locals>.<lambda> at 0x7fbde97eae18>
INFO:tensorflow:Error transforming entity <function read.<locals>.<lambda> at 0x7fbde97eae18>
Traceback (most recent call last):
  File ""/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 78, in parse_entity
    return parse_str(source, preamble_len=len(future_features)), source
  File ""/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 139, in parse_str
    module_node = gast.parse(src)
  File ""/databricks/python/lib/python3.7/site-packages/gast/gast.py"", line 240, in parse
    return ast_to_gast(_ast.parse(*args, **kwargs))
  File ""/usr/lib/python3.7/ast.py"", line 35, in parse
    return compile(source, filename, mode, PyCF_ONLY_AST)
  File ""<unknown>"", line 4
    .map(lambda x: decode(x, mean, var))
    ^
SyntaxError: invalid syntax

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 506, in converted_call
    converted_f = conversion.convert(target_entity, program_ctx)
  File ""/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 322, in convert
    free_nonglobal_var_names)
  File ""/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 240, in _convert_with_cache
    entity, program_ctx)
  File ""/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 469, in convert_entity_to_ast
    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)
  File ""/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 630, in convert_func_to_ast
    node, source = parser.parse_entity(f, future_features=future_features)
  File ""/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 118, in parse_entity
    return parse_str(source, preamble_len=len(future_features)), source
  File ""/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 145, in parse_str
    raise ValueError('expected exactly one node node, found {}'.format(nodes))
ValueError: expected exactly one node node, found []
WARNING:tensorflow:Entity <function read.<locals>.<lambda> at 0x7fbde97eae18> could not be transformed and will be executed as-is. Please report this to the AutoGraph
 team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []
[v2_compute_ndcg_geov4.02.ipynb.zip](https://github.com/tensorflow/tensorflow/files/4011226/v2_compute_ndcg_geov4.02.ipynb.zip)"
35502,GPU Support for Poisson Random Noise Generator in TensorFlow,"I am interested in using Generative Adversarial Network (GAN) to generate synthetic images for algorithm training. Poisson-distributed noise is an important source of noise in images. However, it does not appear that the TensorFlow Poisson noise generator has GPU support. The following code fails to execute:

```
import tensorflow as tf

with tf.Session() as sess:
    with tf.device(""/gpu:0""):
        test = sess.run(tf.random.poisson(1.0, [], dtype=tf.float32))
print(test)
```
with the error message
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation random_poisson/RandomPoissonV2: node random_poisson/RandomPoissonV2 (defined at /home/kjmiller/anaconda3/envs/tf_clone/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748)  was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0 ]. Make sure the device specification refers to a valid device.
	 [[random_poisson/RandomPoissonV2]]
```
When training my model (the code is too much to place here), I get
```
LookupError: No gradient defined for operation 'Generator/RandomPoissonV2' (op type: RandomPoissonV2)
```
I have tried overriding the gradient in the hopes that is the issue, but that did not work. I am pretty should that the second error also has to do with GPU support, as the `lam` argument input to tf.random.poisson is a tensor that lives in GPU memory.

It would be a tremendous help if TensorFlow had GPU support for Poisson-distributed noise, as this is a fundamental noise source for any image taken with a camera."
35501,cuDNN-based LSTM implementation not used when eager execution is disabled,"**System information**
OS: Debian 10, x64
GPU: GeForce GTX 1060 6GB
Python: 3.7.3
TensorFlow: 2.0.0 installed from source (git tag v2.0.0)
Bazel: 0.26.1
GCC: 6.5.0
CUDA: 10.1
cuDNN: 7.6.4

**Sample code**
```python
import numpy
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Input, LSTM

# uncommenting this causes performance issues
# tf.compat.v1.disable_eager_execution()

i = Input(shape=(1024, 32))
o = LSTM(units=32)(i)
m = Model(i, o)

m.compile('sgd', 'mse')
m.fit(numpy.zeros((512, 1024, 32)), numpy.zeros((512, 32)))
```

**Output with eager execution enabled**
```
Train on 512 samples
2019-12-30 17:06:25.988147: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_2026_2206' and '__inference___backward_cudnn_lstm_with_fallback_2026_2206_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_2818' both implement 'lstm_c56c9d1d-36f6-4e94-9410-ecd020c8700a' but their signatures do not match.
512/512 [==============================] - 2s 3ms/sample - loss: 0.0000e+00
```

**Output with eager execution disabled**
```
WARNING:tensorflow:From /home/<user>/virtualenvs/tensorflow-2.0.0/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Train on 512 samples
512/512 [==============================] - 8s 15ms/sample - loss: 0.0000e+00
```

**Discussion**
Note how the runtime is very significantly slower when eager execution is disabled. It seems that the cuDNN-based implementation of LSTM is not used whenever eager execution is disabled, as is seen [on this line in the code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent_v2.py#L1066) where `self.could_use_cudnn` ends up being `False`. This seems wrong to me, as there's no reason not to use  cuDNN in that situation.

The warning about the skipped optimization is discussed in #30263 and apparently can be ignored, as per qlzh727's comment."
35500,A puzzling & fatal error occurred in the tf.matmal(),"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

System information

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04 and win 10
- TensorFlow installed from (source or binary):
Install in the conda integration environment
conda create -n tf2-gpu tensorflow-gpu=2.0
- TensorFlow version (use command below):
tf-gpu 2.0 stable  &  tf-gpu 2.0 beta
- Python version:
3.6
- CUDA/cuDNN version:
CUDA: 10.0.0130-0
cuDNN: 7.6.5
- GPU model and memory:
GeForce GTX 850M
GeForce RTX 2070S

You can collect some of this information using our environment capture
script
You can also obtain the TensorFlow version with: 1. TF 1.0: python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"" 2. TF 2.0: python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""

Describe the current behavior & expected behavior

The following error occurs when using  tf.matmal() to compute the product of multidimensional tensors on the gpu.

    import tensorflow as tf
    import numpy as np
    
    j = np.random.rand(10, 6, 1130, 16, 8)
    k = np.random.rand(10, 6, 1130, 8, 1)
    # with tf.device(""CPU:0""):
    j = tf.cast(j, dtype=tf.float32)
    k = tf.cast(k, dtype=tf.float32)
    
    a = tf.matmul(j, k)[9, 3]
    b = tf.matmul(j[9], k[9])[3]
    c = tf.matmul(j[9, 3], k[9, 3])
    
    print(tf.reduce_all(tf.equal(a, b)))
    print(tf.reduce_all(tf.equal(b, c)))
    
    '''
    tf.Tensor(False, shape=(), dtype=bool)  # The correct output would be True
    tf.Tensor(True, shape=(), dtype=bool)
    '''

This error does not occur while using the CPU.

    ...
    
    with tf.device(""CPU:0""):
        j = tf.cast(j, dtype=tf.float32)
        k = tf.cast(k, dtype=tf.float32)
    
        a = tf.matmul(j, k)[9, 3]
        b = tf.matmul(j[9], k[9])[3]
        c = tf.matmul(j[9, 3], k[9, 3])
    
        print(tf.reduce_all(tf.equal(a, b)))
        print(tf.reduce_all(tf.equal(b, c)))
    
    '''
    tf.Tensor(True, shape=(), dtype=bool)
    tf.Tensor(True, shape=(), dtype=bool)
    '''

This error will not occur even if you reduce the size of some dimension a bit.

We make the following changes:

    # j = np.random.rand(10, 6, 1130, 16, 8)
    # k = np.random.rand(10, 6, 1130, 8, 1)
    j = np.random.rand(10, 6, 1129, 16, 8)  # 1130 --> 1129
    k = np.random.rand(10, 6, 1129, 8, 1)

also use the gpu:

    j = tf.cast(j, dtype=tf.float32)
    k = tf.cast(k, dtype=tf.float32)
    
    a = tf.matmul(j, k)[9, 3]
    b = tf.matmul(j[9], k[9])[3]
    c = tf.matmul(j[9, 3], k[9, 3])
    
    print(tf.reduce_all(tf.equal(a, b)))
    print(tf.reduce_all(tf.equal(b, c)))
    
    '''
    tf.Tensor(True, shape=(), dtype=bool)
    tf.Tensor(True, shape=(), dtype=bool)
    '''

I tested it on different GPU and OS , but got the same error.

When errors occur, I have compared the specific differences between the two methods ( a and b), and found that it is not the slight differences that cause the error.

    print(tf.reduce_sum(a-b))
    '''
    tf.Tensor(-1.3804454e+38, shape=(), dtype=float32)
    '''
"
35499,can´t train image segmentation with model.fit,"Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10.0.18362
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary): through pip 
TensorFlow version (use command below): 2.0
Python version: 3.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: using CPU
GPU model and memory: using CPU



Hi there,
I'm just getting started with tensorflow and keras. So far I kind of understand how to do training on images and how to create my network. But I got some issues which I´m not sure how to solve, and could be due to some bug:

Basically I want to do image segmentation. I´m first trying to use data that is already in numpy. So I have numpy array of (10,180,180,1) (so 10 cases of images with 180x180 and 1 channel) for both the images and its respective segmentations. I have a simple network defined as:

inputs = tf.keras.layers.Input((180,180,1))
c1 = tf.keras.layers.SeparableConv2D(15,(5,5), use_bias=True, activation='relu',kernel_initializer='he_normal',padding='same')(inputs) 

c2 = tf.keras.layers.SeparableConv2D(20,(3,3),use_bias=True, activation='relu',kernel_initializer='he_normal',padding='same')(c1)

outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='softmax')(c2)

Then I try to train it using:
`model.fit(training_ims,training_labels,batch_size=2,epochs=1)`

This gives this error:

> Train on 10 samples
Traceback (most recent call last):
  File ""<input>"", line 2, in <module>
  File ""C:\Users\Manuel_LincBiotech\Desktop\LincBiotech\python programs\venv\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File ""C:\Users\Manuel_LincBiotech\Desktop\LincBiotech\python programs\venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 324, in fit
    total_epochs=epochs)
  File ""C:\Users\Manuel_LincBiotech\Desktop\LincBiotech\python programs\venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""C:\Users\Manuel_LincBiotech\Desktop\LincBiotech\python programs\venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 86, in execution_function
    distributed_function(input_fn))
  File ""C:\Users\Manuel_LincBiotech\Desktop\LincBiotech\python programs\venv\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""C:\Users\Manuel_LincBiotech\Desktop\LincBiotech\python programs\venv\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 496, in _call
    raise ValueError(""Creating variables on a non-first call to a function""
ValueError: Creating variables on a non-first call to a function decorated with tf.function.
 2/10 [=====>........................] - ETA: 1s>>> training_ims

I´m quite new with this so I´m not remotely sure what is happening, tried looking for something similar before but didn´t find anything.

Thanks in advance"
35496,why tf do not work,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35495,"Consuming sets of files :There are many datasets distributed as a set of files, where each file is an example.","I test  and run code from: https://www.tensorflow.org/guide/data#applying_arbitrary_python_logic_with_tfpy_func
import pickle as pi
import os
import cv2
import numpy as np
import tensorflow as tf
import pathlib
flowers_root = tf.keras.utils.get_file(
    'flower_photos',
    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
    untar=True)
flowers_root = pathlib.Path(flowers_root)
for item in flowers_root.glob(""*""):
  print(item.name)
#dir_training_data = '/laptrinh python/ket_qua'
#flowers_root = pathlib.Path('/laptrinh python/test_file_anh/training')
list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))

for f in list_ds.take(5):
  print(f.numpy())

def process_path(file_path):
  label = tf.strings.split(file_path, '/')[-2]
  return tf.io.read_file(file_path), label

labeled_ds = list_ds.map(process_path)  
for image_raw, label_text in labeled_ds.take(1):
  print(repr(image_raw.numpy()[:100]))
  print()
  print(label_text.numpy())

And happen the error as following:

Traceback (most recent call last):

  File ""F:\laptrinh python\laptrinh_python_chinh_thuc\model_training.py"", line 33, in <module>
    for image_raw, label_text in labeled_ds.take(1):

  File ""C:\Users\Xuan Hau\.conda\envs\tf-gpu\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py"", line 622, in __next__
    return self.next()

  File ""C:\Users\Xuan Hau\.conda\envs\tf-gpu\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py"", line 666, in next
    return self._next_internal()

  File ""C:\Users\Xuan Hau\.conda\envs\tf-gpu\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py"", line 651, in _next_internal
    output_shapes=self._flat_output_shapes)

  File ""C:\Users\Xuan Hau\.conda\envs\tf-gpu\lib\site-packages\tensorflow_core\python\ops\gen_dataset_ops.py"", line 2673, in iterator_get_next_sync
    _six.raise_from(_core._status_to_exception(e.code, message), None)

  File ""<string>"", line 3, in raise_from

InvalidArgumentError: {{function_node __inference_Dataset_map_process_path_106}} slice index -1 of dimension 0 out of bounds.
	 [[{{node strided_slice}}]] [Op:IteratorGetNextSync]"
35494,distribut data io problem and I want to know way,"hi, I find some bug. Code is

import tensorflow as tf


if __name__ == '__main__':
    def Gen():
        for i in range(10):
            yield(i,2,3,4,5,6,7,8)

    dataset = tf.data.Dataset.from_generator(Gen, output_types=(tf.float32,tf.float32,tf.int32,tf.int32,tf.int32,tf.float32,tf.int32,tf.int32),output_shapes=None,args=None)
    for one_batch in dataset:
        print('one batch',one_batch)

    print(""******end**********"")

    num_gpu=1
    devices = ['/device:GPU:{}'.format(i) for i in range(num_gpu)]
    strategy = tf.distribute.MirroredStrategy(devices)

    input_context = tf.distribute.InputContext(num_input_pipelines=1,
            input_pipeline_id=0,
            num_replicas_in_sync=1)

    with strategy.scope():
        def dataset_fn(input_context):
            dataset = tf.data.Dataset.from_generator(Gen, output_types=(tf.float32,tf.float32,tf.int32,tf.int32,tf.int32,tf.float32,tf.int32,tf.int32),output_shapes=None,args=None)
            return dataset.shard(
                    input_context.num_input_pipelines, input_context.input_pipeline_id)

        train_dist_dataset = strategy.experimental_distribute_datasets_from_function(dataset_fn)

        for one_batch  in train_dist_dataset:
            print('****one batch*******',one_batch)

The code can be run, but in distribut ""for one_batch  in train_dist_dataset:"" at the end batch will be error.
Traceback (most recent call last):
  File ""/usr/local/python35/lib/python3.5/pdb.py"", line 1665, in main
    pdb._runscript(mainpyfile)
  File ""/usr/local/python35/lib/python3.5/pdb.py"", line 1546, in _runscript
    self.run(statement)
  File ""/usr/local/python35/lib/python3.5/bdb.py"", line 431, in run
    exec(cmd, globals, locals)
  File ""<string>"", line 1, in <module>
  File ""/search/speech/hubo/git/tf-code-acoustics/tf2.0-model/io_test.py"", line 45, in <module>
    for one_batch  in train_dist_dataset:
  File ""/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 275, in __next__
    return self.get_next()
  File ""/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 304, in get_next
    global_has_value, replicas = _get_next_as_optional(self, self._strategy)
  File ""/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 200, in _get_next_as_optional
    iterator._iterators[i].get_next_as_list(new_name))  # pylint: disable=protected-access
  File ""/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 878, in get_next_as_list
    lambda: _dummy_tensor_fn(data.value_structure))
  File ""/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 1204, in cond
    result = false_fn()
  File ""/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 878, in <lambda>
    lambda: _dummy_tensor_fn(data.value_structure))
  File ""/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 801, in _dummy_tensor_fn
    result.append(create_dummy_tensor(feature_shape, feature_type))
  File ""/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 784, in create_dummy_tensor
    for dim in feature_shape.dims:
TypeError: 'NoneType' object is not iterable
Uncaught exception. Entering post mortem debugging
Running 'cont' or 'step' will restart the program

I want to know why."
35493,tf.recompute_grad() throws dimension mismatched error when concatenating tensors with different number of channels ,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0 / 7.6
- GPU model and memory: GTX 1080 / 8GB

**Describe the current behavior**

My model is a segmentation model with DenseNet like structure. There are many concatenate operations between the encoder and decoder tensors. I want to recompute the gradients on these concatenate layers during backpropagation to limit the amount of GPU memory usage. A similar approach can be found here : https://github.com/joeyearsley/efficient_densenet_tensorflow.  I tried to use tf.recompute_grad() on a wrapper function which has a concatenate layer inside but it would raise an error when the channel dimensions of input tensors are not matched.

**Describe the expected behavior**
The concatenate layer should not raise an error when concatenating inputs with different number of channels.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
https://colab.research.google.com/drive/1d7zSGbYmocnupUpDIJLnlXt-3vH5bi83#scrollTo=xcPT-MWZAuvK&uniqifier=1

**Other info / logs**
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)
   1609   try:
-> 1610     c_op = c_api.TF_FinishOperation(op_desc)
   1611   except errors.InvalidArgumentError as e:

InvalidArgumentError: Dimension 3 in both shapes must be equal, but are 16 and 32. Shapes are [?,30,30,16] and [?,30,30,32].
	From merging shape 0 with other shapes. for 'packed_7' (op: 'Pack') with input shapes: [?,30,30,16], [?,30,30,32].

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
16 frames
<ipython-input-24-689917ca63e3> in <module>()
----> 1 model = test_model()
      2 history = model.fit(train_images, train_labels, epochs=10, 
      3                     validation_data=(test_images, test_labels))

<ipython-input-23-caa5b725d514> in test_model()
      4     x_list.append(layers.Conv2D(16, (3,3), activation='relu')(x_in))
      5     x_list.append(layers.Conv2D(32, (3,3), activation='relu')(x_in))
----> 6     x = efficient_concat(x_list)
      7     x = layers.Flatten()(x)
      8     x = layers.Dense(10, activation='softmax')(x)

<ipython-input-3-fa43d4abcee2> in efficient_concat(input_list)
      4         return x
      5     wraper = tf.recompute_grad(wraper)
----> 6     return wraper(input_list)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/custom_gradient.py in decorated(*args, **kwargs)
    164     """"""Decorated function with custom gradient.""""""
    165     if context.executing_eagerly():
--> 166       return _eager_mode_decorator(f, *args, **kwargs)
    167     else:
    168       return _graph_mode_decorator(f, *args, **kwargs)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/custom_gradient.py in _eager_mode_decorator(f, *args, **kwargs)
    333 
    334   input_tensors = [ops.convert_to_tensor(x) for x
--> 335                    in list(args) + list(variables)]
    336   arg_count = len(args)
    337   def actual_grad_fn(*result_grads):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/custom_gradient.py in <listcomp>(.0)
    332   flat_result = [gen_array_ops.identity(x) for x in flat_result]
    333 
--> 334   input_tensors = [ops.convert_to_tensor(x) for x
    335                    in list(args) + list(variables)]
    336   arg_count = len(args)

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)
   1182   preferred_dtype = deprecation.deprecated_argument_lookup(
   1183       ""dtype_hint"", dtype_hint, ""preferred_dtype"", preferred_dtype)
-> 1184   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
   1185 
   1186 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
   1240       name=name,
   1241       preferred_dtype=dtype_hint,
-> 1242       as_ref=False)
   1243 
   1244 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)
   1294 
   1295     if ret is None:
-> 1296       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1297 
   1298     if ret is NotImplemented:

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py in _autopacking_conversion_function(v, dtype, name, as_ref)
   1276   elif dtype != inferred_dtype:
   1277     v = nest.map_structure(_cast_nested_seqs_to_dtype(dtype), v)
-> 1278   return _autopacking_helper(v, dtype, name or ""packed"")
   1279 
   1280 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py in _autopacking_helper(list_or_tuple, dtype, name)
   1182     # checking.
   1183     if all(ops.is_dense_tensor_like(elem) for elem in list_or_tuple):
-> 1184       return gen_array_ops.pack(list_or_tuple, name=name)
   1185   must_pack = False
   1186   converted_elems = []

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_array_ops.py in pack(values, axis, name)
   6302   axis = _execute.make_int(axis, ""axis"")
   6303   _, _, _op = _op_def_lib._apply_op_helper(
-> 6304         ""Pack"", values=values, axis=axis, name=name)
   6305   _result = _op.outputs[:]
   6306   _inputs_flat = _op.inputs

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    791         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,
    792                          input_types=input_types, attrs=attr_protos,
--> 793                          op_def=op_def)
    794       return output_structure, op_def.is_stateful, op
    795 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in create_op(***failed resolving arguments***)
    546     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access
    547         op_type, inputs, dtypes, input_types, name, attrs, op_def,
--> 548         compute_device)
    549 
    550   def capture(self, tensor, name=None):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)
   3427           input_types=input_types,
   3428           original_op=self._default_original_op,
-> 3429           op_def=op_def)
   3430       self._create_op_helper(ret, compute_device=compute_device)
   3431     return ret

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)
   1771           op_def, inputs, node_def.attr)
   1772       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,
-> 1773                                 control_input_ops)
   1774     # pylint: enable=protected-access
   1775 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)
   1611   except errors.InvalidArgumentError as e:
   1612     # Convert to ValueError for backwards compatibility.
-> 1613     raise ValueError(str(e))
   1614 
   1615   return c_op

ValueError: Dimension 3 in both shapes must be equal, but are 16 and 32. Shapes are [?,30,30,16] and [?,30,30,32].
	From merging shape 0 with other shapes. for 'packed_7' (op: 'Pack') with input shapes: [?,30,30,16], [?,30,30,32]."
35492,"Tensorflow ""devel-gpu-py3"" docker image is stale, has not been updated for 2 months","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Docker image
- TensorFlow version: 2.0
- Python version: 3
- Installed using virtualenv? pip? conda?: Docker
- Bazel version (if compiling from source): default from the docker image
- GCC/Compiler version (if compiling from source): default from the docker image
- CUDA/cuDNN version: 10.0
- GPU model and memory: Nvidia gt750m



**Describe the problem**

I have been trying to create the TensorFlow-GPU wheel for my legacy hardware for which I need to build Tensorflow 2.0 with CUDA compute 3.0. The docker image for building a TensorFlow-GPU wheel is stale and has not been updated for [2 months](https://hub.docker.com/layers/tensorflow/tensorflow/devel-gpu-py3/images/sha256-2c17ae6134fc0abbc8128c62d74efd4d9312a04f92483c8f31279f630d5a5800). 

For the sake of reducing redundancy, I'll link the previous issue:
https://github.com/tensorflow/tensorflow/issues/34719

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
35491,Error: Not in GZIP format,"Getting below exception while following ""https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android""


org.gradle.api.InvalidUserDataException: Cannot expand TAR 'D:\WorkSpaces\ML_WorkSpace\TF_Android\app\build\intermediates\mobilenet_v1_1.0_224.tgz'.
	at org.gradle.api.internal.file.archive.TarFileTree.cannotExpand(TarFileTree.java:133)
	... 56 more
Caused by: org.gradle.api.resources.ResourceException: Could not read D:\WorkSpaces\TF_Android\app\build\intermediates\mobilenet_v1_1.0_224.tgz.
	at org.gradle.internal.resource.ResourceExceptions.readFailed(ResourceExceptions.java:36)
	at org.gradle.api.internal.file.archive.compression.GzipArchiver.read(GzipArchiver.java:64)
	at org.gradle.api.internal.file.MaybeCompressedFileResource.read(MaybeCompressedFileResource.java:55)
	at org.gradle.api.internal.file.archive.TarFileTree.visit(TarFileTree.java:78)
	... 107 more
Caused by: java.util.zip.ZipException: Not in GZIP format
	at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:165)
	at org.gradle.api.internal.file.archive.compression.GzipArchiver.read(GzipArchiver.java:61)
	... 109 more

**Don't know, what could be the problem?**"
35490,Why doesn't ```tf.keras.losses.binary_crossentropy``` raise error,"```
x = np.arange(10,dtype=np.float64).reshape(10,1)
#x.shape = (10,1)

y = np.arange(10,dtype=np.float64)
#y.shape = (10,)

tf.keras.losses.binary_crossentropy(y_true=y, y_pred=x)
#this line does't raise error

tf.keras.metrics.BinaryAccuracy()(y_true=y, y_pred=x)
#this line neither

tf.keras.metrics.Precision()(y_true=y, y_pred=x)
#this line raise an error
```
I think ```binary_crossentropy``` and ```BinaryAccuracy```  should raise an ValueError like ```tf.keras.metrics.Precision```: 
```
ValueError: Shapes (128, 1) and (128,) are incompatible
```"
