Issue Number,Issue Title,Issue Body
21814,How to use GridLSTMCell or Grid3LSTMCell on image data in tensorflow?,"I am thinking about applying GridLSTM on image data. But I cannot figure out how to orgnize my inputs to fit the requirements of `GridLSTMCell` or `Grid3LSTMCell`. I found some example use cases for `GridLSTMCell`, such as [link1 ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/kernel_tests/rnn_cell_test.py)and [link2](https://github.com/phvu/grid-lstm-tensorflow). But they focus on none-image data and help little for me.

By applying GridLSTM on image data with shape `(batch_size,38,38,64)` , I wish to get output with shape `(batch_size,38,38,num*4)`. Here, `num` refers to the count of units GridLSTM outputs in each position of the feature map with shape `(38,38)` starting from each feature map corner.

The following is my test code, I am just not able to make it work.

```
import tensorflow as tf
from tensorflow.contrib.rnn import static_rnn
from tensorflow.contrib.grid_rnn import Grid3LSTMCell,Grid2LSTMCell
import numpy as np

#tf.enable_eager_execution()
with tf.Session() as sess:
    shape = (32,38,38,64)
    inputs = tf.constant(np.ones(shape,dtype=np.float32),dtype=np.float32)
    cell = Grid3LSTMCell(8)
    out = static_rnn(cell,inputs,dtype=np.float32)
    ouput = sess.run(out)
```
Any help is appropriated.


Have I written custom code: n/a
OS Platform and Distribution: n/a
TensorFlow installed from: https://pypi.org/project/tensorflow/
TensorFlow version: 1.7.0
Bazel version: n/a
CUDA/cuDNN version: n/a
GPU model and memory: n/a
Exact command to reproduce: n/a
Mobile device: n/a"
21811,Estimators created from Keras models expect unconsumed outputs to be provided with data in training/evaluation.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian Jessie
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 2.7.9
- **Exact command to reproduce**: See gist [here](https://gist.github.com/zmjjmz/3f621aaafc5683238ade6224e5dedcb6)
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Mobile device**: N/A

### Describe the problem
Sorry, this is a bit arcane/edge-casey. Basically I have a Keras model that I want to convert into an Estimator for use with the `train_and_evaluate` framework. One aspect of these models is that I have layers that are outputs but do not contribute to the loss that are part of the deployed model's contract -- e.g. an `oov_code` that indicates if the input (text) was out-of-vocabulary. 

The way I've been doing this _before_ switching to a `keras.model_to_estimator` method was to have the Keras model's loss and metrics be set to just the parts contribute to the loss in the `ModeKeys.TRAIN` or `ModeKeys.EVAL` modes and then have the `export_outputs` argument take all of them in the `PREDICT` mode, e.g. in this [gist](https://gist.github.com/zmjjmz/b84248a8a1ff21afd1c172e0abbba2e8). I can't do this fine-grained thing in `model_to_estimator`, which is sort of the first problem.

However, theoretically the `Model.compile` function allows for the `loss` and `metrics` arguments to be dictionaries of output names (derived from the layer names) to their respective loss / metric functions. When using `Model.fit`, one can (as in line 49 of the provided reproduction) provide a dictionary of `output:labels` which are then appropriately routed to their respective losses / metrics (as can be seen by running it - note the `classes_accuracy` and `classes_loss` printouts.

Unfortunately, when using `model_to_estimator` followed by `train_and_evaluate` my eternal enemy `_create_ordered_io` expects to find a value in `y` for all outputs, irrespective of the `loss` and `metrics` not specifying any such requirements. 

Confusingly I am promised separately that `we will not be expecting any data to be passed to ""reporting"" during training.` as shown in the provided logs.

Basically, it would be nice if `_create_ordered_io` had some way to 'know' that these outputs should be ignored for these steps, OR for a way to specify different outputs according to the `ModeKeys` in `model_to_estimator`. I can workaround this at the moment by providing a fake output in the `numpy_input_fn`'s `y` argument (e.g. what happens if you set `incl_reporting=True` in the `get_input_fn` function in the reproduction script), however this is really not ideal for a variety of reasons.

### Source code / logs
The gist provided [here](https://gist.github.com/zmjjmz/3f621aaafc5683238ade6224e5dedcb6) has both reproduction code and comments with the outputs.
"
21809,100% CPU load after import tensorflow as tf when using USB camera stream on Windows 10,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0
- **Python version**: 
- **Bazel version (if compiling from source)**: Python 3.5.5 :: Anaconda, Inc.
-**Mobile device: No Mobile device
- **CUDA/cuDNN version**: no GPU
- **GPU model and memory**: no GPU
- **Exact command to reproduce: python testCamera.py

### Describe the problem
I am using the pYuEye uEye API to get live images for my iDS industry USB 3.0 camera, see the simple attached code. When I run the code without importing tensorflow everything is fine. I get an image every 100 ms. But when I just add the line 'import tensorflow as tf' my CPU (i7 4770) has a 100% CPU load. I do not use any function from tensorflow.

Does anyone have an idea what could be the problem?


Cheers,
Wunder0
### Source code / logs
```
from Kameraobjekt import Kamera
import numpy as np
import tensorflow as tf


def startCam():
    EnableAutoparameter = True      
    EnableHardwareGain = False 
    AOI_x = 112        
    AOI_y = 84        
    AOI_width = 800     
    AOI_height = 600    
    Buffersize = 3    
    
    Cam.init()  
    Cam.SetParams(EnableAutoparameter, EnableHardwareGain)
    Cam.SetAOI(AOI_x, AOI_y, AOI_width, AOI_height)
    Cam.SetFPS(10.)  
    Cam.AllocMem(Buffersize)
    Cam.StartCaptureImage()
    Cam.setExposure()

Cam = Kamera()
startCam()
while True:
    Puffer = Cam.GetImage() 
```"
21807,Profiling Tensorflow C++ with nvidia profiler/tf.Profiler,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: bazel
- **TensorFlow version (use command below)**: 1.8
- **Python version**:2.7
- **Bazel version (if compiling from source)**: 0.14.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0 20160609
- **CUDA/cuDNN version**: 9/7.1
- **GPU model and memory**: 1080ti/11gb dual
- **Exact command to reproduce**:
nvprof bazel-bin/tensorflow/examples/label_image/label_image from [Link](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image)
works, but when i run nvvp it throws me an error, that application profiled cannot be found. I am looking at an TF alternative [tf.Profiler](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler) , but it doesn't have any documentation for C++ api

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21805,Unable to convert frozen graph model to required fromat,"I am working on the AIY vision kit and trying to implement object detection in a video stream. I have tried following the method posted on the AIY website(https://aiyprojects.withgoogle.com/) but seem to be facing a problem when I try to convert the model using the bonnet model compiler. This is a brief summary of the steps I followed:

    1.create annotations for each image in the training set in xml format(256x256).
    2.convert all the images to 256x256.
    3.create the tf records.
    4.downloaded the base model of ssd_mobilnet_v1_coco.config and made the required changes.(set 
       num classes = 2, and path to train and val tf records).
    5.trained the model
    6.exported the frozen graph to binaryproto format. These 5 steps are can be seen by following this 
       link,the same images were used but resized.(https://medium.freecodecamp.org/trackin ... 
       c86419225e) all the functions such as create tf record and other related functions can be found in 
       this git repo https://github.com/bourdakos1/Custom-Object-Detection
After I export the model, I get the following error while using the bonnet model compiler:

2018-08-14 10:39:19.285588: I external/org_tensorflow/tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 556 operators, 893 arrays (0 quantized)

2018-08-14 10:39:19.302398: I external/org_tensorflow/tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 556 operators, 893 arrays (0 quantized)

2018-08-14 10:39:19.324886: F external/org_tensorflow/tensorflow/contrib/lite/toco/tooling_util.cc:628] Check failed: dim >= 1 (0 vs. 1)

I have also attached th3 frozen graph below
[frozen_inference_graph.zip](https://github.com/tensorflow/tensorflow/files/2310933/frozen_inference_graph.zip)
"
21802,./ configure fails due to nccl2 on ubuntu 16.04,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution** :Linux Ubuntu 16.04)
-CPU- Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz		: 1596,00MHz
```
$ uname -a
Linux Dell-T5500 4.15.0-32-generic #35~16.04.1-Ubuntu SMP Fri Aug 10 21:54:34 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
```
- **TensorFlow installed from (source or binary)**:
installed from source using git
- **TensorFlow version (use command below)**: Release 1.10.0
- **Python version**:
```
$ python
Python 3.6.5 | packaged by conda-forge | (default, Apr  6 2018, 13:39:56) 
[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
```
- **Bazel version (if compiling from source)**: bazel 0.15.0
- **GCC/Compiler version (if compiling from source)**:
`gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) `

- **CUDA/cuDNN version**:
```
~$ apt search cudnn
En train de trier... Fait
Recherche en texte intégral... Fait
libcudnn7/now 7.1.4.18-1+cuda9.2 amd64  [installé, local]
  cuDNN runtime libraries
libcudnn7-dev/now 7.1.4.18-1+cuda9.2 amd64  [installé, local]
  cuDNN development libraries and headers
```
- **GPU model and memory**: _GTX960 4Gb_
```
$ nvidia-smi
Wed Aug 22 15:43:58 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.51                 Driver Version: 396.51                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 960     Off  | 00000000:03:00.0  On |                  N/A |
| 39%   35C    P8    12W / 130W |    272MiB /  4042MiB |      1%      Default |
+-------------------------------+----------------------+----------------------+
```
- **Mobile device:** NA
- **Exact command to reproduce**:
./configure was executed from a virtual environment:
```
$ ./configure
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.15.0 installed.
Please specify the location of python. [Default is /home/jeanpat/anaconda3/envs/DeepFish/bin/python]: 


Found possible Python library paths:
  /home/jeanpat/anaconda3/envs/DeepFish/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/home/jeanpat/anaconda3/envs/DeepFish/lib/python3.6/site-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: Y
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n
No Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon AWS Platform support? [Y/n]: n
No Amazon AWS Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n
No Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: y
GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: n
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: Y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.2

Please specify the location where CUDA 9.2 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.14

Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 

Do you wish to build TensorFlow with TensorRT support? [y/N]: N
No TensorRT support will be enabled for TensorFlow.

Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 

Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:


Invalid path to NCCL 2 toolkit, /usr/local/cuda-9.2/lib/libnccl.so.2 or /usr/local/cuda-9.2/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2
Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]:
```


### Describe the problem
**./configure** script failed to detect nccl2 which was install from a nvidia deb package:


### Source code / logs
```
apt show libnccl2
Package: libnccl2
Version: 2.2.13-1+cuda9.2
Priority: optional
Section: libs
Source: nccl
Maintainer: cudatools <cudatools@nvidia.com>
Installed-Size: 74,5 MB
Depends: libc6 (>= 2.3.4), libgcc1 (>= 1:3.0), libstdc++6 (>= 4.1.1)
Download-Size: 27,6 MB
APT-Manual-Installed: yes
APT-Sources: file:/var/nccl-repo-2.2.13-ga-cuda9.2  Packages
Description: NVIDIA Collectives Communication Library (NCCL) Runtime
 NCCL (pronounced ""Nickel"") is a stand-alone library of standard collective
 communication routines for GPUs, such as all-gather, reduce, broadcast, etc.,
 that have been optimized to achieve high bandwidth over PCIe. NCCL supports up
 to eight GPUs and can be used in either single- or multi-process (e.g., MPI)
 applications.
```

```
~$ locate libnccl
/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/nccl/libnccl_ops_op_lib.lo
/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/nccl/libnccl_ops_op_lib.lo-2.params
/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/nccl_archive/libnccl.pic.a
/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/nccl_archive/libnccl.pic.a-2.params
/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/external/nccl_archive/debian/libnccl-dev.install
/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/external/nccl_archive/debian/libnccl-dev.manpages
/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/external/nccl_archive/debian/libnccl1.install.in
/home/jeanpat/Developpement/Archives-cuda/CUDA 9/libnccl-dev_2.1.4-1+cuda9.0_amd64.deb
/home/jeanpat/Developpement/Archives-cuda/CUDA 9/libnccl2_2.1.4-1+cuda9.0_amd64.deb
/home/jeanpat/anaconda3/envs/DeepFish/lib/libnccl.so
/home/jeanpat/anaconda3/envs/DeepFish/lib/libnccl.so.1
/home/jeanpat/anaconda3/envs/DeepFish/lib/libnccl.so.1.3.5
/home/jeanpat/anaconda3/envs/DeepFish/lib/libnccl_static.a
/home/jeanpat/anaconda3/pkgs/nccl-1.3.4-cuda8.0_1/lib/libnccl.so
/home/jeanpat/anaconda3/pkgs/nccl-1.3.4-cuda8.0_1/lib/libnccl.so.1
/home/jeanpat/anaconda3/pkgs/nccl-1.3.4-cuda8.0_1/lib/libnccl.so.1.3.4
/home/jeanpat/anaconda3/pkgs/nccl-1.3.4-cuda8.0_1/lib/libnccl_static.a
/home/jeanpat/anaconda3/pkgs/nccl-1.3.5-cuda9.0_0/lib/libnccl.so
/home/jeanpat/anaconda3/pkgs/nccl-1.3.5-cuda9.0_0/lib/libnccl.so.1
/home/jeanpat/anaconda3/pkgs/nccl-1.3.5-cuda9.0_0/lib/libnccl.so.1.3.5
/home/jeanpat/anaconda3/pkgs/nccl-1.3.5-cuda9.0_0/lib/libnccl_static.a
/usr/lib/x86_64-linux-gnu/libnccl.so
/usr/lib/x86_64-linux-gnu/libnccl.so.2
/usr/lib/x86_64-linux-gnu/libnccl.so.2.2.13
/usr/lib/x86_64-linux-gnu/libnccl_static.a
/usr/share/doc/libnccl-dev
/usr/share/doc/libnccl2
/usr/share/doc/libnccl-dev/NCCL-SLA.txt.gz
/usr/share/doc/libnccl-dev/changelog.Debian.gz
/usr/share/doc/libnccl-dev/copyright
/usr/share/doc/libnccl2/NCCL-SLA.txt.gz
/usr/share/doc/libnccl2/changelog.Debian.gz
/usr/share/doc/libnccl2/copyright
/var/lib/dpkg/info/libnccl-dev.list
/var/lib/dpkg/info/libnccl-dev.md5sums
/var/lib/dpkg/info/libnccl2.list
/var/lib/dpkg/info/libnccl2.md5sums
/var/lib/dpkg/info/libnccl2.shlibs
/var/lib/dpkg/info/libnccl2.triggers
/var/nccl-repo-2.2.13-ga-cuda9.2/libnccl-dev_2.2.13-1+cuda9.2_amd64.deb
/var/nccl-repo-2.2.13-ga-cuda9.2/libnccl2_2.2.13-1+cuda9.2_amd64.deb
```

```
$ locate nccl.h
/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/nccl/third_party/nccl/nccl.h
/home/jeanpat/.cache/bazel/_bazel_jeanpat/25351aa6d762f8d39f0cff26612e1488/external/nccl_archive/src/nccl.h
/home/jeanpat/anaconda3/envs/DeepFish/include/nccl.h
/home/jeanpat/anaconda3/envs/DeepFish/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/nccl.h
/home/jeanpat/anaconda3/envs/DeepFish/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/python_nccl.h
/home/jeanpat/anaconda3/envs/DeepFish/pkgs/pytorch-0.4.0-py36_cuda9.1.85_cudnn7.1.2_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/nccl.h
/home/jeanpat/anaconda3/envs/DeepFish/pkgs/pytorch-0.4.0-py36_cuda9.1.85_cudnn7.1.2_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/python_nccl.h
/home/jeanpat/anaconda3/envs/PyTorch/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/nccl.h
/home/jeanpat/anaconda3/envs/PyTorch/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/python_nccl.h
/home/jeanpat/anaconda3/pkgs/nccl-1.3.4-cuda8.0_1/include/nccl.h
/home/jeanpat/anaconda3/pkgs/nccl-1.3.5-cuda9.0_0/include/nccl.h
/home/jeanpat/anaconda3/pkgs/pytorch-0.4.0-py36_cuda8.0.61_cudnn7.1.2_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/nccl.h
/home/jeanpat/anaconda3/pkgs/pytorch-0.4.0-py36_cuda8.0.61_cudnn7.1.2_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/python_nccl.h
/home/jeanpat/anaconda3/pkgs/pytorch-0.4.0-py36_cuda9.0.176_cudnn7.1.2_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/nccl.h
/home/jeanpat/anaconda3/pkgs/pytorch-0.4.0-py36_cuda9.0.176_cudnn7.1.2_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/python_nccl.h
/home/jeanpat/anaconda3/pkgs/pytorch-0.4.0-py36_cuda9.1.85_cudnn7.1.2_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/nccl.h
/home/jeanpat/anaconda3/pkgs/pytorch-0.4.0-py36_cuda9.1.85_cudnn7.1.2_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/python_nccl.h
/home/jeanpat/anaconda3/pkgs/pytorch-0.4.0-py36hdf912b8_0/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/nccl.h
/home/jeanpat/anaconda3/pkgs/pytorch-0.4.0-py36hdf912b8_0/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/python_nccl.h
/home/jeanpat/anaconda3/pkgs/pytorch-cpu-0.4.0-py36_cpu_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/nccl.h
/home/jeanpat/anaconda3/pkgs/pytorch-cpu-0.4.0-py36_cpu_1/lib/python3.6/site-packages/torch/lib/include/torch/csrc/cuda/python_nccl.h
/usr/include/nccl.h
```"
21801,tensorflow-gpu failed init from systemd service,"I have wrote an application with Java / C / Python who works well since a few weeks and executed in Bash by:
`source ../frozen-graph-env/bin/activate && /usr/bin/java -jar test.jar --start`

pip freeze of my environment:

> absl-py==0.2.2
> astor==0.6.2
> bleach==1.5.0
> gast==0.2.0
> grpcio==1.12.1
> html5lib==0.9999999
> Markdown==2.6.11
> numpy==1.14.5
> pkg-resources==0.0.0
> protobuf==3.6.0
> six==1.11.0
> tensorboard==1.6.0
> tensorflow-gpu==1.6.0
> termcolor==1.1.0
> Werkzeug==0.14.1

The goal now is to execute the application with systemd:
`ExecStart=/bin/bash -c ""source ../frozen-graph-env/bin/activate && /usr/bin/java -jar test.jar --start""`

The Java application start then exit because tensorflow-gpu failed to be imported. I have no log in systemd / syslog to show it (not the best way to open an issue, sorry) but if I replace _tensorflow-gpu 1.6.0_ by _tensorflow 1.6.0_, i.e. without GPU support, the application works well.

I'm importing tensorflow in my python script with:
`import tensorflow as tf`

Is it the same issue of [tensorflow gpu failed init from systemd service - Stack Overflow](https://www.google.fr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=2ahUKEwjprca82IDdAhUhC8AKHZGVAiYQFjACegQIBxAB&url=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F47429354%2Ftensorflow-gpu-failed-init-from-systemd-service&usg=AOvVaw0JTtqGmcGLXHUG9f_76xBz)?

**Is there a specific way to use tensorflow-gpu with systemd? Is it an issue? How can I help to investigate?**

------------------------

### System information
- **Have I written custom code: yes**
- **OS Platform and Distribution: Linux Ubuntu 16.04**
- **Bazel version: NA**
- **CUDA/cuDNN version: NA**
- **GPU model and memory: NA**
- **Exact command to reproduce: NA**
- **Mobile device: NA**
- **TensorFlow installed from pip**
- **TensorFlow version: v1.6.0-0-gd2e24b6039 1.6.0**
- **TensorFlow GPU version: v1.6.0-0-gd2e24b6039 1.6.0**"
21800,Custom tensorflow op with output shape determined by the input tensor,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- Linux Ubuntu 16.04
- PC
- Binary
- 1.10
- 3.5
- 9.0/7.0
- V100 
- **Exact command to reproduce**:
```
REGISTER_OP(""GroupPoint"")
    .Input(""points: float32"")
    .Input(""idx: int32"")
    .Input(""length: int32"")
    .Output(""out: float32"")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
        ::tensorflow::shape_inference::ShapeHandle dims1; **// shape N x C**
        c->WithRank(c->input(0), 2, &dims1);
        ::tensorflow::shape_inference::ShapeHandle dims2; **// shape L**
        c->WithRank(c->input(1), 1, &dims2);
        ::tensorflow::shape_inference::ShapeHandle dims3; **// scalar, shape 0**
        c->WithRank(c->input(2), 0, &dims3);
        ::tensorflow::shape_inference::ShapeHandle output = c->MakeShape({c->Dim(dims2, 2), L});
        **// L should have the value of input(2)**
        c->set_output(0, output); **// shape input(2) x C**
        return Status::OK();
    });
```

### Describe the problem
Basically I want to write a customized op to output a tensor with shape which is determined by the value of input(2), i.e the third input. 
More specifically, I want to get 'length' number of 'idx' (i.e. idx[0:length] ) then to gather those indices from 'points'. ( i.e. tf.gather(points, idx[0:length]))
The stock tensor-flow apis such as tf.slice and tf.gather would not allow me to do this because my op (also an customised op) to get input 'idx' has to be run on GPU. Error Message as:

**Tensor(""strided_slice:0"", shape=(128, 16), dtype=float32, device=/device:GPU:0)
Tensor(""strided_slice_1:0"", shape=(32,), dtype=int32, device=/device:GPU:0) Tensor(""strided_slice_2:0"", shape=(), dtype=i
nt32, device=/device:GPU:0)
2018-08-22 13:28:55.108318: E tensorflow/core/common_runtime/executor.cc:696] Executor failed to create kernel. Not found
: No registered 'QueryBallPoint' OpKernel for CPU devices compatible with node QueryBallPoint = QueryBallPoint[nsample=32
, radius=1, _device=""/device:GPU:0""](Const_1, Const_2)
        .  Registered:  device='GPU'
[[Node: QueryBallPoint = QueryBallPoint[nsample=32, radius=1, _device=""/device:GPU:0""](Const_1, Const_2)]]
Tensor(""GatherV2:0"", shape=(?,), dtype=int32, device=/device:GPU:0)**

I have tried to digging from the source code of class ::tensorflow::shape_inference::InferenceContext*, but failed, I have no idea how to do this, any kind of suggestions would be greatly helpful, thanks"
21799,Keras.fit fails when using tf.data.Datasets with sparse labels,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: r1.10
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 5.2
- **GPU model and memory**: NVIDIA TITAN X
- **Exact command to reproduce**: See below

### Describe the problem
Keras model.fit function now accepts tf.data.Datasets as an argument, however sparse output labels cause an error. 

### Source code / logs

1. Build a custom Keras model:
model = tf.keras.applications.MobileNet(weights=None, include_top=True, input_shape=(32,32,3), classes=100)

2. Compile with 'sparse_categorical_crossentropy':
model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy',     metrics=['accuracy'])

3. This model will train successfully if passed a numpy array pair (x, sparse_y), but fails when this is wrapped in tf.data.Dataset interface. See traceback below:

  File ""C:\PycharmProjects\mi_exp\.venv\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1278, in fit
    validation_split=validation_split)
  File ""C:\PycharmProjects\mi_exp\.venv\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 917, in _standardize_user_data
    exception_prefix='target')
  File ""C:\PycharmProjects\mi_exp\.venv\lib\site-packages\tensorflow\python\keras\engine\training_utils.py"", line 182, in standardize_input_data
    'with shape ' + str(data_shape))
ValueError: Error when checking target: expected reshape_2 to have 2 dimensions, but got array with shape (None,)
"
21797,why tf has no attribute 'ThreadPoolOptionProto',"tf version:  common cpu tensorflow v1.9 (I'v tried 1.10 too)
i wanna config the num_threads in run_config, using:
`tf.ThreadPoolOptionProto(num_threads=xx)`
 but an ERROR throw:
model 'tensorflow' has no attrbute 'ThreadPoolOptionProto'.

While, in the proto file: `tensorflow/core/protobuf/config.proto`,  The message of 'ThreadPoolOptionProto' has been defined.
so, why i cannot use it, and how to use?
thank you very much."
21796,"int() argument must be a string, a bytes-like object or a number at InceptionResNetV2","### System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): window10 and jupyter
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone X
- TensorFlow installed from (source or binary): source 
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.1 / 7.1
- **GPU model and memory**: 
- **Exact command to reproduce**:

### Describe the problem

i used tensorflow1.9 function `tf.keras.applications.InceptionResNetV2` and follow the tensorflow document from [here](https://tensorflow.google.cn/versions/r1.9/api_docs/python/tf/keras/applications/InceptionResNetV2?authuser=2&hl=vi) ,but can't used on the tensorflow  [example code,](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb)
it will be print error`int() argument must be a string, a bytes-like object or a number, not'TensorShape'`but others Library (vgg16,19 & inception_v3..and so on) can work!
 
### Source code / 
input  image-----
`def load_image(image_path):`
    `img = tf.read_file(image_path)`
   ` img = tf.image.decode_jpeg(img, channels=3)`
   ` img = tf.image.resize_images(img, (299, 299))`
    `img = tf.keras.applications.inception_resnet_v2.preprocess_input(img)`
   ` return img, image_path`

create mode -----
`image_model =tf.keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False, 
                                                                                                                        weights='imagenet')`


the error -------
`TypeError                                 Traceback (most recent call last)
<ipython-input-8-fe73201476b6> in <module>()
      1 startTime=time.time()
----> 2 image_model =tf.keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False, weights='imagenet')
      3 
      4 #image_model = inception_v4.create_model(weights='imagenet', include_top=True)
      5 new_input = image_model.input

D:\anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\keras\applications\inception_resnet_v2.py in InceptionResNetV2(include_top, weights, input_tensor, input_shape, pooling, classes)
    304   for block_idx in range(1, 11):
    305     x = inception_resnet_block(
--> 306         x, scale=0.17, block_type='block35', block_idx=block_idx)
    307 
    308   # Mixed 6a (Reduction-A block): 17 x 17 x 1088

D:\anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\keras\applications\inception_resnet_v2.py in inception_resnet_block(x, scale, block_type, block_idx, activation)
    187       output_shape=K.int_shape(x)[1:],
    188       arguments={'scale': scale},
--> 189       name=block_name)([x, up])
    190   if activation is not None:
    191     x = Activation(activation, name=block_name + '_ac')(x)

D:\anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in __call__(self, inputs, *args, **kwargs)
    712           input_shapes = nest.map_structure(lambda x: x.get_shape(), inputs)
    713 
--> 714         output_shapes = self.compute_output_shape(input_shapes)
    715         output_shapes = nest.flatten(output_shapes)
    716         outputs = [

D:\anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\keras\layers\core.py in compute_output_shape(self, input_shape)
    675 
    676   def compute_output_shape(self, input_shape):
--> 677     input_shape = tuple(tensor_shape.TensorShape(input_shape).as_list())
    678 
    679     if self._output_shape is None:

D:\anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\tensor_shape.py in __init__(self, dims)
    539       else:
    540         # Got a list of dimensions
--> 541         self._dims = [as_dimension(d) for d in dims_iter]
    542     self._ndims = None
    543 

D:\anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\tensor_shape.py in <listcomp>(.0)
    539       else:
    540         # Got a list of dimensions
--> 541         self._dims = [as_dimension(d) for d in dims_iter]
    542     self._ndims = None
    543 

D:\anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\tensor_shape.py in as_dimension(value)
    480     return value
    481   else:
--> 482     return Dimension(value)
    483 
    484 

D:\anaconda\envs\tensorflow\lib\site-packages\tensorflow\python\framework\tensor_shape.py in __init__(self, value)
     35       raise TypeError(""Cannot convert %s to Dimension"" % value)
     36     else:
---> 37       self._value = int(value)
     38       if (not isinstance(value, compat.bytes_or_text_types) and
     39           self._value != value):

TypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorShape'`"
21794,Mapping data directories using -v flag gives empty directories in the container,"Running the docker using docker run ... -v /dir:/dir 
gives the correct dir name in the container but without any data in it
"
21793,Images come without keras,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21792,Can't connect to docker as remote interpreter when running on a remote machine,"I am using the container on a remote machine

When I run regular python on it, I use pycharm to run ssh interpreter, it syncs the needed directories and runs the wanted scripts and works great, 

I need the docker to somehow expose it's filesystem and interpreter on a host's port and connect to it via pycharm remote interpreter (from another machine) 

"
21791,can't connect to container as remote interpreter when running container on remote machine,"I am working with a remote machine, 

And using pycharm to run remote interpreter

I need the docker to somehow expose the python interpreter via a host's port so I can connect to it, sync directories and run python scripts"
21788,tf.keras.utils.multi_gpu_model does use only one GPU,"Tensorflow 1.10
Docker container 1.10.0-gpu-py3 from https://hub.docker.com/r/tensorflow/tensorflow/
I think this is a bug. I expect tf.keras.utils.multi_gpu_model to use multiple GPUs. However with the following example it uses only 1 (checking with nvidia-smi):

```
import tensorflow as tf
from tensorflow.keras.applications import Xception
from tensorflow.keras.utils import multi_gpu_model
import numpy as np

num_samples = 32
height = 224
width = 224
num_classes = 1000

with tf.device('/cpu:0'):
    model = Xception(weights=None,input_shape=(height, width, 3),classes=num_classes)

# Replicates the model on 2 GPUs.
# This assumes that your machine has 2 available GPUs.
parallel_model = multi_gpu_model(model, gpus=2)
parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

# Generate dummy data.
x = np.random.random((num_samples, height, width, 3)).astype(np.float32)

dataset = tf.data.Dataset.from_tensor_slices(x)
dataset = dataset.batch(32)
dataset = dataset.repeat()

iterator = dataset.make_one_shot_iterator()
batch = iterator.get_next()

y = parallel_model(batch)

sess = tf.keras.backend.get_session()
while True:
    try:
        result = sess.run(y)
        print(result.shape)
    except tf.errors.OutOfRangeError:
        break
```





See also https://stackoverflow.com/questions/51962659/tf-keras-utils-multi-gpu-model-does-use-only-one-gpu
"
21787,tflite runs much slower than tfmobile ...,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu14.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Xiaomi 8
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0 / 7.1
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem

I test performance of tf-mobile, tf-lite, tf-mobile-int8, tf-lite-int8 on android, and I find that the speed of tf-lite is much slower than tf-mobile.

1. I use `freeze_graph` to generate `A.pb` file from `checkpoint` for testing tf-mobile performance.

2. I use `toco_convert` to convert `A.pb` file to `A.tflite` file for for testing tf-lite performance.

3. I use `transform_graph` to get quantitative `AQ.pb` file from `A.pb` file for testing tf-mobile int8 performance.

4. I train a model with the same architecture by adding the line `tf.contrib.quantize.create_training_graph()`  and get the `checkpoint` file. Then I replace the line with `tf.contrib.quantize.create_eval_graph()` to generate the `A.pbtxt` file, and use `checkpoint` file and `A.pbtxt` file to get `A8.pb` with fake quantization nodes. Finally, I use `toco_convert` to get the `A8.tflite` file.

5. I test the performance with these 4 files on android, each runs several times for inference on the same image, and the result is listed below:

tf-mobile:           357ms per image
tf-mobile int8:    356ms per image
tf-lite:                 844ms per image
tf-lite int8;          571ms per image

**I wonder why tf-lite is much slower than tf-mobile.**

PS: the model architecture only contains: CONV+BN+RELU, RESHAPE, FULLY-CONTECT ops.

The features shape from CONV+BN+RELU is [B,T,C], then I reshape it to [-1,C] and go on to the fc layer, then reshape the out with shape [B*T,K] to [B,T,K], which is the final result I expected.

**I wonder is the reshape op the brings the worse performance ?**

**Thank you very much ...**

"
21786,MirroredStrategy error with Object detection retraining,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Not on Mobile device
- **TensorFlow installed from (source or binary)**: Built from source using branch 'v1.9.0-0-g25c197e'
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: Build label: 0.15.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: Cuda version:9.1 cuDNN version:7.1.2
- **GPU model and memory**: 4 x GRID K520, Memory: 3.94GiB
- **Exact command to reproduce**:
python object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --num_eval_steps=${NUM_EVAL_STEPS} \
    --alsologtostderr

Following is the code change I made to https://github.com/tensorflow/models/blob/master/research/object_detection/model_main.py

```
def main(unused_argv):
  flags.mark_flag_as_required('model_dir')
  flags.mark_flag_as_required('pipeline_config_path')

  distribution = tf.contrib.distribute.MirroredStrategy()
  config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir,train_distribute=distribution)
```

### Describe the problem
I am retraining ssd_mobilenet_v1_coco and following the steps as per https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md

I am able to run the retraining using OneDeviceStrategy and None distribution.
However when using MirroredStrategy I get the following error:

```2018-08-22 06:49:29.272903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-08-22 06:49:29.273436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: GRID K520 major: 3 minor: 0 memoryClockRate(GHz): 0.797
pciBusID: 0000:00:03.0
totalMemory: 3.94GiB freeMemory: 3.90GiB
2018-08-22 06:49:29.309479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-08-22 06:49:29.310018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: 
name: GRID K520 major: 3 minor: 0 memoryClockRate(GHz): 0.797
pciBusID: 0000:00:04.0
totalMemory: 3.94GiB freeMemory: 3.90GiB
2018-08-22 06:49:29.343433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-08-22 06:49:29.343977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 2 with properties: 
name: GRID K520 major: 3 minor: 0 memoryClockRate(GHz): 0.797
pciBusID: 0000:00:05.0
totalMemory: 3.94GiB freeMemory: 3.90GiB
2018-08-22 06:49:29.379652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-08-22 06:49:29.380222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 3 with properties: 
name: GRID K520 major: 3 minor: 0 memoryClockRate(GHz): 0.797
pciBusID: 0000:00:06.0
totalMemory: 3.94GiB freeMemory: 3.90GiB
2018-08-22 06:49:29.380429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1, 2, 3
2018-08-22 06:49:30.617246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-22 06:49:30.617312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 2 3 
2018-08-22 06:49:30.617327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N N N N 
2018-08-22 06:49:30.617345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   N N N N 
2018-08-22 06:49:30.617360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 2:   N N N N 
2018-08-22 06:49:30.617371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 3:   N N N N 
2018-08-22 06:49:30.617926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3648 MB memory) -> physical GPU (device: 0, name: GRID K520, pci bus id: 0000:00:03.0, compute capability: 3.0)
2018-08-22 06:49:30.653271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 3648 MB memory) -> physical GPU (device: 1, name: GRID K520, pci bus id: 0000:00:04.0, compute capability: 3.0)
2018-08-22 06:49:30.689029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 3648 MB memory) -> physical GPU (device: 2, name: GRID K520, pci bus id: 0000:00:05.0, compute capability: 3.0)
2018-08-22 06:49:30.724507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 3648 MB memory) -> physical GPU (device: 3, name: GRID K520, pci bus id: 0000:00:06.0, compute capability: 3.0)
WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f679afa32f0>) includes params argument, but params are not passed to Estimator.
2018-08-22 06:49:30.781121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1, 2, 3
2018-08-22 06:49:30.781362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-22 06:49:30.781389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 2 3 
2018-08-22 06:49:30.781409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N N N N 
2018-08-22 06:49:30.781424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   N N N N 
2018-08-22 06:49:30.781442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 2:   N N N N 
2018-08-22 06:49:30.781458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 3:   N N N N 
2018-08-22 06:49:30.781801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/device:GPU:0 with 3648 MB memory) -> physical GPU (device: 0, name: GRID K520, pci bus id: 0000:00:03.0, compute capability: 3.0)
2018-08-22 06:49:30.781913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/device:GPU:1 with 3648 MB memory) -> physical GPU (device: 1, name: GRID K520, pci bus id: 0000:00:04.0, compute capability: 3.0)
2018-08-22 06:49:30.781999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/device:GPU:2 with 3648 MB memory) -> physical GPU (device: 2, name: GRID K520, pci bus id: 0000:00:05.0, compute capability: 3.0)
2018-08-22 06:49:30.782122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/device:GPU:3 with 3648 MB memory) -> physical GPU (device: 3, name: GRID K520, pci bus id: 0000:00:06.0, compute capability: 3.0)
WARNING:tensorflow:num_readers has been reduced to 2 to match input file shards.
WARNING:tensorflow:From /home/ubuntu/tensorflow/models/research/object_detection/core/preprocessor.py:1205: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 106, in <module>
    tf.app.run()
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 102, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 447, in train_and_evaluate
    return executor.run()
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 531, in run
    return self.run_local()
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 669, in run_local
    hooks=train_hooks)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 366, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1117, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1160, in _train_model_distributed
    self.config)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/distribute.py"", line 794, in call_for_each_tower
    return self._call_for_each_tower(fn, *args, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 269, in _call_for_each_tower
    coord.join(threads)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 479, in run
    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1107, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/ubuntu/tensorflow/models/research/object_detection/model_lib.py"", line 252, in model_fn
    preprocessed_images, features[fields.InputDataFields.true_image_shape])
  File ""/home/ubuntu/tensorflow/models/research/object_detection/meta_architectures/ssd_meta_arch.py"", line 514, in predict
    preprocessed_inputs)
  File ""/home/ubuntu/tensorflow/models/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py"", line 126, in extract_features
    scope=scope)
  File ""/home/ubuntu/tensorflow/models/research/slim/nets/mobilenet_v1.py"", line 284, in mobilenet_v1_base
    scope=end_point)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 2798, in separable_convolution2d
    collections=weights_collections)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py"", line 297, in model_variable
    use_resource=use_resource)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py"", line 252, in variable
    use_resource=use_resource)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 1328, in get_variable
    constraint=constraint)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 1090, in get_variable
    constraint=constraint)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 427, in get_variable
    return custom_getter(**custom_getter_kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 1734, in wrapped_custom_getter
    *args, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1744, in layer_variable_getter
    return _model_variable_getter(getter, *args, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1735, in _model_variable_getter
    use_resource=use_resource)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py"", line 297, in model_variable
    use_resource=use_resource)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py"", line 252, in variable
    use_resource=use_resource)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/distribute.py"", line 599, in disable_partitioned_variables
    return getter(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 404, in _true_getter
    use_resource=use_resource, constraint=constraint)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 806, in _get_single_variable
    with ops.colocate_with(v):
  File ""/home/ubuntu/anaconda3/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4219, in _colocate_with_for_gradient
    with self.colocate_with(op, ignore_existing):
  File ""/home/ubuntu/anaconda3/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4272, in colocate_with
    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1268, in internal_convert_to_tensor_or_indexed_slices
    value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1107, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py"", line 245, in _tensor_conversion
    assert not as_ref
AssertionError
```
I also switched the gpu using OneDeviceStrategy to any of the 4 available GPU without a problem.
I run into the problem only when using MirroredStrategy. Reproducibility 100%"
21785,Tensorflow and Opencv in C++,"Hello,

I just buy a Tensorflow and OpenCV application in C++ in Ubuntu. As soon as I add a Tensorflow object in my program I can't read images from the VideoCapture. The sended images are all black.


### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: IUbuntu 16.04
- **TensorFlow installed from (source or binary)**: I use Tensorflow 1.5 source to generate the 2 so files
- **TensorFlow version (use command below)**: 1.5 (source
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) 
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
------------------------



I don't really understand why Tensorflow so file change the behaviors of OpenCV.
"
21782,profiler trace_steps is not match global step in distribution training,"### System information

== cat /etc/issue ===============================================
Linux gpu0198 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7 (Core)""
VERSION_ID=""7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux gpu0198 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                              1.14.5
protobuf                           3.6.1
tensorflow-gpu                     1.10.0

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.10.0
tf.GIT_VERSION = v1.10.0-0-g656e7a2b34
tf.COMPILER_VERSION = v1.10.0-0-g656e7a2b34
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH :/usr/local/nvidia/cpu_lib:/nodemanager/lib/native:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/java//jre/lib/amd64/server:/nodemanager/lib/native
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Aug 22 03:12:21 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.46                 Driver Version: 390.46                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  On   | 00000000:86:00.0 Off |                    0 |
| N/A   28C    P0    38W / 250W |      0MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a 


### Describe the problem

When I use tensorflow profiler to get the timeline for distribution training, I found the trace_steps didn't match the global step and it result in useless timeline.
I check the profile_context.py and find that the profile step is not related to the global step.

distribution config:
ps 2
worker 8
1 gpu per worker

### Source code / logs
sync code:
with tf.contrib.tfprof.ProfileContext(profile_dir=FLAGS.train_dir,
                                                trace_steps=range(100, 200,3),
                                                dump_steps=[200],
                                                enabled=FLAGS.enable_profile,
                                                debug=FLAGS.enable_profile_debug) as pctx:
...
...
                optimizer = tf.train.SyncReplicasOptimizer(
                    opt=optimizer,
                    replicas_to_aggregate=FLAGS.replicas_to_aggregate,
                    total_num_replicas=worker_replicas,
                    variable_averages=variable_averages,
                    variables_to_average=moving_average_variables)


sync train log :
worker 0:
debug: tracing step: 148
debug: tracing step: 151
INFO:tensorflow:global step 19: loss = 2.1377 (8.300 sec/step)
debug: tracing step: 154
debug: tracing step: 157

async code:
with tf.contrib.tfprof.ProfileContext(profile_dir=FLAGS.train_dir,
                                                trace_steps=range(800, 900),
                                                dump_steps=[200],
                                                enabled=FLAGS.enable_profile,
                                                debug=FLAGS.enable_profile_debug) as pctx:

async train log:
worker 0:
debug: tracing step: 838
INFO:tensorflow:global step 2198: loss = 2.0687 (6.361 sec/step)
debug: tracing step: 839

worker 7:
debug: tracing step: 825
INFO:tensorflow:global step 3137: loss = 2.5605 (7.343 sec/step)
debug: tracing step: 826
"
21779,Tensorflow C++ build the tensorflow.dll failed,"when i use cmake to compile the tensorflow.dll ,it shows a lot of error and the the number of the *.vcxproj is 266 but I just success only 43. Here is my configuration:
 python:3.5.2
cuda:none 
cudnn:none
tensorflow:1.8.0 
vs：2015 
swig:3.0.12 
Git：2.18 
cmake:3.10.2 

![default](https://user-images.githubusercontent.com/36782510/44437048-d9cfba80-a5ea-11e8-8b0a-98bb6f912ad1.png)

the error list：
E:\Win_TF\tensorflow-master\tensorflow/c/c_api.h(1113): warning C4190: “TF_NewWhile”有指定的 C 链接，但返回了与 C 不兼容的 UDT“TF_WhileParams” (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\profiler\internal\tfprof_graph.cc)
43>  E:\Win_TF\tensorflow-master\tensorflow/c/c_api.h(1069): note: 参见“TF_WhileParams”的声明 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\profiler\internal\tfprof_graph.cc)
43>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\profiler\internal\tfprof_code.cc)
43>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\profiler\tfprof_options.cc)
43>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\profiler\tfprof_options.cc)
43>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\profiler\tfprof_options.cc)
43>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\profiler\tfprof_options.cc)
43>  tfprof_node.cc
43>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\profiler\internal\advisor\internal_checker_runner_dummy.cc)
43>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\profiler\internal\advisor\internal_checker_runner_dummy.cc)
43>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\profiler\internal\advisor\internal_checker_runner_dummy.cc)
43>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\profiler\internal\advisor\internal_checker_runner_dummy.cc)
43>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\profiler\internal\tfprof_code.cc)
43>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\profiler\internal\tfprof_code.cc)
43>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\profiler\internal\tfprof_code.cc)
43>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\profiler\internal\tfprof_code.cc)
43>E:\Win_TF\tensorflow-master\tensorflow/core/util/saved_tensor_slice_util.h(113): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF
...............
.........
.............
232>------ 已启动生成: 项目: contrib_image_ops_gen_python, 配置: Release x64 ------
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\gather_nd_op_cpu_impl_7.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/kernels/gather_nd_op_cpu_impl.h(139): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\gather_nd_op_cpu_impl_7.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\gather_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\gather_op.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\gather_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\gather_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\generate_vocab_remapping_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\generate_vocab_remapping_op.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\generate_vocab_remapping_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\gather_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\generate_vocab_remapping_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\guarantee_const_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\guarantee_const_op.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\guarantee_const_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\generate_vocab_remapping_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\guarantee_const_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\guarantee_const_op.cc)
167>  histogram_op.cc
167>  host_constant_op.cc
167>  i_remote_fused_graph_ops_definitions.cc
167>  identity_n_op.cc
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\histogram_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\histogram_op.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\histogram_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\histogram_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\histogram_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\host_constant_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\host_constant_op.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\host_constant_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\i_remote_fused_graph_ops_definitions.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\i_remote_fused_graph_ops_definitions.cc)

..........
\tensorflow-master\tensorflow\core\kernels\quantized_instance_norm.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_instance_norm.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_matmul_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_matmul_op.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_matmul_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_mul_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_mul_op.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_mul_op.cc)
242>LINK : fatal error LNK1181: 无法打开输入文件“E:\Win_TF\tensorflow-master\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj”
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_matmul_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_mul_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_matmul_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_mul_op.cc)
245>------ 已启动生成: 项目: contrib_layers_sparse_feature_cross_ops_gen_python, 配置: Release x64 ------
167>  quantized_reshape_op.cc
167>  quantized_resize_bilinear_op.cc
167>  queue_base.cc
244>LINK : fatal error LNK1181: 无法打开输入文件“E:\Win_TF\tensorflow-master\tensorflow\contrib\cmake\build\tf_python_op_gen_main.dir\Release\python_op_gen.obj”
246>------ 已启动生成: 项目: tf_contrib_reduce_slice_ops_ops, 配置: Release x64 ------
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_pooling_ops.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_pooling_ops.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_pooling_ops.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_pooling_ops.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_pooling_ops.cc)
167>  queue_op.cc
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_reshape_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_reshape_op.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_reshape_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_reshape_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_reshape_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_resize_bilinear_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_resize_bilinear_op.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_resize_bilinear_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_resize_bilinear_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\quantized_resize_bilinear_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\queue_base.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\queue_base.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\queue_base.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\queue_base.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\queue_base.cc)
167>  queue_ops.cc
167>  random_crop_op.cc
167>  random_op.cc
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\queue_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\queue_op.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\queue_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\queue_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\queue_op.cc)
167>  random_poisson_op.cc
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\queue_ops.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\queue_ops.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\queue_ops.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\queue_ops.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\queue_ops.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_crop_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_crop_op.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_crop_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_crop_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_crop_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_op.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_op.cc)
246>  reduce_slice_ops.cc
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_op.cc)
167>  random_shuffle_op.cc
167>  random_shuffle_queue_op.cc
167>  range_sampler.cc
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\range_sampler.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_poisson_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_poisson_op.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_poisson_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_poisson_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_poisson_op.cc)
167>  reader_ops.cc
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_shuffle_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_shuffle_op.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_shuffle_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_shuffle_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_shuffle_op.cc)
246>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数
246>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported
246>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数
246>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障
246>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_shuffle_queue_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\random_shuffle_queue_op.cc)

......................
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\reverse_op.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\resource_variable_ops.cc)
167>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\resource_variable_ops.cc)
167>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\core\kernels\resource_variable_ops.cc)
...................
263>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称
260>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\contrib\periodic_resample\ops\array_ops.cc)
260>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\contrib\periodic_resample\ops\array_ops.cc)
260>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\contrib\periodic_resample\ops\array_ops.cc)
260>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\contrib\periodic_resample\ops\array_ops.cc)
261>cl : 命令行 warning D9025: 正在重写“/DTF_COMPILE_LIBRARY”(用“/UTF_COMPILE_LIBRARY”)
261>  hyperplane_lsh_probes.cc
264>------ 已启动生成: 项目: _beam_search_ops, 配置: Release x64 ------
261>cl : 命令行 warning D9025: 正在重写“/DTF_COMPILE_LIBRARY”(用“/UTF_COMPILE_LIBRARY”)
261>  nearest_neighbor_ops.cc
263>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported
263>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数
263>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障
263>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数
261>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\contrib\nearest_neighbor\kernels\hyperplane_lsh_probes.cc)
261>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\contrib\nearest_neighbor\kernels\hyperplane_lsh_probes.cc)
261>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\contrib\nearest_neighbor\kernels\hyperplane_lsh_probes.cc)
261>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\contrib\nearest_neighbor\kernels\hyperplane_lsh_probes.cc)
261>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称 (编译源文件 E:\Win_TF\tensorflow-master\tensorflow\contrib\nearest_neighbor\kernels\hyperplane_lsh_probes.cc)
264>cl : 命令行 warning D9025: 正在重写“/DTF_COMPILE_LIBRARY”(用“/UTF_COMPILE_LIBRARY”)
264>  beam_search_ops.cc
264>E:\Win_TF\tensorflow-master\tensorflow/core/framework/tensor_types.h(105): error C2899: 不能在模板声明之外使用类型名称
264>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C4579: 'tensorflow::Variant::in_place': in-class initialization for type 'const tensorflow::Variant::in_place_t' is not yet implemented; static member will remain uninitialized at runtime but use in constant-expressions is supported
264>E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): error C2131: 表达式的计算结果不是常数
264>  E:\Win_TF\tensorflow-master\tensorflow/core/framework/variant.h(284): note: 因为返回临时项或对其的引用的地址导致了故障
264>E:\Win_TF\tensorflow-master\tensorflow/core/lib/gtl/array_slice_internal.h(89): error C2064: 项不会计算为接受 0 个参数的函数
265>------ 已启动生成: 项目: ALL_BUILD, 配置: Release x64 ------
========== 生成: 成功 42 个，失败 223 个，最新 0 个，跳过 0 个 ==========

"
21778,tf.keras.estimator.model_to_estimator ignores layer reuse for Sequential models,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **Mobile device**: N/A
- **TensorFlow installed from**: binary
- **TensorFlow version**: 1.10.0
- **Python version**: 3.6
- **Bazel version**: N/A
- **GCC/Compiler version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: `python my_custom_script.py`

**Overview:**

If a model is created using `tf.keras.models.Sequential` with reuse of some layers (including those that are parameterless) then the estimator created from `tf.keras.estimator.model_to_estimator` may contain a different model from the original Keras model.

This also may produce an error of shape mismatch in case that the `tf.keras.estimator.model_to_estimator` is called after fitting the Sequential model. 

**Simple example:**
```
import tensorflow as tf
import numpy as np

max_pool = tf.keras.layers.MaxPooling2D(2)
model = tf.keras.Sequential([
    max_pool,
    max_pool,
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy')

X, y = np.zeros((32, 8, 8, 1)), np.ones((32,))
model.fit(X, y)

tf.keras.estimator.model_to_estimator(keras_model=model)
```
**The error produced:**
```
...
  File ""~/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 847, in assert_is_compatible_with
    raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))
ValueError: Shapes (16, 1) and (4, 1) are incompatible
```


**Explanation:**

The mismatch happens because `tf.keras.estimator.model_to_estimator` traverses `model.layers` which has a single entry of max pooling. Therefore, instead of making 2 max pooling operations it performs a single one which results in **16** outputs after `tf.keras.layers.Flatten()` instead of **4** that was intended initially.

If `model.fit` is not called then `tf.keras.estimator.model_to_estimator` does not produce any errors which is actually more concerning since it may be not obvious that the new model is different from the initial Sequential model."
21777,many configure script options do nothing,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
      N/A - issue is with TF build itself
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
      Have tried MacOS, Centos7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
      N/A
- **TensorFlow installed from (source or binary)**:
      source
- **TensorFlow version (use command below)**:
      ./configure
- **Python version**:
      Have tried py 2.7, 3.6
- **Bazel version (if compiling from source)**:
      Have tried 1.4, 1.5, 1.8, 1.9
- **GCC/Compiler version (if compiling from source)**:
      N/A
- **CUDA/cuDNN version**:
      N/A
- **GPU model and memory**:
      N/A
- **Exact command to reproduce**:
      ./configure
--


There appears to be a bug when running tensorflow's configure script prior to building with bazel.
Many of the yes/no questions result in identical configuration files being written out, no matter what the user may answer.   This can be verified by:

1. ensure a completely clean git repo via: git clean -xdf
2. run the configure script, answer No to the question ""Do you wish to build TensorFlow with Google Cloud Platform support?"", or many of the other similar questions.
3. check which files have changed via: git status --ignored
4. observe contents of generated .tf_configure.bazelrc  
5. ensure a completely clean git repo via: git clean -xdf
6. run the configure script, give the opposite answer as given in #2
7. observe that the contents of .tf_configure.bazelrc are the same as in #4 
8. :(

I have tried this on macos and centos7, with checkouts of TF 1.4, 1.5, 1.8, 1.9...

```
jkeller@L127.local:~/mio/tensorflow$ git checkout v1.8.0
HEAD is now at 93bc2e2072... Merge pull request #18928 from tensorflow/release-patch-4-1
jkeller@L127.local:~/mio/tensorflow$ git clean -xdf
jkeller@L127.local:~/mio/tensorflow$ git status --ignored
HEAD detached at v1.8.0
nothing to commit, working tree clean
jkeller@L127.local:~/mio/tensorflow$ ./configure 
You have bazel 0.13.0 installed.
Please specify the location of python. [Default is /usr/local/opt/python/bin/python3.6]: 


Found possible Python library paths:
  /usr/local/Cellar/python/3.6.4_3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/usr/local/Cellar/python/3.6.4_3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages]

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: N
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: N
No Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: N
No Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: N
No Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: N
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: N
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: N
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: N
Clang will not be downloaded.

Do you wish to build TensorFlow with MPI support? [y/N]: N
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
Configuration finished
jkeller@L127.local:~/mio/tensorflow$ cat .tf_configure.bazelrc 
build --action_env PYTHON_BIN_PATH=""/usr/local/opt/python/bin/python3.6""
build --action_env PYTHON_LIB_PATH=""/usr/local/Cellar/python/3.6.4_3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages""
build --force_python=py3
build --host_force_python=py3
build --python_path=""/usr/local/opt/python/bin/python3.6""
build:gcp --define with_gcp_support=true
build:hdfs --define with_hdfs_support=true
build:s3 --define with_s3_support=true
build:kafka --define with_kafka_support=true
build:xla --define with_xla_support=true
build:gdr --define with_gdr_support=true
build:verbs --define with_verbs_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_CUDA=""0""
build --action_env TF_DOWNLOAD_CLANG=""0""
build --define grpc_no_ares=true
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
build --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
jkeller@L127.local:~/mio/tensorflow$ git status --ignored
HEAD detached at v1.8.0
Ignored files:
  (use ""git add -f <file>..."" to include in what will be committed)

	.bazelrc
	.tf_configure.bazelrc
	tools/python_bin_path.sh

nothing to commit, working tree clean
jkeller@L127.local:~/mio/tensorflow$
```"
21775,Suddenly OOM (GPU has free memory),
21772,dataloss error tf.records at random times,"OS Platform and Distribution --> Ubuntu - 18.04.1
TensorFlow installed from --> using pip
TensorFlow version --> 1.8.0
Bazel version --> don't know 
CUDA/cuDNN version --> cuda 9
GPU model and memory --> nvidia Titan XP 12GB


I am stuck at a very strange issue for a long time. This is my problem - 
I have a tfrecords file (name = ""Input.tfrecords"") from which I read data and then I do some modifcation to the data and store it to another tfrecords file (name = ""Output.tfrecods"") . Below is the code snippet --

    tf.reset_default_graph()
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    
    
    def _bytes_feature(value):
        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
    
    
    def _str_feature(value):
        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode('utf-8')]))
    
    
    def _float_feature(value):
        return tf.train.Feature(float_list=tf.train.FloatList(value=value.reshape(-1)))
    
    
    def _int64_feature(value):
        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))
    
    
    def som_function(FLAGS):
        with tf.Graph().as_default() as g:
    
            tfr_writer = tf.python_io.TFRecordWriter(FLAGS.Output_tfrdatafile)
    
            dataset = tf.data.TFRecordDataset(FLAGS.Input_tfrdatafile)
    
            dataset = dataset.map(lambda x: reader.initial_parser(x, FLAGS.HEIGHT, FLAGS.WIDTH))
    
            dataset = dataset.batch(FLAGS.BATCH_SIZE)
            iterator = dataset.make_one_shot_iterator()
    
            images, original_ig, img_name = iterator.get_next()
    
    
            org_batch = tf.Variable(tf.random_normal([FLAGS.BATCH_SIZE, FLAGS.HEIGHT, FLAGS.WIDTH, 3]), trainable=False)
            initial = tf.Variable(tf.random_normal([FLAGS.BATCH_SIZE, FLAGS.HEIGHT, FLAGS.WIDTH, 3]))
            org_batch_assign_op = org_batch.assign(original_ig)
    
            initial_assign_op = initial.assign(images)
    
            total_loss = #someloss function
    
    
    
            train_op = tf.train.MomentumOptimizer(FLAGS.LEARNING_RATE, momentum=0.95, use_nesterov=True,
                                                  name=""non_paraopt_SGD"").minimize(total_loss,
                                                                                   global_step=global_step)
    
    
            init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
    
            with tf.Session(config=config) as sess:
                sess.run(init_op)
                start_time = time.time()
                batches_count = 0
                while True:
                    try:
                        _, _, image_names = sess.run([initial_assign_op,org_batch_assign_op,  img_name])
    
                        //some code that updates initial variable
    
                        org_batch = tf.cast(org_batch, tf.uint8)
                        image_t, org_image_t = sess.run([initial, org_batch])
    
                        if not FLAGS.addNetworklose:
                            lambda_val = np.zeros(image_t.shape).astype(np.float32)
    
                        for i in range(image_t.shape[0]):
                            filename = str(image_names[i], 'utf-8')
    
                            example = tf.train.Example(features=tf.train.Features(feature={
                                    'file_name': _str_feature(filename),
                                    'float_image': _float_feature(image_t[i] + reader.mean_pixel),
                                    'image_raw': _bytes_feature(org_image_t[i].tostring()),
                                    'lambda_image': _float_feature(lambda_val[i])
                                }))
                            tfr_writer.write(example.SerializeToString())
    
                        batches_count = batches_count + 1
                    except tf.errors.OutOfRangeError:
                        print(""final time elspased"", (time.time() - start_time))
                        print('Done doing non paramteric part')
                        break
    
                tfr_writer.close()


I always succesfully creats the  ""Output.tfrecods"".But whenever I read the file  ""Output.tfrecods""file, I randomly get the **Dataloss Error**. This is where I am trying to read the Output.tfrecods file.
```
def start_training(FLAGS):
    tf.reset_default_graph()
    run_id = FLAGS.MODEL_NAME if FLAGS.MODEL_NAME else str(uuid.uuid4())

    if not os.path.exists(FLAGS.summary_path):
        os.makedirs(FLAGS.summary_path)

    model_path = '%s/%s' % (FLAGS.MODEL_DIR, run_id)
    if not os.path.exists(model_path):
        os.makedirs(model_path)

    training_dataset = tf.data.TFRecordDataset(FLAGS.Training_tfrdatafile)
    training_dataset = training_dataset.map(lambda x: reader.lambda_parser(x, FLAGS.HEIGHT, FLAGS.WIDTH))

    training_dataset = training_dataset.batch(FLAGS.BATCH_SIZE)
    min_queue_examples = int(FLAGS.EPOCHS * 0.4)
    training_dataset = training_dataset.shuffle(buffer_size=min_queue_examples + 3 * FLAGS.BATCH_SIZE)

    validation_dataset = tf.data.TFRecordDataset(FLAGS.Validation_tfrdatafile)
    validation_dataset = validation_dataset.map(lambda x: reader.lambda_parser(x, FLAGS.HEIGHT, FLAGS.WIDTH))
    validation_dataset = validation_dataset.batch(FLAGS.VAL_BATCH_SIZE)
    
    iterator = tf.data.Iterator.from_structure(training_dataset.output_types,
                                               training_dataset.output_shapes)

    training_init_op = iterator.make_initializer(training_dataset)
    validation_init_op = iterator.make_initializer(validation_dataset)

    target_ig, images, img_name, _ = iterator.get_next()

    ae_inputs = tf.placeholder(tf.float32, (None, FLAGS.HEIGHT, FLAGS.WIDTH, 3),
                               name='auto_input')  # input to the network (MNIST images)
    target = tf.placeholder(tf.float32, (None, FLAGS.HEIGHT, FLAGS.WIDTH, 3),
                            name='target')

    ae_output = model.net(ae_inputs, training=True)

    learning_rate = tf.placeholder(tf.float32, shape=[], name='learning_rate')
    img_loss = tf.nn.l2_loss((target - ae_output), name='img_l2loss') / tf.to_float(size2)

    tv_loss = total_variation_loss(ae_output, FLAGS.HEIGHT, FLAGS.WIDTH)
    loss = img_loss + 200.0 * tv_loss

    global_step = tf.Variable(FLAGS.gs_val, name=""p_global_step"", trainable=False)

    
    train_op = tf.train.AdamOptimizer(learning_rate, name=""p_trainopt"").minimize(loss, global_step=global_step)
    
    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
    return_lr = FLAGS.LEARNING_RATE

    with tf.Session(config=config) as sess:
        
        saver = tf.train.Saver(tf.trainable_variables())
        if FLAGS.ModelFromName:
            file = FLAGS.MODEL_FILENAME
        else:
            file = tf.train.latest_checkpoint(model_path)
        # file = model_path + '/' + FLAGS.MODEL_FILENAME

        sess.run(init_op)
        if file:
            print('Restoring model from {}'.format(file))
            saver.restore(sess, file)

        start_time = time.time()
        start_ti = time.time()
        acc_loss = []
        vgg_pl_loss = []
        img_pl_loss = []
        cl_pl_loss = []
        sl_pl_loss = []
        fl_pl_loss = []

        cont_los_FF = 0
        stl_los_FF = 0

        total_batches = FLAGS.TOTALINPUT // FLAGS.BATCH_SIZE
        val_batches = FLAGS.TOTALVAL // FLAGS.VAL_BATCH_SIZE
        print('the value of total batches is: ', total_batches)
        for ep_count in range(FLAGS.EPOCHS):
            sess.run(training_init_op)
            count = 1
            st_t = time.time()
            # if ep_count+1 <= FLAGS.EPOCHS  or ep_count == 0:
            while True:
                try:
                    ig_b, tg_b = sess.run([images, target_ig])

                    _, loss_t,img_l2_t, step = sess.run([train_op, loss, img_loss],feed_dict={ae_inputs: ig_b,
                                                                                           target: tg_b,
                                                                                           learning_rate: FLAGS.LEARNING_RATE})
                                                                                
                    count = count + 1
                except tf.errors.OutOfRangeError:
                    print(""final time elspased"", (time.time() - st_t))
                    #  print('Done doing non paramteric part')
                    break

            print('Number of epochs done= ', (ep_count + 1))
            if (ep_count + 1) % FLAGS.chanelr == 0:
                print('the value of count is: ', count)
                FLAGS.LEARNING_RATE = FLAGS.LEARNING_RATE / FLAGS.div
                print('learning rate now: ', FLAGS.LEARNING_RATE)
                return_lr = FLAGS.LEARNING_RATE

        print(step, loss_t, elapsed_time)
        saver.save(sess, model_path + '/style-model', global_step=step)
        nameofmodel = model_path + '/style-model-' + str(step)
        print(""final time elspased"", (time.time() - start_ti))
        print('Done training -- epoch limit reached')

    return step, return_lr, nameofmodel
```




I have to restart my system and re-run the above code for 5-6 times and then it works fine. And when I run the same code on a different linux machine its work fine all the time. I really don't know what is the issue here.

Thanks in advance. Please comment if need more explanation from my side. 

Here is the stack trace
```

Traceback (most recent call last):
  File ""/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
    return fn(*args)
  File ""/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 775087002
         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,256,256,3], [?,256,256,3], [?], [?,256,256,3]], output_types=[DT_FLOAT, DT_FLOAT, DT_STRI
NG, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""Maincreateall.py"", line 240, in <module>
    main()
  File ""Maincreateall.py"", line 194, in main
    opts_para[""gs_val""], nameofmodel = tm.train_model(opts_para)
  File ""/home/suryabhan/Desktop/New_NST_MAC/teststuff.py"", line 465, in train_model
    gs,_,nameofmodel = start_training(FLAGS)
  File ""/home/suryabhan/Desktop/New_NST_MAC/teststuff.py"", line 277, in start_training
    ig_b, tg_b = sess.run([images, target_ig])
  File ""/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 775087002
         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,256,256,3], [?,256,256,3], [?], [?,256,256,3]], output_types=[DT_FLOAT, DT_FLOAT, DT_STRI
NG, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]

Caused by op 'IteratorGetNext', defined at:
  File ""Maincreateall.py"", line 240, in <module>
    main()
  File ""Maincreateall.py"", line 194, in main
    opts_para[""gs_val""], nameofmodel = tm.train_model(opts_para)
  File ""/home/suryabhan/Desktop/New_NST_MAC/teststuff.py"", line 465, in train_model
    gs,_,nameofmodel = start_training(FLAGS)
  File ""/home/suryabhan/Desktop/New_NST_MAC/teststuff.py"", line 137, in start_training
    target_ig, images, img_name,_ = iterator.get_next()
  File ""/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 370, in get_next
    name=name)), self._output_types,
  File ""/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 1466, in iterator_get_next
    output_shapes=output_shapes, name=name)
  File ""/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

DataLossError (see above for traceback): corrupted record at 775087002
         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,256,256,3], [?,256,256,3], [?], [?,256,256,3]], output_types=[DT_FLOAT, DT_FLOAT, DT_STRI
NG, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Iterator)]]
```


"
21771,/usr/local,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21769,Cannot set name of tf.keras.models,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora Linux 27
- **TensorFlow installed from (source or binary)**: Binary wheel
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6
- **Keras version**: 2.2.0
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See below
- **Mobile device**: N/A

### Describe the problem

The name of models created with the embedded version of Keras cannot be changed, as seen in the example. Raises an Attribute error. The same works with the standalone Keras library.

### Source code / logs
```
EMBEDDED = True
if EMBEDDED:
    from tensorflow.keras.layers import Input, Dense
    from tensorflow.keras.models import Model
else:
    from keras.layers import Input, Dense
    from keras.models import Model

inp = Input((32,))
x = Dense(1)(inp)
model = Model(inp, x)
model.name = 'mymodel'
```
Setting EMBEDDED to False it uses the independent Keras library, and it works.

Here is the full traceback:
```
Traceback (most recent call last):
  File ""/home/david/.PyCharm2018.2/config/scratches/scratch_17.py"", line 13, in <module>
    model.name = 'mymodel'
  File ""/home/david/.virtualenvs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 432, in __setattr__
    super(Network, self).__setattr__(name, value)
AttributeError: can't set attribute
```"
21767,`MirrorStrategy` is not working with `tf.estimator.DNNClassifier`,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Changed from `models/samples/core/get_started/premade_estimator.py`
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.64
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
No
- **TensorFlow installed from (source or binary)**:
Compile from source 1.10
- **TensorFlow version (use command below)**:
1.10
- **Python version**:
2.7.12
- **Bazel version (if compiling from source)**:
0.16.1
- **GCC/Compiler version (if compiling from source)**:
5.4.0
- **CUDA/cuDNN version**:
9.1
- **GPU model and memory**:
2 x TITAN Xp/12GB
- **Exact command to reproduce**:
`python premade_estimator.py`

### Describe the problem
I try to use `MirrorStrategy` to parallel `premade_estimator.py`. The  code is below. I made two changes:
(1) I make a fake input_fn to build (features, labels) from random values.
(2) I add two lines to use `MirrorStrategy` when initializing the `conf` when building `estimator`.

It woks fine if I do not use `MirrosStrategy`(without (2)), but fails as following logs output.

BTW: I also tried the original input_fn, reported the same errors.

Appreciate the help

### Source code / logs

```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import tensorflow as tf

import iris_data


parser = argparse.ArgumentParser()
parser.add_argument('--batch_size', default=32, type=int, help='batch size')
parser.add_argument('--train_steps', default=1000, type=int,
                    help='number of training steps')

CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',
                    'PetalLength', 'PetalWidth', 'Species']

def train_input_fn_fake(features_name, batch_size):
    features_len = len(features_name)
    data_len = 10000
    features = tf.random_uniform([data_len, features_len], minval=0, maxval=1, dtype=tf.float32)
    labels = tf.random_uniform([data_len, 1], minval=0, maxval=2, dtype=tf.int32)
    def map_dnn_input(features, labels):
        features = tf.split(features, features_len)
        features = dict(zip(features_name, features))
        return features, labels
    dataset = tf.data.Dataset.from_tensor_slices((features, labels)).map(map_dnn_input)
    dataset = dataset.shuffle(1000).repeat().batch(batch_size)
    return dataset



def main(argv):
    args = parser.parse_args(argv[1:])

    # Feature columns describe how to use the input.
    my_feature_columns = []
    my_features = []
    for key in CSV_COLUMN_NAMES:
        my_features.append(key)
        my_feature_columns.append(tf.feature_column.numeric_column(key=key))


    # configure multi gpu
    distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)
    config = tf.estimator.RunConfig(train_distribute=distribution)

    # Build 2 hidden layer DNN with 10, 10 units respectively.
    classifier = tf.estimator.DNNClassifier(
        feature_columns=my_feature_columns,
        # Two hidden layers of 10 nodes each
        hidden_units=[100, 100],
        # The model must choose between 3 classes.
        n_classes=3,
        # Config
        config = config
        )

    # Train the Model.
    classifier.train(
        input_fn=lambda:train_input_fn_fake(my_features,
                                            args.batch_size),
        steps=args.train_steps,
        )

    
if __name__ == '__main__':
    tf.logging.set_verbosity(tf.logging.INFO)
    tf.app.run(main)
```



Log output:
```

2018-08-21 12:27:58.645341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 11.91GiB freeMemory: 10.62GiB
2018-08-21 12:27:58.874232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 1 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:02:00.0
totalMemory: 11.91GiB freeMemory: 11.74GiB
2018-08-21 12:27:58.875049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1
2018-08-21 12:27:59.303516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-21 12:27:59.303548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 
2018-08-21 12:27:59.303553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y 
2018-08-21 12:27:59.303557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N 
2018-08-21 12:27:59.303931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/device:GPU:0 with 10261 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-08-21 12:27:59.405388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/device:GPU:1 with 11361 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:02:00.0, compute capability: 6.1)
INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0
INFO:tensorflow:Configured nccl all-reduce.
2018-08-21 12:27:59.555447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1
2018-08-21 12:27:59.555515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-21 12:27:59.555524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 
2018-08-21 12:27:59.555530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y 
2018-08-21 12:27:59.555535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N 
2018-08-21 12:27:59.555693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10261 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-08-21 12:27:59.555813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11361 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:02:00.0, compute capability: 6.1)
INFO:tensorflow:Calling model_fn.
WARNING:tensorflow:Partitioned variables are disabled when using DistributionStrategy.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:batch_all_reduce invoked for batches size = 6 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:Error reported to Coordinator: 
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 175, in _call_for_each_tower
    **merge_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 660, in _distributed_apply
    self._create_slots(var_list)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adagrad.py"", line 73, in _create_slots
    with ops.colocate_with(v):
  File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 4080, in _colocate_with_for_gradient
    with self.colocate_with(op, ignore_existing):
  File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 4132, in colocate_with
    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1285, in internal_convert_to_tensor_or_indexed_slices
    value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1124, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/values.py"", line 445, in _tensor_conversion_mirrored
    assert not as_ref
AssertionError
Traceback (most recent call last):
  File ""premade_estimator.py"", line 84, in <module>
    tf.app.run(main)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""premade_estimator.py"", line 78, in main
    steps=args.train_steps,
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 343, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1166, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 1294, in _train_model_distributed
    self.config)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/distribute.py"", line 857, in call_for_each_tower
    return self._call_for_each_tower(fn, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 418, in _call_for_each_tower
    return _call_for_each_tower(self, fn, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 181, in _call_for_each_tower
    coord.join(threads)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
    yield
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 175, in _call_for_each_tower
    **merge_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 660, in _distributed_apply
    self._create_slots(var_list)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adagrad.py"", line 73, in _create_slots
    with ops.colocate_with(v):
  File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 4080, in _colocate_with_for_gradient
    with self.colocate_with(op, ignore_existing):
  File ""/usr/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 4132, in colocate_with
    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1285, in internal_convert_to_tensor_or_indexed_slices
    value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1124, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/values.py"", line 445, in _tensor_conversion_mirrored
    assert not as_ref
AssertionError

```"
21763,`padding='causal'` is in document but cannot be used with `tf.keras.layers.Conv1D`,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
I use docker tensorflow/tensorflow:1.10.0-devel-gpu-py3
- **TensorFlow version (use command below)**:
1.10.0
- **Python version**:
Python 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
see my gist below

### Describe the problem
In the [document](https://www.tensorflow.org/versions/r1.10/api_docs/python/tf/keras/layers/Conv1D), we can use `causal` option for  `padding`. But `causal` option causes error in r1.10. We should remove `causal` from the document or support `causal`. There are related issues, #14933, #15000 and #15037. But all of them are closed. I think they should be reopen.

### Source code / logs
https://gist.github.com/dhgrs/ca552f5804ddfb4db9669f9189eba76f
"
21762,"""not all arguments converted during string formatting"" in rnn_cell_impl","I believe there's a mistake in the ```BasicLSTMCell.build``` method:

```python
raise ValueError(""Expected inputs.shape[-1] to be known, saw shape: %s""
                       % inputs_shape)
```
will be called with inputs_shape being a tuple and % formatting will interpret this tuple as multiple formatting arguments. Can we change it to

```python
raise ValueError(""Expected inputs.shape[-1] to be known, saw shape: %s""
                       % (inputs_shape,))
```
?"
21761,[XLA] code generation for ARM NEON produce very slow code with tensorflow 1.9.0 and 1.10.0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Cortex A7
- **TensorFlow installed from (source or binary)**: from source
- **TensorFlow version (use command below)**: v1.9.0 and superior
- **Python version**: 3.4
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: 4.9.4
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
tfcompile --graph=network.pb --config=network.config.pbtxt  --cpp_class=Network --out_header=network.h --out_function_object=network.o --xla_enable_fast_math=true --target_triple=armv7a-none-android --target_features=+neon --entry_point=run_network

arm-linux-androideabi-g++ -shared -o test.so network.o

(details of network.pb and network.config.pbtxt are not relevant, The only thing I can say is that it contains 2D Convolution and Dense layer)

### Describe the problem
I use XLA AOT (tfcompile) to compile network down to shared library with tfcompile and it was working great until v1.9.0.
Starting from v1.9.0, I see huge speed penalty (more than 15x slowdown) when executing the network on ARM v7 + NEON. After some investigation, **I think it is pretty clear that v1.9.0 and v1.10.0 do not use specialized neon runtime for matmul and conv** (even when +neon is specified in target_features) as shown in the following objdumps:

- ** With tensorflow v1.8.0 and previous versions
objdump -CT test.so               

test.so:     file format elf32-little

DYNAMIC SYMBOL TABLE:
00000000      DF *UND*	00000000  LIBC        __cxa_finalize
00000000      DF *UND*	00000000  LIBC        __cxa_atexit
00000000      D  *UND*	00000000              **__xla_cpu_runtime_EigenConvF32**
00000000      D  *UND*	00000000              **__xla_cpu_runtime_EigenMatMulF32**
000003c0 g    DF .text	0000abc0  Base        run_network
00672004 g    D  *ABS*	00000000  Base        __bss_start
00672004 g    D  *ABS*	00000000  Base        _end
00672004 g    D  *ABS*	00000000  Base        _edata

- ** With tensorflow v1.9.0 and superior:

objdump -CT test.so 

test.so:     file format elf32-little

DYNAMIC SYMBOL TABLE:
00000000      DF *UND*	00000000  LIBC        __cxa_finalize
00000000      DF *UND*	00000000  LIBC        __cxa_atexit
00000320 g    DF .text	0000cb40  Base        run_network
00674004 g    D  *ABS*	00000000  Base        __bss_start
00674004 g    D  *ABS*	00000000  Base        _end
00674004 g    D  *ABS*	00000000  Base        _edata

I tried to follow the code generation process but got lost in LLVM code generation.
Could you have a look at the problem, please? Any help will be helpful.

### Source code / logs
Running code just call run_network function in the shared library with the correct parameters."
21760,OverflowError: timeout value is too large,"Preliminaries：win10, tensorflow-gpu1.8. I got bugs like this.
Traceback (most recent call last):
  File ""D:/Workspace/PythonProjects/studyProjects/multisensory/src/sep_video.py"", line 450, in <module>
    ig.show(table)
  File ""D:\Workspace\PythonProjects\studyProjects\multisensory\src\aolib\img.py"", line 13, in show
    return imtable.show(*args, **kwargs)
  File ""D:\Workspace\PythonProjects\studyProjects\multisensory\src\aolib\imtable.py"", line 72, in show_table
    html_rows = html_from_rows(table, output_dir)
  File ""D:\Workspace\PythonProjects\studyProjects\multisensory\src\aolib\imtable.py"", line 413, in html_from_rows
    html_rows.append(""<td>"" + ""<td>"".join(html_from_cell(x, output_dir) for x in row))
  File ""D:\Workspace\PythonProjects\studyProjects\multisensory\src\aolib\imtable.py"", line 413, in <genexpr>
    html_rows.append(""<td>"" + ""<td>"".join(html_from_cell(x, output_dir) for x in row))
  File ""D:\Workspace\PythonProjects\studyProjects\multisensory\src\aolib\imtable.py"", line 308, in html_from_cell
    return x.make_html(output_dir)
  File ""D:\Workspace\PythonProjects\studyProjects\multisensory\src\aolib\imtable.py"", line 587, in make_html
    make_video(fname, self.ims, self.fps, sound = self.sound)
  File ""D:\Workspace\PythonProjects\studyProjects\multisensory\src\aolib\imtable.py"", line 498, in make_video
    [(i, x, in_dir, tmp_ext) for i, x in enumerate(ims)])
  File ""D:\Workspace\PythonProjects\studyProjects\multisensory\src\aolib\util.py"", line 2725, in parmap
    ret = pool.map_async(f, xs).get(10000000)
  File ""C:\Anaconda3\envs\tensorflow-gpu\lib\multiprocessing\pool.py"", line 638, in get
    self.wait(timeout)
  File ""C:\Anaconda3\envs\tensorflow-gpu\lib\multiprocessing\pool.py"", line 635, in wait
    self._event.wait(timeout)
  File ""C:\Anaconda3\envs\tensorflow-gpu\lib\threading.py"", line 551, in wait
    signaled = self._cond.wait(timeout)
  File ""C:\Anaconda3\envs\tensorflow-gpu\lib\threading.py"", line 299, in wait
    gotit = waiter.acquire(True, timeout)
OverflowError: timeout value is too large
"
21759,tf.placeholder changes random number sequence generated by a seed,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**
Debian Buster (Debian 10)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**
N/A
- **TensorFlow installed from (source or binary):**
N/A
- **TensorFlow version (use command below):**
1.8.0
- **Python version:**
2.7.14
- **Bazel version (if compiling from source):**
N/A
- **GCC/Compiler version (if compiling from source):**
7.3.0
- **CUDA/cuDNN version:**
N/A
- **GPU model and memory:**
N/A
- **Exact command to reproduce:**
N/A

Here's the code to test. Inserting a placeholder changes the sequence of `a` and `b`.
``` python
tf.set_random_seed(1234)
#c = tf.placeholder(dtype=tf.float32, shape=[10,10])
a = tf.random_uniform([1])
b = tf.random_normal([1])

print(""Session 1"")
with tf.Session() as sess1:
  print(sess1.run(a))  # generates 'A1'
  print(sess1.run(a))  # generates 'A2'
  print(sess1.run(b))  # generates 'B1'
  print(sess1.run(b))  # generates 'B2'

print(""Session 2"")
with tf.Session() as sess2:
  print(sess2.run(a))  # generates 'A1'
  print(sess2.run(a))  # generates 'A2'
  print(sess2.run(b))  # generates 'B1'
  print(sess2.run(b))  # generates 'B2'
```
Few Observations - 
1. I tried adding the line `c = tf.placeholder(dtype=tf.float32, shape=[10,10])` before and after `tf.set_random_seed(1234)`. The issue persists.
2. Insertion of placeholder does not affect the outcome when op level seed is set.

Does the placeholder affect the sequence in which random numbers are generated by graph level seed?"
21758,Libtensorflow bazel build link error LNK2019 on debug mode on Windows ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
     Yes, made change of bazel command in
     tensorflow\tensorflow\tools\ci_build\windows\libtensorflow_cpu.sh
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
     Windows 10 64bit 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
      N/A
- **TensorFlow installed from (source or binary)**:
     source
- **TensorFlow version (use command below)**:
     Tensorflow version 1.10
- **Python version**:
     Python 3.5
- **Bazel version (if compiling from source)**:
     Bazel 0.15.2
- **GCC/Compiler version (if compiling from source)**:
     VS2015(Microsoft Visual Studio 14.0\VC)
- **CUDA/cuDNN version**:
     N/A
- **GPU model and memory**:
     N/A
- **Exact command to reproduce**:
     Code path: tensorflow\tensorflow\tools\ci_build\windows\libtensorflow_cpu.sh
     Bazel command(my code change): 
     bazel --output_user_root=${TMPDIR} build --distinct_host_configuration=false -c **dbg** -- 
     copt=/arch:AVX -s --strip=never --verbose_failures tensorflow:libtensorflow.so 
     tensorflow/tools/lib_package:clicenses_generate 
### Describe the problem
I want to **build libtensorflow on debug mode with bazel on Windows**, and change the corresponding  bazel command from '-c opt' to '-c dbg', but when build process 'action Linking tensorflow:libtensorflow.so', encountered the following link error:
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol 

### Source code / logs
Here comes the specific error log:
[libtensorflow_dbg_error_log.txt](https://github.com/tensorflow/tensorflow/files/2305961/libtensorflow_dbg_error_log.txt)

ERROR: D:/tensorflow/libtensorflow/tensorflow/tensorflow/BUILD:520:1: Linking of rule '//tensorflow:libtensorflow.so' failed (Exit 1120): link.exe failed: error executing command
  cd C:/tmp/k6dssnhw/execroot/org_tensorflow
C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /DLL /SUBSYSTEM:CONSOLE -DEFAULTLIB:advapi32.lib /MACHINE:X64 /DEBUG:FULL @bazel-out/x64_windows-dbg/bin/tensorflow/libtensorflow.so-2.params /DEBUG:FULL /INCREMENTAL:NO
depth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,int,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<int,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@H$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBH$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@H$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
depth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol ""public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,int,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<int,4,1,__int64>,16,struct Eigen::MakePointer>)"" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@H$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBH$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@H$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function ""public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)"" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)
bazel-out/x64_windows-dbg/bin/tensorflow/libtensorflow.so : fatal error LNK1120: 32 unresolved externals
INFO: Elapsed time: 1609.057s, Critical Path: 741.15s"
21756,input and output of @tf.custom_gradient,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: TensorFlow 1.10
- **Python version**: Python 3.6.5 by Anaconda
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.0/ cuDNN 7.1
- **GPU model and memory**: NVIDIA GeForce GTX 1080Ti 11G
- **Exact command to reproduce**: N/A


### Describe the problem
I am confusing about the input and output of [tf.custom_gradient](https://www.tensorflow.org/api_docs/python/tf/custom_gradient).

#### Input

In [doc](https://www.tensorflow.org/api_docs/python/tf/custom_gradient), it says:

 `x` is a `Tensor` or sequence of `Tensor` inputs to the function. But with multiple inputs, instead of taking a sequence of `Tensor`s, function `f` takes `N` positional arguments. I think this is a mistake in documentation. A sequence of `Tensor`s can't be passed to `f` which can be reproduced by code below:
```python3
def self_define_op_multiple_inputs():
    @tf.custom_gradient
    def loss_func(input_):
        x = input_[0]
        label = input_[2]

        def grad(dy):
            return [dy, dy]

        return x - label, grad

    x = tf.range(10, dtype=tf.float32)
    y = tf.range(10, dtype=tf.int32)

    loss = loss_func([x, y])


if __name__ == '__main__':
    self_define_op_multiple_inputs()
```
It will try to convert `[x, y]` to a single `Tensor` and raises a error:
```
/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""/home/hyh/projects/benchmark/test.py"", line 280, in <module>
    self_define_op_multiple_inputs()
  File ""/home/hyh/projects/benchmark/test.py"", line 276, in self_define_op_multiple_inputs
    loss = loss_func([x, y])
  File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py"", line 111, in decorated
    return _graph_mode_decorator(f, *args, **kwargs)
  File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py"", line 124, in _graph_mode_decorator
    args = [ops.convert_to_tensor(x) for x in args]
  File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py"", line 124, in <listcomp>
    args = [ops.convert_to_tensor(x) for x in args]
  File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 998, in convert_to_tensor
    as_ref=False)
  File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1094, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 961, in _autopacking_conversion_function
    return _autopacking_helper(v, inferred_dtype, name or ""packed"")
  File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 903, in _autopacking_helper
    elem))
TypeError: Cannot convert a list containing a tensor of dtype <dtype: 'int32'> to <dtype: 'float32'> (Tensor is: <tf.Tensor 'range_1:0' shape=(10,) dtype=int32>)
```
While change to positional arguments can fix the bug:

```python3
@tf.custom_gradient
    def loss_func(x, label):

        def grad(dy):
            return [dy, dy]
```


Related discussion can be found at [https://stackoverflow.com/questions/51836242/tf-custom-gradient-with-multiple-inputs](https://stackoverflow.com/questions/51836242/tf-custom-gradient-with-multiple-inputs).


#### Output
This is the problem about the output of `grad_fn`.
In doc, `grad_vars` is a `list<Tensor>`  with the derivatives of `Tensor`s in `y` with respect to the variables, and signature is `g(*grad_ys, variables=None)`. 
1. Is `variables` is original `variables` or the gradient of `variables` like `grad_ys`?
2. Return `grad_vars` as a `list<Tensor>` will raise an error:
```python3

def self_define_op_multiple_inputs():
    @tf.custom_gradient
    def loss_func(x):
        w = tf.get_variable(""margin_inner_product_layer/W"", shape=(1,), dtype=tf.float32,
                            initializer=tf.constant_initializer([10]), use_resource=True)

        def grad(dy, variables=None):
            return dy, [variables]  # just for testing

        return tf.multiply(x, w), grad

    x = tf.constant([5], dtype=tf.float32, shape=(1,))

    loss = loss_func(x)
    dl = tf.gradients(loss, x)

    with tf.Session(config=config) as sess:
        derivative = sess.run(dl)
        print(derivative)


if __name__ == '__main__':
    self_define_op_multiple_inputs()

```

It seems like it handles `grad_vars` as a `Tensor`:

```
/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""/home/hyh/projects/benchmark/test.py"", line 259, in <module>
    self_define_op_multiple_inputs()
  File ""/home/hyh/projects/benchmark/test.py"", line 251, in self_define_op_multiple_inputs
    dl = tf.gradients(loss, x)
  File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 596, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 795, in _GradientsHelper
    _LogOpGradients(op, out_grads, in_grads)
  File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 945, in _LogOpGradients
    "", "".join([x.name for x in in_grads if _FilterGrad(x)]))
  File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 945, in <listcomp>
    "", "".join([x.name for x in in_grads if _FilterGrad(x)]))
AttributeError: 'list' object has no attribute 'name'
```
Change `grad_vars` to `Tensor` doesn't work either:
```python3

def self_define_op_multiple_inputs():
    @tf.custom_gradient
    def loss_func(x):
        w = tf.get_variable(""margin_inner_product_layer/W"", shape=(1,), dtype=tf.float32,
                            initializer=tf.constant_initializer([10]), use_resource=True)

        def grad(dy, variables=None):
            return dy, variables  # just for testing

        return tf.multiply(x, w), grad

    x = tf.constant([5], dtype=tf.float32, shape=(1,))

    loss = loss_func(x)
    dl = tf.gradients(loss, x)

    with tf.Session(config=config) as sess:
        derivative = sess.run(dl)
        print(derivative)


if __name__ == '__main__':
    self_define_op_multiple_inputs()
```
"
21755,"Getting Shifted values of Close Price in Tensorflow training, validation and Testing. Python","I am trying to experiment with the Tensorflow. I am supplying the sequence of 5 out of which 4 are input and the 5th value is the output from the close price.
The graphs are plotted according to the respective values of the output as the original and the predicted for the training, validation and Testing.

But the graph shows shifted output. See the graphs:
![file](https://user-images.githubusercontent.com/13446197/44395357-44371b00-a557-11e8-8e22-94c4484fcbac.jpg)

I do not understand what is the problem as I am trying to get the prediction one step ahead of the given input.

Here is the training code that I am trying to run:

```
index_in_epoch = 0;
perm_array  = np.arange(x_train.shape[0])
np.random.shuffle(perm_array)

# function to get the next batch
def get_next_batch(batch_size):
    global index_in_epoch, x_train, perm_array   
    start = index_in_epoch
    index_in_epoch += batch_size

    if index_in_epoch > x_train.shape[0]:
        np.random.shuffle(perm_array) # shuffle permutation array
        start = 0 # start next epoch
        index_in_epoch = batch_size

    end = index_in_epoch
    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]

# parameters
n_steps = seq_len-1 
n_inputs = x_train.shape[2]#4
n_neurons = 500
n_outputs = y_train.shape[1]#4
n_layers = 2
learning_rate = 0.0001
batch_size =10
n_epochs = 1000#200 
train_set_size = x_train.shape[0]
test_set_size = x_test.shape[0]

tf.reset_default_graph()

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_outputs])

layers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons, 
                                 activation=tf.nn.leaky_relu, use_peepholes = True)
         for layer in range(n_layers)]

multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)
rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)

stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) 
stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
outputs = outputs[:,n_steps-1,:] # keep only last output of sequence

loss = tf.reduce_mean(tf.square(outputs - y)) # loss function = mean squared error 
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) 
training_op = optimizer.minimize(loss)

saver = tf.train.Saver()
display = 40
with tf.Session() as sess: 
    sess.run(tf.global_variables_initializer())
    plt.ion()
    fig = plt.figure()
    fig.set_size_inches(30,10)
    ax1 = fig.add_subplot(231)
    line1, = ax1.plot(y_train[:display],color='blue', label='train original',marker=""."")
    line2, = ax1.plot(y_train[:display],color='red', label='train Prediction',marker=""."")
    ax2 = fig.add_subplot(232)
    line3, = ax2.plot(y_valid[:display],color='blue', label='valid original',marker=""."")
    line4, = ax2.plot(y_valid[:display],color='red', label='valid Prediction',marker=""."")
    ax3 = fig.add_subplot(233)
    line5, = ax3.plot(y_test[:display],color='blue', label='valid original',marker=""."")
    line6, = ax3.plot(y_test[:display],color='red', label='valid Prediction',marker=""."")
    ax4 = fig.add_subplot(234)
    candlestick2_ohlc(ax4,df_train_candle.o.values[:display],df_train_candle.h.values[:display],df_train_candle.l.values[:display],df_train_candle.c.values[:display],width=0.6)
    ax5 = fig.add_subplot(235)
    candlestick2_ohlc(ax5,df_valid_candle.o.values[:display],df_valid_candle.h.values[:display],df_valid_candle.l.values[:display],df_valid_candle.c.values[:display],width=0.6)
    ax6 = fig.add_subplot(236)
    candlestick2_ohlc(ax6,df_test_candle.o.values[:display],df_test_candle.h.values[:display],df_test_candle.l.values[:display],df_test_candle.c.values[:display],width=0.6)
    ax1.set_title('Training')
    ax2.set_title('Validation')
    ax3.set_title('Testing')
    ax1.set_xlabel('time')
    ax2.set_xlabel('time')
    ax3.set_xlabel('time')
    ax1.set_ylabel(""Train Reversal"")
    ax2.set_ylabel(""Valid Reversal"")
    ax3.set_ylabel(""Test Reversal"")

    ax4.set_title('Training Candles')
    ax4.set_xlabel('time')
    ax4.set_ylabel(""Train OHLC"")
    ax5.set_title('Validation Candles')
    ax5.set_xlabel('time')
    ax4.set_ylabel(""Valid OHLC"")
    ax6.set_title('Testing Candles')
    ax6.set_xlabel('time')
    ax4.set_ylabel(""Test OHLC"")


    plt.show()
    #if(tf.train.checkpoint_exists(tf.train.latest_checkpoint(""modelsOHLC""))):
     #   saver.restore(sess, tf.train.latest_checkpoint(""modelsOHLC""))
      #  print(tf.train.latest_checkpoint(""modelsOHLC"") + ""Session Loaded for Testing"")
    for iteration in range(int(n_epochs*train_set_size/batch_size)):
        x_batch, y_batch = get_next_batch(batch_size) # fetch the next training batch 
        sess.run(training_op, feed_dict={X: x_batch, y: y_batch}) 
        if iteration % int(1*train_set_size/batch_size) == 0:
            mse_train = loss.eval(feed_dict={X: x_train, y: y_train}) 
            mse_valid = loss.eval(feed_dict={X: x_valid, y: y_valid}) 
            mse_test = loss.eval(feed_dict={X: x_test, y: y_test})
            y_train_pred = sess.run(outputs, feed_dict={X: x_train})
            y_valid_pred = sess.run(outputs, feed_dict={X: x_valid})
            y_test_pred = sess.run(outputs, feed_dict={X: x_test})
            line2.set_ydata(y_train_pred[:display])
            line4.set_ydata(y_valid_pred[:display])
            line6.set_ydata(y_test_pred[:display])
            ax1.set_title(""Training Loss: ""+str(mse_train))
            ax2.set_title(""Validation Loss: ""+str(mse_valid))
            ax3.set_title(""Testing Loss: ""+str(mse_test))
            plt.pause(0.01)
            print('%.2f epochs: MSE train/valid/test = %.10f/%.10f/%.10f'%(
                iteration*batch_size/train_set_size, mse_train, mse_valid,mse_test))
            save_path = saver.save(sess, ""modelsOHLC\\model""+str(iteration)+"".ckpt"")
```

The input data for the above is here

[Input file](https://gist.github.com/JafferWilson/c17a4d70b4aaf839b76bde13f7e32141)

You can access the Jupyter version of the code from here: [Jupyter Version of the Code
](https://gist.github.com/JafferWilson/2f7a2374e7b5ea4f92c2edda8b9f7691)
Please guys let me know what I have missed due to which the graph is getting shifted. I am in an ambiguous situation, please let me know what I can do to correct it.   

I have opened a question on Stackoverflow too, here is the [link for it](https://stackoverflow.com/questions/51945064/getting-shifted-values-of-close-price-in-tensorflow-training-validation-and-tes)."
21754,How to compile tensorflow's C++ library on Ubuntu with GPU support?,"Many answers to show compile tensorflow's C++ library ,but it only support cpu , I want to know the exact method to compile tensorflow's C++ library on Ubuntu with GPU support."
21752,tflite-nnapi sometimes returns error result on same image.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu14.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Xiaomi 8, OnePlus 5T, Oppo.
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0 / 7.1

### Describe the problem

I use tflite in android phone and it return right result. But when setting `tflite.useNNAPI(true)`, the result sometimes is error, while sometimes is right on the same images. See the source code below for details.

### Source code / logs

    private boolean NNAPI = true; # when set to `false` everything is ok, but when set to `true`, sometimes returns error result on the same images.

    private String getResult(final Bitmap bitmap) {
        tflite.setUseNNAPI(NNAPI );

        startTime = System.currentTimeMillis();

        int width = bitmap.getWidth();
        int height = bitmap.getHeight();
        float scale = (float)IMG_HEIGHT / height;
        int nWidth = (int) Math.floor(width * scale);
        int nHeight = (int) Math.floor(height * scale);
        int IMG_WIDTH =  (int) Math.ceil((float)nWidth/BLOCK)*BLOCK;

        Bitmap nBitmap = Bitmap.createScaledBitmap(bitmap, nWidth, nHeight, true);

        int intValues[] = new int[nWidth * nHeight];
        nBitmap.getPixels(intValues, 0, nWidth, 0, 0, nWidth, nHeight);

        ByteBuffer imgData = ByteBuffer.allocateDirect(IMG_WIDTH*IMG_HEIGHT);
        imgData.order(ByteOrder.nativeOrder());

        for (int i = 0; i < IMG_HEIGHT; ++i) {
            for (int k = 0; k < IMG_WIDTH; ++k) {
                if(i >= nHeight || k >= nWidth) {
                    imgData.put((byte)0);
                    continue;
                }

                int val = intValues[i*nWidth+k];
                int b = val & 0xFF;
                int g = (val >> 8) & 0xFF;
                int r = (val >> 16) & 0xFF;

                float gray = (float) (r * 0.30 + g * 0.59 + b * 0.11);
                imgData.put((byte) gray);
            }
        }

        endTime1 = System.currentTimeMillis();

        int MAX_LENGTH = IMG_WIDTH / BLOCK;
        int dims[] = { 1, IMG_HEIGHT, IMG_WIDTH, 1 };
        byte outputs[][] = new byte[MAX_LENGTH][KIND];

        tflite.resizeInput(0, dims);
        tflite.run(imgData, outputs);

        endTime2 = System.currentTimeMillis();

        int indices[] = new int[MAX_LENGTH];
        for(int i=0; i < MAX_LENGTH; ++i) {
            float prob = Byte.MIN_VALUE;
            int index = KIND-1;
            for (int k = 0; k < KIND; ++k) {
                if (outputs[i][k] > prob) {
                    index = k;
                    prob = outputs[i][k];
                }
            }
            indices[i] = index;
        }

        int blank = KIND - 1;
        StringBuilder sb = new StringBuilder();
        for(int i=0; i < MAX_LENGTH; ++i) {
            if(indices[i] == blank) { continue; }

            if(i==0 || indices[i] != indices[i-1]) {
                sb.append(CHARS[indices[i]]);
            }
        }

        endTime3 = System.currentTimeMillis();
        return sb.toString();
    }
"
21751,Why does tf.feature_column.input_layer return my feature values in rearranged order,"I currently am testing some ideas with `LinearClassifier` with `TensorFlow`'s New API. Have created some features using just `tf.feature_column.numeric_column` and feed them into a `tf.feature_column.input_layer` layer as following:

    layer_test = tf.feature_column.input_layer(input_fn('data.csv')[0], feature_columns_raw)

And then simply check if my input features are corrently fed via:

    with tf.Session() as sess:
        init = tf.global_variables_initializer()
        sess.run(init)
        layer_test_val = sess.run(layer_test)
        print(layer_test_val)

But I found if very strange and counterintuitive that column order in `layer_test_val` were completely changed out of my control. It's no big deal if the `LinearClassifier` can eventually get the decent prediction accuracy despite the rearranged features' order. However, if what I need is the learned weights of those features, how can I make sure that the output of 
`weights = lr_classifier.get_variable_value('logits/kernel')` is in the exact order of the features as they are specified."
21750,SwappingPass technology problem in grappler,"System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
Mobile device : N/A
TensorFlow installed from (source or binary): source
TensorFlow version (use command below):1.8.0
Python version: 3.5
Bazel version (if compiling from source):0.10.0
GCC/Compiler version (if compiling from source): c++11
CUDA/cuDNN version: 9/7
GPU model and memory: gtx 1080ti, 11G
Exact command to reproduce: N/A
Describe the problem:

Describe the problem:
1.
if I use this config "" gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.5)"", why the prop.memory_size() is not following the ""per_process_gpu_memory_fraction=0.5"" size ? 

In IdentifySwappingCandidates (memory_optimizer.cc:530) 
 if (mem_usage.used_memory <= prop.memory_size()) {
      continue;
    }
 int64 required_savings = mem_usage.used_memory - prop.memory_size()

The size is determined by the user is also let the tensorflow know to calculate the demand.

2.
Although the tensorflow has a swap function, I found it only for kernel tensors not feature tensors.
If I want to add a new swap data function for feature tensors, can it be added in memory_optimizer.cc file?
"
21749,TypeError: convolution() got multiple values for argument 'weights_regularizer',"I got error like this, how do I fix it, please help me
Traceback (most recent call last):
  File ""D:/Workspace/PythonProjects/studyProjects/multisensory/src/sep_video.py"", line 398, in <module>
    ret = run(arg.vid_file, t, arg.clip_dur, pr, gpus[0], mask = arg.mask, arg = arg, net = net)
  File ""D:/Workspace/PythonProjects/studyProjects/multisensory/src/sep_video.py"", line 294, in run
    net.init()
  File ""D:/Workspace/PythonProjects/studyProjects/multisensory/src/sep_video.py"", line 42, in init
    pr, reuse = False, train = False)
  File ""D:\Workspace\PythonProjects\studyProjects\multisensory\src\sourcesep.py"", line 953, in make_net
    vid_net_full = shift_net.make_net(ims, sfs, pr, None, reuse, train)
  File ""D:\Workspace\PythonProjects\studyProjects\multisensory\src\shift_net.py"", line 419, in make_net
    sf_net = slim.conv2d(sf_net,num_outputs= 64, kernel_size= [65, 1], scope = 'sf/conv1_1', stride = [4, 1], padding='SAME', reuse = reuse) 
  File ""C:\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
  File ""C:\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\contrib\layers\python\layers\layers.py"", line 1154, in convolution2d
    conv_dims=2)
  File ""C:\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\contrib\framework\python\ops\arg_scope.py"", line 183, in func_with_args
    return func(*args, **current_args)
TypeError: convolution() got multiple values for argument 'weights_regularizer'"
36555,Transpose op only supports 1D-4D input arrays,"When I run tflite object detection on android demo, I got this problem:

java.lang.RuntimeException: java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/contrib/lite/kernels/transpose.cc Transpose op only supports 1D-4D input arrays.Node number 148 (TRANSPOSE) failed to prepare.

Thanks "
21748,train_spec.max_steps is ignored by tf.estimator.train_and_evaluate(),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:  CUDA Version 9.1.85
- **GPU model and memory**: Tesla K80
- **Exact command to reproduce**: The following script

```
import tensorflow as tf
tf.logging.set_verbosity(20)
import math
import numpy as np

x = np.array([[1,2,3.3, 2,10], [2,4.6,45,5,3],[5,5,56,2,1],[1,2,3,4,100],[10,2,1,4,50]]).astype(np.float32)
y=np.array([0,0,0,1,1]).astype(np.float32)
batch_size = 2

train_input_fn = tf.estimator.inputs.numpy_input_fn(x={'x': x},
        y=y,
        batch_size=batch_size,
        num_epochs=1,
        shuffle=True
        )

eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={'x': np.array([[1,2,3,1,4], [1,23,4,1,90]]).astype(np.float32)},
        y=np.array([0,1]).astype(np.float32),
        batch_size=batch_size,
        num_epochs=1,
        shuffle=False
        )

dnn = tf.estimator.DNNClassifier([32],
    feature_columns=[tf.feature_column.numeric_column('x', shape=[5])])

steps_per_epoch = math.ceil(len(x) / batch_size)
epochs = 3

train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=epochs * steps_per_epoch)
eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)

tf.estimator.train_and_evaluate(dnn, train_spec, eval_spec)
```

### Describe the problem
tf.estimator.train_and_evaluate() stops after 1 iteration with tf 1.10. It works fine with tf 1.9. Training should occur 9 times (=3 epochs x 3 steps per epoch) in total.

### Source code / logs

The traceback with tf v1.9.0-0-g25c197e023 1.9.0:
```
INFO:tensorflow:Using default config.
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpgn5y57s6
INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpgn5y57s6', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.tr
aining.server_lib.ClusterSpec object at 0x7f29437f6908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 
0, '_num_worker_replicas': 1}
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
2018-08-21 01:06:30.556250: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpgn5y57s6/model.ckpt.
INFO:tensorflow:loss = 11.228095, step = 1
INFO:tensorflow:Saving checkpoints for 3 into /tmp/tmpgn5y57s6/model.ckpt.
INFO:tensorflow:Loss for final step: 0.016822023.
INFO:tensorflow:Calling model_fn.
WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to ""careful_interpolation"" instead.
WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to ""careful_interpolation"" instead.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-08-21-01:06:31
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from /tmp/tmpgn5y57s6/model.ckpt-3
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-08-21-01:06:32
INFO:tensorflow:Saving dict for global step 3: accuracy = 1.0, accuracy_baseline = 0.5, auc = 0.99999905, auc_precision_recall = 0.999999, average_loss = 0.27123037, global_step = 3, label/mean = 0.5, los
s = 0.54246074, precision = 1.0, prediction/mean = 0.4936065, recall = 1.0
...
---Information of steps 1-9 is shown.---
```

With tf 1.10, the program ends after step 3:
```
INFO:tensorflow:Using default config.
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp686id5oc
INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp686id5oc', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f10547942b0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
2018-08-21 01:17:17.979737: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp686id5oc/model.ckpt.
INFO:tensorflow:loss = 6.8426137, step = 1
INFO:tensorflow:Saving checkpoints for 3 into /tmp/tmp686id5oc/model.ckpt.
INFO:tensorflow:Calling model_fn.
WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to ""careful_interpolation"" instead.
WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to ""careful_interpolation"" instead.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-08-21-01:17:19
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from /tmp/tmp686id5oc/model.ckpt-3
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-08-21-01:17:19
INFO:tensorflow:Saving dict for global step 3: accuracy = 1.0, accuracy_baseline = 0.5, auc = 0.99999905, auc_precision_recall = 0.999999, average_loss = 0.371122, global_step = 3, label/mean = 0.5, loss = 0.742244, precision = 1.0, prediction/mean = 0.42290705, recall = 1.0
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3: /tmp/tmp686id5oc/model.ckpt-3
INFO:tensorflow:Loss for final step: 1.7909913.
```"
21747,device_properties_pb2 file was not found under the tensorflow.core.protobuf directory,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21745,Distributed tensorflow worker hangs at TF_CloseSession() when using MonitoredTrainingSession,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  RHEL7 and also Mac OS X 10.13.6
- **TensorFlow installed from (source or binary)**: binary (pip install)
- **TensorFlow version (use command below)**: 1.9.0+
- **Python version**: 2.7 (RHEL7), 3.6 (Mac)
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See below

### Describe the problem
When using `MonitoredTrainingSession` in TensorFlow version 1.9 or higher, I'm seeing the following deadlock/hang (as reported by the `hanging-threads` pip package) when the context manager exits.  Note: I do not see this hang for versions 1.8 or earlier.  Also, note that this does not occur if using the older `tf.train.Supervisor` API.
```
----------     Thread 140682110711616 hangs       ----------
	File ""trainer.py"", line 102, in <module>
		tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
	File ""/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
		_sys.exit(main(argv))
	File ""trainer.py"", line 67, in main
		print(""step: {}"".format(step))
	File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 689, in __exit__
		self._close_internal(exception_type)
	File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 726, in _close_internal
		self._sess.close()
	File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 974, in close
		self._sess.close()
	File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1121, in close
		_WrappedSession.close(self)
	File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 974, in close
		self._sess.close()
	File ""/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 974, in close
		self._sess.close()
	File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 690, in close
		tf_session.TF_CloseSession(self._session)
```

The [code](https://gist.github.com/leewyang/7bf6d0df0328fbc4e2e97275d741a044) that generates this is based on the [Distributed Tensorflow](https://www.tensorflow.org/deploy/distributed) documentation (with a trivial/dummy model).  I start one PS node and two worker nodes on a single box as follows:
```
rm -rf /tmp/train_logs; \
python trainer.py \
     --ps_hosts=localhost:2222 \
     --worker_hosts=localhost:2223,localhost:2224 \
     --job_name=ps --task_index=0

python trainer.py \
     --ps_hosts=localhost:2222 \
     --worker_hosts=localhost:2223,localhost:2224 \
     --job_name=worker --task_index=0

python trainer.py \
     --ps_hosts=localhost:2222 \
     --worker_hosts=localhost:2223,localhost:2224 \
     --job_name=worker --task_index=1
```

I've been able to reproduce this quite consistently on:
- Mac 10.13.6, Python 3.6, TensorFlow 1.10
- RHEL7, Python2.7, TensorFlow 1.9

And the symptom goes away when switching to 1.8 or earlier."
21744,Loading contrib ops in Windows via C API,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
binary: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-1.10.0.zip
- **TensorFlow version (use command below)**:
1.10.0
- **Python version**:
3.6.5
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
MSVC 14.15
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
described below

### Describe the problem
I am trying to load and run a graph with contrib ops (specifically lstm ops) trained in python via the C API. On Linux and Mac, I am able to do this using `TF_LoadLibrary` and the `_lstm_ops.so` from the python wheel. However, doing the same on Windows yields an error status `<path to _lstm_ops.dll> not found`, and I am sure I entered the global path correctly (and thus am confused why the error seems to be with finding the library itself, not with registering the ops).  Is there something I am missing? Any tips/suggestions someone can provide?

Further, is there a better/more supported way of doing this in general?
"
21742,"An exception has occurred, use %tb to see the full traceback.","

### System information
- I copied/pasted the classify_image.py
- window 10 64 bits
- anaconda 64 bits / anaconda navigator 1.8.7 / spyder 3.3.1
- TensorFlow version 1.10 installed form anaconda/python prompt
- **Python version**: 3.5 spyder (installed 3.7)

Hello guys,

I opened classify_image.py in spyder from the master of tensorflow (cloned on my PC before), When I executed the script I have this  message:
![classify](https://user-images.githubusercontent.com/17961650/44364942-9a10b200-a4c8-11e8-9edc-04fd48fb0d60.jpg)

""""""""""""""""""""""""""""""""""""""
runfile('E:/developpements/tensorflow_projects/models-master/tutorials/image/imagenet/classify_image.py', wdir='E:/developpements/tensorflow_projects/models-master/tutorials/image/imagenet')
giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca (score = 0.89632)
indri, indris, Indri indri, Indri brevicaudatus (score = 0.00766)
lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens (score = 0.00266)
custard apple (score = 0.00138)
earthstar (score = 0.00104)
An exception has occurred, use %tb to see the full traceback.

SystemExit

E:\developpements\anaconda\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.
  warn(""To exit: use 'exit', 'quit', or Ctrl-D."", stacklevel=1)
""""""""""""""""""""""""""""""""""""""""""


(tensorflow) C:\Users\david>python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0

thanks,

David."
21741,CUDA 10 release,"Is Cuda toolkit 10 compatible with the latest version of Tensorflow?
Are there any benchmarks?"
21740,"How to fix ValueError: Cannot feed value of shape (0, 0) for Tensor 'Placeholder_85:0', which has shape '(?, 10)'","ValueError: Cannot feed value of shape (0, 0) for Tensor 'Placeholder_85:0', which has shape '(?, 10)'"
21738,Model learns with keras but doesn't learn with tf.keras,"The below code would just work fine on keras,

````python
import tensorflow as tf

from keras.layers import Flatten,Dense,Dropout
from keras.models import Sequential

#from tensorflow.python.keras.layers import Flatten,Dense,Dropout
#from tensorflow.python.keras.models import Sequential

mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

import keras
print(f'tf version = {tf.__version__} ')

model = Sequential([
  Flatten(input_shape=(28,28)),
  Dense(512, kernel_initializer='uniform', activation=tf.nn.relu),
  Dropout(0.2),
  Dense(10, kernel_initializer='uniform', activation=tf.nn.softmax)
])


model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=1)
model.evaluate(x_test, y_test)
````


**KERAS**
````Using TensorFlow backend.
tf version = 1.10.0 
Epoch 1/1
2018-08-20 20:57:06.754898: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA

   32/60000 [..............................] - ETA: 14:00 - loss: 2.3347 - acc: 0.1250
  224/60000 [..............................] - ETA: 2:14 - loss: 2.1174 - acc: 0.3839 
  416/60000 [..............................] - ETA: 1:19 - loss: 1.8965 - acc: 0.4712
  576/60000 [..............................] - ETA: 1:03 - loss: 1.7533 - acc: 0.5365
  ....
 .....
59744/60000 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9308
59936/60000 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9310
60000/60000 [==============================] - 22s 360us/step - loss: 0.2363 - **acc: 0.9310**
````
but if you the import layers from tf.keras instead of the keras(i.e comment lines 3 & 4 and uncomment lines 6 & 7), the weights won't be updated and model doesn't learn at all.

**tf.keras**
````
Using TensorFlow backend.
tf version = 1.10.0 
Epoch 1/1
2018-08-20 21:08:05.112916: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
   32/60000 [..............................] - ETA: 10:26 - loss: 14.1033 - acc: 0.1250
  224/60000 [..............................] - ETA: 1:42 - loss: 14.0464 - acc: 0.1205 
  448/60000 [..............................] - ETA: 58s - loss: 14.3412 - acc: 0.1027 
....
....
59744/60000 [============================>.] - ETA: 0s - loss: 13.2745 - acc: 0.1744
59904/60000 [============================>.] - ETA: 0s - loss: 13.2730 - acc: 0.1745
60000/60000 [==============================] - 19s 323us/step - loss: 13.2708 - **acc: 0.1746**
````

The problem is with **kernel_initalizer**, if you give a unsupported initalizer instead of giving an error tf.keras just fails somewhere. However keras works just fine. 


"
21737,Population Based training with tf.Estimator,"OS Platform and Distribution N/A
TensorFlow installed from N/A
TensorFlow version 1.10
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce N/A
Mobile device N/A

Feature request:

Can you implement population-based training with tf.Estimator? 
https://deepmind.com/blog/population-based-training-neural-networks/"
21736,Is it EEG a tool for Performance tracing open-sourced ?,"As describe in paper 9.2 section [TensorFlow:
Large-Scale Machine Learning on Heterogeneous Distributed Systems](http://download.tensorflow.org/paper/whitepaper2015.pdf):

> We also have an internal tool called EEG (not included in the initial open source release in November, 2015) that we use to collect and visualize very fine-grained information about the exact ordering and performance...

I want to know that Does EEG open source? 
EEG includes 3 tools:

1. Linux ftrace
2. Internal Google tracing tools
3. The CUDA Profiling Tools Interface

And what's the method of ""internal Google tracing tools""?

Have I written custom code:  N/A
OS Platform and Distribution: Ubuntu 16.04
TensorFlow installed from: docker
TensorFlow version: 1.8.0
Bazel version: N/A
CUDA/cuDNN version 9.1, 7.0
GPU model and memory P100, 16GB
Exact command to reproduce N/A
Mobile device N/A"
21735,sparse_categorical_crossentropy of keras is not available,"https://github.com/tensorflow/tensorflow/blob/05f8ea8e9522a3027d4f3f7a54d716bfafed427a/tensorflow/python/keras/metrics.py#L584-L591

I don't know why `sparse_categorical_crossentropy` is NOT decorated by `tf_export` like `binary_accuracy`, `sparse_top_k_categorical_accuracy` and so on.
As a result , we have to explicitly import `sparse_categorical_crossentropy`
 i.e. `from tensorflow.keras.metrics import sparse_categorical_crossentropy`

*additional information*:
Have I written custom code: No
OS Platform and Distribution:  Ubuntu 16.04.5 LTS
TensorFlow installed from: pypi
TensorFlow version: tensorflow-gpu 1.10.0
Bazel version: 0.15.0
CUDA/cuDNN version: CUDA 9.0, cuDNN v7.2.1 
GPU model and memory: nvidia GTX 1080TI, 11GB GDDR5
 Exact command to reproduce: `from tensorflow import keras; metric_func =keras.metrics.sparse_categorical_crossentropy`
Mobile device: N/A
Thanks "
21734,Latest tensorflow build from source requires keras but no where in the docs it is mentioned,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: latest
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.16
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**: 9.2/7.2
- **GPU model and memory**: NVidia Tesla V100/16 GB
- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package with default configuration

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Latest tensorflow build from source requires keras but it is not mentioned in the documentation.

![screenshot from 2018-08-20 19-06-00](https://user-images.githubusercontent.com/1001052/44344133-86a41d80-a4ad-11e8-980a-13430f2a3fce.png)
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21732,Request for functions to handle duplicates in SparseTensors,"`tensorflow` currently handles duplicates in `SparseTensor` by keeping the value for the last index of a repeated coordinate. However, some operations for this would come in quite handy, namely non-max or non-min suppression (keeping the maximum or minimum values for repeated coordinates), sum (summing the values for repeated coordinates) or mean (averaging the values for repeated coordinates). 

There is currently a solution for this in https://stackoverflow.com/questions/38233821/merge-duplicate-indices-in-a-sparse-tensor. It works great, but to be honest it feels a bit clunky to use."
21731,Issue with tensorflow runtime. Failed to load DLL.,"
The issue 
I have installed tensorflow 1.10.0 on my Win 7 machine and trying to do an 
import tensorflow as tf 
gives an ""Dll failed to load"" error. 


### Source code / logs
 Code
import tensorflow as tf

Error on the console
runfile('C:/Users/Teluser/Desktop/Megha/FujitsuCode/test_tensorflow.py', wdir='C:/Users/Teluser/Desktop/Megha/FujitsuCode')
Traceback (most recent call last):

  File ""<ipython-input-1-88270b61ce99>"", line 1, in <module>
    runfile('C:/Users/Teluser/Desktop/Megha/FujitsuCode/test_tensorflow.py', wdir='C:/Users/Teluser/Desktop/Megha/FujitsuCode')

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 705, in runfile
    execfile(filename, namespace)

  File ""C:\ProgramData\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/Teluser/Desktop/Megha/FujitsuCode/test_tensorflow.py"", line 8, in <module>
    import tensorflow as tf

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\ProgramData\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\ProgramData\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

I found many such issues but no exact resolution. Please can anyone help me here to understand what I am missing ?"
21730,The Hessian computation does not work for graphs using `tf.gather`.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Binary.
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: '3.6.5 (default, Mar 29 2018, 03:28:50) \n[GCC 5.4.0 20160609]'
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

```
import tensorflow as tf
import numpy as np
theta = tf.get_variable('theta', shape=[10], dtype=tf.float32, initializer=tf.zeros_initializer)
indices = tf.placeholder(tf.int64, shape=(None, 3))
Y = tf.gather(theta, indices)
loss = tf.reduce_sum(tf.square(Y))
grads = tf.gradients(loss, theta)
with tf.Session() as sess:
   print(sess.run(grads, feed_dict={theta: np.arange(10), indices: np.array([[1,3,6], [2,4,5]])}))

hess = tf.hessians(loss, theta)
with tf.Session() as sess:
   print(sess.run(hess, feed_dict={theta: np.arange(10), indices: np.array([[1,3,6], [2,4,5]])}))
```

### Describe the problem
When trying to compute Hessians with a graph that uses `tf.gather`, it throws a `TypeError: 'IndexedSlices' object is not subscriptable`. This I believe is because the gradients are returned as `IndexedSlice` objects whereas Tensorflow is expecting them to be a normal array.

What I would expect is a sort of 2D `IndexedSlice` object for `hess` such that `hess.values[i, j]` is the second partial derivative of `loss` with respect to the `theta` indices `hess.indices[i]` and `hess.indices[j]`.

### Source code / logs
The traceback:

```TypeError                                 Traceback (most recent call last)
<ipython-input-11-b8b35a8e0799> in <module>()
----> 1 hess = tf.hessians(loss, theta)

~/Documents/Ravenholm/citadel/citadel_env/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in hessians(ys, xs, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)
    999           lambda j, result: (j + 1,
   1000                              result.write(j, gradients(_gradient[j], x)[0])),
-> 1001           loop_vars
   1002       )
   1003 

~/Documents/Ravenholm/citadel/citadel_env/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)
   2814     loop_context = WhileContext(parallel_iterations, back_prop, swap_memory)  # pylint: disable=redefined-outer-name
   2815     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)
-> 2816     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
   2817     return result
   2818 

~/Documents/Ravenholm/citadel/citadel_env/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)
   2638       self.Enter()
   2639       original_body_result, exit_vars = self._BuildLoop(
-> 2640           pred, body, original_loop_vars, loop_vars, shape_invariants)
   2641     finally:
   2642       self.Exit()

~/Documents/Ravenholm/citadel/citadel_env/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)
   2588         structure=original_loop_vars,
   2589         flat_sequence=vars_for_body_with_tensor_arrays)
-> 2590     body_result = body(*packed_vars_for_body)
   2591     if not nest.is_sequence(body_result):
   2592       body_result = [body_result]

~/Documents/Ravenholm/citadel/citadel_env/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in <lambda>(j, result)
    998           lambda j, _: j < n,
    999           lambda j, result: (j + 1,
-> 1000                              result.write(j, gradients(_gradient[j], x)[0])),
   1001           loop_vars
   1002       )

TypeError: 'IndexedSlices' object is not subscriptable
```"
21729,`fit` method of subclassed `tf.keras.Model` doesn't work with multi inputs `tf.data.Dataset` when validation data exsits  ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: '1.11.0-dev20180820'
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: None

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

`fit` method of subclassed `tf.keras.Model` doesn't work with multi inputs `tf.data.Dataset` when validation data exsits  

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import numpy as np
import tensorflow as tf

source = (
    ((tf.constant(np.random.normal(0, 1, (1024, 2)), dtype=tf.float32),
      tf.constant(np.random.normal(0, 1, (1024, 2)), dtype=tf.float32))),
    tf.constant(np.random.randint(0, 2, (1024, 2)), dtype=tf.float32)
)

train = tf.data.Dataset.from_tensor_slices(source).batch(128, drop_remainder=True).repeat(10)
valid = tf.data.Dataset.from_tensor_slices(source).batch(128, drop_remainder=True).repeat(10)


class Model(tf.keras.Model):
    def __init__(self):
        super(Model, self).__init__()
        self.d = tf.keras.layers.Dense(2, activation=""softmax"")

    def call(self, inputs, training=True, mask=None):
        return self.d(inputs[0] + inputs[1])


m = Model()
m.compile(tf.train.AdamOptimizer(0.001),
          loss=[""categorical_crossentropy""])
m.fit(x=train, validation_data=valid, steps_per_epoch=8)
```
log
```
ValueError: ('Error when checking model input: expected no data, but got:', (<tf.Tensor 'IteratorGetNext_1:0' shape=(128, 2) dtype=float32>, <tf.Tensor 'IteratorGetNext_1:1' shape=(128, 2) dtype=float32>))
```"
21728,Can't compile frozen facenet graph (Proper version),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: not mobile
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: clang
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**: `./tfcompile --graph=frozen_20170512-110547.pb --config=frozen_20170512-110547.pbtxt --cpp_class=""my_super_class"" --target_features=""+avx2""`

### Describe the problem
I've tried to compile facenet https://github.com/davidsandberg/facenet frozen graph - 20180408-102900
And I've got this error:
`INVALID ARGUMENTS: Unable to functionalize control flow in graph: Switch ('InceptionResnetV1/Conv2d_1a_3x3/BatchNorm/cond/Switch_1') has operands ('InceptionResnetV1/Conv2d_1a_3x3/BatchNorm/cond/Switch_1/Switch' and 'InceptionResnetV1/Conv2d_1a_3x3/BatchNorm/cond/pred_id') that have different switch depths (1 != 0)
`

Is there any workaround for this? Does Tensorflow support compiling BatchNorm?
### Source code / logs
My .pbtxt:
```
feed {
  id { node_name: ""input"" }
  shape {
    dim { size: 160 }
    dim { size: 160 }
  }
}

fetch {
  id { node_name: ""embeddings"" }
}
```
"
21727,Tensorflow compile error with win10,"### System information
- **OS Win10**
- **TensorFlow version 1.9**:
- **Python version 3.5.2**:
- **swig version 3.0.12**:
- **cmake version 3.12.1**:
- **git version 2.18.0**:

### Describe the problem
When I compiled the Tensorflow with source code, there exist some errors: 

 c_api.cc.obj : error LNK2019: 无法解析的外部符号 ""void __cdecl tensorflow::NewRemoteDevices(class tensorflow::Env *,class tens
orflow::WorkerCacheInterface *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >
const &,class std::function<void __cdecl(class tensorflow::Status const &,class std::vector<class tensorflow::Device *,
class std::allocator<class tensorflow::Device *> > *)>)"" (?NewRemoteDevices@tensorflow@@YAXPEAVEnv@1@PEAVWorkerCacheInt
erface@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$function@$$A6AXAEBVStatus@tensorflow@@PEAV
?$vector@PEAVDevice@tensorflow@@V?$allocator@PEAVDevice@tensorflow@@@std@@@std@@@Z@5@@Z)，该符号在函数 ""class tensorflow::Stat
us __cdecl `anonymous namespace'::GetAllRemoteDevices(class std::vector<class std::basic_string<char,struct std::char_t
raits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char
>,class std::allocator<char> > > > const &,class tensorflow::WorkerCacheInterface *,class std::unique_ptr<class tensorf
low::DeviceMgr,struct std::default_delete<class tensorflow::DeviceMgr> > *)"" (?GetAllRemoteDevices@?A0x87213361@@YA?AVS
tatus@tensorflow@@AEBV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_stri
ng@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@PEAVWorkerCacheInterface@3@PEAV?$unique_ptr@VDeviceMgr@tensor
flow@@U?$default_delete@VDeviceMgr@tensorflow@@@std@@@5@@Z) 中被引用 [D:\Tensorflow_20180816\tensorflow\tensorflow\contrib\
cmake\build\pywrap_tensorflow_internal.vcxproj]
  c_api.cc.obj : error LNK2019: 无法解析的外部符号 ""public: class tensorflow::Status __cdecl tensorflow::SessionMgr::CreateSessi
on(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class tensorflow::Se
rverDef const &,bool)"" (?CreateSession@SessionMgr@tensorflow@@QEAA?AVStatus@2@AEBV?$basic_string@DU?$char_traits@D@std@
@V?$allocator@D@2@@std@@AEBVServerDef@2@_N@Z)，该符号在函数 ""class tensorflow::Status __cdecl `anonymous namespace'::UpdateTFE
_ContextWithServerDef(int,class tensorflow::ServerDef const &,struct TFE_Context *)"" (?UpdateTFE_ContextWithServerDef@?
A0x87213361@@YA?AVStatus@tensorflow@@HAEBVServerDef@3@PEAUTFE_Context@@@Z) 中被引用 [D:\Tensorflow_20180816\tensorflow\tens
orflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj]
  c_api.cc.obj : error LNK2019: 无法解析的外部符号 ""public: class tensorflow::Status __cdecl tensorflow::SessionMgr::WorkerSessi
onForSession(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std:
:shared_ptr<struct tensorflow::WorkerSession> *)"" (?WorkerSessionForSession@SessionMgr@tensorflow@@QEAA?AVStatus@2@AEBV
?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$shared_ptr@UWorkerSession@tensorflow@@@5@@Z)，该符号在函数
""class tensorflow::Status __cdecl `anonymous namespace'::UpdateTFE_ContextWithServerDef(int,class tensorflow::ServerDef
 const &,struct TFE_Context *)"" (?UpdateTFE_ContextWithServerDef@?A0x87213361@@YA?AVStatus@tensorflow@@HAEBVServerDef@3
@PEAUTFE_Context@@@Z) 中被引用 [D:\Tensorflow_20180816\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal
.vcxproj]
  c_api.cc.obj : error LNK2019: 无法解析的外部符号 ""class tensorflow::eager::EagerClientCache * __cdecl tensorflow::eager::NewGr
pcEagerClientCache(class std::shared_ptr<class tensorflow::GrpcChannelCache>)"" (?NewGrpcEagerClientCache@eager@tensorfl
ow@@YAPEAVEagerClientCache@12@V?$shared_ptr@VGrpcChannelCache@tensorflow@@@std@@@Z)，该符号在函数 ""class tensorflow::Status __
cdecl `anonymous namespace'::UpdateTFE_ContextWithServerDef(int,class tensorflow::ServerDef const &,struct TFE_Context
*)"" (?UpdateTFE_ContextWithServerDef@?A0x87213361@@YA?AVStatus@tensorflow@@HAEBVServerDef@3@PEAUTFE_Context@@@Z) 中被引用 [
D:\Tensorflow_20180816\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcxproj]
  D:\Tensorflow_20180816\tensorflow\tensorflow\contrib\cmake\build\Release\pywrap_tensorflow_internal.dll : fatal error
 LNK1120: 4 个无法解析的外部命令 [D:\Tensorflow_20180816\tensorflow\tensorflow\contrib\cmake\build\pywrap_tensorflow_internal.vcx
proj]

can anyone help me, thanks."
21726,assign_moving_average function explodes when smoothing bounded inputs,"### System information
- Problem encountered when running custom code
- Linux Ubuntu 16.04 LTS
- TensorFlow installed from source
- TensorFlow version 1.8.0
- Python 3.5.2 
- Bazel Version 0.11.1
- GCC 4.9.4
- CUDA 9.1.85, CuDNN 7.1.2
- GPU: Gtx1070, 8GB memory

### Describe the problem
In training a network consisting of the architecture described below, I encountered a problem with batch normalization layers. I found that the Keras batch normalization moving average parameters were unstable. While decreasing loss was observed over several iterations, it would regularly explode to a very high value. In observing the batch normalization weights in Tensorboard I notice that the moving mean in batch normalization layers explode at the same iteration of this loss explosion, while the batch statistics remain well behaved as shown in the picture below. This doesn't add up as the moving averages are a stable function of the batch statistics.

I noticed that the only Keras code employed in updating these parameters is the `assign_moving_average` function in `tensorflow/tensorflow/python/training/moving_averages.py`. Looking deeper this function in turn uses the `assign_sub` function in `tensorflow/tensorflow/python/ops/state_ops.py` which by default does not use any locking and hence its behaviour may be undefined. Perhaps this is the cause, and if so, should the default be to ignore locking? How certain is it that this wont run into undefined behaviour?

When I replace the Tensorflow `assign_moving_average` function with a simple handcrafted `(1-self.momentum)*mean + (self.momentum)*self.moving_mean` in the Keras BatchNormalization object, this problem never occurs.

### Source code / logs
For reference I am training a three layer convolutional network with three max pooling layers and three batch normalization layers, followed by two dense layers and two batch normalization layers at the output. The network employs a triplet loss and so the weights are each employed three times prior to back propogation. I hope to get a chance some time in the future to reproduce this problem in a simple example that I can share here.

The `decay` parameter to the tensorflow `assign_moving_average` function in this case is 0.99.

![batchnormweightexplosion](https://user-images.githubusercontent.com/18222703/44322682-51db8c00-a403-11e8-9d1b-f8554b626412.png)
"
21725,"Quantize model: does not have MinMax information, and is not a constant array when quantize the concat op layer","tensorflow verison: 1.10
problem:I will describle this problem in two cases(one can quantize succeed, the other can not success).
tf.concat(
    values,
    axis,
    name='concat'
). This's the concat api.

1.if i use concat like this： tf.concat(values=['t1', 't2'], axis=3). My model can quantize succeed.
2.if i use concat like this： tf.concat(values=['t1'], axis=3).some errors occured: concat does not have MinMax information, and is not a constant array. Cannot proceed with quantization.
t1 and t2 are the layer name.

I quantize my model like this:
bazel-bin/tensorflow/contrib/lite/toco/toco \
--input_file=/home/admin_pc/model_test/output_qt1.pb \
--output_file=/home/admin_pc/model_test/mobilenet_qg.tflite \
--input_fromat=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE \
--inference_type=QUANTIZED_UINT8 \
--input_array=image \
--output_array=Pose/concat_stage7 \
--input_shape=1,224,224,3 \
--change_concat_input_ranges=false"
21724,[license] fft2d's license is overly short and provides no enough declaration,"In order to create tensorflow package for linux distributions, the distributions maintainers need to make sure the software being redistributed are free software. While looking into TensorFlow's dependencies, I found fft2d's license very confusing because it doesn't provide enough detail.

https://github.com/tensorflow/tensorflow/blob/master/third_party/fft2d/LICENSE

I raised a discussion in Debian's legal mailing list https://lists.debian.org/debian-legal/2018/08/msg00005.html , and the two follow-ups said the fft2d license is missing the right to distribute modified code. If modified fft2d indeed cannot be redistributed, then fft2d cannot be considered as a free software.

Apart from that, is it possible to build TensorFlow without fft2d with sensible change to the build system? I haven't looked into the code yet."
21723,I got a keras_applications ModuleNotFoundError when I compile my tensorflow 1.10,"hi, I want compile my tensorflow 1.10 to update. But I got an error when I compile it:
ModuleNotFoundError: No module named 'keras_applications'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines failed build steps.
Anyone has the same error?"
21722,"TensorFlow Samples Do Not Run - Can't get past documented 16358, 17393 regardless version used","Installed without error tensorflow 1.10 as instructed on the tensorflow.org site(https://www.tensorflow.org/install/install_windows)

Executed without error the following sample code from tensorflow.org site (https://www.tensorflow.org/tutorials/keras/basic_text_classification)
import tensorflow as tf
from tensorflow import keras
import numpy as np
print(tf.__version__)
imdb = keras.datasets.imdb

When the next sample line was executed it generated the errors documented under ""import tensorflow failed, ""ImportError: DLL load failed"". Even after install visual studio 2015, Microsoft Visual C++ 2015 Redistributable Update 3.  #17393"" displayed:
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)

I tried uninstalling & installing every version of tensorflow from 1.5 to present version. The error occurred for each version.

In attempting to get this short, simple tensorflow example, I have encountered not only this DLL issue but also:
AttributeError: module 'numpy' has no attribute '__version___'
ModuleNotFoundError: No module named 'keras'
(Request for updating keras/datasets files to r1.5  #16358)

Does not appear the documented tensorflow site sample(s) works.
"
21721,Analysis of target '//tensorflow/python/eager:core' failed; build aborted,"### System information
- MacOS 10.13.4
- commit hash `c894b86481da31c291e6d763f68c9f60a811f7fe`
- Python 2.7.14 in conda virtualenv
- Bazel

```
Build label: 0.15.2-homebrew
Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Jul 17 13:06:19 2018 (1531832779)
Build timestamp: 1531832779
Build timestamp as int: 1531832779
```
- No CUDA
- to reproduce `bazel test //tensorflow/python/...`

### Describe the problem

I have added functionality to tensorflow/tensorflow/python/ops/image_ops_impl.py and corresponding unit tests in tensorflow/tensorflow/python/ops/image_ops_test.py

I originally forked tensorflow from the master branch, made these changes on my local machine, rebased and commit. No changes to C++ code. 

Then I created and activated a virtualenv.

### Source code / logs

```
ERROR: /Users/isaacsultan/Code/tensorflow/third_party/python_runtime/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):
    File ""/Users/isaacsultan/Code/tensorflow/third_party/py/python_configure.bzl"", line 308
        _create_local_python_repository(repository_ctx)
    File ""/Users/isaacsultan/Code/tensorflow/third_party/py/python_configure.bzl"", line 270, in _create_local_python_repository
        _check_python_lib(repository_ctx, python_lib)
    File ""/Users/isaacsultan/Code/tensorflow/third_party/py/python_configure.bzl"", line 213, in _check_python_lib
        _fail((""Invalid python library path: %...))
    File ""/Users/isaacsultan/Code/tensorflow/third_party/py/python_configure.bzl"", line 28, in _fail
        fail((""%sPython Configuration Error:%...)))
Python Configuration Error: Invalid python library path: /usr/local/lib/python2.7/dist-packages
 and referenced by '//third_party/python_runtime:headers'
ERROR: Analysis of target '//tensorflow/python:control_flow_util' failed; build aborted: Analysis failed
INFO: Elapsed time: 4.603s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (8 packages loaded)
FAILED: Build did NOT complete successfully (8 packages loaded)
    currently loading: tensorflow/core ... (2 packages)
```"
21720,Build fails for versions higher than 0.10 whenever MPI support is on.,"I have tried to build TF from source for r1.4, r1.6, r1.8, r1.9 with the same result. It always fails if MPI support is on.

Only r0.10 build does not fail.

Googling, for which revision and which MPI version would work,  has yielded no results."
21719,Protobuf 3.6.1 import issue with tensorflow 1.10,"Version: master/v3.6.1
Language: Python 3.6x64 on Windows 10

Steps to reproduce the behavior:
**Followed instructions mentioned on tensorflow.org's guide on installing tensorflow on Windows
(www.tensorflow.org/install/install_windows)

1. Installed CUDA 9.0 (latest release supported by tf_gpu)
2. Installed cuDNN v7 for (CUDA 9.0)
3. Installed tensorflow_gpu using native-pip
**protobuf is part of tf gpu installation 
4. After the installation, i wanted to test the tf, so as usual tried:
  import tensorflow
  but got an error:==>
  Traceback (most recent call last):
  File ""<pyshell#0>"", line 1, in <module>
    import tensorflow
  File ""C:\Users\Niraj\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Niraj\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Users\Niraj\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\core\framework\graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""C:\Users\Niraj\AppData\Local\Programs\Python\Python36\lib\site-packages\google\protobuf\descriptor.py"", line 47, in <module>
    from google.protobuf.pyext import _message
ImportError: DLL load failed: The specified procedure could not be found.

Initially i thought the issue was related to CUDA or cdnn itself, but after hours of trial & error with multiple cuda & cudnn version combination, i decided to focus on the library mentioned in the error message itself.  
I'm not very new to python extensions so obvious work around for me was to try earlier version of protobuf, so i tried: 

pip uninstall protobuf
pip install protobuf==3.6.0

and tada!! tensorflow started working.
I'm still not sure if this was actually caused by protobuf or incorrect installations etc.
My sincere apologies if this issue is unrelated.
Please let me know if you need more details on this issue."
21718,"Error importing tensorflow.  Unless you are using bazel, you should not try to import tensorflow from its source directory; please exit the tensorflow source tree, and relaunch your python interpreter from there.","Traceback (most recent call last):
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Python Files\csv_to_tfrecord.py"", line 16, in <module>
    import tensorflow as tf
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Ketan Ingale\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there."
21717,parallel_for's jacobian and batch_jacobian fail when inputs are disjoint in the comp. graph,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: reproducible on macOS High Sierra 10.13.4 and Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: reproducible on both source and binary
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.15.2-homebrew
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Mobile device**: N/A
- **Exact command to reproduce**:

```python
import tensorflow as tf
from tensorflow.python.ops.parallel_for import gradients

x = tf.placeholder(tf.float32, (None,))
y = tf.placeholder(tf.float32, (None,))
print(gradients.jacobian(x, y))
```

### Describe the problem
When the example script is run, the following exception is thrown

```
Traceback (most recent call last):
  File ""test-2.py"", line 5, in <module>
    print(gradients.jacobian(x, 5))
  File ""/Users/asobolev/dev/tensorflow/pfor_test/venv/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/gradients.py"", line 58, in jacobian
    pfor_outputs = control_flow_ops.pfor(loop_fn, output_size)
  File ""/Users/asobolev/dev/tensorflow/pfor_test/venv/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py"", line 122, in pfor
    outputs.append(converter.convert(loop_fn_output))
  File ""/Users/asobolev/dev/tensorflow/pfor_test/venv/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/pfor.py"", line 1075, in convert
    output = self._convert_helper(y)
  File ""/Users/asobolev/dev/tensorflow/pfor_test/venv/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/pfor.py"", line 1108, in _convert_helper
    assert isinstance(y, ops.Tensor), y
AssertionError: None
```

This is due to the `tf.gradients(y, x)` (which sits under to hood of `jacobian`) returning `None` when `y` is independent of `x` in terms of computational graph, that is, when `y` is not achievable from `x` by any computation path, and `parallel_for` does not like working with `None`s.

This is not affecting me as I have found workarounds, but it's still a bug that needs fixing."
21714,Error converting custom model to .tflite using Toco,"### System information
- **Have I written custom code**: Yes, see neural net structure [here](https://github.com/gmalsagov/Emotion-Multiclass-CNN/blob/master/cnn-embeddings/cnn_embeddings.py)
- **OS Platform and Distribution**: Mac OS X 10.13.4
- **TensorFlow installed from (source or binary)**: pip (binary)
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**: 0.15.2-homebrew
- **GCC/Compiler version (if compiling from source)**: v1.10.0-rc1-19-g656e7a2b34
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 
toco \
--graph_def_file='path to frozen_model.pb' \
--input_format=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE \
--inference_type=FLOAT \
--input_type=INT \
--input_arrays=input_x \
--output_arrays=output/predictions \
--input_shapes=1,78 \
--output_file='path to model.tflite'

### Describe the problem
I have a Tensorflow Convolutional Neural Network model for Multi-class Sentiment Analysis. I removed Dropout layer from the frozen model and it produces inference fine. When I am trying to convert it into a TfLite version using Toco I am getting an error that has something to do with dimensions of an input tensor (see error below). This tensor leads to a convolution tensor whose function takes a 4d input tensor.

`2018-08-19 15:27:50.977853: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:130] Check failed: input_shape.dimensions_count() == 4 Conv ops require 4D inputs. Input array ""embedding/ExpandDims"" is 3D.`

Output from terminal:

`
2018-08-19 15:27:45.418932: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/tensorflow/venv/bin/toco"", line 11, in <module>
    sys.exit(main())
  File ""/tensorflow/venv/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 370, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/tensorflow/venv/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/tensorflow/venv/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 366, in run_main
    _convert_model(tflite_flags)
  File ""/tensorflow/venv/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py"", line 143, in _convert_model
    output_data = converter.convert()
  File ""/tensorflow/venv/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py"", line 374, in convert
    dump_graphviz_video=self.dump_graphviz_video)
  File ""/tensorflow/venv/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 246, in toco_convert
    input_data.SerializeToString())
  File ""/tensorflow/venv/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py"", line 106, in toco_convert_protos
    (stdout, stderr))
RuntimeError: TOCO failed see console for info.
2018-08-19 15:27:50.968669: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 40 operators, 57 arrays (0 quantized)
2018-08-19 15:27:50.975773: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 40 operators, 57 arrays (0 quantized)
2018-08-19 15:27:50.977853: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:130] Check failed: input_shape.dimensions_count() == 4 Conv ops require 4D inputs. Input array ""embedding/ExpandDims"" is 3D.

None
`

### Source code / logs

I am using tf.expand_dims command only when declaring embedding layer to expand tensor to 4d prior to feeding it into a tf.nn.conv2d function:

`self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)
conv = tf.nn.conv2d(self.embedded_chars, W, strides=[1, 1, 1, 1], padding='VALID', name='conv')
`

Does this mean that tensorflow lite doesn't support expand dims yet? Would appreciate an explanation or any possible walkarounds for my issue.

Thanks"
21713,No module named 'tensorflow.compiler',"```
import tensorflow as tf
tf.contrib.layers.conv2d(inputs=h_pool1,num_outputs=64,kernel_size=[5,5],stride=[1,1],padding='SAME', activation_fn=tf.nn.relu) 
```
When  I executing above code,the errors orrurs,help me please!
env:tensorflow1.9+jupyter notebook +windows10 + python 3.6
```
~\AppData\Roaming\Python\Python36\site-packages\tensorflow\contrib\tpu\python\tpu\tpu_feed.py in <module>()
     26 from six.moves import xrange  # pylint: disable=redefined-builtin
     27 
---> 28 from tensorflow.compiler.xla.experimental.xla_sharding import xla_sharding
     29 from tensorflow.compiler.xla.python_api import xla_shape
     30 from tensorflow.contrib.tpu.python.ops import tpu_ops

ModuleNotFoundError: No module named 'tensorflow.compiler'
```"
21710,GPU cannot detected using Tensor C++ Windows library that was built from sources,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21708,tf.contrib.estimator.InMemoryEvaluatorHook runs every step,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: same code as in #21590
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: from conda, also tried with pip - same effect
- **TensorFlow version (use command below)**: conda: 1.9.0, pip: v1.10.0-0-g656e7a2b34 1.10.0 
- **Python version**: conda: Python 3.6.6 :: Anaconda custom (64-bit), pip: 3.5.2
- **CUDA/cuDNN version**: CUDA Version 9.0.176 
- **GPU model and memory**:  GeForce GTX 1080 Ti 12GB
- **Exact command to reproduce**: python3 cnn_mnist.py

### Describe the problem
When evaluating during training with tf.contrib.estimator.InMemoryEvaluatorHook, regardless of the parameter every_n_iter, evaluation takes place every single step.
```
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Evaluation [1/1]
INFO:tensorflow:Finished evaluation at 2018-08-18-21:39:14
INFO:tensorflow:Saving dict for global step 3: accuracy = 0.5, global_step = 3, loss = 0.6996169
INFO:tensorflow:Starting evaluation at 2018-08-18-21:39:14
INFO:tensorflow:Graph was finalized.
2018-08-18 23:39:14.756353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-18 23:39:14.756388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-18 23:39:14.756407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-08-18 23:39:14.756425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-08-18 23:39:14.756536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10195 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Evaluation [1/1]
INFO:tensorflow:Finished evaluation at 2018-08-18-21:39:14
INFO:tensorflow:Saving dict for global step 4: accuracy = 0.49, global_step = 4, loss = 0.69892335
INFO:tensorflow:Starting evaluation at 2018-08-18-21:39:14
INFO:tensorflow:Graph was finalized.
2018-08-18 23:39:14.881081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-08-18 23:39:14.881101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-18 23:39:14.881106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-08-18 23:39:14.881111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-08-18 23:39:14.881199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10195 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Evaluation [1/1]
INFO:tensorflow:Finished evaluation at 2018-08-18-21:39:14
INFO:tensorflow:Saving dict for global step 5: accuracy = 0.49, global_step = 5, loss = 0.6985285
INFO:tensorflow:Starting evaluation at 2018-08-18-21:39:14
INFO:tensorflow:Graph was finalized.
```
### Source code / logs
Code is the same as in  #21590: 
[cnn_mnist.txt](https://github.com/tensorflow/tensorflow/files/2299878/cnn_mnist.txt)

Same problem exists with my custom estimator.
"
21707,AttributeError: 'AddLocationForm' object has no attribute 'found_location',"### Describe the problem
i am building a kivy  weather app. but suddenly i got an error, that i cant solve it please help me
i am using python version 2 and ubuntu 18.04
my source code and debug log are following bellow.














### Source code 
import json
from kivy.app import App
from kivy.uix.boxlayout import BoxLayout
from kivy.properties import ObjectProperty
from kivy.network.urlrequest import UrlRequest


class AddLocationForm(BoxLayout): 
     search_input = ObjectProperty()
	 
	 
     def search_location(self):  
         search_template = ""http://api.openweathermap.org/data/2.5/find?q={}&type=like""
         search_url = search_template.format(self.search_input.text)
         request = UrlRequest(search_url, self.found_location)  
		 
	 def found_location(self,request,data):
          data = json.loads(data.decode()) if not isinstance(data, dict) else data
         cities = [""{} ({})"".format(d['name'], d['sys']['country'])
            for d in data['list']]
         self.search_results.item_strings = cities 	 
		
class WeatherApp(App):
     pass

if __name__ == '__main__':
     WeatherApp().run()

 ###DEBUGGING log:
   exec(__kvlang__.co_value, idmap)
   File ""/home/midhun/Downloads/W2/weather.kv"", line 16, in <module>
     on_press: root.search_location() 			
   File ""main.py"", line 15, in search_location
     request = UrlRequest(search_url, self.found_location)  
 AttributeError: 'AddLocationForm' object has no attribute 'found_location'

weather.kv:
AddLocationForm:

<AddLocationForm>:
    orientation: ""vertical""
	search_input: search_box
	search_results: search_results_list 
    BoxLayout:
        height: ""40dp""  
        size_hint_y: None
        TextInput:
		    id: search_box
            size_hint_x: 50  
        Button:
            text: ""Search""
            size_hint_x: 25
            on_press: root.search_location() 			
        Button:
            text: ""Current Location""
            size_hint_x: 25  
    ListView:  
        id: search_results_list  
        item_strings: [ ]  
"
21706,ModuleNotFoundError: No module named 'official' Error with Official MNIST Model in Colaboratory Notebook,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

I am using stock example script from Official MNIST Model found here: https://github.com/tensorflow/models/tree/master/official/mnist

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Windows 10

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: 
N/A
- **TensorFlow installed from (source or binary)**:
I don't know
- **TensorFlow version (use command below)**:
1.9
- **Python version**:
3.6
- **Bazel version (if compiling from source)**:
Don't know
- **GCC/Compiler version (if compiling from source)**:
Don't know
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
from official.mnist import dataset

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I have been unable to import the necessary libraries to run the Official MNIST model in a Colaboratory Notebook, although I have been able to run this model successfully using an Anaconda Prompt. I have tried copying the ""Official"" folder found within the Tensorflow directory into the same folder in my google drive where I save my Colaboratory notebooks (in the hopes that this would be the local directory that the notebook would attempt to access when making imports).

How can I import these libraries from the Tensorflow ""Official"" folder in a Colaboratory Notebook? Is it possible to do so? If not, could this be added as a feature?

Thank you!

### Source code / logs
My Colaboratory Notebook can be found [here](https://colab.research.google.com/drive/13Gl9l-xnIL4OeNgd-Wqrb2bMcG10NUF1)

Here are text file attachments containing the code I tried using, as well as the error trace:

[MNIST Colab Code.txt](https://github.com/tensorflow/tensorflow/files/2299747/MNIST.Colab.Code.txt)
[Error Trace - MNIST Colab.txt](https://github.com/tensorflow/tensorflow/files/2299748/Error.Trace.-.MNIST.Colab.txt)
"
21705,tf.parallel_stack,parallel_stack doesn't support TF_BOOL type . is right?
21704,[cmake] deduplicate symbols in libtensorflow.so and pywrap_tensorflow_internal.so,"They contained nearly the same symbols. We can build another shared object shipping common symbols, and the common shared object can be used for both libtensorflow.so and pywrap_tensorflow_internal.so .

This is an example patch, which produces `libtensorflow_.so` as the common shared object. (maybe better named `libtensorflow_internal.so`)

```patch
diff --git a/tensorflow/contrib/cmake/tf_python.cmake b/tensorflow/contrib/cmake/tf_python.cmake
index 6d86daf5f1..ad180396c5 100755
--- a/tensorflow/contrib/cmake/tf_python.cmake
+++ b/tensorflow/contrib/cmake/tf_python.cmake
@@ -577,13 +577,8 @@ if(WIN32)
     )
 endif(WIN32)
 
-# pywrap_tensorflow_internal is a shared library containing all of the
-# TensorFlow runtime and the standard ops and kernels. These are installed into
-# tf_python/tensorflow/python/.
-add_library(pywrap_tensorflow_internal SHARED
-    ${pywrap_tensorflow_internal_src}
+add_library(tensorflow_ SHARED
     $<TARGET_OBJECTS:tf_c>
-    $<TARGET_OBJECTS:tf_c_python_api>
     $<TARGET_OBJECTS:tf_core_lib>
     $<TARGET_OBJECTS:tf_core_cpu>
     $<TARGET_OBJECTS:tf_core_framework>
@@ -601,6 +596,21 @@ add_library(pywrap_tensorflow_internal SHARED
     $<$<BOOL:${tensorflow_ENABLE_GPU}>:$<TARGET_OBJECTS:tf_stream_executor>>
     ${pywrap_tensorflow_deffile}
 )
+target_link_libraries(tensorflow_ PRIVATE
+    ${tf_core_gpu_kernels_lib}
+    ${tensorflow_EXTERNAL_LIBRARIES}
+    tf_protos_cc
+)
+set_target_properties(tensorflow_ PROPERTIES VERSION ""1.10.0"" SOVERSION ""1.10"")
+
+# pywrap_tensorflow_internal is a shared library containing all of the
+# TensorFlow runtime and the standard ops and kernels. These are installed into
+# tf_python/tensorflow/python/.
+add_library(pywrap_tensorflow_internal SHARED
+    ${pywrap_tensorflow_internal_src}
+    $<TARGET_OBJECTS:tf_c_python_api>
+    ${pywrap_tensorflow_deffile}
+)
 
 # There is a bug in GCC 5 resulting in undefined reference to a __cpu_model function when
 # linking to the tensorflow library. Adding the following libraries fixes it.
@@ -618,6 +628,7 @@ target_include_directories(pywrap_tensorflow_internal PUBLIC
 )
 
 target_link_libraries(pywrap_tensorflow_internal PRIVATE
+    tensorflow_
     ${tf_core_gpu_kernels_lib}
     ${tensorflow_EXTERNAL_LIBRARIES}
     tf_protos_cc
```

By using the resulting shared object I can build the `libtensorflow.so` in the build directory with this command
```
g++ ../../../cc/framework/ops.cc ../../../cc/framework/scope.cc -I ../../../../ -I ./eigen/src/eigen -I . -o x -ltensorflow_ -L. -shared -fPIC
```
and the resulting shared object only takes 1mb of disk space.

```
ls -lh x libtensorflow_.so* libpywrap_tensorflow_internal.so                 ~/tensorflow.pkg/tensorflow.orig/tensorflow/contrib/cmake/x
-rwxr-xr-x 1 lumin lumin 5.5M Aug 18 11:37 libpywrap_tensorflow_internal.so
lrwxrwxrwx 1 lumin lumin   22 Aug 18 11:36 libtensorflow_.so -> libtensorflow_.so.1.10
lrwxrwxrwx 1 lumin lumin   24 Aug 18 11:36 libtensorflow_.so.1.10 -> libtensorflow_.so.1.10.0
-rwxr-xr-x 1 lumin lumin 674M Aug 18 11:36 libtensorflow_.so.1.10.0
-rwxr-xr-x 1 lumin lumin 648K Aug 18 11:48 x
```

Is my understanding correct? I can sumbit a PR for this symbol deduplication.

---

```
readelf -d x                                                                 ~/tensorflow.pkg/tensorflow.orig/tensorflow/contrib/cmake/x

Dynamic section at offset 0x68d58 contains 28 entries:
  Tag        Type                         Name/Value
 0x0000000000000001 (NEEDED)             Shared library: [libtensorflow_.so.1.10]
 0x0000000000000001 (NEEDED)             Shared library: [libstdc++.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [libm.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [libgcc_s.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]
```

```
readelf -d libpywrap_tensorflow_internal.so                                  ~/tensorflow.pkg/tensorflow.orig/tensorflow/contrib/cmake/x

Dynamic section at offset 0x3a4790 contains 51 entries:
  Tag        Type                         Name/Value
 0x0000000000000001 (NEEDED)             Shared library: [libgcc_s.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libtensorflow_.so.1.10]
 0x0000000000000001 (NEEDED)             Shared library: [libgif.so.7]
 0x0000000000000001 (NEEDED)             Shared library: [libpng16.so.16]
 0x0000000000000001 (NEEDED)             Shared library: [libjpeg.so.9]
 0x0000000000000001 (NEEDED)             Shared library: [liblmdb.so.0]
 0x0000000000000001 (NEEDED)             Shared library: [libjsoncpp.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libfarmhash.so.0]
 0x0000000000000001 (NEEDED)             Shared library: [libhighwayhash.so.0]
 0x0000000000000001 (NEEDED)             Shared library: [libnsync.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libnsync_cpp.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libprotobuf.so.16]
 0x0000000000000001 (NEEDED)             Shared library: [libre2.so.4]
 0x0000000000000001 (NEEDED)             Shared library: [libsqlite3.so.0]
 0x0000000000000001 (NEEDED)             Shared library: [libz.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libgrpc++.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libgrpc.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [libsnappy.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libpthread.so.0]
 0x0000000000000001 (NEEDED)             Shared library: [libdl.so.2]
 0x0000000000000001 (NEEDED)             Shared library: [libpython3.6m.so.1.0]
 0x0000000000000001 (NEEDED)             Shared library: [libstdc++.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [libm.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [libgomp.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [ld-linux-x86-64.so.2]
 0x000000000000000e (SONAME)             Library soname: [libpywrap_tensorflow_internal.so]
```

```
readelf -d libtensorflow_.so                                                 ~/tensorflow.pkg/tensorflow.orig/tensorflow/contrib/cmake/x

Dynamic section at offset 0x1a5a5ff8 contains 48 entries:
  Tag        Type                         Name/Value
 0x0000000000000001 (NEEDED)             Shared library: [libgif.so.7]
 0x0000000000000001 (NEEDED)             Shared library: [libpng16.so.16]
 0x0000000000000001 (NEEDED)             Shared library: [libjpeg.so.9]
 0x0000000000000001 (NEEDED)             Shared library: [liblmdb.so.0]
 0x0000000000000001 (NEEDED)             Shared library: [libjsoncpp.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libfarmhash.so.0]
 0x0000000000000001 (NEEDED)             Shared library: [libhighwayhash.so.0]
 0x0000000000000001 (NEEDED)             Shared library: [libnsync.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libnsync_cpp.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libprotobuf.so.16]
 0x0000000000000001 (NEEDED)             Shared library: [libre2.so.4]
 0x0000000000000001 (NEEDED)             Shared library: [libsqlite3.so.0]
 0x0000000000000001 (NEEDED)             Shared library: [libz.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libgrpc++.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libgrpc.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [libsnappy.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libpthread.so.0]
 0x0000000000000001 (NEEDED)             Shared library: [libdl.so.2]
 0x0000000000000001 (NEEDED)             Shared library: [libstdc++.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [libm.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [libgomp.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libgcc_s.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [ld-linux-x86-64.so.2]
 0x000000000000000e (SONAME)             Library soname: [libtensorflow_.so.1.10]
```"
21701,Divergence between tf 1.9 macOS vs Win10 ?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21700,Numpy 15.0 support,"It seems numpy newest version has comes out for a long time, but as far as tensorflow 1.10, it not support numpy 15.0 yet.
While pytorch using numpy 15.0 above, this is really awkward... I have to using 2 versions numpy"
21698,INT TFLITE very much slower than FLOAT TFLITE,"Hi Guys. I am using Mobilenet 0.25,128. I used the pretrained models provided in the repo for obtaining the int tflite and float tflite models for the same [found here](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md). I am trying to infer some images. Using the imagenet images and some other test images as well, the FLOAT TFLITE is faster than the INT TFLITE (FLOAT TFLITE takes roughly 3-4 milliseconds while INT one takes 8-9 ms). Any suggestions as to why this is happening ?I am running the inferences using the tflite interpreter following the documentation [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md#interpreter). This issue happens with the tf-nightly builds as well as the normal tensorflow. Tried on both, and also on both python 2 as well as 3.
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:Binary
- **TensorFlow version (use command below)**:1.10 (tf-nightly) 
- **Python version**:2
- **Bazel version (if compiling from source)**
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: No
- **GPU model and memory**: No



### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs

# ####INT TFLITE CODE####

```
#Load TFLite model and allocate tensors. Select the appropriate model and give its path
interpreter = tf.contrib.lite.Interpreter(model_path='mobilenet_v1_0.25_128_quant.tflite')



#For allocating the model tensors
interpreter.allocate_tensors()

#Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
#print(input_details)
#print(output_details)

k=1
for image_path in TEST_IMAGE_PATHS:
	#Loading the image and resizing into the correct shape
	#Change the .astype(np.uint8) (used when using int tflite) to .astype(np.float32) if using float tflite
	img = np.array(PIL.Image.open(image_path).resize((128, 128))).astype(np.uint8)

	img = img.reshape(1,128,128,3)
	#print(input_details)
	#print (img.shape)
	
	#Setting the input image to the input tensor
	interpreter.set_tensor(input_details[0]['index'], img)

	#Running inference and timing it
	start = time.time()
	interpreter.invoke()
	end = time.time()

	#Getting the output information
	output_data = interpreter.get_tensor(output_details[0]['index'])

	#Converting the prediction into human readable form
	#label_map = imagenet.create_readable_names_for_imagenet_labels()  
	print(""Top 1 Prediction: "", output_data.argmax()-1, output_data.max(), k+1,labels[output_data.argmax()-1])
	
	total_infer_time+=(end-start)
	k+=1

#printing average inference time along with the accuracy
print(""Total infer time avg (in seconds) == "",total_infer_time/no_images)



```
# ####FLOAT TFLITE CODE####
```
#Load TFLite model and allocate tensors. Select the appropriate model and give its path
interpreter = tf.contrib.lite.Interpreter(model_path='mobilenet_v1_0.25_128.tflite')


#For allocating the model tensors
interpreter.allocate_tensors()

#Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
#print(input_details)
#print(output_details)
k = 1           
for image_path in TEST_IMAGE_PATHS:
	print image_path
	#Loading the image and resizing into the correct shape
	#Change the .astype(np.uint8) (used when using int tflite) to .astype(np.float32) if using float tflite
	img = np.array(PIL.Image.open(image_path).resize((128, 128))).astype(np.float32) / 128 - 1

	img = img.reshape(1,128,128,3)
	#print (img.shape)
	
	#Setting the input image to the input tensor
	interpreter.set_tensor(input_details[0]['index'], img)

	#Running inference and timing it
	start = time.time()
	interpreter.invoke()
	end = time.time()

	#Getting the output information
	output_data = interpreter.get_tensor(output_details[0]['index'])

	#Converting the prediction into human readable form
	#label_map = imagenet.create_readable_names_for_imagenet_labels()  
	#print(""Top 1 Prediction: "", output_data.argmax()-1, output_data.max(), k+1,labels[output_data.argmax()-1])
	
	#detect_dog(output_data)


	'''
	#For accuracy
	number = []
	number = int(mobilenet_labels[k])
	        

	if(number==output_data.argmax()):
	    accuracy+=1

	'''
	total_infer_time+=(end-start)
	print total_infer_time/(k+1)
	k+=1

#printing average inference time along with the accuracy
print(""Total infer time avg (in seconds) == "",total_infer_time/no_images)

``````

```"
21697,ProfilerHook generates wrong timeline traces,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A
- **TensorFlow installed from (source or binary)**:dockerhub 1.9.0-devel-gpu
- **TensorFlow version (use command below)**:1.9.0
- **Python version**:2.7.12
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:9.0/7.1
- **GPU model and memory**:Tesla V100 16GB
- **Exact command to reproduce**:
        model：https://github.com/tensorflow/models/tree/master/research/deep_speech
        command:python deep_speech.py --model_dir=./deepSpeech --train_data_dir=./librispeech_data/train-clean-100/LibriSpeech/train-clean-100.csv --eval_data_dir=./librispeech_data/dev-clean/LibriSpeech/dev-clean.csv --num_gpus=-1 --wer_threshold=0.23 --seed=1 --hooks=profilerhook**
     
### Describe the problem
     The time of the timeline trace  is not correct， which lasts hundreds of years.It's correct at the beginning, and becoming abnormal from the third trace（the 21th step for save_steps=10）.   

### Source code / logs
![image](https://user-images.githubusercontent.com/4105051/44328172-71ba9080-a493-11e8-8926-92249de87f76.png)

"
21696,TensorFlow GDR RDMA verbs compilation failure on 1.10 ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux 7.4.1708
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 7.2.0
- **CUDA/cuDNN version**: 9.2.1.88
- **GPU model and memory**: nVidia Volta V100 16GB
- **Exact command to reproduce**: bazel build --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

I attempted to compile TensorFlow 1.10.0 using RDMA/VERBS options+GDR options as I'm using an all infiniband EDR network with my Volta V100 HPC environment. The same compile-time strings for Bazel build worked correctly without this error in RC 1.0.8rc1 and this error was not thrown. I am using mvpachi2 + GNU GCC 7.2.0, CUDA 9.2.8.11 + Basel 0.16.0 at compile time. NCCL is at v 2.2.13. The error basel throws after 16xxx objects is as follows:
                                                                                                                                      ^
```
At global scope:
cc1plus: warning: unrecognized command line option '-Wno-self-assign'
ERROR: /opt/ohpc/pub/apps/tensorflow_1.10/tensorflow/BUILD:576:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Aborted): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)
2018-08-18 16:25:22.109083: F tensorflow/core/framework/allocator_registry.cc:52] New registration for AllocatorFactory with name=BFCRdmaAllocator priority=101 at location tensorflow/contrib/gdr/gdr_memory_manager.cc:204 conflicts with previous registration at location tensorflow/contrib/verbs/rdma_mgr.cc:277

INFO: Elapsed time: 934.788s, Critical Path: 236.40s
INFO: 13405 processes: 13405 local.
FAILED: Build did NOT complete successfully

```

### Source code / logs
The above is about all I've gotten out of it, thus far. There is no obvious compile time log output from Basel-build that I can see of use. 
"
21695,"TensorFlow not logging any event files, tensorboard showing nothing","I'm currently running `TensorFlow 1.10.0`. My custom estimator is created with `tf.estimator.Estimator`, and run without a glitch. However, I don't find any event files under `model_dir `, and TensorBoard simply shows nothing.

Here's how I setup my estimator:
```
classifier= tf.estimator.Estimator(
    model_fn=lr_model_fn, model_dir=PATH)
```
and I open TensorBoard via `tensorboard --logdir=tf_models/ --host=127.0.0.1` where `tf_models` is the dir `PATH`.

Files logged in `model_dir ` include `checkpoint`, `graph.pbtxt`, `model.ckpt-*`, etc. No `events.out.tfevents*` files reside there. Is this normal? Did I have something misconfigured?

EDIT: 
It seems that it only outputs `events.out.tfevents.*` file the very first time in a Jupyter session. If I clean up the PATH `model_dir` and re-train the model whatever times afterwards, there's always a events file missing there. And I have to restart Jupyter to make it work again. Is this a bug or something?"
21693,GPU Kernels for the RandomUniform Op,@alextp Is there a particular reason why there is an int32 type constraint on the shape input to the RandomUniform op GPU kernels? It's currently forcing some ops on the CPU and due to initialization colocation constraints this propagates to some variables and their updates making training about twice as slow on a machine with a Pascal 1080. Thanks!
21687,ERROR: /bin/bash: line 1: 12830 Segmentation fault  ,"Hi,
i'm trying to install tensorflow with CUDA support and I have the following issue whe i execute
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures

ERROR:

/bin/bash: line 1: 12830 Segmentation fault      bazel-out/host/bin/tensorflow/create_tensorflow.python_api --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/k8-opt/genfiles/tensorflow --apiname=tensorflow --apiversion=1 --package=tensorflow.python --output_package=tensorflow bazel-out/k8-opt/genfiles/tensorflow/__init__.py bazel-out/k8-opt/genfiles/tensorflow/app/__init__.py bazel-out/k8-opt/genfiles/tensorflow/bitwise/__init__.py bazel-out/k8-opt/genfiles/tensorflow/compat/__init__.py bazel-out/k8-opt/genfiles/tensorflow/data/__init__.py bazel-out/k8-opt/genfiles/tensorflow/debugging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/distributions/__init__.py bazel-out/k8-opt/genfiles/tensorflow/dtypes/__init__.py bazel-out/k8-opt/genfiles/tensorflow/errors/__init__.py bazel-out/k8-opt/genfiles/tensorflow/feature_column/__init__.py bazel-out/k8-opt/genfiles/tensorflow/gfile/__init__.py bazel-out/k8-opt/genfiles/tensorflow/graph_util/__init__.py bazel-out/k8-opt/genfiles/tensorflow/image/__init__.py bazel-out/k8-opt/genfiles/tensorflow/io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/initializers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/activations/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/densenet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/inception_resnet_v2/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/inception_v3/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/mobilenet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/nasnet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/resnet50/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/vgg16/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/vgg19/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/applications/xception/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/backend/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/callbacks/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/constraints/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/datasets/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/datasets/boston_housing/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/datasets/cifar10/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/datasets/cifar100/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/datasets/fashion_mnist/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/datasets/imdb/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/datasets/mnist/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/datasets/reuters/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/estimator/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/initializers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/layers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/losses/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/metrics/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/models/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/optimizers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/preprocessing/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/preprocessing/image/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/preprocessing/sequence/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/preprocessing/text/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/regularizers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/wrappers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/keras/wrappers/scikit_learn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/layers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/linalg/__init__.py bazel-out/k8-opt/genfiles/tensorflow/logging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/losses/__init__.py bazel-out/k8-opt/genfiles/tensorflow/manip/__init__.py bazel-out/k8-opt/genfiles/tensorflow/math/__init__.py bazel-out/k8-opt/genfiles/tensorflow/metrics/__init__.py bazel-out/k8-opt/genfiles/tensorflow/nn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/nn/rnn_cell/__init__.py bazel-out/k8-opt/genfiles/tensorflow/profiler/__init__.py bazel-out/k8-opt/genfiles/tensorflow/python_io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/quantization/__init__.py bazel-out/k8-opt/genfiles/tensorflow/resource_loader/__init__.py bazel-out/k8-opt/genfiles/tensorflow/strings/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/builder/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/loader/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/main_op/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/signature_constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/signature_def_utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/tag_constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/saved_model/utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/sets/__init__.py bazel-out/k8-opt/genfiles/tensorflow/sparse/__init__.py bazel-out/k8-opt/genfiles/tensorflow/spectral/__init__.py bazel-out/k8-opt/genfiles/tensorflow/summary/__init__.py bazel-out/k8-opt/genfiles/tensorflow/sysconfig/__init__.py bazel-out/k8-opt/genfiles/tensorflow/test/__init__.py bazel-out/k8-opt/genfiles/tensorflow/train/__init__.py bazel-out/k8-opt/genfiles/tensorflow/train/queue_runner/__init__.py bazel-out/k8-opt/genfiles/tensorflow/user_ops/__init__.py
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 4401.213s, Critical Path: 251.23s
INFO: 7900 processes: 7900 local.
FAILED: Build did NOT complete successfully

OS: CentOS 6.2
Tensorflow installed from source.
Tensorflow version: 1.10
Python version: 2.7
Bazel version: 0.16.0
GCC version: 6.3.0
CUDA 8.0
cuDNN: 6.0
GPU: GeForce GTX 770"
21686,Error to compile tflite bechmark (rpi cross build) ,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.10
- **Exact command to reproduce**: ./tensorflow/contrib/lite/build_rpi_lib.sh

I'm having problems compilling tflite for rpi. 

```
./tensorflow/contrib/lite/build_rpi_lib.sh
+ set -e
+++ dirname ./tensorflow/contrib/lite/build_rpi_lib.sh
++ cd ./tensorflow/contrib/lite
++ pwd
+ SCRIPT_DIR=/home/thom/workspace/tensorflow/tensorflow/contrib/lite
+ cd /home/thom/workspace/tensorflow/tensorflow/contrib/lite/../../..
+ CC_PREFIX=arm-linux-gnueabihf-
+ make -j 3 -f tensorflow/contrib/lite/Makefile TARGET=RPI TARGET_ARCH=armv7
/bin/sh: 1: [[: not found
/home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/bin/rpi_armv7/benchmark_model
arm-linux-gnueabihf-g++ -O3 -DNDEBUG --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/../../../ -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/ -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/tools/benchmark/benchmark_tflite_model.cc -o /home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/tools/benchmark/benchmark_tflite_model.o
arm-linux-gnueabihf-g++ -O3 -DNDEBUG --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/../../../ -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/ -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/tools/benchmark/benchmark_model.cc -o /home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/tools/benchmark/benchmark_model.o
In file included from ./tensorflow/contrib/lite/tools/benchmark/benchmark_model.h:26:0,
                 from tensorflow/contrib/lite/tools/benchmark/benchmark_model.cc:16:
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h: In instantiation of ‘tflite::benchmark::TypedBenchmarkParam<T>::TypedBenchmarkParam(const T&) [with T = int]’:
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:39:50:   required from ‘static std::unique_ptr<tflite::benchmark::BenchmarkParam> tflite::benchmark::BenchmarkParam::Create(const T&) [with T = int]’
tensorflow/contrib/lite/tools/benchmark/benchmark_model.cc:53:65:   required from here
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:53:20: error: ‘static tflite::benchmark::BenchmarkParam::ParamType tflite::benchmark::BenchmarkParam::GetValueType() [with T = int]’ is private
   static ParamType GetValueType();
                    ^
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:62:39: error: within this context
       : BenchmarkParam(GetValueType<T>()), value_(value) {}
                                       ^
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h: In instantiation of ‘tflite::benchmark::TypedBenchmarkParam<T>::TypedBenchmarkParam(const T&) [with T = float]’:
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:39:50:   required from ‘static std::unique_ptr<tflite::benchmark::BenchmarkParam> tflite::benchmark::BenchmarkParam::Create(const T&) [with T = float]’
tensorflow/contrib/lite/tools/benchmark/benchmark_model.cc:54:67:   required from here
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:53:20: error: ‘static tflite::benchmark::BenchmarkParam::ParamType tflite::benchmark::BenchmarkParam::GetValueType() [with T = float]’ is private
   static ParamType GetValueType();
                    ^
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:62:39: error: within this context
       : BenchmarkParam(GetValueType<T>()), value_(value) {}
                                       ^
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h: In instantiation of ‘tflite::benchmark::TypedBenchmarkParam<T>::TypedBenchmarkParam(const T&) [with T = std::__cxx11::basic_string<char>]’:
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:39:50:   required from ‘static std::unique_ptr<tflite::benchmark::BenchmarkParam> tflite::benchmark::BenchmarkParam::Create(const T&) [with T = std::__cxx11::basic_string<char>]’
tensorflow/contrib/lite/tools/benchmark/benchmark_model.cc:56:75:   required from here
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:53:20: error: ‘static tflite::benchmark::BenchmarkParam::ParamType tflite::benchmark::BenchmarkParam::GetValueType() [with T = std::__cxx11::basic_string<char>]’ is private
   static ParamType GetValueType();
                    ^
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:62:39: error: within this context
       : BenchmarkParam(GetValueType<T>()), value_(value) {}
                                       ^
tensorflow/contrib/lite/Makefile:204: recipe for target '/home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/tools/benchmark/benchmark_model.o' failed
make: *** [/home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/tools/benchmark/benchmark_model.o] Error 1
make: *** Waiting for unfinished jobs....
In file included from ./tensorflow/contrib/lite/tools/benchmark/benchmark_model.h:26:0,
                 from ./tensorflow/contrib/lite/tools/benchmark/benchmark_tflite_model.h:25,
                 from tensorflow/contrib/lite/tools/benchmark/benchmark_tflite_model.cc:16:
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h: In instantiation of ‘tflite::benchmark::TypedBenchmarkParam<T>::TypedBenchmarkParam(const T&) [with T = std::__cxx11::basic_string<char>]’:
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:39:50:   required from ‘static std::unique_ptr<tflite::benchmark::BenchmarkParam> tflite::benchmark::BenchmarkParam::Create(const T&) [with T = std::__cxx11::basic_string<char>]’
tensorflow/contrib/lite/tools/benchmark/benchmark_tflite_model.cc:167:74:   required from here
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:53:20: error: ‘static tflite::benchmark::BenchmarkParam::ParamType tflite::benchmark::BenchmarkParam::GetValueType() [with T = std::__cxx11::basic_string<char>]’ is private
   static ParamType GetValueType();
                    ^
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:62:39: error: within this context
       : BenchmarkParam(GetValueType<T>()), value_(value) {}
                                       ^
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h: In instantiation of ‘tflite::benchmark::TypedBenchmarkParam<T>::TypedBenchmarkParam(const T&) [with T = bool]’:
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:39:50:   required from ‘static std::unique_ptr<tflite::benchmark::BenchmarkParam> tflite::benchmark::BenchmarkParam::Create(const T&) [with T = bool]’
tensorflow/contrib/lite/tools/benchmark/benchmark_tflite_model.cc:172:74:   required from here
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:53:20: error: ‘static tflite::benchmark::BenchmarkParam::ParamType tflite::benchmark::BenchmarkParam::GetValueType() [with T = bool]’ is private
   static ParamType GetValueType();
                    ^
./tensorflow/contrib/lite/tools/benchmark/benchmark_params.h:62:39: error: within this context
       : BenchmarkParam(GetValueType<T>()), value_(value) {}
                                       ^
tensorflow/contrib/lite/Makefile:204: recipe for target '/home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/tools/benchmark/benchmark_tflite_model.o' failed
make: *** [/home/thom/workspace/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/tools/benchmark/benchmark_tflite_model.o] Error 1
```
"
21685,Bazel build for llvm fails in sandboxed mode,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.9.0, 1.10.0, master
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: 6.3
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**:
- **Exact command to reproduce**:
```
$ bazel build @llvm//:orc_jit --spawn_strategy=sandboxed
INFO: Analysed target @llvm//:orc_jit (0 packages loaded).
INFO: Found 1 target...
ERROR: /home/sguo/.cache/bazel/_bazel_sguo/312e4bfd87ddc7075e69d12d6c5f1e44/external/llvm/BUILD.bazel:1641:1: C++ compilation of rule '@llvm//:orc_jit' failed (Exit 1)
external/llvm/lib/ExecutionEngine/Orc/CompileOnDemandLayer.cpp:12:40: fatal error: llvm/Bitcode/BitcodeWriter.h: No such file or directory
 #include ""llvm/Bitcode/BitcodeWriter.h""
                                        ^
compilation terminated.
Target @llvm//:orc_jit failed to build
Use --verbose_failures to see the command lines of failed build steps.
```
```
$ bazel build @llvm//:analysis --spawn_strategy=sandboxed
INFO: Analysed target @llvm//:analysis (0 packages loaded).
INFO: Found 1 target...
ERROR: /home/sguo/.cache/bazel/_bazel_sguo/312e4bfd87ddc7075e69d12d6c5f1e44/external/llvm/BUILD.bazel:598:1: C++ compilation of rule '@llvm//:analysis' failed (Exit 1)
external/llvm/lib/Analysis/MemoryBuiltins.cpp:24:39: fatal error: llvm/Analysis/Utils/Local.h: No such file or directory
 #include ""llvm/Analysis/Utils/Local.h""
                                       ^
compilation terminated.
Target @llvm//:analysis failed to build
Use --verbose_failures to see the command lines of failed build steps.
```

### Describe the problem

The bazel build targets for llvm are missing header file declarations. They only build when Bazel doesn't enforce a sandboxed environment. When sandboxing in enabled, the builds fail.

### Source code / logs

"
21684,"Two optimizers, with two assign ops on all .get_variable variables when using biderctional_dynamic_rnn","### Environment Information
```

== cat /etc/issue ===============================================
Linux nazariy-box 4.15.0-32-generic #35~16.04.1-Ubuntu SMP Fri Aug 10 21:54:34 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux nazariy-box 4.15.0-32-generic #35~16.04.1-Ubuntu SMP Fri Aug 10 21:54:34 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                              1.14.3     
numpydoc                           0.8.0      
protobuf                           3.5.2.post1
tensorflow                         1.8.0      

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/home/nazariy/anaconda/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH /usr/local/lib:/usr/lib/lp_solve/lp_solve_5.5
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: 105: tf_env_collect.sh: nvidia-smi: not found

== cuda libs  ===================================================
/usr/local/MATLAB/R2017b/bin/glnxa64/libcudart.so.8.0.44
```
### Additional System Questions
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.1
TensorFlow installed from (source or binary): pip
TensorFlow version (use command below): 1.8.0
Python version: 3.6.5
Bazel version (if compiling from source): none
GCC/Compiler version (if compiling from source): none
CUDA/cuDNN version: none
GPU model and memory: none
Exact command to reproduce: none
Mobile device: No

### Problem outline
I have written an architecture whereby I have attention layers and in the middle I have a `bidirectional_dynamic_rnn` with forward and backward GRU cells like so:

```
class TensorGRU(object):

    def __init__(self, inputs, hidden_size, dtype=tf.float64):
        with tf.name_scope(""GRULayer""):
            self.GRU_forward = tf.nn.rnn_cell.GRUCell(num_units=hidden_size, activation=tf.nn.tanh, name=""GRU_Forward"")
            self.GRU_backward = tf.nn.rnn_cell.GRUCell(num_units=hidden_size, activation=tf.nn.tanh, name=""GRU_Backward"")

            self.GRU_bi, _ = tf.nn.bidirectional_dynamic_rnn(self.GRU_forward, self.GRU_backward, inputs=inputs,
                                                             dtype=dtype)
            self.h = tf.concat([self.GRU_bi[0], self.GRU_bi[1]], axis=2, name='h')
            tf.summary.histogram('GRU_Output', self.h)
```
As soon as I add the `tf.nn.bidirectional_dymaic_rnn` line I get two optimizers (Adam in my case) ""attached"" to every `.get_variable()` variable in my code. For example, see the screenshot provided from Tensorboard Debugger:
<img width=""204"" alt=""issue_1"" src=""https://user-images.githubusercontent.com/13678461/44277083-6f60f800-a241-11e8-8e20-fe0b7ac85fe2.PNG"">
and from graph:
<img width=""548"" alt=""issue_2"" src=""https://user-images.githubusercontent.com/13678461/44277095-77b93300-a241-11e8-9c04-f378b8409e22.PNG"">

Here is the MLP code for your convenience, but there is nothing special there:

```
class MLP(object):

    # incoming is V, which is [batch, 2*hidden]
    def __init__(self, n_input, n_hidden_1, n_hidden_2, n_classes, dtype=tf.float64,
                 input_layer=None, activation_func=None):

        if input_layer is None:
            self.input_layer = tf.placeholder(dtype=dtype, shape=(None, n_input),
                                              name=""PL_Output"")
        else:
            if isinstance(input_layer, tf.Tensor):
                self.input_layer = input_layer
            else:
                self.input_layer = tf.convert_to_tensor(input_layer, name=""PL_Output"")

        if activation_func is None:
            self.activation_func = tf.tanh
        else:
            self.activation_func = activation_func

        with tf.variable_scope(""MLP_Variables""):
            self.weights = {
                'l1': tf.get_variable(""MLP_Layer_1_Weights"", dtype=dtype, shape=(n_input, n_hidden_1),
                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),
                'l2': tf.get_variable(""MLP_Layer_2_Weights"", dtype=dtype, shape=(n_hidden_1, n_hidden_2),
                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),
                'out': tf.get_variable(""MLP_Layer_Out_Weights"", dtype=dtype, shape=(n_hidden_2, n_classes),
                                       initializer=tf.contrib.layers.xavier_initializer(dtype=dtype))
            }

            self.biases = {
                'l1': tf.get_variable(""MLP_Layer_1_Biases"", dtype=dtype, shape=(n_hidden_1,),
                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),
                'l2': tf.get_variable(""MLP_Layer_2_Biases"", dtype=dtype, shape=(n_hidden_2,),
                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),
                'out': tf.get_variable(""MLP_Layer_Out_Biases"", dtype=dtype, shape=(n_classes,),
                                       initializer=tf.contrib.layers.xavier_initializer(dtype=dtype))
            }

        self.layer_1 = None
        self.layer_2 = None
        self.layer_out = None
        self.expected_output = None

        self.multilayer_perceptron()

    def multilayer_perceptron(self):
        self.layer_1 = tf.add(tf.matmul(self.input_layer, self.weights['l1']), self.biases['l1'])
        self.layer_1 = self.activation_func(self.layer_1)

        self.layer_2 = tf.add(tf.matmul(self.layer_1, self.weights['l2']), self.biases['l2'])
        self.layer_2 = self.activation_func(self.layer_2)

        self.layer_out = tf.add(tf.matmul(self.layer_2, self.weights['out']), self.biases['out'])
        self.layer_out = self.activation_func(self.layer_out)

        self.expected_output = tf.nn.softmax(self.layer_out, axis=1)

        return self.expected_output
```

**I would expect that each variable only has one optimizer**."
21683,Error while freezing a graph,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:No
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:1.10
- **Python version**:3.6
- **Bazel version (if compiling from source)**:0.16.0
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:bazel-bin/tensorflow/python/tools/freeze_graph \ --input_graph= /home/my-model/premade_model105/graph.pbtxt \ --input_checkpoint = /home/my-model/premade_model105/model.ckpt-1000.meta \ --output_graph=/home/my-model/premade_model105/frozen_graph.pb \ --output_node_names= dnn/logits/BiasAdd\ --input_binary=false 


### Describe the problem
I am trying to freeze a graph an filling the parameter input_checkpoint with a valid path but I am still getting this error and following the instructions of the official page:

_The input_checkpoint should be the most recent saved checkpoint. As mentioned in the checkpoint section, you need to give the common prefix to the set of checkpoints here, rather than a full filename._

but Iam getting this error:

### **Input checkpoint '' doesn't exist!**


I am using a DNN premade classifier
"
21680,FixedLengthRecordDataset does not support compression_type,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.8.0-4337-gbfcfad55b7 1.9.0-rc0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: 0.14.1
- **GCC/Compiler version (if compiling from source)**: 7.2.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

TextLineDataset and TFRecordDataset both support a ""compression_type"" argument, allowing compressed files to be read.  However, FixedLengthRecordDataset does not for some reason.  Would adding this be feasible?"
21679,How to clip weights of a dense layer between every step of training op?,"Is there any possible way to do a custom op (for example, to clip by values of the weights of a dense layer manually every training step)

    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
        weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='logits/kernel')[0]
        clip_op = tf.assign(weights, tf.clip_by_value(weights, 0.01, 0.1))
        train_op = optimizer.minimize(
            loss=loss,
            global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

Above, I want to clip the weights of `logits/kernel` layer between every training op, but it does not work as intended, the weights of that layer would still drift outside the range of (0.01, 0.1). I wonder what I'm missing here."
21676,embedding_ops_test fails on AVX512 builds,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.8.0-6288-g335336a', '1.10.0-rc1')
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**: [bazel release 0.15.0]
- **GCC/Compiler version (if compiling from source)**: g++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A 
- **Exact command to reproduce**:

bazel test --config=opt -- //tensorflow/python/kernel_tests:embedding_ops_test

### Describe the problem

The test case fails on AVX512 builds.  See the logs below.

### Source code / logs

```
======================================================================
FAIL: testTransform (__main__.EmbeddingLookupTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/user/.cache/bazel/_bazel_user/0c0d18d29bdc5ed463543206f26b0a12/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/embedding_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/embedding_ops_test.py"", line 609, in testTransform
    self.assertAllEqual(simple, sharded)
  File ""/home/user/.cache/bazel/_bazel_user/0c0d18d29bdc5ed463543206f26b0a12/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/embedding_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1466, in assertAllEqual
    np.testing.assert_array_equal(a, b, err_msg=msg)
  File ""/home/user/.local/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py"", line 855, in assert_array_equal
    verbose=verbose, header='Arrays are not equal')
  File ""/home/user/.local/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py"", line 779, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Arrays are not equal

(mismatch 16.6666666667%)
 x: array([[2., 2., 2.],
       [2., 2., 2.],
       [2., 2., 2.],
       [2., 2., 2.]])
 y: array([[2., 2., 2.],
       [2., 2., 2.],
       [2., 2., 2.],
       [2., 2., 2.]])

----------------------------------------------------------------------
Ran 2 tests in 3.798s

FAILED (failures=1)
not equal where =  (array([2, 3]), array([0, 1]))
not equal lhs =  [2. 2.]
not equal rhs =  [2. 2.]
~
```

"
21675,op type not registered 'DenseToDenseSetOperation' when running tfcompile,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (code from maskrcnn, and export.py to freeze my model)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Laptop
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: ('v1.9.0-rc2-2314-g3a99980', '1.10.0')
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0 20160609
- **CUDA/cuDNN version**: no cuda
- **GPU model and memory**:  Geforce GT 730M, 1Gb memory
- **Exact command to reproduce**: 

`bazel-bin/tensorflow/compiler/aot/tfcompile --graph=/home/mehdi/Desktop/data/resnet101_model.pb --config=/home/mehdi/Desktop/data/config.pbtxt --cpp_class=""foo::bar"" --out_function_object=""./out_model.o"" --out_header=""./out.h"" --out_metadata_object=""./out_helper.o""`

I have been testing [mask rcnn](https://github.com/matterport/Mask_RCNN) in order to do object detection and segmentation on images. It worked quite well and now I am trying to export my model to a standalone binary that can be used without having to install tensorflow on my target system. In order to do this I tried to use tfcompile. The first step was to take my .h5 keras model and converted it to a frozen tf .pb file. I used [this script](https://github.com/ericj974/Mask_RCNN/blob/master/scripts/export.py) that was especially written to convert mask rcnn models. I then downloaded the source code of tensorflow, compiled it by running

`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --jobs 3`

before that, I set support XLA JIT to true when running configure.
After compilation was done I installed tensorflow and it was importable from python.

Then in order to compile my model I run:
`bazel-bin/tensorflow/compiler/aot/tfcompile --graph=/home/mehdi/Desktop/data/resnet101_model.pb --config=/home/mehdi/Desktop/data/config.pbtxt --cpp_class=""foo::bar"" --out_function_object=""./out_model.o"" --out_header=""./out.h"" --out_metadata_object=""./out_helper.o""`

where resnet101_model.pb is the result of running export.py on my keras .h5 model.
config.pgtxt is as follows:

```
# Each feed is a positional input argument for the generated function.  The order
# of each entry matches the order of each input argument.  Here “x_hold” and “y_hold”
# refer to the names of placeholder nodes defined in the graph.
feed {
  id { node_name: ""input_image"" }
  shape {
    dim { size: 320 }
    dim { size: 320 }
    dim { size: 4 }
  }
}

# Each fetch is a positional output argument for the generated function.  The order
# of each entry matches the order of each output argument.  Here “x_y_prod”
# refers to the name of a matmul node defined in the graph.
fetch {
  id { node_name: ""mrcnn_mask/Reshape_1"" }
}
```

Unfortunately I can't upload my model as Github only allows 10Mb. But [here](https://github.com/matterport/Mask_RCNN/releases) is a similar model trained on the same network  (mask_rcnn_balloon.h5)

tfcompile fails with the following error:

`2018-08-16 12:38:27.078016: F tensorflow/compiler/aot/tfcompile_main.cc:150] Non-OK-status: status status: Not found: Op type not registered 'DenseToDenseSetOperation' in binary running on mehdi-t440p. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) tf.contrib.resampler should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. Aborted`

This is difficult to understand for me as DenseToDenseSetIOperation is existing and registered in tensorflow/core/ops/set_ops.cc (line 35).
I searched for tutorials or documentation, even watched the video presentation made by tensorflow guys, but there is no troubleshooting support anywhere. So I thought Github is the right place for this kind of problem.



"
21673,Feature Request: make grid image summary adapt to different channel number,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.10.0
- **Python version**:
3.6
- **Bazel version**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
N/A
- **Mobile device**:
N/A

### Describe the problem

In image-to-image translation, sometimes we need to transform a grey image to a colored one and vice versa. In this case, image summary need to concatenate different images with different number of channels together. In tensorflow.contrib.gan standard library, this will produce a bug.

I request to add an automatic mechanism to complete image channel, e.g. tile up gery image to have 3 channels. It's quite simple, and I write a workaround code. Hope this can help you add this feature.

### Source code / logs

First, a compulsory function for converting channel number. Notice it's input is a 3D image (H, W, C).

```python
def convert_channel(image):
  H, W = image.get_shape()[:2]
  image = array_ops.tile(image, [1, 1, math_ops.maximum(1, 4 - array_ops.shape(image)[2])])
  image = array_ops.slice(image, [0, 0, 0], [H, W, 3])
  return image
```

Second, apply this before any concatenation of image summary.

Thank you.

"
21671, ./tensorflow/contrib/lite/java/build_aar_for_release.sh command build .so library failed ,"### Output information for the command :
```
++ mktemp -d
+ TMPDIR=/tmp/tmp.1epU8vtRBz
+ trap 'rm -rf /tmp/tmp.1epU8vtRBz' EXIT
+ VERSION=1.0
+ BUILDER=bazel
+ BASEDIR=tensorflow/contrib/lite
+ CROSSTOOL=//external:android/crosstool
+ HOST_CROSSTOOL=@bazel_tools//tools/cpp:toolchain
+ BUILD_OPTS='--cxxopt=--std=c++11 -c opt'
+ CROSSTOOL_OPTS='--crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain'
+ test -d tensorflow/contrib/lite
+ rm -rf /tmp/tmp.1epU8vtRBz
+ mkdir -p /tmp/tmp.1epU8vtRBz/jni
+ build_basic_aar /tmp/tmp.1epU8vtRBz
+ local OUTDIR=/tmp/tmp.1epU8vtRBz
+ bazel build --cxxopt=--std=c++11 -c opt tensorflow/contrib/lite/java:tensorflowlite.aar
INFO: Build options have changed, discarding analysis cache.
INFO: Analysed target //tensorflow/contrib/lite/java:tensorflowlite.aar (0 packages loaded).
INFO: Found 1 target...
Target //tensorflow/contrib/lite/java:tensorflowlite.aar up-to-date:
  bazel-bin/tensorflow/contrib/lite/java/tensorflowlite.aar
INFO: Elapsed time: 0.499s, Critical Path: 0.00s
INFO: 0 processes.
INFO: Build completed successfully, 1 total action
+ unzip -d /tmp/tmp.1epU8vtRBz bazel-bin/tensorflow/contrib/lite/java/tensorflowlite.aar
Archive:  bazel-bin/tensorflow/contrib/lite/java/tensorflowlite.aar
  inflating: /tmp/tmp.1epU8vtRBz/AndroidManifest.xml  
  inflating: /tmp/tmp.1epU8vtRBz/classes.jar  
   creating: /tmp/tmp.1epU8vtRBz/res/
  inflating: /tmp/tmp.1epU8vtRBz/R.txt  
+ sed -i -e 's/<application>/<uses-sdk android:targetSdkVersion=""25""\/><application>/' /tmp/tmp.1epU8vtRBz/AndroidManifest.xml
+ build_arch arm64-v8a arm64-v8a /tmp/tmp.1epU8vtRBz
+ local ARCH=arm64-v8a
+ local CONFIG=arm64-v8a
+ local OUTDIR=/tmp/tmp.1epU8vtRBz
+ mkdir -p /tmp/tmp.1epU8vtRBz/jni/arm64-v8a/
+ bazel build --cxxopt=--std=c++11 -c opt --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=arm64-v8a tensorflow/contrib/lite/java:libtensorflowlite_jni.so
INFO: Build options have changed, discarding analysis cache.
INFO: Analysed target //tensorflow/contrib/lite/java:libtensorflowlite_jni.so (0 packages loaded).
INFO: Found 1 target...
ERROR: /home/tclxa/tensorflow/tensorflow/contrib/lite/profiling/BUILD:37:1: C++ compilation of rule '//tensorflow/contrib/lite/profiling:time' failed (Exit 1)
In file included from tensorflow/contrib/lite/profiling/time.cc:15:
./tensorflow/contrib/lite/profiling/time.h:18:10: fatal error: 'cstdint' file not found
#include <cstdint>
         ^
1 error generated.
Target //tensorflow/contrib/lite/java:libtensorflowlite_jni.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 0.575s, Critical Path: 0.15s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
+ rm -rf /tmp/tmp.1epU8vtRBz
```
when I try again, this information would changed.
```
.....
.....
.....
ERROR: /home/tclxa/tensorflow/tensorflow/contrib/lite/BUILD:272:1: C++ compilation of rule '//tensorflow/contrib/lite:util' failed (Exit 1)
In file included from tensorflow/contrib/lite/util.cc:15:
./tensorflow/contrib/lite/util.h:24:10: fatal error: 'vector' file not found
#include <vector>
         ^
1 error generated.
Target //tensorflow/contrib/lite/java:libtensorflowlite_jni.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
....
....
```

It`s a kind of problem.  I try to change gcc-5.4 into gcc-4.7 and g++-5.4 into g++-4.7 .  but its don`t work. so how to solve this problem,  very thanks."
21670,freeze_graph returns with error,"I used `bazel-bin/tensorflow/python/tools/freeze_graph` to freeze my graph. This program complied with bazel several minutes ago.
But this returns with error:

```
Traceback (most recent call last):
  File ""/home/roche/git/download/tensorflow-1.10.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py"", line 48, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/roche/git/download/tensorflow-1.10.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 63, in <module>
    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
  File ""/home/roche/git/download/tensorflow-1.10.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/framework_lib.py"", line 52, in <module>
    from tensorflow.python.framework.importer import import_graph_def
  File ""/home/roche/git/download/tensorflow-1.10.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/importer.py"", line 27, in <module>
    from tensorflow.python.framework import function
  File ""/home/roche/git/download/tensorflow-1.10.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/function.py"", line 39, in <module>
    from tensorflow.python.ops import variable_scope as vs
  File ""/home/roche/git/download/tensorflow-1.10.0/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py"", line 181, in <module>
    class _ReuseMode(enum.Enum):
NameError: name 'enum' is not defined
```

Here's my environment specification:
Sys:Debian 9.5
python:python 3.5.3
Tensorflow: tensorflow (1.10.0) from pip3
bazel:0.16.1
gcc:gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516

tensorflow souce code from 1.10.0 release.
"
21669,no device type supports both of those nodes and the other nodes colocated with them.,"    clipped_gradients, _ = tf.clip_by_global_norm(gradients, 1.0)
  File ""D:\anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\ops\clip_ops.py"", line 222, in clip_by_global_norm
    use_norm = global_norm(t_list, name)
  File ""D:\anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\ops\clip_ops.py"", line 161, in global_norm
    half_squared_norms.append(gen_nn_ops.l2_loss(v))
  File ""D:\anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 4662, in l2_loss
    ""L2Loss"", t=t, name=name)
  File ""D:\anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""D:\anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\framework\ops.py"", line 3290, in create_op
    op_def=op_def)
  File ""D:\anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\framework\ops.py"", line 1654, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access


InvalidArgumentError (see above for traceback): Cannot colocate nodes 'model/optimizer/global_norm/L2Loss_17' and 'model/optimizer/gradients/model/inference/CudnnRNN_1_grad/CudnnRNNBackprop' because no device type supports both of those nodes and the other nodes colocated with them.
Colocation Debug Info:
Colocation group had the following types and devices:
CudnnRNNBackprop: GPU
Identity:
L2Loss: CPU

Colocation members and user-requested devices:
  model/optimizer/gradients/model/inference/CudnnRNN_1_grad/CudnnRNNBackprop (CudnnRNNBackprop)
  model/optimizer/gradients/model/inference/CudnnRNN_1_grad/tuple/control_dependency_3 (Identity)
  model/optimizer/global_norm/L2Loss_17 (L2Loss)

         [[Node: model/optimizer/global_norm/L2Loss_17 = L2Loss[T=DT_FLOAT, _class=[""loc:@model/optimizer/gradients/model/inference/CudnnRNN_1_grad/CudnnRNNBackprop""]](model/optimizer/gradients/model/inference/CudnnRNN_1_grad/tuple/control_dependency_3)]]

Why? and how to fix it, thanks
"
21668,Constrastive Loss Docs and Implementation,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
The current contrastive loss documentation in tf.contrib.losses.metric_learning says that the embedding must be l2 normalized. While it works, there are also other networks that do not l2 normalize their embeddings. Also there are other normalization one can use to make the network converge. I think it should be written as a note or a hint rather then being a requirement.

Also I think it would be nice if we can change the distance function used inside the computation with the euclidean distance being the default.

"
21667,Automatically remove garbage-collected objects from TF graph,"**Setting up tensorflow graphs in python feels strange since the graph keeps old and unused parts.
Would it be possible to remove garbage-collected operations/tensors from the Tensorflow graph?**

When I tried to create my first graphs, I very soon ran into the problem of leftover placeholders when simply re-entering code parts.
I still have to reset my whole graph in order to change it without side effects.

Programming with tensorflow would feed way more ""pythonic"", if I could add and remove parts to the graph by creating and deleting python objects.


------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: none
- **GCC/Compiler version (if compiling from source)**: none
- **CUDA/cuDNN version**: none
- **GPU model and memory**: none
- **Exact command to reproduce**: none"
21663,how to train the network with mini-batches in eager execution?,"
### Describe the problem
I find a simple code from tensorflow documents to train a simple neural network by eager execution.
I need to know that how should I modify the code to train the network with mini-batches?
### Source code / logs
from __future__ import absolute_import, division, print_function
import tensorflow as tf
import numpy as np
tfe = tf.contrib.eager
NUM_EXAMPLES = 1000
training_inputs = tf.random_normal([NUM_EXAMPLES])
noise = tf.random_normal([NUM_EXAMPLES])
training_outputs = training_inputs * 3 + 2 

def prediction(input, weight, bias):
  return input * weight + bias

def loss(weights, biases):
  error = prediction(training_inputs, weights, biases) - training_outputs
  return tf.reduce_sum(tf.square(error))

def grad(weights, biases):
  with tf.GradientTape() as tape:
    loss_value = loss(weights, biases)
  return tape.gradient(loss_value, [weights, biases])

train_steps = 20000
learning_rate = 0.01
W = tfe.Variable(5.)
B = tfe.Variable(10.)
optimizer=tf.train.AdamOptimizer(0.1)

for i in range(train_steps):
    grads=grad(W,B)
    optimizer.apply_gradients(zip(grads,[W,B]),global_step=tf.train.get_or_create_global_step())
    if i % 20 == 0:
     print(""Loss at step {:03d}: {:.3f}"".format(i, loss(W, B)))
"
21661,Reusing pretrained model in my own model,"Hello,
I am creating a tensorflow model with a new session and you will find below the pseudocode for it

```
inp = tf.placeholder()   #----- placeholder for model1
# conv and relu layers
intermediate = conv2d()  # ----- an intermediate output that I want to use in model2
# conv and relu layers
loss = tf.reduce_mean( sigmoid_cross_entropy_with_logits( predicted , real ) )

sess1.run( loss , feed_dict = { inp : images } )
```
Now I want to use a pretrained model and add the output of the final layer in the loss function. So for example,
```

sess2 = tf.Session()
load_model( sess2 )
```
The pretrained model has its own placeholders and I would like to send the value in my intermediate tensor to the placeholder of the pretrained model. Thus, I want to run the pretrained model in every batch and evaluate its prediction.

The problem is that on searching I found out that we cannot feed_dict tensors to placeholders. Is there any other way I can go about and evaluate the second model everytime the first model learns? I asked this question on Stackoverflow and haven't received any responses.

Thanks,"
21659,Estimator Save Model,How to save model based on eval's accuracy in Estimator
21657,"error when use ""tf.get_variable_scope().reuse_variables()""","@lukaszkaiser 

import tensorflow as tf
import tensorflow.contrib.slim as slim
import numpy as np
from scipy.io import loadmat, savemat
import scipy.spatial.distance as ssd
from sbir_sampling import triplet_sampler_asy
from sbir_util import *
from ops import spatial_softmax, reshape_feats
import os, errno

NET_ID = 0 #0 for step3 pre-trained model, 1 for step2 pre-trained model


def attentionNet(inputs, pool_method='sigmoid'):
    assert(pool_method in ['sigmoid', 'softmax'])
    with slim.arg_scope([slim.conv2d],
                        activation_fn=tf.nn.relu,
                        weights_initializer=tf.truncated_normal_initializer(0.0, 0.1),
                        weights_regularizer=slim.l2_regularizer(0.0005),
                        trainable=True):
        net = slim.conv2d(inputs, 256, [1, 1], padding='SAME', scope='conv1')
        if pool_method == 'sigmoid':
            net = slim.conv2d(net, 1, [1, 1], activation_fn=tf.nn.sigmoid, scope='conv2')
        else:
            net = slim.conv2d(net, 1, [1, 1], activation_fn=None, scope='conv2')
            net = spatial_softmax(net)
    return net


def sketch_a_net_sbir(inputs, trainable):
    with slim.arg_scope([slim.conv2d, slim.fully_connected],
                        activation_fn=tf.nn.relu,
                        weights_initializer=tf.truncated_normal_initializer(0.0, 0.1),
                        weights_regularizer=slim.l2_regularizer(0.0005),
                        trainable=False):
        with slim.arg_scope([slim.conv2d], padding='VALID'):
            # x = tf.reshape(inputs, shape=[-1, 225, 225, 1])
            conv1 = slim.conv2d(inputs, 64, [15, 15], 3, scope='conv1_s1')
            conv1 = slim.max_pool2d(conv1, [3, 3], scope='pool1')
            conv2 = slim.conv2d(conv1, 128, [5, 5], scope='conv2_s1')
            conv2 = slim.max_pool2d(conv2, [3, 3], scope='pool2')
            conv3 = slim.conv2d(conv2, 256, [3, 3], padding='SAME', scope='conv3_s1')
            conv4 = slim.conv2d(conv3, 256, [3, 3], padding='SAME', scope='conv4_s1')
            conv5 = slim.conv2d(conv4, 256, [3, 3], padding='SAME', scope='conv5_s1')  # trainable=trainable
            conv5 = slim.max_pool2d(conv5, [3, 3], scope='pool3')
            conv5 = slim.flatten(conv5)
            fc6 = slim.fully_connected(conv5, 512, trainable=trainable, scope='fc6_s1')
            fc7 = slim.fully_connected(fc6, 256, activation_fn=None, trainable=trainable, scope='fc7_sketch')
            fc7 = tf.nn.l2_normalize(fc7, dim=1)
    return fc7


def sketch_a_net_dssa(inputs, trainable):
    with slim.arg_scope([slim.conv2d, slim.fully_connected],
                        activation_fn=tf.nn.relu,
                        weights_initializer=tf.truncated_normal_initializer(0.0, 0.1),
                        weights_regularizer=slim.l2_regularizer(0.0005),
                        trainable=False):  # when test 'trainable=True', don't forget to change it
        with slim.arg_scope([slim.conv2d], padding='VALID'):
            # x = tf.reshape(inputs, shape=[-1, 225, 225, 1])
            conv1 = slim.conv2d(inputs, 64, [15, 15], 3, scope='conv1_s1')
            conv1 = slim.max_pool2d(conv1, [3, 3], scope='pool1')
            conv2 = slim.conv2d(conv1, 128, [5, 5], scope='conv2_s1')
            conv2 = slim.max_pool2d(conv2, [3, 3], scope='pool2')
            conv3 = slim.conv2d(conv2, 256, [3, 3], padding='SAME', scope='conv3_s1')
            conv4 = slim.conv2d(conv3, 256, [3, 3], padding='SAME', scope='conv4_s1')
            conv5 = slim.conv2d(conv4, 256, [3, 3], padding='SAME', trainable=trainable, scope='conv5_s1')
            conv5 = slim.max_pool2d(conv5, [3, 3], scope='pool3')
            # residual attention
            att_mask = attentionNet(conv5, 'softmax')
            att_map = tf.multiply(conv5, att_mask)
            att_f = tf.add(conv5, att_map)
            attended_map = tf.reduce_sum(att_f, reduction_indices=[1, 2])
            attended_map = tf.nn.l2_normalize(attended_map, dim=1)
            att_f = slim.flatten(att_f)
            fc6 = slim.fully_connected(att_f, 512, trainable=trainable, scope='fc6_s1')
            fc7 = slim.fully_connected(fc6, 256, activation_fn=None, trainable=trainable, scope='fc7_sketch')
            fc7 = tf.nn.l2_normalize(fc7, dim=1)
            # coarse-fine fusion
            final_feature_map = tf.concat(1, [fc7, attended_map])
    return final_feature_map


def init_variables(model_file='/home/soe/PycharmProjects/Deep_SBIR_tf-master/model/sketchnet_init.npy'):
    if NET_ID==0:
        pretrained_paras = ['conv1_s1', 'conv2_s1', 'conv3_s1', 'conv4_s1', 'conv5_s1', 'fc6_s1', 'fc7_sketch']
    else:
        pretrained_paras = ['conv1_s1', 'conv2_s1', 'conv3_s1', 'conv4_s1', 'conv5_s1', 'fc6_s1']
    d = np.load(model_file).item()
    init_ops = []  # a list of operations
    for var in tf.global_variables():
        for w_name in pretrained_paras:
            if w_name in var.name:
                print('Initialise var %s with weight %s' % (var.name, w_name))
                try:
                    if 'weights' in var.name:
                        # using assign(src, dst) to assign the weights of pre-trained model to current network
                        # init_ops.append(var.assign(d[w_name+'/weights:0']))
                        init_ops.append(var.assign(d[w_name]['weights']))
                    elif 'biases' in var.name:
                        # init_ops.append(var.assign(d[w_name+'/biases:0']))
                        init_ops.append(var.assign(d[w_name]['biases']))
                except KeyError:
                     if 'weights' in var.name:
                        # using assign(src, dst) to assign the weights of pre-trained model to current network
                        init_ops.append(var.assign(d[w_name+'/weights:0']))
                        # init_ops.append(var.assign(d[w_name]['weights']))
                     elif 'biases' in var.name:
                        init_ops.append(var.assign(d[w_name+'/biases:0']))
                        # init_ops.append(var.assign(d[w_name]['biases']))
                except:
                     if 'weights' in var.name:
                        # using assign(src, dst) to assign the weights of pre-trained model to current network
                        init_ops.append(var.assign(d[w_name][0]))
                        # init_ops.append(var.assign(d[w_name]['weights']))
                     elif 'biases' in var.name:
                        init_ops.append(var.assign(d[w_name][1]))
                        # init_ops.append(var.assign(d[w_name]['biases']))
    return init_ops


def compute_euclidean_distance(x, y):
    """"""
    Computes the euclidean distance between two tensorflow variables
    """"""

    d = tf.square(tf.sub(x, y))
    d = tf.sqrt(tf.reduce_sum(d))  # What about the axis ???
    return d


def square_distance(x, y):
    return tf.reduce_sum(tf.square(x - y), axis=1)


def compute_triplet_loss(anchor_feature, positive_feature, negative_feature, margin):
    with tf.name_scope(""triplet_loss""):
        d_p_squared = square_distance(anchor_feature, positive_feature)
        d_n_squared = square_distance(anchor_feature, negative_feature)
        loss = tf.maximum(0., d_p_squared - d_n_squared + margin)
        return tf.reduce_mean(loss), tf.reduce_mean(d_p_squared), tf.reduce_mean(d_n_squared)


def main(subset, sketch_dir, image_dir, sketch_dir_te, image_dir_te, triplet_path, mean, hard_ratio, batch_size, phase, phase_te, net_model):

    ITERATIONS = 20000
    VALIDATION_TEST = 200
    perc_train = 0.9
    MARGIN = 0.3
    SAVE_STEP = 200
    model_path = ""/home/soe/PycharmProjects/Deep_SBIR_tf-master/model/%s/%s/"" % (subset, net_model)
    #pre_trained_model = '/home/soe/PycharmProjects/Deep_SBIR_tf-master/model/sketchnet_init.npy'
    pre_step = 0
    if not os.path.exists(model_path):
        os.makedirs(model_path)


    # Siamease place holders
    train_anchor_data = tf.placeholder(tf.float32, shape=(None, 225, 225, 1), name=""anchor"")
    train_positive_data = tf.placeholder(tf.float32, shape=(None, 225, 225, 1), name=""positive"")
    train_negative_data = tf.placeholder(tf.float32, shape=(None, 225, 225, 1), name=""negative"")

    # Creating the architecturek
    if net_model == 'deep_sbir':
        train_anchor = sketch_a_net_sbir(tf.cast(train_anchor_data, tf.float32) - mean, True)
        tf.get_variable_scope().reuse_variables()
        train_positive = sketch_a_net_sbir(tf.cast(train_positive_data, tf.float32) - mean, True)
        train_negative = sketch_a_net_sbir(tf.cast(train_negative_data, tf.float32) - mean, True)
    elif net_model == 'DSSA':
        train_anchor = sketch_a_net_dssa(tf.cast(train_anchor_data, tf.float32) - mean, True)
        tf.get_variable_scope().reuse_variables()
        train_positive = sketch_a_net_dssa(tf.cast(train_positive_data, tf.float32) - mean, True)
        train_negative = sketch_a_net_dssa(tf.cast(train_negative_data, tf.float32) - mean, True)
    else:
        print 'Please define the net_model'

    init_ops = init_variables()
    loss, positives, negatives = compute_triplet_loss(train_anchor, train_positive, train_negative, MARGIN)

    # Defining training parameters
    batch = tf.Variable(0)
    learning_rate = 0.001
    data_sampler = triplet_sampler_asy.TripletSamplingLayer()
    data_sampler_te = triplet_sampler_asy.TripletSamplingLayer()
    data_sampler.setup(sketch_dir, image_dir, triplet_path, mean, hard_ratio, batch_size, phase)
    data_sampler_te.setup(sketch_dir_te, image_dir_te, triplet_path, mean, hard_ratio, batch_size, phase_te)
    optimizer = tf.train.MomentumOptimizer(momentum=0.9, learning_rate=learning_rate).minimize(loss,
                                                                                               global_step=batch)
    #validation_prediction = tf.nn.softmax(lenet_validation)
    # saver = tf.train.Saver(max_to_keep=5)
    dst_path = '/home/soe/PycharmProjects/Deep_SBIR_tf-master/log'
    model_id = '%s_%s_log.txt' % (subset, net_model)
    filename = dst_path+'/'+model_id
    # f = open(filename, 'a')
    # Training
    with tf.Session() as session:

        session.run(tf.global_variables_initializer())
        session.run(init_ops)
        for step in range(ITERATIONS):
            f = open(filename, 'a')
            batch_anchor, batch_positive, batch_negative = data_sampler.get_next_batch()

            feed_dict = {train_anchor_data: batch_anchor,
                         train_positive_data: batch_positive,
                         train_negative_data: batch_negative
                         }
            _, l = session.run([optimizer, loss], feed_dict=feed_dict)
            # save_path = saver.save(session, model_path, global_step=step)
            print(""Iter %d: Loss Train %f"" % (step+pre_step, l))
            f.write(""Iter ""+str(step+pre_step) + "": Loss Train: "" + str(l))
            f.write(""\n"")
            # train_writer.add_summary(summary, step)

            if step % SAVE_STEP == 0:
                str_temp = '%smodel-iter%d.npy' % (model_path, step+pre_step)
                save_dict = {var.name: var.eval(session) for var in tf.global_variables()}
                np.save(str_temp, save_dict)

            if step % VALIDATION_TEST == 0:
                batch_anchor, batch_positive, batch_negative = data_sampler_te.get_next_batch()

                feed_dict = {train_anchor_data: batch_anchor,
                             train_positive_data: batch_positive,
                             train_negative_data: batch_negative
                             }

                lv = session.run([loss], feed_dict=feed_dict)
                # test_writer.add_summary(summary, step)
                print(""Loss Validation {0}"".format(lv))
                f.write(""Loss Validation: "" + str(lv))
                f.write(""\n"")
            f.close()


if __name__ == '__main__':
    # 'deep_sbir'(the model of cvpr16) or 'DSSA'(the model of iccv17)
    net_model = 'deep_sbir'  
    subset = 'shoes'
    mean = 250.42
    hard_ratio = 0.75
    batch_size = 128
    phase = 'TRAIN'
    phase_te = 'TEST'
    base_path = './data'
    sketch_dir = '%s/%s/%s_sketch_db_%s.mat' % (base_path, subset, subset, phase.lower())
    image_dir = '%s/%s/%s_edge_db_%s.mat' % (base_path, subset, subset, phase.lower())
    triplet_path = '%s/%s/%s_annotation.json' % (base_path, subset, subset) # pseudo annotations for handbags
    sketch_dir_te = '%s/%s/%s_sketch_db_%s.mat' % (base_path, subset, subset, phase_te.lower())
    image_dir_te = '%s/%s/%s_edge_db_%s.mat' % (base_path, subset, subset, phase_te.lower())
    main(subset, sketch_dir, image_dir, sketch_dir_te, image_dir_te, triplet_path, mean, hard_ratio, batch_size, phase, phase_te, net_model)




-------------------------------------------------------------------------------------------------------------------------------------------

This is my code used above, in the # creating the architecture part, the code ""tf.get_variable_scope().reuse_variables()"" cause an error report : 
Variable fc6_s1/weights/Momentum/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?

I read some issues about reuse and I know the problem is the sentence trying to use some variable not created. But I am a very bigginer of tensorflow, so I don't know how  to fix this code. If any people can help me, thank you very much !"
21656,Closing TFRecordWriter twice should throw an error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**:3.6.6

### Describe the problem
Closing a TFRecordWriter twice should throw an error since it leads to undefined behavior down the road. Specifically, trying to read the tfrecords (containing n records) using a TFRecordDataset leads to an infinite loop when trying to read the n+1 element. Normally, this would raise an OutOfRangeError."
21655,tf.image api not enough to handle image problems,"My image data stored in hdfs,so when do data augmentation ,I need to use tf.gfile.FastGFile to read, then convert to PIL.Image or skimage ,it's complexed. I also know tf support some augmentation api ,but it's not enough.When tf support more image handle api like pillow or skimage? Thank you very much ~~~"
21653,Endless restarting of session when run distribute traning with tensorflow 1.8,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat 4.8.5-4
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Not applicable
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.8
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: 4.9.2
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: M40 11448MiB
- **Exact command to reproduce**:
```
Cleanup partition error: Unavailable: OS Error
```

### Descripe problem
Training the same model with tensorflow r1.4 is OK. With tensorflow r1.8, the session restarts without stop at the end of training. I have checked the traning log. Some workers went wrong when clearing partition. This error caused session restarting. But at this time, the other workers have alreadly stopped running. Thus, the restarting workers went wrong with master init error and start another session restarting progress.

### Logs
```
### End of Training
[2018-08-16 14:54:54.279 193 master_session.cc:1754] [ERROR] Cleanup partition error: Unavailable: OS Error
[2018-08-16 14:54:54.282 tf_logging.py:126] [WARNING] An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. Error: OS Error
[2018-08-16 14:54:55.281 193 master_session.cc:1754] [ERROR] Cleanup partition error: Unavailable: OS Error
[2018-08-16 14:54:55.333 tf_logging.py:116] [INFO] Graph was finalized.
[2018-08-16 14:54:55.404 tf_logging.py:116] [INFO] Restoring parameters from hdfs://...
[2018-08-16 14:54:56.288 193 master.cc:284] [ERROR] Master init: Unavailable: OS Error
[2018-08-16 14:54:56.288 193 master_session.cc:1017] [INFO] DeregisterGraph error: Unavailable: OS Error
[2018-08-16 14:54:56.288 193 master_session.cc:1017] [INFO] DeregisterGraph error: Unavailable: OS Error
[2018-08-16 14:54:56.288 193 master_session.cc:1017] [INFO] DeregisterGraph error: Unavailable: OS Error
[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2
[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:3
[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:4
[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6
[2018-08-16 14:55:05.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7
[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2
[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:3
[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:4
[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6
[2018-08-16 14:55:15.450 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7
[2018-08-16 14:55:16.451 196 master.cc:284] [ERROR] Master init: Unavailable: OS Error
[2018-08-16 14:55:25.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2
[2018-08-16 14:55:25.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:3
[2018-08-16 14:55:25.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6
[2018-08-16 14:55:25.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7
[2018-08-16 14:55:35.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2
[2018-08-16 14:55:35.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:3
[2018-08-16 14:55:35.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6
[2018-08-16 14:55:35.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7
[2018-08-16 14:55:44.452 195 master.cc:284] [ERROR] Master init: Unavailable: OS Error
[2018-08-16 14:55:45.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:2
[2018-08-16 14:55:45.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:6
[2018-08-16 14:55:45.451 4571 master.cc:236] [INFO] CreateSession still waiting for response from worker: /job:worker/replica:0/task:7
```
"
21652,"Build failed due to missing ""tensorflow/contrib/constrained_optimization""","
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS High Sierra 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Not applicable
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.10
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: 0.15.2-homebrew
- **Clang version**: 
```
clang --version
Apple LLVM version 9.1.0 (clang-902.0.39.2)
Target: x86_64-apple-darwin17.7.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin
```
- **CUDA/cuDNN version**: CPU only
- **GPU model and memory**: CPU only
- **Exact command to reproduce**: 
```bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package```

### Describe the problem
Following the help page that describes how to install TensorFlow from source, I first ran `./configure`(I declined each question and used valid paths for Python) followed by `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`

### Source code / logs
```
Starting local Bazel server and connecting to it...
..........................................
ERROR: /Users/.../Build/tensorflow/tensorflow/tools/pip_package/BUILD:180:1: no such package 'tensorflow/contrib/constrained_optimization': BUILD file not found on package path and referenced by '//tensorflow/tools/pip_package:build_pip_package'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package 'tensorflow/contrib/constrained_optimization': BUILD file not found on package path
INFO: Elapsed time: 55.970s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (6 packages loaded)
```
Dots in the first path are used to conceal the real path."
21651,"What is the python mechanism to ""Bring in all of the public TensorFlow interface into this # module."" in tensorflow/__init__.py","What is the python mechanism to ""Bring in all of the public TensorFlow interface into this module."" in tensorflow/__init__.py.

I don't see an explicit command to import any functions like tf.reduce_sum."
21648,ReadSavedModel & related operations should follow symlinks,"https://github.com/tensorflow/tensorflow/blob/98010279f40e4963512ba2f2f39c3d732aef7b93/tensorflow/cc/saved_model/reader.cc#L34

Consider that line, and now consider what happens if `export_dir` is the pathname of a symlink to a directory.

The way I'm reading this, the naive file join will fail to find the kSavedModelFilenamePb file that the op is looking for, since it will be looking at `<symlink>/saved_model.pb` instead of `<link_target>/saved_model.pb` as (I think) would be logical.

This came up for me recently. I have one process that's continually training and occasionally promoting a new version of the model to a SavedModel for inference in a second process. The second process runs concurrently and occasionally loads the latest SavedModel from disk.

Atomically swapping a directory for another directory is either not possible on POSIX or, if it is possible, I couldn't find out a way from searching. However, it's easy to atomically swap the referent of a symlink. My naive hope was to maintain a symlink to the ""current best SavedModel"" which the second process would be able to use. The second process is Rust, and it's binding to the C API, which I traced all the way back to the linked line above.

Admittedly this isn't extremely easy to fix (or maybe it's not possible to fix - not sure if holding a directory fd preserves the directory's contents from removal by another process). In order to be useful for my use case, doing the easy thing of resolving the symlink in one operation and then doing file joins wouldn't work, because the symlink is subject to swap to a new referent at any point of time, and that could happen between the resolution of the symlink and ops on the referents. 

Anyway, there is a workaround for this use case (some sort of file locking should do the trick). But, I wanted to point this out."
21647,ValueError: Attempted to map inputs that were not found in graph_def: [input:0],"Hello! 

I have asked this question in stackoverflow but at this point it merits a long post. 

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
 Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
NA
- **TensorFlow installed from (source or binary)**:
pip 
- **TensorFlow version (use command below)**:
1.8
- **Python version**:
Python 2.7.12
- **Bazel version (if compiling from source)**:
NA
- **GCC/Compiler version (if compiling from source)**:
NA
- **CUDA/cuDNN version**:
9.2
- **GPU model and memory**:
 7611MiB (gpu nvidia p4)
- **Exact command to reproduce**:
`ValueError: Attempted to map inputs that were not found in graph_def: [input:0]`


### Describe the problem
Trying to create a frozen graph, technically succeeding at doing that but then unable to run it. I am unsure about what the tags do. I want to use the model for inference,  but wasn't able to load it in a frozen graph in training and serving mode.  I suppose I have to restore the session? the documentation does not show that. And I double checked my output node's name is softmax. I saw the frozen graph in tensorboard and looks fine, but does not load into the graph. Help? I found people with issues like this but none resolved or their issues I already fixed in my code.

### Source code / logs
This is the code to freeze from and not frozen  .pb file and its variables. 
```
import tensorflow as tf
export_dir='../catmod'
with tf.Session(graph=tf.Graph()) as sess:
    # I have not been able to find how the tags affect this. Does training work? I tried as well with serve
    # serve gave the same error of input:0  * I checked with saved_model_cli
    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.TRAINING], export_dir)
    output_graph = ""frozen_graph.pb"" 
    # changing nodes from refswitch to switch then moving it accordingly 
    for node in gd.node:
        if node.op == 'RefSwitch':
            node.op = 'Switch'
            for index in xrange(len(node.input)):
                if 'moving_' in node.input[index]:
                    node.input[index] = node.input[index] + '/read'
        elif node.op == 'AssignSub':
            node.op = 'Sub'
            if 'use_locking' in node.attr: del node.attr['use_locking']
    gd = sess.graph.as_graph_def()
    output_nodes=['softmax']
    output_graph_def = tf.graph_util.convert_variables_to_constants(
       sess, # The session is used to retrieve the weights
       gd,
       output_nodes# The output node names are used to select the usefull nodes
    )



    # Finally we serialize and dump the output graph to the filesystem
    with tf.gfile.GFile(output_graph, ""wb"") as f:
        f.write(output_graph_def.SerializeToString())
sess.close()


```
This code is functional as I have used it with pre-frozen graphs.  (parts of) Code to load the frozen graph which I used with other frozen graphs that I got from the internet:

```
import tensorflow as tf
import numpy as np
from tensorflow.python.platform import gfile
from tensorflow.python.client import timeline
from tensorflow.python.util import compat

### Loading the model 

def getModel(pb_frozen):
    with gfile.FastGFile(pb_frozen,'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
    return graph_def

# Running the model 
tf.logging.info(""Starting execution"")

gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.50)
tf.reset_default_graph()

g = tf.Graph()

dummy_input = np.random.random_sample((batch_size,224,224,3))
outlist=[]
# creating the graph 
with g.as_default():
    inc=tf.constant(dummy_input, dtype=tf.float32)
    dataset=tf.data.Dataset.from_tensors(inc)
    dataset=dataset.repeat()
    iterator=dataset.make_one_shot_iterator()
    next_element=iterator.get_next()
    out = tf.import_graph_def(
      graph_def=  getModel(pb_frozen), # loading graph 
      input_map={""input"":next_element},
      return_elements=['dense/bias']
    )
    out = out[0].outputs[0]
    outlist.append(out)

``` 
And then is run in a session. 


"
21646,Tensorflow seq2seq demo(RNN-LSTM) can't convert into .tf file from toco,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No. only call functions from tensorflow.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.9.0-rc2-1412-ge747550
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: NVIDIA 1080TI, 12GB GDDR5
- **Exact command to reproduce**: `python nmt.nmt.py --num_gpus=1 --src=de --tgt=en --hparams_path=nmt/standard_hparams/small.json --vocab_prefix=nmt/scripts/wmt16_de_en/vocab.bpe.32000 --out_dir=deen_one --inference_input_file=10.de --inference_output_file=output_infer`

### Describe the problem
I tried two version of toco converter(python, command).
1. python version passed 'NoOp not registered error(log on below)
2. command version will appear 'NoOp not registered error

I have a few questions.
1. does tensorflow support convert RNN network into .tf file now?
2. what are the different between python version toco(**lite.py etc. tf.contrib.lite.TocoConverter.from_frozen_graph**) and command version? which one is the newest ones?
3. I have already used **tf.graph_util.convert_variables_to_constants** python script to convert normal saved model into frozen graph but it appears (Op type not registered 'NoOp' #19267).
4. I tried **tf.graph_util.remove_training_nodes function** for removing training nodes(may contain NoOp?) but the model will be broken(unable to inference, it may change some of the Op from the old graph then it pops errors again.). strange thing is **NoOp** problem is passed during toco converting.

I'm using the most recent version of TF (7/27/2018)


### Source code / logs
Here is my [repro example](https://github.com/lkfo415579/nmt_custom)(**Machine Translation Project origin from google group**, [origin_repro](https://github.com/tensorflow/nmt)), i modified some of the code in order make it simpler for lite converter project. 
Converting into frozen and lite model's code : [code](https://github.com/lkfo415579/nmt_custom/blob/041dd8ffe4afcaf0dfe96d86d8de17a079372fcc/nmt/inference.py#L213)

My structure of frozen graph
[structure.txt](https://github.com/tensorflow/tensorflow/files/2240393/structure.txt)


**Using python version toco converter produced below log**
```
2018-07-30 11:32:13.520742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1, 2
2018-07-30 11:32:14.163167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-30 11:32:14.163211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 2 
2018-07-30 11:32:14.163216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y Y 
2018-07-30 11:32:14.163220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N Y 
2018-07-30 11:32:14.163223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 2:   Y Y N 
2018-07-30 11:32:14.163801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9027 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1)
2018-07-30 11:32:14.270717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10033 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1)
2018-07-30 11:32:14.387717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10033 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1)
2018-07-30 11:32:14.822919: W tensorflow/core/framework/allocator.cc:108] Allocation of 74850304 exceeds 10% of system memory.
2018-07-30 11:32:14.890107: W tensorflow/core/framework/allocator.cc:108] Allocation of 74850304 exceeds 10% of system memory.
2018-07-30 11:32:14.985703: W tensorflow/core/framework/allocator.cc:108] Allocation of 74850304 exceeds 10% of system memory.
2018-07-30 11:32:15.062000: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: HashTableV2
2018-07-30 11:32:15.062047: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: InitializeTableV2
2018-07-30 11:32:15.064555: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: HashTableV2
2018-07-30 11:32:15.064572: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: InitializeTableV2
2018-07-30 11:32:15.067112: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: HashTableV2
2018-07-30 11:32:15.067129: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: InitializeTableV2
2018-07-30 11:32:15.067144: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LookupTableFindV2
2018-07-30 11:32:15.067168: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Iterator
2018-07-30 11:32:15.067184: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: IteratorToStringHandle
2018-07-30 11:32:15.067193: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorSliceDataset
2018-07-30 11:32:15.067206: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: MapDataset
2018-07-30 11:32:15.067219: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: MapDataset
2018-07-30 11:32:15.067230: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: MapDataset
2018-07-30 11:32:15.067242: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: PaddedBatchDatasetV2
2018-07-30 11:32:15.067253: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: MakeIterator
2018-07-30 11:32:15.067262: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: IteratorGetNext
2018-07-30 11:32:15.067291: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.067302: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.067330: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.067340: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.067350: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Size
2018-07-30 11:32:15.067498: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.067514: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.067543: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayScatterV3
2018-07-30 11:32:15.067560: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.067570: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.067578: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.067586: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.067595: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.067611: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.067620: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.067629: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LoopCond
2018-07-30 11:32:15.067657: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayReadV3
2018-07-30 11:32:15.067666: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.067674: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.067685: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.067710: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.067722: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.067740: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.067750: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.067770: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.067781: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.067814: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.067829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayWriteV3
2018-07-30 11:32:15.067838: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.067856: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.067864: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.067871: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.067877: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.067883: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.067888: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArraySizeV3
2018-07-30 11:32:15.067903: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayGatherV3
2018-07-30 11:32:15.067915: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: ReverseSequence
2018-07-30 11:32:15.068041: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.068055: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.068084: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayScatterV3
2018-07-30 11:32:15.068101: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.068111: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.068120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.068128: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.068136: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.068153: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.068162: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.068171: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LoopCond
2018-07-30 11:32:15.068197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayReadV3
2018-07-30 11:32:15.068207: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.068215: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.068223: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.068247: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.068258: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.068277: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.068287: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.068307: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.068318: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.068348: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.068363: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayWriteV3
2018-07-30 11:32:15.068373: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.068392: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.068400: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.068406: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.068413: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.068419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.068426: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArraySizeV3
2018-07-30 11:32:15.068443: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayGatherV3
2018-07-30 11:32:15.068455: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: ReverseSequence
2018-07-30 11:32:15.068473: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LookupTableFindV2
2018-07-30 11:32:15.068486: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LookupTableFindV2
2018-07-30 11:32:15.068505: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Round
2018-07-30 11:32:15.068842: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.068857: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.068894: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: ListDiff
2018-07-30 11:32:15.069261: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Size
2018-07-30 11:32:15.069434: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: OneHot
2018-07-30 11:32:15.069459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: OneHot
2018-07-30 11:32:15.069542: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LogicalOr
2018-07-30 11:32:15.069563: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.069578: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.069591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.069604: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069613: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069622: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069629: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069637: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069646: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069653: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069660: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069667: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069673: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069679: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069695: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069703: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069711: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069719: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069735: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069743: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069778: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.069793: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LoopCond
2018-07-30 11:32:15.069862: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.070046: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.070059: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.070078: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.070088: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.070108: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.070119: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.070166: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.070177: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.070196: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.070206: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.070227: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.070239: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.070284: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.070335: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.070347: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.070361: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.070390: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.070400: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.070413: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.070422: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.070437: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.070447: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.070459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.070468: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.070487: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.070497: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.070518: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.070592: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.070626: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.070637: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.070650: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.070841: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.070854: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.070890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: ListDiff
2018-07-30 11:32:15.070933: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.070996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: OneHot
2018-07-30 11:32:15.071008: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.071060: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: OneHot
2018-07-30 11:32:15.071140: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-07-30 11:32:15.071150: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-07-30 11:32:15.071158: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-07-30 11:32:15.071204: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LogicalOr
2018-07-30 11:32:15.071530: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.071551: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.071577: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayWriteV3
2018-07-30 11:32:15.071588: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.071597: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayWriteV3
2018-07-30 11:32:15.071607: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.071617: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayWriteV3
2018-07-30 11:32:15.071626: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.071657: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071664: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071672: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071678: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071685: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071691: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071698: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071705: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071711: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071718: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071724: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071729: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071736: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071742: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071749: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071755: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071761: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.071781: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArraySizeV3
2018-07-30 11:32:15.071799: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayGatherV3
2018-07-30 11:32:15.071810: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArraySizeV3
2018-07-30 11:32:15.071827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayGatherV3
2018-07-30 11:32:15.071836: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArraySizeV3
2018-07-30 11:32:15.071852: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayGatherV3
2018-07-30 11:32:15.071868: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: GatherTree
2018-07-30 11:32:15.071878: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LookupTableFindV2
2018-07-30 11:32:15.071889: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.071898: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.071922: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SaveV2
2018-07-30 11:32:15.071945: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: RestoreV2
2018-07-30 11:32:15.071954: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.071964: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.071973: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.071983: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.071990: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.071999: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.072008: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.072017: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.072026: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.072035: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.072044: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.072054: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.072062: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.072072: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.072081: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.072089: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.072098: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.072107: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.074697: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: HashTableV2
2018-07-30 11:32:15.074715: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: InitializeTableV2
2018-07-30 11:32:15.077298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: HashTableV2
2018-07-30 11:32:15.077313: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: InitializeTableV2
2018-07-30 11:32:15.079817: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: HashTableV2
2018-07-30 11:32:15.079833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: InitializeTableV2
2018-07-30 11:32:15.079847: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LookupTableFindV2
2018-07-30 11:32:15.079866: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Iterator
2018-07-30 11:32:15.079876: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: IteratorToStringHandle
2018-07-30 11:32:15.079886: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorSliceDataset
2018-07-30 11:32:15.079898: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: MapDataset
2018-07-30 11:32:15.079911: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: MapDataset
2018-07-30 11:32:15.079922: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: MapDataset
2018-07-30 11:32:15.079934: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: PaddedBatchDatasetV2
2018-07-30 11:32:15.079943: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: MakeIterator
2018-07-30 11:32:15.079952: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: IteratorGetNext
2018-07-30 11:32:15.079980: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.079991: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.080018: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.080028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.080038: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Size
2018-07-30 11:32:15.080183: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.080197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.080226: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayScatterV3
2018-07-30 11:32:15.080244: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080253: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080262: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080271: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080279: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080294: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080304: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080314: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LoopCond
2018-07-30 11:32:15.080341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080351: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080359: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayReadV3
2018-07-30 11:32:15.080367: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080395: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.080406: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.080425: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.080435: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.080452: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080464: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080492: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080509: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080518: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayWriteV3
2018-07-30 11:32:15.080536: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.080544: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.080551: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.080557: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.080564: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.080571: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArraySizeV3
2018-07-30 11:32:15.080588: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayGatherV3
2018-07-30 11:32:15.080600: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: ReverseSequence
2018-07-30 11:32:15.080769: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.080786: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.080816: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayScatterV3
2018-07-30 11:32:15.080834: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080843: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080852: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080861: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080868: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080884: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080893: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080903: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LoopCond
2018-07-30 11:32:15.080941: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080950: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080958: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayReadV3
2018-07-30 11:32:15.080967: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.080993: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.081005: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.081023: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.081033: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.081050: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.081061: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.081091: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.081108: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.081118: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayWriteV3
2018-07-30 11:32:15.081136: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.081143: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.081150: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.081156: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.081161: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.081168: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArraySizeV3
2018-07-30 11:32:15.081185: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayGatherV3
2018-07-30 11:32:15.081197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: ReverseSequence
2018-07-30 11:32:15.081213: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LookupTableFindV2
2018-07-30 11:32:15.081226: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LookupTableFindV2
2018-07-30 11:32:15.081244: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Round
2018-07-30 11:32:15.081547: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.081560: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.081596: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: ListDiff
2018-07-30 11:32:15.081997: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Size
2018-07-30 11:32:15.082180: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: OneHot
2018-07-30 11:32:15.082207: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: OneHot
2018-07-30 11:32:15.082281: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LogicalOr
2018-07-30 11:32:15.082301: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.082316: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.082330: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.082343: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082353: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082361: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082386: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082394: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082402: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082410: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082418: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082426: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082433: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082441: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082449: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082457: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082465: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082473: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082481: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082489: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082524: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082539: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LoopCond
2018-07-30 11:32:15.082609: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082798: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.082811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.082829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.082840: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.082858: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082870: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082918: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.082930: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.082949: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.082959: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.082977: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.082989: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.083035: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.083091: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.083102: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.083113: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.083146: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.083157: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.083171: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.083181: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.083197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.083207: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.083218: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.083226: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.083245: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.083255: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.083275: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.083351: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.083388: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.083400: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.083411: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.083610: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.083622: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.083660: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: ListDiff
2018-07-30 11:32:15.083704: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.083772: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.083783: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: OneHot
2018-07-30 11:32:15.083836: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: OneHot
2018-07-30 11:32:15.083918: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-07-30 11:32:15.083928: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-07-30 11:32:15.083941: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-07-30 11:32:15.083989: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LogicalOr
2018-07-30 11:32:15.084328: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.084346: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.084376: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.084386: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayWriteV3
2018-07-30 11:32:15.084397: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.084406: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayWriteV3
2018-07-30 11:32:15.084416: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.084425: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayWriteV3
2018-07-30 11:32:15.084456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084464: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084471: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084478: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084485: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084491: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084498: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084504: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084511: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084518: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084524: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084531: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084538: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084544: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084551: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084557: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084564: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084570: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084577: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.084584: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArraySizeV3
2018-07-30 11:32:15.084601: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayGatherV3
2018-07-30 11:32:15.084612: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArraySizeV3
2018-07-30 11:32:15.084641: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayGatherV3
2018-07-30 11:32:15.084654: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArraySizeV3
2018-07-30 11:32:15.084671: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayGatherV3
2018-07-30 11:32:15.084686: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: GatherTree
2018-07-30 11:32:15.084698: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LookupTableFindV2
2018-07-30 11:32:15.084709: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: VariableV2
2018-07-30 11:32:15.084718: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084743: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SaveV2
2018-07-30 11:32:15.084767: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: RestoreV2
2018-07-30 11:32:15.084778: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084794: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084809: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084823: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084844: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084853: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084862: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084871: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084880: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084889: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084898: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084907: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084916: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084925: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084934: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084942: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084950: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.084980: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SaveV2
2018-07-30 11:32:15.085003: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: RestoreV2
2018-07-30 11:32:15.085013: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085031: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085041: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085050: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085059: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085068: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085077: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085086: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085096: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085105: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085114: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085123: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085132: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085141: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085150: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085160: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085169: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Assign
2018-07-30 11:32:15.085187: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: HashTableV2
2018-07-30 11:32:15.085200: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: HashTableV2
2018-07-30 11:32:15.085213: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Iterator
2018-07-30 11:32:15.085223: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: IteratorGetNext
2018-07-30 11:32:15.215031: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Size
2018-07-30 11:32:15.215217: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.215234: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.215266: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayScatterV3
2018-07-30 11:32:15.215286: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.215295: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.215304: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.215312: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.215321: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.215336: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.215346: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.215356: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LoopCond
2018-07-30 11:32:15.215385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.215394: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.215403: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayReadV3
2018-07-30 11:32:15.215411: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.217988: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.218018: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.218063: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.218081: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.218091: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayWriteV3
2018-07-30 11:32:15.218109: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.218117: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.218124: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.218131: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArraySizeV3
2018-07-30 11:32:15.218150: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayGatherV3
2018-07-30 11:32:15.218161: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: ReverseSequence
2018-07-30 11:32:15.218278: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.218292: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.218322: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayScatterV3
2018-07-30 11:32:15.218340: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.218349: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.218356: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.218363: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.218369: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.218384: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.218394: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.218404: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LoopCond
2018-07-30 11:32:15.218433: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.218443: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.218451: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayReadV3
2018-07-30 11:32:15.218459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.221141: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.221169: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.221201: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.221220: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.221229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayWriteV3
2018-07-30 11:32:15.221249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.221256: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.221263: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.221270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArraySizeV3
2018-07-30 11:32:15.221288: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayGatherV3
2018-07-30 11:32:15.221299: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: ReverseSequence
2018-07-30 11:32:15.221317: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LookupTableFindV2
2018-07-30 11:32:15.221331: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LookupTableFindV2
2018-07-30 11:32:15.221352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Round
2018-07-30 11:32:15.222197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: ListDiff
2018-07-30 11:32:15.222450: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Size
2018-07-30 11:32:15.222699: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: OneHot
2018-07-30 11:32:15.222724: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: OneHot
2018-07-30 11:32:15.222745: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LogicalOr
2018-07-30 11:32:15.222760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.222775: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayV3
2018-07-30 11:32:15.222788: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.222797: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.222805: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.222813: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.222821: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.222829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.222837: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.222844: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.222852: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.222860: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.222868: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.222875: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.222883: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.222890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.222919: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.222936: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LoopCond
2018-07-30 11:32:15.222990: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.226937: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.226976: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.229695: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.229729: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.229780: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.230108: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.230143: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.230153: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.230171: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.230181: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.230201: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.230272: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.231168: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.295597: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: ListDiff
2018-07-30 11:32:15.295669: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.295742: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.295754: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: OneHot
2018-07-30 11:32:15.295853: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-07-30 11:32:15.295863: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-07-30 11:32:15.295871: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-07-30 11:32:15.295920: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LogicalOr
2018-07-30 11:32:15.296186: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.296206: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.296226: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.296234: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayWriteV3
2018-07-30 11:32:15.296246: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Enter
2018-07-30 11:32:15.296256: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayWriteV3
2018-07-30 11:32:15.296283: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.296291: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.296299: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Exit
2018-07-30 11:32:15.296307: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArraySizeV3
2018-07-30 11:32:15.296324: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayGatherV3
2018-07-30 11:32:15.296335: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArraySizeV3
2018-07-30 11:32:15.296352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: TensorArrayGatherV3
2018-07-30 11:32:15.296368: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: GatherTree
2018-07-30 11:32:15.296378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: LookupTableFindV2
2018-07-30 11:32:15.446032: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 3083 operators, 5273 arrays (0 quantized)
2018-07-30 11:32:15.569851: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 753 operators, 1254 arrays (0 quantized)
2018-07-30 11:32:15.587405: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 753 operators, 1254 arrays (0 quantized)
2018-07-30 11:32:15.605187: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 688 operators, 1130 arrays (0 quantized)
2018-07-30 11:32:15.622634: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 688 operators, 1130 arrays (0 quantized)
2018-07-30 11:32:15.640010: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 688 operators, 1130 arrays (0 quantized)
2018-07-30 11:32:15.649520: F tensorflow/contrib/lite/toco/tooling_util.cc:685] Check failed: dim >= 1 (0 vs. 1)

None
```"
21645,Error: corrupted size vs. prev_size when using python3.6,"Hi, 
I install tensorflow-1.9; then install pytorch=0.4 at python2 and python3 respectively.
I use python2 to run my code, no error is reported; but when using python3 to run it, an error is report after finishing run. The error like as follows:
Error in `/restools/tools/miniconda/miniconda3/bin/python': corrupted size vs. prev_size: 0x000055b2bcb5c690 ***"
21644,Not found: FeedInputs: unable to find feed output phase_train,"I try to use  Facenet  by   tensorflow C++ API (VS2015),it can load graph,but it doesn't work.
Not found: FeedInputs: unable to find feed output phase_train
code like this:
tensorflow::Tensor input_tensor(DT_FLOAT, TensorShape({ 2 , iHeight, iWidth, depth }));
auto input_tensor_mapped = input_tensor.tensor<float, 4>(); 
...................................................
tensorflow::Tensor phase(DT_BOOL, TensorShape());
	 phase.scalar<int>()() = FALSE;

	input_tensor.shape();
	std::vector<std::pair<std::string, tensorflow::Tensor>> inputs = {
		{ ""input"", input_tensor },                                    //it's OK
		{"" phase_train"",phase }      // it' bad    
	};
 please help me,thansk"
21643,Tensorflow Lite Custom op have no effect even source code *.cc has modified,"System information
== cat /etc/issue ===============================================
Linux master 4.15.0-29-generic #31~16.04.1-Ubuntu SMP Wed Jul 18 08:54:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.5.0-12ubuntu1~16.04) 5.5.0 20171010
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions. There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

== uname -a =====================================================
Linux master 4.15.0-29-generic #31~16.04.1-Ubuntu SMP Wed Jul 18 08:54:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy 1.14.5
protobuf 3.6.0
tensorflow 1.8.0
tensorflow-tensorboard 0.4.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/usr/local/lib/python2.7/dist-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type.
from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/lib64/:/usr/local/cuda/lib64/:/usr/local/cuda-9.0/lib64:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Mon Jul 30 11:49:23 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.45 Driver Version: 396.45 |
|-------------------------------+----------------------+----------------------+
| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |
| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |
|===============================+======================+======================|
| 0 Quadro P600 Off | 00000000:03:00.0 On | N/A |
| 34% 43C P8 N/A / N/A | 574MiB / 1997MiB | 1% Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes: GPU Memory |
| GPU PID Type Process name Usage |
|=============================================================================|
| 0 1152 G /usr/lib/xorg/Xorg 237MiB |
| 0 2213 G compiz 79MiB |
| 0 2589 G ...-token=224D99F526E00AE3A3C0EF4D5E6D103A 113MiB |
| 0 5305 G ...-token=53BA9EE005E85645FEB6537828E37D64 141MiB |
+-----------------------------------------------------------------------------+

== cuda libs ===================================================
/usr/local/cuda-9.2/doc/man/man7/libcudart.7
/usr/local/cuda-9.2/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148


### Describe the problem
I converted caffe ssd model to tensorflow/tensorfow lite. There are small differences in caffe and tensorflow detection layer (i.e., decode box and coordinates difference). 
Tensorflow lite does not support all ops of tensorflow.
Detection layer in tensorflow lite is implemented in detect_postprocess.cc as custom op.
I changed detect_postprocess.cc to apply caffes' decode box and other functions. Then, I compiled tf with ""bazel build -c opt --jobs=8 //tensorflow/tools/pip_package:build_pip_package"" and also tried with ""bazel build -c opt ""//tensorflow/contrib/lite/kernels:builtin_ops"",
However, problem is when I load detect.tflite on android and/or PC (with tensorfow lite python api <<tf.VERSION = 1.8.0 does not work,tf version:: 1.10.0 workd>>), the detection results return the same result as what detect_postprocess.cc supposed to work even I changed functionality of detect_postprocess.cc.

In other words, tensorflow lite always refers very first version of detect_postprocess.cc

What is the problem?

"
21642,MKL Build Failing with new PR Merge,"Hello,

It looks like with the latest changes to the BUILD file under `tools/pip_package`, the references to MKL DNN is not being picked up any longer, because there's no `intel_mkl_ml` folder under `third_party`. There are `mkl` and `mkl_dnn` but not `intel_mkl_ml` under `third_party`, and that being said, it looks like PR #408 which has this MKL ""fix"" actually passed all the build tests (red X next to the commit)...

For what it's worth, I have both MKL and MKL-DNN built in my environment with the correct paths set, but TF always seems to want to build the MKL-DNN libraries themselves, so if I could even explicitly point to my environment's MKL library files, then that'd also work.

## Build command:
```
echo ""startup --batch"" >>/etc/bazel.bazelrc && \
    echo ""build --spawn_strategy=standalone --genrule_strategy=standalone"" >>/etc/bazel.bazelrc && \
    echo ""deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8"" | tee /etc/apt/sources.list.d/bazel.list && \
    curl https://bazel.build/bazel-release.pub.gpg | apt-key add - && \
    apt-get update && apt-get install -y bazel  && rm -rf /var/lib/apt/lists/* && \
    ldconfig && \
    pip uninstall -y tensorflow-tensorboard tb-nightly tf-nightly tensorflow && \
    cd /opt && \
    git clone https://github.com/tensorflow/tensorflow.git && \
    cd /opt/tensorflow && \
    # cp /opt/tensorflow/third_party/mkl/LICENSE /opt/tensorflow/third_party/mkl_dnn/LICENSE && \
    /bin/bash ./configure \
    && \
    bazel build \
    --config=mkl --config=opt  \
    --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" \
    --copt=-msse4.2 --copt=-msse4.1 --copt=-mavx --copt=-msse2 --copt=-msse3  \
    --copt=-O3 --copt=-mfpmath=both \
    --copt=""-DMKL_LP64"" \
    --copt=""-fPIC"" \
    --linkopt=""-lmkl_gf_lp64"" \
    --linkopt=""-lmkl_gnu_thread"" \
    --linkopt=""-dl"" \
    --linkopt=""-lpthread"" \
    --linkopt=""-lmkl_core"" \
    --linkopt=""-lm"" \
    --linkopt=""-lmkl_rt"" \
    --linkopt=""-lmkldnn"" \
    tensorflow/tools/pip_package:build_pip_package && \
    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip && \
    pip install --no-deps /tmp/pip/tensorflow-*.whl 
```

## Error:
```
/opt/tensorflow/tensorflow/tools/pip_package/BUILD:206:1: no such package 'third_party/intel_mkl_ml': BUILD file not found on package path and referenced by '//tensorflow/tools/pip_package:build_pip_package'```"
21638,[XLA] ResourceExhaustedError when trying to define a Sequential model in Keras under jit_scope context manager,"Intel i5-2430M, 6 GB RAM
Linux Ubuntu 16.04 | Python 3.6.5 | Bazel 0.16.0 | GCC 5.4.0
TensorFlow 1.8 compiled from source with MKL and XLA support

---

So, my issue is that I try to use XLA for CPU via Keras that is embedded in TensorFlow 1.8 using the `tf.contrib.compiler.jit.experimental_jit_scope` (for CPU it's the only way I know to enable XLA, using `ConfigProto` doesn't work on CPU for me). For some strange reason I am thrown `ResourceExhaustedError` when trying to allocate 0 bytes.  Looks like something's wrong, either in TensorFlow or Keras. Below is the listing of the code I use and the full trace.

### Code

```python
import tensorflow as tf
from tensorflow.python.client import timeline

import numpy as np

JIT_SCOPE = tf.contrib.compiler.jit.experimental_jit_scope

options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)                   
run_metadata = tf.RunMetadata()

(train_x, train_y), _ = tf.keras.datasets.mnist.load_data()

train_x = np.expand_dims(train_x, axis=-1) / 255.
train_y = tf.keras.utils.to_categorical(train_y)

with JIT_SCOPE():                                                               
    model = tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(16, (3, 3), activation=""relu"", input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPool2D((2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(10, activation=""softmax"")
    ])

    model.compile(""sgd"", ""categorical_crossentropy"", options=options, run_metadata=run_metadata)

model.fit(train_x, train_y) # error happens at this moment

trace = timeline.Timeline(step_stats=run_metadata.step_stats)
with open(""timeline.ctr.json"", ""w"") as f:
    f.write(trace.generate_chrome_trace_format())

```

### Traceback

```
Epoch 1/1
2018-08-15 20:28:54.784459: I tensorflow/compiler/xla/service/service.cc:159] XLA service 0x7f70ec071a30 executing computations on platform Host. Devices:
2018-08-15 20:28:54.784509: I tensorflow/compiler/xla/service/service.cc:167]   StreamExecutor device (0): <undefined>, <undefined>
2018-08-15 20:28:55.548381: E tensorflow/core/common_runtime/bfc_allocator.cc:246] tried to allocate 0 bytes
2018-08-15 20:28:55.548481: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
2018-08-15 20:28:55.561315: E tensorflow/core/common_runtime/bfc_allocator.cc:246] tried to allocate 0 bytes
2018-08-15 20:28:55.561365: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
---------------------------------------------------------------------------
ResourceExhaustedError                    Traceback (most recent call last)
~/Work/2018_Summer_CERN/tf_v_tmva/tf_opt/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1321     try:
-> 1322       return fn(*args)
   1323     except errors.OpError as e:

~/Work/2018_Summer_CERN/tf_v_tmva/tf_opt/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1306       return self._call_tf_sessionrun(
-> 1307           options, feed_dict, fetch_list, target_list, run_metadata)
   1308 

~/Work/2018_Summer_CERN/tf_v_tmva/tf_opt/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1408           self._session, options, feed_dict, fetch_list, target_list,
-> 1409           run_metadata)
   1410     else:

ResourceExhaustedError: Out of memory while trying to allocate 0 bytes.
	 [[Node: cluster_1/_4/_5 = _XlaLaunch[Nresources=0, Targs=[], Tconstants=[], Tresults=[DT_FLOAT], function=cluster_1[_XlaCompiledKernel=true, _XlaNumConstantArgs=0, _XlaNumResourceArgs=0], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


During handling of the above exception, another exception occurred:

ResourceExhaustedError                    Traceback (most recent call last)
<ipython-input-46-dbab7a29ab1f> in <module>()
----> 1 model.fit(train_x[:1000], train_y[:1000], epochs=1)

~/Work/2018_Summer_CERN/tf_v_tmva/tf_opt/.venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1214           initial_epoch=initial_epoch,
   1215           steps_per_epoch=steps_per_epoch,
-> 1216           validation_steps=validation_steps)
   1217 
   1218   def evaluate(self,

~/Work/2018_Summer_CERN/tf_v_tmva/tf_opt/.venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training_arrays.py in fit_loop(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
    243           ins_batch[i] = ins_batch[i].toarray()
    244 
--> 245         outs = f(ins_batch)
    246         if not isinstance(outs, list):
    247           outs = [outs]

~/Work/2018_Summer_CERN/tf_v_tmva/tf_opt/.venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py in __call__(self, inputs)
   2797       feed_dict = {}
   2798 
-> 2799     session = get_session()
   2800     data_tensors_to_feed = []
   2801     for tensor, value in zip(self.inputs, inputs):

~/Work/2018_Summer_CERN/tf_v_tmva/tf_opt/.venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py in get_session()
    440   if not _MANUAL_VAR_INIT:
    441     with session.graph.as_default():
--> 442       _initialize_variables(session)
    443   return session
    444 

~/Work/2018_Summer_CERN/tf_v_tmva/tf_opt/.venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py in _initialize_variables(session)
    671       v._keras_initialized = True
    672     if uninitialized_vars:
--> 673       session.run(variables_module.variables_initializer(uninitialized_vars))
    674 
    675 

~/Work/2018_Summer_CERN/tf_v_tmva/tf_opt/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    898     try:
    899       result = self._run(None, fetches, feed_dict, options_ptr,
--> 900                          run_metadata_ptr)
    901       if run_metadata:
    902         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~/Work/2018_Summer_CERN/tf_v_tmva/tf_opt/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1133     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1134       results = self._do_run(handle, final_targets, final_fetches,
-> 1135                              feed_dict_tensor, options, run_metadata)
   1136     else:
   1137       results = []

~/Work/2018_Summer_CERN/tf_v_tmva/tf_opt/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1314     if handle is None:
   1315       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1316                            run_metadata)
   1317     else:
   1318       return self._do_call(_prun_fn, handle, feeds, fetches)

~/Work/2018_Summer_CERN/tf_v_tmva/tf_opt/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333         except KeyError:
   1334           pass
-> 1335       raise type(e)(node_def, op, message)
   1336 
   1337   def _extend_graph(self):

ResourceExhaustedError: Out of memory while trying to allocate 0 bytes.
	 [[Node: cluster_1/_4/_5 = _XlaLaunch[Nresources=0, Targs=[], Tconstants=[], Tresults=[DT_FLOAT], function=cluster_1[_XlaCompiledKernel=true, _XlaNumConstantArgs=0, _XlaNumResourceArgs=0], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
```
"
21637,Tensorflow Prediction Artifacts,"### System Information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0.176/7.1.2
- **GPU model and memory**: Tesla P40 24GB
- **Exact command to reproduce**: python3 example.py

### Describe the problem
Hello,
I am using Tensorflow on 3D image data and I observe some strange prediction artifacts once my input exceeds a certain size.
I created a minimal working example (below) which produces the artifacts. In the example all weights of the loaded u-net are initialized to 0.5 and then a 3D volume (Z: 180, Y: 196, X: 240) with a gradient in Z direction is given as prediction input. The returned output contains the gradient up to Z around 140 and then the output values change to smaller values. 
![observation](https://user-images.githubusercontent.com/5450869/44163361-7661fc00-a0c3-11e8-8c67-a3524370b632.png)

If a slightly smaller 3D volume (Z: 176, Y: 196, X: 240) is given, the output contains the full gradient as expected. 

Unfortunately I don't know if this is a TensorFlow, CUDA, cuDNN or some other sort of bug. I am grateful for any explanation or any hints on what it could be or how to fix this issue (without cropping/tiling the input).

### Source code / logs

The minimum example:
```
import tensorflow as tf
from tensorflow.keras.models import model_from_json
import numpy as np
from tifffile import imsave

# load u-net architecture, kernel-size = 3, depth = 2
json_file = open('model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
model = model_from_json(loaded_model_json)

# set all weights to 0.5
w = model.get_weights()
init = []
for l in w:
    init.append(l*0 + 0.5)

model.set_weights(init)

tmp = np.zeros((180, 196, 240), np.float32)
for i in range(tmp.shape[0]):
    tmp[i] = i

tmp = tmp[:, :, :, np.newaxis]

prediction_fail = np.moveaxis(model.predict(np.moveaxis(tmp[np.newaxis], 4, -1))[0], 3, 0)
tmp = tmp[:176]
prediction_succ = np.moveaxis(model.predict(np.moveaxis(tmp[np.newaxis], 4, -1))[0], 3, 0)

imsave('input.tif', tmp)
imsave('prediction_fail.tif', prediction_fail[0])
imsave('prediction_success.tif', prediction_succ[0])
```

The model.json with the u-net architecture:
```
{""class_name"": ""Model"", ""config"": {""name"": ""model_3"", ""layers"": [{""name"": ""input"", ""class_name"": ""InputLayer"", ""config"": {""batch_input_shape"": [null, null, null, null, 1], ""dtype"": ""float32"", ""sparse"": false, ""name"": ""input""}, ""inbound_nodes"": []}, {""name"": ""down_level_0_no_0"", ""class_name"": ""Conv3D"", ""config"": {""name"": ""down_level_0_no_0"", ""trainable"": true, ""filters"": 32, ""kernel_size"": [3, 3, 3], ""strides"": [1, 1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1, 1], ""activation"": ""relu"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""VarianceScaling"", ""config"": {""scale"": 1.0, ""mode"": ""fan_avg"", ""distribution"": ""uniform"", ""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""inbound_nodes"": [[[""input"", 0, 0, {}]]]}, {""name"": ""down_level_0_no_1"", ""class_name"": ""Conv3D"", ""config"": {""name"": ""down_level_0_no_1"", ""trainable"": true, ""filters"": 32, ""kernel_size"": [3, 3, 3], ""strides"": [1, 1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1, 1], ""activation"": ""relu"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""VarianceScaling"", ""config"": {""scale"": 1.0, ""mode"": ""fan_avg"", ""distribution"": ""uniform"", ""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""inbound_nodes"": [[[""down_level_0_no_0"", 0, 0, {}]]]}, {""name"": ""max_0"", ""class_name"": ""MaxPooling3D"", ""config"": {""name"": ""max_0"", ""trainable"": true, ""pool_size"": [2, 2, 2], ""padding"": ""valid"", ""strides"": [2, 2, 2], ""data_format"": ""channels_last""}, ""inbound_nodes"": [[[""down_level_0_no_1"", 0, 0, {}]]]}, {""name"": ""down_level_1_no_0"", ""class_name"": ""Conv3D"", ""config"": {""name"": ""down_level_1_no_0"", ""trainable"": true, ""filters"": 64, ""kernel_size"": [3, 3, 3], ""strides"": [1, 1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1, 1], ""activation"": ""relu"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""VarianceScaling"", ""config"": {""scale"": 1.0, ""mode"": ""fan_avg"", ""distribution"": ""uniform"", ""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""inbound_nodes"": [[[""max_0"", 0, 0, {}]]]}, {""name"": ""down_level_1_no_1"", ""class_name"": ""Conv3D"", ""config"": {""name"": ""down_level_1_no_1"", ""trainable"": true, ""filters"": 64, ""kernel_size"": [3, 3, 3], ""strides"": [1, 1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1, 1], ""activation"": ""relu"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""VarianceScaling"", ""config"": {""scale"": 1.0, ""mode"": ""fan_avg"", ""distribution"": ""uniform"", ""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""inbound_nodes"": [[[""down_level_1_no_0"", 0, 0, {}]]]}, {""name"": ""max_1"", ""class_name"": ""MaxPooling3D"", ""config"": {""name"": ""max_1"", ""trainable"": true, ""pool_size"": [2, 2, 2], ""padding"": ""valid"", ""strides"": [2, 2, 2], ""data_format"": ""channels_last""}, ""inbound_nodes"": [[[""down_level_1_no_1"", 0, 0, {}]]]}, {""name"": ""middle_0"", ""class_name"": ""Conv3D"", ""config"": {""name"": ""middle_0"", ""trainable"": true, ""filters"": 128, ""kernel_size"": [3, 3, 3], ""strides"": [1, 1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1, 1], ""activation"": ""relu"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""VarianceScaling"", ""config"": {""scale"": 1.0, ""mode"": ""fan_avg"", ""distribution"": ""uniform"", ""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""inbound_nodes"": [[[""max_1"", 0, 0, {}]]]}, {""name"": ""middle_2"", ""class_name"": ""Conv3D"", ""config"": {""name"": ""middle_2"", ""trainable"": true, ""filters"": 64, ""kernel_size"": [3, 3, 3], ""strides"": [1, 1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1, 1], ""activation"": ""relu"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""VarianceScaling"", ""config"": {""scale"": 1.0, ""mode"": ""fan_avg"", ""distribution"": ""uniform"", ""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""inbound_nodes"": [[[""middle_0"", 0, 0, {}]]]}, {""name"": ""up_sampling3d_5"", ""class_name"": ""UpSampling3D"", ""config"": {""name"": ""up_sampling3d_5"", ""trainable"": true, ""size"": [2, 2, 2], ""data_format"": ""channels_last""}, ""inbound_nodes"": [[[""middle_2"", 0, 0, {}]]]}, {""name"": ""concatenate_5"", ""class_name"": ""Concatenate"", ""config"": {""name"": ""concatenate_5"", ""trainable"": true, ""axis"": -1}, ""inbound_nodes"": [[[""up_sampling3d_5"", 0, 0, {}], [""down_level_1_no_1"", 0, 0, {}]]]}, {""name"": ""up_level_1_no_0"", ""class_name"": ""Conv3D"", ""config"": {""name"": ""up_level_1_no_0"", ""trainable"": true, ""filters"": 64, ""kernel_size"": [3, 3, 3], ""strides"": [1, 1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1, 1], ""activation"": ""relu"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""VarianceScaling"", ""config"": {""scale"": 1.0, ""mode"": ""fan_avg"", ""distribution"": ""uniform"", ""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""inbound_nodes"": [[[""concatenate_5"", 0, 0, {}]]]}, {""name"": ""up_level_1_no_2"", ""class_name"": ""Conv3D"", ""config"": {""name"": ""up_level_1_no_2"", ""trainable"": true, ""filters"": 32, ""kernel_size"": [3, 3, 3], ""strides"": [1, 1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1, 1], ""activation"": ""relu"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""VarianceScaling"", ""config"": {""scale"": 1.0, ""mode"": ""fan_avg"", ""distribution"": ""uniform"", ""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""inbound_nodes"": [[[""up_level_1_no_0"", 0, 0, {}]]]}, {""name"": ""up_sampling3d_6"", ""class_name"": ""UpSampling3D"", ""config"": {""name"": ""up_sampling3d_6"", ""trainable"": true, ""size"": [2, 2, 2], ""data_format"": ""channels_last""}, ""inbound_nodes"": [[[""up_level_1_no_2"", 0, 0, {}]]]}, {""name"": ""concatenate_6"", ""class_name"": ""Concatenate"", ""config"": {""name"": ""concatenate_6"", ""trainable"": true, ""axis"": -1}, ""inbound_nodes"": [[[""up_sampling3d_6"", 0, 0, {}], [""down_level_0_no_1"", 0, 0, {}]]]}, {""name"": ""up_level_0_no_0"", ""class_name"": ""Conv3D"", ""config"": {""name"": ""up_level_0_no_0"", ""trainable"": true, ""filters"": 32, ""kernel_size"": [3, 3, 3], ""strides"": [1, 1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1, 1], ""activation"": ""relu"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""VarianceScaling"", ""config"": {""scale"": 1.0, ""mode"": ""fan_avg"", ""distribution"": ""uniform"", ""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""inbound_nodes"": [[[""concatenate_6"", 0, 0, {}]]]}, {""name"": ""up_level_0_no_2"", ""class_name"": ""Conv3D"", ""config"": {""name"": ""up_level_0_no_2"", ""trainable"": true, ""filters"": 32, ""kernel_size"": [3, 3, 3], ""strides"": [1, 1, 1], ""padding"": ""same"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1, 1], ""activation"": ""relu"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""VarianceScaling"", ""config"": {""scale"": 1.0, ""mode"": ""fan_avg"", ""distribution"": ""uniform"", ""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""inbound_nodes"": [[[""up_level_0_no_0"", 0, 0, {}]]]}, {""name"": ""conv3d_3"", ""class_name"": ""Conv3D"", ""config"": {""name"": ""conv3d_3"", ""trainable"": true, ""filters"": 1, ""kernel_size"": [1, 1, 1], ""strides"": [1, 1, 1], ""padding"": ""valid"", ""data_format"": ""channels_last"", ""dilation_rate"": [1, 1, 1], ""activation"": ""linear"", ""use_bias"": true, ""kernel_initializer"": {""class_name"": ""VarianceScaling"", ""config"": {""scale"": 1.0, ""mode"": ""fan_avg"", ""distribution"": ""uniform"", ""seed"": null}}, ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}}, ""kernel_regularizer"": null, ""bias_regularizer"": null, ""activity_regularizer"": null, ""kernel_constraint"": null, ""bias_constraint"": null}, ""inbound_nodes"": [[[""up_level_0_no_2"", 0, 0, {}]]]}, {""name"": ""add_19"", ""class_name"": ""Add"", ""config"": {""name"": ""add_19"", ""trainable"": true}, ""inbound_nodes"": [[[""conv3d_3"", 0, 0, {}], [""input"", 0, 0, {}]]]}, {""name"": ""activation_52"", ""class_name"": ""Activation"", ""config"": {""name"": ""activation_52"", ""trainable"": true, ""activation"": ""linear""}, ""inbound_nodes"": [[[""add_19"", 0, 0, {}]]]}], ""input_layers"": [[""input"", 0, 0]], ""output_layers"": [[""activation_52"", 0, 0]]}, ""keras_version"": ""2.2.0"", ""backend"": ""tensorflow""}
```
"
21636,"dataset cache, repeat problem","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21635,Can't compile frozen facenet graph ,"Have I written custom code: NO
OS Platform and Distribution: Ubuntu 16.04 x86_64
TensorFlow installed from: github, from source
TensorFlow version: 1.9
Bazel version: 0.16.1
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: 
`./tfcompile --graph=frozen_20170512-110547.pb --config=frozen_20170512-110547.pbtxt --cpp_class=""my_super_class"" --target_features=""+avx2""`
Mobile device: No

My .pbtx:
```
feed {
  id { node_name: ""input"" }
  shape {
    dim { size: 160 }
    dim { size: 160 }
  }
}

fetch {
  id { node_name: ""embeddings"" }
}
```
And I've got this:
INVALID ARGUMENTS: Unable to functionalize control flow in graph: Switch ('InceptionResnetV1/Conv2d_1a_3x3/BatchNorm/cond/Switch_1') has operands ('InceptionResnetV1/Conv2d_1a_3x3/BatchNorm/cond/Switch_1/Switch' and 'InceptionResnetV1/Conv2d_1a_3x3/BatchNorm/cond/pred_id') that have different switch depths (1 != 0)
 
What Do I do wrong?"
21634,Compilation Error: suffix or operands invalid for `vpaddd',"Hi,
i'm trying to install tensorflow with CUDA support and I have the following issue whe i execute 
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures

ERROR: 
/home/user/.cache/bazel/_bazel_user/95b102d38f4be088a2028c5510fdf1c2/external/boringssl/BUILD:130:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command
cd /home/user/.cache/bazel/_bazel_user/95b102d38f4be088a2028c5510fdf1c2/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/share/apps/libxc-3.0.1/lib/:/share/apps/libgpuarray/lib/:/opt/python/lib/:/share/apps/gcc-6.3.0/lib64:/opt/gridengine/lib/linux-x64 \
    PATH=/share/apps/python/python2.7/bin/:/opt/python/bin/:/share/apps/cmake-3.9.1/bin:/share/apps/gcc-6.3.0/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/bio/ncbi/bin:/opt/bio/mpiblast/bin:/opt/bio/EMBOSS/bin:/opt/bio/clustalw/bin:/opt/bio/tcoffee/bin:/opt/bio/hmmer/bin:/opt/bio/phylip/exe:/opt/bio/mrbayes:/opt/bio/fasta:/opt/bio/glimmer/bin:/opt/bio/glimmer/scripts:/opt/bio/gromacs/bin:/opt/bio/gmap/bin:/opt/bio/tigr/bin:/opt/bio/autodocksuite/bin:/opt/bio/wgs/bin:/opt/eclipse:/opt/ganglia/bin:/opt/ganglia/sbin:/usr/java/latest/bin:/opt/rocks/bin:/opt/rocks/sbin:/opt/gridengine/bin/linux-x64:/share/apps/local/bin:/home/user/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/external/boringssl/_objs/crypto/chacha20_poly1305_x86_64.pic.d -iquote external/boringssl -iquote bazel-out/host/genfiles/external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -isystem external/boringssl/src/include -isystem bazel-out/host/genfiles/external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -g0 '-march=native' -Wa,--noexecstack '-D_XOPEN_SOURCE=700' -Wall -Werror '-Wformat=2' -Wsign-compare -Wmissing-field-initializers -Wwrite-strings -Wshadow -fno-common '-std=c11' -Wmissing-prototypes -Wold-style-definition -Wstrict-prototypes -c external/boringssl/linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S -o bazel-out/host/bin/external/boringssl/_objs/crypto/chacha20_poly1305_x86_64.pic.o)
external/boringssl/linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S: Assembler messages:
external/boringssl/linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S:4104: Error: no such instruction: `vbroadcasti128 0(%r9),%ymm4'
external/boringssl/linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S:4105: Error: no such instruction: `vbroadcasti128 16(%r9),%ymm8'
external/boringssl/linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S:4106: Error: no such instruction: `vbroadcasti128 32(%r9),%ymm12'
external/boringssl/linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S:4107: Error: suffix or operands invalid for `vpaddd'
external/boringssl/linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S:4118: Error: suffix or operands invalid for `vpaddd'
external/boringssl/linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S:4119: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S:4120: Error: suffix or operands invalid for `vpshufb'
external/boringssl/linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S:4121: Error: suffix or operands invalid for `vpaddd'
external/boringssl/linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S:4122: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S:4123: Error: suffix or operands invalid for `vpsrld'
external/boringssl/linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S:4124: Error: suffix or operands invalid for `vpslld'
external/boringssl/linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S:4125: Error: suffix or operands invalid for `vpxor'
external/boringssl/linux-x86_64/crypto/cipher_extra/chacha20_poly1305_x86_64.S:4126: Error: suffix or operands invalid for `vpaddd'
"
21633,TFLite Model Benchmark Tool : tensorflow/contrib/lite/error_reporter.cc:52: error: undefined reference to '__android_log_vprint',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Macbook Pro  10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: 1.10.0
- **TensorFlow version (use command below)**:  tf.VERSION = 1.10.0
- **Python version**: Python 2.7.10
- **Bazel version (if compiling from source)**: bazel release 0.15.2-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.1.0 (clang-902.0.39.2)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
-**Android SDK Version : Android SDK Platform 28
-**ANdroid NDK Version : android-ndk-r14b
- **Exact command to reproduce**:
bazel build -c opt \
  --config=android_arm \
  --cxxopt='--std=c++11' \
  tensorflow/contrib/lite/tools/benchmark:benchmark_model


### Describe the problem
I am trying to build the benchmark_model for android for the TFLite Model Benchmark Tool, but I get an ""error: undefined reference to '__android_log_vprint'"" error.

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/benchmark#reducing-variance-between-runs-on-android

bazel build -c opt \
  --config=android_arm \
  --cxxopt='--std=c++11' \
  tensorflow/contrib/lite/tools/benchmark:benchmark_model

### Source code / logs

ERROR: 
tensorflow/contrib/lite/error_reporter.cc:52: error: undefined reference to '__android_log_vprint'
tensorflow/contrib/lite/nnapi_delegate.cc:45: error: undefined reference to '__android_log_vprint'
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/contrib/lite/tools/benchmark:benchmark_model failed to build

"
21632,Python free error,"```
*** Error in `python': free(): invalid pointer: 0x00007fe15f3e5340 ***
```
The above error happened when I included the following 2 lines.
```
from tensorflow.python.ops import math_ops
from tensorflow.contrib.boosted_trees.python.ops import quantile_ops
```"
21631,Keras model converted to an Estimator does not handle 1D labels correctly,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: No
- **TensorFlow version (use command below)**: v1.10.0-rc1-19-g656e7a2b34 1.10.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: see script below (keras model converges, not estimator)

### Describe the problem
Keras model converges on MNIST (> 97% accuracy after 5 epochs with batches of 32 instances), using the class IDs as labels (i.e., a 1D array of int32). However, after I convert the model to an Estimator, it does not converge (around 10% accuracy, again after 5 epochs with batches of 32 instances). Once I reshape the labels to a column vector (`y_train.reshape(-1, 1)`), it converges well. I double-checked with a `DNNClassifier`, and it accepts the labels both as a 1D array or as a column vector (2D). I believe this is a bug, and people might spend hours searching for the cause (as I did).

### Source code / logs
If you run the code below, it will train three models on MNIST, first using 1D labels, then using 2D labels (i.e., column vectors):

1. a Keras model
2. the same Keras model converted to an Estimator
3. an equivalent `DNNClassifier`

The final outputs below show that they all converge except for the Keras model converted to an Estimator when the labels are 1D (I removed the ""You CPU supports..."" and ""Using temporary folder..."" warnings from the outputs). The behavior is identical in eager mode and in graph mode.

```
==== Training with 1D labels... ====
Keras model:
    Eval: [0.07029167589747813, 0.9792]
Keras model converted to an estimator:
    Eval: ({'accuracy': 0.10245253, 'loss': 0.07640489, 'global_step': 18751}, [])
DNNClassifier:
    Eval: ({'accuracy': 0.1135, 'average_loss': 2.3010197, 'loss': 291.2683, 'global_step': 9375}, [])

==== Training with 2D labels... ====
Keras model:
    Eval: [0.06944960513310507, 0.9779]
Keras model converted to an estimator:
    Eval: ({'accuracy': 0.97171676, 'loss': 0.09046984, 'global_step': 18751}, [])
DNNClassifier:
    Eval: ({'accuracy': 0.976, 'average_loss': 0.08481478, 'loss': 10.736049, 'global_step': 9375}, [])
```

Here is the source code:

```python
from __future__ import division, print_function, unicode_literals

import logging
logging.basicConfig(level=logging.WARNING)

import tensorflow as tf
import tensorflow.contrib.eager as tfe
#tf.enable_eager_execution() # same problem in eager mode or graph mode
from tensorflow import keras
import numpy as np

n_epochs = 5
batch_size = 32

def create_keras_model():
    keras.backend.clear_session()
    model = keras.models.Sequential([
        keras.layers.Dense(300, activation=""relu"", input_shape=[28*28]),
        keras.layers.Dense(100, activation=""relu""),
        keras.layers.Dense(10, activation=""softmax"")
    ])
    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)
    model.compile(loss=""sparse_categorical_crossentropy"", optimizer=optimizer,
                  metrics=[""accuracy""])
    return model

def create_keras_estimator():
    model = create_keras_model()
    input_name = model.input_names[0]
    estimator = tf.keras.estimator.model_to_estimator(model)
    return estimator, input_name

def create_dnn_estimator():
    input_name = ""pixels""
    pixels = tf.feature_column.numeric_column(input_name, shape=[28 * 28])
    estimator = tf.estimator.DNNClassifier(hidden_units=[300, 100, 10],
                                           n_classes=10,
                                           feature_columns=[pixels])
    return estimator, input_name

def train_and_evaluate_estimator(estimator, input_name,
                                 X_train, y_train, X_test, y_test):
    train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={input_name: X_train}, y=y_train, num_epochs=5, batch_size=32,
        shuffle=True)
    test_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={input_name: X_test}, y=y_test, shuffle=False)
    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)
    eval_spec = tf.estimator.EvalSpec(input_fn=test_input_fn)
    return tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)

def load_and_scale_MNIST():
    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()
    X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0
    X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0
    y_train = y_train.astype(np.int32)
    y_test = y_test.astype(np.int32)
    return X_train, y_train, X_test, y_test

if __name__ == '__main__':
    X_train, y_train, X_test, y_test = load_and_scale_MNIST()
    
    for run in range(2):
        if run == 0:
            print(""==== Training with 1D labels... ===="")
            assert y_train.ndim == 1 and y_test.ndim == 1
        else:
            y_train = y_train.reshape(-1, 1)
            y_test = y_test.reshape(-1, 1)
            print()
            print(""==== Training with 2D labels... ===="")
            assert y_train.ndim == 2 and y_test.ndim == 2

        print(""Keras model:"")
        keras_model = create_keras_model()
        keras_model.fit(X_train, y_train, epochs=n_epochs,
                        batch_size=batch_size, verbose=0)
        print(""    Eval:"", keras_model.evaluate(X_test, y_test, verbose=0))

        print(""Keras model converted to an estimator:"")
        keras_estimator, input_name = create_keras_estimator()
        print(""    Eval:"", train_and_evaluate_estimator(
            keras_estimator, input_name, X_train, y_train, X_test, y_test))

        print(""DNNClassifier:"")
        dnn_estimator, input_name = create_dnn_estimator()
        print(""    Eval:"", train_and_evaluate_estimator(
            dnn_estimator, input_name, X_train, y_train, X_test, y_test))
```"
21630,Distributed training with SyncReplicasOptimizer may throw session close error after training ,"
### System information

== cat /etc/issue ===============================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.1.0 (clang-902.0.39.1)
Target: x86_64-apple-darwin17.5.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy                              1.14.2
protobuf                           3.5.2.post1
tensorflow                         1.8.0
tensorflow-hub                     0.1.0
tensorflow-model-analysis          0.6.0
tensorflow-serving-api             1.8.0
tensorflow-tensorboard             1.5.0
tensorflow-transform               0.6.0
tensorflowjs                       0.1.0
tensorflowonspark                  1.0.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 106: nvidia-smi: command not found

== cuda libs  ===================================================

== cat /etc/issue ===============================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.1.0 (clang-902.0.39.1)
Target: x86_64-apple-darwin17.5.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy                              1.14.2
protobuf                           3.5.2.post1
tensorflow                         1.8.0
tensorflow-hub                     0.1.0
tensorflow-model-analysis          0.6.0
tensorflow-serving-api             1.8.0
tensorflow-tensorboard             1.5.0
tensorflow-transform               0.6.0
tensorflowjs                       0.1.0
tensorflowonspark                  1.0.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 106: nvidia-smi: command not found

== cuda libs  ===================================================

### Describe the problem

We have used `MonitoredTrainingSession` and  `SyncReplicasOptimizer` for distributed sync training. All the master(the worker with index 0) and workers will work synchronously. But at the end of master process, it throw `CancelledError` while trying to close the session. Sometimes we re-run all the processes and the error will not be thrown.

<img width=""1439"" alt=""screen shot 2018-08-15 at 5 37 58 pm"" src=""https://user-images.githubusercontent.com/2715000/44142776-dbe72ba0-a0b3-11e8-892f-dc3012fde656.png"">

### Source code / logs

Above."
21629,Workers or master may not start when other tasks exit in async distributed training,"
### System information

== cat /etc/issue ===============================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.1.0 (clang-902.0.39.1)
Target: x86_64-apple-darwin17.5.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy                              1.14.2
protobuf                           3.5.2.post1
tensorflow                         1.8.0
tensorflow-hub                     0.1.0
tensorflow-model-analysis          0.6.0
tensorflow-serving-api             1.8.0
tensorflow-tensorboard             1.5.0
tensorflow-transform               0.6.0
tensorflowjs                       0.1.0
tensorflowonspark                  1.0.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 106: nvidia-smi: command not found

== cuda libs  ===================================================

== cat /etc/issue ===============================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.1.0 (clang-902.0.39.1)
Target: x86_64-apple-darwin17.5.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy                              1.14.2
protobuf                           3.5.2.post1
tensorflow                         1.8.0
tensorflow-hub                     0.1.0
tensorflow-model-analysis          0.6.0
tensorflow-serving-api             1.8.0
tensorflow-tensorboard             1.5.0
tensorflow-transform               0.6.0
tensorflowjs                       0.1.0
tensorflowonspark                  1.0.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 106: nvidia-smi: command not found

== cuda libs  ===================================================

### Describe the problem

We have implement the async distributed TensorFlow training and find some issues. Once all the master(the worker with index 0), ps and workers start, the master will start training. And sometime(not always) one of the workers will halt and have no chance to end the progress.

Here is the [test distributed script](https://github.com/tobegit3hub/distributed_tensorflow/blob/master/auto_stop_ps/task.py) .

Here is the log of master which is normal and successfully finished.

<img width=""1437"" alt=""screen shot 2018-08-15 at 5 07 00 pm"" src=""https://user-images.githubusercontent.com/2715000/44141773-bbfdb51e-a0b0-11e8-922a-c16e15d76637.png"">

Here is the log of ps which is normal and join for all workers finished.

<img width=""1439"" alt=""screen shot 2018-08-15 at 5 06 48 pm"" src=""https://user-images.githubusercontent.com/2715000/44141836-dda5391c-a0b0-11e8-82b0-a1390182454b.png"">

Here is the log of worker which is abnormal. It failed to run since master has been finished. If the epoch is larger and master need more time to run, this worker may run and finish as well.

<img width=""1439"" alt=""screen shot 2018-08-15 at 5 06 32 pm"" src=""https://user-images.githubusercontent.com/2715000/44141884-097955dc-a0b1-11e8-8f56-0e68d1478744.png"">

### Source code / logs

Above.
"
21627,The return value of _has_variables() in freeze_graph.py is not consistent with what its name imply,"https://github.com/tensorflow/tensorflow/blob/3109a8fb2c966ea5ce198c8906a703ee48af75fb/tensorflow/python/tools/freeze_graph.py#L71-L74



I'm a bit of surprised to notice this. Shouldn't the helper function `_has_variables` simply returns `False` (line 74) when no `Variable` has been found in the operations? Shall we change the function name to `_has_no_variables` or reverse the true/false logic?

Furthermore, 
https://github.com/tensorflow/tensorflow/blob/3109a8fb2c966ea5ce198c8906a703ee48af75fb/tensorflow/python/tools/freeze_graph.py#L171-L173

if `_has_variables` returns true, why it prints out ""no variable were found....""? 

Bad naming.

Have I written custom code: no
OS Platform and Distribution: ubuntu 14.04
TensorFlow installed from: tensorflow-gpu
TensorFlow version: 1.9
Bazel version: N/A
CUDA/cuDNN version: 9.0/7.0
GPU model and memory: TITAN Xp 12GB
Exact command to reproduce: N/A (just read the source code I pointed out)
Mobile device: N/A"
21624,unknown type name 'lzma_check',
21623,How to fix pb to tflite error,"### System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):mac os 10.13.5

TensorFlow installed from (source or binary):pip

TensorFlow version (use command below):'1.8.0

Python version: 3.6.4

Bazel version (if compiling from source):
Build label: 0.14.0-homebrew
Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri Jun 1 14:26:58 2018 (1527863218)
Build timestamp: 1527863218
Build timestamp as int: 1527863218

GCC/Compiler version (if compiling from source):no

CUDA/cuDNN version:no

GPU model and memory:no

Exact command to reproduce:no


### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
`directory:/models-master/research/slim/nets/mobilenet_v1_train.py`
mobilenet_v1_train.py:Training department PBTXT and CKPT
`directory:/tensorflow/tensorflow/python/tools/freeze_graph.py`
freeze_graph.py:Use the command to roll out the pb
The command:
```
bazel-bin/tensorflow/python/tools/freeze_graph 
--input_graph=/image/model_frozen.pb \
--input_checkpoint=/image/model.ckpt-10000 \
--output_node_names=MobilenetV1/Predictions/Reshape_1 \
--output_graph=/image/freesn_frozen.pb
```

We need to turn pb into tflite
The command:

```
bazel build tensorflow/contrib/lite/toco:toco && \
  /Users/dchealth/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco \
  --input_file=/image/freesn_frozen.pb \
  --output_file=/image/freesn_frozen.tflite \
  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \
  --inference_type=QUANTIZED_UINT8 \
  --input_shape=""1,224, 224,3"" \
  --input_array=input \
  --output_array=MobilenetV1/Predictions/Reshape_1 \
  --std_value=128 --mean_value=128

```

### Pb turns tflite wrong
2018-08-09 13:53:42.799490: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: FIFOQueueV2
2018-08-09 13:53:42.800203: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: QueueDequeueManyV2
2018-08-09 13:53:42.819149: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.819195: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.819249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.819387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.819493: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.819514: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.819537: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.819665: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.819816: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.819840: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.819866: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.820013: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.820132: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.820154: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.820179: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.820315: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.820452: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.820471: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.820491: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.820596: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.820700: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.820718: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.820739: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.820844: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.820966: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.820984: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.821000: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.821112: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.821235: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.821258: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.821282: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.821434: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.821565: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.821585: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.821605: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.821731: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.821832: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.821851: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.821875: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.821997: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.822146: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.822170: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.822195: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.822334: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.822434: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.822458: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.822477: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.822582: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.822715: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.822736: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.822761: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.822887: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.822993: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.823037: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.823061: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.823205: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.823339: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.823360: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.823387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.823494: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.823608: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.823641: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.823664: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.823808: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.823951: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.823971: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.823996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.824128: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.824234: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.824270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.824292: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.824424: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.824582: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.824606: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.824630: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.824771: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.824884: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.824898: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.824914: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.825035: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.825182: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.825201: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.825224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.825366: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.825489: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.825510: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.825528: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.825666: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.825811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.825833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.825849: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.825990: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.826120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.826137: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.826167: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.826307: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.826479: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.826503: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.826526: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.826639: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.826794: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.826810: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.826832: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.826965: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.827130: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.827155: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.827191: I tensorflow/contrib/lite/toco/import_tensorflow.cc:181] Unsupported data type in placeholder op: 2
2018-08-09 13:53:42.827316: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Reciprocal
2018-08-09 13:53:42.827440: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.827708: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.827756: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.827922: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.827961: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.828082: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.828208: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.828248: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.828352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.828514: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.828576: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.828722: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.828760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.828858: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.828940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.828974: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.829096: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.829233: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.829297: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.829499: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.829542: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.829696: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.829852: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.829909: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.830068: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.830206: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.830266: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.830462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.830499: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.830618: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.830749: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.830788: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.830882: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.831007: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.831082: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.831230: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.831267: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.831362: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.831449: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.831484: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.831603: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.831741: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.831817: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.831964: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.832000: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.832092: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.832173: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.832208: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.832327: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.832469: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.832532: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.832701: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.832740: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.832839: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.832921: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.832955: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.833071: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.833216: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.833290: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.833467: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.833538: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.833661: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.833822: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.833850: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.833996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.834158: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.834222: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.834440: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.834483: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.834604: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.834716: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.834757: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.834874: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.835019: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.835083: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.835298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.835334: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.835456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.835566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.835602: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.835727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.835870: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.835935: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.836186: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.836249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.836370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.836493: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.836538: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.836664: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.836830: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.836891: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.837101: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.837133: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.837226: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.837333: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.837383: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.837507: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.837628: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.837685: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.837852: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.837883: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.837975: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.838071: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.838121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.838244: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.838363: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.838417: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.838584: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.838619: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.838720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.838832: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.838886: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.838984: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.839103: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.839156: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.839329: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.839366: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.839535: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.839620: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.839651: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.839744: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.839857: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.839920: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.840090: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.840140: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.840240: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.840336: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.840378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.840496: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.840644: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.840707: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.840876: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.840911: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.841008: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.841105: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.841155: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.841273: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.841392: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.841455: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.841622: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.841658: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.841771: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.841859: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.841885: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.841985: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.842103: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.842171: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.842353: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.842385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.842476: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.842575: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.842624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.842747: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.842880: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.842927: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.843095: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.843137: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.843231: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.843328: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.843386: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.843502: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.843638: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.843679: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.843848: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.843890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.843986: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.844097: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.844138: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.844256: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.844382: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.844440: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.844619: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.844669: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.844767: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.844875: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.844926: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.845040: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.845161: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.845210: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.845381: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.845414: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.845511: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.845611: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.845662: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.845779: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.845899: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.845949: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.846122: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.846154: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.846252: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.846348: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.846383: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.846493: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.846611: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.846659: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.846824: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.846850: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.846990: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.847086: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.847123: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.847277: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.847426: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.847497: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.847717: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.847751: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.847846: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.847946: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.848000: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.848121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.848244: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.848297: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.848464: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.848498: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.848590: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.848689: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.848741: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.848861: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.848984: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.849044: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Assign
2018-08-09 13:53:42.849213: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.849264: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.849366: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.849509: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:42.849552: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignAdd
2018-08-09 13:53:42.849650: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: AssignSub
2018-08-09 13:53:43.170870: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2450 operators, 3763 arrays (0 quantized)
2018-08-09 13:53:43.375158: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 2058 operators, 3203 arrays (0 quantized)
2018-08-09 13:53:43.563516: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2058 operators, 3203 arrays (0 quantized)
2018-08-09 13:53:43.603805: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting ""MobilenetV1/Logits/Dropout_1b/dropout/random_uniform/RandomUniform"" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set ""seed"" or ""seed2"" attr non-zero to fix this
2018-08-09 13:53:43.763273: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1386 operators, 2360 arrays (0 quantized)
2018-08-09 13:53:43.817254: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting ""MobilenetV1/Logits/Dropout_1b/dropout/random_uniform/RandomUniform"" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set ""seed"" or ""seed2"" attr non-zero to fix this
2018-08-09 13:53:43.887386: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 1386 operators, 2360 arrays (0 quantized)
2018-08-09 13:53:43.958709: F tensorflow/contrib/lite/toco/tooling_util.cc:1618] Array MobilenetV1/Logits/Dropout_1b/dropout/random_uniform/RandomUniform, which is an input to the Add operator producing the output array MobilenetV1/Logits/Dropout_1b/dropout/add, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
Abort trap: 6"
21622,"NotFoundError (see above for traceback): Key conv1/biases not found in checkpoint 	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:worker/replica:0/task:0/device:CPU:0""](_recv_save/Const_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21620,transform_graph-2.params,"There is nothing error when building bazel,but it is transform_graph-2.params rather than transform_graph in folder bazel-bin/tensorflow/tools/graph_transforms/.Why?"
21619,tf.data.Dataset data remain as tensor even after sess.run()?,"Hello Professional Tensorflow Developers,

I apologize if I am making a noob mistake over here. But my finding about the data that remain in the tf.data.Dataset remain as tensor even after sess.run(). Below are the source code:

```

import matplotlib.pyplot as plt
from scipy import misc
import numpy as np
from skimage import exposure
from math import ceil
import tensorflow as tf
import math
import glob
import collections
import re
from pathlib import Path
from PIL import Image
from skimage import data, img_as_float, io, img_as_uint
from skimage._shared._warnings import expected_warnings
from sklearn.preprocessing import LabelBinarizer
from sklearn.preprocessing import OneHotEncoder
print(""Tensorflow version "" + tf.__version__)

import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()

tf.set_random_seed(0)



int_list = [0,1,2,3,4,5,6,7,8,9]

lb = LabelBinarizer()



def parse_function(data_list):

	print(""data_list: "", data_list,""\n"")

	print(""tf.print: "", tf.Print(data_list, [data_list]),""\n"")

	graph1 = tf.Graph()
	
	with graph1.as_default():
		x = tf.placeholder(tf.int32)
		y = x * 1
		#z = tf.Variable(data_list)

	print(""x: "", x, ""\n"")
	#print(""z: "", z, ""\n"")	

	#sess = tf.InteractiveSession()

	#with sess.as_default():
	with tf.Session() as sess:
		sess.run(tf.global_variables_initializer())
		#sess.run(tf.local_variables_initializer())
		#tf.global_variables_initializer().run()
		#tf.local_variables_initializer().run()
		#tf.tables_initializer().run()
		
		#feed = {x: sess.run(tf.data_list)}
		#feed = {x: data_list}
		
		#data = sess.run(data_list)
		
		#print(""data: "", data,""\n"")
		#print(""feed: "", feed, ""\n"")

		print(""tf.print: "", tf.Print(data_list, [data_list]),""\n"")
	        
                print(""sess.run(data_list): "", sess.run(data_list), ""\n"")

		#label_eval = sess.run(y, feed_dict=feed)
		#label_eval = sess.run(z)
		#label_eval = sess.run(data_list)

		#print(""x: "", x, ""\n"")

		#label_eval = x.eval()
		#print(""label_eval: "", label_eval,""\n"")
		#one_hot_label = tf.one_hot(label, 10) #no issue
		#one_hot_label = lb.fit_transform(label_eval)
		#print(""one_hot_label: "", one_hot_label, ""\n"")

	sess.close()

	#return one_hot_label

int_list = tf.convert_to_tensor(int_list, dtype=tf.float32)

int_dataset = tf.data.Dataset.from_tensor_slices(int_list).map(lambda z: parse_function(z))

int_dataset = int_dataset.batch(100)

int_iterator = int_dataset.make_initializable_iterator()

int_iterator = train_iterator.get_next()
```

The error output:

```
Tensorflow version 1.8.0
2018-08-15 10:09:18.992919: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
data_list:  Tensor(""arg0:0"", shape=(), dtype=float32) 

tf.print:  Tensor(""Print:0"", shape=(), dtype=float32) 

x:  Tensor(""Placeholder:0"", dtype=int32) 

tf.print:  Tensor(""Print_1:0"", shape=(), dtype=float32) 

Traceback (most recent call last):
  File ""/home/dragon/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 282, in __init__
    fetch, allow_tensor=True, allow_operation=True))
  File ""/home/dragon/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3590, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/home/dragon/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3669, in _as_graph_element_locked
    raise ValueError(""Tensor %s is not an element of this graph."" % obj)
ValueError: Tensor Tensor(""arg0:0"", shape=(), dtype=float32) is not an element of this graph.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""./tf_dataset.py"", line 93, in <module>
    int_dataset = tf.data.Dataset.from_tensor_slices(int_list).map(lambda z: parse_function(z))
  File ""/home/dragon/.local/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 851, in map
    return MapDataset(self, map_func)
  File ""/home/dragon/.local/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1839, in __init__
    self._map_func.add_to_graph(ops.get_default_graph())
  File ""/home/dragon/.local/lib/python3.5/site-packages/tensorflow/python/framework/function.py"", line 484, in add_to_graph
    self._create_definition_if_needed()
  File ""/home/dragon/.local/lib/python3.5/site-packages/tensorflow/python/framework/function.py"", line 319, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""/home/dragon/.local/lib/python3.5/site-packages/tensorflow/python/framework/function.py"", line 336, in _create_definition_if_needed_impl
    outputs = self._func(*inputs)
  File ""/home/dragon/.local/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1804, in tf_map_func
    ret = map_func(nested_args)
  File ""./tf_dataset.py"", line 93, in <lambda>
    int_dataset = tf.data.Dataset.from_tensor_slices(int_list).map(lambda z: parse_function(z))
  File ""./tf_dataset.py"", line 72, in parse_function
    print(""sess.run(data_list): "", sess.run(data_list), ""\n"")
  File ""/home/dragon/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/dragon/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)
  File ""/home/dragon/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 427, in __init__
    self._fetch_mapper = _FetchMapper.for_fetch(fetches)
  File ""/home/dragon/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 253, in for_fetch
    return _ElementFetchMapper(fetches, contraction_fn)
  File ""/home/dragon/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 289, in __init__
    'Tensor. (%s)' % (fetch, str(e)))
ValueError: Fetch argument <tf.Tensor 'arg0:0' shape=() dtype=float32> cannot be interpreted as a Tensor. (Tensor Tensor(""arg0:0"", shape=(), dtype=float32) is not an element of this graph.)
```

pip list:
```
pip3 list
Package                       Version               
----------------------------- ----------------------
absl-py                       0.2.2                 
aiohttp                       2.2.5                 
aniso8601                     1.3.0                 
apturl                        0.5.2                 
asn1crypto                    0.23.0                
astor                         0.6.2                 
async-timeout                 2.0.0                 
attrs                         17.3.0                
Automat                       0.6.0                 
base58                        0.2.5                 
beautifulsoup4                4.6.0                 
behave                        1.2.5                 
BigchainDB                    1.2.0.dev0            
bigchaindb-driver             0.4.0                 
bleach                        1.5.0                 
blinker                       1.3                   
Brlapi                        0.6.4                 
cached-property               1.3.0                 
certifi                       2017.7.27.1           
cffi                          1.11.2                
chardet                       3.0.4                 
checkbox-support              0.22                  
click                         6.7                   
cloudpickle                   0.5.3                 
command-not-found             0.3                   
constantly                    15.1.0                
cryptoconditions              0.6.0.dev1            
cryptography                  2.1.3                 
cssselect                     1.0.1                 
cycler                        0.10.0                
dask                          0.18.1                
decorator                     4.1.2                 
defer                         1.0.6                 
docker                        2.5.1                 
docker-compose                1.16.1                
docker-pycreds                0.2.1                 
dockerpty                     0.4.1                 
docopt                        0.6.2                 
dpkt                          1.9.1                 
enum34                        1.1.6                 
feedparser                    5.1.3                 
Flask                         0.12.2                
Flask-Cors                    3.0.3                 
Flask-RESTful                 0.3.6                 
gast                          0.2.0                 
grpcio                        1.13.0                
guacamole                     0.9.2                 
gunicorn                      19.7.1                
h5py                          2.8.0                 
html5lib                      0.9999999             
httplib2                      0.9.1                 
hyperlink                     17.3.1                
hypothesis                    3.18.5                
hypothesis-regex              0.2                   
idna                          2.6                   
incremental                   17.5.0                
itsdangerous                  0.24                  
Jinja2                        2.9.6                 
jsonschema                    2.5.1                 
Keras                         2.2.0                 
Keras-Applications            1.0.2                 
Keras-Preprocessing           1.0.1                 
language-selector             0.1                   
logstats                      0.2.1                 
louis                         2.6.4                 
lxml                          4.1.1                 
Mako                          1.0.3                 
Markdown                      2.6.11                
MarkupSafe                    1.0                   
matplotlib                    2.0.2                 
multidict                     3.3.0                 
multipipes                    0.1.0                 
networkx                      1.11                  
nose                          1.3.7                 
numpy                         1.14.0                
oauthlib                      1.0.3                 
olefile                       0.44                  
onboard                       1.2.0                 
padme                         1.1.1                 
parse                         1.8.2                 
parse-type                    0.3.4                 
parsel                        1.2.0                 
pexpect                       4.0.1                 
Pillow                        5.2.0                 
pip                           10.0.1                
plainbox                      0.25                  
protobuf                      3.5.1                 
psutil                        5.3.0                 
ptyprocess                    0.5                   
py                            1.4.34                
pyasn1                        0.4.2                 
pyasn1-modules                0.2.1                 
pycparser                     2.18                  
pycups                        1.9.73                
pycurl                        7.43.0                
PyDispatcher                  2.0.5                 
Pygments                      2.2.0                 
pygobject                     3.20.0                
PyJWT                         1.3.0                 
pymongo                       3.5.1                 
PyNaCl                        1.1.2                 
pyOpenSSL                     17.4.0                
pyparsing                     2.2.0                 
pysha3                        1.0.2                 
pytest                        3.2.3                 
pytest-mock                   1.6.3                 
python-apt                    1.1.0b1+ubuntu0.16.4.2
python-dateutil               2.6.1                 
python-debian                 0.1.27                
python-rapidjson              0.0.11                
python-rapidjson-schema       0.1.1                 
python-systemd                231                   
pytz                          2017.2                
PyWavelets                    0.5.2                 
pyxdg                         0.25                  
PyYAML                        3.12                  
queuelib                      1.4.2                 
reportlab                     3.3.0                 
requests                      2.18.4                
rethinkdb                     2.3.0.post6           
scikit-image                  0.14.0                
scikit-learn                  0.19.0                
scipy                         0.19.1                
Scrapy                        1.4.0                 
selenium                      3.7.0                 
service-identity              17.0.0                
sessioninstaller              0.0.0                 
setuptools                    38.4.0                
six                           1.11.0                
statsd                        3.2.1                 
system-service                0.3                   
tensorboard                   1.8.0                 
tensorflow                    1.8.0                 
tensorflow-tensorboard        0.4.0                 
termcolor                     1.1.0                 
texttable                     0.9.1                 
toolz                         0.9.0                 
tornado                       4.5.2                 
Twisted                       17.9.0                
ubuntu-drivers-common         0.0.0                 
ufw                           0.35                  
unattended-upgrades           0.1                   
unity-scope-calculator        0.1                   
unity-scope-chromiumbookmarks 0.1                   
unity-scope-colourlovers      0.1                   
unity-scope-devhelp           0.1                   
unity-scope-firefoxbookmarks  0.1                   
unity-scope-gdrive            0.7                   
unity-scope-manpages          0.1                   
unity-scope-openclipart       0.1                   
unity-scope-texdoc            0.1                   
unity-scope-tomboy            0.1                   
unity-scope-virtualbox        0.1                   
unity-scope-yelp              0.1                   
unity-scope-zotero            0.1                   
urllib3                       1.22                  
usb-creator                   0.3.0                 
w3lib                         1.18.0                
websocket-client              0.44.0                
Werkzeug                      0.14.1                
wheel                         0.30.0                
xdiagnose                     3.8.4.1               
xkit                          0.0.0                 
XlsxWriter                    0.7.3                 
yarl                          0.13.0                
zope.interface                4.4.3 
```          

I am using Ubuntu 16.04 64 Bits, Python 3.5.2 and tensorflow 1.8.0

Additional information:

Have I written custom code

No

OS Platform and Distribution

Linux OS Ubuntu 16.04 x64 

TensorFlow installed from

Tensorflow 1.8.0 CPU only version install from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.8.0rc1-cp35-cp35m-linux_x86_64.whl

Bazel version

n/a

CUDA/cuDNN version

n/a

GPU model and memory

n/a due to I am using CPU only version

Exact command to reproduce

Please execute my source code which mentioned above

Mobile device

n/a due to I am using PC not mobile device

Thanks
"
21618,AttributeError: module 'tensorflow.contrib.lite.python.convert_saved_model' has no attribute 'convert',"### System information
- **OS Platform and Distribution: Linux Ubuntu 16.04**:
- **TensorFlow installed from source**:
- **TensorFlow version 1.10**:
- **Python version 3.6**:
- **Bazel version Build label: 0.16.0**:
- **Exact command to reproduce: convert_saved_model.convert(saved_model_dir=saved_model, output_tflite=""/TF_Lite_Model"")**:

I am trying to convert my premade DNN Model to tflite file, using the function:

from tensorflow.contrib.lite.python import convert_saved_model
**convert_saved_model.convert**(saved_model_dir=saved_model, output_tflite=""/TF_Lite_Model"")

I have the last verison of tensorflow installed 1.10
I am using UBUNTU 16.04

the error is the following:

AttributeError: module 'tensorflow.contrib.lite.python.convert_saved_model' has no attribute 'convert'


"
21617,Error trying to convert from saved model to tflite,"### System information
- **OS Platform and Distribution: Linux Ubuntu 16.04**:
- **TensorFlow installed from source**:
- **TensorFlow version 1.10**:
- **Python version 3.6**:
- **Bazel version Build label: 0.16.0**:

While trying to convert a saved model to tflite file I get the following error:

F tensorflow/contrib/lite/toco/tflite/export.cc:363] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.toco_convert(). Here is a list of operators for which you will need custom implementations: AsString, ParseExample.\nAborted (core dumped)\n' None

I am using the DNN premade Estimator.

```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import tensorflow as tf
IRIS_TRAINING = ""iris_training.csv""
IRIS_TEST = ""iris_test.csv""
INPUT_TENSOR_NAME = 'inputs'

def main():
training_set = tf.contrib.learn.datasets.base.load_csv_with_header(
    filename=IRIS_TRAINING,
    target_dtype=np.int,
    features_dtype=np.float32)

feature_columns = [tf.feature_column.numeric_column(INPUT_TENSOR_NAME, shape=[4])]

# Build 3 layer DNN with 10, 20, 10 units respectively.
classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,
                           hidden_units=[10, 20, 10],
                           n_classes=3,
                           model_dir=""/tmp/iris_model"")


# Define the training inputs
train_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={INPUT_TENSOR_NAME: np.array(training_set.data)},
    y=np.array(training_set.target),
    num_epochs=None,
    shuffle=True)

# Train model.
classifier.train(input_fn=train_input_fn, steps=2000)

inputs = {'x': tf.placeholder(tf.float32, [4])}
tf.estimator.export.ServingInputReceiver(inputs, inputs)

saved_model=classifier.export_savedmodel(export_dir_base=""/tmp/iris_model"", serving_input_receiver_fn=serving_input_receiver_fn)

converter = tf.contrib.lite.TocoConverter.from_saved_model(saved_model)
tflite_model = converter.convert()

def serving_input_receiver_fn():
    feature_spec = {INPUT_TENSOR_NAME: tf.FixedLenFeature(dtype=tf.float32, shape=[4])}
    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)()

if __name__ == ""__main__"":
    main()
```

Iris files can be downloaded form the following links:

IRIS_TRAINING FILE: ""http://download.tensorflow.org/data/iris_training.csv""

IRIS_TEST FILE: ""http://download.tensorflow.org/data/iris_test.csv"""
21615,Fine tuning a model by building a Scaffold in Mirrored Strategy is not supported,"### System information
Have I written custom code: Yes
OS Platform and Distribution: Linux Ubuntu 18.04
Mobile device: N/A
TensorFlow installed from: source
TensorFlow version: ('v1.9.0-rc2-2165-g7b80190d52', '1.10.0-rc1')
Python version: 2.7
Bazel version: 0.15.2
GCC/Compiler version: gcc version 6.4.0 20180424
CUDA/cuDNN version: 9.1 / 7.1
GPU model and memory: K80,  12GB
Exact command to reproduce: N/A

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I have been trying to  fine tune my model using Mirrored Strategy. To initialize the weights from a given checkpoint, I pass a scaffold object to Estimator_Spec object, with init_fn as a parameter. However, when I run it, the program crashes (refer to the stacktrace below for more details). I looked around for similar issue, but couldn't find any. Hence, I had to dig through the source code to figure the cause of the issue. While exploring, I ran into this message:
**TODO(anjalisridhar): Figure out how to resolve the following scaffold parameters: init_feed_dict, init_fn**
called here:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L1599

I was wondering whether fixing this is on priority list, or whether it has been sitting on the back burner until other issues have been resolved.

i am not sure how to present a reproducible code in this context, but do let me know if I need to clarify some more.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


INFO:tensorflow:Using config: {'_save_checkpoints_secs': 120, '_session_config': allow_soft_placement: true
, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7fd221bfad10>, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd221bfacd0>, '_model_dir': '../data/model2', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}
2018-08-11 08:50:44.010343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:04:00.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2018-08-11 08:50:44.164489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 1 with properties:
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:05:00.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2018-08-11 08:50:44.164798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1
2018-08-11 08:50:44.719187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-11 08:50:44.719236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1
2018-08-11 08:50:44.719243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y
2018-08-11 08:50:44.719247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N
2018-08-11 08:50:44.719771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/device:GPU:0 with 10757 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:04:00.0, compute capability: 3.7)
2018-08-11 08:50:44.885422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/device:GPU:1 with 10757 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0
INFO:tensorflow:Configured nccl all-reduce.
2018-08-11 08:50:45.166638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1
2018-08-11 08:50:45.166758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-11 08:50:45.166769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1
2018-08-11 08:50:45.166777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y
2018-08-11 08:50:45.166793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N
2018-08-11 08:50:45.167078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10757 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:04:00.0, compute capability: 3.7)
2018-08-11 08:50:45.167251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10757 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:batch_all_reduce invoked for batches size = 34 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Done calling model_fn.
Traceback (most recent call last):
  File ""train.py"", line 167, in <module>
    tf.app.run()
  File ""/home/weinman/virtualenv/tf-master-7b80190d52/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""train.py"", line 164, in main
    classifier.train( input_fn=_get_input, max_steps=FLAGS.max_num_steps )
  File ""/home/weinman/virtualenv/tf-master-7b80190d52/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 343, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/weinman/virtualenv/tf-master-7b80190d52/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1127, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/home/weinman/virtualenv/tf-master-7b80190d52/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1256, in _train_model_distributed
    grouped_estimator_spec.scaffold, self._train_distribution)
  File ""/home/weinman/virtualenv/tf-master-7b80190d52/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1576, in _combine_distributed_scaffold
    init_fn = distribution.group(init_fn)
  File ""/home/weinman/virtualenv/tf-master-7b80190d52/local/lib/python2.7/site-packages/tensorflow/python/training/distribute.py"", line 1007, in group
    return control_flow_ops.group(value, name=name)
  File ""/home/weinman/virtualenv/tf-master-7b80190d52/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3396, in group
    ""'%s' with type '%s'"" % (inp, type(inp)))
**TypeError: Expected tf.group() expected Tensor arguments not '<function <lambda> at 0x7fcfe46a8c08>' with type '<type 'function'>'**"
21614,Keras to estimator errors for RNNs,"It seems like a bug that the following model results in the following assert:

The model (simplified from seq2seq):
```python

import tensorflow as tf

params = {
    'seq_len': 32,
    'vocab_size': 50,
    'emb_act_size': 32,
    'rnn_act_size': 64,
    'out_size': 16
}


def mfunc(params):
    inputs = tf.keras.layers.Input(shape=(50, ), dtype='int32')
    emb = tf.keras.layers.Embedding(params[""vocab_size""],
                                    params[""emb_act_size""])(inputs)
    lstm1, s_h, s_c = tf.keras.layers.LSTM(
        params[""rnn_act_size""],
        return_sequences=True,
        return_state=True,
        implementation=2)(emb)
    lstm2 = tf.keras.layers.LSTM(
        params[""rnn_act_size""], return_sequences=True)(
            lstm1, initial_state=[s_h, s_c])
    y = tf.keras.layers.Dense(params[""out_size""], activation='softmax')(lstm2)
    model = tf.keras.Model(inputs, y)
    model.compile(optimizer='sgd', loss='categorical_crossentropy')
    return model

x = mfunc(params)
tf.keras.estimator.model_to_estimator(keras_model=x)
```
The assert we are hitting in TF 1.10:
```python
Traceback (most recent call last):
  File ""Downloads/tmp.py"", line 30, in <module>
    tf.keras.estimator.model_to_estimator(keras_model=x)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/keras.py"", line 548, in model_to_estimator
    keras_weights)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/keras.py"", line 449, in _save_first_checkpoint
    custom_objects)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/keras.py"", line 315, in _clone_and_build_model
    model = models.clone_model(keras_model, input_tensors=input_tensors)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/models.py"", line 263, in clone_model
    return _clone_functional_model(model, input_tensors=input_tensors)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/models.py"", line 168, in _clone_functional_model
    **kwargs))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/recurrent.py"", line 526, in __call__
    self._num_constants)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/recurrent.py"", line 2590, in _standardize_args
    assert initial_state is None and constants is None
AssertionError
```

Here is the result of the environment capture script:
https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
== cat /etc/issue ===============================================
Linux 97ddc7134519 4.9.93-linuxkit-aufs #1 SMP Wed Jun 6 16:55:56 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 97ddc7134519 4.9.93-linuxkit-aufs #1 SMP Wed Jun 6 16:55:56 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy              1.14.5
protobuf           3.6.0
tensorflow         1.10.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.10.0
tf.GIT_VERSION = v1.10.0-0-g656e7a2b34
tf.COMPILER_VERSION = v1.10.0-0-g656e7a2b34
Sanity check: array([1], dtype=int32)"
21613,Implement a Transformer class compatible with seq2seq library,"The current implementation from tensor2tensor is not compatible to the seq2seq library. Another possible implementation in openNMT also write the helper function like `dynamic_decode` itself. This is an attempt to try to add Transformer to have the same interface as `RNNCell` so that it can be more easily used.

Have I written custom code: N/A
OS Platform and Distribution: N/A
TensorFlow installed from: N/A
TensorFlow version: N/A
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A
Mobile device: N/A"
21612,conv2d support for bfloat16,"As in the reference for r1.8.0 of tensorflow, the tf.nn.conv2d support bfloat16, but actually when run conv2d with bfloat16 format, got error:
No OpKernel was registered to support Op 'Conv2D' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]

So is reference wrong, or it is something which would be added later?
"
21609,Tensorflow profiling data for every layers of CNN,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
Not applicable
- **TensorFlow installed from (source or binary)**:
Binary. pip install in virtualenv
- **TensorFlow version (use command below)**:
- **Python version**:
1.9.0
- **Bazel version (if compiling from source)**:
Not applicable
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
Problem is related to profiling of Tensorflow model for specific network arhitecture, for instance, consider vgg16. We want to get all operator level profiling data on a layer (Layers in CNN) by layer basis. I have used tf.profiler.Profiler class and was able to generate profiling data. But those data are all aggregration of all operations. In tensorflow lite there is a mechanism using which we can generate layer by layer, operator level profiling data.
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/tools/benchmark/README.md
============================== Run Order ==============================
	             [node type]	  [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	    0.000	    4.269	    4.269	  0.107%	  0.107%	     0.000	        0	[MobilenetV1/MobilenetV1/Conv2d_0/Relu6]
	       DEPTHWISE_CONV_2D	    4.270	    2.150	    2.150	  0.054%	  0.161%	     0.000	        0	[MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6]
	                 CONV_2D	    6.421	    6.107	    6.107	  0.153%	  0.314%	     0.000	        0	[MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Relu6]
Is there anyway I can generate that kind of data in tensorflow?


### Source code / logs
Tensorflow benchmarking script is being used to generate profiling data.
https://github.com/tensorflow/benchmarks.git"
21605,Mirroed strategy,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21604,how to automatically restart worker/master after occasional crash,"HI! I'm running distributed training of tensorflow, I have a very simple question, does TF provide some feature for monitor and automatically restart crashed worker/master all all training process, as currently I can only manually restart failed worker."
21603,Default device_filters are only set if session_config is None,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

The changelog for 1.10 mentions that starting from this version `RunConfig` comes with a default value for `device_filters`. However, the default [is only applied](https://github.com/tensorflow/tensorflow/blob/ffc9be66bc1503ec12f10f43ff622d9193062e97/tensorflow/python/estimator/run_config.py#L525-L530) if the user did not provide any value for `session_config`, i.e. if the following code does not get the default:

```
config = tf.estimator.RunConfig(session_config=tf.ConfigProto())
```

I think it might be more useful to patch the `session_config` instead of replacing it fully.

Side question: why are the following two lines needed? Why isn't it enough to just set `device_filters`?

https://github.com/tensorflow/tensorflow/blob/ffc9be66bc1503ec12f10f43ff622d9193062e97/tensorflow/python/estimator/run_config.py#L538-L540"
21602,Android build failed using custom tensorflow model. Conflict workspace Error,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
   Yes, I have modified no. of classes to be detect while training the model
- **OS Platform and Distribution: 14.04 LTS
- **Mobile device if the issue happens on mobile device**: Motoroloa, Android version 7.1.1
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: 7.2.0
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: 4GB RAM, 64-bit

### Describe the problem
I had trained my custom model with different objects for the detection purpose. I had used ssd_mobilenet_v2_coco for my training. I could pull out the object detection on desktop with the customised trained model by using the webcam or passing an image. But I am unable to port it on android app. 
It is interesting that I am able to convert the file in .tflite format (the one which will be used by bazel) and for android. However, after the app installation it is getting crashed. The app is forcibly closed. It may be noted here that the classifier and speech app is working fine and there is no force shut down, but the problem remained the same with detect app. I then followed this [link](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) for change in the DetectorActivity.java inside  tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/DetectorActivity.java as below `private static final int TF_OD_API_INPUT_SIZE = 300;
  private static final boolean TF_OD_API_IS_QUANTIZED = true;
  private static final String TF_OD_API_MODEL_FILE = ""detect.tflite"";
  //private static final String TF_OD_API_LABELS_FILE = ""file:///android_asset/coco_labels_list.txt"";
  private static final String TF_OD_API_LABELS_FILE = ""file:///android_asset/labelmap.txt"";`

where detect.tflite and labelmap.txt both files are from the customized model.

Then I ran the command 
`bazel build -c opt --cxxopt='--std=c++11' ""//tensorflow/contrib/lite/examples/android:tflite_demo""` and this successfully build the apk file and other dependencies. Meanwhile, it showed a conflict error. Errors read as below:
`CONFLICT: asset:WORKSPACE is provided with ambiguous priority from:
	external/tflite_mobilenet/WORKSPACE
	external/tflite_conv_actions_frozen/WORKSPACE

CONFLICT: asset:WORKSPACE is provided with ambiguous priority from:
	external/tflite_mobilenet_ssd/WORKSPACE
	external/tflite_conv_actions_frozen/WORKSPACE`

When I tried to build the apk over phone, the app is installed but it is getting closed forcefully. However,  the classifier and speech apps are working perfectly fine.


NOTE: The pre trained models are working absolutely fine on both desktop and as an app. However, the issue is with importing the custom app on phone.

"
21601,Enqueuing tensors by value with tf.add no longer works on 1.10,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes (see snippet below)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2 1.10.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.16.0
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.2
- **GPU model and memory**:
- **Exact command to reproduce**: See below.


### Describe the problem
There seems to be a possible regression affecting the use of tf.add to enqueue tensors by value instead of by reference within a same device. Check the following snippet.

```python
import tensorflow as tf

with tf.Session() as sess:
  v = tf.get_variable('v', shape=(), dtype=tf.int32)
  sess.run(v.assign(123))
  q = tf.FIFOQueue(10, tf.int32)
  sess.run(q.enqueue(tf.add(v, 0)))
  sess.run(v.assign(456))
  print(sess.run(q.dequeue()))
```

In previous versions of TensorFlow (last checked was probably 1.8) the above snippet returned 123 since the value of the variable was copied when enqueued thanks to the use of tf.add. However, on TensorFlow 1.10 the snippet returns 456 if using a tf.Session block and 123 if using a tf.InteractiveSession. Enabling or disabling GPU with CUDA_VISIBLE_DEVICES does not seem to affect.

Is this an intentional change of behavior? If so, is there any new or better way to enqueue tensors by value instead of by reference?"
21600,CUDA unified memory fail to increase on demand in r1.10 on PPC64le,"### Description

Based on the commit to [introduce an option to allocate CUDA unified memory](https://github.com/tensorflow/tensorflow/commit/b1139814f91c5216eb5ff229ee7e1982e5f4e888?diff=split), theoretically we could expect a memory swap between CPU and GPU while running datasets with high memory demands. 

However, my tensorflow build still hits OOM. 
I was training [official resnet with cifar10 dataset](https://github.com/tensorflow/models/tree/master/official/resnet), with also a large batch ---- 48*128. 

I built another version without CUDA unified memory from commit [fixed ppc64le compile failure libpng](https://github.com/tensorflow/tensorflow/commit/5aefa441276b5fdf2fec5e7cb282630c104f6f4a), 
Both tensorflow build hits OOM when I try to increase batch size from 47\*128 to 48\*128. 
Based on this comparision, I was convinced that CUDA unified memory is not functioning in my build. 

However, I tried to set `per_process_gpu_memory_fraction` to 200 and pass the config to the `tf.Session`, it worked for much larger batches. 
Then I realized that the UVM cannot grow on demand. Setting `allow_growth` to True won't help. 

In a word, can we make the CUDA unified memory allocation grow on demand?

Could you possibly suggest a reason?
Did I miss out any configurations?

Thank you in advance!
----------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:  source
- **TensorFlow version (use command below)**:  r1.10
- **Python version**:  3.5.2
- **Bazel version (if compiling from source)**:  0.15.0
- **GCC/Compiler version (if compiling from source)**:  gcc (Ubuntu/IBM 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**:  9.2/7.1
- **GPU model and memory**:   4 x Tesla P100, each has 16G memory; 1T RAM"
21599,how to remove nodes from restored graph?,"hi, all:
   I restored a graph from a meta file.
```python
    saver = tf.train.import_meta_graph(LOCAL_TRAINED_MODEL_DIR+'/saved_model-0.meta')
    saver.restore(sess, tf.train.latest_checkpoint(LOCAL_TRAINED_MODEL_DIR))
    graph = tf.get_default_graph()
```


how could I delete some nodes from the default graph? The nodes can be accessed by get_tensor_by_name(). and could I reuse the trained weights please? 
    Thanks!
   "
21598,Dilation of keras.Conv2DTranspose does not do anything,"### Describe the problem
Well using tf.keras.Conv2DTranspose I noticed the dilation function did not dilate. No matter what size of dilation you choose the result is the same.

I did found the problem in keras as well but I did not now if I should report it here too: https://github.com/keras-team/keras/issues/8159


### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: v8.0
- **GPU model and memory**: GeForce 1050
- **Exact command to reproduce**: See below

### Source code / logs
```
import tensorflow as tf
import numpy as np

Conv2D = tf.keras.layers.Conv2D
Conv2DTranspose = tf.keras.layers.Conv2DTranspose
Input = tf.keras.layers.Input
Model = tf.keras.Model

input = Input(shape=(None, None, 1), name='input')
x = Conv2DTranspose(8, (2, 2), dilation_rate=2, padding='valid')(input)
x = Conv2DTranspose(8, (3, 3), dilation_rate=2, padding='valid')(x)
x = Conv2DTranspose(8, (3, 3), dilation_rate=2, padding='valid')(x)

model_dilation = Model(inputs=input, outputs=x, name='dil')

x = Conv2DTranspose(8, (2, 2), dilation_rate=1, padding='valid')(input)
x = Conv2DTranspose(8, (3, 3), dilation_rate=1, padding='valid')(x)
x = Conv2DTranspose(8, (3, 3), dilation_rate=1, padding='valid')(x)

model_no_dilation = Model(inputs=input, outputs=x, name='no_dil')

output_dil = model_dilation.predict(np.zeros((1, 5, 5, 1)))
output_no_dil = model_no_dilation.predict(np.zeros((1, 5, 5, 1)))

print(""shape dil"", np.shape(output_dil))
print(""shape no dil"", np.shape(output_no_dil))
```

result:
```
shape dil (1, 10, 10, 8)
shape no dil (1, 10, 10, 8)
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

"
21597,"In Tensorflow 1.10 Keras models with datasets not working, gives  TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float64 of argument 'x'","Please go to Stack Overflow for help and support:
https://stackoverflow.com/questions/tagged/tensorflow
If you open a GitHub issue, here is our policy:
1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Google Colab 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue hap
pens on mobile device**: Web
- **TensorFlow installed from (source or binary)**:Binary
- **TensorFlow version (use command below)**:1.10
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: 
https://www.tensorflow.org/guide/keras
import tensorflow as tf
from tensorflow import keras
import numpy as np
import os
data =np.random.random((1000,32))
labels =np.random.random((1000,10))
dataset1 = tf.data.Dataset.from_tensor_slices((data, labels))
dataset1 = dataset1.batch(32)
dataset1 = dataset1.repeat()
model = keras.Sequential()  
model.add(keras.layers.Dense(64,activation='relu'))
model.add(keras.layers.Dense(64,activation='relu'))
model.add(keras.layers.Dense(10,activation='softmax'))

model.compile(optimizer=tf.train.AdamOptimizer(0.001),
          loss='categorical_crossentropy',
          metrics=['accuracy'])
model.fit(dataset1 ,epochs=10,steps_per_epoch=30)

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float64 of argument 'x'.
If I add type 00,32))
data.astype(np.float32) does not help, looks like dataset is broken in 1.10
l
/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in _apply_op_helper(self, op_type_name, name, **keywords)
    544                   ""%s type %s of argument '%s'."" %
    545                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,
--> 546                    inferred_from[input_arg.type_attr]))
    547 
    548           types = [values.dtype]

TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float64 of argument 'x'.
### Source code / logs

[Stackoverflow question](https://stackoverflow.com/questions/51814224/tensorflow-keras-sample-code-throws-typeerror-input-y-of-mul-op-has-type)

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21596,TFLite：How to generate partial quantized model,"I only found the way to generate fully quantized model.
https://www.tensorflow.org/performance/quantization

But for high accuracy, maybe we should retrain the network by adding fake quantization for the front part and keeping the last part unchanged. Finally we'd like to generate such partial quantized TFLITE model for mobile application.
Is there any tool to implement except merging two separately generated models manually ?

Thanks"
21594,memory_layer bug,"In Tensorflow 1.10, the following lines conflict, since `None` has no attribute `dtype`.

https://github.com/tensorflow/tensorflow/blob/5326a7fb028d6a66286e9e929dc9454c12e81522/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L176

https://github.com/tensorflow/tensorflow/blob/5326a7fb028d6a66286e9e929dc9454c12e81522/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L198

https://github.com/tensorflow/tensorflow/blob/5326a7fb028d6a66286e9e929dc9454c12e81522/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L204"
21592,TF lite can build & run on Linux?,"As the title says, is tf lite can be built and ran on non-mobile environment, such as Linux or others:

"
21591,How to solve No OpKernel was registered to support Op 'CTCLoss',"Now I put the trained model on the android device, suggesting that there is no ""No OpKernel was registered to support Op 'CTCLoss' with these attrs."" How can I get him support Op 'CTCLoss'
My PC is Ubuntu 16.04 and AndroidStudio is 3.0.
Tensotflow is 1.6

-------------------------------------------------- ----------------------------
The error message is as follows:

08-14 10:43:30.332 11166-11166/com.example.hc.digitalgesturerecognition E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[the_input], outputs:[output_node0]
08-14 10:43:30.336 11166-11166/com.example.hc.digitalgesturerecognition E/AndroidRuntime: FATAL EXCEPTION: main
                                                                                          Process: com.example.hc.digitalgesturerecognition, PID: 11166
                                                                                          java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'CTCLoss' with these attrs. Registered devices: [CPU], Registered kernels:
                                                                                            <no registered kernels>
                                                                                          
                                                                                          [[Node: ctc/CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=false](ctc/Log, ctc/ToInt64, ctc/ToInt32_2, ctc/ToInt32_1)]]
                                                                                              At org.tensorflow.Session.run(Native Method)
                                                                                              At org.tensorflow.Session.access$100(Session.java:48)
                                                                                              At org.tensorflow.Session$Runner.runHelper(Session.java:298)
                                                                                              At org.tensorflow.Session$Runner.run(Session.java:248)
                                                                                              At org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)
                                                                                              At org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)
                                                                                              At org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:187)
                                                                                              At com.example.hc.digitalgesturerecognition.Classifier.predict(Classifier.java:115)
                                                                                              At com.example.hc.digitalgesturerecognition.CameraActivity$2.onImageAvailable(CameraActivity.java:285)
                                                                                              At android.media.ImageReader$ListenerHandler.handleMessage(ImageReader.java:812)
                                                                                              At android.os.Handler.dispatchMessage(Handler.java:108)
                                                                                              At android.os.Looper.loop(Looper.java:166)
                                                                                              At android.app.ActivityThread.main(ActivityThread.java:7425)
                                                                                              At java.lang.reflect.Method.invoke(Native Method)
                                                                                              At com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:245)
                                                                                              At com.android.internal.os.ZygoteInit.main(ZygoteInit.java:921)"
21590,All metrics evaluate to 0.0 when using  tf.contrib.estimator.InMemoryEvaluatorHook,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Slight modification to cnn_mnist.py to include `InMemoryEvaluatorHook` . Original code can be found here [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A
- **TensorFlow installed from (source or binary)**: binary (i think)
- **TensorFlow version (use command below)**:1.9.0 (with PR#20822)
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:7.5
- **GPU model and memory**: 2x TITAN X (12189MiB) & Quadro P5000 (16273MiB)
- **Exact command to reproduce**:  python3 cnn_mnist.py

### Describe the problem
When evaluating during training with `tf.contrib.estimator.InMemoryEvaluatorHook`, all evaluation metrics are evaluated to 0.0 after the very first time the evaluation hook is triggered. My suspicion is that it is caused by the use of `tf.estimator.inputs.numpy_input_fn`.

### Source code:
Attached is the script I used. Basically, just added the following line:
` evaluator = tf.contrib.estimator.InMemoryEvaluatorHook(estimator=mnist_classifier, input_fn=eval_input_fn, every_n_iter=50)` and passed the hook to `mnist_classifer.train()`

[cnn_mnist.txt](https://github.com/tensorflow/tensorflow/files/2285277/cnn_mnist.txt)


Here is the relevant output from running the training for 200 steps:

INFO:tensorflow:Starting evaluation at 2018-08-14-02:15:35
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-08-14-02:15:37
INFO:tensorflow:Saving dict for global step 0: accuracy = 0.0915, global_step = 0, loss = 2.30028
INFO:tensorflow:loss = 2.29529, step = 0
...
INFO:tensorflow:Starting evaluation at 2018-08-14-02:15:37
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-08-14-02:15:37
INFO:tensorflow:Saving dict for global step 50: accuracy = 0.0, global_step = 50, loss = 0.0
...
INFO:tensorflow:Starting evaluation at 2018-08-14-02:15:38
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-08-14-02:15:38
INFO:tensorflow:Saving dict for global step 100: accuracy = 0.0, global_step = 100, loss = 0.0
INFO:tensorflow:global_step/sec: 103.013
INFO:tensorflow:loss = 2.27342, step = 100 (0.971 sec)
...
INFO:tensorflow:Starting evaluation at 2018-08-14-02:15:38
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-08-14-02:15:38
INFO:tensorflow:Saving dict for global step 150: accuracy = 0.0, global_step = 150, loss = 0.0
...
INFO:tensorflow:Starting evaluation at 2018-08-14-02:15:39
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-08-14-02:15:39
INFO:tensorflow:Saving dict for global step 200: accuracy = 0.0, global_step = 200, loss = 0.0
...
INFO:tensorflow:Saving checkpoints for 200 into ./tmp/mnist_convnet_model/model.ckpt.
INFO:tensorflow:Starting evaluation at 2018-08-14-02:15:39
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-08-14-02:15:39
INFO:tensorflow:Saving dict for global step 200: accuracy = 0.0, global_step = 200, loss = 0.0
INFO:tensorflow:Loss for final step: 2.25765.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
...
INFO:tensorflow:Starting evaluation at 2018-08-14-02:15:40
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from ./tmp/mnist_convnet_model/model.ckpt-200
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-08-14-02:15:40
INFO:tensorflow:Saving dict for global step 200: accuracy = 0.2611, global_step = 200, loss = 2.25732
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: ./tmp/mnist_convnet_model/model.ckpt-200
{'global_step': 200, 'loss': 2.2573187, 'accuracy': 0.26109999}
"
21588, ./tensorflow/contrib/lite/build_rpi_lib.sh /bin/sh: 1: [[: not found make: arm-linux-gnueabihf-: Command not found,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04 64bit
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:  NO devices
- **TensorFlow installed from (source or binary)**:  pip install tensorflow-gpu==1.9
- **TensorFlow version (use command below)**:  1.9 gpu
- **Python version**:   python2.7
- **Bazel version (if compiling from source)**: NO Bazel
- **GCC/Compiler version (if compiling from source)**:   gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**:  cuda9.0 cudnn7.0
- **GPU model and memory**: gtx1070
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

### Describe the problem
get help from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/rpi.md

my work Dir is :  /home/icare/yqli/tensorflow

I can successfully run the  :     
./tensorflow/contrib/lite/download_dependencies.sh
chmod 777 ./tensorflow/contrib/lite/build_rpi_lib.sh

but when I run the  :   
./tensorflow/contrib/lite/build_rpi_lib.sh

erros occurs ：
icare@icare-5F:tensorflow$ ./tensorflow/contrib/lite/build_rpi_lib.sh
+ set -e
+++ dirname ./tensorflow/contrib/lite/build_rpi_lib.sh
++ cd ./tensorflow/contrib/lite
++ pwd
+ SCRIPT_DIR=/home/icare/yqli/tensorflow/tensorflow/contrib/lite
+ cd /home/icare/yqli/tensorflow/tensorflow/contrib/lite/../../..
+ CC_PREFIX=arm-linux-gnueabihf-
+ make -j 3 -f tensorflow/contrib/lite/Makefile TARGET=RPI TARGET_ARCH=armv7
/bin/sh: 1: [[: not found
/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/bin/rpi_armv7/benchmark_model
arm-linux-gnueabihf- g++ --std=c++11 -O3 -DNDEBUG -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/../../../ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/allocation.cc -o /home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/allocation.o
arm-linux-gnueabihf- g++ --std=c++11 -O3 -DNDEBUG -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/../../../ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/arena_planner.cc -o /home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/arena_planner.o
make: arm-linux-gnueabihf-: Command not found
arm-linux-gnueabihf- gcc --std=c++11 -O3 -DNDEBUG -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -I. -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/../../../ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/ -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/eigen -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/neon_2_sse -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/farmhash/src -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/downloads/flatbuffers/include -I/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/context.c -o /home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/context.o
make: arm-linux-gnueabihf-: Command not found
tensorflow/contrib/lite/Makefile:203: recipe for target '/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/allocation.o' failed
make: *** [/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/allocation.o] Error 127
make: *** Waiting for unfinished jobs....
tensorflow/contrib/lite/Makefile:203: recipe for target '/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/arena_planner.o' failed
make: *** [/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/arena_planner.o] Error 127
make: arm-linux-gnueabihf-: Command not found
tensorflow/contrib/lite/Makefile:207: recipe for target '/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/context.o' failed
make: *** [/home/icare/yqli/tensorflow/tensorflow/contrib/lite/gen/obj/rpi_armv7/tensorflow/contrib/lite/context.o] Error 127

It seems that the “arm-linux-gnueabihf- g++” can not be found,

icare@icare-5F:tensorflow$ arm-linux-gnueabihf- g++
arm-linux-gnueabihf-: command not found

But it can find ""arm-linux-gnueabihf-g++""
icare@icare-5F:tensorflow$ arm-linux-gnueabihf-g++
arm-linux-gnueabihf-g++: fatal error: no input files
compilation terminated.

why  there is a space between the ""arm-linux-gnueabihf-"" and ""g++"",
how to solve the problem?

I have never modified the build_rpi_lib.sh and teh Makefile   
                                                                                                         

thanks for your time"
21587,tf.keras: regularizer does not add to total losses,"I try to add l2 regularizer for tf.keras.layers.CuDNNLSTM,  here is my code for creating layers:


```
   regulizer = tf.keras.regularizers.l2(l=1) 
    net = tf.keras.layers.CuDNNLSTM(lstm_units,
                                    kernel_initializer=tf.keras.initializers.glorot_normal(),
                                    unit_forget_bias=True,
                                    recurrent_initializer=tf.keras.initializers.glorot_normal(),
                                    kernel_regularizer=regulizer,
                                    return_sequences=True)(input_data)
```
after I create my graph, I gather all the losses with 
`tf.get_collection(tf.GraphKeys.LOSSES)`

but 
**1. only  mse error is in the return list.
2. the initial losses is same as without regularizer even if I set regularizer co-efficient very big.**

Does someone know how to apply regularizer to keras layer in tensorflow?"
21585,Freezing a graph from a .pb file,"Hello!
This is not a bug or a feature request but I cannot find answers on stackoverflow and have tried multiple ways of fixing this. This is my stack-overflow question: https://stackoverflow.com/questions/51826706/tensorflow-load-a-pb-file-and-then-save-it-as-a-frozen-graph-issues
I went to the discussion forums and after finding some bugs in that code got redirected here. 
This is the code that I have tried:
```
import tensorflow as tf
import sys
from tensorflow.python.platform import gfile

from tensorflow.core.protobuf import saved_model_pb2
from tensorflow.python.util import compat
constant_values = {}

with tf.Session() as sess:
    model_filename ='saved_model.pb'
    with gfile.FastGFile(model_filename, 'rb') as f:

      	data = compat.as_bytes(f.read())# reads binary
      	sm = saved_model_pb2.SavedModel()
        print(sm)
      	sm.ParseFromString(data)# parses through file. 
      	print(sm)
      	if 1 != len(sm.meta_graphs):
      		print('More than one graph found. Not sure which to write')
      		sys.exit(1)
            
        # Attempt at initializing the variables here 
        g_in = tf.import_graph_def(sm.meta_graphs[0].graph_def)
        constant_ops = [op for op in sess.graph.get_operations() if op.type == ""Const""]
        for constant_op in constant_ops:
            constant_values[constant_op.name] = sess.run(constant_op.outputs[0])
            
        # Creating frozen graph.
        output_graph = ""frozen_grapha.pb""
        output_nodes = [n.name for n in tf.get_default_graph().as_graph_def().node]
        output_graph_def = tf.graph_util.convert_variables_to_constants(
           sess, # The session is used to retrieve the weights
           sess.graph_def,
           output_nodes# The output node names are used to select the usefull nodes
        ) 

        # Finally we serialize and dump the output graph to the filesystem
        with tf.gfile.GFile(output_graph, ""wb"") as f:
            f.write(output_graph_def.SerializeToString())
        print(""%d ops in the final graph."" % len(output_graph_def.node))
        print(g_in)
LOGDIR='.'
train_writer = tf.summary.FileWriter(LOGDIR)
train_writer.add_graph(sess.graph)

```

The error I get:
```

tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value NAME_OF_NODE_IN_GRAPH
```

Thank you for your time!"
21584,Possible memory leak?,"Hi I was looking into this. The fact that we are adding the loss to the total inside the gradient_tape would save all the previous iterations within the epoch. Am I wrong?

https://github.com/tensorflow/tensorflow/blob/2942555be63f2dbf557cdf7be2bcbd5d4e9d0daf/tensorflow/contrib/eager/python/examples/gan/mnist.py#L243"
21583,python XML content control,"
Below is a code snippet from the XML content that annotates the object tag and annotates it. Some XML files do not have an object tag. In this case it gives a keyError error. How can I pass such xml files without giving keyError

        annotation = read_xml(annotation_path)
        image = read_image(image_path)

        gt_boxes = []

        for b in annotation['object']:
            try:
                label_id = self.classes.index(b['name'])
            except ValueError:
                continue

            gt_boxes.append({
                'label': label_id,
                'xmin': b['bndbox']['xmin'],
                'ymin': b['bndbox']['ymin'],
                'xmax': b['bndbox']['xmax'],
                'ymax': b['bndbox']['ymax'],
            })
I want to put a control code above ""for b in annotation['object']:"" command. Help me please"
21582,Memory leak due to nsync::nsync_waiter_new_()?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
My own build from source as well as google binary found at: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.8.0.tar.gz

- **TensorFlow version (use command below)**:
1.8.0 - using C APi, not python

- **Python version**: 
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

N/A, using google TF 1.8.0 cpu build, calling libtensorflow.so via C API

### Describe the problem

I get valgrind reported memory leak(s), when using TF 1.8 (updating from previously used TF 1.2).  Specifically, the leak comes from nsync::nsync_waiter_new_().   The valgrind output looks like this:

`==1947== 400 bytes in 1 blocks are possibly lost in loss record 80,541 of 82,598
==1947==    at 0x4C2DBF6: malloc (vg_replace_malloc.c:299)
==1947==    by 0xC9A6C34: nsync::nsync_waiter_new_() (in /usr/local/lib/libtensorflow_framework.so)
==1947==    by 0xC9A741E: nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) (in /usr/local/lib/libtensorflow_framework.so)
==1947==    by 0xC9A7AE4: nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) (in /usr/local/lib/libtensorflow_framework.so)
==1947==    by 0x73DCA4A: ??? (in /usr/local/lib/libtensorflow.so)
==1947==    by 0x73DCA9A: ??? (in /usr/local/lib/libtensorflow.so)
==1947==    by 0x73E228F: ??? (in /usr/local/lib/libtensorflow.so)
==1947==    by 0x73EBCE4: ??? (in /usr/local/lib/libtensorflow.so)
==1947==    by 0x5258D29: ??? (in /usr/local/lib/libtensorflow.so)
==1947==    by 0x5259925: TF_SessionRun (in /usr/local/lib/libtensorflow.so)`

I think I may have found the source by looking at the nsync code located at: https://github.com/google/nsync/blob/master/internal/common.c

I cannot observe where the memory allocated by malloc() in nsync_waiter_new_() is freed().  Furthermore, I don't see how the memory could safely be freed because the function allows for a caller-supplied allocator, which it conditionally uses, but does not seem to record which allocator was used, or have a mechanism for calling a caller-provided free().

I believe that this is a memory leak bug in Tensorflow, being caused by the nsync dependency.  

Because the optimized monolithic build of libtensorflow does not seem to have debug symbols enabled by default, and because this call can occur in so many different contexts; it is difficult to generate an appropriate valgrind suppresions file which does not suppress too much.


"
21581,Cannot convert to TFRecords,"I have three text files `A.txt, B.txt,C.txt` the labels are `A,B and C`

    
    def convert(file_paths, labels, out_path):

        print(""Converting: "" + out_path)

        # Number of images. Used when printing the progress.
        num_files = len(file_paths)

        # Open a TFRecordWriter for the output-file.
       with tf.python_io.TFRecordWriter(out_path) as writer:
    
        # Iterate over all the image-paths and class-labels.
        for i, (path, label) in enumerate(zip(file_paths, labels)):
               # Print the percentage-progress.
               print_progress(count=i, total=num_files - 1)

               lines=getModifiedLines(path)


              # Create a dict with the data we want to save in the
              # TFRecords file. You can add more relevant data here.
              data = \
                  {
                      'text': wrap_int64(lines),
                      'label': wrap_int64(label)
                  }

              # Wrap the data as TensorFlow Features.
              feature = tf.train.Features(feature=data)

              # Wrap again as a TensorFlow Example.
              example = tf.train.Example(features=feature)

              # Serialize the data.
              serialized = example.SerializeToString()
              writer.write(serialized)

    def getModifiedLines(filePath):
        data = open(filePath, 'r', encoding='UTF8', errors='ignore').read()
        lines = re.split(""\n"", data)
        all_lines=[]
        for line in lines:
          _l=count_vect.fit_transform(line)
          all_lines.append(_l)

    return all_lines

 I am getting error at line `'text': wrap_int64(lines),`

>Error
`[<12071x21108 sparse matrix of type '<class 'numpy.int64'>'
	with 226655 stored elements in Compressed Sparse Row format>] has type 
   <class 'list'>, but expected one of: (<class 'int'>,)`




Have I written custom code  **Yes**
OS Platform and Distribution  **Ubuntu 16.04**
TensorFlow installed from   **https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp27-none-linux_x86_64.whl**

TensorFlow version  **1.9**
Bazel version 
CUDA/cuDNN version   **i think 7**
GPU model and memory **NVIDIA Titan V 12288MB**
Mobile device  **No**"
21578,Useless sampleRateList?,"In the Tensorflow Mobile Android simple speech recognition sample, why feed the inferenceInterface with a sampleRateList which seems useless? The sampleRateList is just initialized without meaningful values.

[Code line 311](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/SpeechActivity.java#L311)"
21575,the tensorflow-CPU version can support X86 in window??,"Now i want to compile tensorflow-CPU by cmake+vs2015 ,which is to get tensorflow.dll and tensorflow.lib，but now I can't sure tensorf-CPU can support X86 in window platform？
thanks

"
21574,"Tensorflow Lite, python API does not work ","### System information
- **TensorFlow version:  1.9.0**
- **Python version:  3.5**

### Describe the problem
I am try run TFlite model file with Python API (like in example: 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md), but I get an error: 
**ImportError: /home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils39NeonMatrixBatchVectorMultiplyAccumulateEPKaiiS2_PKfiPfi**

### Source code / logs
My code:
```
import tensorflow as tf

if __name__ == ""__main__"":
 
   # Load TFLite model and allocate tensors.
   interpreter = tf.contrib.lite.Interpreter(model_path=""./mobilenet_v1_0.25_128_quant.tflite"")
 
   interpreter.allocate_tensors()
 
   #Get input and output tensors.
   input_details = interpreter.get_input_details()
   output_details = interpreter.get_output_details()

   print(input_details)
   print(output_details)
```

Log output:
```
Traceback (most recent call last):
  File ""tflite_test.py"", line 12, in <module>
    interpreter = tf.contrib.lite.Interpreter(model_path=""/home/pi/test/mobilenet_v1_0.25_128_quant/mobilenet_v1_0.25_128_quant.tflite"")
  File ""/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter.py"", line 50, in __init__
    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(
  File ""/home/pi/.local/lib/python3.5/site-packages/tensorflow/python/util/lazy_loader.py"", line 53, in __getattr__
    module = self._load()
  File ""/home/pi/.local/lib/python3.5/site-packages/tensorflow/python/util/lazy_loader.py"", line 42, in _load
    module = importlib.import_module(self.__name__)
  File ""/usr/lib/python3.5/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 986, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 958, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 673, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 673, in exec_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
  File ""/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 28, in <module>
    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()
  File ""/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
  File ""<frozen importlib._bootstrap>"", line 693, in _load
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 577, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 914, in create_module
  File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
ImportError: /home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils39NeonMatrixBatchVectorMultiplyAccumulateEPKaiiS2_PKfiPfi
```"
21573,Question: Scatter_nd_op order of operations on GPU ,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: gcc4.8
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: AMD R9 Nano
- **Exact command to reproduce**: N/A

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Hi,
We are working on a SYCL implementation of TensorFlow. When implementing the backend for the scatter_nd operation we were wondering what is the behavior of concurrent updates.
The test in [scatter_nd_ops_test.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/scatter_nd_ops_test.py#L231) assumes that the operations are applied in a non-deterministic order, which matches what the [documentation](https://www.tensorflow.org/api_docs/python/tf/scatter_nd) says. This corner case is costly to handle on the GPU and it seems that the [CUDA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc#L81) implementation does not handle it, neither does ours.


The documentation says: 

> The order in which updates are applied is nondeterministic, so the output will be nondeterministic if indices contains duplicates.

The test checks for Add and Sub of two concurrent indices to result in the total summation (or subtraction) of both updates to apply (order of additions is nondeterministic but the result is). On a replace operation the output would be nondeterministic (hence the test not checking for it).
With a non blocking GPU implementation the second update applied to an index may (and will probably) overwrite the first update from a different thread.

Our question then is:
Is the test wrong to test that concurrent summation & subtraction is deterministic (in which case we would be happy to write a PR to correct that)?
Otherwise, is the CUDA implementation wrong? In which case the documentation should probably be more descriptive on what happens with concurrent add/sub compared to replace.

Thanks,
"
21572,tf.estimator.train_and_evaluate got wrong Evaluation accuray and loss,"I use tf.estimator.train_and_evaluate to train and evaluate my model. This is my code:

```
def model_fn(features, labels, mode):
    is_training = (mode == tf.estimator.ModeKeys.TRAIN)
    with slim.arg_scope(resnet_v2.resnet_arg_scope()):
        logits, endpoints = resnet_v2.resnet_v2_50(features, num_classes=cls_num,
                is_training=is_training)
    logits = tf.squeeze(logits, [1, 2])
    preds = tf.argmax(logits, 1)
    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
    accuracy = tf.metrics.accuracy(labels=labels, predictions=preds)
    metrics = {'accuracy': accuracy}
    if mode == tf.estimator.ModeKeys.EVAL:
        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)
    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)
    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())
    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)

def train_input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((np.array(img_train), label_train))
    dataset = dataset.repeat(5).batch(8)
    return dataset

def eval_input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((np.array(img_test), label_test))
    dataset = dataset.repeat(1).batch(8)
    return dataset

estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir='logs')
train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)
eval_specs = tf.estimator.EvalSpec(input_fn=eval_input_fn)
tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs)
```
The training step is ok and the loss became very small (about 0.001), but the evaluation result is wrong (the following is the evaluaiton log):

```
...
INFO:tensorflow:Saving dict for global step 625: accuracy = 0.5, global_step = 625, loss = 1330830600000.0
...
```
The task is very simple, just a binaray classfication. I don not think it is overfitting. Is there something wrong for my evaluation code?"
21571,Custom CUDA op kernel launch dimensions,"Are there any kind of restrictions on the grid dimensions for custom op CUDA kernels?

Thanks"
21570,[TensorFlow Android Camera Demo] NOTHING DETECTED with SSD FPN model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Window 10 64bit
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Samsung Galaxy S7 Edge, RAM 4GB
- **TensorFlow installed from (source or binary)**: binary with pip: pip install tensorflow
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: Python 3.6.4 |Anaconda custom (64-bit)
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: None

### Describe the problem
I retrained the model using [SSD FPN](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config). The training process was ok, total loss was acceptable at 0.5 after 1.7K steps.
The problem: I imported to [TensorFlow Android Camera Demo](https://github.com/tensorflow/tensorflow/tree/r1.10/tensorflow/examples/android), at DetectorActivity, I modified:
```
private static final String TF_OD_API_MODEL_FILE = ""file:///android_asset/frozen_inference_graph.pb""; private static final String TF_OD_API_LABELS_FILE = ""file:///android_asset/my_list.txt"";
NOTHING DETECTED. 
```
Just to confirm that, the app was working with [SSD Mobilenet V1](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config) model (re-trained with my own data)

I also changed:
`private static final int TF_OD_API_INPUT_SIZE = 640; ` at DetectorActivity.java because SSD FPN uses input-size 640x640, but not working"
21569,tfgan.gan_train_ops is incompatible with DistibutionStrategy,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 7.0
- **GPU model and memory**: GTX 980M(4G memory)
- **Exact command to reproduce**: N/A

### Describe the problem
tfgan.gan_train_ops is incompatible with DistibutionStrategy.

https://github.com/tensorflow/tensorflow/blob/e7475504094b3018973740df470d2c9ee73a4fd5/tensorflow/contrib/gan/python/train.py#L972 will cause ""You must specify an aggregation method to update a MirroredVariable in Tower Context."" error.
"
21567,[question] [tflite] How to serialize Interpreter?,"Sorry for question, but could you, please, give me hint? I want to construct tflite model with C++ API (not with toco converter) and serialize it. If I use 'Interpreter' class for model creation, can I then convert it to FlatBufferModel somehow? Or the only opportunity is to create model via FlatBufferBuilder API?"
21566,optimize_for_inference output broken graph,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
No.
- **TensorFlow installed from (source or binary)**:
Source.
- **TensorFlow version (use command below)**:
1.10.0
- **Python version**:
2.7
- **Bazel version (if compiling from source)**:
0.15.2
- **GCC/Compiler version (if compiling from source)**:
7.3.0
- **CUDA/cuDNN version**:
9.2/7.1.4.18
- **GPU model and memory**:
GTX 960 / 4G
- **Exact command to reproduce**:
I trained a ssdlite_mobilenet_v2 model and run the following command to optimize the frozen graph.

python -m tensorflow.python.tools.optimize_for_inference \
    --input=frozen_inference_graph.pb \
    --output=optimized_frozen_inference_graph.pb  \
    --input_names=image_tensor --placeholder_type_enum=4 \
    --output_names=num_detections,detection_classes,detection_scores,detection_boxes

It gave a lot of the following warnings but succeeded:
WARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm'

However, the outputted graph is broken:
benchmark_model --graph=optimized_frozen_inference_graph.pb --show_flops --input_layer=image_tensor --input_layer_type=uint8 --input_layer_shape=1,300,300,3 --output_layer=detection_boxes,detection_scores,detection_classes,num_detections

2018-08-13 08:05:27.077759: E tensorflow/core/framework/types.cc:102] Unrecognized DataType enum value 161403164
2018-08-13 08:05:27.080527: E tensorflow/tools/benchmark/benchmark_model.cc:275] Could not create TensorFlow Session: Invalid argument: Input 0 of node Preprocessor/map/while/add/y was passed int32 from Preprocessor/map/while/Switch:1 incompatible with expected unknown dtype enum (161403164)_ref.
"
21564,Tensorflow code is not running on GPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0/7.1.4
- **GPU model and memory**: K80
- **Exact command to reproduce**: 
```
import tensorflow as tf
with tf.device('/GPU:0'):
  w = tf.constant(2.0,shape=[3,3,3,3,1])
  inpt = tf.constant(2.0,shape=[1,5,5,5,3])
  conv = tf.nn.conv3d(inpt,w,[1,1,1,1,1],'SAME')
  init_op = tf.global_variables_initializer()
  with tf.Session() as sess:
    sess.run(init_op)
    print('Convolution Output')
    print( sess.run(conv))
```

I have installed tensorflow from sources and modified(just some print statements in both CPU kernel and GPU kernel) inside '**conv_ops_3d.cc**'. And built from sources, but when I am running on GPU it is always calling CPU code.
Similarly I checked for backprop using '**tf.test.compute_gradient_error**', by keeping some print statements in '**conv_grad_ops_3d.cc**' in CPU kernels and GPU kernels. But this time, it is calling GPU code only (from 'conv_grad_ops_3d.cc' to 'conv_ops_3d.cc'). May be I am missing something small when I am installing from sources..! From the attached file, you can see the yellow colored lines which I changed which are just print statements.
[Conv_ops_3d.pdf](https://github.com/tensorflow/tensorflow/files/2281807/Conv_ops_3d.pdf)
"
21562,tf.estimator.DNNClassifier TypeError: __init__() got an unexpected keyword argument 'batch_norm',"Hello ,
          I've run into the following error and could not find any relevant post on stack overflow. I'm trying to create a DNN classifier using the tf.estimator.DNNClassifier. My  tf.GIT_VERSION is  v1.9.0-0-g25c197e023  and  tf.VERSION  is 1.9.0

### Problem

**TypeError: __init__() got an unexpected keyword argument 'batch_norm'**


It doesn't seem like batch_norm is one of the params
Updating tensorflow to 1.10.0 causes another issue saying bitwise cannot be imported


### Source code / logs

**inspect.getargspec(tf.estimator.DNNClassifier).args** 

gives 

<class 'list'>: ['self', 'hidden_units', 'feature_columns', 'model_dir', 'n_classes', 'weight_column', 'label_vocabulary', 'optimizer', 'activation_fn', 'dropout', 'input_layer_partitioner', 'config', 'warm_start_from', 'loss_reduction']

It has self as a param and the estimator.params dictionary is always None and doesn't give information about the values




"
21558,contrib/quantize vs Graph Transform Tool,"Hi,

I am wondering if the quantize_weights and quantize_nodes transforms in Graph Transform Tool use the same method as contrib/quantizei in this paper: https://arxiv.org/abs/1712.05877"
21557,"In DNNClassifier evaluation results, the 'loss' is not always = 'average_loss' * batch_size","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: yes
- **TensorFlow version (use command below)**: v1.10.0-rc1-19-g656e7a2b34 1.10.0
- **Python version**: Python 3.6.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

### Describe the problem
The evaluation results of a `DNNClassifier` include a `'loss'` and an `'average_loss`'.  It seems that the `'average_loss'` is correct (same as when I compute it manually), while the `'loss'` is weird. 
 It seems that it's roughly equal to the `'average_loss'` times the batch size, but not always. This seems like a bug to me.

I wrote a test (see the code below), and here is its output with different batch sizes (notice that the loss/average_loss ratio is often equal or close to the batch size, but not always, and sometimes it is exactly one off, or simply far from it):

```
Batch size: 1 loss: 0.75871724 average_loss: 0.75871724 Ratio: 1.0
Batch size: 11 loss: 8.33756 average_loss: 0.7587179 Ratio: 10.989012
Batch size: 21 loss: 15.806628 average_loss: 0.75871813 Ratio: 20.833334
Batch size: 31 loss: 22.991457 average_loss: 0.7587181 Ratio: 30.30303
Batch size: 41 loss: 30.348719 average_loss: 0.75871795 Ratio: 40.0
Batch size: 51 loss: 37.935898 average_loss: 0.75871795 Ratio: 50.0
Batch size: 61 loss: 44.63047 average_loss: 0.758718 Ratio: 58.82353
Batch size: 71 loss: 50.581203 average_loss: 0.7587181 Ratio: 66.666664
Batch size: 81 loss: 58.36292 average_loss: 0.75871795 Ratio: 76.92307
Batch size: 91 loss: 68.974365 average_loss: 0.758718 Ratio: 90.90909
Batch size: 101 loss: 75.8718 average_loss: 0.758718 Ratio: 100.0
Batch size: 111 loss: 75.871796 average_loss: 0.75871795 Ratio: 100.0
Batch size: 121 loss: 84.302 average_loss: 0.758718 Ratio: 111.111115
Batch size: 131 loss: 94.83975 average_loss: 0.758718 Ratio: 125.0
Batch size: 141 loss: 94.83975 average_loss: 0.758718 Ratio: 125.0
```

### Source code / logs
```python
from __future__ import division, print_function, unicode_literals

import logging
logging.basicConfig(level=logging.WARNING) # or else it's too verbose

import tensorflow as tf
#tf.enable_eager_execution()  # same exact output if you activate eager execution
import numpy as np

# make the code reproducible (see https://youtu.be/Ys8ofBeR2kA ;-) )
np.random.seed(1234)
config = tf.estimator.RunConfig(tf_random_seed=1234)

num_col = tf.feature_column.numeric_column(""X"", shape=[10])
dnn_clf = tf.estimator.DNNClassifier(hidden_units=[200, 100], n_classes=3,
                                     feature_columns=[num_col], config=config)

# some random data
X = np.random.randn(1000, 10)
y = np.random.randint(3, size=1000)

# train the model
input_fn = tf.estimator.inputs.numpy_input_fn(
    x={""X"": X}, y=y, num_epochs=10, batch_size=32, shuffle=False)
dnn_clf.train(input_fn=input_fn)

# evaluate the model with different batch sizes
for batch_size in range(1, 151, 10):
    input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""X"": X}, y=y, batch_size=batch_size, shuffle=False)
    eval_results = dnn_clf.evaluate(input_fn=input_fn)
    print(""Batch size:"", batch_size, end="" "")
    print(""loss:"", eval_results[""loss""], end="" "")
    print(""average_loss:"", eval_results[""average_loss""], end="" "")
    print(""Ratio:"", eval_results[""loss""] / eval_results[""average_loss""])
```
"
21555,Better  tf.contrib.estimator.early_stopping (Feature request),"
### System information
N/A

### Describe the problem
The current early stopping code doesn't take into account any trends in the loss, and therefore can be thrown off by a single low loss. It would be better to use an exponential moving average of the loss to compute the lowest loss, as that will be less likely to be thrown off by a single very low loss. "
21554,Kernel Restarting The kernel appears to have died. It will restart automatically. importing tensorflow,"hey comrades. am a new ubuntu user, i read most articles on how to install tensorflow in anaconda so that i can run it in the notebook.
i am however facing this challenge which is disturbing having thought i've succeeded. and it's nagging me.
the moment i do ""import tensorflow as tf"", and upon running, i receive the following;
_**Kernel Restarting
The kernel appears to have died. It will restart automatically.**_
i am totally beaten. what should i do? any help is highly regarded.
Note that my kernel works fine with other import of numpy, pandas,... etc. Issues are only available with import of tensorflow.
thank you."
21550,Horrible,"Really a waste of time. Installing all kind of s**t, winpython, anaconda, then, finally, checking that tensorflow doesnt work with 32bit systems. Why cant tensorflow write:
""Only 64 bit supported!!"""
21549,tf.contrib.ffmpeg.decode_video Error,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**:  2.7
- **GCC/Compiler version (if compiling from source)**: 4.8
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: Titan 1080 Ti

### Describe the problem
I try to use tf.contrib.ffmpeg.decode_video to decode *avi video, but i got errors when processing some videos.

### Source code / logs

Source code:

with tf.Session() as sess:
    for pth in pths:
        print(pth)
        vids = tf.contrib.ffmpeg.decode_video(tf.read_file(pth))
        np_frames = sess.run(vids)


Log Information:

UnknownError                              Traceback (most recent call last)
<ipython-input-3-525c5b7a8b81> in <module>()
      5         print(pth)
      6         vids = tf.contrib.ffmpeg.decode_video(tf.read_file(pth))
----> 7         np_frames = sess.run(vids)
      8         im_shapes += [np_frames.shape]

/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    875     try:
    876       result = self._run(None, fetches, feed_dict, options_ptr,
--> 877                          run_metadata_ptr)
    878       if run_metadata:
    879         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1098     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1099       results = self._do_run(handle, final_targets, final_fetches,
-> 1100                              feed_dict_tensor, options, run_metadata)
   1101     else:
   1102       results = []

/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1270     if handle is None:
   1271       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1272                            run_metadata)
   1273     else:
   1274       return self._do_call(_prun_fn, handle, feeds, fetches)

/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
   1289         except KeyError:
   1290           pass
-> 1291       raise type(e)(node_def, op, message)
   1292 
   1293   def _extend_graph(self):

UnknownError: Output created by FFmpeg [967449600] does not match description [100, 608, 1360, 3]
	 [[Node: DecodeVideo_5 = DecodeVideo[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_5)]]

Caused by op u'DecodeVideo_5', defined at:
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/ipykernel/kernelapp.py"", line 486, in start
    self.io_loop.start()
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tornado/ioloop.py"", line 1064, in start
    handler_func(fd_obj, events)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events
    self._handle_recv()
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv
    self._run_callback(callback, msg)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback
    callback(*args, **kwargs)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
    handler(stream, idents, msg)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/ipykernel/ipkernel.py"", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/ipykernel/zmqshell.py"", line 537, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2714, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2818, in run_ast_nodes
    if self.run_code(code, result):
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2878, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-3-525c5b7a8b81>"", line 6, in <module>
    vids = tf.contrib.ffmpeg.decode_video(tf.read_file(pth))
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/contrib/ffmpeg/ffmpeg_ops.py"", line 108, in decode_video
    return gen_decode_video_op_py.decode_video(contents)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/contrib/ffmpeg/ops/gen_decode_video_op_py.py"", line 46, in decode_video
    ""DecodeVideo"", contents=contents, name=name)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
    op_def=op_def)
  File ""/home/flowerfan/anaconda3/envs/tf27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

UnknownError (see above for traceback): Output created by FFmpeg [967449600] does not match description [100, 608, 1360, 3]
	 [[Node: DecodeVideo_5 = DecodeVideo[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_5)]]

"
21548,how to build .tflite file and two outputs,"this is my command:
bazel build //tensorflow/contrib/lite/toco:toco

bazel-bin/tensorflow/contrib/lite/toco/toco  
 --input_file=/tmp/frozen_graph.pb  
 --input_format=TENSORFLOW_GRAPHDEF  
 --output_format=TFLITE  
 --output_file=/tmp/mobilenet_v1_224.tflite  
 --inference_type=FLOAT  
 --input_arrays=input 
 --output_arrays=MobilenetV1/Predictions/Reshape_1 
 --output_arrays=MobilenetV1/Logists 
 --input_shares=1,224,224,3


add this .tflite in android demo , but only find one output. maybe  the .tflite  file load one output,  I  think this problem is this last output_arrays over coverage the next output_arrays. Could anyone can sovle this problem , very appreciate "
21547,tf.image.decode_image cannot decode b64-encoded string in Python3.6,"```python
import tensorflow as tf
import base64

with open('~/001.jpg', 'r') as img:
	encoded_string = base64.b64encode(img.read())
	image_tensor = tf.image.decode_image(encoded_string, channels=3)
	print(image_tensor)
```

When run above code with Python 2, it is ok, the result is:
Tensor(""decode_image/cond_jpeg/Merge:0"", dtype=uint8)

However, when run with Python 3, it shows:
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte"
21544,OutOfRangeError: read less bytes than requested,"The training works fine on my local computer and did not work on my Ubuntu Amazon EC2 server when I increased the size of training data. 

**The first error: [W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184: (out of range, Read less bytes than requested).**

My Ubuntu Amazon EC2 server has less memory, less CPU. Reading the error, I am not sure whether that is the problem. 

Any clue? Thanks."
21543,ConditionalBijectors not Chainable due to Bijector _Mapping not supporting deep dicts,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX High Sierra 10.13.1
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0
- **Python version**: Python 3.6.5 :: Anaconda, Inc.
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

### Describe the problem
I would like to use the [bijectors.Chain](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/bijectors/chain.py) to create a flow of bijector that extend [bijectors.ConditionalBijector](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/bijectors/conditional_bijector.py#L28). It looks like Chain [already supports conditioning under the hood](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/bijectors/chain.py#L263), but the other parts of Bijector are not fully compatible with it.

Suppose we have the following test function:


```python
import tensorflow as tf
import tensorflow_probability as tfp

tfb = tfp.bijectors


def test_chained_condition_bijector(ChainClass):
    flow = ChainClass([
        BijectorWithSupportForConditioning(name=""bijector_1""),
        BijectorWithSupportForConditioning(name=""bijector_2"")
    ])

    x = tf.random_uniform([5])
    flow.forward(
        x,
        bijector_1={'conditions': tf.random_uniform([2])},
        bijector_2={'conditions': tf.random_uniform([3])},
    )
```

Calling `test_chained_condition_bijector` with `bijector.Chain`, we get the following error:

```python
>>> test_chained_condition_bijector(tfb.Chain)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/kristian/code/softqlearning-private/broken_conditional_bijector.py"", line 52, in test_chained_condition_bijector
    'bijector_2': { 'conditions': tf.random_uniform([3]) },
TypeError: forward() got an unexpected keyword argument 'bijector_1'
```

This error can be overcome by creating a `ConditionalChain` class, which extends `ConditionalBijector`:
```python
class ConditionalChain(tfb.ConditionalBijector, tfb.Chain):
    pass
```

Now, if we run the same code again, this time with ConditionalChain, we get the following:

```python
>>> test_chained_condition_bijector(ConditionalChain)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/kristian/code/softqlearning-private/broken_conditional_bijector.py"", line 52, in test_chained_condition_bijector
    'bijector_2': { 'conditions': tf.random_uniform([3]) },
  File ""/Users/kristian/anaconda3/envs/softlearning/lib/python3.6/site-packages/tensorflow/python/ops/distributions/util.py"", line 1461, in _fn
    return fn(*args, **kwargs)
  File ""/Users/kristian/anaconda3/envs/softlearning/lib/python3.6/site-packages/tensorflow_probability/python/bijectors/conditional_bijector.py"", line 35, in forward
    return self._call_forward(x, name, **condition_kwargs)
  File ""/Users/kristian/anaconda3/envs/softlearning/lib/python3.6/site-packages/tensorflow/python/ops/distributions/bijector_impl.py"", line 750, in _call_forward
    mapping = self._lookup(x=x, kwargs=kwargs)
  File ""/Users/kristian/anaconda3/envs/softlearning/lib/python3.6/site-packages/tensorflow/python/ops/distributions/bijector_impl.py"", line 1014, in _lookup
    return self._from_x.get(mapping.x_key, mapping)
TypeError: unhashable type: 'dict'
```

This happens because the [_deep_tuple](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/ops/distributions/bijector_impl.py#L126), which is used to cache Bijector inputs/outputs, doesn't support nested dicts.

I would expect the code to run normally with `ConditionalChain`. Not sure if this is a feature or a bug. If it's a bug, then this is a bug report, and if it's a feature, then this is a feature request to change the behavior. It's also possible that I'm misusing the ConditionalBijectors, in which case I should move this to stackoverflow.

Here's the full code snippet to test the behavior:
```python
import tensorflow as tf
import tensorflow_probability as tfp

tf.enable_eager_execution()


tfb = tfp.bijectors


class BijectorWithSupportForConditioning(tfb.ConditionalBijector):
    def __init__(self, validate_args=False, name=""exp""):
        super(BijectorWithSupportForConditioning, self).__init__(
            validate_args=validate_args,
            forward_min_event_ndims=0,
            name=name)

    def _concat_input(self, x, **condition_kwargs):
        return tf.concat(
            [x] + [condition_kwargs[k] for k in sorted(condition_kwargs)],
            axis=-1)

    def _forward(self, x, **condition_kwargs):
        return tf.exp(self._concat_input(x, **condition_kwargs))

    def _inverse(self, y, **condition_kwargs):
        return tf.log(self._concat_input(y, **condition_kwargs))

    def _inverse_log_det_jacobian(self, y, **condition_kwargs):
        return -self._forward_log_det_jacobian(self._inverse(
            self._concat_input(y, **condition_kwargs)))

    def _forward_log_det_jacobian(self, x, **condition_kwargs):
        return self._concat_input(x, **condition_kwargs)


class ConditionalChain(tfb.ConditionalBijector, tfb.Chain):
    pass


def test_chained_condition_bijector(ChainClass):
    flow = ChainClass([
        BijectorWithSupportForConditioning(name=""bijector_1""),
        BijectorWithSupportForConditioning(name=""bijector_2"")
    ])

    x = tf.random_uniform([5])
    flow.forward(
        x,
        bijector_1={'conditions': tf.random_uniform([2])},
        bijector_2={'conditions': tf.random_uniform([3])},
    )


if __name__ == '__main__':
    test_chained_condition_bijector(tfb.Chain)
    # test_chained_condition_bijector(ConditionalChain)
```"
21542,fatal: not a git repository: 'E:/tensorflow/tensorflow/contrib/cmake/build/nsync/src/nsync/.git',"i want to use cmake+VS2015+win10 to compile tensorflow-CPU,to get tensorflow.dll and tensorflow.lib，in total 6 errors , 
but always happen :
Creating directories for 'zlib'
  Building Custom Rule E:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt
  CMake does not need to re-run because E:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to
  -date.
  Performing download step (git clone) for 'zlib'
  Cloning into 'zlib'...
  fatal: Out of memory, malloc failed (tried to allocate 947912704 bytes)
  Cloning into 'zlib'...
  fatal: Out of memory, malloc failed (tried to allocate 947912704 bytes)
  Cloning into 'zlib'...
  fatal: Out of memory, malloc failed (tried to allocate 947912704 bytes)
  -- Had to git clone more than once:
  CMake Error at E:/tensorflow/tensorflow/contrib/cmake/build/zlib/tmp/zlib-gitclone.cmake:66 (message):
    Failed to clone repository: 'https://github.com/madler/zlib'
 3 times.
C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: “cmd.exe”已退 出，
代码为 1。 [E:\tensorflow\tensorflow\contrib\cmake\build\zlib.vcxproj]


how to solve it??thanks,please

"
21540,The nn_test unit test testGradient.L2LossTest fails on AVX512 builds,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.8.0-6288-g335336a', '1.10.0-rc1')
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**: [bazel release 0.15.0]
- **GCC/Compiler version (if compiling from source)**: g++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A 
- **Exact command to reproduce**:

bazel test --config=opt -- //tensorflow/python:nn_test

### Describe the problem

The test case fails on AVX512 builds.  See the logs below.

### Source code / logs
```
======================================================================
FAIL: testGradient (__main__.L2LossTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/ops/nn_test.py"", line 224, in testGradient
    self.assertLess(err, err_tolerance)
AssertionError: 1.275390903998641e-11 not less than 1e-11

----------------------------------------------------------------------
Ran 81 tests in 7.372s

FAILED (failures=1)
0.0208333333333
0.00566666666667
0.0075
0.0208333333333
0.00566666666667
0.0075
0.0208333333333
0.00566666666667
0.0075
0.0208333333333
0.00566666666667
0.0075
L2Loss gradient err = 1.27539e-11 
L2Normalize gradient err = 4.2424e-08 
L2Normalize gradient err = 5.45829e-07 
L2Normalize gradient err = 7.61142e-05 
```
"
21538,why the results of tf.contrib.crf.viterbi_decode is different from tf.contrib.crf.crf_decode?,"### Describe the problem
For viterbi decode,i have try the both api:
tf.contrib.crf.viterbi_decode and tf.contrib.crf.crf_decode.
I got different results.

### Source code 
the way i use  tf.contrib.crf.viterbi_decode:
            for logit, sequence_length in zip(logits, sequence_lengths):
                logit = logit[:sequence_length] # keep only the valid steps
                viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(
                        logit, trans_params)
the way i use  tf.contrib.crf.crf_decode:
 viterbi_sequence, viterbi_score = tf.contrib.crf.crf_decode(self.logits, self.trans_params,self.sequence_lengths)"
21537,problem when num_enqueue == batch_size,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 18.04 

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A

- **TensorFlow installed from (source or binary)**:
source 

- **TensorFlow version (use command below)**: 
v1.08, v1.10

- **Python version**:
2.7

- **Bazel version (if compiling from source)**:
0.16.0

- **GCC/Compiler version (if compiling from source)**:
6.4.0 

- **CUDA/cuDNN version**:
9.2

- **GPU model and memory**:
NVIDIA 1080 Ti

- **Exact command to reproduce**:
```
      image_list = [ tf.image.crop_to_bounding_box(image, 16, 16, 224, 224),
                    tf.image.crop_to_bounding_box(image, 0, 0, 224, 224),
                    tf.image.crop_to_bounding_box(image, 32, 0, 224, 224),
                    tf.image.crop_to_bounding_box(image, 0, 32, 224, 224),
                    tf.image.crop_to_bounding_box(image, 32, 32, 224, 224),
                    tf.image.flip_left_right(tf.image.crop_to_bounding_box(image, 16, 16, 224, 224)),
                    tf.image.flip_left_right(tf.image.crop_to_bounding_box(image, 0, 0, 224, 224)),
                    tf.image.flip_left_right(tf.image.crop_to_bounding_box(image, 32, 0, 224, 224)),
                    tf.image.flip_left_right(tf.image.crop_to_bounding_box(image, 0, 32, 224, 224)),
                    tf.image.flip_left_right(tf.image.crop_to_bounding_box(image, 32, 32, 224, 224)) ]
      image = tf.stack([image_list[x] for x in range(10)], axis=0)
      label = tf.stack([label for x in range(10)], axis=0)

    #Make batches
    if objective == ""test"":  x, y_ = tf.train.batch([image, label], batch_size=10,        capacity=200,  num_threads=2, enqueue_many=True, allow_smaller_final_batch=True)
```
And then evaluate accuracy with:
```
  for i in range(int(math.ceil(nTest))):
    pred, trueLabel, top_five_ = sess.run([test_y, test_y_, top_five])
    pred = int(np.argmax(np.mean(pred, axis=0)))
    truth = int(np.argmax(trueLabel[0]))
    test_accuracy_ = (1. if pred == truth else 0.)
    accuracies.append(test_accuracy_)
accuracy = np.mean(accuracies)
```

### Describe the problem
When there are 10 images being enqueued into a tf.train.batch() with batch_size = 10, the reported accuracy is artificially low (e.g., 5%). If you change the batch_size to a higher number (e.g., 100) [and adjust the accuracy calculation accordingly], the reported accuracy is much higher (e.g., 50%). Seems there's some bug about num_enqueued == batch_size. 

"
21532,How to take weight from file checkpoint ckpt ?,i want to take weight parameter from model object detection api ? how to do that ?
21531,Build from source with python3.7 failed(prototype changed),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:arch
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:1.10.0
- **Python version**:python-3.7
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: gcc-7
- **CUDA/cuDNN version**:cuda-9.2,cudnn-7.1
- **GPU model and memory**:24G
- **Exact command to reproduce**:

build from newest source with Python3.7 failed at 
`external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:42: error: invalid conversion from 'const char*' to 'char*'`
As the https://docs.python.org/3.7/c-api/unicode.html#c.PyUnicode_AsUTF8AndSize said 
`Changed in version 3.7: The return type is now const char * rather of char *` for `PyUnicode_AsUTF8AndSize() prototype` 
Maybe you should update it accordingly."
21530,"[Bug] Broken Combination: Non-SGD Optimizer, tf.Variable(), and Estimator Framework","The following combination is broken when simultaneously used:
* Optimizer other than SGD
* Estimator framework
* using `tf.Variable` rather than `tf.get_variable`
The bug might be more specific, since I am basing this directly off of the full setup found in the official resnet example in `models/`

I have made a 3-line modification to the official resnet example illustrate problem:
https://github.com/tensorflow/models/compare/master...liuyipei:BUG_NonSGDOptimizer_Variable_Estimator

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I can illustrate the bug with minimal change to an official example.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
centos-release-7-4.1708.el7.centos.x86_64
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
v1.10.0-0-g656e7a2b34 1.10.0
- **Python version**:
3.4.5
- **CUDA/cuDNN version**:
CUDA 9, cuDNN 7
- **Bazel version**:
N/A
- **Mobile device**:
N/A
- **GPU model and memory**:
Nvidia GTX1080
- **Exact command to reproduce**:
```
cd models/official/resnet
git remote add liuyipei git@github.com:liuyipei/models.git
git fetch liuyipei
git checkout liuyipei/BUG_NonSGDOptimizer_Variable_Estimator
python cifar10_download_and_extract.py
python cifar10_main.py
```


### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I changed 3 lines inside an officially supported model to illustrate my point.
https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py
I added a `z` variable. If I use `tf.Variable` directly, an error happens at training time.
```
      z = tf.Variable(0, dtype=tf.float32, trainable=True, name='tf_Variable_direct') # this fails at optimization time
      # z=tf.get_variable(""tf_get_variable"", [], dtype=tf.float32) #this works!
      inputs = tf.identity(inputs+z, 'final_reduce_mean')
```

The error is:
```
  File ""/home/yiliu/gpuenv/lib/python3.4/site-packages/tensorflow/python/training/momentum.py"", line 98, in _apply_dense
    use_nesterov=self._use_nesterov).op
  File ""/home/yiliu/gpuenv/lib/python3.4/site-packages/tensorflow/python/training/gen_training_ops.py"", line 634, in apply_momentum
    name=name)
  File ""/home/yiliu/gpuenv/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py"", line 641, in _apply_op_helper
    ""(e.g.: a tf.Variable)"") % (op_type_name, input_name))
TypeError: 'ApplyMomentum' Op requires that input 'accum' be a mutable tensor (e.g.: a tf.Variable)
```

I have the following additional anecdotal information:
* The bug does not manifest when `GradientDescentOptimizer` is used, presumably because it doesn't involve momentum updates.
* The bug does not manifest itself when I declare the variable using `tf.get_variable`
* I did some more digging (work not shown), but it seems to have something to do with the dtype of the object created directly by `tf.Variable` having dtype `float32_ref` after coming through `optimizer.compute_gradients(loss * loss_scale)` inside `resnet_run_loop.py`. This is unlike the other variables, which have `float32` in that context. Maybe there is some kind of confusion in type checking."
21528,Optimization flags are ignored in builds,"user defined optimization flags, i.e. bazel build -copt=""xxx"", regardless whether passed from command-line or are not propagated to compilation steps. This creates a problem especially in cuda builds and same for both clang and nvcc based builds. Here is an example

Bazel command-line
```shell
bazel build -s -c opt --copt=""-march=sandybridge"" --copt=""-mtune=broadwell"" --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" tensorflow/tools/pip_package:build_pip_package
```

one of the compilation commands
```
external/local_config_cuda/crosstool/extra_tools/bin/clang -MD -MF bazel-out/host/bin/tensorflow/compiler/xla/service/gpu/_objs/gpu_executable/tensorflow/compiler/xla/service/gpu/tuple_thunk.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/compiler/xla/service/gpu/_objs/gpu_executable/tensorflow/compiler/xla/service/gpu/tuple_thunk.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTENSORFLOW_USE_JEMALLOC -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D_DEBUG -DLLVM_BUILD_GLOBAL_ISEL -iquote . -iquote bazel-out/host/genfiles -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote external/jemalloc -iquote bazel-out/host/genfiles/external/jemalloc -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/genfiles/external/local_config_cuda -iquote external/llvm -iquote bazel-out/host/genfiles/external/llvm -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/jemalloc/include -isystem bazel-out/host/genfiles/external/jemalloc/include -isystem bazel-out/host/bin/external/jemalloc/include -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/llvm/lib/IR -isystem bazel-out/host/genfiles/external/llvm/lib/IR -isystem bazel-out/host/bin/external/llvm/lib/IR -isystem external/llvm/include/llvm/IR -isystem bazel-out/host/genfiles/external/llvm/include/llvm/IR -isystem bazel-out/host/bin/external/llvm/include/llvm/IR -isystem external/llvm/include -isystem bazel-out/host/genfiles/external/llvm/include -isystem bazel-out/host/bin/external/llvm/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 -c tensorflow/compiler/xla/service/gpu/tuple_thunk.cc -o bazel-out/host/bin/tensorflow/compiler/xla/service/gpu/_objs/gpu_executable/tensorflow/compiler/xla/service/gpu/tuple_thunk.pic.o
```"
21526,Address missing TensorFlow operations to TFLite:,"We track operations that we need add to TensorFlow Lite here:

- [ ] Conv3d (https://github.com/tensorflow/tensorflow/issues/19658)
- [ ] Quantized TransposeConv (https://github.com/tensorflow/tensorflow/issues/21394)
- [ ] SpaceToBatchND (non-4D) (https://github.com/tensorflow/tensorflow/issues/21266)
- [ ] BatchToSpaceND (non-4D) (https://github.com/tensorflow/tensorflow/issues/21266)
- [ ] SquaredDifference (quantized) (https://github.com/tensorflow/tensorflow/issues/21986)
- [ ] t2t model ops (Abs, All, BatchMatMul, Cos, DatasetToSingleElement, Enter, Exit, Fill, FloorMod, GatherNd, IsFinite, ListDiff, LoopCond, MapDataset, MatrixBandPart, Merge, PaddedBatchDatasetV2, Range, RefEnter, ScatterNd, Switch, TensorSliceDataset, Where, ZerosLike, convert_gradient_to_tensor_HBc3xYw22Mw) (https://github.com/tensorflow/tensorflow/issues/20679)
- [ ] RandomUniform (https://github.com/tensorflow/tensorflow/issues/23651)
- [ ] Quantized Div (https://github.com/tensorflow/tensorflow/issues/21526)

Please comment with new operations you may want, we will add them to the list and remove your comment. Thanks!"
21525,A little bug from the api tf.layers.dence page.,"There is a small mistake on your page of the description of the api tf.layers.dense.
The line:
`This layer implements the operation: outputs = activation(inputs.kernel + bias) Where activation is the `
I think the . should be · ?"
21524,Nightly Build Binaries,"@av8ramit Are there any updates on ways to obtain nightly builds for the dynamic libraries (i.e., libtensorflow.so and libtensorflow_framework.so)? In the beginning of the summer you had mentioned that you were working on a new way to do that."
21523,SyncReplicaOptimizer prints global step warning unnecessarily,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**:  cuda-9.0/ cuDNN 7
- **GPU model and memory**: TITAN X 12GB
- **Exact command to reproduce**:  Run the code below to produce the warnings

### Describe the problem
This is less a bug and more a suggested improvement. The setup illustrated in the code above is common for reinforcement learning algorithms where the workers are used to collect experience by acting in an environment according to the current policy in a distributed fashion and then synchronizing to update the parameters of the shared networks (stored on the parameter server). There are two ops that are run (multiple times) in a single iteration of the training loop. One is an `act` op and the other is the `update` op. The `act` op has nothing to do with the optimizer while the `update` op does. As such, running the `act` op should not affect the `global_step` while the `update` op should increment `global_step`. The problem, in my estimation, is that tensorflow does not recognize this difference and complains (by printing a warning) that `global_step` is not incremented every time the `act` op is run.

This is extremely annoying as thousands of warnings are printed and useful information is lost in the torrent of text. Turning off all warning is the workaround I am using right now, but this is clearly not a long-term solution.

The exact warning is:

``` 

WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 835 vs previous value: 835. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
```

### Source code / logs
```
import os
import multiprocessing as mp
import tensorflow as tf
import numpy as np

num_workers=4

def model(images):
    net = tf.layers.dense(images, 500, activation=tf.nn.relu)
    net = tf.layers.dense(net, 500, activation=tf.nn.relu)
    net = tf.layers.dense(net, 10, activation=None)
    return net

def train_run_parallel(cluster, job, task, num_workers):
    server = tf.train.Server(cluster, job_name=job, task_index=task)
    if job == 'ps':
        server.join()
    else:
        worker_device = ""/job:worker/task:{}"".format(task)
        with tf.device(tf.train.replica_device_setter(cluster=cluster)):
            def train_next_batch(batchsize):
                imgs = np.random.rand(batchsize,784)
                inds = np.random.randint(0, 10, (batchsize))
                labels = np.eye(10)[inds]
                return imgs,labels
            
            images = tf.placeholder(tf.float32, [None, 784])
            labels = tf.placeholder(tf.int32, [None, 10])    

            logits = model(images)
            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))

            hooks = []
            global_step = tf.train.get_or_create_global_step()
            optimizer = tf.train.AdamOptimizer(learning_rate=1e-04)
            optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=num_workers,
                                        total_num_replicas=num_workers, )
            
            hooks.append(optimizer.make_session_run_hook(task==0, num_tokens=0))
            train_op = optimizer.minimize(loss, global_step=global_step,
                                          aggregation_method=tf.AggregationMethod.ADD_N)
            
            
            mon_sess = tf.train.MonitoredTrainingSession(master=server.target,
                                                   is_chief=(task == 0),
                                                   checkpoint_dir=""./test_checkpoint_dir"",
                                                   hooks=hooks)

            for e in range(1000):
                # simulate roll-out for reinforcement learning
                for _ in range(100):
                    img_batch, label_batch = train_next_batch(32)
                    pred_logits = mon_sess.run([logits], feed_dict={images:img_batch})
                img_batch, label_batch = train_next_batch(32)
                _, ls, step = mon_sess.run([train_op, loss, global_step],
                                            feed_dict={images: img_batch, labels: label_batch})
                if step % 10 == 0:
                    print(""Worker %d, Train step %d, loss: %f"" % (task, step, ls))
            server.join()
            
def main():    
    cluster = tf.train.ClusterSpec({
        'worker': ['localhost:'+str(30352+w) for w in range(num_workers)],
        'ps': [
            'localhost:30351'
        ]
    })

    job_task_index_map = [('ps', 0)]
    for w in range(num_workers): job_task_index_map.append(('worker', w))

    procs = []

    for job, task in job_task_index_map:
        proc = mp.Process(target=train_run_parallel, args=(cluster, job, task, num_workers))
        procs.append(proc)
        proc.start()
    
    for proc in procs:
        proc.join()

if __name__ == '__main__':
    main()
```
"
21522,Model Parallelism Memory usage issue,"------------------------

### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **Mobile device**: N/A
- **TensorFlow installed from**: From binary
- **TensorFlow version**: 1.9
- **Python version**: 3.6.1
- **Bazel version**: N/A
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0.176
- **GPU model and memory**: Tested on 4X GeForce GTX 1080 with 10GB
- **Exact command to reproduce**: `CUDA_VISIBLE_DEVICES=0,1,2,3 python3 train.py --data-dir=Datasets_Path/cityscapes/ --input-size=""1000,1000"" --batch-size=1`

### Describe the problem
I run the above Model training on single GPU and on multiple GPUs and it looks like the memory is being allocated several times. The model size is about 10,7 GB. By distributing over several GPUs the total model size should not be increased. But as you can see it almost triples. Thereby I can not profit from model parallelism. Even a input size of 1200 x 1200 return an OOM error.

The following screenshots show the memory usage on different GPUs:

### On GPU:0
![gpu0](https://user-images.githubusercontent.com/8779942/43915687-c2372af4-9c0b-11e8-8eac-c7d3d28b7844.png)


### On GPU:0 and 1
![gpu01](https://user-images.githubusercontent.com/8779942/43915680-bda22de0-9c0b-11e8-957e-79321a8a834b.png)


### On GPU:0, 1 and 3
![gpu012](https://user-images.githubusercontent.com/8779942/43915644-a35d67ec-9c0b-11e8-8cd6-e79e948699c4.png)


### On GPU:0, 1,2 and 3
![gpu0123](https://user-images.githubusercontent.com/8779942/43915637-9b08e7a6-9c0b-11e8-9621-5e679af4bae6.png)




### Source code / logs
[Source code](https://github.com/reger-men/PSPNet-Tensorflow-ModelParallelism)
"
21520,Extension 'tensorflow/tensorflow.bzl' has errors,"------------------------

### System information
- **OS Platform and Distribution** : Linux Ubuntu 16.04
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.16.0
- **CUDA/cuDNN version**: 8.0 / 6.0
- **GPU model and memory**: nvidia GTX 860m 16gb
- **TensorFlow installed from**: Haven't install
- **TensorFlow version**: wanted to install 0.12
- **Exact command to reproduce**: NA
- **Mobile device**: NA


### Describe the problem
When I try to download the Tensorflow-gpu from the source with Bazel, it says:

ERROR: /home/kevin/tensorflow/tensorflow/tensorflow.bzl:582:12: name 'set' is not defined
ERROR: error loading package '': Extension 'tensorflow/tensorflow.bzl' has errors


### Source code / logs
$ git checkout r0.12
$ ./configure 
~/tensorflow ~/tensorflow
Please specify the location of python. [Default is /usr/bin/python]: 
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] 
No Google Cloud Platform support will be enabled for TensorFlow

Do you wish to build TensorFlow with Hadoop File System support? [y/N] No Hadoop File System support will be enabled for TensorFlow
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

Using python library path: /usr/local/lib/python2.7/dist-packages
Do you wish to build TensorFlow with OpenCL support? [y/N] 
No OpenCL support will be enabled for TensorFlow
Do you wish to build TensorFlow with GPU support? [y/N] y
GPU support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 
Please specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 
Please specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 
Starting local Bazel server and connecting to it...
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
Starting local Bazel server and connecting to it...
ERROR: /home/kevin/tensorflow/tensorflow/tensorflow.bzl:582:12: name 'set' is not defined
ERROR: error loading package '': Extension 'tensorflow/tensorflow.bzl' has errors
Building: no action

"
21518,"1.10 build fails with ""No module named 'keras_applications'""","
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Fedora 28
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.16
- **GCC/Compiler version (if compiling from source)**: 7.3
- **CUDA/cuDNN version**: 9.2
- **GPU model and memory**: Quadro M2200 4G
- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

Hi, a recent checkout of master (~ 2 hours ago) fails (for me) with

```
ERROR: /home/key/code/tensorflow/tensorflow/BUILD:584:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Exit 1)
/home/key/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 81, in <module>
    from tensorflow.python import keras
  File ""/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/keras/__init__.py"", line 25, in <module>
    from tensorflow.python.keras import applications
  File ""/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/keras/applications/__init__.py"", line 21, in <module>
    import keras_applications
ModuleNotFoundError: No module named 'keras_applications'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 3855.754s, Critical Path: 156.93s
INFO: 8932 processes: 8932 local.
FAILED: Build did NOT complete successfully
```

Could you please advise?"
21516,Build failes '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1): gcc failed: error executing command,"I have an error when I try to build tensorflow with Bazel.
python3 -V
Python 3.7.0

gcc -v
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapper

 c++ -v
Using built-in specs.
COLLECT_GCC=c++
COLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapper

bazel version
Build label: 0.16.0- (@non-git)

cat /etc/redhat-release
Red Hat Enterprise Linux Server release 7.5 (Maipo)

Comand
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures 


ERROR: /root/.cache/bazel/_bazel_root/2fbd50cf126faad144af4acc96cb7595/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1): gcc failed: error executing command


Can you help me please?






"
21515,BUG: optimizer.compute_gradients() produces inconsistent gradient with the same training instance and label ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu Server & Window 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 & v1.9.0-0-g25c197e023
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: V9.0.178
- **GPU model and memory**: NVIDIA Tesla V100-SXM2-16GB & NVIDIA GeForce GTX 1080 Ti 11GB
- **Exact command to reproduce**:
### Source code / logs

```
import tensorflow as tf
import numpy as np

#########################################################################
## Define Tensorflow Wrapper
print('Define Tensorflow Wrapper......')
def weight_variable(shape):
    """"""Create a bias variable with appropriate initialization.""""""
    initial = tf.constant(0.2, shape=shape)
    return tf.Variable(initial)

def bias_variable(shape):
    """"""Create a bias variable with appropriate initialization.""""""
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

def initialize_variable(shape, name):
    with tf.name_scope(name + '_layer'):
        with tf.name_scope('weights'):
            weights = weight_variable(shape)
        with tf.name_scope('biases'):
            biases = bias_variable([1,shape[3],1,1])
        return weights, biases

def conv2dLayer(inputs, weights, biases, name, act=tf.nn.relu):
    preactivate = tf.nn.conv2d(inputs, weights, strides=[1,1,1,1], padding='VALID', data_format=""NCHW"", dilations=[1,1,1,1], name='conv2d') + biases
    return act(preactivate, name='activation')

def max_pool_3x3(x, name):
    return tf.nn.max_pool(x, ksize=[1, 1, 3, 3], strides=[1, 1, 3, 3], padding='VALID', data_format=""NCHW"", name=name)
    
#########################################################################
## Create Neural Network
print('Create Neural Network......')
# Initial Trainable Parameters
Conv1W,Conv1b = initialize_variable([3,3,1,2],'Conv1')
Conv2W,Conv2b = initialize_variable([3,3,2,2],'Conv2')
Conv3W,Conv3b = initialize_variable([3,3,2,2],'Conv3')
Conv4W,Conv4b = initialize_variable([3,3,2,2],'Conv4')
fc1W,fc1b = initialize_variable([39,14,2,372],'fc1')
fc2W,fc2b = initialize_variable([1,1,372,372],'fc2')
fc3W,fc3b = initialize_variable([1,1,372,1],'fc3')
# Placeholder for input and output
x = tf.placeholder(tf.float32, [None,1,372,None], name='xInput')
y_ = tf.placeholder(tf.float32, [None,1,1,None], name='yInput')
# Architecture
Conv1 = conv2dLayer(x,Conv1W,Conv1b,'Conv1')
Conv2 = conv2dLayer(Conv1,Conv2W,Conv2b,'Conv2')
MaxPool1 = max_pool_3x3(Conv2,'MaxPool1')
Conv3 = conv2dLayer(MaxPool1,Conv3W,Conv3b,'Conv3')
Conv4 = conv2dLayer(Conv3,Conv4W,Conv4b,'Conv4')
MaxPool2 = max_pool_3x3(Conv4,'MaxPool2')
fc1 = conv2dLayer(MaxPool2,fc1W,fc1b,'fc1')
fc2 = conv2dLayer(fc1,fc2W,fc2b,'fc2')
y = conv2dLayer(fc2,fc3W,fc3b,'fc3',act=tf.identity)
# Define loss
crossEntropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y, name='crossEntropy')
lossValue = tf.reduce_mean(crossEntropy, name='lossValue')
# Define optimizer
#optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.001,rho=0.95,epsilon=1e-08,use_locking=False,name='Adadelta')
#optimizer = tf.train.AdagradOptimizer(learning_rate=0.01,initial_accumulator_value=0.1,use_locking=False,name='Adagrad')
#optimizer = tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.9,beta2=0.999,epsilon=1e-08,use_locking=False,name='Adam')
#optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.95, name='nestrov', use_nesterov=True)
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01, name='GradientDescent')
gvs = optimizer.compute_gradients(lossValue)
train_step = optimizer.apply_gradients(gvs)

#########################################################################
## Illustrate compute_gradients() random issue
print('Load x and y input......')
data = np.load('testGrad.npz')
xInput = data['x']
yInput = data['y']

print(""Session 1"")
with tf.Session() as sess1:
    sess1.run(tf.global_variables_initializer())
    lossValue_eval1, gvs_eval1 = sess1.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})
    sess1.run(train_step, feed_dict={x:xInput,y_:yInput})
    lossValue1_eval1, gvs1_eval1, = sess1.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})
    sess1.close()

print(""Session 2"")
with tf.Session() as sess2:
    sess2.run(tf.global_variables_initializer())
    lossValue_eval2, gvs_eval2 = sess2.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})
    sess2.run(train_step, feed_dict={x:xInput,y_:yInput})
    lossValue1_eval2, gvs1_eval2, = sess2.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})
    sess2.close()

print('--------------------------------------------------')
print('Check whether they have the same values in each session')
print('Before Training: ')
print('lossValue? %r' % all(lossValue_eval1==lossValue_eval2))
check_gvs_BT = np.zeros((14,2))
for i in range(14):
    for j in range(2):
        check_gvs_BT[i][j] = all(gvs_eval1[i][j] == gvs_eval2[i][j])
print('gvs? %r' % all(check_gvs_BT))
print('After Training: ')
print('lossValue? %r' % all(lossValue1_eval1==lossValue1_eval2))
check_gvs_AT = np.zeros((14,2))
for i in range(14):
    for j in range(2):
        check_gvs_AT[i][j] = all(gvs1_eval1[i][j] == gvs1_eval2[i][j])
print('gvs? %r' % all(check_gvs_AT))

tf.reset_default_graph()
```

### Describe the problem
By executing the above code with the fixed training instance and label (which you can download from https://www.dropbox.com/s/828v71z4tm399z2/testGrad.npz?dl=0 ), we can see that the gradient value computed by compute_gradients(),  gvs_eval1[0][0][0,0,0,0] & gvs_eval2[0][0][0,0,0,0] are not always the same in each session. This happens regardless which optimizer you use.

Correct me if I have made some stupid mistake in this code. As I have tried simple network architecture (for example, training y = weight * x), and compute_gradients() seems to be working fine. 

"
21513,"Keras callbacks: TensorBoard class does not have ""embeddings"" parameters.","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: Pip (PyPI) 
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Hi, I am using keras for my training and I wish to incorporate tensorboard's embedding visualization features. However, when I am trying to set the parameters on the TensorBoard's callback, it report that:
TypeError: __init__() got an unexpected keyword argument 'embeddings_freq'

I have then decided to take a look at the github source code for callbacks.py (/tensorflow/tensorflow/python/keras/callbacks.py)

From there I can see that the initialization of ""Tensorboard"" class does have the embeddings parameters. 

I then go through the ""callbacks.py"" in my site-packages and what I found out is that the callbacks.py is different from the one on github (I am currently running on tensorflow 1.10). 

As a way to double confirm my finding is true, I then proceed to PyPI to download the wheel file. I then proceed to extract them and check the ""callbacks.py"" and it seems like there aren't any embeddings parameters for ""TensorBoard"" class as well!
![image](https://user-images.githubusercontent.com/40532196/43889045-beeff878-9bf5-11e8-9031-74e0752dfefa.png)

As shown in the capture above, the embeddings parameters do exist on the comments; however it is nowhere to be found in the rest of the code file (as evident in the bottom window where ""embeddings_freq"" only have 1 match in the comment)

If this isn't bug feel free to enlighten me and delete this issue. Thank you!




### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21511,Hi i  run the flowing code,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21510,tf.image.hsv_to_rgb cannot seem to compute gradient,"### Describe the problem 
I found tf.image.hsv_to_rgb cannot seem to compute gradient. However, In principle, the hsv2rgb operation just a linear transformation. Ummm, I am not sure that whether I misunderstand the tf.image.hsv_to_rgb op, otherwise, this is a feature request. 

### Source code / logs 

    new_image_hsv = tf.random_normal(shape=[32,32,3]) 
    new_image_rgb = tf.image.hsv_to_rgb(new_image_hsv, name='hsv2rgb') 
    hsv2rgb = tf.get_default_graph().get_operation_by_name('hsv2rgb') 
    print('hsv2rgb: ', get_gradient_function(hsv2rgb)) #hsv2rgb: None"
21506,export_tflite_ssd_graph_lib.py didn't work with 1.10.0-rc1 ?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux  4.17.11-arch1
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None of them
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:b'v1.9.0-rc2-1981-gb674f63ab1' 1.10.0-rc1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.15.2- (@non-git)
- **GCC/Compiler version (if compiling from source)**: gcc-7
- **CUDA/cuDNN version**: cuda-9.2 cudnn-7.1
- **GPU model and memory**: 24G
- **Exact command to reproduce**: 

**Everything works fine before I updated the newest source code ,compiled and installed it**.
With the installed version(1.10.0-rc1) the **export_tflite_ssd_graph_lib.py**  said 

`
2018-08-09 03:18:34.547212: I tensorflow/tools/graph_transforms/transform_graph.cc:318] Applying strip_unused_nodes
2018-08-09 03:18:34.565931: I tensorflow/tools/graph_transforms/transform_graph.cc:318] Applying fold_constants
Traceback (most recent call last):
  File ""export_tflite_ssd_graph.py"", line 137, in <module>
    tf.app.run(main)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""export_tflite_ssd_graph.py"", line 133, in main
    FLAGS.max_classes_per_detection)
  File ""/home/mae/tf-newest-nightly/tensorflow/tensorflow/models/research/object_detection/export_tflite_ssd_graph_lib.py"", line 280, in export_tflite_graph
    nms_score_threshold, nms_iou_threshold, num_classes, scale_values)
  File ""/home/mae/tf-newest-nightly/tensorflow/tensorflow/models/research/object_detection/export_tflite_ssd_graph_lib.py"", line 137, in append_postprocessing_op
    output_names, transforms)
  File ""/usr/lib/python3.6/site-packages/tensorflow/tools/graph_transforms/__init__.py"", line 51, in TransformGraph
    transforms_string, status)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'TFLite_Detection_PostProcess' in binary running on Mae. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.
`

Other scripts such as train and eval can work except the exporter."
21505,"[TFLite] Is there any plan to support ""ExtractImagePatches"" and ""Elu"" ?","------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Samsung Galaxy)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.10
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: 1080ti
- **Exact command to reproduce**:



### Describe the problem
Is there any plan to support ""ExtractImagePatches"" and ""Elu"" in TFLite ?

### Source code / logs
N/A
"
21504,Linking of rule '//tensorflow/core/grappler/costs:analytical_cost_estimator_test' failed,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10.0-rc1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.16.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: bazel build --tool_tag=ijwb:CLion --compilation_mode=dbg --copt=-O0 --copt=-g --strip=never --dynamic_mode=off --curses=no --color=yes --experimental_ui=no --progress_in_terminal_title=no --test_filter=AnalyticalCostEstimatorTest.SimpleTest:AnalyticalCostEstimatorTest/*.SimpleTest:*/AnalyticalCostEstimatorTest.SimpleTest/*:*/AnalyticalCostEstimatorTest/*.SimpleTest --config=monolithic --build_event_binary_file=/tmp/intellij-bep-991f7840-b7e9-4fc1-8f46-b72fcee6be72 --nobuild_event_binary_file_path_conversion -- //tensorflow/core/grappler/costs:analytical_cost_estimator_test


I'm trying to run [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/analytical_cost_estimator_test.cc) tensorflow test in debug mode using Bazel on CLion IDE. And, I get this error message:

ERROR: /home/name/tensorflow/tensorflow/core/grappler/costs/BUILD:317:1: Linking of rule '//tensorflow/core/grappler/costs:analytical_cost_estimator_test' failed (Exit 1).
collect2: error: ld returned 1 exit status
Target //tensorflow/core/grappler/costs:analytical_cost_estimator_test failed to build

The command that is used is:

bazel build --tool_tag=ijwb:CLion --compilation_mode=dbg --copt=-O0 --copt=-g --strip=never --dynamic_mode=off --curses=no --color=yes --experimental_ui=no --progress_in_terminal_title=no --test_filter=AnalyticalCostEstimatorTest.SimpleTest:AnalyticalCostEstimatorTest/*.SimpleTest:*/AnalyticalCostEstimatorTest.SimpleTest/*:*/AnalyticalCostEstimatorTest/*.SimpleTest --config=monolithic --build_event_binary_file=/tmp/intellij-bep-991f7840-b7e9-4fc1-8f46-b72fcee6be72 --nobuild_event_binary_file_path_conversion -- //tensorflow/core/grappler/costs:analytical_cost_estimator_test

Same command with --verbose_failures added:

 (cd /home/name/.cache/bazel/_bazel_name/0dcafb4e3b790050f9e05bb5277a4ba3/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/name/bin:/home/name/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda-8.0/bin:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  /usr/bin/gcc -o bazel-out/k8-dbg/bin/tensorflow/core/grappler/costs/analytical_cost_estimator_test '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../..,-rpath,$ORIGIN/../../..' -pthread -pthread '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -B/usr/bin -pass-exit-codes -Wl,@bazel-out/k8-dbg/bin/tensorflow/core/grappler/costs/analytical_cost_estimator_test-2.params)."
21503,Installing tensorflow-serving-api 1.9.1 makes the package of tensorflow unuseable,"### System information

== cat /etc/issue ===============================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.1.0 (clang-902.0.39.1)
Target: x86_64-apple-darwin17.5.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy                              1.14.2
protobuf                           3.5.2.post1
tensorflow                         1.8.0
tensorflow-hub                     0.1.0
tensorflow-model-analysis          0.6.0
tensorflow-serving-api             1.8.0
tensorflow-tensorboard             1.5.0
tensorflow-transform               0.6.0
tensorflowjs                       0.1.0
tensorflowonspark                  1.0.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 106: nvidia-smi: command not found

== cuda libs  ===================================================

== cat /etc/issue ===============================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.1.0 (clang-902.0.39.1)
Target: x86_64-apple-darwin17.5.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy                              1.14.2
protobuf                           3.5.2.post1
tensorflow                         1.8.0
tensorflow-hub                     0.1.0
tensorflow-model-analysis          0.6.0
tensorflow-serving-api             1.8.0
tensorflow-tensorboard             1.5.0
tensorflow-transform               0.6.0
tensorflowjs                       0.1.0
tensorflowonspark                  1.0.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 106: nvidia-smi: command not found

== cuda libs  ===================================================

### Describe the problem

After installing the package `tensorflow-serving-api`, the package `tensorflow` works abnormally.

Firstly, we can run the script when TensorFlow installed to print the version.

```
import tensorflow as tf
print(tf.__version__)
```

Then we install `tensorflow-serving-api` with `pip` and try to run the same script. It throws the exception like this.

```
AttributeError: module 'tensorflow' has no attribute '__version__'
```

If we uninstall `tensorflow-serving-api` and reinstall `tensorflow`, everything works normally.


### Source code / logs

<img width=""1280"" alt=""screen shot 2018-08-09 at 11 41 10 am"" src=""https://user-images.githubusercontent.com/2715000/43878299-e97209e4-9bd0-11e8-877b-949b85c80fa1.png"">
"
21502,tf.train.Saver() will save both the checkpoint and meta-graph,"I found that, `tf.train.Saver()` will save both the `checkpoint` and `meta-graph`.
However in many tensorflow tutorial, it adopt the following usage:
```py
saver = tf.train.Saver()
saver.save(sess, 'my-save-dir/my-model-10000')    # will generate my-model-10000.meta
saver.export_meta_graph('my-save-dir/my-model-10000.meta')  # not need
```


# the following case

https://www.tensorflow.org/api_guides/python/meta_graph#Import_a_MetaGraph

```py
import tensorflow as tf
import math
# Creates an inference graph.
# Hidden 1
images = tf.constant(1.2, tf.float32, shape=[100, 28])
with tf.name_scope(""hidden1""):
  weights = tf.Variable(
      tf.truncated_normal([28, 128],
                          stddev=1.0 / math.sqrt(float(28))),
      name=""weights"")
  biases = tf.Variable(tf.zeros([128]),
                       name=""biases"")
  hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)
# Hidden 2
with tf.name_scope(""hidden2""):
  weights = tf.Variable(
      tf.truncated_normal([128, 32],
                          stddev=1.0 / math.sqrt(float(128))),
      name=""weights"")
  biases = tf.Variable(tf.zeros([32]),
                       name=""biases"")
  hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)
# Linear
with tf.name_scope(""softmax_linear""):
  weights = tf.Variable(
      tf.truncated_normal([32, 10],
                          stddev=1.0 / math.sqrt(float(32))),
      name=""weights"")
  biases = tf.Variable(tf.zeros([10]),
                       name=""biases"")
  logits = tf.matmul(hidden2, weights) + biases
  tf.add_to_collection(""logits"", logits)

init_all_op = tf.global_variables_initializer()

with tf.Session() as sess:
  # Initializes all the variables.
  sess.run(init_all_op)
  # Runs to logit.
  sess.run(logits)
  # Creates a saver.
  saver0 = tf.train.Saver()
  saver0.save(sess, 'my-save-dir/my-model-10000')            # it save both checkpoints and meta-graph
  # Generates MetaGraphDef.
  saver0.export_meta_graph('my-save-dir/my-model-10000.meta')  # you can comment this line
```

# env
```yml
tensorflow=1.8.0
```

# background

I read the tensorflow docs, and search in stackoverflow.
I just find similar question, but no proper answer, like this one

https://stackoverflow.com/questions/45208587/relationship-between-tensorflow-saver-exporter-and-"
21501,How to encrypt the tflite file？,"Now  I have my own tflite file，and I am worried that others will copy my file. I want to encrypt my tflite file，so even if others have my file，they cannot use it. 
Is there a way to encrypt the file?"
21500,Memory error with tensorflow GPU,"I have NVIDIA GPU Titan V installed on my remote machine and I am trying to do simple multi class lassification using CNN


`train_file = './data/csv_data.csv.zip'`
 `x_raw, y_raw, df, labels = data_helper.load_data_and_labels(train_file)`


here my csv_data zip file is 65MB .

`max_document_length = max([len(x.split(' ')) for x in x_raw])`
`logging.info('The maximum length of all sentences: {}'.format(max_document_length))`
`vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)`
`x = np.array(list(vocab_processor.fit_transform(x_raw)))`
`y = np.array(y_raw)`


I am getting memory error at 
`vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)`

I think it is trying to load all the data in memory which is obviously a bad practice. How can I optimize this code to load it in the memory efficiently.

Can anyone please point me to a detailed example or tutorial? 

Thanks"
21498,bayesflow,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21492,Public API to preempt tf.train.Server,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

`tf.train_and_evaluate` uses `time.sleep` to (optimistically) synchronize the startup of chief/worker nodes in the distributed mode (see [estimator/training.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/training.py#L753)). The implicit assumption in this logic is that by the time the worker is spawned, the chief has already started its `tf.train.Server`, i.e. the scheduler should be aware of the assumption and should schedule and initialize the chief first. This might not be easily achievable on general purpose systems like YARN.

One possible solution to this is to synchronize the chief/worker tasks on a barrier, and then preemptively start the server right after the barrier, but prior to calling `train_and_evaluate`. This does not eliminate the race condition entirely but makes it much less likely in practice. The only problem here is that `train_and_evaluate` does not provide a documented way to account for preempted servers. The undocumented way is:

https://github.com/tensorflow/tensorflow/blob/2b4fd1c2b7a37367e61bbae3d27d194a894cb7bb/tensorflow/python/estimator/training.py#L747-L748

I was wondering if it would be possible to make this part of the `train_and_evaluate` contract public or, alternatively, address the issue in another way?"
21491,Consider automatic casting rules for promoting dtypes,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NA
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: NA
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: NA
- **TensorFlow version (use command below)**: unknown 1.10.0-rc1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:

Arithmetic between objects with different dtypes only works with Python scalars, not Tensors:
```
# this works
>>> tf.constant(1.0) * 2
<tf.Tensor: id=4, shape=(), dtype=float32, numpy=2.0>

# this doesn't
>>> tf.constant(1.0) * tf.constant(2)
InvalidArgumentError: cannot compute Mul as input #0 was expected to be a int32 tensor but is a float tensor [Op:Mul] name: mul/
```

### Describe the problem

NumPy has well defined [casting rules](https://docs.scipy.org/doc/numpy/reference/ufuncs.html#casting-rules) for converting between dtypes. If an operation cannot be performed natively on the given dtypes (e.g., `np.add` between float32 and float64), then one or more of the inputs will be promoted to a higher data-type, e.g., float32 -> float64.

This is highly convenient, and remains one of the major annoyances when porting NumPy code to TensorFlow. TensorFlow code ends up littered with calls to `tf.cast()`. It's particularly annoying for any use-cases that require types other than float32, e.g., models that use complex numbers, because you can't simply cast every argument to float32.

An argument for not implementing this would be that TensorFlow strives to be more explicit than NumPy, and this could lead to unintended performance degradation. But I'm struggling to imagine cases where this would actually be the case. It's also easy to avoid by using explicit casting on everything, which could still be utilized by users who concerned about it. There are lots of areas where NumPy got things wrong by being undisciplined (e.g., indexing rules) but I have not heard any complaints about this one."
21488,Tensorflow PIP Package fails to build - AttributeError: module 'pandas' has no attribute 'core',"This may be a bug in the Python PIP build script(s), or something askew in my environment.  I am trying to build Tensorflow from source, to include AVX/AVX2 support.  It always fails with the error log pasted below.  I have tried the latest git pull over the last week or so (i.e. multiple git pulls, followed by ""bazel clean ; bazel build <options>"".  I have also tried the same with the r1.10 branch.

There is [this somewhat similar question on StackOverflow](https://stackoverflow.com/questions/36521691/importing-pandas-gives-error-attributeerror-module-pandas-has-no-attribute-c).  That is more about using Pandas, rather than building Tensorflow.  At any rate I tried all the suggestions to no avail.  And my Pandas is working:

```
$ python -c ""import pandas as pd ; import pandas.core.ops as ops ; print(pd.__version__)"" && echo $?
0.23.4
0
```

System Details:
-----------------

  - Have I written custom code: NO
  - OS Platform and Distribution: CentOS Linux 7.4.1708 with Anaconda Python3 distribution
  - Mobile device: N/A
  - TensorFlow installed from: not yet installed, trying to build from source (no build-halting errors during the C/C++ portion of the build)
  - TensorFlow version: latest git pull over the last several days, both master and r1.10 branch
  - Python version: Python 3.6.6 :: Anaconda custom (64-bit)
  - Bazel version (if compiling from source):
```
$ bazel version
Build label: 0.16.0- (@non-git)
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Jul 31 18:27:17 2018 (1533061637)
Build timestamp: 1533061637
Build timestamp as int: 1533061637
```
  - GCC/Compiler version: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)
  - CUDA/cuDNN version: N/A
  - GPU model and memory: N/A
  - Exact command to reproduce:
```
  $ git clone https://github.com/tensorflow/tensorflow.git
  $ cd tensorflow
  $ ./configure # accept all defaults
  $ bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 --verbose_failures -k //tensorflow/tools/pip_package:build_pip_package
```

Output of tf_env_collect.sh:
------------------------------

== cat /etc/issue ===============================================
Linux septictank 3.10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 19:03:37 UTC 2018 x
86_64 x86_64 x86_64 GNU/Linux
VERSION=""7 (Core)""
VERSION_ID=""7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux septictank 3.10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 19:03:37 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                              1.15.0
numpydoc                           0.8.0
protobuf                           3.5.2

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""/home/matt/tensorflow/tensorflow/python/platform/self_check.py"", line 2$, in <module>
    from tensorflow.python.platform import build_info
ImportError: cannot import name 'build_info'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/matt/tensorflow/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-i$port
  File ""/home/matt/tensorflow/tensorflow/python/__init__.py"", line 49, in <modul
e>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/matt/tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 25,
in <module>
    from tensorflow.python.platform import self_check
  File ""/home/matt/tensorflow/tensorflow/python/platform/self_check.py"", line 27
, in <module>
    raise ImportError(""Could not import tensorflow. Do not import tensorflow ""
ImportError: Could not import tensorflow. Do not import tensorflow from its sour
ce directory; change directory to outside the TensorFlow source tree, and relaun
ch your Python interpreter from there.

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================


Error message from bazel:
-----------------------------
```
ERROR: /home/matt/tensorflow/tensorflow/python/estimator/api/BUILD:12:1: Couldn't build file tensorflow/python/estimator/api/__init__.py: Executing genrule //tensorflow/python/estimator/api:estimator_python_api_gen failed (Exit 1): bash failed: error executing command
  (cd /home/matt/.cache/bazel/_bazel_matt/b51f46dcf2ef174fa43a943ad13629f0/execroot/org_tensorflow && \
  exec env - \
    PATH=/opt/anaconda3/bin:/opt/anaconda3/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/local/bin:/sbin:/usr/sbin:/usr/local/bin:/sbin:/usr/sbin \
    PYTHON_BIN_PATH=/opt/anaconda3/bin/python \
    PYTHON_LIB_PATH=/opt/anaconda3/lib/python3.6/site-packages \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api  --apidir=bazel-out/k8-opt/genfiles/tensorflow/python/estimator/api --apiname=estimator --apiversion=2 --package=tensorflow.python.estimator --output_package=tensorflow.python.estimator.api bazel-out/k8-opt/genfiles/tensorflow/python/estimator/api/__init__.py bazel-out/k8-opt/genfiles/tensorflow/python/estimator/api/estimator/__init__.py bazel-out/k8-opt/genfiles/tensorflow/python/estimator/api/estimator/export/__init__.py bazel-out/k8-opt/genfiles/tensorflow/python/estimator/api/estimator/inputs/__init__.py')
tf.estimator package not installed.
Traceback (most recent call last):
  File ""/home/matt/.cache/bazel/_bazel_matt/b51f46dcf2ef174fa43a943ad13629f0/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 511, in <module>
    main()
  File ""/home/matt/.cache/bazel/_bazel_matt/b51f46dcf2ef174fa43a943ad13629f0/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 504, in main
    importlib.import_module(args.package)
  File ""/opt/anaconda3/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/matt/.cache/bazel/_bazel_matt/b51f46dcf2ef174fa43a943ad13629f0/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api.runfiles/org_tensorflow/tensorflow/python/estimator/__init__.py"", line 25, in <module>
    import tensorflow.python.estimator.estimator_lib
  File ""/home/matt/.cache/bazel/_bazel_matt/b51f46dcf2ef174fa43a943ad13629f0/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api.runfiles/org_tensorflow/tensorflow/python/estimator/estimator_lib.py"", line 41, in <module>
    from tensorflow.python.estimator.inputs import inputs
  File ""/home/matt/.cache/bazel/_bazel_matt/b51f46dcf2ef174fa43a943ad13629f0/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api.runfiles/org_tensorflow/tensorflow/python/estimator/inputs/inputs.py"", line 23, in <module>
    from tensorflow.python.estimator.inputs.pandas_io import pandas_input_fn
  File ""/home/matt/.cache/bazel/_bazel_matt/b51f46dcf2ef174fa43a943ad13629f0/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api.runfiles/org_tensorflow/tensorflow/python/estimator/inputs/pandas_io.py"", line 31, in <module>
    import pandas as pd
  File ""/opt/anaconda3/lib/python3.6/site-packages/pandas/__init__.py"", line 42, in <module>
    from pandas.core.api import *
  File ""/opt/anaconda3/lib/python3.6/site-packages/pandas/core/api.py"", line 10, in <module>
    from pandas.core.groupby.groupby import Grouper
  File ""/opt/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/__init__.py"", line 2, in <module>
    from pandas.core.groupby.groupby import (
  File ""/opt/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py"", line 49, in <module>
    from pandas.core.frame import DataFrame
  File ""/opt/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py"", line 74, in <module>
    from pandas.core.series import Series
  File ""/opt/anaconda3/lib/python3.6/site-packages/pandas/core/series.py"", line 67, in <module>
    import pandas.core.ops as ops
AttributeError: module 'pandas' has no attribute 'core'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
"
21487,TensorRT Can't identify the cuda device,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
('v1.9.0-rc2-1924-g054b046', '1.10.0-rc1'). Current master branch
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: CUDA 9.0 cuDNN 7.1.4
- **GPU model and memory**: Titan V
- **Exact command to reproduce**:
First train the example small model use [mnist.py](https://gist.github.com/qinyao-he/60171806c40b7b46354234b896c7c2d5).
Then use tensorflow built-in tools to freeze the graph:
```
python -m tensorflow.python.tools.freeze_graph --input_graph log/graph.pbtxt --input_checkpoint log/model.ckpt-20000 --output_node_names softmax_tensor --output_graph log/freeze_graph.pb
```
Finally use [tensorrt.py](https://gist.github.com/qinyao-he/28ddedb7f561bb3cb4ba880833f14a89) to optimize the graph use TensorRT engine.

### Describe the problem
The log shows TensorRT could not find cuda devices. And the graph remains unchanged after the conversion.

### Source code / logs
> 2018-08-08 13:47:27.322236: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1                                                                                 
> 2018-08-08 13:47:27.322317: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session                                                                                        
> 2018-08-08 13:47:27.322928: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA                      
> 2018-08-08 13:47:27.327805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties:                                                                               
> name: TITAN V major: 7 minor: 0 memoryClockRate(GHz): 1.455
> pciBusID: 0000:03:00.0
> totalMemory: 11.78GiB freeMemory: 11.36GiB
> 2018-08-08 13:47:27.327827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0                                                                                 
> 2018-08-08 13:47:27.694717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:                                                
> 2018-08-08 13:47:27.694749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0
> 2018-08-08 13:47:27.694754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N
> 2018-08-08 13:47:27.694984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10938 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:03:00.0, compute capability: 7.0)
> 2018-08-08 13:47:27.935823: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2923] Segment @scope '', converted to graph                                                                     
> 2018-08-08 13:47:27.935851: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:415] Can't find a device placement for the op!                                                                  
> 2018-08-08 13:47:27.946155: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:799] Cluster is set but device '' is not found in the cluster                                                   
> 2018-08-08 13:47:27.946194: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:916] Can't identify the cuda device. Running on device 0                                                        
> 2018-08-08 13:47:28.915936: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger runtime.cpp (16) - Cuda Error in allocate: 2                                                         
> 2018-08-08 13:47:28.916287: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger runtime.cpp (16) - Cuda Error in allocate: 2                                                         
> 2018-08-08 13:47:28.916332: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:933] Engine my_trt_op_0 creation for segment 0, composed of 22 nodes failed: Internal: Failed to build TensorRT engine. Skipping...
> 2018-08-08 13:47:28.967359: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2923] Segment @scope '', converted to graph                                                                     
> 2018-08-08 13:47:28.967390: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:415] Can't find a device placement for the op!                                                                  
> 2018-08-08 13:47:28.968228: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:799] Cluster is set but device '' is not found in the cluster                                                   
> 2018-08-08 13:47:28.968242: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:916] Can't identify the cuda device. Running on device 0                                                        
> 2018-08-08 13:47:28.974512: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger runtime.cpp (16) - Cuda Error in allocate: 2                                                         
> 2018-08-08 13:47:28.974935: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger runtime.cpp (16) - Cuda Error in allocate: 2                                                         
> 2018-08-08 13:47:28.974968: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:933] Engine my_trt_op_0 creation for segment 0, composed of 22 nodes failed: Internal: Failed to build TensorRT engine. Skipping...
> 2018-08-08 13:47:28.979975: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:198] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
> 2018-08-08 13:47:28.981037: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:198] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.
> 2018-08-08 13:47:28.982158: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:403] Optimization results for grappler item: tf_graph                                                          
> 2018-08-08 13:47:28.982172: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   constant folding: Graph size after: 52 nodes (-11), 51 edges (-13), time = 72.462ms.                    
> 2018-08-08 13:47:28.982176: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   layout: Graph size after: 52 nodes (0), 51 edges (0), time = 9.414ms.                                   
> 2018-08-08 13:47:28.982179: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   TensorRTOptimizer: Graph size after: 52 nodes (0), 51 edges (0), time = 1003.37ms.                      
> 2018-08-08 13:47:28.982183: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   constant folding: Graph size after: 52 nodes (0), 51 edges (0), time = 34.038ms.                        
> 2018-08-08 13:47:28.982186: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   TensorRTOptimizer: Graph size after: 52 nodes (0), 51 edges (0), time = 20.332ms.                       
> 2018-08-08 13:47:28.982193: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:403] Optimization results for grappler item: my_trt_op_0_native_segment                                        
> 2018-08-08 13:47:28.982198: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   constant folding: Graph size after: 23 nodes (0), 22 edges (0), time = 1.062ms.                         
> 2018-08-08 13:47:28.982204: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   layout: Graph size after: 23 nodes (0), 22 edges (0), time = 0.657ms.                                   
> 2018-08-08 13:47:28.982240: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   TensorRTOptimizer: Graph size after: 23 nodes (0), 22 edges (0), time = 0.16ms.                         
> 2018-08-08 13:47:28.982246: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   constant folding: Graph size after: 23 nodes (0), 22 edges (0), time = 0.898ms.                         
> 2018-08-08 13:47:28.982253: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   TensorRTOptimizer: Graph size after: 23 nodes (0), 22 edges (0), time = 0.14ms.                         
> 2018-08-08 13:47:29.038047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0                                                                                 
> 2018-08-08 13:47:29.038105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:                                                
> 2018-08-08 13:47:29.038120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0
> 2018-08-08 13:47:29.038131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N
> 2018-08-08 13:47:29.038327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10938 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:03:00.0, compute capability: 7.0)"
21485,Keras save_weights/load_weights error for custom layer,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: Quardo M4000
- **Exact command to reproduce**: N/A

### Describe the problem
I create a simple custom layer performing a multiply. However, if I do not define ""get_config()"" function for my custom layer, after saving and loading weights, the layer's weights are not loaded. If I implemented the ""get_config()"" function for my custom layer as suggested by many online discussions, the program crashed when calling ""save_weights"" with the error message ""TypeError: can't pickle _thread.RLock objects"". According to some online discussions, I change the ""self.add_variable"" by ""K.variable"", but the crash still happened. I have went through many online discussions on Github and Stackoverflow, but none worked for me. Therefore, I doubt whether there are some bugs for ""save_weights"" or I use it in a wrong way. Thank you.

My codes are as follows:

	class MaskLayer(Layer):
	  def __init__(self, data_format=None, **kwargs):
		self.data_format = data_format
		super(MaskLayer, self).__init__(trainable=False, **kwargs)

	  def build(self, input_shape):
		if self.data_format == 'channels_first':
		  channel_axis = 1
		else:
		  channel_axis = -1
		dimension = input_shape[channel_axis]
		# Either definition of self.mask below lead to the same results
		self.mask = self.add_variable(name='mask', shape=dimension, 
                                                            initializer='ones', trainable=False, dtype=self.dtype)
	#    self.mask = K.variable(np.ones(dimension), name='mask')
		super(MaskLayer, self).build(input_shape)

	  def call(self, inputs):
		return inputs * self.mask

	  def set_weights(self, weights):
		K.batch_set_value([(self.mask, weights)])

	  # If this get_config is enalbed, the program crashed when calling ""save_weights""
	#  def get_config(self):
	#    config = {'mask': self.mask}
	#    base_config = super(MaskLayer, self).get_config()
	#    return dict(list(base_config.items()) + list(config.items()))
		
	def test():
	  data = np.ones((1,1,4,5), dtype=float)

	  model = keras.Sequential()
	  mask_layer = MaskLayer()
	  model.add(mask_layer)

	  for layer in model.layers:
		print(layer.name)
		weights = layer.get_weights()
		print(weights)
	  results = model.predict(data)
	  print(results)
	  print('-------------------------------')
	  mask_layer.set_weights([1,-1,0,2,-2])

	  for layer in model.layers:
		print(layer.name)
		weights = layer.get_weights()
		print(weights)
	  results = model.predict(data)
	  print(results)
	  print('-------------------------------')
	  
	  model.save_weights('models/test/test')

	  new_model = keras.Sequential()
	  mask_layer = MaskLayer()
	  new_model.add(mask_layer)

	  new_model.load_weights('models/test/test')
	  for layer in new_model.layers:
		print(layer.name)
		weights = layer.get_weights()
		print(weights)

	  results_new = new_model.predict(data)
	  print(results_new)
"
21484,"the url is invalid, maybe you need to update it",The answer to the question is invalid. Can you update it?
21482,"tf.nn.embedding_lookup_sparse without combiner, to get result similar to tf.nn.embedded_lookup using dense input params","I would like to undrestand if `tf.nn.embedding_lookup_sparse` with sparse tensor can be used to get result similar to `tf.nn.embedded_lookup ` which requires dense representation of sparse tensor.

Currently im converting the sparse tensor to dense tensor (using `tf.sparse_tensor_to_dense`) and suppling the same as input to  `tf.nn.embedded_lookup `. This works well as expected

However from the naming convention it seemed `tf.nn.embedding_lookup_sparse` is doing the same process using the sparse tensor without the need to convert to dense representation. But the documentation specifies the method tries to combine the values in the dense representation based on some strategy. 

is there any way we can get the embeddng lookup on the sparse tensor, without combining the row values using the `tf.nn.embedding_lookup_sparse` 

One relavent question on stackoverlfow is copied for reference
[https://stackoverflow.com/questions/45789532/embedding-lookup-sparse-of-tensorflow-without-combiner](url)
"
21478,"Implement ""quantize_graph"" will print ""tf.estimator package not installed.""","Making quantization by using command ""sudo bazel-bin/tensorflow/tools/quantization/quantize_graph ......."" in the directory of tensorflow, and it will input ""tf.estimator package not installed."" And in this directory, implemt ""python->import tensorflow"", it will return error message. But in other directory, implemt ""python->import tensorflow"", it will be normally. The folllowing is my envirnment information: 
 OS Platform and Distribution: Linux Ubuntu 18.04
 TensorFlow installed from: using TF source code (CPU) for graph conversion, not using binary-python(GPU) for inference
TensorFlow version:  using r1.10.0-rc1
Python version: 3.6
Bazel version: 0.16.0

"
21476,Transforming a dataset pre-processing to an inference graph,"If we are working with data with fixed dimensions we can easily load the data in a dataset using from_tensor_slices. Thus, when working on inference, we can use a placeholder that will give the dataset the numpy array.

When working with data with varying dimensions (text and a like) most of the answers I've seen suggest using the from_generator function and it works. However there doesn't seem to be a way to make if work in inference.

Several solutions I though of are running the dataset ops separately from the inference graph or serialize the data and have the inference graph read it.

Is there a recommended way of doing this? I want to reuse the dataset preprocessing codes in the training rather than implement them again with in numpy so that I can feed it to the inference graph. If not I think there is a value in having it in tensorflow.

Have I written custom code: N/A
OS Platform and Distribution: N/A
TensorFlow installed from: N/A
TensorFlow version: master branch
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A
Mobile device: N/A"
21475,FusedBatchNorm problem during benchmark test,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None
- **TensorFlow installed from (source or binary)**: 1.9
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.11.1 (for covert and benchmark the model)
- **GCC/Compiler version (if compiling from source)**: None, installed from build
- **CUDA/cuDNN version**: Not used
- **GPU model and memory**: Not used
- **Exact command to reproduce**:

### Problem description
I'm trying to convert a piece of mobilenetV1 (using slim model) from the master repository [https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md](url); In particular I have added 2 simple convolution after the _MobilenetV1/Conv2d_8_pointwise/Conv2D_. Obviously the number of parameters is more less than the original MobilenetV1. Without train the model, I have initialized all the variables and save them into a freezed model. Now If I visualize the model with tensorboard is the same of that downloaded from the link above until the _MobilenetV1/Conv2d_8_pointwise/Conv2D_ (It compare the Conv2D and the FusedBatchNorm, Gamma, Beta, var and so on). The problems cames when I try to benchmark the model using bazel: the original one is more faster than mine _(FLOPs estimated 1.14B VS 3.31B)_ and also in summaries of the original model the _FusedBatchNorm_ Node type doesn't compare instead in mine model yes. Where am I doing wrong? And there is also the possibility of combining the conv2D and the batch norm layer?

### logs


------------------------- ORIGINAL MOBILENETV1 ---------------------------------------------------------------

>...stat_summarizer.cc:468] 	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
...stat_summarizer.cc:468] 	                  Conv2D	       15	    52.027	    56.168%	    56.168%	 12447.652	       15
...stat_summarizer.cc:468] 	   DepthwiseConv2dNative	       13	    15.413	    16.640%	    72.808%	  7905.664	       13
...stat_summarizer.cc:468] 	                     **Mul**	       27	    12.018	    12.975%	    85.783%	     0.000	       27
...stat_summarizer.cc:468] 	                     **Add**	       27	    11.806	    12.746%	    98.529%	     0.000	       27
...stat_summarizer.cc:468] 	                   **Relu6**	       27	     1.163	     1.256%	    99.784%	     0.000	       27
...stat_summarizer.cc:468] 	                   Const	       83	     0.120	     0.130%	    99.914%	     0.000	       83
...stat_summarizer.cc:468] 	                 Softmax	        1	     0.027	     0.029%	    99.943%	     0.000	        1
...stat_summarizer.cc:468] 	                 AvgPool	        1	     0.022	     0.024%	    99.967%	     4.096	        1
...stat_summarizer.cc:468] 	                    NoOp	        1	     0.007	     0.008%	    99.974%	     0.000	        1
...stat_summarizer.cc:468] 	                 BiasAdd	        1	     0.007	     0.008%	    99.982%	     0.000	        1
...stat_summarizer.cc:468] 	                 Squeeze	        1	     0.006	     0.006%	    99.988%	     0.000	        1
...stat_summarizer.cc:468] 	                 _Retval	        1	     0.005	     0.005%	    99.994%	     0.000	        1
...stat_summarizer.cc:468] 	                    _Arg	        1	     0.004	     0.004%	    99.998%	     0.000	        1
...stat_summarizer.cc:468] 	                Identity	        1	     0.002	     0.002%	   100.000%	     0.000	        1
--------------------------------------------------------------------------------------------------------------

---------------------------- MY MODEL ------------------------------------------------------------------------
>...stat_summarizer.cc:468] 	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
...stat_summarizer.cc:468] 	          **FusedBatchNorm**	       18	   151.477	    48.975%	    48.975%	    72.704	       18
...stat_summarizer.cc:468] 	                  Conv2D	       11	   138.036	    44.630%	    93.605%	 15457.344	       11
...stat_summarizer.cc:468] 	   DepthwiseConv2dNative	        8	    18.148	     5.868%	    99.472%	  9300.352	        8
...stat_summarizer.cc:468] 	                    Relu	       18	     1.581	     0.511%	    99.984%	     0.000	       18
...stat_summarizer.cc:468] 	                   Const	       29	     0.038	     0.012%	    99.996%	     0.000	       29
...stat_summarizer.cc:468] 	                 _Retval	        1	     0.005	     0.002%	    99.997%	     0.000	        1
...stat_summarizer.cc:468] 	                    _Arg	        1	     0.004	     0.001%	    99.999%	     0.000	        1
...stat_summarizer.cc:468] 	                    NoOp	        1	     0.004	     0.001%	   100.000%	     0.000	        1
--------------------------------------------------------------------------------------------------------------
OK, I have noted the differen Relu activation but I do not think that's the problem for the fusedbatchnorm layer.."
21473,Do not generate LSTM op by incorrect param check logic,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: x
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.10
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.16.0
- **GCC/Compiler version (if compiling from source)**: 7.3.0
- **CUDA/cuDNN version**: 9.2
- **GPU model and memory**: 
- **Exact command to reproduce**:

### Describe the problem
**Related to input param of state forget gate activation**

I made a simple RNN test code with below 2 lines. I attached the full code as a file. [rnn.py.txt](https://github.com/tensorflow/tensorflow/files/2269704/rnn.py.txt)
```
lstm_layer=tf.nn.rnn_cell.BasicLSTMCell(num_units)
outputs,_=tf.nn.static_rnn(lstm_layer,input,dtype=tf.float32)
```
And I tried to generate tflite file from frozen pb via toco. But it failed to generate LSTM operation because `state_forget_sig`(State forget gate activation) did not have `Split` as an input parameter. Instead it had a `Add` operation as an input and the `Add` operation had a `Split` operation. It generates the same graph when I tried to use `dynamic_rnn`. I attached the tensorboard image. 

![state_forget_sig](https://user-images.githubusercontent.com/7223627/43870358-09d57a3c-9bb2-11e8-9d7d-2f6dcd8dd5ab.png)

I fixed some codes and it succesfully generated LSTM operations.
```
Number of all operator types: 5
	EXPAND_DIMS                           :    2 	 (instrs: ???)
	CONCATENATION                         :    2 	 (instrs: 0)
	CUSTOM(Fill)                          :    2 	 (instrs: ???)
	LSTM                                  :   28 	 (instrs: ???)
	FULLY_CONNECTED                       :    1 	 (instrs: 7,307,045,943,451,530,714)
Number of all operators                       :   35 	 (total instrs: 7,307,045,943,451,530,714)
```

If I understand it correctly, I will make a PR soon.
"
21472,RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7,"**Environment:**
-  MacOS (High Sierra)
-  Python 3.7
-  Tensorflow 1.9.0

After I've imported tensorflow, I've got warning message link this
`RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7`

And I've tried to run demo.py from this repo https://github.com/mystic123/tensorflow-yolo-v3
still got error message
 `SystemError: <built-in function AppendFloat32ArrayToTensorProto> returned NULL without setting an error`


"
21471,N_class from DNN estimator Problem,"`assertion failed: [Label IDs must >= 0]`
I am getting this class again and again and i can not understand why.
Please tell me what is label and n_class?
i have 3 labels and they are asking me to make n_class greater than labels. When i do it, it does not do anything.
"
21470,NCCL is not supported on Windows ,"------------------------
### System information
Windows 10 x64 pro 17314.365, i7 7900X R6E, TTxp (*4), 16G DDR4 3000@2666 (*6) 
Channel .
Tensorflow 1.9.0 with AVX2 simd, CUDA9.2.148, Cudnn7.1.4, py3.6.6 , VS2017 15.7(with cmake), CP/SM6.1

### Describe the problem
Op type not registered 'NcclAllReduce' in binary running on Windows10.
With onedevicestrategy no problems. I noticed that NCCL only works in Linux.

### Source code / logs
tensorflow/contrib/distribute/python/examples/simple_tfkeras_example.py


> `
> from __future__ import absolute_import
> from __future__ import division
> from __future__ import print_function
> 
> import sys
> 
> import numpy as np
> import tensorflow as tf
> 
> 
> def input_fn():
>   x = np.random.random((1024, 10))
>   y = np.random.randint(2, size=(1024, 1))
>   x = tf.cast(x, tf.float32)
>   dataset = tf.data.Dataset.from_tensor_slices((x, y))
>   dataset = dataset.repeat(10)
>   dataset = dataset.batch(32)
>   return dataset
> 
> 
> def main(args):
> 
>   model_dir = './'
>   print('Using %s to store checkpoints.' % model_dir)
> 
>   # Define tf.keras Model.
>   model = tf.keras.Sequential()
>   model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(10,)))
>   model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
> 
>   # Compile tf.keras Model.
>   optimizer = tf.train.GradientDescentOptimizer(0.2)
>   model.compile(loss='binary_crossentropy', optimizer=optimizer)
>   model.summary()
>   tf.keras.backend.set_learning_phase(True)
> 
>   # Define a DistributionStrategy and convert the tf.keras Model to a
>   # tf.Estimator that utilizes the DistributionStrategy.
>   strategy = tf.contrib.distribute.MirroredStrategy()
>   config = tf.estimator.RunConfig(
>       train_distribute=strategy)
>   keras_estimator = tf.keras.estimator.model_to_estimator(
>       keras_model=model, config=config, model_dir=model_dir)
> 
>   # Train and evaluate the tf.Estimator.
>   keras_estimator.train(input_fn=input_fn, steps=10)
>   eval_result = keras_estimator.evaluate(input_fn=input_fn)
>   print('Eval result: {}'.format(eval_result))
> 
> if __name__ == '__main__':
>   tf.app.run(argv=sys.argv)`

Logs:


> D:\Anaconda3\envs\tensorflow\python.exe Z:/PyProj/leaf/othertest.py
> Using ./ to store checkpoints.
> _________________________________________________________________
> Layer (type)                 Output Shape              Param #   
> =================================================================
> dense (Dense)                (None, 16)                176       
> _________________________________________________________________
> dense_1 (Dense)              (None, 1)                 17        
> =================================================================
> Total params: 193
> Trainable params: 193
> Non-trainable params: 0
> _________________________________________________________________
> 2018-08-08 16:28:32.965695: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1392] Found device 0 with properties: 
> name: TITAN Xp COLLECTORS EDITION major: 6 minor: 1 memoryClockRate(GHz): 1.582
> pciBusID: 0000:17:00.0
> totalMemory: 12.00GiB freeMemory: 9.93GiB
> 2018-08-08 16:28:33.068708: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1392] Found device 1 with properties: 
> name: TITAN Xp COLLECTORS EDITION major: 6 minor: 1 memoryClockRate(GHz): 1.582
> pciBusID: 0000:18:00.0
> totalMemory: 12.00GiB freeMemory: 9.93GiB
> 2018-08-08 16:28:33.170068: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1392] Found device 2 with properties: 
> name: TITAN Xp COLLECTORS EDITION major: 6 minor: 1 memoryClockRate(GHz): 1.582
> pciBusID: 0000:65:00.0
> totalMemory: 12.00GiB freeMemory: 9.93GiB
> 2018-08-08 16:28:33.280326: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1392] Found device 3 with properties: 
> name: TITAN Xp COLLECTORS EDITION major: 6 minor: 1 memoryClockRate(GHz): 1.582
> pciBusID: 0000:b3:00.0
> totalMemory: 12.00GiB freeMemory: 9.93GiB
> 2018-08-08 16:28:33.281257: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1471] Adding visible gpu devices: 0, 1, 2, 3
> 2018-08-08 16:28:35.779554: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2018-08-08 16:28:35.779811: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:958]      0 1 2 3 
> 2018-08-08 16:28:35.779990: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   N N N N 
> 2018-08-08 16:28:35.780385: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 1:   N N N N 
> 2018-08-08 16:28:35.780639: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 2:   N N N N 
> 2018-08-08 16:28:35.783787: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 3:   N N N N 
> 2018-08-08 16:28:35.784232: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9612 MB memory) -> physical GPU (device: 0, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:17:00.0, compute capability: 6.1)
> 2018-08-08 16:28:36.220466: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9612 MB memory) -> physical GPU (device: 1, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:18:00.0, compute capability: 6.1)
> 2018-08-08 16:28:36.655982: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 9612 MB memory) -> physical GPU (device: 2, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:65:00.0, compute capability: 6.1)
> 2018-08-08 16:28:37.091894: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 9612 MB memory) -> physical GPU (device: 3, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:b3:00.0, compute capability: 6.1)
> 2018-08-08 16:28:37.529496: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1471] Adding visible gpu devices: 0, 1, 2, 3
> 2018-08-08 16:28:37.530168: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2018-08-08 16:28:37.530377: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:958]      0 1 2 3 
> 2018-08-08 16:28:37.530527: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   N N N N 
> 2018-08-08 16:28:37.530675: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 1:   N N N N 
> 2018-08-08 16:28:37.530822: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 2:   N N N N 
> 2018-08-08 16:28:37.530971: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 3:   N N N N 
> 2018-08-08 16:28:37.531268: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9612 MB memory) -> physical GPU (device: 0, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:17:00.0, compute capability: 6.1)
> 2018-08-08 16:28:37.531988: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9612 MB memory) -> physical GPU (device: 1, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:18:00.0, compute capability: 6.1)
> 2018-08-08 16:28:37.532651: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 9612 MB memory) -> physical GPU (device: 2, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:65:00.0, compute capability: 6.1)
> 2018-08-08 16:28:37.533329: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 9612 MB memory) -> physical GPU (device: 3, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:b3:00.0, compute capability: 6.1)
> 2018-08-08 16:28:37.570852: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1471] Adding visible gpu devices: 0, 1, 2, 3
> 2018-08-08 16:28:37.571420: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2018-08-08 16:28:37.571620: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:958]      0 1 2 3 
> 2018-08-08 16:28:37.571766: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   N N N N 
> 2018-08-08 16:28:37.571910: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 1:   N N N N 
> 2018-08-08 16:28:37.572056: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 2:   N N N N 
> 2018-08-08 16:28:37.572200: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 3:   N N N N 
> 2018-08-08 16:28:37.572479: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9612 MB memory) -> physical GPU (device: 0, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:17:00.0, compute capability: 6.1)
> 2018-08-08 16:28:37.573133: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9612 MB memory) -> physical GPU (device: 1, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:18:00.0, compute capability: 6.1)
> 2018-08-08 16:28:37.573824: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 9612 MB memory) -> physical GPU (device: 2, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:65:00.0, compute capability: 6.1)
> 2018-08-08 16:28:37.574554: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 9612 MB memory) -> physical GPU (device: 3, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:b3:00.0, compute capability: 6.1)
> 2018-08-08 16:28:37.902576: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1471] Adding visible gpu devices: 0, 1, 2, 3
> 2018-08-08 16:28:37.903108: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2018-08-08 16:28:37.903311: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:958]      0 1 2 3 
> 2018-08-08 16:28:37.903456: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   N N N N 
> 2018-08-08 16:28:37.903604: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 1:   N N N N 
> 2018-08-08 16:28:37.903751: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 2:   N N N N 
> 2018-08-08 16:28:37.903896: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 3:   N N N N 
> 2018-08-08 16:28:37.904189: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/device:GPU:0 with 9612 MB memory) -> physical GPU (device: 0, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:17:00.0, compute capability: 6.1)
> 2018-08-08 16:28:37.904886: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/device:GPU:1 with 9612 MB memory) -> physical GPU (device: 1, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:18:00.0, compute capability: 6.1)
> 2018-08-08 16:28:37.905868: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/device:GPU:2 with 9612 MB memory) -> physical GPU (device: 2, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:65:00.0, compute capability: 6.1)
> 2018-08-08 16:28:37.906462: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/device:GPU:3 with 9612 MB memory) -> physical GPU (device: 3, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:b3:00.0, compute capability: 6.1)
> Traceback (most recent call last):
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1589, in _create_c_op
>     c_op = c_api.TF_FinishOperation(op_desc)
> tensorflow.python.framework.errors_impl.InvalidArgumentError: Op type not registered 'NcclAllReduce' in binary running on DESKTOP-K1BEKCL. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. while building NodeDef 'NcclAllReduce'
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""Z:/PyProj/leaf/othertest.py"", line 66, in <module>
>     tf.app.run(argv=sys.argv)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
>     _sys.exit(main(argv))
>   File ""Z:/PyProj/leaf/othertest.py"", line 61, in main
>     keras_estimator.train(input_fn=input_fn, steps=10)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 366, in train
>     loss = self._train_model(input_fn, hooks, saving_listeners)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1117, in _train_model
>     return self._train_model_distributed(input_fn, hooks, saving_listeners)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1160, in _train_model_distributed
>     self.config)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\distribute.py"", line 794, in call_for_each_tower
>     return self._call_for_each_tower(fn, *args, **kwargs)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\distribute\python\mirrored_strategy.py"", line 269, in _call_for_each_tower
>     coord.join(threads)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\coordinator.py"", line 389, in join
>     six.reraise(*self._exc_info_to_raise)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\six.py"", line 693, in reraise
>     raise value
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\coordinator.py"", line 297, in stop_on_exception
>     yield
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\distribute\python\mirrored_strategy.py"", line 263, in _call_for_each_tower
>     self, *merge_args, **merge_kwargs)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\optimizer.py"", line 652, in _distributed_apply
>     reduced_grads = distribution.batch_reduce(""sum"", grads_and_vars)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\distribute.py"", line 840, in batch_reduce
>     return self._batch_reduce(method_string, value_destination_pairs)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\distribute\python\mirrored_strategy.py"", line 310, in _batch_reduce
>     value_destination_pairs)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\distribute\python\cross_tower_ops.py"", line 177, in batch_reduce
>     return self._batch_reduce(method_string, value_destination_pairs)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\distribute\python\cross_tower_ops.py"", line 475, in _batch_reduce
>     [v[0] for v in value_destination_pairs])
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\distribute\python\cross_tower_ops.py"", line 520, in _batch_all_reduce
>     device_grad_packs)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\distribute\python\cross_tower_utils.py"", line 37, in aggregate_gradients_using_nccl
>     agg_grads = nccl.all_sum(single_grads)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\nccl\python\ops\nccl_ops.py"", line 47, in all_sum
>     return _apply_all_reduce('sum', tensors)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\nccl\python\ops\nccl_ops.py"", line 228, in _apply_all_reduce
>     shared_name=shared_name))
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\nccl\ops\gen_nccl_ops.py"", line 58, in nccl_all_reduce
>     num_devices=num_devices, shared_name=shared_name, name=name)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
>     op_def=op_def)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 3414, in create_op
>     op_def=op_def)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1756, in __init__
>     control_input_ops)
>   File ""D:\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1592, in _create_c_op
>     raise ValueError(str(e))
> ValueError: Op type not registered 'NcclAllReduce' in binary running on DESKTO Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. while building NodeDef 'NcclAllReduce'

"
21469,ci.tensorflow.org is now deprecated.  Where can I get nightly builds for pi-zero AND regular pis when I only have a pi3?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Raspbian Stretch 9
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:Raspberry Pi 3 Model B
- **TensorFlow installed from (source or binary)**:latest pi-zero nightly binary that was on jenkins.
- **TensorFlow version (use command below)**:1.9.0 (ultimately not relevant so this should be good enough)
- **Python version**:3.5.3
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:N/A
- **GPU model and memory**:VideoCore IV
- **Exact command to reproduce**:
pip3 install tf-nightly

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Now, that ci.tensorflow.org is gone.  I'm struggling to find an easy way to view changelogs between nightly builds AND download both pi-zero and regular raspberry pi nightly builds.  The reason why this is useful is because recently #21198 armv7 optimizations interferred with the tflite interpreter.  The current most practical work around was to go to ci.tensorflow.org and grab the pi-zero binary instead of the default one you get when installing tf-nightly or tensorflow on the raspberry pi.  I can see how this wouldn't be a problem for other tensorflow versions that only have one binary that works for only that specific machine.  I understand that you can use the 2.7 tensorflow vs 3.n tensorflow on other machines, but those you can do `pip install tensorflow` or `pip3 install tensorflow`.  This isn't possible with the raspberry pi as far as I can tell, or at least it's not documented clearly anywhere that I can.  Furthermore, it was great being able to roll back a couple of days and just download the .whl from yesterday with ease.  I saved many hours by just playing around with the available .whl's instead of having to cross-compile each one.  
tl;dr I miss ci.tensorflow.org.  Please let me choose whether to install the tf nightly binaries of either pi-zero or regular raspberry-pi by choice since they both work on the raspberry pi.
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21468,typeerror,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21467,FP16 Batch Matmul still does not run,"**System information**

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Slightly.

OS Platform and Distribution : centos6.3

TensorFlow installed from: conda

TensorFlow version (use command below): v1.8.0(it already has the code TF_CALL_half(REGISTER_BATCH_MATMUL_GPU);)

Python version: 3.6

CUDA/cuDNN version: 9.2

GPU model and memory: V100 

Exact command to reproduce:

with tf.device(""/gpu:0""):
    a = tf.random_normal(dtype=tf.float16, shape=[5, 2, 3], name='a')
    b = tf.random_normal(dtype=tf.float16, shape=[5, 3, 2], name='b')
    c = tf.matmul(a, b)
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=False))
print(sess.run(c).shape)


the problem:

Source code / logs

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'MatMul_1': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Registered kernels:
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]

         [[Node: MatMul_1 = BatchMatMul[T=DT_HALF, adj_x=false, adj_y=false, _device=""/devic



"
21464,"StridedSlice (OpKernel was found, but attributes didn't match)","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Docker ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary - docker
- **TensorFlow version (use command below)**: nightly version as of August 6th 2018
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:   V9.0.176
- **GPU model and memory**: 4 * GTX 1080 Ti 
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I have written code to train a boosted trees estimator. The code gives an error even before it starts training if run on the GPU. On the other hand, it runs without any issues on the CPU. 

The issue seems to be with the StridedSlice op and an incompatible op. 

I have attached the log of the error below. 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/function.py:993: calling Graph.create_op (from tensorflow.python.framework.ops) with compute_shapes is deprecated and will be removed in a future version.
Instructions for updating:
Shapes are always computed; don't use the compute_shapes as it has no effect.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:Saving checkpoints for 0 into GBDT_multi/GPU/vLat-vLon/model.ckpt.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1291     try:
-> 1292       return fn(*args)
   1293     except errors.OpError as e:

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1276       return self._call_tf_sessionrun(
-> 1277           options, feed_dict, fetch_list, target_list, run_metadata)
   1278 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1366         self._session, options, feed_dict, fetch_list, target_list,
-> 1367         run_metadata)
   1368 

NotFoundError: No registered 'StridedSlice' OpKernel for GPU devices compatible with node {{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_BOOL, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](is_active, strided_slice/stack, strided_slice/stack_1, strided_slice/stack)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT32]
  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='CPU'; T in [DT_VARIANT]
  device='CPU'; T in [DT_RESOURCE]
  device='CPU'; T in [DT_STRING]
  device='CPU'; T in [DT_BOOL]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_UINT16]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_INT64]

	 [[{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_BOOL, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](is_active, strided_slice/stack, strided_slice/stack_1, strided_slice/stack)]]
	 [[{{node head/historyyawDiff5/dense_make_stats_update_pTOiIS60uTM}} = dense_make_stats_update_pTOiIS60uTM_specialized_for_head_currentspeed1_dense_make_stats_update_pTOiIS60uTM[_device=""/job:localhost/replica:0/task:0/device:GPU:0""](ConstantFolding/head/unstack_13-folded-0/_377, head/unstack_11/_379, gbdt/transform_features_13/history_yawDiff_5/ExpandDims, head/split:25, gbdt_1/GradientTreesPartitionExamples/_381, head/Gradients/head/mean_squared_error/SquaredDifference_grad/Reshape, head/stack, head/Sum)]]
	 [[{{node head/cond_1/cond/split/_1463}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_3016_head/cond_1/cond/split"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
<ipython-input-18-5992a8bb32bf> in <module>()
     13             }
     14 
---> 15 _ = train_GBDT_regressor()

<ipython-input-15-c864123abd09> in train_GBDT_regressor()
    128         learn_runner.run(experiment_fn=create_gbdt_experiment, 
    129                          output_dir=TRAIN_DATA['model_dir'],
--> 130                          schedule=""train_and_evaluate""
    131                         )      
    132 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    304               'in a future version' if date is None else ('after %s' % date),
    305               instructions)
--> 306       return func(*args, **kwargs)
    307     return tf_decorator.make_decorator(
    308         func, new_func, 'deprecated',

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py in run(experiment_fn, output_dir, schedule, run_config, hparams)
    223   schedule = schedule or _get_default_schedule(run_config)
    224 
--> 225   return _execute_schedule(experiment, schedule)
    226 
    227 

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py in _execute_schedule(experiment, schedule)
     50     logging.error('Allowed values for this experiment are: %s', valid_tasks)
     51     raise TypeError('Schedule references non-callable member %s' % schedule)
---> 52   return task()
     53 
     54 

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py in train_and_evaluate(self)
    670                   hooks=self._eval_hooks)
    671           ]
--> 672       self.train(delay_secs=0)
    673 
    674     # If the checkpoint_and_export flag and appropriate estimator configuration

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py in train(self, delay_secs)
    387         max_steps=self._train_steps,
    388         hooks=self._train_monitors + extra_hooks,
--> 389         saving_listeners=self._saving_listeners)
    390 
    391   def evaluate(self, delay_secs=None, name=None):

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py in _call_train(self, _sentinel, input_fn, steps, hooks, max_steps, saving_listeners)
    881           max_steps=max_steps,
    882           hooks=hooks,
--> 883           saving_listeners=saving_listeners)
    884     else:
    885       return self._estimator.fit(

/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    341 
    342       saving_listeners = _check_listeners_type(saving_listeners)
--> 343       loss = self._train_model(input_fn, hooks, saving_listeners)
    344       logging.info('Loss for final step: %s.', loss)
    345       return self

/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1127       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1128     else:
-> 1129       return self._train_model_default(input_fn, hooks, saving_listeners)
   1130 
   1131   def _train_model_default(self, input_fn, hooks, saving_listeners):

/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)
   1161       return self._train_with_estimator_spec(estimator_spec, worker_hooks,
   1162                                              hooks, global_step_tensor,
-> 1163                                              saving_listeners)
   1164 
   1165   def _train_model_distributed(self, input_fn, hooks, saving_listeners):

/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)
   1369       loss = None
   1370       while not mon_sess.should_stop():
-> 1371         _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
   1372     return loss
   1373 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)
    581                           feed_dict=feed_dict,
    582                           options=options,
--> 583                           run_metadata=run_metadata)
    584 
    585   def run_step_fn(self, step_fn):

/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)
   1057                               feed_dict=feed_dict,
   1058                               options=options,
-> 1059                               run_metadata=run_metadata)
   1060       except _PREEMPTION_ERRORS as e:
   1061         logging.info('An error was raised. This may be due to a preemption in '

/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)
   1148         raise six.reraise(*original_exc_info)
   1149       else:
-> 1150         raise six.reraise(*original_exc_info)
   1151 
   1152 

/usr/local/lib/python3.5/dist-packages/six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)
   1133   def run(self, *args, **kwargs):
   1134     try:
-> 1135       return self._sess.run(*args, **kwargs)
   1136     except _PREEMPTION_ERRORS:
   1137       raise

/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)
   1205                                   feed_dict=feed_dict,
   1206                                   options=options,
-> 1207                                   run_metadata=run_metadata)
   1208 
   1209     for hook in self._hooks:

/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)
    985 
    986   def run(self, *args, **kwargs):
--> 987     return self._sess.run(*args, **kwargs)
    988 
    989   def run_step_fn(self, step_fn, raw_session, run_with_hooks):

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    885     try:
    886       result = self._run(None, fetches, feed_dict, options_ptr,
--> 887                          run_metadata_ptr)
    888       if run_metadata:
    889         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1108     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1109       results = self._do_run(handle, final_targets, final_fetches,
-> 1110                              feed_dict_tensor, options, run_metadata)
   1111     else:
   1112       results = []

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1284     if handle is None:
   1285       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1286                            run_metadata)
   1287     else:
   1288       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1306           self._config.experimental.client_handles_error_formatting):
   1307         message = error_interpolation.interpolate(message, self._graph)
-> 1308       raise type(e)(node_def, op, message)
   1309 
   1310   def _extend_graph(self):

NotFoundError: No registered 'StridedSlice' OpKernel for GPU devices compatible with node {{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_BOOL, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](is_active, strided_slice/stack, strided_slice/stack_1, strided_slice/stack)
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; T in [DT_INT32]
  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='CPU'; T in [DT_VARIANT]
  device='CPU'; T in [DT_RESOURCE]
  device='CPU'; T in [DT_STRING]
  device='CPU'; T in [DT_BOOL]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_UINT16]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_INT64]

	 [[{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_BOOL, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](is_active, strided_slice/stack, strided_slice/stack_1, strided_slice/stack)]]
	 [[{{node head/historyyawDiff5/dense_make_stats_update_pTOiIS60uTM}} = dense_make_stats_update_pTOiIS60uTM_specialized_for_head_currentspeed1_dense_make_stats_update_pTOiIS60uTM[_device=""/job:localhost/replica:0/task:0/device:GPU:0""](ConstantFolding/head/unstack_13-folded-0/_377, head/unstack_11/_379, gbdt/transform_features_13/history_yawDiff_5/ExpandDims, head/split:25, gbdt_1/GradientTreesPartitionExamples/_381, head/Gradients/head/mean_squared_error/SquaredDifference_grad/Reshape, head/stack, head/Sum)]]
	 [[{{node head/cond_1/cond/split/_1463}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_3016_head/cond_1/cond/split"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```"
21463,Keras model with SparseTensor as inputs,"I'm trying to setup a Keras model with sparse input:

```
input_layer = tf.keras.layers.Input(shape=(10, ), sparse=True)
weights = tf.get_variable(name='weights', shape=(10, 1))

weights_mult = lambda x: tf.sparse_tensor_dense_matmul(x, weights)
output_layer=  tf.keras.layers.Lambda(weights_mult)(input_layer)
model = tf.keras.Model([input_layer], output_layer)
model.compile(loss='binary_crossentropy',
                  optimizer=tf.keras.optimizers.Adam(lr=0.0001),
                  metrics=['accuracy'])

```
This fails with 
```
TypeError                                 Traceback (most recent call last)
<ipython-input-3-822e68cb2d64> in <module>()
      7 model.compile(loss='binary_crossentropy',
      8                   optimizer=tf.keras.optimizers.Adam(lr=0.0001),
----> 9                   metrics=['accuracy'])

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)
    457       # Add regularization penalties
    458       # and other layer-specific losses.
--> 459       for loss_tensor in self.losses:
    460         total_loss += loss_tensor
    461

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in losses(self)
    635       else:
    636         relevant_inputs.append(inputs)
--> 637     reachable = tf_utils.get_reachable_from_inputs(relevant_inputs, losses)
    638     relevant_conditional_losses = [x for x in losses if x in reachable]
    639     unconditional_losses = [

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py in get_reachable_from_inputs(inputs, targets)
    115       outputs = [x.op]
    116     else:
--> 117       raise TypeError('Expected Operation, Variable, or Tensor, got ' + str(x))
    118
    119     for y in outputs:

TypeError: Expected Operation, Variable, or Tensor, got SparseTensor(indices=Tensor(""input_1/indices:0"", shape=(?, 2), dtype=int64), values=Tensor(""input_1/values:0"", shape=(?,), dtype=float32), dense_shape=Tensor(""input_1/shape:0"", shape=(2,), dtype=int64))
```

It seems to me that tensorflow/python/keras/utils/tf_utils.py:get_reachable_from_inputs doesn't recognize SparseTensor as a correct input type. Is there anyway to setup a Keras model with sparse input?"
21461,Bazel: use Tensorflow as external dependency,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  1.9.0
- **Python version**: 2.7.14
- **Bazel version (if compiling from source)**: 0.14.1
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

```
$ bazel run :hello_world
INFO: Build options have changed, discarding analysis cache.
INFO: Analysed target //:hello_world (29 packages loaded).
INFO: Found 1 target...
Target //:hello_world up-to-date:
  bazel-bin/hello_world
INFO: Elapsed time: 7.108s, Critical Path: 0.32s
INFO: 1 process, local.
INFO: Build completed successfully, 4 total actions
INFO: Build completed successfully, 4 total actions
Traceback (most recent call last):
  File ""/home/nmerino/.cache/bazel/_bazel_nmerino/daaf1ed794b21b41620d835ce6042e7a/execroot/__main__/bazel-out/k8-fastbuild/bin/hello_world.runfiles/__main__/hello_world.py"", line 1, in <module>
    import tensorflow as tf
ImportError: No module named tensorflow
```
### Describe the problem
Hey there! I'm trying to import Tensorflow in my current project, which use Bazel as its build tool. For some reason Bazel cannot find Tensorflow even though it is specified in the requirements file. As Bazel is the building tool for Tensorflow, and also gaining popularity in the community I would like if you could help me solving this issue.
_Just in case you are wondering I'm not trying to use Tensorflow to infer stuffs, rather to serialize data to TFRecords._

### Source code / logs

`hello_world.py`
```
import tensorflow as tf

msg = tf.constant('Hello World!')

with tf.Session() as sess:
	print sess.run(msg)
```
`WORKSPACE`
```
git_repository(
  name = ""io_bazel_rules_python"",
  remote = ""https://github.com/bazelbuild/rules_python.git"",
  commit = ""44711d8ef543f6232aec8445fb5adce9a04767f9"")

load(
    ""@io_bazel_rules_python//python:pip.bzl"",
    ""pip_repositories"",
    ""pip_import"")
pip_repositories()
pip_import(
    name = ""pip"",
    requirements = ""//:requirements.txt"")
load(
    ""@pip//:requirements.bzl"",
    pip_install = ""pip_install"")
pip_install()
```
`BUILD.bazel`
```
package(default_visibility = [""//visibility:public""])
load(
    ""@pip//:requirements.bzl"",
    ""requirement"")
load(
    ""@io_bazel_rules_python//python:python.bzl"",
    ""py_binary"",
    ""py_library"")

py_binary(
    name = ""hello_world"",
    srcs = [
        ""hello_world.py""
    ],
    deps = [
        requirement(""tensorflow""),
    ]
)
```
`requirements.txt`
```
tensorflow==1.9.0
```"
21460,Keras load weights does not work,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: Quardo M4000
- **Exact command to reproduce**: 

### Describe the problem
I try a simple Keras model. After saving the model weights, I tried to reload it. However, the ""load_weights"" function does not really load the weights, but still use the initializer's setting in the model.

### Source code / logs
	def test_load():
	  data = np.ones((1,1,4,5), dtype=float)
	  
	  model = keras.Sequential()
	  model.add(Conv2D(5, 1, use_bias=False, kernel_initializer='ones', trainable=False))

	  results = model.predict(data)
	  print(results)

	  for layer in model.layers:
	    weights = layer.get_weights()
	    layer.set_weights(np.zeros_like(weights))

	  results = model.predict(data)
	  print(results)

	  model.save_weights('./temp')

	  new_model = keras.Sequential()
	  new_model.add(Conv2D(5, 1, use_bias=False, kernel_initializer='uniform', trainable=False))
	  
	  new_model.load_weights('./temp')

	  results = new_model.predict(data)
	  print(results)

Before saving the model, everything works as expected. However, after loading, the loaded weights are always random according to the initializer. I wonder whether there is some bugs in the codes or I used it in a wrong way. Thank you.
"
21459,Changing optimizer of restored network messes up training output,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
 Ubuntu 16.04.2 LTS (GNU/Linux 4.4.0-130-generic x86_64)

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A

- **TensorFlow installed from (source or binary)**:
binary

- **TensorFlow version (use command below)**:
v1.4.0-rc1-11-g130a514 1.4.0

- **Python version**:
Python 3.5.2

- **Bazel version (if compiling from source)**:
N/A

- **GCC/Compiler version (if compiling from source)**:
N/A

- **CUDA/cuDNN version**:
Cuda compilation tools, release 8.0, V8.0.61

- **GPU model and memory**:
GeForce GTX 1080 Ti (11GB)

- **Exact command to reproduce**:


### Describe the problem
I am trying to train a model to do face detection, using Resnet_v1_50 as a backbone net and restoring it from the checkpoint provided in tf-slim. I am following the paper ""MB-FCN for Face Detection"" in building my network. Initially trained with gradient descent, the model was converging slowly. When the optimizer was changed from Gradient Descent (which ResNet was trained with) to a different optimizer (momentum, adam) there were wildly different outputs, that were nowhere near correct even though the training and validation losses were lower and care was taken to initialize all weights for new variables (from momentum for example). There seems to perhaps be an issue with the tf Saver class (restoring variables goes wrong) or an issue changing the optimizer of a restored network from the one it was originally trained with. I also had this issue when using a different model for text detection (changing optimizer messing with output). Restoring the weights from the fine-tuned model with another optimizer goes wrong.

### Source code / logs
Code while training from ResNet:
    global_step = tf.train.get_or_create_global_step()
    with slim.arg_scope(resnet_v1.resnet_arg_scope()):
        vars_to_restore = slim.get_variables_to_restore(['resnet_v1_50'])

    optimizer = tf.train.GradientDescentOptimizer(0.001)
    optimizer = optimizer.minimize(loss, global_step=global_step) 

    config = tf.ConfigProto(allow_soft_placement=True)
    config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1
    overall_saver = tf.train.Saver(slim.get_variables_to_restore())

    with tf.Session(config=config) as sess:
        resnet_model_path = 'backbone_net/resnet_v1_50.ckpt'
        vars_to_initialize = []
        all_vars = slim.get_variables_to_restore()
        restored_vars = set(vars_to_restore)
        for var in all_vars:
            if var not in restored_vars:
                vars_to_initialize.append(var)

        restorer = tf.train.Saver(vars_to_restore)
        restorer.restore(sess, resnet_model_path)
        sess.run(tf.variables_initializer(vars_to_initialize))

Line to save weights while training:
        overall_saver.save(sess, 'weights/model.ckpt', global_step=global_step)

Code to restore weights once fine-tuned:
    iterator, init_op = create_dataset('test_imgs.csv', is_training=False)
    b1_cls, b2_cls, b1_reg, b2_reg, ratio_h, ratio_w = mbfcn_model(iterator, is_training=False)

    config = tf.ConfigProto(allow_soft_placement=True)
    config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1

    with tf.Session(config=config) as sess:
        chkpt = 'weights/'
        restorer = tf.train.Saver(slim.get_variables_to_restore())
        restorer.restore(sess, tf.train.latest_checkpoint(chkpt))
"
21458,speech recognition tutorial bug,"Have I written custom code no
OS Platform and Distribution N/A
TensorFlow installed from N/A
TensorFlow version N/A
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce N/A
Mobile device N/A

I'm doing the Speech Commands tutorial and looking at the low_latency_conv architecture here:
https://github.com/tensorflow/tensorflow/blob/23c218785eac5bfe737eec4f8081fd0ef8e0684d/tensorflow/examples/speech_commands/models.py#L283-L302

The architecture is supposed to be cnn-one-fstride4 from this paper: https://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf

The paper says ""The baseline DNN architecture consists of 3 hidden layers with 128 hidden units/layer and a softmax layer. Each hidden layer uses a rectified linear unit (ReLU) nonlinearity.""

However the network generated by the code only contains one ReLU. Subsequently, it just has three stacked MatMul operations with bias, and no interleaved nonlinearities. This is really equivalent to just one MatMul with bias, since you can compose such stacked operations.

Why are the additional ReLUs not included in this network?"
21455,multi-hot representation and sequence_categorical_column_with_identity,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.6.5
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **mobile device**: N/A
- **Exact command to reproduce**: See code below

### Describe the problem
`sequence_feature_columns` are great, but they are not compatible with the multi-hot representation of a categorical column. I want to multi-hot represent a feature of shape `[batch, time, dim]` with `indicator_column` and `sequence_categorical_column_with_identity`, but when using the `sequence_input_layer` it produces a `TypeError.` Are you familiar with this problem and do you have a solution for this or am I missing something? Thank you very much for any help!

### Source code / logs
A simple example that produces the error:
```python
x = tf.placeholder(tf.int32, [None, None, 2])
column = tf.feature_column.indicator_column(tf.contrib.feature_column.sequence_categorical_column_with_identity('x', 3))
input_layer, sequence_length = tf.contrib.feature_column.sequence_input_layer({'x': x}, [column])
```
```python
TypeError: sequence item 2: expected str instance, Tensor found
```"
21454,Turning off Teacher Forcing in decoders of Seq2Seq models ,"Have I written custom code:N/A
OS Platform and Distribution :macOS Sierra 10.12.6
TensorFlow installed from : tensforflow.org
TensorFlow version: TensorFlow Version: 1.3.0
Bazel version: NA
CUDA/cuDNN version:NA
GPU model and memory:NA
Exact command to reproduce:NA
Mobile device:NA


Hello , 

I'm posting this request here however I'm not sure if this is a new feature request or it is something already doable in tensorflow but I can not find find an example or any documentation about it  . 

Geting to the Point . I'm experimenting with  seq2seq models . I have followed all the examples available and all is good. Now my model uses Teacher forcing ( passing the true output to the decoder network during training ) and nI  would like to turn it off to see how the model performs without it . unfortunately I do not see any possible way to do it without making my own Helper . I tried to use the inferenceHelper during the Training instead of the TraningHelper. Since the inferenceHelper does not require the true output but the model gives a run time error after some epochs where inferenceHelper is returning predictions with shape other than the expected. my guess is that I inferenceHelper is not meant to be used during training ( as the name suggests) . 

is there a way to turn off the Teacher Forcing , if not are there any plans to incorporate this in Tensorflow ? 

my original decoder network code : 

```
`def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, 
                     target_sequence_length, max_summary_length, 
                     output_layer, keep_prob):
""""""
Create a decoding layer for training
:param encoder_state: Encoder State
:param dec_cell: Decoder RNN Cell
:param dec_embed_input: Decoder embedded input
:param target_sequence_length: The lengths of each sequence in the target batch
:param max_summary_length: The length of the longest sequence in the batch
:param output_layer: Function to apply the output layer
:param keep_prob: Dropout keep probability
:return: BasicDecoderOutput containing training logits and sample_id
""""""

training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,
                                                    sequence_length=target_sequence_length,
                                                    time_major=False)

training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper, encoder_state, output_layer)

training_decoder_output = tf.contrib.seq2seq.dynamic_decode(training_decoder,
                                                            impute_finished=True,
                                                            maximum_iterations=max_summary_length)[0]
return training_decoder_output
````



thank you "
21449,Traceback (most recent call last):,"While running this code on CYGWIN 

`$ python -m scripts.label_image \
>   --graph=tf_files/retrained_graph.pb  \
>   --image=tf_files/flower_photos/daisy/3475870145_685a19116d.jpg`

this error showed up

` Traceback (most recent call last):
 File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/home/Admin/tensorflow-for-poets-2/scripts/label_image.py"", line 24, in <module>
    import numpy as np
ImportError: No module named numpy
`"
21447,Camera is showing as half screen while running the application (Android),Android - Oneplus 3
21446,the error of freeze model,"- OS Platform:  Ubuntu 16.04
- TensorFlow installed from:  anaconda
- TensorFlow version: GPU-1.8
- Python version: 3.6.6
- CUDA/cuDNN version: 9.0 / 7.1.2
- GPU model and memory:16G

The model is mobilenetV2 + LSTM + attention.
MobilenetV2 is restored from the model zoo in [https://github.com/tensorflow/models/tree/master/research/slim](https://github.com/tensorflow/models/tree/master/research/slim). It is not trained during the training of the whole model.
LSTM and attention is trained.

The code about model is shown as following.
```
    def mobilenet_v2_rnn_attention(self, X, num_classes, dropout_keep_prob=0.8, is_train=False):
        rnn_size = 4096
        num_layers = 8
        attention_size = 1024
        arg_scope = training_scope()
        with slim.arg_scope(arg_scope):
            net_vis, end_points = mobilenet_base(X, num_classes=num_classes)
        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding='SAME'):
            with tf.variable_scope('Logits_out'):
                orig_shape = net_vis.get_shape().as_list()
                net = tf.reshape(net_vis, [-1, orig_shape[1] * orig_shape[2], orig_shape[3]])

                def lstm_cell():
                    return tf.contrib.rnn.LSTMCell(rnn_size)

                stack = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(0, num_layers)], state_is_tuple=True)
                net, _ = tf.nn.dynamic_rnn(stack, net, dtype=tf.float32)
                # (B, T, D) => (T, B, D)
                net = tf.transpose(net, (1, 0, 2))
                net = attention(net, attention_size, True)
                net = slim.fully_connected(net, num_classes, activation_fn=None, scope='Logits_out1')
        return net, net_vis
```




At the first, I export the graph as following.
```
    X = tf.placeholder(tf.float32, [None, height, width, 3], name=""inputs_placeholder"")
    net, net_vis = build_model(X, num_classes, 1.0, False, arch_model)
    net = tf.nn.sigmoid(net)
    predict = tf.reshape(net, [-1, num_classes], name='predictions')

    graph_def = graph.as_graph_def()

    with gfile.GFile(FLAGS.output_file, 'wb') as f:
      f.write(graph_def.SerializeToString())
```

Then, freeze the model using the api

```
python /root/anaconda3/pkgs/tensorflow-base-1.8.0-py36hc1a7637_0/lib/python3.6/site- 
    packages/tensorflow/python/tools/freeze_graph.py \
      --input_graph=${INFERENCE_GRAPH_PATH}/${TMP_GRAPH} \
      --input_checkpoint=${CHECKPOINT_PATH}/model.ckpt-3800 \
      --input_binary=true \
      --output_graph=${CHECKPOINT_PATH}/${EXPORT_INF_GRAPH} \
      --output_node_names=${OUTPUT_NODE_NAME}
```

The error is shown as following.

`[libprotobuf FATAL google/protobuf/wire_format.cc:830] CHECK failed: (output->ByteCount()) == (expected_endpoint): : Protocol message serialized to a size different from what was originally expected.  Perhaps it was modified by another thread during serialization?`"
21444,Estimator API -- Incorrect Super-Type Check for Hooks,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04.4 LTS (Xenial Xerus)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: From source
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**: 7.1.4.18
- **GPU model and memory**: Quadro K620 and Tesla K40c -- 2GB and 11.5GB respectively.
- **Exact command to reproduce**:
```
def model_fn(features, labels, mode):
    
    loss = tfsi_model(features)
    if mode == tf.estimator.ModeKeys.TRAIN:
        train_op, grads, saver = minimize(loss)
        writer, merged = prepare_summary(tf.get_default_graph(), loss, grads)
        chkpt_hook = tf.train.CheckpointSaverHook(
            FLAGS.model_dir, 
            save_steps=FLAGS.saving_ckpt_freq, 
            saver=saver
        )
        summ_hook = tf.train.SummarySaverHook(
            save_steps=FLAGS.saving_summ_freq,
            output_dir=FLAGS.log_directory, 
            summary_writer=writer,
            summary_op=merged
        )
        hooks = [chkpt_hook, summ_hook]
        return tf.estimator.EstimatorSpec(mode, 
                                          loss=loss, 
                                          train_op=train_op, 
                                          training_chief_hooks=hooks)
    else:
        return tf.estimator.EstimatorSpec(mode, loss=loss)
```

Hello Tensorflow Devs,
I'm sorry to report too many failures within Estimator API just in couple of days; but it seems it is really buggy, guys. When it checks out for the (super)type of session hooks, it looks for **tensorflow.python.training.session_run_hook.SessionRunHook**; but ""isinstance"" method fails to do so for it assumes that their type is **tensorflow.python.training.basic_session_run_hooks** though it is not. Would you mind replacing it with a proper validation method, please?

```

INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_keep_checkpoint_max': 3, '_save_checkpoints_steps': 15, '_num_ps_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_num_worker_replicas': 1, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f8d98aaf780>, '_tf_random_seed': None, '_task_id': 0, '_device_fn': None, '_model_dir': './output', '_save_summary_steps': 5, '_session_config': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8d98aafcf8>, '_log_step_count_steps': 100, '_service': None, '_global_id_in_cluster': 0, '_is_chief': True, '_task_type': 'worker', '_master': ''}
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.
INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0
INFO:tensorflow:Configured nccl all-reduce.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:batch_all_reduce invoked for batches size = 66 with algorithm = nccl and num_packs = 1
Preparing Tensor Summaries...
Adding summary for training loss
INFO:tensorflow:Create CheckpointSaverHook.
Type of hook:  <class 'tensorflow.python.training.basic_session_run_hooks.SummarySaverHook'>
INFO:tensorflow:Done calling model_fn.
Preparing Tensor Summaries...
Adding summary for training loss
INFO:tensorflow:Create CheckpointSaverHook.
Type of hook:  <class 'tensorflow.python.training.basic_session_run_hooks.SummarySaverHook'>
INFO:tensorflow:Done calling model_fn.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-10-cc8d515a6342> in <module>()
     12 train_spec = tf.estimator.TrainSpec(gen_input_fn('train', FLAGS.num_epochs))
     13 valid_spec = tf.estimator.EvalSpec(gen_input_fn('valid', 1))
---> 14 tf.estimator.train_and_evaluate(classifier, train_spec, valid_spec)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)
    445         '(with task id 0).  Given task id {}'.format(config.task_id))
    446 
--> 447   return executor.run()
    448 
    449 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py in run(self)
    529         config.task_type != run_config_lib.TaskType.EVALUATOR):
    530       logging.info('Running training and evaluation locally (non-distributed).')
--> 531       return self.run_local()
    532 
    533     # Distributed case.

/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py in run_local(self)
    667           input_fn=self._train_spec.input_fn,
    668           max_steps=self._train_spec.max_steps,
--> 669           hooks=train_hooks)
    670 
    671       if not self._continuous_eval_listener.before_eval():

/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    364 
    365       saving_listeners = _check_listeners_type(saving_listeners)
--> 366       loss = self._train_model(input_fn, hooks, saving_listeners)
    367       logging.info('Loss for final step: %s.', loss)
    368       return self

/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1115   def _train_model(self, input_fn, hooks, saving_listeners):
   1116     if self._distribution:
-> 1117       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1118     else:
   1119       return self._train_model_default(input_fn, hooks, saving_listeners)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)
   1248             training_hooks=training_hooks,
   1249             training_chief_hooks=training_chief_hooks,
-> 1250             scaffold=scaffold)
   1251         return self._train_with_estimator_spec(estimator_spec, worker_hooks,
   1252                                                hooks, global_step_read_tensor,

/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/model_fn.py in __new__(cls, mode, predictions, loss, train_op, eval_metric_ops, export_outputs, training_chief_hooks, training_hooks, scaffold, evaluation_hooks, prediction_hooks)
    304         raise TypeError(
    305             'All hooks must be SessionRunHook instances, given: {}'.format(
--> 306                 hook))
    307 
    308     scaffold = scaffold or monitored_session.Scaffold()

TypeError: All hooks must be SessionRunHook instances, given: PerDevice:{'/job:localhost/replica:0/task:0/device:GPU:1': <tensorflow.python.training.basic_session_run_hooks.CheckpointSaverHook object at 0x7f8ce4173ac8>, '/job:localhost/replica:0/task:0/device:GPU:0': <tensorflow.python.training.basic_session_run_hooks.CheckpointSaverHook object at 0x7f8d140390b8>}
```"
21442,tensorflow.python.framework.errors_impl.InternalError: Could not allocate ndarray,"
HI,
I am using running Places pre-trained VGG16 network in python  using CPU. I got this error? How to solve this memory issue after several computations? Thanks
C:/Experiment/PicAlertData/public/4326658334.jpg
Traceback (most recent call last):
  File ""C:\Python3.5\lib\site-packages\tensorflow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\Python3.5\lib\site-packages\tensorflow\python\client\session.py"", line 1307, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Python3.5\lib\site-packages\tensorflow\python\client\session.py"", line 1409, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: Could not allocate ndarray

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Chiru\places_trained_vgg16.py"", line 274, in <module>
    model = VGG16_Places365(weights='places')
  File ""C:\Users\Chiru\places_trained_vgg16.py"", line 221, in VGG16_Places365
    model.load_weights(weights_path)
  File ""C:\Python3.5\lib\site-packages\keras\engine\network.py"", line 1161, in load_weights
    f, self.layers, reshape=reshape)
  File ""C:\Python3.5\lib\site-packages\keras\engine\saving.py"", line 928, in load_weights_from_hdf5_group
    K.batch_set_value(weight_value_tuples)
  File ""C:\Python3.5\lib\site-packages\keras\backend\tensorflow_backend.py"", line 2440, in batch_set_value
    get_session().run(assign_ops, feed_dict=feed_dict)
  File ""C:\Python3.5\lib\site-packages\tensorflow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\Python3.5\lib\site-packages\tensorflow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Python3.5\lib\site-packages\tensorflow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\Python3.5\lib\site-packages\tensorflow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Could not allocate ndarray"
21441,building error on mac,"ERROR: /private/var/tmp/_bazel_chenxu/eac512cc1715441e7965427cfb136e20/external/protobuf_archive/BUILD:665:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:172:13: error: _assigning to 'char *' from incompatible type 'const char *'_
        if (PyString_AsStringAndSize(key, &name, &name_size) < 0) {
            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:22: note: expanded from macro 'PyString_AsStringAndSize'
       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \
                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:189:13: error: _assigning to 'char *' from incompatible type 'const char *'_
        if (PyString_AsStringAndSize(key, &camelcase_name, &name_size) < 0) {
            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:22: note: expanded from macro 'PyString_AsStringAndSize'
       ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \
                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

------------------------

### System information

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
MACOS high sierra
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
lastest
- **Python version**:
3.7
- **Bazel version (if compiling from source)**:
 bazel version
homebrew build
Build label: 0.15.2-homebrew
- **GCC/Compiler version (if compiling from source)**:
mac default gcc( Apple LLVM version 9.1.0 (clang-902.0.39.2))


"
21440,Feature request : Allow variable (None) input_shape for Tflite toco,"Hi,

- Have I written custom code: **NO**
- OS Platform and Distribution: **Ubuntu 18.0.4 LTS**
- TensorFlow installed from: **Github 1.9 release branch**
- TensorFlow version: **1.9**
- Bazel version: **0.16.0**
- CUDA/cuDNN version: **9.1 / 7.0**
- GPU model and memory: **GTX 1070 Ti, 8 Go**
- Exact command to reproduce: **see below**

I'm working on a FCN (Fully Convolutional Network https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) and wanted to make it work on mobile. The advantage of this type of network is that we can use any dimensions as input, the network still work. (for example, an image 512x512 or 512x384 or 612x358...)

TfLite allow us to convert a freezed graph (.pb) to Tflite format (.tflite), but we need to put a fixed size for ""input_shape"".

 bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=frozen.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=/tmp/test.tflite --inference_input_type=FLOAT --input_arrays=input_image --output_arrays=output_image --input_shapes=""1,**384,512**,3""

And this what I would like:

 bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=frozen.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=/tmp/test.tflite --inference_input_type=FLOAT --input_arrays=input_image --output_arrays=output_image --input_shapes=""1,**None,None**,3""

I'm wondering if anyone is working on this feature, because I think it can be very useful for a lot of people.

Thank you for your time, and sorry if I duplicate any thread, I didn't find this feature request."
21439,Reduce_sum error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64 bits
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.5
- **CUDA/cuDNN version**: 9.0 / v7
- **GPU model and memory**: Nvidia GTX TITANX 12GB, driver up to date (398.82)
- **Bazel version**: N/A
- **Mobile device**: N/A
- **Exact command to reproduce**:

### Describe the problem
Hi,

See test case below. On this specific setting it happens that tf.reduce_sum returns wrong values, as attested by the loss which should always return 0, but does not in this case (see logs at the end).

The bug does not happen when running on CPU, or when batchSize is strictly lesser than 33, or when ""models"" tensor has only 3 dimensions (even if we increase its size), or when ""models"" is smaller, or when the reduce_sum is applied on all axes instead of the last axis.

The reduce_sum is also correct for the first 32 elements of its first dimension, and only goes wrong for the 33rd (and next) element(s). 

If we replace the ""diff = inputTensor - inputTensor"" tensor by a tf.zeros tensor, the bug does not appear.

The bug also happens with other reducing tensorflow methods (such as tf.reduce_min, tf.reduce_max and tf.reduce_mean).

Thanks in advance

### Source code / logs
```
import tensorflow as tf
import numpy as np
from random import random

batchSize = 33

np.random.seed(0)
models = np.random.rand(batchSize, 2050, 2050, 2)
models = np.asarray(models, dtype = ""float32"")

dataPlaceholder = tf.placeholder(""float32"", models.shape)
inputTensor = dataPlaceholder

diff = inputTensor - inputTensor
# diff = tf.zeros(inputTensor.shape)

sum = tf.reduce_sum(diff, axis = 3)

loss = tf.reduce_max(sum)
loss = tf.Print(loss, [diff[batchSize-1,0,0], sum[batchSize-1,0,0]], message=""diff[batchSize-1,0,0], reduce_sum[batchSize-1,0,0] : "")

if __name__ == '__main__':
	with tf.Session() as sess:
		batchLoss = sess.run(loss, feed_dict = {dataPlaceholder: models[:batchSize]})
		print(""batch loss : {}"".format(batchLoss))

---------------------------------------------------
batch loss : 0.999999463558197
diff[batchSize-1,0,0], reduce_sum[batchSize-1,0,0] : [0 0][0.154408231]
```"
21438,Resnet50.py apply pooling at last layer regardless pooling parameter.,"### Describe the problem
Resnet50.py apply pooling at last layer regardless pooling parameter.
The issue has been fixed in keras 2.2.0
### Source code / logs
Current:
```python  
  x = AveragePooling2D((7, 7), name='avg_pool')(x)
  if include_top:
    x = Flatten()(x)
    x = Dense(classes, activation='softmax', name='fc1000')(x)
  else:
    if pooling == 'avg':
      x = GlobalAveragePooling2D()(x)
    elif pooling == 'max':
      x = GlobalMaxPooling2D()(x)
```
Fix:
```python
  if include_top:
    x = AveragePooling2D((7, 7), name='avg_pool')(x)
    x = Flatten()(x)
    x = Dense(classes, activation='softmax', name='fc1000')(x)
  else:
    if pooling == 'avg':
      x = GlobalAveragePooling2D()(x)
    elif pooling == 'max':
      x = GlobalMaxPooling2D()(x)
```"
21437,Resnet50 applying last pooling layer regardless pooling parameter,"### Describe the problem
Resnet50.py apply pooling at last layer regardless pooling parameter.

### Source code / logs
Current:
  x = AveragePooling2D((7, 7), name='avg_pool')(x)
  if include_top:
    x = Flatten()(x)
    x = Dense(classes, activation='softmax', name='fc1000')(x)
  else:
    if pooling == 'avg':
      x = GlobalAveragePooling2D()(x)
    elif pooling == 'max':
      x = GlobalMaxPooling2D()(x)
Fix:
  if include_top:
    x = AveragePooling2D((7, 7), name='avg_pool')(x)
    x = Flatten()(x)
    x = Dense(classes, activation='softmax', name='fc1000')(x)
  else:
    if pooling == 'avg':
      x = GlobalAveragePooling2D()(x)
    elif pooling == 'max':
      x = GlobalMaxPooling2D()(x)"
21435,Docs error: sparse_softmax_cross_entropy_with_logits,"There is an error in documentation of sparse_softmax_cross_entropy_with_logits:
https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits

there is:
""logits and labels must have the same dtype (either float16, float32, or float64).""

should be:
""logits should have dtype float16, float32, or float64,
 labels should have dtype int32 or int64""

(in Args section of this docs it is written correctly)"
21434,Broken link to the API stability page ,"The link mentioned in the API stability warning is broken: https://www.tensorflow.org/install/install_go
It should be changed in this file as well: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/install/install_go.md"
21433,Shape mismatch after sampling,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary (pip install tensorflow-gpu)
- **TensorFlow version (use command below)**: 1.9 ('v1.9.0-0-g25c197e023', '1.9.0')
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9 / 7
- **GPU model and memory**:  Nvidia Titan Xp (12 Gb)
- **Exact command to reproduce**: see below


### Describe the problem
Since I upgraded from version 1.8 to 1.9, there seems to be an issue with the shape of tensors sampled from `tf.contrib.distributions.MultivariateNormalDiag.sample()` using the sample_shape argument: While `get_shape()` indicates that the output tensor is shaped as [sample_shape, dim] (where dim is the dimensionality of the distribution), using this tensor in `tf.multiply()` results in an output with transposed shape, whereas `tf.add()` throws an 'Incompatible shapes' error.

The error seems to indicate that a transpose operation is being used on the sampled tensor:
```
InvalidArgumentError (see above for traceback): Incompatible shapes: [100,5] vs. [5,100]
	 [[Node: add_4 = Add[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](ones_12, MultivariateNormalDiag_4/sample/affine_linear_operator/forward/DistributionShape_1/undo_make_batch_of_event_sample_matrices/rotate_transpose/transpose)]]
	 [[Node: add_4/_3 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_14_add_4"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```

On version 1.9, the code below outputs
```
sampled shape  (100, 5)
normal multiply  (100, 5)
multiply sampling output  (5, 100)
``` 
 If I downgrade to 1.8 again, I get the expected
```
sampled shape  (100, 5)
normal multiply  (100, 5)
multiply sampling output  (100, 5)
```

### Source code / logs
```
import tensorflow as tf

a = tf.ones([100, 5])
b = tf.ones([100, 5])
sam = tf.contrib.distributions.MultivariateNormalDiag(
                loc=[0., 0., 0., 0., 0.],
                scale_diag=[1., 1., 1., 1., 1.])
c = sam.sample(sample_shape=[100])

mul1 = tf.multiply(a, b)
mul2 = tf.multiply(a, c)
add = a + c


sess = tf.Session()
init = tf.initialize_all_variables()
sess.run(init)
val_c = c.eval(session=sess)
val_1 = mul1.eval(session=sess)
val_2 = mul2.eval(session=sess)
# this throws an error:
# val_3 = add.eval(session=sess)

print 'sampled shape ', val_c.shape
print 'normal multiply ', val_1.shape
print 'multiply sampling output ', val_2.shape
```

"
21432,"BUG: Initial values are still random, even both the graph-level and the operation seed are set","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu Server & Window 10 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 & v1.9.0-0-g25c197e023
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: V9.0.178
- **GPU model and memory**: NVIDIA Tesla V100-SXM2-16GB & NVIDIA GeForce GTX 1080 Ti 11GB
- **Exact command to reproduce**:

### Source code / logs
```
import tensorflow as tf

seed = 1
tf.set_random_seed(seed)

initializer1 = tf.orthogonal_initializer(gain=1.0, seed=seed, dtype=tf.float32)
Conv1W = tf.identity(tf.Variable(initializer1.__call__(shape=[3,3,1,64])),name='Conv1')

initializer2 = tf.orthogonal_initializer(gain=1.0, seed=seed, dtype=tf.float32)
Conv2W = tf.identity(tf.Variable(initializer2.__call__(shape=[3,3,64,64])),name='Conv2')

initializer3 = tf.orthogonal_initializer(gain=1.0, seed=seed, dtype=tf.float32)
fc1W = tf.identity(tf.Variable(initializer3.__call__(shape=[39,14,64,372])),name='fc1')

initializer4 = tf.orthogonal_initializer(gain=1.0, seed=seed, dtype=tf.float32)
fc2W = tf.identity(tf.Variable(initializer4.__call__(shape=[1,1,372,372])),name='fc2')

initializer5 = tf.orthogonal_initializer(gain=1.0, seed=seed, dtype=tf.float32)
fc3W = tf.identity(tf.Variable(initializer5.__call__(shape=[1,1,372,1])),name='fc3')

print(""Session 1"")
with tf.Session() as sess1:
    tf.global_variables_initializer().run()
    Conv1W_eval1 = sess1.run(Conv1W)
    Conv2W_eval1 = sess1.run(Conv2W)
    fc1W_eval1 = sess1.run(fc1W)
    fc2W_eval1 = sess1.run(fc2W)
    fc3W_eval1 = sess1.run(fc3W)

print(""Session 2"")
with tf.Session() as sess2:
    tf.global_variables_initializer().run()
    Conv1W_eval2 = sess2.run(Conv1W)
    Conv2W_eval2 = sess2.run(Conv2W)
    fc1W_eval2 = sess2.run(fc1W)
    fc2W_eval2 = sess2.run(fc2W)
    fc3W_eval2 = sess2.run(fc3W)
print('--------------------------------------------------')
print('Conv1W? %d' % all(Conv1W_eval1==Conv1W_eval2))
print('Conv2W? %d' % all(Conv2W_eval1==Conv2W_eval2))
print('fc1W? %d' % all(fc1W_eval1==fc1W_eval2))
print('fc2W? %d' % all(fc2W_eval1==fc2W_eval2))
print('fc3W? %d' % all(fc3W_eval1==fc3W_eval2))

```

### Describe the problem

By executing the above code (one or two times), we can see that the initial value of the tensors, fc1W & fc2W are NOT always the same in both session, even though the graph-level and the operation seed are set.

The randomness seems to be depended on the shape of the tensors. For example, the initial value of the tensors, Conv1W and Conv2W are always the same.

Putting all tf.Variable into/under each session has the same effect."
21431,"I get error not found symbol variable ""Fill"" in Zeros class","Hello guys i get error in Zeros class for not found symbol variable ""Fill"" in android studio and in Build panel showing this error and not running sample app from tensorflow to my android devices

error: cannot find symbol variable Fill where T is a type-variable: T extends Object declared in class Zeros"
21429,TocoConvertor: converting keras models to tflite doesn't support custom objects,"Have I written custom code: No
OS Platform and Distribution: Mac 
TensorFlow installed from: pip
TensorFlow version: 1.10-rc1
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: `tensorflow.contrib.lite.TocoConverter.from_keras_model_file(model_file, custom_objects=CUSTOM_OBJECTS)`
Mobile device: N/A

-----

This is required to convert models that use custom layers or loss functions

this is the fix (will submit a PR soon):
```
From 2c2179765cc9006762cf75c6a1b587e06895b869 Mon Sep 17 00:00:00 2001
From: Ophir Yoktan <ophir@ziprecruiter.com>
Date: Wed, 1 Aug 2018 10:16:10 +0300
Subject: [PATCH] add support for custom_objects when loading keras model for
 conversion

---
 tensorflow/contrib/lite/python/lite.py | 5 +++--
 1 file changed, 3 insertions(+), 2 deletions(-)

diff --git a/tensorflow/contrib/lite/python/lite.py b/tensorflow/contrib/lite/python/lite.py
index 2f9b9d469a2..c12617df061 100644
--- a/tensorflow/contrib/lite/python/lite.py
+++ b/tensorflow/contrib/lite/python/lite.py
@@ -274,7 +274,8 @@ def from_keras_model_file(cls,
                             model_file,
                             input_arrays=None,
                             input_shapes=None,
-                            output_arrays=None):
+                            output_arrays=None,
+                            custom_objects=None):
     """"""Creates a TocoConverter class from a tf.keras model file.
 
     Args:
@@ -293,7 +294,7 @@ def from_keras_model_file(cls,
     """"""
     _keras.backend.clear_session()
     _keras.backend.set_learning_phase(False)
-    keras_model = _keras.models.load_model(model_file)
+    keras_model = _keras.models.load_model(model_file, custom_objects=custom_objects)
     sess = _keras.backend.get_session()
 
     # Get input and output tensors.
```"
21427,"builld tf 1.10.0, but version shows rc1","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04 x64
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10
- **Python version**: 3.6.6 x64
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.2/7.1.5
- **GPU model and memory**: GTX1080Ti GDDR5X 11GB  X 6
- **Exact command to reproduce**: 

pip generated as 1.10
install complete and shows 1.10


import tensorflow as tf
print(tf.__verison__)

1.10.0-rc1"
21421,Android: cannot find -lpthread for simple binary,"I'm trying to compile a simple application for Android and getting linker errors: Bazel is linking in `-lpthread`, even though Android doesn't support that.

The application code is as follows:

*tensorflow/demo-bug/main.cc*:
```
int main() {
  return 0;
}
```

The BUILD file is as follows:

*tensorflow/demo-bug/BUILD*:
```
cc_binary(
    name = ""main"",
    srcs = [""main.cc""],
    deps = [""//tensorflow/core:core_cpu""],
)
```

If you don't include `//tensorflow/core:core_cpu` in the `deps`, the error doesn't happen. For some reason, that dependency pulls in the `-lpthread` from somewhere.

When you run the `bazel build` command (see below), you get the following error:
```
external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: cannot find -lpthread
```
Indeed, looking through the command that it uses to compile it, you see the flag:
```
...-stl/llvm-libc++/libs/armeabi-v7a/libunwind.a -ldl -lm -ldl -lpthread -lm -pthread -lm -lm -static-libgcc -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64 -no-canonical-prefixes ...
```
(Full command below)
```
  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -o bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/demo-bug/main bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/demo-bug/_objs/main/tensorflow/demo-bug/main.o -Wl,-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libcore_cpu_internal.lo -Wl,-no-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libmeta_optimizer.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libarithmetic_optimizer.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libgraph_optimizer_stage.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libauto_parallel.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libcustom_graph_optimizer_registry_impl.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libdebug_stripper.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libdependency_optimizer.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libfunction_optimizer.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/liblayout_optimizer.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libloop_optimizer.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libmemory_optimizer.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libstatic_schedule.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/costs/libgraph_memory.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/clusters/libvirtual_cluster.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/costs/libop_level_cost_estimator.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/costs/libvirtual_scheduler.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/costs/libvirtual_placer.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/libdevices.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/utils/libtraversal.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libmodel_pruner.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libgraph_rewriter.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libremapper.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libconstant_folding.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libscoped_allocator_optimizer.a -Wl,-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libscoped_allocator_ops_op_lib.lo -Wl,-no-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libshape_optimizer.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/optimizers/libsymbolic_shapes.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/costs/libgraph_properties.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/costs/libutils.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/clusters/libutils.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libgpu_id_impl.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/libgraph_view.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/clusters/libcluster.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/utils/libframe.a -Wl,-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libcore_cpu_base.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libfunction_ops_op_lib.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libfunctional_grad.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libfunctional_ops_op_lib.lo -Wl,-no-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/utils/libcolocation.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/utils/libfunctions.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/utils/libtopological_sort.a -Wl,-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/kernels/libno_op.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/kernels/libsendrecv_ops.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libno_op_op_lib.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libsendrecv_ops_op_lib.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libcore_cpu_impl.lo -Wl,-no-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libgraph.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/libgrappler_item.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/libop_types.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/libutils.a -Wl,-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libframework_internal_impl.lo -Wl,-no-whole-archive bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libprotos_all_proto_text.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/liberror_codes_proto_text.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/liblib_internal_impl.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/liblib_hash_crc32c_accelerate_internal.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/liblib_proto_parsing.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libabi.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libcore_stringpiece.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libplatform_base.a bazel-out/armeabi-v7a-py3-opt/bin/external/snappy/libsnappy.a bazel-out/armeabi-v7a-py3-opt/bin/external/double_conversion/libdouble-conversion.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/grappler/costs/libop_performance_data_cc_impl.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libversion_lib.a bazel-out/armeabi-v7a-py3-opt/bin/external/gif_archive/libgif.a bazel-out/armeabi-v7a-py3-opt/bin/external/jpeg/libjpeg.a bazel-out/armeabi-v7a-py3-opt/bin/external/jpeg/libsimd_armv7a.a bazel-out/armeabi-v7a-py3-opt/bin/external/com_googlesource_code_re2/libre2.a bazel-out/armeabi-v7a-py3-opt/bin/external/farmhash_archive/libfarmhash.a bazel-out/armeabi-v7a-py3-opt/bin/external/fft2d/libfft2d.a bazel-out/armeabi-v7a-py3-opt/bin/external/highwayhash/libsip_hash.a bazel-out/armeabi-v7a-py3-opt/bin/external/highwayhash/libarch_specific.a bazel-out/armeabi-v7a-py3-opt/bin/external/png_archive/libpng.a bazel-out/armeabi-v7a-py3-opt/bin/external/zlib_archive/libzlib.a bazel-out/armeabi-v7a-py3-opt/bin/external/nsync/libnsync_cpp.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/libprotos_all_proto_cc_impl.a bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/core/liberror_codes_proto_cc_impl.a bazel-out/armeabi-v7a-py3-opt/bin/external/protobuf_archive/libprotobuf.a bazel-out/armeabi-v7a-py3-opt/bin/external/protobuf_archive/libprotobuf_lite.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libandroid_support.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libc++.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libc++_static.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libc++abi.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a/libunwind.a -ldl -lm -ldl -lpthread -lm -pthread -lm -lm -static-libgcc -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64 -no-canonical-prefixes -target armv7-none-linux-androideabi -Wl,--fix-cortex-a8 -Lexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a '--sysroot=external/androidndk/ndk/platforms/android-23/arch-arm')
```

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see above
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.9.0 (git tag)
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: Android NDK r17b
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
```
# ARM v7
bazel \
  --crosstool_top=//external:android/crosstool \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  --config=android \
  --cpu=armeabi-v7a \
  --fat_apk_cpu=armeabi-v7a \
  --verbose_failures \
  --cxxopt=-std=c++11 \
  --config=monolithic \
  //tensorflow/demo-bug:main

# ARM v8
bazel \
  --crosstool_top=//external:android/crosstool \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  --config=android \
  --cpu=arm64-v8a \
  --fat_apk_cpu=arm64-v8a \
  --verbose_failures \
  --cxxopt=-std=c++11 \
  --config=monolithic \
  //tensorflow/demo-bug:main
```"
21417,Discrepancy between Python and C++ loading of corrupt SavedModels,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS High Seirra 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NOT MOBILE
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: ('v1.9.0-0-g25c197e023', '1.9.0')
- **Python version**:Python 2.7.10
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 
 	- Train and export the MNIST model with `python mnist.py`. This will export a model in tmp directory in the same folder as mnist.py script.

 	-  Remove the variables.index file from ./tmp/mnist_model/15*/variables/ folder. This is an important step required to reproduce the partial model load issue.
	- Load the model in python with load.py (might want to change path on line 6) by executing
	`python load.py`. This should throw an error. Expected behavior.
	- Load the model in java with Load.java (might want to change path on line 6) by executing
	`javac -cp libtensorflow-1.9.0.jar Load.java` and
	`java -cp libtensorflow-1.9.0.jar:. -Djava.library.path=./jni Load`. This does not throw an error. Only when you send some input data to this model does it throw an error. Unexpected behavior.

### Describe the problem
This bug is to illustrate the partial model load discrepancy in the Python and Java API.

- The exported model directory consists of a saved_model.pb file corresponding to the 
graph of the exported model. The directory also consists of a variables directory. This variables
directory consists of a variables.index file which list the variables used by the model and a variables.data file. Note that it is possible to have a model with no variables directory for a model with zero variables.


- Situation: In cases where model does not have the variables.index file, Python API throws an exception whereas Java API doesn't throw an exception.

If you load the partial model in Python with

`tf.saved_model.loader.load`
the missing variables.index file issue is caught and it throws an error. This is because, in python when it loads the model, it restores the variables too: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/loader_impl.py#L349. It sees that some of the variables needed in the graph, are not specified via the variables.index file, hence results in UndefinedError.

If you load the partial model in Java with

`SavedModelBundle.load`
the missing variables.index file issue is not caught and therefore it does not throw an error. This is because in Java, when it loads the model: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/loader.cc#L171, it sees that the variables.index file is missing, hence makes the assumption that the graph does not have any variables.





### Source code / logs
mnist.py
```python
import tensorflow as tf
import numpy as np

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data')

def input(dataset):
    return dataset.images, dataset.labels.astype(np.int32)

# Specify feature
feature_columns = [tf.feature_column.numeric_column(""x"", shape=[28, 28])]

# Build 2 layer DNN classifier
classifier = tf.estimator.DNNClassifier(
    feature_columns=feature_columns,
    hidden_units=[256, 32],
    optimizer=tf.train.AdamOptimizer(1e-4),
    n_classes=10,
    dropout=0.1,
    model_dir=""./tmp/mnist_model""
)

# Define the training inputs
train_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={""x"": input(mnist.train)[0]},
    y=input(mnist.train)[1],
    num_epochs=None,
    batch_size=50,
    shuffle=True
)

classifier.train(input_fn=train_input_fn, steps=100)

# Define the test inputs
test_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={""x"": input(mnist.test)[0]},
    y=input(mnist.test)[1],
    num_epochs=1,
    shuffle=False
)

# Evaluate accuracy
accuracy_score = classifier.evaluate(input_fn=test_input_fn)[""accuracy""]
print(""\nTest Accuracy: {0:f}%\n"".format(accuracy_score*100))
image = tf.placeholder(tf.float32,[None])
classifier.export_savedmodel(""./tmp/mnist_model"", 
	tf.estimator.export.build_raw_serving_input_receiver_fn({""x"":image}))
```

load.py (might want to change path in line 6)
```python
import tensorflow as tf 

sess = tf.Session()
tf.saved_model.loader.load(sess, 
	[tf.saved_model.tag_constants.SERVING],
	'./tmp/mnist_model/1533576273')
```

Load.java (might want to change path in line 6)
```java
import org.tensorflow.SavedModelBundle;

public class Load {
  public static void main(String[] args) throws Exception {
  	SavedModelBundle savedModelBundle = SavedModelBundle.load(
  		""./tmp/mnist_model/1533576273/"", ""serve"");
  }
}
```

Here's the output when loading the model in python:
```sh
$ python load.py
/Users/priyankj/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-08-06 15:27:37.363952: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-08-06 15:27:37.411644: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at save_restore_tensor.cc:170 : Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./tmp/mnist_model/1533576273/variables/variables
Traceback (most recent call last):
  File ""load.py"", line 6, in <module>
    './tmp/mnist_model/1533576273')
  File ""/Users/priyankj/lib/python2.7/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 229, in load
    saver.restore(sess, variables_path)
  File ""/Users/priyankj/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1768, in restore
    six.reraise(exception_type, exception_value, exception_traceback)
  File ""/Users/priyankj/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1752, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/Users/priyankj/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/Users/priyankj/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/Users/priyankj/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/Users/priyankj/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./tmp/mnist_model/1533576273/variables/variables
	 [[Node: save_1/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]

Caused by op u'save_1/RestoreV2', defined at:
  File ""load.py"", line 6, in <module>
    './tmp/mnist_model/1533576273')
  File ""/Users/priyankj/lib/python2.7/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 219, in load
    saver = tf_saver.import_meta_graph(meta_graph_def_to_load, **saver_kwargs)
  File ""/Users/priyankj/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1960, in import_meta_graph
    **kwargs)
  File ""/Users/priyankj/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py"", line 744, in import_scoped_meta_graph
    producer_op_list=producer_op_list)
  File ""/Users/priyankj/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/Users/priyankj/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 442, in import_graph_def
    _ProcessNewOps(graph)
  File ""/Users/priyankj/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 234, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""/Users/priyankj/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3563, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""/Users/priyankj/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3450, in _create_op_from_tf_operation
    ret = Operation(c_op, self)
  File ""/Users/priyankj/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./tmp/mnist_model/1533576273/variables/variables
	 [[Node: save_1/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]
```

Here's the output of loading the model in Java:
```sh
$ javac -cp libtensorflow-1.9.0.jar Load.java
$ java -cp libtensorflow-1.9.0.jar:. -Djava.library.path=./jni Load
2018-08-06 14:56:48.242783: I tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: ./tmp/mnist_model/1533576273/
2018-08-06 14:56:48.244329: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA
2018-08-06 14:56:48.246854: I tensorflow/cc/saved_model/loader.cc:161] Restoring SavedModel bundle.
2018-08-06 14:56:48.246888: I tensorflow/cc/saved_model/loader.cc:171] The specified SavedModel has no variables; no checkpoints were restored.
2018-08-06 14:56:48.246894: I tensorflow/cc/saved_model/loader.cc:196] Running LegacyInitOp on SavedModel bundle.
2018-08-06 14:56:48.250752: I tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: success. Took 8012 microseconds.
```"
21416,"Android: Compiling ops yields ""THIS_TYPE_IS_NOT_SUPPORTED"" for cwise_op","Using TF 1.9.0, I cannot build applications for Android due to errors from `cwise_op`. 

Building for ARM v8, the error is `THIS_TYPE_IS_NOT_SUPPORTED`:
```In file included from tensorflow/core/kernels/cwise_op_lgamma.cc:16:
In file included from ./tensorflow/core/kernels/cwise_ops_common.h:29:
In file included from ./tensorflow/core/kernels/cwise_ops.h:23:
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:31:
In file included from external/eigen_archive/unsupported/Eigen/CXX11/../SpecialFunctions:50:
external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:177:5: error: static_assert failed ""THIS_TYPE_IS_NOT_SUPPORTED""
    EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),
```

For ARM v7, the error is `no matching function for call to 'acosh'`:
```In file included from tensorflow/core/kernels/cwise_op_acosh.cc:16:
In file included from ./tensorflow/core/kernels/cwise_ops_common.h:29:
./tensorflow/core/kernels/cwise_ops.h:55:12: error: no matching function for call to 'acosh'
    return std::acosh(a);
           ^~~~~~~~~~
```

The exact bazel command is quite simple and is provided below.

This is an isolated issue which crops up when trying to build a larger target (which depends on the cwise kernels). I can provide additional information about the larger target, but the error is the same and boils down to being unable to compile cwise_ops. The same error also occurred on TF 1.8.0.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No (failure occurs when compiling TF target)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.9 (git tag v1.9.0)
- **Python version**: (3.5)
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: Android NDK r16b
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```
# For ARM v8
bazel \
  --crosstool_top=//external:android/crosstool \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  --config=android \
  --cpu=arm64-v8a \
  --fat_apk_cpu=arm64-v8a \
  --verbose_failures \
  --cxxopt=-std=c++11 \
  --copt=-D__ANDROID_TYPES_SLIM__ \
  --copt=-DIS_MOBILE_PLATFORM \
  //tensorflow/core/kernels:cwise_op

# For ARM v7
bazel \
  --crosstool_top=//external:android/crosstool \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  --config=android \
  --cpu=armeabi-v7a \
  --fat_apk_cpu=armeabi-v7a \
  --verbose_failures \
  --cxxopt=-std=c++11 \
  --copt=-D__ANDROID_TYPES_SLIM__ \
  --copt=-DIS_MOBILE_PLATFORM \
  //tensorflow/core/kernels:cwise_op
```
"
21415,tensorflow ScipyOptimizerInterface feeding error ,"-----------------------
My code are as follow:
```
p_f = tf.get_variable('p_f', shape=(1, 1)) #corresponding index 0 in 

W1_1 = tf.constant(paras[0], dtype=tf.float32, name='W1_1' )
W1_2_1 = tf.constant(paras[1][0, :], dtype=tf.float32, shape=[1, 79], name='W1_2_1')
W1_2_2 = tf.constant(paras[1][1:, :], dtype=tf.float32, name='W1_2_2')
b1 = tf.constant(paras[2], dtype=tf.float32, name='b1' )
W2_1_1 = tf.constant(paras[3][0, :], dtype=tf.float32, shape=[1, 1], name='W1_2_1')
W2_1_2 = tf.constant(paras[3][1:, :], dtype=tf.float32, name='W1_2_2')
W2_2 = tf.constant(paras[4], dtype=tf.float32, name='W2_2' )
b2 = tf.constant(paras[5], dtype=tf.float32, name='b2' )

p_sub =  tf.placeholder(shape=(None, 18), dtype=tf.float32 , name=""p_sub"")
p_t =  tf.placeholder(shape=(None, 60), dtype=tf.float32, name= 'p_t') #the other part except for the price

D = tf.matmul(tf.nn.sigmoid(tf.matmul(p_f, W1_2_1)+ tf.matmul(p_t, W1_2_2) + tf.matmul(p_sub, W1_1)+b1), W2_2)+tf.matmul(p_f, W2_1_1)+ tf.matmul(p_t, W2_1_2)+b2
    
p_loss = tf.reduce_mean(-p_f*D) #self defined loss

###parameters
learning_rate = 0.5
optimizer = tf.contrib.opt.ScipyOptimizerInterface(p_loss, method='L-BFGS-B', options={'maxiter': 1000})

with tf.Session() as sess:
    init = tf.global_variables_initializer()
    for step in range(419):
        #parameters 
        x1_train = tv_train.iloc[step*batch_size:step*batch_size+batch_size, 66:84].values 
        x1_test = tv_test.iloc[:, 66:84].values
        x2_train = np.concatenate((tv_train.iloc[step*batch_size:step*batch_size+batch_size, 4:11].values, tv_train.iloc[step*batch_size:step*batch_size+batch_size, 14:67].values), 1) 
        #x2_train = tf.concat(axis=1, values=[tv_train.iloc[step*batch_size:step*batch_size+batch_size, 4:11].values, tv_train.iloc[step*batch_size:step*batch_size+batch_size, 14:67].values])
        x2_test = np.concatenate((tv_test.iloc[:, 4:11].values, tv_test.iloc[:, 14:67].values), 1) 
        
        sess.run(init, feed_dict={p_sub: x1_train, 
                                  p_t: x2_train})
        optimizer.minimize(sess) 
        ret=sess.run(p_f)
        
        if step % 200 == 0:
            print()
            print(ret)
            print()
```
```
----------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1349     try:
-> 1350       return fn(*args)
   1351     except errors.OpError as e:

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1328                                    feed_dict, fetch_list, target_list,
-> 1329                                    status, run_metadata)
   1330 

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    472             compat.as_text(c_api.TF_Message(self.status.status)),
--> 473             c_api.TF_GetCode(self.status.status))
    474     # Delete the underlying status object from memory otherwise it stays alive

InvalidArgumentError: You must feed a value for placeholder tensor 'p_sub' with dtype float and shape [?,18]
	 [[Node: p_sub = Placeholder[dtype=DT_FLOAT, shape=[?,18], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-67-986c52132edf> in <module>()
     39         sess.run(init, feed_dict={p_sub: x1_train, 
     40                                   p_t: x2_train})
---> 41         optimizer.minimize(sess)
     42         ret=sess.run(p_f)
     43 

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\opt\python\training\external_optimizer.py in minimize(self, session, feed_dict, fetches, step_callback, loss_callback, **run_kwargs)
    205         packed_bounds=self._packed_bounds,
    206         step_callback=step_callback,
--> 207         optimizer_kwargs=self.optimizer_kwargs)
    208     var_vals = [
    209         packed_var_val[packing_slice] for packing_slice in self._packing_slices

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\opt\python\training\external_optimizer.py in _minimize(self, initial_val, loss_grad_func, equality_funcs, equality_grad_funcs, inequality_funcs, inequality_grad_funcs, packed_bounds, step_callback, optimizer_kwargs)
    404 
    405     import scipy.optimize  # pylint: disable=g-import-not-at-top
--> 406     result = scipy.optimize.minimize(*minimize_args, **minimize_kwargs)
    407 
    408     message_lines = [

C:\ProgramData\Anaconda3\lib\site-packages\scipy\optimize\_minimize.py in minimize(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)
    448     elif meth == 'l-bfgs-b':
    449         return _minimize_lbfgsb(fun, x0, args, jac, bounds,
--> 450                                 callback=callback, **options)
    451     elif meth == 'tnc':
    452         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,

C:\ProgramData\Anaconda3\lib\site-packages\scipy\optimize\lbfgsb.py in _minimize_lbfgsb(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)
    326             # until the completion of the current minimization iteration.
    327             # Overwrite f and g:
--> 328             f, g = func_and_grad(x)
    329         elif task_str.startswith(b'NEW_X'):
    330             # new iteration

C:\ProgramData\Anaconda3\lib\site-packages\scipy\optimize\lbfgsb.py in func_and_grad(x)
    276     else:
    277         def func_and_grad(x):
--> 278             f = fun(x, *args)
    279             g = jac(x, *args)
    280             return f, g

C:\ProgramData\Anaconda3\lib\site-packages\scipy\optimize\optimize.py in function_wrapper(*wrapper_args)
    290     def function_wrapper(*wrapper_args):
    291         ncalls[0] += 1
--> 292         return function(*(wrapper_args + args))
    293 
    294     return ncalls, function_wrapper

C:\ProgramData\Anaconda3\lib\site-packages\scipy\optimize\optimize.py in __call__(self, x, *args)
     61     def __call__(self, x, *args):
     62         self.x = numpy.asarray(x).copy()
---> 63         fg = self.fun(x, *args)
     64         self.jac = fg[1]
     65         return fg[0]

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\opt\python\training\external_optimizer.py in loss_grad_func_wrapper(x)
    365     def loss_grad_func_wrapper(x):
    366       # SciPy's L-BFGS-B Fortran implementation requires gradients as doubles.
--> 367       loss, gradient = loss_grad_func(x)
    368       return loss, gradient.astype('float64')
    369 

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\opt\python\training\external_optimizer.py in eval_func(x)
    276 
    277       augmented_fetch_vals = session.run(
--> 278           augmented_fetches, feed_dict=augmented_feed_dict)
    279 
    280       if callable(callback):

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    893     try:
    894       result = self._run(None, fetches, feed_dict, options_ptr,
--> 895                          run_metadata_ptr)
    896       if run_metadata:
    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1126     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1127       results = self._do_run(handle, final_targets, final_fetches,
-> 1128                              feed_dict_tensor, options, run_metadata)
   1129     else:
   1130       results = []

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1342     if handle is None:
   1343       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1344                            options, run_metadata)
   1345     else:
   1346       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1361         except KeyError:
   1362           pass
-> 1363       raise type(e)(node_def, op, message)
   1364 
   1365   def _extend_graph(self):

InvalidArgumentError: You must feed a value for placeholder tensor 'p_sub' with dtype float and shape [?,18]
	 [[Node: p_sub = Placeholder[dtype=DT_FLOAT, shape=[?,18], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'p_sub', defined at:
  File ""C:\ProgramData\Anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\ProgramData\Anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\traitlets\config\application.py"", line 658, in launch_instance
    app.start()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\zmq\eventloop\ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tornado\ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\zmq\eventloop\zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\ipykernel\zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2717, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2821, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-67-986c52132edf>"", line 18, in <module>
    p_sub =  tf.placeholder(shape=(None, 18), dtype=tf.float32 , name=""p_sub"")
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1680, in placeholder
    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 4105, in _placeholder
    ""Placeholder"", dtype=dtype, shape=shape, name=name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 3160, in create_op
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1625, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'p_sub' with dtype float and shape [?,18]
	 [[Node: p_sub = Placeholder[dtype=DT_FLOAT, shape=[?,18], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```"
21414,issue in Google LOGO...,"hi there..
my wife got a meticulous problem in the official LOGO of Tensorflow.

see the logo:
(https://user-images.githubusercontent.com/39337227/43732365-c2b3a27e-99c6-11e8-94dc-735a1727678e.JPG)

in T of the logo, its have 4 column but in shadow its have 3 column.
corroct it.

we also accept any gift for that . 🥇 "
21413,tensorflow-1.0.0rc0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.,"
I've activated tensorflow in terminal, and I'm trying to set up tensorflow fold. When I try doing this, the above error pops up
`tensorflow-1.0.0rc0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.`

I searched other forums and tried to resolve the issue by trying the solutions suggested, but I wasn't able to solve the problem. 
Thanks!

"
21412,Error in Distribution Strategy with train_and_evaluate,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.4 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.0-rc1 and nightly
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: No
- **GCC/Compiler version (if compiling from source)**: No
- **CUDA/cuDNN version**: 9.0.176
- **GPU model and memory**: Tesla V100 16152MiB
- **Exact command to reproduce**: Save the snippet in test.py and run ""python test.py""


### Describe the problem
Using the distribute strategy (OneDeviceStrategy) I have a crash with train_and_evaluate method of the estimator.
The code below works with train and evaluate as separeted functions, but it doesn't work with train_and_evaluate.
I have attached a snipped with train, evaluate and train_and_evaluate methods togheter to gather the behaviour differences. At the end I've put the output error log.
Note: it works with tensorflow 1.9.0 and codalab
```
from tensorflow import keras as ks
import numpy as np
import tensorflow as tf
from tensorflow.python.estimator import keras as keras_lib


tf.logging.set_verbosity(tf.logging.INFO)


def input_fn():
    x = np.random.random((1024, 10))
    y = np.random.randint(2, size=(1024, 1))
    x = tf.cast(x, tf.float32)
    dataset = tf.data.Dataset.from_tensor_slices((x, y))
    dataset = dataset.repeat(100)
    dataset = dataset.batch(1)
    return dataset


model = ks.Sequential()
model.add(ks.layers.Dense(16, activation='relu', input_shape=(10,)))
model.add(ks.layers.Dense(1, activation='sigmoid'))

optimizer = tf.train.GradientDescentOptimizer(0.2)

model.compile(loss='binary_crossentropy', optimizer=optimizer)

strategy = tf.contrib.distribute.OneDeviceStrategy(""device:GPU:0"")
config = tf.estimator.RunConfig(train_distribute=strategy)

keras_estimator = keras_lib.model_to_estimator(
  keras_model=model,
  config=config)

keras_estimator.train(input_fn=input_fn, steps=10)
keras_estimator.evaluate(input_fn=input_fn, steps=3)

train_spec = tf.estimator.TrainSpec(
    input_fn=input_fn,
    max_steps=20)
eval_spec = tf.estimator.EvalSpec(
    input_fn=input_fn,
    steps=3)

tf.estimator.train_and_evaluate(keras_estimator, train_spec, eval_spec)
```

### Source code / logs
```
INFO:tensorflow:Using the Keras model provided.
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_xh7nsci
INFO:tensorflow:Using config: {'_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_keep_checkpoint_every_n_hours': 10000, '_global_id_in_cluster': 0, '_task_type': 'worker', '_session_config': None, '_is_chief': True, '_evaluation_master': '', '_log_step_count_steps': 100, '_model_dir': '/tmp/tmp_xh7nsci', '_save_checkpoints_secs': 600, '_save_summary_steps': 100, '_service': None, '_save_checkpoints_steps': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9a715b1240>, '_device_fn': None, '_keep_checkpoint_max': 5, '_num_worker_replicas': 1, '_train_distribute': <tensorflow.contrib.distribute.python.one_device_strategy.OneDeviceStrategy object at 0x7f9a630426d8>}
2018-08-06 11:45:02.557498: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-08-06 11:45:02.679737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-08-06 11:45:02.680248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1404] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:1e.0
totalMemory: 15.77GiB freeMemory: 15.36GiB
2018-08-06 11:45:02.680294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Adding visible gpu devices: 0
2018-08-06 11:45:03.032268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:964] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-06 11:45:03.032321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:970]      0 
2018-08-06 11:45:03.032339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0:   N 
2018-08-06 11:45:03.032678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14856 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)
2018-08-06 11:45:03.032984: E tensorflow/core/common_runtime/gpu/gpu_device.cc:228] Illegal GPUOptions.experimental.num_dev_to_dev_copy_streams=0 set to 1 instead.
2018-08-06 11:45:03.353069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Adding visible gpu devices: 0
2018-08-06 11:45:03.353135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:964] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-06 11:45:03.353154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:970]      0 
2018-08-06 11:45:03.353162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0:   N 
2018-08-06 11:45:03.353297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14856 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
2018-08-06 11:45:04.042046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Adding visible gpu devices: 0
2018-08-06 11:45:04.042111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:964] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-06 11:45:04.042128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:970]      0 
2018-08-06 11:45:04.042148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0:   N 
2018-08-06 11:45:04.042305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14856 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)
INFO:tensorflow:Restoring parameters from /tmp/tmp_xh7nsci/keras_model.ckpt
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp_xh7nsci/model.ckpt.
INFO:tensorflow:loss = 0.6695193, step = 0
INFO:tensorflow:Saving checkpoints for 10 into /tmp/tmp_xh7nsci/model.ckpt.
INFO:tensorflow:Loss for final step: 0.3536753.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-08-06-11:45:04
INFO:tensorflow:Graph was finalized.
2018-08-06 11:45:04.822515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Adding visible gpu devices: 0
2018-08-06 11:45:04.822570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:964] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-06 11:45:04.822594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:970]      0 
2018-08-06 11:45:04.822613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0:   N 
2018-08-06 11:45:04.822780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14856 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)
INFO:tensorflow:Restoring parameters from /tmp/tmp_xh7nsci/model.ckpt-10
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Evaluation [1/3]
INFO:tensorflow:Evaluation [2/3]
INFO:tensorflow:Evaluation [3/3]
INFO:tensorflow:Finished evaluation at 2018-08-06-11:45:04
INFO:tensorflow:Saving dict for global step 10: global_step = 10, loss = 0.1408012
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10: /tmp/tmp_xh7nsci/model.ckpt-10
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
2018-08-06 11:45:05.220540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Adding visible gpu devices: 0
2018-08-06 11:45:05.220581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:964] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-06 11:45:05.220600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:970]      0 
2018-08-06 11:45:05.220615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0:   N 
2018-08-06 11:45:05.220760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14856 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)
INFO:tensorflow:Restoring parameters from /tmp/tmp_xh7nsci/model.ckpt-10
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 10 into /tmp/tmp_xh7nsci/model.ckpt.
INFO:tensorflow:loss = 1.5840994, step = 10
INFO:tensorflow:Saving checkpoints for 20 into /tmp/tmp_xh7nsci/model.ckpt.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
Traceback (most recent call last):
  File ""test2.py"", line 45, in <module>
    tf.estimator.train_and_evaluate(keras_estimator, train_spec, eval_spec)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 451, in train_and_evaluate
    return executor.run()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 590, in run
    return self.run_local()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 691, in run_local
    saving_listeners=saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 376, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1143, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1368, in _train_model_distributed
    saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1451, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 695, in __exit__
    self._close_internal(exception_type)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py"", line 727, in _close_internal
    h.end(self._coordinated_creator.tf_sess)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 470, in end
    self._save(session, last_step)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 489, in _save
    if l.after_save(session, step):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 497, in after_save
    self._evaluate(global_step_value)  # updates self.eval_result
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 517, in _evaluate
    self._evaluator.evaluate_and_export())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py"", line 884, in evaluate_and_export
    hooks=self._eval_spec.hooks)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 463, in evaluate
    input_fn, hooks, checkpoint_path)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 1474, in _evaluate_build_graph
    model_fn_lib.LOSS_METRIC_KEY] = metrics_lib.mean(estimator_spec.loss)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/metrics_impl.py"", line 376, in mean
    mean_t = distribute_lib.get_tower_context().merge_call(
AttributeError: 'NoneType' object has no attribute 'merge_call'

```
"
21411,Tensorflow does not recognize GPU ,"
OS: Ubuntu 18.04.1 LTS
Tensorflow: 1.9.0 
Tensorflow- GPU: 1.9.0
Both installed from pip/pip3

Graphics Card: GeForce GTX 1080 TI 
Nvidia SMI Driver Version: 390.77
Python: 2.7.15, 3.6.5 
CUDA Toolkit:  9.0, V9.0.176 ( I also have 8 installed) 
CuDNN: 
![asfd](https://user-images.githubusercontent.com/22109013/43723326-5e2d6d0a-9965-11e8-9362-e5ff24c99e4f.png)

Running this: 
![screenshot from 2018-08-06 10-34-04](https://user-images.githubusercontent.com/22109013/43722870-4de50daa-9964-11e8-9648-050d643e0d16.png)

I get the following errors/CPU only: 
![screenshot from 2018-08-06 10-33-35](https://user-images.githubusercontent.com/22109013/43722894-5ad5ef02-9964-11e8-9b17-721294c7e15d.png)


I need to be able to access/run models off the GPU and am struggling to debug this issue alone. This seems to be a common problem, any help would be appreciated! "
21410,Memory leak with tf.py_func,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.9.0-rc2-1738-ge70f94e 1.10.0-rc1
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.16.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.2 / 7
- **GPU model and memory**: Nvidia 1080Ti, 11 GB
- **Exact command to reproduce**: ./leak.py (see attached file)

### Describe the problem
The attached script uses more and more CPU memory in each iteration until it runs out of memory. It does not do anything useful but it is a small test case to demonstrate the problem."
21409,[Feature Request] Main improvements for the c++-API,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: a bit, but this is not the problem
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64 Bit
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: every version
- **Python version**: no python bindings
- **Bazel version (if compiling from source)**: using cmake
- **GCC/Compiler version (if compiling from source)**: MSVC 1910
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**: not necessary

## Main improvements for the c++-API.
I am really having a lot of trouble using the tensorflow api with c++. Building from sources took some time, but i fixed it. At the moment i am able to train a basic model, but not really further. I would really appreciate, if you take a look at these points 

### Very Important
- Official support for c++
- Offer downloading the c++ built binaries for Windows and Linux, CPU and GPU, 64bit
- Way more examples for c++ (tried some [here](https://github.com/PinkySan/TensorflowHandlingTests))
- Exporting the models must be supported (e.g. simple Save) (tried it [here](https://github.com/PinkySan/TensorflowSimpleSaver))

### Important
- Focussing the error is out of heap space building-problem
- Building the static library without the shared library (both in the same cmake file)

### Nice to have
- Conanize the build (externals should be conan packages)
- Conanize tensorflow itself


If anyone else got other ideas or problems, feel free to add these"
21408,The installation procedure for Ubuntu does not work.,"I tried to follow the procedure for installing on Ubuntu 16.04 or later.
I am using Ubuntu 16.04 with the latest updates on 64-bit AMD/Intel.
I am using Python 3, version 3.5.2. I checked the version of pip3, it is 8.1.1
The step of the procedure which fails is:
virtualenv --system-site-packages -p python3 venv

and the error messages are as follows:
Already using interpreter /usr/bin/python3
Using base prefix '/usr'
New python executable in /home/nick/tensorflow/venv/bin/python3
Also creating executable in /home/nick/tensorflow/venv/bin/python
Installing setuptools, pkg_resources, pip, wheel...
  Complete output from command /home/nick/tensorflow/venv/bin/python3 - setuptools pkg_resources pip wheel:
  Collecting setuptools
Exception:
Traceback (most recent call last):
  File ""/home/nick/tensorflow/venv/share/python-wheels/urllib3-1.13.1-py2.py3-none-any.whl/urllib3/connectionpool.py"", line 560, in urlopen
    body=body, headers=headers)
  File ""/home/nick/tensorflow/venv/share/python-wheels/urllib3-1.13.1-py2.py3-none-any.whl/urllib3/connectionpool.py"", line 346, in _make_request
    self._validate_conn(conn)
  File ""/home/nick/tensorflow/venv/share/python-wheels/urllib3-1.13.1-py2.py3-none-any.whl/urllib3/connectionpool.py"", line 787, in _validate_conn
    conn.connect()
  File ""/home/nick/tensorflow/venv/share/python-wheels/urllib3-1.13.1-py2.py3-none-any.whl/urllib3/connection.py"", line 252, in connect
    ssl_version=resolved_ssl_version)
  File ""/home/nick/tensorflow/venv/share/python-wheels/urllib3-1.13.1-py2.py3-none-any.whl/urllib3/util/ssl_.py"", line 305, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File ""/usr/lib/python3.5/ssl.py"", line 377, in wrap_socket
    _context=self)
  File ""/usr/lib/python3.5/ssl.py"", line 752, in __init__
    self.do_handshake()
  File ""/usr/lib/python3.5/ssl.py"", line 988, in do_handshake
    self._sslobj.do_handshake()
  File ""/usr/lib/python3.5/ssl.py"", line 633, in do_handshake
    self._sslobj.do_handshake()
ConnectionResetError: [Errno 104] Connection reset by peer
"
21407,Error installing tensorflow from source,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.6
- **TensorFlow installed from (source or binary)**: Source with CPU-only support
- **TensorFlow version (use command below)**: b'v1.9.0-rc2-1183-g1b33df1814' 1.9.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: gcc version 7.3.0 (MacPorts gcc7 7.3.0_1)
- **CUDA/cuDNN version:** N/A
- **GPU model and memory:** N/A
- **Mobile device:** N/A
- **Exact command to reproduce**: 
`bazel build --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --verbose_failures //tensorflow/tools/pip_package:build_pip_package`

(I used the flag `--cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""` as suggested at https://www.tensorflow.org/install/install_sources; removing this flag leads to the same error)

### Describe the problem
The above command yields an error related to gcc (the complete log file is attached):
`ERROR: /private/var/tmp/_bazel_ptighin/1e40e60bad53a8010892ea7aefd4f97f/external/protobuf_archive/BUILD:260:1: Linking of rule '@protobuf_archive//:js_embed' failed (Exit 1): cc_wrapper.sh failed: error executing command
  (cd /private/var/tmp/_bazel_ptighin/1e40e60bad53a8010892ea7aefd4f97f/execroot/org_tensorflow && \
  exec env - \
    APPLE_SDK_PLATFORM='' \
    APPLE_SDK_VERSION_OVERRIDE='' \
    PATH=/Users/ptighin/bin:opt/local/libexec/gnubin:/opt/local/bin:/opt/local/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/opt/local/libexec/gnubin:/opt/local/bin:/opt/local/sbin \
    XCODE_VERSION_OVERRIDE=9.4.1 \
  external/local_config_cc/cc_wrapper.sh -fobjc-link-runtime -Wl,-S -o bazel-out/host/bin/external/protobuf_archive/js_embed bazel-out/host/bin/external/protobuf_archive/_objs/js_embed/external/protobuf_archive/src/google/protobuf/compiler/js/embed.o -headerpad_max_install_names -lc++ -no-canonical-prefixes)
gcc: error: unrecognized command line option '-fobjc-link-runtime'; did you mean '-fgnu-runtime'?
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 117.328s, Critical Path: 28.23s
INFO: 63 processes: 63 local.
FAILED: Build did NOT complete successfully`

### Source code / logs
[logs.txt](https://github.com/tensorflow/tensorflow/files/2262594/logs.txt)

### Temporary solution that worked for me
Go to `/private/var/tmp/_bazel_<YOUR_USERNAME>/<HASH>/execroot/org_tensorflow/external/local_config_cc/cc_wrapper.sh`  (`<YOUR_USERNAME>` and `<HASH>` can be found in the error message) and change line 56
`/opt/local/bin/gcc ""$@""` (or wherever your C++ compiler is)
to
`/opt/local/bin/clang ""$@""` (or wherever your clang compiler is)

The problem seems to be that the wrapper of the C++-compiler points to gcc but passes clang-type flags."
21406,Error with tf 1.10.0rc1,"Hi,

After upgrading to 1.10.0rc1, I got an error: E tensorflow/core/common_runtime/gpu/gpu_device.cc:228] Illegal GPUOptions.experimental.num_dev_to_dev_copy_streams=0 set to 1 instead. However, the program is still running. Is it still ok with this error?

Thanks,
Tuan

"
21405,Tensorflow Moving Average Optimizer and Distribution Strategy,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04.4 LTS (Xenial Xerus)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: From source
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**: 7.1.4.18
- **GPU model and memory**: Quadro K620 and Tesla K40c -- 2GB and 11.5GB respectively.
- **Exact command to reproduce**:
```
with tf.Graph().as_default() as graph:
    config = tf.estimator.RunConfig(
        model_dir=""./output"",
        save_summary_steps=FLAGS.saving_summ_freq, 
        save_checkpoints_steps=FLAGS.saving_ckpt_freq,
        keep_checkpoint_max=3,
        train_distribute=tf.contrib.distribute.MirroredStrategy()
    )
    classifier = tf.estimator.Estimator(
        model_fn, 
        config=config)
    train_spec = tf.estimator.TrainSpec(gen_input_fn('train', FLAGS.num_epochs))
    valid_spec = tf.estimator.EvalSpec(gen_input_fn('valid', 1))
    tf.estimator.train_and_evaluate(classifier, train_spec, valid_spec)
```
Hello Tensorflow Devs,

I try to add exponential moving average support to the optimization step. However, this new Estimator API backed by the ""Mirrored Distrubution Strategy"" fails due to a tensor conversion method specific to this strategy.

When I call ema.apply_gradients(...) it ends up with the following exception:
```

INFO:tensorflow:Using config: {'_model_dir': './output', 1    365       saving_listeners = _check_listeners_type(saving_listeners)
--> 366       loss = self._train_model(input_fn, hooks, saving_listeners)
    367       logging.info('Loss for final step: %s.', loss)
    368       return self

/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1115   def _train_model(self, input_fn, hooks, saving_listeners):
   1116     if self._distribution:
-> 1117       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1118     else:
   1119       return self._train_model_default(input_fn, hooks, saving_listeners)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)
   1158             labels,  # although this will be None it seems
   1159             model_fn_lib.ModeKeys.TRAIN,
-> 1160             self.config)
   1161 
   1162         # TODO(anjalisridhar): Figure out how to resolve the following scaffold

/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py in call_for_each_tower(self, fn, *args, **kwargs)
    792     """"""
    793     _require_cross_tower_context(self)
--> 794     return self._call_for_each_tower(fn, *args, **kwargs)
    795 
    796   def _call_for_each_tower(self, fn, *args, **kwargs):

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py in _call_for_each_tower(self, fn, *args, **kwargs)
    267       for t in threads:
    268         t.should_run.set()
--> 269       coord.join(threads)
    270 
    271     return values.regroup({t.device: t.main_result for t in threads})

/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py in join(self, threads, stop_grace_period_secs, ignore_live_threads)
    387       self._registered_threads = set()
    388       if self._exc_info_to_raise:
--> 389         six.reraise(*self._exc_info_to_raise)
    390       elif stragglers:
    391         if ignore_live_threads:

/usr/local/lib/python3.5/dist-packages/six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py in stop_on_exception(self)
    295     """"""
    296     try:
--> 297       yield
    298     except:  # pylint: disable=bare-except
    299       self.request_stop(ex=sys.exc_info())

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py in run(self)
    477                 self._captured_var_scope, reuse=self.tower_id > 0), \
    478             variable_scope.variable_creator_scope(self.variable_creator_fn):
--> 479           self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
    480           self.done = True
    481       finally:

/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
   1105 
   1106     logging.info('Calling model_fn.')
-> 1107     model_fn_results = self._model_fn(features=features, **kwargs)
   1108     logging.info('Done calling model_fn.')
   1109 

<ipython-input-9-2239e101f763> in model_fn(features, labels, mode)
      3     loss = tfsi_model(features)
      4     if mode == tf.estimator.ModeKeys.TRAIN:
----> 5         train_op, grads, saver = minimize(loss)
      6         writer, merged = prepare_summary(tf.get_default_graph(), loss, grads)
      7         chkpt_hook = tf.train.CheckpointSaverHook(

<ipython-input-7-8dbd2a0df6d6> in minimize(loss)
     17         train_op = ema.apply_gradients(
     18             grads,
---> 19             global_step=tf.train.get_or_create_global_step()
     20         )
     21         return train_op, grads, ema.swapping_saver()

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/opt/python/training/moving_average_optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)
     97     if self._sequential_update:
     98       with ops.control_dependencies([train_op]):
---> 99         ma_op = self._ema.apply(var_list)
    100     else:
    101       ma_op = self._ema.apply(var_list)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py in apply(self, var_list)
    428         zero_debias = self._averages[var] in zero_debias_true
    429         updates.append(assign_moving_average(
--> 430             self._averages[var], var, decay, zero_debias=zero_debias))
    431       return control_flow_ops.group(*updates, name=scope)
    432 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py in assign_moving_average(variable, value, decay, zero_debias, name)
     82   with ops.name_scope(name, ""AssignMovingAvg"",
     83                       [variable, value, decay]) as scope:
---> 84     with ops.colocate_with(variable):
     85       decay = ops.convert_to_tensor(1.0 - decay, name=""decay"")
     86       if decay.dtype != variable.dtype.base_dtype:

/usr/lib/python3.5/contextlib.py in __enter__(self)
     57     def __enter__(self):
     58         try:
---> 59             return next(self.gen)
     60         except StopIteration:
     61             raise RuntimeError(""generator didn't yield"") from None

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in _colocate_with_for_gradient(self, op, gradient_uid, ignore_existing)
   4217   def _colocate_with_for_gradient(self, op, gradient_uid,
   4218                                   ignore_existing=False):
-> 4219     with self.colocate_with(op, ignore_existing):
   4220       if gradient_uid is not None and self._control_flow_context is not None:
   4221         try:

/usr/lib/python3.5/contextlib.py in __enter__(self)
     57     def __enter__(self):
     58         try:
---> 59             return next(self.gen)
     60         except StopIteration:
     61             raise RuntimeError(""generator didn't yield"") from None

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in colocate_with(self, op, ignore_existing)
   4270     if op is not None and not isinstance(op, Operation):
   4271       # We always want to colocate with the reference op.
-> 4272       op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op
   4273 
   4274     # By default, colocate_with resets the device function stack,

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor_or_indexed_slices(value, dtype, name, as_ref)
   1266   else:
   1267     return internal_convert_to_tensor(
-> 1268         value, dtype=dtype, name=name, as_ref=as_ref)
   1269 
   1270 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
   1105 
   1106     if ret is None:
-> 1107       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1108 
   1109     if ret is NotImplemented:

/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py in _tensor_conversion(var, dtype, name, as_ref)
    243   # Try to avoid assignments to and other mutations of MirroredVariable
    244   # state except through a DistributionStrategy.update() call.
--> 245   assert not as_ref
    246   return ops.internal_convert_to_tensor(
    247       var.get(), dtype=dtype, name=name, as_ref=as_ref)

AssertionError: 
```
Here is the code for creating optimizer and applying backpropagation to the specified loss function:
```

def minimize(loss):

    lr = tf.constant(learning_rate_schedule[0], dtype=tf.float32)
    for key, val in learning_rate_schedule.items():
        lr = tf.cond(
            tf.less(tf.train.get_or_create_global_step(), key), 
            lambda : lr,
            lambda : tf.constant(val, dtype=tf.float32)
        )
    opt = tf.train.AdamOptimizer(learning_rate=lr, epsilon=FLAGS.epsilon)
    if FLAGS.is_ema_enabled:
        ema = tf.contrib.opt.MovingAverageOptimizer(
            opt, 
            num_updates=tf.train.get_or_create_global_step()
        )
        grads = ema.compute_gradients(loss)
        train_op = ema.apply_gradients(
            grads,
            global_step=tf.train.get_or_create_global_step()
        )
        return train_op, grads, ema.swapping_saver()
    else:
        grads = opt.compute_gradients(loss)
        train_op = opt.apply_gradients(
            grads, 
            global_step=tf.train.get_or_create_global_step()
        )
        return train_op, grads, tf.train.Saver()
```
It seems that it causes a trouble when ""internal_tensor_conversion"" receives a reference variable though I am not sure of it. Am I doing something wrong or is it a bug?

Thank you for the help in advance."
21404,Request: Add Golang API bindings for C-API TF_AddGradients(),"Hi,

currently the Golang bindings to Tensorflow (via the C API) allow me to define a graph, do most computations, load a trained model, and perform inference. However, it would be very helpful to be able to obtain gradients aswell. It looks like the C-API now has the TF_AddGradients call which makes it possible to get grads for some (but not all) operations. 

Adding a binding to this call in the Go API would allow me to use the C API through golang as a general autodiff library, which would help with some projects (such as the implementation of gradient-based stochastic samplers in Go like SGHMC). If this is actually easier than it looks, any pointers on how I might attempt it would be welcomed. Thanks!"
21403,Crash during folder creation from Estimator exporter.py in Python 3.6,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0 and 1.9.0
- **Python version**: 3.6
- **GPU model and memory**: GTX 1080 Ti
- **Exact command to reproduce**: See [here](https://github.com/tensorflow/models/issues/4996#issue-347431961)

### Describe the problem
When exporting of a snapshot during training with Estimator, [`export.py`](https://github.com/tensorflow/tensorflow/blob/e70f94ee089abbb9eb70361b5cdef55aa9beb18b/tensorflow/python/estimator/export/export.py#L600) creates an invalid path with Python 3.6, due to handling of strings as bytes (!!!). 

In [`estimator.py`](https://github.com/tensorflow/tensorflow/blob/e70f94ee089abbb9eb70361b5cdef55aa9beb18b/tensorflow/python/estimator/estimator.py#L779), the following string is generated for the variable `export_dir`: `b'C:/Users/Alex/pet-train\\export\\Servo\\1533549311'`, which is afterwards corrupted by the `get_temp_export_dir` command, which defaces it into `b""C:/Users/Alex/pet-train\\export\\Servo\\temp-b'1533549311'""` which further down the road causes the program to crash with the following exception:

```
Traceback (most recent call last):
  File ""C:\Program Files\JetBrains\PyCharm 2018.1.2\helpers\pydev\pydevd.py"", line 1664, in <module>
    main()
  File ""C:\Program Files\JetBrains\PyCharm 2018.1.2\helpers\pydev\pydevd.py"", line 1658, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""C:\Program Files\JetBrains\PyCharm 2018.1.2\helpers\pydev\pydevd.py"", line 1068, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""C:\Program Files\JetBrains\PyCharm 2018.1.2\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""C:/Users/Alex/Repositories/MusicObjectDetector-TF2/research/object_detection/model_main.py"", line 101, in <module>
    tf.app.run()
  File ""C:\Programmieren\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""C:/Users/Alex/Repositories/MusicObjectDetector-TF2/research/object_detection/model_main.py"", line 97, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""C:\Programmieren\Anaconda3\lib\site-packages\tensorflow\python\estimator\training.py"", line 439, in train_and_evaluate
    executor.run()
  File ""C:\Programmieren\Anaconda3\lib\site-packages\tensorflow\python\estimator\training.py"", line 518, in run
    self.run_local()
  File ""C:\Programmieren\Anaconda3\lib\site-packages\tensorflow\python\estimator\training.py"", line 657, in run_local
    eval_result = evaluator.evaluate_and_export()
  File ""C:\Programmieren\Anaconda3\lib\site-packages\tensorflow\python\estimator\training.py"", line 858, in evaluate_and_export
    self._export_eval_result(eval_result, is_the_final_export)
  File ""C:\Programmieren\Anaconda3\lib\site-packages\tensorflow\python\estimator\training.py"", line 889, in _export_eval_result
    is_the_final_export=is_the_final_export)
  File ""C:\Programmieren\Anaconda3\lib\site-packages\tensorflow\python\estimator\exporter.py"", line 177, in export
    is_the_final_export)
  File ""C:\Programmieren\Anaconda3\lib\site-packages\tensorflow\python\estimator\exporter.py"", line 123, in export
    strip_default_attrs=self._strip_default_attrs)
  File ""C:\Programmieren\Anaconda3\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 644, in export_savedmodel
    builder = saved_model_builder.SavedModelBuilder(temp_export_dir)
  File ""C:\Programmieren\Anaconda3\lib\site-packages\tensorflow\python\saved_model\builder_impl.py"", line 92, in __init__
    file_io.recursive_create_dir(self._export_dir)
  File ""C:\Programmieren\Anaconda3\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 374, in recursive_create_dir
    pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(dirname), status)
  File ""C:\Programmieren\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a directory: C:/Users/Alex/pet-train\export\Servo\temp-b'1533549311'; No such file or directory

Process finished with exit code -1
```
"
21402,Features: train.saver  namescoped-restoring,"Hello:

Scenario of need:
Joe the ML-engineer wants a model that predict multiple thing out of a single img (lets say object detection & some environment values: hour of day, type of surroundings: inside/outside/forest/stadium/etc).
So Joe train a model to get its object detected.
Then Joe train a model to get its environment data.
And now, Joe want a model that does both in a single prediction.
But Joe may want to have a clean **modulated model** (for scalable/maintainable purposes)

Scenario of usage:
Joe create its model of object detection using the same function he used for first creating it, but in a name scope.
Joe then use the `saver` to restore his checkpoint, adding the name scope as parameter to indicate that his variables are now contained in a name_scope thus their name has changed.
Then the same for the environment model.


This feature need was inspired by this issue: 
[make-predictions-with-an-old-model-without-losing-the-current-model](https://stackoverflow.com/questions/51654259/make-predictions-with-an-old-model-without-losing-the-current-model/51654678?noredirect=1#comment90301946_51654678)

Have I written custom code: N/A
OS Platform and Distribution: N/A
TensorFlow installed from: N/A
TensorFlow version: N/A
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A
Mobile device: N/A"
21401,Request: Be able to tell if tensorflow is making use of tensor cores on V100,"It would be really great if you could tell if the model you were training was successfully making use of the tensor cores on the Nvidia Tesla V100.

This would help in deciding whether spending the extra money on the V100 would be worth it, which it would be if the model is making good use of the tensor cores.

I've tried to find the information using Nvidia's `nvidia-smi` tool, but it doesn't seem to how utilization by type of core.

Thanks
Chris"
21400,contrib.rnn.ConvLSTMCell: zero_state has wrong size if cell has skip connections,"There seems to be a bug in how ConvLSTMCell handles its states:

When calculating the output size, ConvLSTMCell has one more channel dimension (in general, as many more channels as there are input channels) than what was given as output_channels. 
This is only if skip_connection was True at initialization, **and makes sense** (the input is concatenated to the output --> n_channels_out = n_channels_in + original_n_channels_out).

Also, when creating a cell and applying it to inputs and a state:
### Minimal reproducible example
```
cell = tf.contrib.rnn.ConvLSTMCell(conv_ndims=2, input_shape=[80,80,1], output_channels=2,
                                               kernel_shape=[5,5], skip_connection=True)
state= cell.zero_state(10, dtype=tf.float32)
output, next_state = cell(last_outputs, state)
```

--> next_state is an LSTMStateTuple with shapes (?, 80, 80, 2) and (?, 80, 80, **3**). Still makes sense since it stores its hidden state as well as outputs and re-uses them in the next timestep, and the outputs gained an additional channel. But cell.zero_state() will have other shapes ( (?, 80, 80, 2) and (?, 80, 80, 2).)


The zero_state() calculation seems wrong; the code reads (in contrib/rnn/python/ops/rnn_cell.py):

```
    self._total_output_channels = output_channels
    if self._skip_connection:
      self._total_output_channels += self._input_shape[-1]

    state_size = tensor_shape.TensorShape(
        self._input_shape[:-1] + [self._output_channels])
    self._state_size = rnn_cell_impl.LSTMStateTuple(state_size, state_size)
    self._output_size = tensor_shape.TensorShape(
        self._input_shape[:-1] + [self._total_output_channels])
```

Shouldn't it be not` rnn_cell_impl.LSTMStateTuple(state_size, state_size)`, but `rnn_cell_impl.LSTMStateTuple(state_size, self._output_size)` ?

Funnily, it does not disturb the cell to be called with a state of size (80, 80, **2**)   - but for me it raised an error when called again with its next state which has size (80, 80, 3). For completeness, this error was (and this error is _not my problem_, my problem was inconsistency of shapes of the cell states): 

> Traceback (most recent call last):
  File ""/home/me/programs/pycharm-community-2016.1.1/helpers/pydev/_pydevd_bundle/pydevd_comm.py"", line 1079, in do_it
    result = pydevd_vars.evaluate_expression(self.thread_id, self.frame_id, self.expression, self.doExec)
  File ""/home/me/programs/pycharm-community-2016.1.1/helpers/pydev/_pydevd_bundle/pydevd_vars.py"", line 352, in evaluate_expression
    Exec(expression, updated_globals, frame.f_locals)
  File ""/home/me/programs/pycharm-community-2016.1.1/helpers/pydev/_pydevd_bundle/pydevd_exec2.py"", line 3, in Exec
    exec(exp, global_vars, local_vars)
  File ""<string>"", line 1, in <module>
  File ""/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 232, in __call__
    return super(RNNCell, self).__call__(inputs, state)
  File ""/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 717, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py"", line 2113, in call
    4 * self._output_channels, self._use_bias)
  File ""/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py"", line 2207, in _conv
    ""kernel"", filter_size + [total_arg_size_depth, num_features], dtype=dtype)
  File ""/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 1317, in get_variable
    constraint=constraint)
  File ""/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 1079, in get_variable
    constraint=constraint)
  File ""/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 417, in get_variable
    return custom_getter(**custom_getter_kwargs)
  File ""/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 235, in _rnn_get_variable
    variable = getter(*args, **kwargs)
  File ""/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 394, in _true_getter
    use_resource=use_resource, constraint=constraint)
  File ""/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 738, in _get_single_variable
    found_var.get_shape()))
ValueError: Trying to share variable conv_lstm_cell/kernel, but specified shape (5, 5, 4, 8) and found shape (5, 5, 3, 8).




------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: 
binary
- **TensorFlow version (use command below)**:
1.8.0
- **Python version**:
3.6
- **Bazel version (if compiling from source)**: 
-
- **GCC/Compiler version (if compiling from source)**:
-
- **CUDA/cuDNN version**:
- (using CPU version)
- **GPU model and memory**:
- (using CPU version)
- **Exact command to reproduce**:
See description above
- **Mobile device**:
N/A
"
21398,Questiona about Multi gpus using estimator in tensorflow 1.9,"Now I am using estimator to train models in multi gpus.

When I used the original sess.run() and trained in multi gpus before, I found that the batchnorm statistics cannot be shared between multi gpus and had a bad effect on the evaluation result.

So I change the code to tensorflow 1.9 and used estimator to train. 

Estimator can support a distributed multi-server environment without changing your model I am not sure if estimator support sharing the batchnorm statistics between multi gpus. And I think this is very important. 

So I want to make sure about this."
21395,"import_meta_graph + load model, then fail to model_export with signature","I was fail to export model for tf-serving, and this is my error message, but i hava no idea about it:
```
TypeError: Can't convert Operation 'in_table' to Tensor (target dtype=None, name=None, as_ref=True)
```
And those are part of my code:
```
        self.symbol2index = tf.contrib.lookup.MutableHashTable(
            key_dtype=tf.string,
            value_dtype=tf.int64,
            default_value=self.UNK_ID,
            shared_name=""in_table_share"",
            name=""in_table"",
            checkpoint=True)

        self.index2symbol = tf.contrib.lookup.MutableHashTable(
            key_dtype=tf.int64,
            value_dtype=tf.string,
            default_value=self.UNK,
            shared_name=""out_table_share"",
            name=""out_table"",
            checkpoint=True)
```
Save model with:
```
self.saver.save(sess, ""%s/model.ckpt"" % ckpt_dir, global_step=global_step)
```
Load with:
```
ckpt = tf.train.latest_checkpoint(checkpoint_dir=tfFLAGS.train_dir)
graph_from_meta=ckpt + "".meta""
self.saver = tf.train.import_meta_graph(graph_from_meta)
self.saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))
```
model export with:
```
        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(
            inputs={
                'question': question_placeholder,
                'answer': answer_placeholder,
                'question_len': qes_len_placeholder,
                'answer_len': ans_len_placeholder,
            },
            outputs={'output': out_infers},
            method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)

        # legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')
        builder.add_meta_graph_and_variables(
            sess,
            tags=[tf.saved_model.tag_constants.SERVING],
            signature_def_map={'prediction': prediction_signature},
        )
        builder.save()

```

I have try to rebuild graph with the same parameters instead load them with `import_meta_graph`, it works, but it not convenient. 

Hope anyone can help me, thanks!"
21394,"tflite: ""Unimplemented: this graph contains an operator of type TransposeConv for which the quantized form is not yet implemented"". So how long will that be supported?","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21391,tf.linspace doesn't except integers for start/stop and floats for num,"tf.linspace(start=1.0, stop=3.0, num=3) - works fine.
tf.linspace(start=1, stop=3, num=3) yields an error (because start and stop are integers)
tf.linspace(start=1.0, stop=3.0, num=3.0) yields an error (because num is float)
In both cases the error is: ""Could not find valid device for node name: ""LinSpace"" ""

I would expect the following behavior:
* start and stop can be integers (the op can convert it to float if fractions are produced; but see below)
* num can be float, if its modulo with 1 is zero (the op can convert it to int; but see below)
* the error message should reflect the problem precisely
* the comments and reference page should reflect the expected dtype more clearly (at the top, not just in args)

This behavior is how numpy linspace function works (including stating all optional dtype in the autocomplete while typing).

dtype conversion within a TF op might not follow TF design rules as it might create dtype changes without the user awareness. Yet, I would still be happy to have an error message that explicitly states the problem with the dtypes.


### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: See above"
21389,DistributionStrategy in tf.keras doesn't support multi input models,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian and macOS 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.11.0.dev20180805
- **Python version**: 3.6 and 3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: See below

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
@anj-s Huge thanks for adding support for distribution strategies to keras in 92279f8bfa6ce2124439aabfa6db84d722dc2b66 🎉 

While using it together with single input models works great, it fails with an uncaught exception when using it with multiple inputs:

### Source code / logs
Consider the following unit tests:
```python
import numpy as np

from tensorflow.contrib.distribute.python import mirrored_strategy
from tensorflow.python import keras
from tensorflow.python.data.ops import dataset_ops
from tensorflow.python.platform import test
from tensorflow.python.training import gradient_descent

class TrainingTest(test.TestCase):
  def test_dataset_input_tuples(self):
    with self.test_session():
      a = keras.layers.Input(shape=(3,), name='input_a')
      b = keras.layers.Input(shape=(4,), name='input_b')
      x = keras.layers.concatenate([a, b])
      y = keras.layers.Dense(5, name='dense')(x)

      model = keras.Model(inputs=[a, b], outputs=[y])
      model.compile(loss='mse', metrics=['mae'], optimizer='rmsprop')

      inputs_a = np.zeros((10, 3))
      inputs_b = np.zeros((10, 4))
      targets = np.zeros((10, 5))
      dataset = dataset_ops.Dataset.from_tensor_slices(((inputs_a,
                                                         inputs_b),
                                                        targets))
      dataset = dataset.repeat(100)
      dataset = dataset.batch(10)

      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)

  def test_distributed_dataset_input_tuples(self):
    with self.test_session():
      a = keras.layers.Input(shape=(3,), name='input_a')
      b = keras.layers.Input(shape=(4,), name='input_b')
      x = keras.layers.concatenate([a, b])
      y = keras.layers.Dense(5, name='dense')(x)
      model = keras.Model(inputs=[a, b], outputs=[y])

      optimizer = gradient_descent.GradientDescentOptimizer(0.001)
      strategy = mirrored_strategy.MirroredStrategy(['/device:GPU:1',
                                                     '/device:CPU:0'])

      model.compile(loss='mse',
                    metrics=['mae'],
                    optimizer=optimizer,
                    distribute=strategy)

      inputs_a = np.zeros((10, 3))
      inputs_b = np.zeros((10, 4))
      targets = np.zeros((10, 5))
      dataset = dataset_ops.Dataset.from_tensor_slices(((inputs_a,
                                                         inputs_b),
                                                        targets))
      dataset = dataset.repeat(100)
      dataset = dataset.batch(10)

      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)

if __name__ == '__main__':
  test.main()
```

While the first test works well, the second will fail with the following error:
```python-traceback
Epoch 1/1
2/2 [==============================] - 0s 55ms/step - loss: 0.0000e+00 - mean_absolute_error: 0.0000e+00
.WARNING:tensorflow:You are accessing attribute optimizerof theDistributedCallbackModel that may not have been setcorrectly.
/usr/local/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
  return _inspect.getargspec(target)
E.
======================================================================
ERROR: test_distributed_dataset_input_tuples (__main__.TrainingTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""tf_test.py"", line 57, in test_distributed_dataset_input_tuples
    model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1366, in fit
    validation_split=validation_split)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 794, in _standardize_user_data
    validation_split=validation_split)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 713, in _distribution_standardize_user_data
    validate_distributed_dataset_inputs(self._distribution_strategy, x, y)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/distributed_training_utils.py"", line 212, in validate_distributed_dataset_inputs
    ' are of type {}'.format(type(x)))
ValueError: Dataset input to the model should be tensors instead they are of type <class 'tuple'>

----------------------------------------------------------------------
Ran 3 tests in 0.914s

FAILED (errors=1)
```

The same is true when using a dictionary as an input.
Since I'm new to using distribution strategies, I'm unsure if this is just a problem with too strict input validation (as in #20753) or if this needs more work to support this use case.

Thanks for the great work!"
21388,[deleted],[deleted]
21384,_UnreadVariable name property fails in TF 1.10-rc1,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.10.0-rc0-18-ge5e9a8f4e9 1.10.0-rc1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: `tfe.Variable(1).assign(2).name` in eager mode.

### Describe the problem
In eager mode, the result of an assignment is an instance of the class `tensorflow.python.ops.resource_variable_ops._UnreadVariable`. When you call its `name` property, it tries to access `self._parent_op.name`, but its `_parent_op` attribute is `None`, so there is an exception. This is a regression in TF 1.10-rc1, since it worked fine in TF 1.9.0.

### Source code / logs

```python
import tensorflow as tf
import tensorflow.contrib.eager as tfe
tf.enable_eager_execution()

tfe.Variable(1).assign(2).name
```

Outputs the following exception:

```python
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/ageron/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1082, in name
    return self._parent_op.name
AttributeError: 'NoneType' object has no attribute 'name'
```

Since `str()` and `repr()` use the `name` property, they fail as well. This makes it impossible to call `assign()` in a Python shell, since the shell automatically tries to display the output:

```python
>>> import tensorflow as tf
>>> import tensorflow.contrib.eager as tfe
>>> tf.enable_eager_execution()
>>> tfe.Variable(1).assign(2)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/ageron/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 264, in __repr__
    self.name, self.get_shape(), self.dtype.name,
  File ""/Users/ageron/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1082, in name
    return self._parent_op.name
AttributeError: 'NoneType' object has no attribute 'name'
```"
21383,"Using tfrecord + dataset API  + estimator, how to add dynamic input to randomly augment data like in old version","I am now using tensorflow 1.9 and I want to use tfrecord + dataset API  + estimator.
However, I find a question.
For example, I use tfrecord to record the input image and label. However, in each training step, I want to generate random indices to delete the value of some pixels. The indices is generated in each step.

Before I used old version tensorflow, I can do the following
placeholder 1 : image from dataset
placeholder 2: label from dataset
placeholder 3: indices tf.random
And I can use tf.gather_nd(image, indice=inidices), so I can randomly augment the input image.

However, I do not know how to feed the random numpy array in tensorflow 1.9
"
21382,ImportError: No module named '_pywrap_tensorflow',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Microsoft Windows [Version 6.1.7601] 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: pip command
- **TensorFlow version (use command below)**:
- **Python version**: Python 3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I have researched most of the issue but it seems none of them resolves mine. 






### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Microsoft Windows [Version 6.1.7601]
Copyright (c) 2009 Microsoft Corporation.  All rights reserved.

C:\Users\Anmol Anand>python
Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)]
 on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\Anmol Anand\AppData\Local\Programs\Python\Python36-32\lib\site-
packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper

    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(_
_file__)])
  File ""C:\Users\Anmol Anand\AppData\Local\Programs\Python\Python36-32\lib\imp.p
y"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Anmol Anand\AppData\Local\Programs\Python\Python36-32\lib\site-
packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Anmol Anand\AppData\Local\Programs\Python\Python36-32\lib\site-
packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Anmol Anand\AppData\Local\Programs\Python\Python36-32\lib\site-
packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper

    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Anmol Anand\AppData\Local\Programs\Python\Python36-32\lib\site-
packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Anmol Anand\AppData\Local\Programs\Python\Python36-32\lib\site-
packages\tensorflow\python\__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Anmol Anand\AppData\Local\Programs\Python\Python36-32\lib\site-
packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper

    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(_
_file__)])
  File ""C:\Users\Anmol Anand\AppData\Local\Programs\Python\Python36-32\lib\imp.p
y"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Anmol Anand\AppData\Local\Programs\Python\Python36-32\lib\site-
packages\tensorflow\python\__init__.py"", line 66, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Anmol Anand\AppData\Local\Programs\Python\Python36-32\lib\site-
packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Anmol Anand\AppData\Local\Programs\Python\Python36-32\lib\site-
packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper

    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_st
arted/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>"
21381,Is it possible to pass a PyObject into tf.py_func?,"Some times we need to use some information inside tf.pyfunc, and its unnecessary to convert these information into basic types that can feed into tensorflow.

For example, I don't know how to pass a pyObject into  tf.py_func, so I must write like this:
```python
import pickle
from typing import List
import tensorflow as tf
import numpy as np

data: List[int] = [1, 2, 3]
extra_info: List[object] = [{1: ""Some python object that cannot be easily converted into basic types""},
                            {2: ""Some python object that cannot be easily converted into basic types""},
                            {3: ""Some python object that cannot be easily converted into basic types""}
                            ]


def some_func(one_data: int, one_info_serialized: str):
    one_extra_info = pickle.loads(one_info_serialized)
    # just imagine I will use the object
    print(one_extra_info)
    return np.int32(0)


data_pl = tf.placeholder(dtype=tf.int32, shape=())
info_pl = tf.placeholder(dtype=tf.string)
op = tf.py_func(some_func, [data_pl, info_pl], tf.int32)
session = tf.Session()

for one_data, one_info in zip(data, extra_info):
    session.run(op, feed_dict={data_pl: one_data,
                               # How can I pass this PyObject directly?
                               info_pl: pickle.dumps(one_info)})
```"
21380,tf.GradientTape.gradient raise error with tf.nn.relu6,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**: 1.10.0-rc1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
tf.GradientTape.gradient raise error with tf.nn.relu6

### Source code / logs

```python
import tensorflow as tf

tf.enable_eager_execution()
w = tf.contrib.eager.Variable([[1.0]])
with tf.GradientTape() as tape:
    loss = tf.nn.relu6(w * w)
grad = tape.gradient(loss, w)
```

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 858, in gradient
    output_gradients=output_gradients)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py"", line 63, in imperative_grad
    tape._tape, vspace, target, sources, output_gradients)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 116, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py"", line 387, in _Relu6Grad
    return gen_nn_ops.relu6_grad(grad, op.outputs[0])
TypeError: 'NoneType' object is not subscriptable
```
"
21379,Need solution to install TensorFlow on computer without using an Internet Connection.,"I want to install tensorflow on my pc. but due to internet connectivity issues my pc is not having an internet connection. So please tell me the solution to install into my pc. i am very new in the world of programming. So, please provide me the simple procedure of it. "
21377,Tflite does not seem to support LSTM.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.3.1611
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: cuda=8.0.61 cudnn=7.0.5
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See below.

### Source code / logs
See below.

### Describe the problem
I'm trying to convert a `.pb` graph into tflite format, which containes a bi-directional LSTM. However, whenever I run the command:
```
toco --graph_def_file=opt_frozen.pb \
--output_file= output.tflite \
--output_format=TFLITE \
--inference_type=FLOAT \
--inference_input_type=FLOAT \
--input_arrays=[input_tensor_names] \
--output_arrays=[output_tensor_names]
```
It failed with following error messages:
```
2018-08-03 14:35:41.259412: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.259490: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.271357: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.271401: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.271439: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.271454: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.279573: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.279626: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.279989: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.280020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.280043: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.280087: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3
2018-08-03 14:35:41.280107: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3
2018-08-03 14:35:41.280123: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.280141: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.280163: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LogicalAnd
2018-08-03 14:35:41.280175: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LoopCond
2018-08-03 14:35:41.280224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.280266: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.280303: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit
2018-08-03 14:35:41.280317: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3
2018-08-03 14:35:41.280340: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3
2018-08-03 14:35:41.280362: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ReverseV2
2018-08-03 14:35:41.280385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.280412: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.280433: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.280469: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3
2018-08-03 14:35:41.280487: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3
2018-08-03 14:35:41.280502: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.280519: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.280535: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LogicalAnd
2018-08-03 14:35:41.280546: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LoopCond
2018-08-03 14:35:41.280588: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.280626: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.280656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit
2018-08-03 14:35:41.280671: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3
2018-08-03 14:35:41.280693: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3
2018-08-03 14:35:41.280965: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3
2018-08-03 14:35:41.280982: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.280997: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3
2018-08-03 14:35:41.281074: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3
2018-08-03 14:35:41.281102: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ReverseV2
2018-08-03 14:35:41.281123: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3
2018-08-03 14:35:41.281138: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter
2018-08-03 14:35:41.281157: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3
2018-08-03 14:35:41.281228: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3
```
And I assume that LSTM is not currently supported in tflite.
So is that the case? And if it's true, is there any way that I can work around this problem?

"
21376,Tensorflow Lite Don't supoort squared_difference?,"I have made a model trained using tensorflow. 
When I transfer it to tensorflow lite, it has not supported squared difference operation!
How can I solve this problem?
Thanks very much!"
21372,Tensorflow r1.9 build failed to compile sparse_matmul_op.cc on gcc4.8.1 ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 6.9
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: tag r1.9
- **Python version**: Python 2.7.12 :: Anaconda 2.4.1 (64-bit)
- **Bazel version (if compiling from source)**: Build label: 0.16.0- (@non-git)
- **GCC/Compiler version (if compiling from source)**: gcc 4.8.1 / binutils 2.28
- **CUDA/cuDNN version**: cuda90/9.0.176, cudnn/7.1.3
- **GPU model and memory**: K80
- **Exact command to reproduce**: bazel build --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}"" --config=cuda //tensorflow/tools/pip_package:build_pip_package  --verbose_failures

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I cannot build tensorflow from sources.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

The exact command I can reproduce this bug is below:


> external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/core/kernels/_objs/sparse_matmul_op/sparse_matmul_op.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/kernels/_objs/sparse_matmul_op/sparse_matmul_op.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY -iquote . -iquote bazel-out/host/genfiles -iquote bazel-out/host/bin -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote bazel-out/host/bin/external/png_archive -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/genfiles/external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/host/genfiles/external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/host/genfiles/external/png_archive -isystem bazel-out/host/bin/external/png_archive -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/double_conversion -isystem bazel-out/host/genfiles/external/double_conversion -isystem bazel-out/host/bin/external/double_conversion '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' -msse3 -pthread '-DGOOGLE_CUDA=1' -c tensorflow/core/kernels/sparse_matmul_op.cc -o bazel-out/host/bin/tensorflow/core/kernels/_objs/sparse_matmul_op/sparse_matmul_op.pic.o >& console.txt

The console outputs are attached. 
[console.txt](https://github.com/tensorflow/tensorflow/files/2259203/console.txt)
"
21368,Not able to port a 6-layered mobilenet tflite model to mobile,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
Pixel 2
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.8
- **Python version**:
2.7
- **Bazel version (if compiling from source)**:
0.15.2
- **GCC/Compiler version (if compiling from source)**:
5.4.0
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
1. Set the mobileNet model endpoint to conv6-depthwise
2. Re-train the model from scratch with using cifar10 dataset
3. Freeze the graph with the checkpoints  
import tensorflow as tf
from tensorflow.python.framework import graph_util
import os,sys

output_node_names = ""MobilenetV1/Predictions/Reshape""
saver = tf.train.import_meta_graph('/home/users/saman/yitao/tensorflow_android/models/research/slim/batch_32/model.ckpt-156300.meta', clear_devices=True)
graph = tf.get_default_graph()
input_graph_def = graph.as_graph_def()
sess = tf.Session()
saver.restore(sess, ""/home/users/saman/yitao/tensorflow_android/models/research/slim/batch_32/model.ckpt-156300"")
output_graph_def = graph_util.convert_variables_to_constants(
            sess, # The session is used to retrieve the weights
            input_graph_def, # The graph_def is used to retrieve the nodes
            output_node_names.split("","") # The output node names are used to select the usefull nodes
)
output_graph=""frozen-model-conv6-bat-32.pb""
with tf.gfile.GFile(output_graph, ""wb"") as f:
    f.write(output_graph_def.SerializeToString())
sess.close()

(4) Optimize the model 
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=/home/yitao/TF_1.8/tensorflow/my_frozen_pb/frozen-model-conv6-bat-32.pb \
--out_graph=/home/yitao/TF_1.8/tensorflow/my_frozen_pb/frozen-model-conv6-bat-32-optimized.pb \
--inputs='input' \
--outputs='MobilenetV1/Predictions/Reshape' \
--transforms='
  strip_unused_nodes(type=float, shape=""1,32,32,3"")
  remove_nodes(op=Identity, op=CheckNumerics)
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms

(5) Convert the model to tflite
 bazel-bin/tensorflow/contrib/lite/toco/toco \
  --input_file=/home/yitao/TF_1.8/tensorflow/my_frozen_pb/frozen-model-conv6-bat-32-optimized.pb \
  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \
  --output_file=/home/yitao/TF_1.8/tensorflow/my_frozen_pb/frozen-model-conv6-bat-32-optimized.tflite --inference_type=FLOAT \
  --input_type=FLOAT --input_arrays=input \
  --seed2 \
  --output_arrays=MobilenetV1/Predictions/Reshape --input_shapes=1,32,32,3 \
  --allow_custom_ops

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Not able to train a model from scratch and port it Android to utilize the Android Nerual Network api through TFLite. After training a model and following the steps to convert the graph to tflite model, there are still some ops that are not supported by the TFLite runtime in my graph. What should I do? 
Any help is apreciated!

Logcat is throwing the following errors. It seems that those ops are not stripped from the model during the optimization step. 
### Source code / logs
 
08-03 15:14:52.183 10271-10271/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: main
    Process: android.example.com.tflitecamerademo, PID: 10271
    java.lang.RuntimeException: Unable to start activity ComponentInfo{android.example.com.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity}: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find custom op for name 'RandomUniform' with version 1
    Didn't find custom op for name 'FLOOR' with version 1
    Didn't find custom op for name 'RSQRT' with version 1
    Didn't find custom op for name 'FIFOQueueV2' with version 1
    Didn't find custom op for name 'QueueDequeueV2' with version 1
    Didn't find custom op for name 'SquaredDifference' with version 1
    Registration failed.


"
21367,[tf 1.8.0] tf.nn.swish would have issue when restoring using tf.saved_model.loader.load,"Hi,

I found an error for tf.nn.swish for tensorflow version **1.8.0**
First, I save the model using tf.saved_model API, then I restore the model using the same API. 

It turns out to have the following error:

---------------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
<ipython-input-3-a18708547b00> in <module>()
      1 sess = tf.Session(graph=tf.Graph())
----> 2 zzz = tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], 'model_saved')

/usr/local/lib/python2.7/dist-packages/tensorflow/python/saved_model/loader_impl.pyc in load(sess, tags, export_dir, **saver_kwargs)
    217 
    218     # Build a saver by importing the meta graph def to load.
--> 219     saver = tf_saver.import_meta_graph(meta_graph_def_to_load, **saver_kwargs)
    220 
    221     if saver:

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc in import_meta_graph(meta_graph_or_file, clear_devices, import_scope, **kwargs)
   1953       clear_devices=clear_devices,
   1954       import_scope=import_scope,
-> 1955       **kwargs)
   1956 
   1957   if meta_graph_def.HasField(""saver_def""):

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.pyc in import_scoped_meta_graph(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate)
    741         name=(import_scope or scope_to_prepend_to_names),
    742         input_map=input_map,
--> 743         producer_op_list=producer_op_list)
    744 
    745     # Restores all the other collections.

/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)
    430                 'in a future version' if date is None else ('after %s' % date),
    431                 instructions)
--> 432       return func(*args, **kwargs)
    433     return tf_decorator.make_decorator(func, new_func, 'deprecated',
    434                                        _add_deprecated_arg_notice_to_docstring(

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.pyc in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)
    487         try:
    488           results = c_api.TF_GraphImportGraphDefWithResults(
--> 489               graph._c_graph, serialized, options)  # pylint: disable=protected-access
    490           results = c_api_util.ScopedTFImportGraphDefResults(results)
    491         except errors.InvalidArgumentError as e:

FailedPreconditionError: Input 0 ('swish_f32') for 'MatMul_1' was not previously added to ShapeRefiner.

# my solution

def myswish_beta(x):
    """"""
    Swish with beta not-traininable!
    """"""
    beta=tf.Variable(initial_value=1.0,trainable=False,name='swish_beta')
    return x*tf.nn.sigmoid(beta*x)
activation_fun = myswish_beta"
21366,Build fails: ERROR: infinite symlink expansion detected,"I'm getting this error on FreeBSD, it fails in jemalloc even though I answered ""n"" when it asked if to build it:
```
workspace: /usr/ports/science/py-tensorflow/work-py27/tensorflow-1.10.0-rc1
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
ERROR: infinite symlink expansion detected
[start of symlink chain]
/usr/ports/science/py-tensorflow/work-py27/tensorflow-1.10.0-rc1/bazel_ot/2cc1156aa366d993caf19cae8ee5ba0d/external/org_tensorflow
/usr/ports/science/py-tensorflow/work-py27/tensorflow-1.10.0-rc1
[end of symlink chain]
ERROR: /usr/ports/science/py-tensorflow/work-py27/tensorflow-1.10.0-rc1/tensorflow/tools/pip_package/BUILD:123:1: error loading package '@jemalloc//': Encountered error while reading extension file 'third_party/common.bzl': no such package '@org_tensorflow//third_party': Could not access /usr/ports/science/py-tensorflow/work-py27/tensorflow-1.10.0-rc1/bazel_ot/2cc1156aa366d993caf19cae8ee5ba0d/external/org_tensorflow: Infinite symlink expansion and referenced by '//tensorflow/tools/pip_package:licenses'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: error loading package '@jemalloc//': Encountered error while reading extension file 'third_party/common.bzl': no such package '@org_tensorflow//third_party': Could not access /usr/ports/science/py-tensorflow/work-py27/tensorflow-1.10.0-rc1/bazel_ot/2cc1156aa366d993caf19cae8ee5ba0d/external/org_tensorflow: Infinite symlink expansion
INFO: Elapsed time: 5.260s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (75 packages loaded)
```"
21364,@tf.custom_gradient fails in eager execution mode,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX High Sierra 10.13.2
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: Running on CPU
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
@tf.custom_gradient does not work in eager execution mode although the documentation states it does.

### Source code / logs
Providing a minimal example to reproduce:
```python
import tensorflow as tf

tfe = tf.contrib.eager
tf.enable_eager_execution()

@tf.custom_gradient
def log1pexp(x):
    e = tf.exp(x)

    def grad(dy):
        return dy * (1 - 1 / (1 + e))
    return tf.log(1 + e), grad

with tf.GradientTape() as tape:
    x = tf.constant(100.)
    y = log1pexp(x)

print(tape.gradient(y, [x]))
# prints None as default derivative is undefined
# not working properly - it did not use the custom gradient func (and print 1.0)

# Graph mode is working OK!
# x = tf.constant(100.)
# y = log1pexp(x)
# dy = tf.gradients(y, x)
#
# with tf.Session() as sess:
#     print(sess.run(dy))
# # prints 1.0 - the custom derivative
```"
21363,tf.clip_by_global_norm() returns zero tensors if the norm is infinity,"I think this should return NaN tensors instead of zero tensors to signal an error. Users expect `tf.clip_by_global_norm()` to return tensors unaltered or with global norm matching the threshold passed into the function. This invariant is violated by returning zero tensors.

This is caused by `1.0 / use_norm` which is zero for an infinite norm in [clip_ops.py](https://github.com/tensorflow/tensorflow/blob/25c197e02393bd44f50079945409009dd4d434f8/tensorflow/python/ops/clip_ops.py#L257):

```python
scale = clip_norm * math_ops.minimum(
        1.0 / use_norm,
        constant_op.constant(1.0, dtype=use_norm.dtype) / clip_norm)
```

Boilerplate:

- Have I written custom code: No
- OS Platform and Distribution: N/A
- TensorFlow installed from: N/A
- TensorFlow version: master
- Bazel version: N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A
- Exact command to reproduce: N/A
- Mobile device: N/A"
21362,"WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".","Version 1.10.0-rc1
bazel-0.15.0"
21361,[tf.keras] Fit Generator & TensorBoard Callback with Stateful Metrics,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04/OSX
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0/1.10.0rc1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

### Describe the problem
When using ``fit_generator`` with ``TensorBoard`` callback and a ``StatefulMetric`` inside ``tf.keras``:

```
  File ""/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 841, in on_epoch_end
    summary_value.simple_value = value.item()
AttributeError: 'float' object has no attribute 'item'
```
**This issue was fixed in regular Keras** (https://github.com/keras-team/keras/pull/10673), but has not been reviewed (https://github.com/tensorflow/tensorflow/pull/21071) yet in TensorFlow

### Source code / logs

Heres a dummy example of fit_generator with a dummy stateful metric that demonstrates the error:

```python
import tensorflow as tf
from tensorflow.python.keras.layers import Input, Dense
from tensorflow.python.keras.models import Model
from tensorflow.python.ops import state_ops

import numpy as np

class BatchCounter(tf.keras.layers.Layer):

        def __init__(self, name=""batch_counter"", **kwargs):
            super(BatchCounter, self).__init__(name=name, **kwargs)
            self.stateful = True
            self.batches = tf.keras.backend.variable(value=0, dtype=""int32"")

        def reset_states(self):
            tf.keras.backend.set_value(self.batches, 0)

        def __call__(self, y_true, y_pred):
            current_batches = self.batches * 1
            self.add_update(
              state_ops.assign_add(self.batches,
                                   tf.keras.backend.variable(value=1, dtype=""int32"")))
            return current_batches + 1

class DummyGenerator(object):
    """""" Dummy data generator. """"""

    def run(self):
        while True:
            yield np.ones((10, 1)), np.zeros((10, 1))

train_gen = DummyGenerator()
val_gen = DummyGenerator()

# Dummy model
inputs = Input(shape=(1,))
outputs = Dense(1)(inputs)
model = Model(inputs=inputs, outputs=outputs)
model.compile(loss=""mse"", optimizer=""adam"", metrics=[BatchCounter()])

model.fit_generator(
    train_gen.run(),
    steps_per_epoch=5,
    epochs=200,
    validation_data=val_gen.run(),
    validation_steps=5,
    callbacks=[tf.keras.callbacks.TensorBoard()])
```
"
21359,Object detection: Unable to get mAP,"Right now I'm facing a problem which is unable to get the mAP value from my trained model.
I run below commands parallelly to monitor the mAP of my model.

Train.py
`python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_resnet101_kitti.config --num_clones=2 --ps_tasks=1`

Eval.py
`CUDA_VISIBLE_DEVICES="""" python eval.py --logtostderr --pipeline_config_path=training/faster_rcnn_resnet101_kitti.config --checkpoint_dir=training/ --eval_dir=images/test `

Tensorboard
`tensorboard --logdir=imag/test`

However in the tensorboard there is no scalar tab. There is only Images and Graphs tab. My ultimate goal is to calculate the detection accuracy of my model.
Please help. 

"
21358,[Bug]Using CUDA_VISIBLE_DEVICES for Specified GPU,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NA
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- **TensorFlow installed from (source or binary):binary
- **TensorFlow version (use command below):1.0.0
- **Python version:3.5
- **Bazel version (if compiling from source)**:NA
- **GCC/Compiler version (if compiling from source)**:NA
- **CUDA/cuDNN version**:8.0
- **GPU model and memory**:GTX1080,8G
- **Exact command to reproduce**:

### Describe the problem
I am trying to train two same structure segmentation model,so i build two tensorflow graphs on two GPU.But,when i use CUDA_VISIBLE_DEVICES specify GPU, i find there exist bug.so I'm trying to do some experiment：CUDA_VISIBLE_DEVICES  is 0,can detect GPU:0,but set the value is 1,can't detect GPU:1,only detec GPU:0,but set the value is 0,1,GPU:0,GPU:1 all can detected..the result is confused.

### Source code / logs
i attach the result.
![1](https://user-images.githubusercontent.com/28638716/43631453-d1eca760-9735-11e8-89da-63eb36502e04.PNG)
![2](https://user-images.githubusercontent.com/28638716/43631455-d31a2220-9735-11e8-8ac6-bdad3ee7ebbe.PNG)
![3](https://user-images.githubusercontent.com/28638716/43631457-d51375c2-9735-11e8-9191-db17eddd16e5.PNG)

anyone know how to do it?"
21357,dnn_dropout in DNNLinearCombinedClassifier doesn't  work,"I use DNNLinearCombinedClassifier to train a wide & deep model with dnn_dropout=0.5。
```
m = tf.estimator.DNNLinearCombinedClassifier(
    model_dir=model_dir,
    linear_feature_columns=crossed_columns,
    dnn_feature_columns=deep_columns,
    dnn_dropout=0.5,
    dnn_hidden_units=[512, 256, 256, 128])

m.train(
    input_fn=input_fn(train_file_name, num_epochs=None, shuffle=True, 
    is_eval=False,steps=train_steps, 
    hooks=[logger_hook])
```

 Then I evaluate model as follow: 
```
m = tf.estimator.DNNLinearCombinedClassifier(
    model_dir=model_dir,
    linear_feature_columns=crossed_columns,
    dnn_feature_columns=deep_columns,
    dnn_dropout=1.0,
    dnn_hidden_units=[512, 256, 256, 128])

results = m.evaluate(
    input_fn=input_fn(test_file_name, num_epochs=1, shuffle=False, is_eval=True),
    steps=None)
```
I find no matter I set dnn_dropout=0.1 or dnn_dropout=1.0, the result are same. It seems that this parameter doesn't work.
My version is 1.4"
21355,"""ValueError: If initializer is a constant, do not specify shape."" when calling dynamic_rnn","
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NA
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Debian 4.9.65
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:NA
- **TensorFlow version (use command below)**:1.8.0
- **Python version**:2.7.14
- **Bazel version (if compiling from source)**:NA
- **GCC/Compiler version (if compiling from source)**:NA
- **CUDA/cuDNN version**:9.1
- **GPU model and memory**:GTX 1080Ti, 11G
- **Exact command to reproduce**:

### Describe the problem
I am trying to implement a LSTM encoder with attention mechanism, the embedding matrix of input sentences with shape [batch_size, seq_length, embedding_size] is passed into a dynamic lstm layer. I'm trying to use the dynamic_rnn wrapper on a BasicLSTMCell. However, an error raised and I can't find any solution about the problem. 

### Source code / logs
The below codes are used to build the graph:
```
with tf.device('/cpu:0'), tf.variable_scope('embedding', reuse=tf.AUTO_REUSE):
    emb_w = tf.get_variable('emb_w', shape=[self.vocab_size, self.embedding_size], 
initializer=tf.random_uniform_initializer())
    embedded_chars = tf.nn.embedding_lookup(emb_w, input_x)
with tf.variable_scope('lstm', reuse=tf.AUTO_REUSE):
    self.lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self.rnn_hidden_size)
    self.lstm_out, _ = tf.nn.dynamic_rnn(self.lstm_cell, inputs=embedded_chars, dtype=tf.float32)
```
And I got such error:
```
Traceback (most recent call last):
  File ""src/tf_simnet.py"", line 252, in <module>
    tf.app.run()
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""src/tf_simnet.py"", line 243, in main
    train(args, config)
  File ""src/tf_simnet.py"", line 46, in train
    model = SimNet(config, args.encoder_type)
  File ""/data00/home/zhaodongdi/workspace/lab-speech/SimNet/src/model.py"", line 137, in __init__
    self.query_encoder = LSTMEncoder(self.query, config)
  File ""/data00/home/zhaodongdi/workspace/lab-speech/SimNet/src/model.py"", line 95, in __init__
    self.lstm_out, _ = tf.nn.dynamic_rnn(self.lstm_cell, inputs=embedded_chars, dtype=tf.float32)
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 627, in dynamic_rnn
    dtype=dtype)
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 824, in _dynamic_rnn_loop
    swap_memory=swap_memory)
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3224, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2956, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2893, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3194, in <lambda>
    body = lambda i, lv: (i + 1, orig_body(*lv))
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 795, in _time_step
    (output, new_state) = call_cell()
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 781, in <lambda>
    call_cell = lambda: cell(input_t, state)
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 339, in __call__
    *args, **kwargs)
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 699, in __call__
    self.build(input_shapes)
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 588, in build
    shape=[input_depth + h_depth, 4 * self._num_units])
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 546, in add_variable
    partitioner=partitioner)
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/training/checkpointable.py"", line 436, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1317, in get_variable
    constraint=constraint)
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1079, in get_variable
    constraint=constraint)
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 425, in get_variable
    constraint=constraint)
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 394, in _true_getter
    use_resource=use_resource, constraint=constraint)
  File ""/data00/home/wupeihao/anaconda4/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 718, in _get_single_variable
    raise ValueError(""If initializer is a constant, do not specify shape."")
ValueError: If initializer is a constant, do not specify shape.
```
Anyone knows how to solve this problem? Thanks a lot."
21352,tf.keras.model layer shows nothing,"tf.keras.Model.layers shows nothing?

![2018-08-03_115300](https://user-images.githubusercontent.com/26696338/43623464-e6dc594e-9713-11e8-8711-c403f6b20995.gif)

"
21351,The Chinese version tutorial miss the png image,"Hello,

I find the page https://www.tensorflow.org/get_started/eager in Chinese version miss the png image, which the link is https://www.tensorflow.org/get_started/eager_files/output_30_0.png. The position is under the ""可视化损失函数随时间推移而变化的情况"" part."
21349,Improve tf.data graph representation in TensorBoard,"Hello,

This is a feature request for making `tf.data` more insightful for TensorBoard. Currently, having a few `.apply` or `.map` and `.filter` calls looks like the image attached.

Although the internals of each `tf.data` inside the box look nice, it's hard to know from TensorBoard how they work together. Moreover, they are not descriptively named, so things get quite confusing.

Two suggestions come to mind:

- Having the ability to add descriptive names (contrasting the current ex: `__function_library__tf_map_func_YdsTEwCNzV0`)
- A graphical representation of how the different `tf.data` calls work together.

![image](https://user-images.githubusercontent.com/7721540/43617282-fe068ea8-9675-11e8-8e1a-c0f4eacff4b1.png)
"
21348,[Bug] cpu memory leak while using GPU with variable length ops.,"```
import tensorflow as tf
import numpy as np
import gc

def NotBuggy():
    with tf.device(""/gpu:0""):
        x = tf.placeholder(dtype=tf.float32, shape=(1000,1))
        col = tf.placeholder(dtype=tf.int32, shape=(None,1))

        p0 = tf.gather_nd( x,col)
        p1 = tf.gather_nd( x,col+1)
        diff = p0 - p1
    sess = tf.Session()
    co = 0
    while (True):
        feed_dict = {x: np.expand_dims(np.arange(1000),1), 
                     col: np.random.randint( 0,900, size=( np.random.randint(500,20000),1 ) ) }
        _res = sess.run(diff, feed_dict=feed_dict)
        co = co + 1
        if (co % 1000 == 0):
            gc.collect()
            print(_res.shape)
            print(co)


def Buggy():
    with tf.device(""/gpu:0""):
        x = tf.placeholder( dtype=tf.float32 , shape=(1000))
        ind = tf.placeholder( dtype=tf.int32, shape=(None) )

        p0 = tf.gather_nd(tf.expand_dims(x,1),tf.expand_dims(ind,1))
        p1 = tf.gather_nd(tf.expand_dims(x,1),tf.expand_dims(ind+1,1))

        diff = p0-p1

    sess = tf.Session()
    co = 0
    while(True):
        #Note that the length of ind is variable
        feed_dict = {x: np.arange(1000),ind: np.random.randint(0, 900, size=(np.random.randint(500, 20000)))}
        #When sized is fixed : doesn't memory leak
        #feed_dict = {x: np.arange(1000),ind: np.random.randint(0, 900, size=(20000,))}
        _res = sess.run(diff, feed_dict=feed_dict)
        co = co+1
        if( co % 1000 == 0):
            gc.collect()
            print(_res.shape)
            print(co)

#NotBuggy()
Buggy()
```

The code above leaks cpu memory quite rapidly ~10 Mb/s.
I tried some differential analysis, the bug seems weird and related to the graph construction.

Have Fun :)

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Lubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0
- **Python version**: Python 3.6.5 (default, May  3 2018, 10:08:28) (also bugs on Python 3.5.2 (default, Nov 17 2016, 17:05:23) )
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**:   CUDA Driver Version / Runtime Version          9.1 / 8.0 , cuDNN N/A
- **GPU model and memory**: NVidia GTx1080 ti 12Go
- **Exact command to reproduce**: Run code given above (It leaks memory quite rapidly ~10 Mb /s)
"
21347,Boosted Trees Regressor - incompatible tensor shapes,"Hi, I used the boosted trees regressor estimator and it gave results on the first run. Cleared my checkpoints  to rerun the model but the program then gave the following value error. Think that this may be a bug because the value error was not raised during my first run, and I am not sure how to fix this error.

````
Traceback (most recent call last):
  File ""main.py"", line 191, in <module>
tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/.../.../.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
_sys.exit(main(argv))
  File ""main.py"", line 170, in main
experiment.train_and_evaluate()
  File ""/.../.../.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 671, in train_and_evaluate
self.train(delay_secs=0)
  File ""/.../.../.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 389, in train
saving_listeners=self._saving_listeners)
  File ""/.../.../.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 881, in _call_train
saving_listeners=saving_listeners)
  File ""/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 366, in train
loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1119, in _train_model
return self._train_model_default(input_fn, hooks, saving_listeners)
 File ""/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1132, in _train_model_default
features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1107, in _call_model_fn
model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/canned/boosted_trees.py"", line 929, in _model_fn
n_batches_per_layer, config)
  File ""/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/canned/boosted_trees.py"", line 650, in _bt_model_fn
logits=logits)
File ""/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/canned/head.py"", line 239, in create_estimator_spec
regularization_losses))
  File ""/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/canned/head.py"", line 1506, in _create_tpu_estimator_spec
train_op = train_op_fn(regularized_training_loss)
  File ""/.../.../.local/lib/python3.6/site-packages/tensorflow/python/estimator/canned/boosted_trees.py"", line 611, in _train_op_fn
array_ops.stack(stats_summaries, axis=0), stamp_token)
  File ""/.../.../.local/lib/python3.6/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 1286, in apply_grad
grad.get_shape().assert_is_compatible_with(self._shape)
  File ""/.../.../.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 847, in assert_is_compatible_with
raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))
ValueError: Shapes (0,) and (0, 63, 2, 2) are incompatible
````

For reference, these are my feature columns.

```
a = tf.feature_column.categorical_column_with_hash_bucket(""a"", hash_bucket_size=1000)
b = tf.feature_column.numeric_column(""b"")
b_bucket = tf.feature_column.bucketized_column(b,  boundaries=np.arange(0,2644,100).tolist())
c= tf.feature_column.numeric_column(""c"")
c_bucket = tf.feature_column.bucketized_column(c,  boundaries=np.arange(0,210,10).tolist())
d = tf.feature_column.numeric_column(""d"")
d_bucket = tf.feature_column.bucketized_column(d,  boundaries=np.arange(0,245,10).tolist())
e = tf.feature_column.categorical_column_with_vocabulary_list(key=""e"", vocabulary_list =[""l"", ""m"", ""h""])
columns = [a, b_bucket, c_bucket, d_bucket, e]
feature_columns = [tf.feature_column.indicator_column(x) for x in columns]
```"
21345,import_graph_def ERROR when NodeDef contains NameAttrList (func) message defined in attr_value.proto - makes the graph unusable / unloadable in TF 1.8 or 1.9 (works in 1.7). ValueError: NodeDef mentions attr '<my_attr>' not in Op<>,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: binary 
- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0
- **Python version**: Python 3.5.2
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: See below.

### Describe the problem
Our simplified code `modify.py` below does the following: 

- Loads an existing TF graph from `graph1.pb` into `graph_def = tf.GraphDef()`
- To each node within the `graph_def`, adds user-defined attributes using `NameAttrList func` protobuf message in [attr_value.proto](https://github.com/tensorflow/tensorflow/blob/25c197e02393bd44f50079945409009dd4d434f8/tensorflow/core/framework/attr_value.proto#L40-L44).
- Saves out the modified `graph_def` to `graph2.pb`
- Loads this new `graph2.pb` and imports to default graph --> Works in TF 1.7, fails in TF 1.8/1.9

This flow works fine in TF 1.7. However due to an added constraint in TF 1.8 or 1.9, when importing a graph containing nodes with user-defined attributes, it fails as follows (full error log included below):
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'my_attr' not in Op<name=VariableV2; signature= -> ref:Ref(dtype); attr=shape:shape; attr=dtype:type; attr=container:string,default=""""; attr=shared_name:string,default=""""; is_stateful=true>; 
NodeDef: import/loss3_classifier/biases = VariableV2[_class=[""loc:@import/loss3_classifier/biases""], _output_shapes=[[1000]], container="""", dtype=DT_FLOAT, my_attr=attr_name[data1=8, data2=2], shape=[1000], shared_name=""""](). 
(Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
```

This is not an issue with the GraphDef interpreting binary mismatching with GraphDef-generating binary, since the same binary is used to generate and interpret. I believe this is failing because the check to ensure all attrs within the NodeDef are valid doesn't account for user-defined attributes given using `NameAttrList func` message in [attr_value.proto](https://github.com/tensorflow/tensorflow/blob/25c197e02393bd44f50079945409009dd4d434f8/tensorflow/core/framework/attr_value.proto#L40-L44). 

This is breaking some of our production code written in TF1.7, when migrating to TF1.9, since the graph is no longer loadable.

### Attachments
[graphs_attachment.zip](https://github.com/tensorflow/tensorflow/files/2254600/graphs_attachment.zip)
Contains:
- graph1.pb
- graph2.pb
- tensorboard events file for graph1.pb
- tensorboard events file for graph2.pb (can only generate using TF1.7, fails with TF1.8/1.9)
- modify.py script

### Source code / logs
#### modify.py
```python
import tensorflow as tf

def add_attr_to_tf_node(node, key, name, data):
  """""" Add a dictionary 'data' to a TF graph node,
  with optional label 'name'. Key is 'key'.
  Each element of 'data' should be a tf.AttrValue() instance
  e.g.  a1 = tf.AttrValue(i=8)
        a2 = tf.AttrValue(i=2)
        attrs = {""data1"": a1, ""data2"": a2}
  """"""
  if name is None:
    node.attr[key].CopyFrom(tf.AttrValue(func={""attr"": data}))
  else:
    node.attr[key].CopyFrom(tf.AttrValue(func={""name"": name, ""attr"": data}))

# Create empty graph_def
graph_def = tf.GraphDef()

# Parse/Load from graph1.pb
with tf.gfile.GFile('graph1.pb', 'rb') as f:
  graph_def.ParseFromString(f.read())

# Add user attributes to each node in graph_def
for node in graph_def.node:
  a1 = tf.AttrValue(i=8)
  a2 = tf.AttrValue(i=2)
  attrs = {""data1"": a1, ""data2"": a2}
  add_attr_to_tf_node(node, 'my_attr', 'attr_name', attrs)

# Serialize/Save modified graph_def to graph2.pb
with tf.gfile.GFile('graph2.pb', 'wb') as f:
  f.write(graph_def.SerializeToString())

# Parse/Load from graph2.pb
with tf.gfile.GFile('graph2.pb', 'rb') as f:
  graph_def.ParseFromString(f.read())

# Import graph_def to defalt graph --> WORKS IN TF-1.7, ERROR WITH TF-1.8/1.9
tf.import_graph_def(graph_def)
```
#### TensorFlow 1.7:
No issues. 

#### TensorFlow 1.8/1.9 (ERROR LOG):
```
Traceback (most recent call last):
  File ""/scratch/tensorflow_1p8/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 489, in import_graph_def
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'my_attr' not in Op<name=VariableV2; signature= -> ref:Ref(dtype); attr=shape:shape; attr=dtype:type; attr=container:string,default=""""; attr=shared_name:string,default=""""; is_stateful=true>; NodeDef: import/loss3_classifier/biases = VariableV2[_class=[""loc:@import/loss3_classifier/biases""], _output_shapes=[[1000]], container="""", dtype=DT_FLOAT, my_attr=attr_name[data1=8, data2=2], shape=[1000], shared_name=""""](). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""modify.py"", line 37, in <module>
    tf.import_graph_def(graph_def)
  File ""/scratch/tensorflow_1p8/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/scratch/tensorflow_1p8/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 493, in import_graph_def
    raise ValueError(str(e))
ValueError: NodeDef mentions attr 'my_attr' not in Op<name=VariableV2; signature= -> ref:Ref(dtype); attr=shape:shape; attr=dtype:type; attr=container:string,default=""""; attr=shared_name:string,default=""""; is_stateful=true>; NodeDef: import/loss3_classifier/biases = VariableV2[_class=[""loc:@import/loss3_classifier/biases""], _output_shapes=[[1000]], container="""", dtype=DT_FLOAT, my_attr=attr_name[data1=8, data2=2], shape=[1000], shared_name=""""](). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).
```"
21344,Fail to print Global Step as specified in documentation,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra version 10.13.6
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**: 3.6.5
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Mobile device**: N/A
- **Exact command to reproduce**:

```
global_step_tensor = tf.Variable(10, trainable=False, name='global_step')
sess = tf.Session()
print('global_step: %s' % tf.train.global_step(sess, global_step_tensor))
```

As found in: [https://www.tensorflow.org/api_docs/python/tf/train/global_step](https://www.tensorflow.org/api_docs/python/tf/train/global_step)

### Describe the problem
I am testing the command to print the global_step and fails 

### Source code / logs

> 2018-08-02 13:14:17.524910: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
> Traceback (most recent call last):
>   File ""/*my project*/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1322, in _do_call
>     return fn(*args)
>   File ""/*my project*/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1307, in _run_fn
>     options, feed_dict, fetch_list, target_list, run_metadata)
>   File ""/*my project*/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1409, in _call_tf_sessionrun
>     run_metadata)
> tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value global_step
> 	 [[Node: _retval_global_step_0_0 = _Retval[T=DT_INT32, index=0, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](global_step)]]
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""/*my project*/"", line *, in <module>
>     print('global_step: %s' % tf.train.global_step(sess, global_step_tensor))
>   File ""/*my project*/venv/lib/python3.6/site-packages/tensorflow/python/training/training_util.py"", line 67, in global_step
>     return int(sess.run(global_step_tensor))
>   File ""/*my project*/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
>     run_metadata_ptr)
>   File ""/*my project*/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
>     feed_dict_tensor, options, run_metadata)
>   File ""/*my project*/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
>     run_metadata)
>   File ""/*my project*/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
>     raise type(e)(node_def, op, message)
> tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value global_step
> 	 [[Node: _retval_global_step_0_0 = _Retval[T=DT_INT32, index=0, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](global_step)]]"
21343,Tensorboard cuts off tops of graphs,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: False
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
```
❯ lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 16.04.3 LTS
Release:	16.04
Codename:	xenial
```
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: False
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**:
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

Obviously, this won't help you, but I'll include for the record:
`tensorboard --logdir=.runs/tensorboard/multi-task-2d/continuous-goals/ --port=6002`

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

```
❯ cat tf_env.txt

== cat /etc/issue ===============================================
Linux rldl6 4.4.0-130-generic #156-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux rldl6 4.4.0-130-generic #156-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy             1.14.0      
protobuf          3.5.2.post1 
tensorflow        1.8.0       

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)

== cat /etc/issue ===============================================
Linux rldl6 4.4.0-130-generic #156-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux rldl6 4.4.0-130-generic #156-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy             1.14.0      
protobuf          3.5.2.post1 
tensorflow        1.8.0       

== check for virtualenv =========================================
True

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /home/ethanbro/.mujoco/mjpro150/bin:/usr/local/cuda-9.1/bin::/usr/lib/nvidia-390
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Thu Aug  2 10:42:05 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.12                 Driver Version: 390.12                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |
| 23%   23C    P8     8W / 250W |    789MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 108...  Off  | 00000000:03:00.0 Off |                  N/A |
| 23%   22C    P8     8W / 250W |      1MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |
| 23%   20C    P8     8W / 250W |      1MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX 108...  Off  | 00000000:83:00.0 Off |                  N/A |
| 23%   18C    P8     8W / 250W |      1MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     21306      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21321      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21356      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21374      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21380      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21382      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21391      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21402      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21412      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21421      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21425      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21426      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21430      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21434      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21435      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21439      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21442      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21446      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21447      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21449      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21450      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21451      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21453      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21454      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21456      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21457      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21460      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21472      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21474      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21479      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21480      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21484      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21498      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21519      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21529      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
|    0     21540      G   .../ethanbro/virtualenvs/sac/bin/python3.6    21MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart.so.9.1.85
/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.1/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
```

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

This is a minor issue, but tensorflow has a tendency to cut off the tops of peaking graph lines after clicking the ""Fit domain to data"" button:
<img width=""313"" alt=""screen shot 2018-08-02 at 10 46 20 am"" src=""https://user-images.githubusercontent.com/10344742/43591539-9523af3a-9641-11e8-9d8d-f7149f07fc04.png"">

Of course, these are the lines I usually care the most about. I find myself constantly setting smoothing to .9, clicking the ""Fit domain to data"" button, and then setting smoothing back to .99. Is this avoidable somehow?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21342,install TF using Anaconda,"Hi

Using ""Use pip in Anaconda"" installation method to install TF on ubuntu 16.04 system with Intel Xeon E5645 CPU, I test installation which give a error message ""Illegal instruction (core dumped)"". **Why?**

But when I use conda in Anaconda to install TF directly (""conda install tensorflow""), I can test installation successfully. **Can this installation method install TF-CPU and TF-GPU versions at the same time？**

Alan"
21341, Does tensorflow  support Xeon E5645 CPU ?,"Hi,

Does tensorflow version r1.9 support Intel Xeon E5645 CPU?

Thanks !
Alan"
21340,"tf.fold{r,l} no longer works on sequence of tensors","### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution**: Ubuntu 16.04
- **Mobile device**: N/A
- **TensorFlow installed from**: binary
- **TensorFlow version (use command below)**: `v1.9.0-0-g25c197e023`
- **Python version**: 3.5.2
- **Bazel version**: N/A
- **GCC/Compiler version**: N/A
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: GTX 1060 (6GB)
- **Exact command to reproduce**: see below

### Describe the problem

Starting from TensorFlow 1.9, it appears that `tf.fold{r,l}` functions no longer work on a list of tensors. The code snippet provided below works fine in 1.8 but not in 1.9 or 1.10-rc1. Do you confirm the bug? Thanks. 

### Source code / logs

```python
import tensorflow as tf

a = tf.constant([1, 2])
b = tf.constant([3, 4])
c = tf.foldl(lambda a, x: a * x, [a, b])

with tf.Session() as sess:
    print(sess.run(c))
```

should print `[3, 8]` but fails with:

```text
Traceback (most recent call last):
  File ""test/foldl.py"", line 5, in <module>
    c = tf.foldl(lambda a, x: a * x, [a, b])
  File ""<python>/envpy3/lib/python3.5/site-packages/tensorflow/python/ops/functional_ops.py"", line 145, in foldl
    swap_memory=swap_memory)
  File ""<python>/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3209, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""<python>/envpy3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2941, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""<python>/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2878, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""<python>/lib/python3.5/site-packages/tensorflow/python/ops/functional_ops.py"", line 138, in compute
    a = fn(a, elem_i)
  File ""test/foldl.py"", line 5, in <lambda>
    c = tf.foldl(lambda a, x: a * x, [a, b])
TypeError: can't multiply sequence by non-int of type 'list'
```"
21339,Error in impoerting tensorflow 1.9,"Hello!
I had installed tensorflow 1.5 for using g2p_seq2seq. But for some error using this, I decided to upgrade my tensorflow version to 1.9. when I use pip install --upgrade tensorflow everything is ok and I have Successfully installed tensorboard-1.9.0 tensorflow-1.9.0 in my CMD, but when I want to import tensorflow I have below error : 

>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\ida\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\ida\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\ida\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\ida\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\ida\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\ida\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#0>"", line 1, in <module>
    import tensorflow
  File ""C:\Users\ida\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\ida\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\ida\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\ida\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\ida\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\ida\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\ida\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\ida\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\ida\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

what should I do now???
I really need to help.

my platform is windows 10, my python version is 3.6.5 and I'm using cpu tensorflow.

Thanks in advance.
"
21338,Max pooling cause error on empty batch,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 3.10.0-693.2.2.el7.x86_64
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: Python 2.7.14 :: Anaconda
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: cuda==9.0, cudnn==7.0.4
- **GPU model and memory**: None
- **Exact command to reproduce**: See below

### Describe the problem
When batch_size is 0, max pooling operation seems to produce an unhandled cudaError_t status. It may cause subsequent operations fail with odd error message. That is extremely difficult to debug.

(This corner case bothers us, where we first extract some bounding boxes and then run traditional convolution operations on areas specified by them. The above error occurs in case that no bounding boxes are detected thus batch_size becomes 0. However, the python exception will be randomly thrown at following operation or following session run steps)

```python
import tensorflow as tf
import numpy as np

x = tf.placeholder(dtype=tf.float32, shape=[None, 4, 4, 1])
pool_op = tf.nn.pool(x, pooling_type=""MAX"", window_shape=[2, 2], strides=[1, 1], padding=""SAME"")

y = tf.placeholder(dtype=tf.float32, shape=[None])
other_op = tf.where(tf.equal(y, 1.0))

normal_data = np.zeros([1, 4, 4, 1], dtype=""float32"")
empty_data = np.zeros([0, 4, 4, 1], dtype=""float32"")

# cudaError is thread local, limit thread pool size to make it easy to reproduce
config = tf.ConfigProto()
config.inter_op_parallelism_threads = 1
with tf.Session(config=config) as sess:
    # run other_op success
    print sess.run(other_op, {y: [1.0, 2.0, 3.0, 4.0]})  # [[0]]

    # run pooling on datas success
    print sess.run(pool_op, {x: normal_data}).shape  # (1, 4, 4, 1)
    print sess.run(pool_op, {x: empty_data}).shape  # (0, 4, 4, 1)

    # run other_op now failed
    print sess.run(other_op, {y: [1.0, 2.0, 3.0, 4.0]})  # err
``` 

Above code report error:
tensorflow.python.framework.errors_impl.InternalError: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true / nonzero indices.  temp_storage_bytes: 1, status: invalid configuration argument

""invalid configuration argument"" seems to be message return by cudaGetError, which indicates a failed kernel launch due to zero or too large number of block threads.

### Source code / logs
![image](https://user-images.githubusercontent.com/7600935/43579974-6dd4967a-9686-11e8-9b22-8288159d155c.png)

"
21337,Problem with softmax on Windows OS,"------------------------
### System information
Windows 10 x64 pro 17314.365, i5 6600k, Z170(clevo), GTX1070(notebook), 16G DDR4 2400 Dual Channel .
Tensorflow 1.9.0 with AVX2 simd, CUDA9.2.148, Cudnn7.1.4, py3.6.6 , VS2017 15.7(with cmake), CP/SM6.1
Tensorflow 1.9.0 offical, CUDA9.0, Cudnn7.05, py3.6.6

### Describe the problem
softmax_cross_entropy_with_logits or softmax_cross_entropy_with_logits_v2 can not be computed on GPU, but sparse_softmax_cross_entropy_with_logits can be computed on GPU. I could add with device cpu to solve the problem. But it seems simlar to https://github.com/tensorflow/models/issues/2803.
The problem may be with softmax kernel, and allow_soft_placement will also solve? 


### Source code / logs
https://github.com/golbin/TensorFlow-Multi-GPUs/blob/master/many-GPUs-MNIST.py
`import datetime

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.python.client import device_lib


def check_available_gpus():
    local_devices = device_lib.list_local_devices()
    gpu_names = [x.name for x in local_devices if x.device_type == 'GPU']
    gpu_num = len(gpu_names)

    print('{0} GPUs are detected : {1}'.format(gpu_num, gpu_names))

    return gpu_num


def model(X, reuse=False):
    with tf.variable_scope('L1', reuse=reuse):
        L1 = tf.layers.conv2d(X, 64, [3, 3], reuse=reuse)
        L1 = tf.layers.max_pooling2d(L1, [2, 2], [2, 2])
        L1 = tf.layers.dropout(L1, 0.7, True)

    with tf.variable_scope('L2', reuse=reuse):
        L2 = tf.layers.conv2d(L1, 128, [3, 3], reuse=reuse)
        L2 = tf.layers.max_pooling2d(L2, [2, 2], [2, 2])
        L2 = tf.layers.dropout(L2, 0.7, True)

    with tf.variable_scope('L2-1', reuse=reuse):
        L2_1 = tf.layers.conv2d(L2, 128, [3, 3], reuse=reuse)
        L2_1 = tf.layers.max_pooling2d(L2_1, [2, 2], [2, 2])
        L2_1 = tf.layers.dropout(L2_1, 0.7, True)

    with tf.variable_scope('L3', reuse=reuse):
        L3 = tf.contrib.layers.flatten(L2_1)
        L3 = tf.layers.dense(L3, 1024, activation=tf.nn.relu)
        L3 = tf.layers.dropout(L3, 0.5, True)

    with tf.variable_scope('L4', reuse=reuse):
        L4 = tf.layers.dense(L3, 256, activation=tf.nn.relu)

    with tf.variable_scope('LF', reuse=reuse):
        LF = tf.layers.dense(L4, 10, activation=None)

    return LF


if __name__ == '__main__':
    # need to change learning rates and batch size by number of GPU
    batch_size = 10000
    learning_rate = 0.001
    total_epoch = 10

    gpu_num = check_available_gpus()

    X = tf.placeholder(tf.float32, [None, 28, 28, 1])
    Y = tf.placeholder(tf.float32, [None, 10])

    losses = []
    X_A = tf.split(X, int(gpu_num))
    Y_A = tf.split(Y, int(gpu_num))

    '''
    Multi GPUs Usage
    Results on P40
     * Single GPU computation time: 0:00:22.252533
     * 2 GPU computation time: 0:00:12.632623
     * 4 GPU computation time: 0:00:11.083071
     * 8 GPU computation time: 0:00:11.990167
     
    Need to change batch size and learning rates
         for training more efficiently
    
    Reference: https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf
    '''
    for gpu_id in range(int(gpu_num)):
        with tf.device(tf.DeviceSpec(device_type=""GPU"", device_index=gpu_id)):
            with tf.variable_scope(tf.get_variable_scope(), reuse=(gpu_id > 0)):
                cost = tf.nn.softmax_cross_entropy_with_logits(
                                logits=model(X_A[gpu_id], gpu_id > 0),
                                labels=Y_A[gpu_id])
                losses.append(cost)

    loss = tf.reduce_mean(tf.concat(losses, axis=0))

    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(
        loss, colocate_gradients_with_ops=True)  # Important!

    init = tf.global_variables_initializer()
    sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))
    sess.run(init)

    mnist = input_data.read_data_sets('/tmp/tensorflow/mnist/input_data', one_hot=True)
    total_batch = int(mnist.train.num_examples/batch_size)
    print(""total: %s, %s, %s"" % (mnist.train.num_examples, total_batch, batch_size))

    start_time = datetime.datetime.now()

    for epoch in range(total_epoch):
        total_cost = 0

        for i in range(total_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            batch_xs = batch_xs.reshape(-1, 28, 28, 1)
            _, cost_val = sess.run([optimizer, loss],
                                   feed_dict={X: batch_xs,
                                              Y: batch_ys})
            total_cost += cost_val

        print(""total cost : %s"" % total_cost)

    print(""--- Training time : {0} seconds /w {1} GPUs ---"".format(
        datetime.datetime.now() - start_time, gpu_num))`

Logs:

`F:\Anaconda3\envs\tf\python.exe F:/ODT/Leaf/leafmg.py
1.9.0
2018-08-02 17:25:58.253241: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1392] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:01:00.0
totalMemory: 8.00GiB freeMemory: 6.62GiB
2018-08-02 17:25:58.253526: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1471] Adding visible gpu devices: 0
2018-08-02 17:26:43.721732: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-02 17:26:43.721924: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:958]      0 
2018-08-02 17:26:43.722057: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   N 
2018-08-02 17:26:43.722281: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/device:GPU:0 with 6394 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
1 GPUs are detected : ['/device:GPU:0']
WARNING:tensorflow:From F:/ODT/Leaf/leafmg.py:81: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See @{tf.nn.softmax_cross_entropy_with_logits_v2}.

2018-08-02 17:26:44.358873: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1471] Adding visible gpu devices: 0
2018-08-02 17:26:44.359084: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-08-02 17:26:44.359266: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:958]      0 
2018-08-02 17:26:44.359392: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   N 
2018-08-02 17:26:44.359563: I c:\users\user\source\repos\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6394 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""F:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""F:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\client\session.py"", line 1305, in _run_fn
    self._extend_graph()
  File ""F:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\client\session.py"", line 1340, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'softmax_cross_entropy_with_logits_sg': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Colocation Debug Info:
Colocation group had the following types and devices: 
SoftmaxCrossEntropyWithLogits: GPU CPU 
Identity: GPU CPU 
ZerosLike: CPU 
Const: GPU CPU 
ExpandDims: GPU CPU 
Neg: GPU CPU 
Mul: GPU CPU 
LogSoftmax: CPU 

Colocation members and user-requested devices:
  softmax_cross_entropy_with_logits_sg (SoftmaxCrossEntropyWithLogits) /device:GPU:0
  gradients/zeros_like (ZerosLike) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims/dim (Const) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims (ExpandDims) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/mul (Mul) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/LogSoftmax (LogSoftmax) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/Neg (Neg) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1/dim (Const) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1 (ExpandDims) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/mul_1 (Mul) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency (Identity) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency_1 (Identity) /device:GPU:0

Registered kernels:
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

	 [[Node: softmax_cross_entropy_with_logits_sg = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/device:GPU:0""](softmax_cross_entropy_with_logits_sg/Reshape, softmax_cross_entropy_with_logits_sg/Reshape_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""F:/ODT/Leaf/leafmg.py"", line 91, in <module>
    sess.run(init)
  File ""F:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""F:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""F:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""F:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'softmax_cross_entropy_with_logits_sg': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Colocation Debug Info:
Colocation group had the following types and devices: 
SoftmaxCrossEntropyWithLogits: GPU CPU 
Identity: GPU CPU 
ZerosLike: CPU 
Const: GPU CPU 
ExpandDims: GPU CPU 
Neg: GPU CPU 
Mul: GPU CPU 
LogSoftmax: CPU 

Colocation members and user-requested devices:
  softmax_cross_entropy_with_logits_sg (SoftmaxCrossEntropyWithLogits) /device:GPU:0
  gradients/zeros_like (ZerosLike) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims/dim (Const) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims (ExpandDims) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/mul (Mul) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/LogSoftmax (LogSoftmax) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/Neg (Neg) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1/dim (Const) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1 (ExpandDims) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/mul_1 (Mul) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency (Identity) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency_1 (Identity) /device:GPU:0

Registered kernels:
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

	 [[Node: softmax_cross_entropy_with_logits_sg = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/device:GPU:0""](softmax_cross_entropy_with_logits_sg/Reshape, softmax_cross_entropy_with_logits_sg/Reshape_1)]]

Caused by op 'softmax_cross_entropy_with_logits_sg', defined at:
  File ""F:/ODT/Leaf/leafmg.py"", line 81, in <module>
    labels=Y_A[gpu_id])
  File ""F:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\util\deprecation.py"", line 250, in new_func
    return func(*args, **kwargs)
  File ""F:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 1968, in softmax_cross_entropy_with_logits
    labels=labels, logits=logits, dim=dim, name=name)
  File ""F:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 1879, in softmax_cross_entropy_with_logits_v2
    precise_logits, labels, name=name)
  File ""F:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 7738, in softmax_cross_entropy_with_logits
    name=name)
  File ""F:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""F:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""F:\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\framework\ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'softmax_cross_entropy_with_logits_sg': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Colocation Debug Info:
Colocation group had the following types and devices: 
SoftmaxCrossEntropyWithLogits: GPU CPU 
Identity: GPU CPU 
ZerosLike: CPU 
Const: GPU CPU 
ExpandDims: GPU CPU 
Neg: GPU CPU 
Mul: GPU CPU 
LogSoftmax: CPU 

Colocation members and user-requested devices:
  softmax_cross_entropy_with_logits_sg (SoftmaxCrossEntropyWithLogits) /device:GPU:0
  gradients/zeros_like (ZerosLike) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims/dim (Const) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims (ExpandDims) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/mul (Mul) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/LogSoftmax (LogSoftmax) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/Neg (Neg) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1/dim (Const) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/ExpandDims_1 (ExpandDims) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/mul_1 (Mul) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency (Identity) /device:GPU:0
  gradients/softmax_cross_entropy_with_logits_sg_grad/tuple/control_dependency_1 (Identity) /device:GPU:0

Registered kernels:
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

	 [[Node: softmax_cross_entropy_with_logits_sg = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/device:GPU:0""](softmax_cross_entropy_with_logits_sg/Reshape, softmax_cross_entropy_with_logits_sg/Reshape_1)]]


Process finished with exit code 1
`
"
21336,Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/conv.cc:191 input->dims->size != 4 (1 != 4)Node 0 failed to prepare,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21335,Possible bug / Documentation Suggestion - Dataset API + Estimator API does not throw an exception when the input filepath to the dataset is incorrect,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: CUDA 9.0  / CUDNN 7.1.4.18
- **GPU model and memory**: GTX 1080 Ti - 9710 MB
- **Exact command to reproduce**: See below

### Describe the problem
When using the Dataset API along with the Estimator API, if the input file path to the dataset, while initializing **tf.data.TFRecordDataset** or **tf.data.FixedLengthRecordDataset**, is incorrect/ file does not exist, it does not immediately throw an exception. Instead the entire network graph is constructed, weights are initialized and checkpoints (if present) are loaded.

The only indication that something is wrong is when the session.run is called inside the function **_train_with_estimator_spec** in the train loop of the estimator.** The loss remains _None_ and the global_step is not updated**. Moreover, the training runs for as many epochs as requested with nothing being updated. 

However, even if one were to use the Dataset API with an incorrect file path but still overwrite the features and labels returned by it with manually set values, the loss is still not computed and remains as _None_ and the same problem as above persists. This is very counter-intuitive and hard to debug.

It would be better if the Dataset API somehow threw an exception in the beginning itself. If this is not possible, it would be nice if this was documented somewhere.

### Source code

1. Clone the tensorflow models from the official repository.
`git clone https://github.com/tensorflow/models.git tfmodels`
`cd tfmodels`

2. Run the **cifar10_download_and_extract.py** file to download and extract the cifar10 dataset.
3. Run the **cifar10_main.py** file to check that the original model is running correctly.

4. Delete the .bin files inside the **cifar-10-batches-bin** folder that was created in step 2. Keep the folder but only delete the contents inside it. By doing so we are passing an empty string to the dataset initializer and hence the dataset iterator is probably returning garbage values.
5. Re-run step 3 and you see the error described.

6. Make a copy of the file **cifar10_main.py** and edit it by adding the following lines just before the return statement in the function **parse_record**.
`image = tf.constant(value=0.0, shape=[32, 32, 3], dtype=tf.float32)`
`label = tf.constant(value=0, shape=[], dtype=tf.int32)`
By doing so we are still using an incorrect initializer but are now replacing the garbage values returned by the reader with known constants (zeros in this case).
7. Re-run the modified file and you still see the error described."
21334,"""could not initialize a memory descriptor"" error using tensorflow on windows using CMake","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 (build 17134) 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.15
- **GCC/Compiler version (if compiling from source)**: CMake > VS 2017
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: `Session::run()`

### Describe the problem

I've built tensorflow (r1.9, using the CMake tools) linked with MKL (v2018 U3) and mkl-dnn (v0.15). I'm running windows 10 (build 17134) on an Intel Core i7-7820HQ CPU. 

I've built mkl-dnn from source and it's test are passing. However, a C++ project that loads a pre-trained tensorflow graph and passes an image for inference gives the following error when calling `Session::run(...)` :
```
W d:\dev\tensorflow\tensorflow\core\framework\op_kernel.cc:1318] OP_REQUIRES failed at mkl_conv_ops.cc:888 : Aborted: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file d:\dev\tensorflow\tensorflow\core\kernels\mkl_conv_ops.cc:886
Aborted: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file d:\dev\tensorflow\tensorflow\core\kernels\mkl_conv_ops.cc:886
         [[Node: conv1/BiasAdd = _MklConv2DWithBias[T=DT_FLOAT, _kernel=""MklOp"", data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""VALID"", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](conv1_pad/Pad, conv1/kernel, conv1/bias, DMT/_0, DMT/_1, DMT/_2)]]
```
The same code (with the same pre-trained model) does work on a linux machine (Intel(R) Xeon(R) CPU E5-2673 v3, running on Microsoft Azure), also with TF built with mkl-dnn.

### Source code / logs

Install MKL 2018 U3 and build MKL-DNN v0.15 from source.

Build TF r1.9 with the following flags:
```
tensorflow_BUILD_ALL_KERNERS=ON
tensorflow_BUILD_CONTRIB_KERNELS=ON
tensorflow_BUILD_SHARED_LIB=ON
tensorflow_ENABLE_GPU=OFF
tensorflow_ENABLE_GRPC_SUPPORT=ON
tensorflow_ENABLE_JEMALLOC_SUPPORT=OFF
tensorflow_ENABLE_MKLDNN_SUPPORT=ON
tensorflow_ENABLE_MKL_SUPPORT=ON
tensorflow_ENABLE_POSITION_INDEPENDENT_CODE=ON
tensorflow_ENABLE_SNAPPY_SUPPORT=ON
tensorflow_OPTIMIZE_FOR_NATIVE_ARCH=ON
tensorflow_WIN_CPU_SIMD_OPTIONS=ON
```

Build passes and generates `tensorflow.dll`. Use keras to save a pre-trained Resnet50 model and attempt inference from C++ (linked with `tensorflow.dll`) as follows:
```
Session* session;
Status status = NewSession(SessionOptions(), &session);

GraphDef graph_def;
status = ReadBinaryProto(Env::Default(), ""../graph/resnet50.pb"", &graph_def);
status = session->Create(graph_def);

Mat img = imread(""../elephant.png""), imgFloat, imgResized;
img.convertTo(imgFloat, CV_32FC3);

Tensor tInput(DT_FLOAT, TensorShape({ 1, 224, 224, 3 }));
auto input_tensor_mapped = tInput.tensor<float, 4>();

Mat imgTensor(224, 224, CV_32FC3);
resize(imgFloat, imgTensor, Size(224, 224));
imgTensor -= Scalar(103.939, 116.779, 123.68);
	
auto source_data = imgTensor.ptr<float>(0);
int width = imgTensor.cols, height = imgTensor.rows, depth=3;
for (int y = 0; y < height; ++y) {
	for (int x = 0; x < width; ++x) {
		for (int c = 0; c < depth; ++c) {
			const float* source_value = source_data + (3*width*y) + (3*x) + (2-c);
			input_tensor_mapped(0, y, x, c) = *source_value;
		}
	}
}

std::vector<std::pair<string, tensorflow::Tensor>> inputs = {
	{ ""input_1"", tInput },
};

std::vector<tensorflow::Tensor> outputs;
status = session->Run(inputs, { ""fc1000/Softmax"" }, {}, &outputs);
if (!status.ok()) {
	std::cout << status.ToString() << ""\n"";
	return 1;
}
```"
21333,the grappler module is not accessible?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21332,TensorFlow 1.9.0 fails on aarch64 platforms,"Building TensorFlow 1.9.0 from source fails on aarch64 platforms (NVIDIA TX1 and Linaro HiKey960) due to using an inappropriate compiler flag (`-mfpu=neon`):
```
ERROR: /home/anton/CK_TOOLS/lib-tensorflow-src-cpu-1.9-compiler.python-2.7.12-linux-64/src/tensorflow/contrib/lite/kernels/internal/BUILD:359:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels/internal:tensor_utils' failed (Exit 1)
gcc: error: unrecognized command line option '-mfpu=neon'
```

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A (a build problem)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 (JetPack 3.1)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: 5.5.0
- **CUDA/cuDNN version**: N/A (CPU ony)
- **GPU model and memory**: N/A (CPU only)
- **Exact command to reproduce**: Install dependencies and build via [CK-TensorFlow](github.com/ctuning/ck-tensorflow):
```
$ sudo apt install liblapack-dev libatlas-dev
$ sudo pip install enum34 mock pillow wheel absl-py scipy ck
$ ck pull repo:ck-tensorflow
$ ck install ck-env:package:tool-bazel-0.15.2-linux
$ ck install package:lib-tensorflow-1.9.0-src-cpu --env.CK_HOST_CPU_NUMBER_OF_PROCESSORS=1
```
**NB:** Restricting the number of building processes to 1 is necessary to prevent running out of memory on the NVIDIA TX1 platform (4 GB and no swap enabled) or similar.

### Describe the problem

Building TensorFlow 1.9.0 fails on aarch64 platforms (NVIDIA TX1 and Linaro HiKey960). Similar instructions for TensorFlow 1.7.0 with Bazel 0.11.1 worked well:
```
$ ck install package:lib-tensorflow-1.7.0-src-cpu --env.CK_HOST_CPU_NUMBER_OF_PROCESSORS=1
$ ck install package:lib-tensorflow-1.7.0-src-cpu-xla --env.CK_HOST_CPU_NUMBER_OF_PROCESSORS=1

$ ck show env --tags=tensorflow,v1.7
Env UID:         Target OS: Bits: Name:                                       Version: Tags:

ed191cc45dda7ee4   linux-64    64 TensorFlow library (from sources, cpu, xla) 1.7      64bits,bazel,channel-stable,host-os-linux-64,lib,needs-bazel,needs-bazel-0.11.1,target-os-linux-64,tensorflow,tensorflow-cpu,v1,v1.7,v1.7.0,vcpu,vsrc,vxla
ed191cc45dda7ee3   linux-64    64 TensorFlow library (from sources, cpu)      1.7      64bits,bazel,channel-stable,host-os-linux-64,lib,needs-bazel,needs-bazel-0.11.1,target-os-linux-64,tensorflow,tensorflow-cpu,v1,v1.7,v1.7.0,vcpu,vsrc
```
They also worked for TensorFlow 1.8.0 with Bazel 0.13.0 after [including a patch](
https://github.com/tensorflow/tensorflow/issues/18643#issuecomment-386245419
) into the CK package:

```
$ sudo apt install liblapack-dev libatlas-dev
$ sudo pip install enum34 mock pillow wheel absl-py scipy ck
$ ck pull repo:ck-tensorflow
$ ck install ck-env:package:tool-bazel-0.13.0-linux
$ ck install package:lib-tensorflow-1.8.0-src-cpu [--env.CK_HOST_CPU_NUMBER_OF_PROCESSORS=1]
```

### Source code / logs
After about 10 hours, the build failed:
```
ERROR: /home/anton/CK_TOOLS/lib-tensorflow-src-cpu-1.9-compiler.python-2.7.12-linux-64/src/tensorflow/contrib/lite/kernels/internal/BUILD:359:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels/internal:tensor_utils' failed (Exit 1)
gcc: error: unrecognized command line option '-mfpu=neon'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
```"
21331,Inconsistent behavior for decode_bmp() with channels specification,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 3.10.0-693.2.2.el7.x86_64
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: Python 2.7.14 :: Anaconda
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: See below

### Describe the problem
We rely on tensorflow.image.decode_xxx() apis to load images. We expect a fixed channel size for decoded images and below code works:

```python
import tensorflow as tf
data = tf.placeholder(dtype=tf.string, shape=())
image = tf.image.decode_jpeg(data, channels=3)
```

However, it does not work for tensorflow.image.decode_bmp() for every images. For example, a bmp image with channel model ""L"":
[my_image.bmp.zip](https://github.com/tensorflow/tensorflow/files/2252717/my_image.bmp.zip)


```python
import tensorflow as tf
data = tf.placeholder(dtype=tf.string, shape=())

image_no_spec = tf.image.decode_bmp(data)
image = tf.image.decode_bmp(data, channels=3)

my_image = open(""my_image.bmp"", ""rb"").read()
with tf.Session() as sess:
    res = sess.run(image_no_spec, {data: my_image})
    print ""Decode shape: "", res.shape   # (666, 1000, 1)

    res = sess.run(image, {data: my_image})
    print ""Decode shape: "", res.shape  # err
```

Since tensorflow.image.decode_image() is a wrapper for decode_xxx() api(s), it may benefits if we can specify channels for all formats safely.

### Source code / logs

Decode shape:  (666, 1000, 1)
Traceback (most recent call last):
  File ""a.py"", line 12, in <module>
    res = sess.run(image, {data: my_image})
  File ""/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1137, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1355, in _do_run
    options, run_metadata)
  File ""/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1374, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: channels attribute 3 does not match bits per pixel from file 1
	 [[Node: DecodeBmp_1 = DecodeBmp[channels=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_Placeholder_0_0)]]

Caused by op u'DecodeBmp_1', defined at:
  File ""a.py"", line 5, in <module>
    image = tf.image.decode_bmp(data, channels=3)
  File ""/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_image_ops.py"", line 513, in decode_bmp
    ""DecodeBmp"", contents=contents, channels=channels, name=name)
  File ""/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3271, in create_op
    op_def=op_def)
  File ""/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): channels attribute 3 does not match bits per pixel from file 1
	 [[Node: DecodeBmp_1 = DecodeBmp[channels=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_Placeholder_0_0)]]

"
21330,"[keras]The model size is 250MB, but it takes up 800MB of memory after loading","The keras model(h5) size is 250MB, but it takes up 800MB of memory after loading

does anyone can explain the reason?"
21328,FusedBatchNorm of TF 1.9 doesn't work fine ,"After importing the frozen model [mobilenet_v2_1.0_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz), I find there are 4 outputs of FusedBatchNorm operation and I can't eval() other 3 tensors:

### System information
- **Ubuntu 18.04 desktop
- **TensorFlow installed from (source or binary)**: binary(pip)
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 2.7.15rc1
- **CUDA/cuDNN version**: CPU only
- **Exact command to reproduce**:
```
ipdb> batch_norm = tf.get_default_graph().get_operation_by_name('MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm')                  
ipdb> batch_norm.outputs
[<tf.Tensor 'MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm:0' shape=(1, 112, 112, 32) dtype=float32>, <tf.Tensor 'MobilenetV2/expan
ded_conv/depthwise/BatchNorm/FusedBatchNorm:1' shape=(32,) dtype=float32>, <tf.Tensor 'MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNor
m:2' shape=(32,) dtype=float32>, <tf.Tensor 'MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm:3' shape=(32,) dtype=float32>, <tf.Tenso
r 'MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm:4' shape=(32,) dtype=float32>]                                                   
ipdb> batch_norm.outputs[1].eval()
Optimizing fused batch norm node name: ""MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm""                                            
op: ""FusedBatchNorm""
input: ""MobilenetV2/expanded_conv/depthwise/depthwise""
input: ""Const_198""                                                                                                                               
input: ""Const_37""
input: ""Const_138""
input: ""Const_120""
device: ""/job:localhost/replica:0/task:0/device:CPU:0""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""data_format""
  value {
    s: ""NHWC""
  }
}
attr {
  key: ""epsilon""
  value {
    f: 0.001
  }
}
attr {
  key: ""is_training""
  value {
    b: false
  }
}

*** InvalidArgumentError: FetchOutputs MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm:1: output index too large, must be < 1

```

BTW, TensorFlow 1.8 works fine."
21327,Cannot benchmark mobilenet_v2_1.0_224.tflite,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac high sierra
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
Galaxy S7, Huawei Mate 10 etc.
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1dc4c5b6f5829c4b8ef7d67f41735e8a0ce9d59a
- **Python version**:
2.7.10
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
```
bazel build -c opt \
  --config=android_arm \
  --cxxopt='--std=c++11' \
  tensorflow/contrib/lite/tools/benchmark:benchmark_model
<copy benchmark file and model to phone>
adb shell /data/local/tmp//tflite_benchmark --graph=/data/local/tmp///mobilenet_v2_1.0_224.tflite --warmup_runs=1 --num_runs=50 --input_layer=input --input_layer_shape=1,244,244,3 --num_threads=4
```

### Describe the problem
Cannot run benchmark on mobilenet_v2_1.0_224.tflite model. Other models seem to be fine. Tried 0.35_96 and 1.4_224. Both are fine.

### Source code / logs
```
Num runs: [50]
Inter-run delay (seconds): [-1]
Num threads: [4]
Benchmark name: []
Output prefix: []
Warmup runs: [1]
Graph: [/data/local/tmp///mobilenet_v2_1.0_224.tflite]
Input layers: [input]
Input shapes: [1,244,244,3]
Use nnapi : [0]
nnapi error: unable to open library libneuralnetworks.so
Loaded model /data/local/tmp///mobilenet_v2_1.0_224.tflite
resolved reporter
tensorflow/contrib/lite/kernels/reshape.cc:58 num_input_elements != num_output_elements (4004 != 1001)
Node number 64 (RESHAPE) failed to prepare.

Failed to allocate tensors!
Aborted
```"
21326,model prediction works differently when running on different system configuration,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:nope
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.9
- **Python version**:3.6
- **Bazel version (if compiling from source)**:dont know
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**: Geforce Gtx 1080 Ti
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflostackoverfloww as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

i am running this neural translation application
https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb
And i made some changes here to store the model as checkpoints so that everytime i dont want to again train the application it works fine in windows 10 gpu system.

when i  run the same code in cpu system its giving different result .
I checked the versions and environment everything is same.how could i solve this issue? 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21325,wrong matrix inverse at tf?,"I find tensorflow give a wrong inverse of some matrix.

for example
```
import numpy as np
import tensorflow as tf


x = np.load('one.npy')
y = tf.matrix_inverse(tf.constant(x))

sess = tf.Session()
sess.run(y)

np.linalg.inv(x)
```

the `one.npy` can be downloaded from `https://kexue.fm/one.npy`


I test it at tf 1.2 and tf 1.8 on ubuntu. both of them give different result from numpy and echo other."
21323,"[TensorFlow Android Camera Demo] add libSVM AAR but ""Native TF methods not found""","I use the demo code successfully.
However, when I wand to add libSVM AAR to the dependencies. 
https://github.com/yctung/AndroidLibSVM

And the debug console show:
```
I/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded
E/zygote64: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)
I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference
E/MTCNN: [*]load model failedjava.lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present in the APK.
I/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded
E/zygote64: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)
I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference
```

In build.gradle:
```
dependencies {
    if (nativeBuildSystem == 'cmake' || nativeBuildSystem == 'none') {
        compile 'org.tensorflow:tensorflow-android:+'
    }
    implementation project(':androidlibsvm-release') --> if I remark this, the issue will disappear.
}
```
Could anyone know how to fix this?

TensorFlow version:
v1.9.0-0-g25c197e023 1.9.0"
21320,"[object_detection] model_main.py failure: tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected size[0] in [0, 100], but got 101","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.9.0
- **Python version**:2.7
- **Bazel version (if compiling from source)**:bazel release 0.11.1
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:cuda 9.1/ cuDNN 7.0
- **GPU model and memory**: 
- **Exact command to reproduce**:python object_detection/model_main.py --alsologtostderr --pipeline_config_path=pipeline.config --model_dir=.

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I am trying a object_detection mobileV2_model for my own custom dataset. It fails with the following exception 
```
Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 101, in <module>
    tf.app.run()
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 97, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 447, in train_and_evaluate
    return executor.run()
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 531, in run
    return self.run_local()
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 669, in run_local
    hooks=train_hooks)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 366, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1119, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1135, in _train_model_default
    saving_listeners)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1336, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 577, in run
    run_metadata=run_metadata)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1053, in run
    run_metadata=run_metadata)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1144, in run
    raise six.reraise(*original_exc_info)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1129, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1201, in run
    run_metadata=run_metadata)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 981, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected size[0] in [0, 100], but got 101
	 [[Node: Slice_55 = Slice[Index=DT_INT32, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](unstack_2:7, zeros_48, stack_55)]]
	 [[Node: Loss/unstack_3/_11039 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_10953_Loss/unstack_3"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op u'Slice_55', defined at:
  File ""object_detection/model_main.py"", line 101, in <module>
    tf.app.run()
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 97, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 447, in train_and_evaluate
    return executor.run()
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 531, in run
    return self.run_local()
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py"", line 669, in run_local
    hooks=train_hooks)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 366, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1119, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1132, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1107, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/local/YORK/ashwani.agarwal/models/research/object_detection/model_lib.py"", line 216, in model_fn
    unpad_groundtruth_tensors=train_config.unpad_groundtruth_tensors)
  File ""/home/local/YORK/ashwani.agarwal/models/research/object_detection/model_lib.py"", line 163, in unstack_batch
    unpadded_tensor = tf.slice(padded_tensor, slice_begin, slice_size)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 576, in slice
    return gen_array_ops._slice(input_, begin, size, name=name)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 7177, in _slice
    ""Slice"", input=input, begin=begin, size=size, name=name)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Expected size[0] in [0, 100], but got 101
	 [[Node: Slice_55 = Slice[Index=DT_INT32, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](unstack_2:7, zeros_48, stack_55)]]
	 [[Node: Loss/unstack_3/_11039 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_10953_Loss/unstack_3"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
this is the pipeline.config

```
model {
  ssd {
    num_classes: 8
    image_resizer {
      fixed_shape_resizer {
        height: 192
        width: 192
      }
    }
    feature_extractor {
      type: ""ssd_mobilenet_v2""
      depth_multiplier: 0.25
      min_depth: 16
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 3.99999989895e-05
          }
        }
        initializer {
          truncated_normal_initializer {
            mean: 0.0
            stddev: 0.0299999993294
          }
        }
        activation: RELU_6
        batch_norm {
          decay: 0.999700009823
          center: true
          scale: true
          epsilon: 0.0010000000475
          train: true
        }
      }
      use_depthwise: true
    }
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    box_predictor {
      convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 3.99999989895e-05
            }
          }
          initializer {
            truncated_normal_initializer {
              mean: 0.0
              stddev: 0.0299999993294
            }
          }
          activation: RELU_6
          batch_norm {
            decay: 0.999700009823
            center: true
            scale: true
            epsilon: 0.0010000000475
            train: true
          }
        }
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.800000011921
        kernel_size: 3
        box_code_size: 4
        apply_sigmoid_to_scores: false
        use_depthwise: true
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 5
        min_scale: 0.20000000298
        max_scale: 0.949999988079
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.333299994469
      }
    }
    post_processing {
      batch_non_max_suppression {
        score_threshold: 9.99999993923e-09
        iou_threshold: 0.600000023842
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
    normalize_loss_by_num_matches: true
    loss {
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_loss {
        weighted_sigmoid {
        }
      }
      hard_example_miner {
        num_hard_examples: 3000
        iou_threshold: 0.990000009537
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 3
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
  }
}
train_config {
  batch_size: 24
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
  keep_checkpoint_every_n_hours: 3
  optimizer {
    rms_prop_optimizer {
      learning_rate {
        exponential_decay_learning_rate {
          initial_learning_rate: 0.00400000018999
          decay_steps: 214089
          decay_factor: 0.949999988079
        }
      }
      momentum_optimizer_value: 0.899999976158
      decay: 0.899999976158
      epsilon: 1.0
    }
  }
  fine_tune_checkpoint: ""/data/model.ckpt-99096""
  num_steps: 25000
  fine_tune_checkpoint_type: ""classification""
}
train_input_reader {
  label_map_path: ""/data/avo_labelmap.pbtxt""
  tf_record_input_reader {
    input_path: ""/data/ssdv2_tfrecord/train_v2_00000000.tfrecords""
  }
}
eval_config {
  num_visualizations: 30
  num_examples: 7091
  eval_interval_secs: 7200
  save_graph: true
  use_moving_averages: true
  min_score_threshold: 0.289999991655
  visualize_groundtruth_boxes: true
  retain_original_images: true
}
eval_input_reader {
  label_map_path: ""/data/avo_labelmap.pbtxt""
  shuffle: false
  num_readers: 1
  tf_record_input_reader {
    input_path: ""/data/ssdv2_tfrecord/test_v7_00000000.tfrecords""
  }
}
```
"
21317,bfloat16 training with GPUs,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 9.1/7.1.1
- **GPU model and memory**: NVIDIA GTX 1080 Ti
- **Exact command to reproduce**: shown below

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I have two issues, one could be feature request, and the other could be seeking confirmation from the Tensorflow team
I am trying to execute the language model training - https://github.com/okuchaiev/f-lm
the model runs fine with the tf.float16 or tf.float32 datatypes for trainable variables.
However, if the trainable variables are tf.bfloat16 type, I am getting the errors (shown below for two separate cases). 
(line 22 and 24 of https://github.com/okuchaiev/f-lm/blob/master/model_utils.py can affect the datatype of varibles. To define dtype as bfloat16, I changed tf.float16 in line 22 and 24 to tf.bfloat16)

Case 1: Looks are bfloat16 not supported for moving_averages:

Error:

 File ""/home/ranjeeths/langModel/f-lm/single_lm_train.py"", line 54, in <module>
    tf.app.run()
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/home/ranjeeths/langModel/f-lm/single_lm_train.py"", line 37, in main
    run_train(dataset, hps, os.path.join(FLAGS.logdir, ""train""), ps_device=""/gpu:0"")
  File ""/home/ranjeeths/langModel/f-lm/run_utils.py"", line 14, in run_train
    model = LM(hps, ""train"", ps_device)
  File ""/home/ranjeeths/langModel/f-lm/language_model.py"", line 62, in __init__
    self.train_op = tf.group(*[self.train_op, ema.apply(variables_to_average)])
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/training/moving_averages.py"", line 386, in apply
    var.name)
TypeError: The variables must be half, float, or double: model/lstm_0/lstm_cell/kernel:0

To get this error in executed the code as follows:
export CUDA_VISIBLE_DEVICES=1
SECONDS=604800
LOGSUFFIX=bigLSTM
python /home/ranjeeths/langModel/f-lm/single_lm_train.py --logdir=/home/ranjeeths/langModel/f-lm/log/bigLSTM/$LOGSUFFIX/ --num_gpus=1 --datadir=/home/ranjeeths/langModel/lm/data/lm1b/1-billion-word-language-modeling-benchmark-r13output/ --hpconfig run_profiler=False,max_time=$SECONDS,num_steps=20,num_shards=8,num_layers=2,learning_rate=0.2,max_grad_norm=1,keep_prob=0.9,emb_size=1024,projected_size=1024,state_size=8192,num_sampled=8192,batch_size=256,float16_rnn=True,float16_non_rnn=False,loss_scale=1.0

(with float16_rnn=True, float16_non_rnn=False, the lstm related variables will all be tf.bfloat16 and remaining variables will be tf.float32)

Case 2: Looks like bfloat16 is not supported for 'Floor' operation (and probably many more)?

Error:

Traceback (most recent call last):
  File ""/home/ranjeeths/langModel/f-lm/single_lm_train.py"", line 54, in <module>
    tf.app.run()
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/home/ranjeeths/langModel/f-lm/single_lm_train.py"", line 37, in main
    run_train(dataset, hps, os.path.join(FLAGS.logdir, ""train""), ps_device=""/gpu:0"")
  File ""/home/ranjeeths/langModel/f-lm/run_utils.py"", line 40, in run_train
    with sv.managed_session(master, config=config) as sess:
  File ""/usr/lib64/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 1000, in managed_session
    self.stop(close_summary_writer=close_summary_writer)
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 828, in stop
    ignore_live_threads=ignore_live_threads)
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 989, in managed_session
    start_standard_services=start_standard_services)
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 726, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 285, in prepare_session
    sess.run(init_op, feed_dict=init_feed_dict)
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Floor' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]

         [[Node: model/model/dropout/Floor = Floor[T=DT_BFLOAT16, _device=""/gpu:0""](model/model/dropout/add)]]

Caused by op u'model/model/dropout/Floor', defined at:
  File ""/home/ranjeeths/langModel/f-lm/single_lm_train.py"", line 54, in <module>
    tf.app.run()
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""/home/ranjeeths/langModel/f-lm/single_lm_train.py"", line 37, in main
    run_train(dataset, hps, os.path.join(FLAGS.logdir, ""train""), ps_device=""/gpu:0"")
  File ""/home/ranjeeths/langModel/f-lm/run_utils.py"", line 14, in run_train
    model = LM(hps, ""train"", ps_device)
  File ""/home/ranjeeths/langModel/f-lm/language_model.py"", line 29, in __init__
    loss = self._forward(i, xs[i], ys[i])
  File ""/home/ranjeeths/langModel/f-lm/language_model.py"", line 87, in _forward
    x = tf.nn.dropout(x, hps.keep_prob)
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 2318, in dropout
    binary_tensor = math_ops.floor(random_tensor)
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 2811, in floor
    ""Floor"", x=x, name=name)
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'Floor' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]

         [[Node: model/model/dropout/Floor = Floor[T=DT_BFLOAT16, _device=""/gpu:0""](model/model/dropout/add)]]

To get this error in executed the code as follows:
export CUDA_VISIBLE_DEVICES=1
SECONDS=604800
LOGSUFFIX=bigLSTM
python /home/ranjeeths/langModel/f-lm/single_lm_train.py --logdir=/home/ranjeeths/langModel/f-lm/log/bigLSTM/$LOGSUFFIX/ --num_gpus=1 --datadir=/home/ranjeeths/langModel/lm/data/lm1b/1-billion-word-language-modeling-benchmark-r13output/ --hpconfig run_profiler=False,max_time=$SECONDS,num_steps=20,num_shards=8,num_layers=2,learning_rate=0.2,max_grad_norm=1,keep_prob=0.9,emb_size=1024,projected_size=1024,state_size=8192,num_sampled=8192,batch_size=256,float16_rnn=False,float16_non_rnn=True,loss_scale=1.0

(with float16_rnn=False, float16_non_rnn=True, the lstm related variables will all be tf.float32 and remaining variables will be tf.bfloat16)


I am not sure if bfloat16 is supported for all operations (not just the ones mentioned above), or whether these errors are due to the fact that I am not using a TPU (I am using a GPU GTX 1080 now). If I use run the above code on a TPU do you think I wont be getting any more errors?








### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21315,building tensorflow shared library in windows debug mode (cmake) /MDd,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r 1.9
### Describe the problem
We get many linking error for building tensorflow shared library in windows when debug mode is selected
(/MDd switch in VC++). We get many error messages like

> mismatch detected for 'RuntimeLibrary': value 'MD_DynamicRelease' doesn't match value 'MDd_DynamicDebug' 

After digging into the issue, I noticed the linker is using wrong GRPC libs. 

`
double_conversion\src\double_conversion\double-conversion\$(Configuration)\double-conversion.lib
zlib\install\lib\zlibstaticd.lib
grpc\src\grpc\Release\grpc++_unsecure.lib
grpc\src\grpc\Release\grpc_unsecure.lib
grpc\src\grpc\Release\gpr.lib
snappy\src\snappy\$(Configuration)\snappy.lib
`
Apparently, the cmake script is using the release version of grpc regardless of the config. I changed it manually and it was fixed. It would be nice if this fix is included in the future patches. 
"
21313,Feature request : sparse tensor in map_fn,"Hello,

I find that a class of useful ops is missing from Tensorflow : the ability to return outputs of various length while iterating over a tensor. Ideally it would support GPU.

I tried to return a sparse_tensor inside map_fn but currently if fails with (TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor )

It would be useful to have a way to raster, i.e., iterate over a tensor and apply a function which can return tensor of different shapes.

For example, input tensor could be the coords of the vertices of a triangle, output for each triangle the list of pixels which would be turned on. We can then use unsorted_segment_xxx on the joint list.

Another example would be for a tensor [[start1,end1],[start2,end2],...,[startn,endn]] -> concat( [range(start1,end1), range(start2,end2),..., range(startn,endn)] )
"
21312,Timeline equivalent for C++,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: bazel
- **TensorFlow version (use command below)**:r1.8
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.14.1
- **GCC/Compiler version (if compiling from source)**: clang 6
- **CUDA/cuDNN version**: 9/7.1
- **GPU model and memory**:  1080ti / 11gb
- **Exact command to reproduce**:
n/a
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

What is the timeline profiler equivalent for Tensorflow C++ api ?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21311,Share the mean and var of batch norm when Multi gpus training,"I want to know if using estimator to train on multi gpus can share the mean and var of batchnorm.

As the example in cifar, it cannot deal with the batchnorm sharing between different gpu"
21310,Getting slow video straming while testing a model and by using ROS ,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:('v1.9.0-0-g25c197e023', '1.9.0')
- **GPU model and memory**:Quadro M1000M , 4 Gb 
     Bazel version: NA
     CUDA/cuDNN version:NA
     Exact command to reproduce: NA
     Mobile device : NA

 
### Describe the problem
I trained a model in order to detect a hopper in a bakery using realsense camera D435 but  when i wanted to test and validate the model I wrote a program to stream the frames and to test the model but the stram was so slow so i was getting all the frames but with a big delay despite i am using my gpu and this is the code : 
Pm: I am using also ROS for that. 

### Source code / logs
#!/usr/bin/env python
import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from collections import defaultdict
from io import StringIO
#from matplotlib import pyplot as plt
#from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

import rospkg
import rospy
from sensor_msgs.msg import Image
from std_msgs.msg import String
from cv_bridge import CvBridge
import cv2
# SET FRACTION OF GPU YOU WANT TO USE HERE
#GPU_FRACTION = 0.4

if tf.__version__ < '1.4.0':
  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')
  
# get an instance of RosPack with the default search paths
rospack = rospkg.RosPack()
# get the file path for rospy_tutorials

path_to_learn_pkg = rospack.get_path('object-detection')
research_module_path = os.path.join(path_to_learn_pkg,""models/research"")
object_detection_module_path = os.path.join(path_to_learn_pkg,""models/research/object_detection"")
sys.path.append(object_detection_module_path)

#print(sys.path)

from object_detection.utils import ops as utils_ops
from utils import label_map_util

from utils import visualization_utils as vis_util

# What model to download.
MODEL_NAME = 'learned_model'
# Path to frozen detection graph. This is the actual model that is used for the object detection.
PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'

#scripts_module_path = os.path.join(path_to_learn_pkg,""scripts/"")
final_path_to_ckpt = os.path.join(path_to_learn_pkg,PATH_TO_CKPT)

# List of the strings that is used to add correct label for each box. In our case mira_robot
PATH_TO_LABELS = os.path.join(path_to_learn_pkg, 'training/object-detection.pbtxt')

NUM_CLASSES = 1


detection_graph = tf.Graph()
	
with detection_graph.as_default():
  od_graph_def = tf.GraphDef()
  with tf.gfile.GFile(final_path_to_ckpt, 'rb') as fid:
    serialized_graph = fid.read()
    od_graph_def.ParseFromString(serialized_graph)
    tf.import_graph_def(od_graph_def, name='')
    
label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)


def load_image_into_numpy_array(image):
  (im_width, im_height) = image.size
  return np.array(image.getdata()).reshape(
      (im_height, im_width, 3)).astype(np.uint8)
      
# Size, in inches, of the output images.
IMAGE_SIZE = (12, 8)
#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
def run_inference_for_single_image(image, graph):
  with graph.as_default():
	    with tf.Session() as sess:
	      # Get handles to input and output tensors
	      ops = tf.get_default_graph().get_operations()
	      all_tensor_names = {output.name for op in ops for output in op.outputs}
	      tensor_dict = {}
	      for key in [
		  'num_detections', 'detection_boxes', 'detection_scores',
		  'detection_classes', 'detection_masks'
	      ]:
		tensor_name = key + ':0'
		if tensor_name in all_tensor_names:
		  tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
		      tensor_name)
	      if 'detection_masks' in tensor_dict:
		# The following processing is only for a single image
		detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
		detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
		# Reframing is required to translate the mask from box coordinates to image coordinates and fit the image size.
		real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
		detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
		detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
		detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
		    detection_masks, detection_boxes, image.shape[0], image.shape[1])
		detection_masks_reframed = tf.cast(
		    tf.greater(detection_masks_reframed, 0.5), tf.uint8)
		# Follow the convention by adding back the batch dimension
		tensor_dict['detection_masks'] = tf.expand_dims(
		    detection_masks_reframed, 0)
	      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')

	      # Run inference
	      output_dict = sess.run(tensor_dict,
		                     feed_dict={image_tensor: np.expand_dims(image, 0)})

	      # all outputs are float32 numpy arrays, so convert types as appropriate
	      output_dict['num_detections'] = int(output_dict['num_detections'][0])
	      output_dict['detection_classes'] = output_dict[
		  'detection_classes'][0].astype(np.uint8)
	      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
	      output_dict['detection_scores'] = output_dict['detection_scores'][0]
	      if 'detection_masks' in output_dict:
		output_dict['detection_masks'] = output_dict['detection_masks'][0]
  return output_dict
  

class RosTensorFlow():
    def __init__(self):
        # Processing the variable to process only half of the frame's lower load
        self._process_this_frame = True
        self._cv_bridge = CvBridge()
	
        self._sub = rospy.Subscriber('image', Image, self.callback, queue_size=1)
        self._pub = rospy.Publisher('result', String, queue_size=1)
        self.score_threshold = rospy.get_param('~score_threshold', 0.1)
        self.use_top_k = rospy.get_param('~use_top_k', 5)
        
        

    def callback(self, image_msg):
        if (self._process_this_frame):
            
            image_np = self._cv_bridge.imgmsg_to_cv2(image_msg, ""bgr8"")
    
            # Expand dimensions since the model expects images to have shapes: [1, None, None, 3]
            image_np_expanded = np.expand_dims(image_np, axis=0)
            # Actual detection.
            output_dict = run_inference_for_single_image(image_np, detection_graph)
            # Visualization of the results of a detection.
            vis_util.visualize_boxes_and_labels_on_image_array(
                image_np,
                output_dict['detection_boxes'],
                output_dict['detection_classes'],
                output_dict['detection_scores'],
                category_index,
                instance_masks=output_dict.get('detection_masks'),
                use_normalized_coordinates=True,
                line_thickness=8)
            cv2.imshow(""Image window"", image_np)
            cv2.waitKey(1)
        else:
            pass
        # We invert it
        self._process_this_frame = not self._process_this_frame
        
        
        
    def main(self):
        rospy.spin()

if __name__ == '__main__':
    rospy.init_node('search_hopper_node')
    tensor = RosTensorFlow()
    tensor.main()"
21309,iOS tensorflow audio recognition example (documentation),"Please provide an example, documentation for tensorflow audio recognition on iOS as you provide on Android [(example).](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/SpeechActivity.java)"
21306,Support for NCE-loss/sampled softmax with dynamic_decode(),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: CUDA 9.1
- **GPU model and memory**: TITAN Xp 12196MB * 8
- **Exact command to reproduce**: NA

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
NCE loss/Sampled Softmax loss both require inputs along with the softmax-weight (used for multiplication to obtain logits). However, the current [sequence_loss()](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/sequence_loss) function in [dynamic_decode()](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode) function expects a loss function that works with logits, not these inputs. To provide these inputs to this custom loss function, the only method right now is to override the [cell ](https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py#L137) being used, [return](https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/seq2seq/python/ops/decoder.py#L265) their outputs (before passing through output layer) all the way up via its BasicDecoder. Then, this output can be used in the custom loss function.

There should be some support for an optional flag which lets one return the output state (before being passed through that matrix) as well, so that it may be passed to the custom loss function. TO compliment this, the custom loss function should also accept a loss in the format (inputs, outputs) instead if just (logits, outputs).

I spent nearly a week into looking for a workaround for this and could not find anywhere, hence this issue/feature request.

P.S.: If anyone has used NCE loss/sampled softmax loss with dynamic_decode() without going through all of this hassle, please let me know. There is nothing on Stack Overflow/Github/Tensorflow's examples about this (there is one example, but they used the logits as inputs to the sequence_loss's custom loss, which is wrong)"
21305,PS/Chief Nodes not terminating & Worker Nodes not accurately terminating with tf.estimator.train_and_evaluate,"I'm using tf.estimator.train_and_evaluate of TF 1.9.

When the training reaches the `max_steps` and `max_steps` is less than the dataset's size, the evaluator and the non-chief workers will terminate as expected. But, the ps nodes and the chief worker do not terminate automatically. They just stuck, and you have to kill them manually.

When the `max_steps` is larger than the dataset's size, only one of the workers will terminate, which I guess is the one that reaches the end of the dataset. The remaining workers and ps nodes stuck.
"
21304,Time_Distributed Keras vs TF Keras ,"Hi, 
there is a Bug when using the Time_Distributed Layer from tf Keras:


```
from tensorflow.python.keras.layers import TimeDistributed,Input,Embedding,LSTM
inu=Input(shape=(None,None))
emb=Embedding(input_dim=100,output_dim=100)(inu)
rnn=TimeDistributed(LSTM(100))(emb)

```
you get the error: 

`as_list() is not defined on an unknown TensorShape.`
If I use the Time_Distributed from Keras directly like this, there is no error: 

```
from keras.layers import TimeDistributed,Input,Embedding,LSTM
inu=Input(shape=(None,None))
emb=Embedding(input_dim=100,output_dim=100)(inu)
rnn=TimeDistributed(LSTM(100))(emb)
```
We can see the difference in how the Time_Distributed gets its shape, which is defined in line 163 in the keras definition [Keras](https://github.com/keras-team/keras/blob/master/keras/layers/wrappers.py#L114) they define a ""get_shape_tuple"" function and use that to not have to call the ""as_list"" as in TF Keras in line 164 [TF Keras](https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/keras/layers/wrappers.py)


Have I written custom code NO
OS Platform and Distribution Ubuntu 
TensorFlow installed from Source
TensorFlow version 1.9 
Bazel version NA 
CUDA/cuDNN version 9, 7.1
GPU model and memory p100 
Exact command to reproduce See above
Mobile device NA"
21303,Memory Issue with my Tensorflow Code,"------------------------

### System information

**Have I written custom code: Yes
OS Platform and Distribution: Linux Ubuntu 16.04
 TensorFlow installed from binary
Bazel version: N/A
Python version 2.7
TensorFlow version 1.9.0
CUDA/cuDNN version CUDA 9.0
GPU model and memory:  Nvidia 860M and memory 4 GB
Exact command to reproduce: N/A
Mobile device:N/A**

**The problem:**
I wrote a tensorflow code DDQN network to run with gpu and I am getting an Odd Memory problem. My processes list from system monitor shows that my memory utilized by the python code remains almost constant and increases very slowly as there is a increase in replay buffer size(Its list which stores samples). But in resources it shows that my memory is increasing steeply and is not analogous with the python program. For example when I start the code the memory utilized by the system is around 3.8 GB with python code taking around 1.5 GB of memory but after sometime like an hour or so the memory utilized by the system increases to 9-10 GB with the python code utilizing around 1.8GB. 

### Source code 
```
import tensorflow as tf
import numpy as np 	
from PIL import Image
import time
import os, os.path
import cPickle as pickle
from colorama import Fore, Back, Style
from collections import deque,OrderedDict
from blist import *
import gc
#import pylab as pl
import sys
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageEnhance


Training_Dataset_1=""Imagenet_Sequencing_filtered.pickle""
#Training_Dataset_2=""OTB_Dataset.pickle""
Training_Dataset_2=""Dataset_VOT.pickle""

Test_Dataset=""OTB_Dataset.pickle""
Results_file=""Test_Results.pickle""
#Replay_Memory_file=""Replay_Memory.pickle""

import Layers
import RNN_State_update
import One_Shot_Representation
import Image_Cropping_Resizing
import Getting_Pretrained_caffenet_Parameters
import Grid_Generation
import Others_Func


weights=Getting_Pretrained_caffenet_Parameters.param()

Number_RNN_Cell_Units=512
Number_of_Actions=11

Mini_batch=32									#Batch Size over which gradient would be evaluated
Max_episodes=200											
K_iterations=20 								#Number of Times to Sample and Perform gradient update before collecting new dataset
Discount_Factor=0.99
Number_of_Epochs=1
Number_of_Training_Videos_1=1000
Number_of_Training_Videos_2=70
Number_of_Testing_videos=94
Batch_Size=1
ReplayMemory=200000
Weight_Update_Step_Size=10000						#Updatation of weights from Evaluation Network to target network

sess=tf.Session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True))

class CNN_RNN_Layers():

# This class combines all the CNNs

	def __init__(self, input,Batch_Size):

		self.input=input 				#T-1 and T Images with cropped using he Groundtruth Function

		self.input_1=input[0:Batch_Size]
		self.input_2=input[Batch_Size: 2*Batch_Size]

		self.Batch_Size=Batch_Size


		with tf.variable_scope(""conv1"", reuse=tf.AUTO_REUSE):

			self.weights_1=weights[0]
			self.bias_1=weights[1]

			self.conv1_input_1= Layers.Convolayer(self.input_1, self.weights_1, self.bias_1, group=1, stride=(4,4) , pad_type='VALID' , name='Convolution_Layer_1_input_1')
			self.conv1_max_pooled_input_1=tf.nn.max_pool(value=self.conv1_input_1, ksize=[1,3,3,1], strides=[1,2,2,1], padding='VALID', name='Conv_1_Maxpool_input_1')
			self.conv1_normalized_input_1=tf.nn.local_response_normalization(self.conv1_max_pooled_input_1,depth_radius=1,bias=1,alpha=0.0001,beta=0.75,name='Conv_1_Normalization_input_1')

		with tf.variable_scope(""conv1"", reuse=tf.AUTO_REUSE):

			self.conv1_input_2= Layers.Convolayer(self.input_2, self.weights_1, self.bias_1, group=1, stride=(4,4) , pad_type='VALID' , name='Convolution_Layer_1_input_2')
			self.conv1_max_pooled_input_2=tf.nn.max_pool(value=self.conv1_input_2, ksize=[1,3,3,1], strides=[1,2,2,1], padding='VALID', name='Conv_1_Maxpool_input_2')
			self.conv1_normalized_input_2=tf.nn.local_response_normalization(self.conv1_max_pooled_input_2,depth_radius=1,bias=1,alpha=0.0001,beta=0.75,name='Conv_1_Normalization_input_2')
			
		with tf.variable_scope(""conv2"", reuse=tf.AUTO_REUSE):

			self.weights_2=weights[2]
			self.bias_2=weights[3]

			self.conv2_input_1= Layers.Convolayer(self.conv1_normalized_input_1, self.weights_2, self.bias_2, group=2, stride=(1,1) , pad_type='SAME' , name='Convolution_Layer_2_input_1')
			self.conv2_max_pooled_input_1=tf.nn.max_pool(self.conv2_input_1, ksize=[1,3,3,1], strides=[1,2,2,1],padding='VALID', name='Conv_2_Maxpool_input_1')
			self.conv2_normalized_input_1=tf.nn.local_response_normalization(self.conv2_max_pooled_input_1,depth_radius=1,bias=1,alpha=0.0001,beta=0.75,name='Conv_2_Normalization_input_1')

		with tf.variable_scope(""conv2"", reuse=tf.AUTO_REUSE):

			self.conv2_input_2= Layers.Convolayer(self.conv1_normalized_input_2, self.weights_2, self.bias_2, group=2, stride=(1,1) , pad_type='SAME' , name='Convolution_Layer_2_input_2')
			self.conv2_max_pooled_input_2=tf.nn.max_pool(self.conv2_input_2, ksize=[1,3,3,1], strides=[1,2,2,1],padding='VALID', name='Conv_2_Maxpool_input_2')
			self.conv2_normalized_input_2=tf.nn.local_response_normalization(self.conv2_max_pooled_input_2,depth_radius=1,bias=1,alpha=0.0001,beta=0.75,name='Conv_2_Normalization_input_2')

			
		with tf.variable_scope(""conv3"", reuse=tf.AUTO_REUSE):

			self.weights_3=weights[4]
			self.bias_3=weights[5]

			self.conv3_input_1= Layers.Convolayer(self.conv2_normalized_input_1, self.weights_3, self.bias_3, group=1, stride=(1,1) , pad_type='SAME' , name='Convolution_Layer_3_input_1')

		with tf.variable_scope(""conv3"", reuse=tf.AUTO_REUSE):	
			
			self.conv3_input_2= Layers.Convolayer(self.conv2_normalized_input_2, self.weights_3, self.bias_3, group=1, stride=(1,1) , pad_type='SAME' , name='Convolution_Layer_3_input_2')
		
		with tf.variable_scope(""conv4"", reuse=tf.AUTO_REUSE):

			self.weights_4=weights[6]
			self.bias_4=weights[7]

			self.conv4_input_1= Layers.Convolayer(self.conv3_input_1, self.weights_4, self.bias_4, group=2, stride=(1,1) , pad_type='SAME' , name='Convolution_Layer_4_input_1')
		with tf.variable_scope(""conv4"", reuse=tf.AUTO_REUSE):
			
			self.conv4_input_2= Layers.Convolayer(self.conv3_input_2, self.weights_4, self.bias_4, group=2, stride=(1,1) , pad_type='SAME' , name='Convolution_Layer_4_input_1')
			
		with tf.variable_scope(""conv5"", reuse=tf.AUTO_REUSE):

			self.weights_5=weights[8]
			self.bias_5=weights[9]

			self.conv5_input_1= Layers.Convolayer(self.conv4_input_1, self.weights_5, self.bias_5, group=2, stride=(1,1) , pad_type='SAME' , name='Convolution_Layer_5_input_1')
			self.conv5_max_pooled_input_1=tf.nn.max_pool(self.conv5_input_1, ksize=[1,3,3,1], strides=[1,2,2,1],padding='VALID', name='Conv_5_Maxpool_input_1')

		with tf.variable_scope(""conv5"", reuse=tf.AUTO_REUSE):	

			self.conv5_input_2= Layers.Convolayer(self.conv4_input_2, self.weights_5, self.bias_5, group=2, stride=(1,1) , pad_type='SAME' , name='Convolution_Layer_5_input_2')
			self.conv5_max_pooled_input_2=tf.nn.max_pool(self.conv5_input_2, ksize=[1,3,3,1], strides=[1,2,2,1],padding='VALID', name='Conv_5_Maxpool_input_2')		
			
		#Starting the Fully Connected Layers After the CNN. Here We use the inbuilt densely connected layer.
		self.input_fully_connected_input_1=tf.layers.flatten(self.conv5_max_pooled_input_1, name='Flatten_input_1')
		self.input_fully_connected_input_2=tf.layers.flatten(self.conv5_max_pooled_input_2, name='Flatten_input_2')

		self.concatenated_output=tf.concat([tf.reshape(self.input_fully_connected_input_1, shape=[Batch_Size, -1]), 
											tf.reshape(self.input_fully_connected_input_2, shape=[Batch_Size, -1])], axis=-1)

class fully_connected():

		def __init__(self, input, scope_name,Batch_Size):

			self.input=input 

			with tf.variable_scope(scope_name) as scope:
											
				with tf.variable_scope('First_Fully_Connected', reuse=tf.AUTO_REUSE):

					self.fully_connected_1_output=tf.layers.dense(inputs = self.input, units=4096, activation=tf.nn.relu, name=""Dense_Layer_2"")

				with tf.variable_scope('Second_Fully_Connected', reuse=tf.AUTO_REUSE):

					self.fully_connected_2_output=tf.layers.dense(inputs = self.fully_connected_1_output, units=2048, activation=tf.nn.relu, name=""Dense_Layer_3"")

				with tf.variable_scope('Output_Layer', reuse=tf.AUTO_REUSE):
			
					self.dense_output= Layers.Dense_Layer(self.fully_connected_2_output, Number_of_Actions, name='Action_Output_Layer')
					self.Action_Output=tf.argmax(self.dense_output, axis=-1, name='Output_Action_as_Given_by_Deep_Networks')

					self.max_Q_Values=tf.reduce_max(self.dense_output, axis=-1)

					self.trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=scope.name)
					self.trainable_vars_by_name = {var.name[len(scope.name):]: var for var in self.trainable_vars}


		def Q_Values_of_Given_State_Action(self, actions_, y_targets):

			self.dense_output=self.dense_output

			actions_=tf.reshape(tf.cast(actions_, tf.int32), shape=(Mini_batch,1))
			z=tf.reshape(tf.range(tf.shape(self.dense_output)[0]), shape=(Mini_batch,1) )

			index_=tf.concat((z,actions_), axis=-1)

			self.Q_Values_Select_Actions=tf.gather_nd(self.dense_output, index_)

			#loss_=tf.divide((tf.reduce_sum (tf.square(Q_Values_Select_Actions-y_targets))), 2)
			#loss_= y_targets-Q_Values_Select_Actions		#Getting the difference between the target network and evaluation network
			loss_=tf.reduce_mean(tf.square(self.Q_Values_Select_Actions-y_targets))

			return loss_




def Target_Values(r_t,s_t_plus_1,termination_criteria):

	#For DDQN

	s_t_plus_1=np.reshape(s_t_plus_1, newshape=(32,18432))

	Output_Q_Values_Eval=sess.run((Eval_Network_Q_Values), feed_dict={x_Eval_Net:s_t_plus_1})	#Getting the Q-Values from evaluation network
	action_=np.argmax(Output_Q_Values_Eval,axis=1)	#Getting the actions array
	Output_Q_Values_target=sess.run((Q_Values_Target), feed_dict={x_Target_Values:s_t_plus_1})	#Getting the Q-Values from target network

	Q_values_selected=[]
	for temp ,a in zip(range(32), action_):

		Q_values_selected.append(Output_Q_Values_target[temp][a])
	y_target_values=Others_Func.Y_Target(Q_values_selected, r_t,Discount_Factor)

	for i in range(32):
		if(termination_criteria[i]==True):
			y_target_values[i]=r_t[i]

	return y_target_values


def Evaluation_Network(s_t, a_t, y_Targets ):

	with tf.variable_scope('Eval_Network') as scope:

		s_t=np.reshape(s_t, newshape=(32, 18432))

		total_loss,_=sess.run((loss,train), feed_dict={x_Eval_Net:s_t, a_t_eval:a_t,y_Targets_eval:y_Targets})	#Train the network and get the loss

		return total_loss #Return the loss


def DQN(samples_):
	
	s_t,a_t,r_t,s_t_plus_1, termination_criteria=Others_Func.samples_seperation_generation(samples_)

	Targets=Target_Values(r_t,s_t_plus_1,termination_criteria)
	Eval=Evaluation_Network(s_t,a_t, Targets)		# Train the evaluation network and get the loss

	del(s_t, a_t,r_t,s_t_plus_1)
	gc.collect()	
	return Eval 	#Return the loss

with tf.device('/device:GPU:0'):

	x_sampling=tf.placeholder(dtype=tf.float32,shape=(Batch_Size*2,227,227,3), name=""x_sampling"") 	#Images resized to 224x224x3

	CNN_RNN_Layer_Sampling=CNN_RNN_Layers(x_sampling,Batch_Size)		#Creating the Object for CNN-RNN Layers
	state_eval_sampling=CNN_RNN_Layer_Sampling.concatenated_output
	
	##########################################
	x_Target_Values=tf.placeholder(dtype=tf.float32,shape=(None, 18432), name=""x_Target_Values"") 	#Images resized to 224x224x3
	fully_connected_Target=fully_connected(x_Target_Values,'Target_Network',Mini_batch)	#Creating the Object for CNN-RNN Layers
	Q_Values_Target=fully_connected_Target.dense_output
	target_max_Q_Value=fully_connected_Target.max_Q_Values
	target_vars = fully_connected_Target.trainable_vars_by_name

	################################################################
	x_Eval_Net=tf.placeholder(dtype=tf.float32,shape=(None,18432), name=""x_Eval_Net"") 	#Images resized to 224x224x3
	a_t_eval=tf.placeholder(dtype=tf.int32,shape=(Mini_batch), name=""a_t_eval"")
	y_Targets_eval=tf.placeholder(dtype=tf.float32,shape=(Mini_batch), name=""y_Targets_eval"")
	
	fully_connected_Eval=fully_connected(x_Eval_Net,  'Eval_Network',Mini_batch)		#Creating the Object for CNN-RNN Layers
	Eval_Network_Q_Values=fully_connected_Eval.dense_output 									#Getting the Q-Values os the current state using the evaluation network
	Eval_Network_max_Q_Value=fully_connected_Eval.max_Q_Values
	eval_vars = fully_connected_Eval.trainable_vars_by_name
	
	loss=fully_connected_Eval.Q_Values_of_Given_State_Action(a_t_eval,y_Targets_eval)
	optimizer = tf.train.AdamOptimizer(learning_rate=0.0001,beta1=0.9)
	train = optimizer.minimize(loss, var_list=tf.trainable_variables(scope='Eval_Network'))

	#######################

	copy_ops =[target_var.assign(eval_vars[var_name]) for var_name, target_var in target_vars.items()]
	copy_online_to_target = tf.group(*copy_ops)

	######################
	init_op = tf.global_variables_initializer()
	sess.run(init_op)
```"
21302,Quantized model not working in Tensorflow Lite iOS,"I am working on Tensorflow-Lite in iOS. A non quantized .pb file when converted to .tflite it works fine, but when I tried converting a working quantized model (pb) to .tflite, with command below:

   ```
 bazel-bin/tensorflow/contrib/lite/toco/toco \
   --input_file=/pathOfQuantizedFile/xyz.pb \
   --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \
   --output_file=/pathOfQuantizedFile/xyz.tflite   --inference_type=FLOAT \
   --input_type=FLOAT --input_arrays=input_node   --output_arrays=output_node    
   --input_shapes=1,256,256  --allow_custom_ops
```

The file gets converted to .tflite successfully but gave this following error when used in the iOS app:

> Didn't find custom op for name 'RSQRT'
> Didn't find custom op for name 'Dequantize'
> Didn't find custom op for name 'ReorderAxes'
> Registration failed.
> Failed to construct interpreter

Tried this with both TensorFlow version - r1.8,  r1.9 and r1.10.

Have I written custom code - No
OS Platform and Distribution - MacOS
TensorFlow - N/A
TensorFlow version - r1.8
Bazel version - 0.11.0
CUDA/cuDNN version - N/A
GPU model and memory - N/A
Exact command to reproduce - N/A"
21301,"After TensorFlow 1.1.0, libtensorflow_inference.so error: undefined reference to 'TF_NewGraph'",
21300,core dumped when using tflite_convert,"When I use tflite_convert to get a tflite model, it coredumps everytime.

The model I trained is based on MobileNetV2 + SSD， and generate a pb file using export_inference_graph.py in object_detection.

here is the log:

'2018-08-01 08:43:35.857541: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.863758: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Equal\n2018-08-01 08:43:35.864497: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.864561: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.864673: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.864710: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.864720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.864731: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.864739: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.864810: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Equal\n2018-08-01 08:43:35.864827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.864856: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.864890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.864940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.864991: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\n2018-08-01 08:43:35.865011: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\n2018-08-01 08:43:35.865049: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.865110: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.865191: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.865271: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\n2018-08-01 08:43:35.865300: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\n2018-08-01 08:43:35.865351: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.865390: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.865467: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.865525: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\n2018-08-01 08:43:35.865546: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\n2018-08-01 08:43:35.865580: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.865612: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.865662: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.865708: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\n2018-08-01 08:43:35.865728: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\n2018-08-01 08:43:35.865762: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.865793: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.865838: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.865885: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\n2018-08-01 08:43:35.865906: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\n2018-08-01 08:43:35.865942: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.865974: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.866025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.866075: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\n2018-08-01 08:43:35.866097: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\n2018-08-01 08:43:35.866132: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.866164: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.866212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.866260: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\n2018-08-01 08:43:35.866280: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\n2018-08-01 08:43:35.866312: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.866345: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.866394: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.866441: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\n2018-08-01 08:43:35.866461: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\n2018-08-01 08:43:35.866496: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.866529: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.866579: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.866629: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\n2018-08-01 08:43:35.866649: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\n2018-08-01 08:43:35.866681: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.866713: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\n2018-08-01 08:43:35.866760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\n2018-08-01 08:43:35.866807: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\n2018-08-01 08:43:35.866829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\n2018-08-01 08:43:35.866874: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Size\n2018-08-01 08:43:35.866884: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Equal\n2018-08-01 08:43:35.867413: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ResizeNearestNeighbor\n2018-08-01 08:43:35.907700: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2041 operators, 3390 arrays (0 quantized)\n2018-08-01 08:43:35.954623: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 2001 operators, 3308 arrays (0 quantized)\n2018-08-01 08:43:36.014150: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2001 operators, 3308 arrays (0 quantized)\n2018-08-01 08:43:36.095756: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:95] Check failed: other_op->type == OperatorType::kTensorFlowMerge \nAborted (core dumped)\n'

Anyone can help ? Thanks a lot"
21298,Error messages when using Dataset.from_generator in debug mode,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.13.3
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA (CPU)
- **GPU model and memory**: NA (CPU)
- **Exact command to reproduce**:

```python
import tensorflow as tf

def gen():
  for val in range(10):
    yield (str(val),)

dataset = tf.data.Dataset.from_generator(gen, (tf.string,))

iter = dataset.make_one_shot_iterator()
next = iter.get_next()

sess = tf.Session()

for ii in range(10):
  print(sess.run(next))
```

### Describe the problem
When using `Dataset.from_generator()` in debug mode (PyCharm) error messages () are printed to the console. The code seems to run as expected. Might be some finalization issues. When not in debug mode everything seems quiet.

### Source code / logs

Error message:
```
Exception ignored in: <generator object _yield_value at 0x181a3d7830>
Traceback (most recent call last):
  File ""/usr/local/miniconda3/envs/aaa/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py"", line 100, in _yield_value
    yield value
SystemError: error return without exception set
Exception ignored in: <generator object _yield_value at 0x181a3d7830>
Traceback (most recent call last):
  File ""/usr/local/miniconda3/envs/aaa/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py"", line 100, in _yield_value
    yield value
SystemError: error return without exception set
Exception ignored in: <generator object _yield_value at 0x181a3d7830>
Traceback (most recent call last):
  File ""/usr/local/miniconda3/envs/aaa/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py"", line 100, in _yield_value
    yield value
.
.
.
```
"
21295,support 3d convolution with float64,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
from pip install

- **TensorFlow version (use command below)**:
tensorflow 1.9.0



- **Python version**:
3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
cuDNN 9.0 v7, CUDA9.1

- **GPU model and memory**:
GTX980Ti 6GB, Geforce Titan X 12GB

- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I'm working on some convolutional calculation on the Tensorflow and got some strange errors with the Double Precision numbers.
I needed to calculate those tensors in float64 because of the precision issues.

The conv3d works fine with me on float32, but on the Double precision, it only works on CPU and on the GPU it causes error.

So is there any way I can get access to this operation?


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

import tensorflow as tf
import numpy as np

with tf.device('/device:GPU:0'):

    a = tf.constant(2 * np.ones([1, 200, 200, 200, 9], dtype=np.float64), dtype=tf.float64)

    b = tf.constant(np.ones([3, 3, 3, 9, 9], dtype=np.float64), dtype=tf.float64)


    sess = tf.Session()

    init = tf.global_variables_initializer()

    sess.run(init)


    c = tf.nn.conv3d(a, b, strides=[1, 1, 1, 1, 1], padding='SAME')
    print(a)
    print(b)
    print(c)
    for i in range(1000):
        print(sess.run(c))

the output says

InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Conv3D': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Registered kernels:
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]

 [[Node: Conv3D = Conv3D[T=DT_DOUBLE, data_format=""NDHWC"", dilations=[1, 1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1, 1], _device=""/device:GPU:0""](Const, Const_1)]]



"
21294,tfconvert float to float_ref valueError ,"
 **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:NA
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:tensorflow-gpu 1.9
- **Python version**:3.5
- **Bazel version (if compiling from source)**:0.15
- **GCC/Compiler version (if compiling from source)**:5.4
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**: geforce 1070/8G
- **Exact command to reproduce**:
Using the command:
 tflite_convert  --graph_def_file=./opt.pb --output_format=TFLITE   --output_file=./model.tflite --inference_type=FLOAT  --input_arrays=inputs   --output_arrays=softmax --input_shapes=1,5000,1,1
You can collect some of this information using our environment capture script:

### Describe the problem
Converting a trained CNN+RNN  model which was freezed by the freeze_graph and optimized  command gives the valueError message:
Input 0 of node save/Assign was passed float from beta1_power:0 incompatible with expected float_ref

I guess the issue was caused by the Adam optimizer 

### Source code / logs
### The parameters saved after trainning process like this
![screenshot from 2018-08-01 10-06-34](https://user-images.githubusercontent.com/32789540/43496953-a018118a-9572-11e8-8aaf-c3001378a205.png)



What can I do now?
"
21287,Possible bug in dynamic_rnn when training on TPU for iterations_per_loop > 1,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Cloud Platform (Linux Debian)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA (TPU training)
- **GPU model and memory**: NA (TPU training)
- **Exact command to reproduce**:

### Describe the problem
Training an RNN (constructed with `dynamic_rnn`) on TPU gives largely different loss values for `iterations_per_loop=1` and `iterations_per_loop=100`. The loss when training on TPU with `iterations_per_loop=1` is very close to the loss when training on CPU, but the loss for `iterations_per_loop=100` case is orders of magnitude different.

See below for the code to reproduce this issue. I also tested it with BasicRNNCell (instead of GRU) and observed the same issue. For easier debugging, I have made the runs deterministic (all the random ops are seeded, repeated runs produce the exact same values).

Note that if I replace my model_fn with a simple linear model containing only matrix multiplication (instead of `dynamic_rnn`) the loss for any value of `iterations_per_loop` will be the same which is as expected. So I suspect there is a bug in using `dynamic_rnn` with TPU. 

### Source code / logs
```
import numpy as np
import tensorflow as tf
from tensorflow.contrib.tpu.python.tpu import tpu_estimator
import subprocess
import os

SEED = 10010

USE_TPU = True

def make_data(params):
    # make training and validation data: sinusoids with random phases
    np.random.seed(SEED)
    num_samp_tr = 1000
    num_samp_val = 1000
    ramps_tr = np.transpose(np.broadcast_to(0.1*np.arange(0,100), (num_samp_tr, params['dims'], 100)), (0, 2, 1))
    rand_phase = np.transpose(np.tile(np.random.randn(num_samp_tr, params['dims']), (100,1,1)), (1, 0, 2))
    ramps_val = np.transpose(np.broadcast_to(0.1*np.arange(0,100), (num_samp_val, params['dims'], 100)), (0, 2, 1))
    rand_phase_val = np.transpose(np.tile(np.random.randn(num_samp_val, params['dims']), (100,1,1)), (1, 0, 2))
    data = {'train_data': np.sin(ramps_tr + rand_phase),
            'valid_data': np.sin(ramps_val + rand_phase_val)}
    return data

def input_fn(data_dict, mode):
    def data_fn(params):
        batch_size = params['batch_size']
        if mode == tf.estimator.ModeKeys.TRAIN:
            dataset = tf.data.Dataset.from_tensor_slices(data_dict['train_data'].astype(np.float32)).cache().repeat().shuffle(buffer_size=10000, seed=SEED)
        else:
            dataset = tf.data.Dataset.from_tensor_slices(data_dict['valid_data'].astype(np.float32)).cache().repeat()
        dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))
        return dataset
    return data_fn

def model_fn(features, mode, params):
    #tf.set_random_seed(SEED) # Use for BasicRNNCell, it does not get an initializer
    batch_size=params['batch_size']
    ts = features.get_shape().as_list()[1]
    seq_len = ts * np.ones([batch_size,])
    with tf.variable_scope('encoder'):
        init_kern = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)
        #cell = tf.contrib.rnn.BasicRNNCell(num_units=20)
        cell = tf.contrib.rnn.GRUCell(num_units=20, kernel_initializer=init_kern)
        _, output_latent = tf.nn.dynamic_rnn(cell=cell, inputs=features, sequence_length=seq_len, dtype=tf.float32)
    with tf.variable_scope('decoder'):
        init_kern = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)
        #cell = tf.contrib.rnn.BasicRNNCell(num_units=20)
        cell = tf.contrib.rnn.GRUCell(num_units=20, kernel_initializer=init_kern)
        z_inps = tf.zeros([batch_size, ts, 1])
        output_recon, _ = tf.nn.dynamic_rnn(cell=cell, inputs=z_inps, initial_state=output_latent, sequence_length=seq_len, dtype=tf.float32)    

    winit = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)
    output_recon = tf.contrib.layers.fully_connected(inputs=output_recon, num_outputs=params['dims'], activation_fn=None, weights_initializer=winit)
    loss = tf.losses.mean_squared_error(features, output_recon)

    global_step = tf.train.get_global_step()
    opt = tf.train.AdamOptimizer(0.01)
    if USE_TPU:
        opt = tf.contrib.tpu.CrossShardOptimizer(opt)
    train_op = opt.minimize(loss, global_step)
    def metric_fn(labels, rec):
        return {
        'MSE': tf.metrics.mean_squared_error(labels, rec),
        }
    tpu_eval_metrics = (metric_fn, [features, output_recon])

    return tpu_estimator.TPUEstimatorSpec(mode=mode,
                                      loss=loss,
                                      train_op=train_op,
                                      eval_metrics=tpu_eval_metrics,
                                      )

def train_model(num_steps, iterations_per_loop, num_shards=1):
    if USE_TPU:
        my_project = subprocess.check_output([
            'gcloud','config','get-value','project'])
        my_zone = subprocess.check_output([
            'gcloud','config','get-value','compute/zone'])
        tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
                tpu=[os.environ['TPU_NAME']],
                )
        tpu_cluster_resolver = tpu_cluster_resolver
    else:
        tpu_cluster_resolver = None

    #tf.logging.set_verbosity(tf.logging.INFO)
    config = tf.ConfigProto(allow_soft_placement=True,
                            log_device_placement=True)

    run_config = tf.contrib.tpu.RunConfig(
        save_checkpoints_steps=400,
        cluster=tpu_cluster_resolver,
        keep_checkpoint_max=1,
        model_dir = 'gs://test-bucket/runs',
        session_config=config,
        tpu_config=tf.contrib.tpu.TPUConfig(iterations_per_loop=iterations_per_loop, num_shards=num_shards))
    
    params = {'dims': 5}
    data = make_data(params)

    train_input = input_fn(data, tf.estimator.ModeKeys.TRAIN)
    eval_input = input_fn(data, tf.estimator.ModeKeys.EVAL)

    model = tf.contrib.tpu.TPUEstimator(model_fn=model_fn, params=params, config=run_config,
        use_tpu=USE_TPU, train_batch_size=100, eval_batch_size=100)
    model.train(train_input, steps=num_steps)
    
    valid_costs = model.evaluate(eval_input, name='valid_data', steps=2)
    print('==== Evaluation:')
    print(valid_costs)
    return valid_costs



print(""==================== Training with iterations_per_loop = 1"")
run1 = train_model(num_steps=100, iterations_per_loop=1, num_shards=1)

# remove checkpoints
subprocess.call(""gsutil -m rm -r gs://test-bucket/runs/*"", shell=True)

print(""==================== Training with iterations_per_loop = 100"")
run2 = train_model(num_steps=100, iterations_per_loop=100, num_shards=1)

print('Summary:')
print('====== iterations_per_loop = 1 :')
print(run1)

print('====== iterations_per_loop = 100 :')
print(run2)
```

### Output (multiple runs):
**CPU Run:**
```{'loss': 0.2408253, 'MSE': 0.24082531, 'global_step': 100}```

**TPU Runs:**
iterations_per_loop=1
Run1:
```{'loss': 0.24119371, 'MSE': 0.2411936, 'global_step': 100}```
Run2:
```{'loss': 0.24119371, 'MSE': 0.2411936, 'global_step': 100}```

iterations_per_loop=100
Run1:
```{'loss': 29.255905, 'MSE': 29.25589, 'global_step': 100}```
Run2:
```{'loss': 29.255905, 'MSE': 29.25589, 'global_step': 100}```"
21284,TensorFlow 1.9.0 deadlocks on libc6 2.19?,"### System information
- **Have I written custom code**: Yes, reproduction repo: https://github.com/ravwojdyla/tf190bug
- **OS Platform and Distribution**: Ubuntu 14.04, Container-Optimized OS
- **TensorFlow installed from (source or binary)**:  binary
- **TensorFlow version (use command below)**: 1.9.0
- **Exact command to reproduce**: see repo https://github.com/ravwojdyla/tf190bug
- **Mobile device**: N/A
- **Python version**: N/A
- **Bazel version**: N/A
- **GCC/Compiler version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A

### Describe the problem
It seems like TF 1.9.0 deadlocks when used concurrently and linked to libc 2.19 (libpthread?)  (Ubuntu 14.04 and COS to name a some). See the reproduction repo. This is specifically a problem if TF 1.9.0 is used on Google Dataflow worker which  currently ship with libc 2.19. The libc/libpthread is currently just a theory, that said I validated that the reproduction code works fine with libc 2.24 (Ubuntu 16.04).

I might continue investigating this issue, but wanted to double check if this is a known problem?

### Source code / logs

Reproduction repo: https://github.com/ravwojdyla/tf190bug

Stack of one of the threads:

```
""Thread-9"" #18 prio=5 os_prio=0 tid=0x00007fa568149800 nid=0x4a runnable [0x00007fa55244e000]
   java.lang.Thread.State: RUNNABLE
        at org.tensorflow.Tensor.allocateScalarBytes(Native Method)
        at org.tensorflow.Tensor.create(Tensor.java:150)
        at org.tensorflow.Tensor.create(Tensor.java:115)
        at org.tensorflow.Tensors.create(Tensors.java:257)
        at sh.rav.TestTF.createTensor(TestTF.java:11)
        at sh.rav.TestTF$1.run(TestTF.java:18)
        at java.lang.Thread.run(Thread.java:745)
```

Those threads are all in runnable state, and in fact they do burn CPU, I checked the native frames as well, it seems like some form of a starvation."
21277,Using TensorFlow's Datasets API causes process to hang in session destructor,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

MacOS High Sierra (10.13.1), though we've also seen this happen on Linux as well, we believe.

- **TensorFlow installed from (source or binary)**:

Source (but happens with the binary version as well)

- **TensorFlow version (use command below)**:

v1.8.0-0-g93bc2e2072 1.8.0

- **Python version**:

Python 3.6.1 (v3.6.1:69c0db5050, Mar 21 2017, 01:21:04)

- **Bazel version (if compiling from source)**:

0.10.1

- **GCC/Compiler version (if compiling from source)**:

Apple LLVM version 9.0.0 (clang-900.0.39.2)

- **CUDA/cuDNN version**:

N/A

- **GPU model and memory**:

N/A

- **Exact command to reproduce**:

Unfortunately the issue isn't that easy to reproduce without running our application (I haven't managed to produce a smaller test case).

### Describe the problem

**Summary:**

We are using TensorFlow's Datasets API. More specifically, we're using `tf.data.Dataset.from_generator` to create a dataset based on a generator function.

When Python comes to garbage collect our `tf.Session` object its destructor makes a call into TensorFlow to delete the session (`tf_session.TF_DeleteSession`). This call hangs because it's trying to execute a `tf.py_func` function, but cannot acquire Python's global interpreter lock. The function its trying to execute appears to be the ""finalize"" function from our dataset.

This looks to me like a bug in TensorFlow, as (my understanding is) that we shouldn't be able to write code that causes this to happen. Although it's clearly a consequence of our specific use of TensorFlow I can't see that we're doing anything in our application that we shouldn't be.

**More Details:**

When our `tf.Session` object is garbage collected in Python, its destructor (`__del__` method) hangs indefinitely. The problem appears to be this call in `BaseSession`:

    tf_session.TF_DeleteSession(self._session)

Running lldb shows the following stack trace:

    * thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGSTOP
      * frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
        frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
        frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18
        frame #3: 0x000000011279a63b libtensorflow_framework.so`nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) + 283
        frame #4: 0x0000000112796eb7 libtensorflow_framework.so`nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) + 423
        frame #5: 0x0000000112797621 libtensorflow_framework.so`nsync::nsync_cv_wait(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*) + 49
        frame #6: 0x00000001090810e3 _pywrap_tensorflow_internal.so`tensorflow::Notification::WaitForNotification() + 67
        frame #7: 0x0000000109d4d809 _pywrap_tensorflow_internal.so`tensorflow::CapturedFunction::RunInstantiated(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) + 649
        frame #8: 0x0000000109cffa21 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 97
        frame #9: 0x0000000109cffb8e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 14
        frame #10: 0x0000000109cfd669 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 105
        frame #11: 0x0000000109cfd6de _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 14
        frame #12: 0x00000001019e98fd libc++.1.dylib`std::__1::__shared_weak_count::__release_shared() + 43
        frame #13: 0x0000000109d0a579 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 169
        frame #14: 0x0000000109d0a5fe _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 14
        frame #15: 0x000000011226db4d libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 301
        frame #16: 0x000000011226dd50 libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::type_index, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 192
        frame #17: 0x0000000109d0c558 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 104
        frame #18: 0x0000000109d0c71e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 14
        frame #19: 0x00000001122670ff libtensorflow_framework.so`tensorflow::OpSegment::Item::~Item() + 63
        frame #20: 0x0000000112267ffd libtensorflow_framework.so`tensorflow::OpSegment::RemoveHold(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 205
        frame #21: 0x000000010b880b42 _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 546
        frame #22: 0x000000010b88108e _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 14
        frame #23: 0x000000010935dfd3 _pywrap_tensorflow_internal.so`TF_DeleteSession + 931
        frame #24: 0x0000000109006e5a _pywrap_tensorflow_internal.so`_wrap_TF_DeleteSession(_object*, _object*) + 122
        frame #25: 0x00000001007bb688 Python`_PyCFunction_FastCallDict + 568
        frame #26: 0x00000001008443e4 Python`call_function + 612
        frame #27: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
        frame #28: 0x00000001008447cc Python`_PyFunction_FastCallDict + 828
        frame #29: 0x000000010075f984 Python`_PyObject_FastCallDict + 356
        frame #30: 0x000000010075faa0 Python`_PyObject_Call_Prepend + 208
        frame #31: 0x000000010075f8d4 Python`_PyObject_FastCallDict + 180
        frame #32: 0x00000001007d6579 Python`slot_tp_finalize + 121
        frame #33: 0x000000010089b18a Python`collect + 1418
        frame #34: 0x000000010089b8c3 Python`_PyGC_CollectIfEnabled + 99
        frame #35: 0x000000010087af57 Python`Py_FinalizeEx + 119
        frame #36: 0x000000010087b0e0 Python`Py_Exit + 16
        frame #37: 0x000000010087ef4c Python`handle_system_exit + 252
        frame #38: 0x000000010087f1a5 Python`PyErr_PrintEx + 437
        frame #39: 0x0000000100880a1d Python`PyRun_SimpleStringFlags + 125
        frame #40: 0x00000001008992a4 Python`Py_Main + 1812
        frame #41: 0x0000000100000dfe Python
        frame #42: 0x0000000100000c34 Python

It appears that the session's destructor is waiting for an op to complete. The culprit seems to be `PyFuncOp`, which doesn't get past this line:

    py_threadstate = PyGILState_Ensure();

So it looks like this op is trying to acquire the GIL but can't. My assumption is that this py_func is the ""finalize"" function for the dataset (from `_GeneratorDataset`).

My assumption is that when Python calls `tf_session.TF_DeleteSession(self._session)` that the GIL should be released, and so `PyFuncOp` should then be able to acquire it again. Indeed, when I write an isolated test to try and reproduce this I don't see this problem, and the GIL is acquired successfully.

Unfortunately, as I mention, I have been unsuccessful in writing an isolated test case to reproduce the problem. The problem only seems to happen when we use our application in a particular scenario, but I haven't been able to isolate exactly what it is about this scenario that causes the problem."
21275,Release RC to jcenter,"I want to use the latest release candidates.
However they are not published to Jcenter: https://bintray.com/google/tensorflow/tensorflow-lite"
21274,Using a tuple of Numpy arrays as validation_data fails when fitting a tf.keras model with a tf.data.Dataset,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.6.6
- **CUDA/cuDNN version**:9.2
- **GPU model and memory**: NA
- **Exact command to reproduce**:
```
model.fit(train_dataset,
          steps_per_epoch=100,
          validation_data=(val_samples, valid_targets),
          epochs=10)
```
### Describe the problem
Usecase: train a tf.keras.Model using the `fit` method. I for training data I have a `tf.data.Dataset` but for  `validation_data` I'd like to have a prefetched tuple `(x_val, y_val)` of Numpy arrays. At the end of the epoch it fails with the error `TypeError: float() argument must be a string or a number, not 'NoneType'. `

It seems it's trying to calculate `validation_steps` but it fails. Passing `validation_steps` will avoid the exception but it's unnecessary and i'm not sure what the `fit` model does with it.

### Source code / logs

```

#### train Dataset

with tf.device('/cpu:0'):
    dataset = tf.data.Dataset.from_tensor_slices(
    (train_df.file.values, train_targets))
    dataset = dataset.apply(
        tf.contrib.data.shuffle_and_repeat(len(train_df)))
    dataset = dataset.apply(tf.contrib.data.map_and_batch(
    map_func=proces_audio, batch_size=batch_size,
        num_parallel_calls=64))

#### model definition
x_logml = tf.keras.Input(shape=(timesteps,input_dim))
x = get_conv_layers(x_logml)  # returns a stack of conv/batch_norm/max_pool layers
x = tf.keras.layers.Dense(128, activation = 'relu')(x) 
x = tf.keras.layers.Dense(len(classes), activation = 'softmax')(x)
model = tf.keras.Model(inputs = x_logml, outputs = x)
model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001), 
              loss=['categorical_crossentropy'],
              metrics=['accuracy'])

model.fit(dataset,
          steps_per_epoch=100,
          validation_data=(val_samples,valid_targets),
#               validation_steps=10,  #including this line will avoid the exception, but not sure what's the point
         epochs=10)
```

as a smoke test, I used `val_samples` and `valid_targets` as training data (`x` and `y`) and it runs fine. So the problem seems to be when using a `tf.data.Dataset` and `(x_val, y_val)`. Using a `tf.data.Dataset` as `validation_data` (with `validation_steps`) also works fine.



error log at the end of epoch:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-22-b8d33816aaa6> in <module>()
      9               validation_data=(val_samples,valid_targets),#val_dataset,
     10 #               validation_steps=len(valid_df)//16,
---> 11              epochs=100)

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1346           initial_epoch=initial_epoch,
   1347           steps_per_epoch=steps_per_epoch,
-> 1348           validation_steps=validation_steps)
   1349 
   1350   def evaluate(self,

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py in fit_loop(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
    218             batch_size=batch_size,
    219             steps=validation_steps,
--> 220             verbose=0)
    221         if not isinstance(val_outs, list):
    222           val_outs = [val_outs]

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py in test_loop(model, inputs, targets, sample_weights, batch_size, verbose, steps)
    459         outs[i] /= steps
    460   else:
--> 461     batches = make_batches(num_samples, batch_size)
    462     index_array = np.arange(num_samples)
    463     for batch_index, (batch_start, batch_end) in enumerate(batches):

~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py in make_batches(size, batch_size)
    465       A list of tuples of array indices.
    466   """"""
--> 467   num_batches = int(np.ceil(size / float(batch_size)))
    468   return [(i * batch_size, min(size, (i + 1) * batch_size))
    469           for i in range(0, num_batches)]

TypeError: float() argument must be a string or a number, not 'NoneType'
```"
21273,error occurring while running my motion detectioncode on jupyter notebook,"usage: ipykernel_launcher.py [-h] [-v VIDEO] [-a MIN_AREA]
ipykernel_launcher.py: error: unrecognized arguments: -f C:\Users\HP\AppData\Roaming\jupyter\runtime\kernel-fb666fe6-3260-4858-b7c8-b37fb9759c42.json
An exception has occurred, use %tb to see the full traceback.

SystemExit: 2"
21272,bad performance when build with jemalloc,"I try to build tf r1.8 with jemalloc.
I test the perfermance of r1.8 with jemalloc, found that the model use more sys use. the result as follow:

```
tf r1.8 with jemalloc:
    virturl memory: 29581 MB,real memory: 16185 MB, user cpu: 751.623, sys cpu: 145.979
tf r1.8 without  jemalloc:
    virturl memory: 24491 MB,real memory: 14520 MB, user cpu: 748.362, sys cpu: 18.4359
```
I tested several models and found that they all have this phenomenon."
21271,tf.nn.softmax_cross_entropy_with_logits_v2 returns wrong value with soft labels,"### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution**: Linux CentOS 7.4
- **Mobile device if the issue happens on mobile device**: N/A
- **TensorFlow installed from**: binary
- **TensorFlow version**: v1.9.0-0-g25c197e023 1.9.0
- **Python version**: Python 3.6.3
- **Bazel version**: N/A
- **GCC/Compiler version**: N/A
- **CUDA/cuDNN version**: 9.0 / 7.0.5
- **GPU model and memory**:  GeForce 940MX, 4GB
- **Exact command to reproduce**: save the codes below as a .py file, and run it with command-line something like `python3 test.py`.

### Describe the problem
I use soft labels (for example, [0.2, 0.8] instead of [0, 1]) in a CNN model, in which I use `tf.nn.softmax_cross_entropy_with_logits_v2` for loss computing. But when I trained the model, the loss became +inf in 10 steps, so I debugged the codes and found that the problem was caused by `tf.nn.softmax_cross_entropy_with_logits_v2`.
So I implemented the softmax and cross_entropy process separately, then the returned value seemed to make sense. 

### Source code / logs
```python
import numpy as np
import tensorflow as tf

tf.enable_eager_execution()
tf.executing_eagerly()

logits = [[-4885.4614, 4878.027], [-5188.321, 5179.7915], [-4121.558, 4114.9995], [-5165.612, 5157.5044], [-4152.7183, 4145.978], [-5175.428, 5167.603], [-4514.224, 4506.477], [-4752.5854, 4745.2524], [-5580.9463, 5572.2275], [-5164.766, 5156.6685], [-4273.686, 4266.31], [-4886.724, 4879.5757], [-5216.2935, 5208.269], [-5411.082, 5402.344], [-6057.239, 6048.3647], [-5314.882, 5306.708], [-5674.2505, 5664.6436], [-5650.7827, 5642.1997], [-4301.4194, 4294.5957], [-5156.4683, 5148.283], [-5032.6797, 5024.821], [-5072.1533, 5064.013], [-4129.488, 4123.4355], [-4915.1147, 4907.8643], [-5256.5747, 5248.351], [-5297.694, 5289.386], [-4979.939, 4971.691], [-4895.983, 4887.4897], [-4757.732, 4749.2886], [-4871.5654, 4863.604], [-4772.0356, 4763.9414], [-4528.853, 4521.4424],]
labels = [[9.6226019e-01, 3.7739828e-02], [9.5367432e-07, 9.9999905e-01], [9.9017835e-01, 9.8216468e-03], [8.3446503e-07, 9.9999917e-01], [9.9999952e-01, 4.9012118e-07], [3.6466300e-02, 9.6353370e-01], [3.2732385e-01, 6.7267615e-01], [2.1918458e-01, 7.8081542e-01], [3.4707606e-02, 9.6529239e-01], [1.3036132e-03, 9.9869639e-01], [4.1835755e-01, 5.8164245e-01], [5.0599152e-01, 4.9400848e-01], [9.9629784e-01, 3.7021455e-03], [1.9747615e-03, 9.9802524e-01], [8.2850456e-06, 9.9999171e-01], [8.1463850e-01, 1.8536153e-01], [7.6112747e-03, 9.9238873e-01], [9.4729370e-01, 5.2706327e-02], [9.4496381e-01, 5.5036198e-02], [3.3008814e-02, 9.6699119e-01], [1.0292470e-02, 9.8970753e-01], [9.9998099e-01, 1.9039073e-05], [7.5116873e-01, 2.4883127e-01], [9.9973243e-01, 2.6756502e-04], [3.1858683e-04, 9.9968141e-01], [3.6358833e-05, 9.9996364e-01], [9.3631679e-01, 6.3683234e-02], [7.9292524e-01, 2.0707479e-01], [9.9999642e-01, 3.6055078e-06], [4.9336654e-01, 5.0663346e-01], [6.9030523e-03, 9.9309695e-01], [9.9974275e-01, 2.5725813e-04],]

logits = np.array(logits, dtype=np.float32)
labels = np.array(labels, dtype=np.float32)

# Check each row of labels is a valid probability distribution.
labels_sum = tf.reduce_sum(labels, axis=-1)
print(labels_sum)

# Cross entropy computed by tf.nn.softmax_cross_entropy_with_logits_v2
cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)
cross_entropy_mean = tf.reduce_mean(cross_entropy)
print(cross_entropy_mean)

# Cross entropy computed separately
softmax = tf.nn.softmax(logits)
cross_entropy_mean = tf.reduce_mean(-tf.reduce_sum(softmax * tf.log(labels), axis=-1))
print(cross_entropy_mean)
```

**Output:**
```
tf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(32,), dtype=float32)
tf.Tensor(4538.922, shape=(), dtype=float32)
tf.Tensor(2.621081, shape=(), dtype=float32)
```"
21269,Compiling TF for Ubuntu: Failed,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04

- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
No

- **TensorFlow installed from (source or binary)**:
Checked out tag origin/r1.9

- **TensorFlow version (use command below)**:
r1.9.0

- **Python version**:
3.6

- **Bazel version (if compiling from source)**:
Build label: 0.15.2
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Jul 17 12:22:40 2018 (1531830160)
Build timestamp: 1531830160
Build timestamp as int: 1531830160

- **GCC/Compiler version (if compiling from source)**:
gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) 

- **CUDA/cuDNN version**:
n/a; compiling for non-GPU machine

- **GPU model and memory**:
n/a

- **Exact command to reproduce**:
`bazel build --config opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni`

You can collect some of this information using our environment capture script:

```
(tensorflow_p36) ubuntu:/efs/tensorflow/tools$ bash tf_env_collect.sh 
Collecting system information...
/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file ""/home/ubuntu/.config/matplotlib/matplotlibrc"", line #2
  (fname, cnt))
/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file ""/home/ubuntu/.config/matplotlib/matplotlibrc"", line #3
  (fname, cnt))
2018-07-31 10:32:11.836897: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-07-31 10:32:12.013176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-07-31 10:32:12.013518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2018-07-31 10:32:12.013553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
Wrote environment to tf_env.txt. You can review the contents of that file.
and use it to populate the fields in the github issue template.

cat tf_env.txt
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Here is the error message.  It doesnt say much:
```
external/curl/lib/vtls/openssl.c: At top level:
cc1: warning: unrecognized command line option ""-Wno-string-plus-int"" [enabled by default]
INFO: Elapsed time: 450.149s, Critical Path: 21.18s
INFO: 1372 processes: 1372 local.
FAILED: Build did NOT complete successfully
```
Here are the 100 lines that precede it:
```
In file included from external/curl/lib/vtls/openssl.c:72:0:
/home/ubuntu/anaconda3/envs/tensorflow_p36/bin/../lib/gcc/../../include/openssl/ocsp.h:567:34: error: unknown type name 'OCSP_RESPONSE'
 int OCSP_RESPONSE_print(BIO *bp, OCSP_RESPONSE *o, unsigned long flags);
                                  ^
external/curl/lib/vtls/openssl.c: In function 'verifystatus':
external/curl/lib/vtls/openssl.c:1262:3: error: unknown type name 'OCSP_RESPONSE'
   OCSP_RESPONSE *rsp = NULL;
   ^
external/curl/lib/vtls/openssl.c:1267:3: warning: implicit declaration of function 'SSL_get_tlsext_status_ocsp_resp' [-Wimplicit-function-declaration]
   long len = SSL_get_tlsext_status_ocsp_resp(connssl->handle, &p);
   ^
external/curl/lib/vtls/openssl.c:1275:3: warning: implicit declaration of function 'd2i_OCSP_RESPONSE' [-Wimplicit-function-declaration]
   rsp = d2i_OCSP_RESPONSE(NULL, &p, len);
   ^
external/curl/lib/vtls/openssl.c:1275:7: warning: assignment makes pointer from integer without a cast [enabled by default]
   rsp = d2i_OCSP_RESPONSE(NULL, &p, len);
       ^
external/curl/lib/vtls/openssl.c:1282:3: warning: implicit declaration of function 'OCSP_response_status' [-Wimplicit-function-declaration]
   ocsp_status = OCSP_response_status(rsp);
   ^
external/curl/lib/vtls/openssl.c:1290:3: warning: implicit declaration of function 'OCSP_response_get1_basic' [-Wimplicit-function-declaration]
   br = OCSP_response_get1_basic(rsp);
   ^
external/curl/lib/vtls/openssl.c:1290:6: warning: assignment makes pointer from integer without a cast [enabled by default]
   br = OCSP_response_get1_basic(rsp);
      ^
external/curl/lib/vtls/openssl.c:1374:3: warning: implicit declaration of function 'OCSP_RESPONSE_free' [-Wimplicit-function-declaration]
   OCSP_RESPONSE_free(rsp);
   ^
external/curl/lib/vtls/openssl.c: In function 'ossl_connect_step1':
external/curl/lib/vtls/openssl.c:2061:5: warning: implicit declaration of function 'SSL_set_tlsext_status_type' [-Wimplicit-function-declaration]
     SSL_set_tlsext_status_type(connssl->handle, TLSEXT_STATUSTYPE_ocsp);
     ^
external/curl/lib/vtls/openssl.c: In function 'get_cert_chain':
external/curl/lib/vtls/openssl.c:2390:9: warning: passing argument 1 of 'X509_get0_signature' from incompatible pointer type [enabled by default]
         X509_get0_signature(&psig, &palg, x);
         ^
In file included from external/boringssl/src/include/openssl/pem.h:66:0,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/x509.h:761:21: note: expected 'const struct ASN1_BIT_STRING **' but argument is of type 'struct ASN1_BIT_STRING **'
 OPENSSL_EXPORT void X509_get0_signature(const ASN1_BIT_STRING **psig,
                     ^
external/curl/lib/vtls/openssl.c:2390:9: warning: passing argument 2 of 'X509_get0_signature' from incompatible pointer type [enabled by default]
         X509_get0_signature(&psig, &palg, x);
         ^
In file included from external/boringssl/src/include/openssl/pem.h:66:0,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/x509.h:761:21: note: expected 'const struct X509_ALGOR **' but argument is of type 'struct X509_ALGOR **'
 OPENSSL_EXPORT void X509_get0_signature(const ASN1_BIT_STRING **psig,
                     ^
external/curl/lib/vtls/openssl.c:2455:11: warning: passing argument 2 of 'RSA_get0_key' from incompatible pointer type [enabled by default]
           RSA_get0_key(rsa, &n, &e, &d);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:82:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/rsa.h:95:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void RSA_get0_key(const RSA *rsa, const BIGNUM **out_n,
                     ^
external/curl/lib/vtls/openssl.c:2455:11: warning: passing argument 3 of 'RSA_get0_key' from incompatible pointer type [enabled by default]
           RSA_get0_key(rsa, &n, &e, &d);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:82:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/rsa.h:95:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void RSA_get0_key(const RSA *rsa, const BIGNUM **out_n,
                     ^
external/curl/lib/vtls/openssl.c:2455:11: warning: passing argument 4 of 'RSA_get0_key' from incompatible pointer type [enabled by default]
           RSA_get0_key(rsa, &n, &e, &d);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:82:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/rsa.h:95:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void RSA_get0_key(const RSA *rsa, const BIGNUM **out_n,
                     ^
external/curl/lib/vtls/openssl.c:2456:11: warning: passing argument 2 of 'RSA_get0_factors' from incompatible pointer type [enabled by default]
           RSA_get0_factors(rsa, &p, &q);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:82:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/rsa.h:100:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void RSA_get0_factors(const RSA *rsa, const BIGNUM **out_p,
                     ^
external/curl/lib/vtls/openssl.c:2456:11: warning: passing argument 3 of 'RSA_get0_factors' from incompatible pointer type [enabled by default]
           RSA_get0_factors(rsa, &p, &q);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:82:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/rsa.h:100:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void RSA_get0_factors(const RSA *rsa, const BIGNUM **out_p,
                     ^
external/curl/lib/vtls/openssl.c:2457:11: warning: passing argument 2 of 'RSA_get0_crt_params' from incompatible pointer type [enabled by default]
           RSA_get0_crt_params(rsa, &dmp1, &dmq1, &iqmp);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:82:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/rsa.h:107:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void RSA_get0_crt_params(const RSA *rsa, const BIGNUM **out_dmp1,
                     ^
external/curl/lib/vtls/openssl.c:2457:11: warning: passing argument 3 of 'RSA_get0_crt_params' from incompatible pointer type [enabled by default]
           RSA_get0_crt_params(rsa, &dmp1, &dmq1, &iqmp);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:82:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/rsa.h:107:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void RSA_get0_crt_params(const RSA *rsa, const BIGNUM **out_dmp1,
                     ^
external/curl/lib/vtls/openssl.c:2457:11: warning: passing argument 4 of 'RSA_get0_crt_params' from incompatible pointer type [enabled by default]
           RSA_get0_crt_params(rsa, &dmp1, &dmq1, &iqmp);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:82:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/rsa.h:107:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void RSA_get0_crt_params(const RSA *rsa, const BIGNUM **out_dmp1,
                     ^
external/curl/lib/vtls/openssl.c:2500:11: warning: passing argument 2 of 'DSA_get0_pqg' from incompatible pointer type [enabled by default]
           DSA_get0_pqg(dsa, &p, &q, &g);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:74:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/dsa.h:101:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void DSA_get0_pqg(const DSA *dsa, const BIGNUM **out_p,
                     ^
external/curl/lib/vtls/openssl.c:2500:11: warning: passing argument 3 of 'DSA_get0_pqg' from incompatible pointer type [enabled by default]
           DSA_get0_pqg(dsa, &p, &q, &g);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:74:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/dsa.h:101:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void DSA_get0_pqg(const DSA *dsa, const BIGNUM **out_p,
                     ^
external/curl/lib/vtls/openssl.c:2500:11: warning: passing argument 4 of 'DSA_get0_pqg' from incompatible pointer type [enabled by default]
           DSA_get0_pqg(dsa, &p, &q, &g);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:74:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/dsa.h:101:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void DSA_get0_pqg(const DSA *dsa, const BIGNUM **out_p,
                     ^
external/curl/lib/vtls/openssl.c:2501:11: warning: passing argument 2 of 'DSA_get0_key' from incompatible pointer type [enabled by default]
           DSA_get0_key(dsa, &pub_key, &priv_key);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:74:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/dsa.h:96:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void DSA_get0_key(const DSA *dsa, const BIGNUM **out_pub_key,
                     ^
external/curl/lib/vtls/openssl.c:2501:11: warning: passing argument 3 of 'DSA_get0_key' from incompatible pointer type [enabled by default]
           DSA_get0_key(dsa, &pub_key, &priv_key);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:74:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/dsa.h:96:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void DSA_get0_key(const DSA *dsa, const BIGNUM **out_pub_key,
                     ^
external/curl/lib/vtls/openssl.c:2533:11: warning: passing argument 2 of 'DH_get0_pqg' from incompatible pointer type [enabled by default]
           DH_get0_pqg(dh, &p, &q, &g);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:73:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/dh.h:102:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void DH_get0_pqg(const DH *dh, const BIGNUM **out_p,
                     ^
external/curl/lib/vtls/openssl.c:2533:11: warning: passing argument 3 of 'DH_get0_pqg' from incompatible pointer type [enabled by default]
           DH_get0_pqg(dh, &p, &q, &g);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:73:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/dh.h:102:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void DH_get0_pqg(const DH *dh, const BIGNUM **out_p,
                     ^
external/curl/lib/vtls/openssl.c:2533:11: warning: passing argument 4 of 'DH_get0_pqg' from incompatible pointer type [enabled by default]
           DH_get0_pqg(dh, &p, &q, &g);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:73:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/dh.h:102:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void DH_get0_pqg(const DH *dh, const BIGNUM **out_p,
                     ^
external/curl/lib/vtls/openssl.c:2534:11: warning: passing argument 2 of 'DH_get0_key' from incompatible pointer type [enabled by default]
           DH_get0_key(dh, &pub_key, &priv_key);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:73:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/dh.h:92:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void DH_get0_key(const DH *dh, const BIGNUM **out_pub_key,
                     ^
external/curl/lib/vtls/openssl.c:2534:11: warning: passing argument 3 of 'DH_get0_key' from incompatible pointer type [enabled by default]
           DH_get0_key(dh, &pub_key, &priv_key);
           ^
In file included from external/boringssl/src/include/openssl/x509.h:73:0,
                 from external/boringssl/src/include/openssl/pem.h:66,
                 from external/boringssl/src/include/openssl/ssl.h:149,
                 from external/curl/lib/urldata.h:86,
                 from external/curl/lib/vtls/openssl.c:41:
external/boringssl/src/include/openssl/dh.h:92:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'
 OPENSSL_EXPORT void DH_get0_key(const DH *dh, const BIGNUM **out_pub_key,
                     ^
external/curl/lib/vtls/openssl.c: At top level:
cc1: warning: unrecognized command line option ""-Wno-string-plus-int"" [enabled by default]
INFO: Elapsed time: 450.149s, Critical Path: 21.18s
INFO: 1372 processes: 1372 local.
FAILED: Build did NOT complete successfully
```

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21268,Failed to load the native tensorflow runtime,"### System information

- **TensorFlow version (use command below)**: 1.9.0
- **Python version**: 3.6.2

### Describe the problem
First time using tensorflow, I have installed it using the command pip3 install --upgrade tensorflow on the terminal. When trying to import tensorflow as tf in python I get the error below

### Source code / logs

import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Aishwarya\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Aishwarya\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Aishwarya\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Aishwarya\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Aishwarya\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Aishwarya\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Aishwarya\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Aishwarya\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Aishwarya\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Aishwarya\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Aishwarya\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 978, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 648, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 560, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 205, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Aishwarya\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Aishwarya\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Aishwarya\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Aishwarya\AppData\Local\Programs\Python\Python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
21267,tf.constant should support dynamic shape,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX 10.13.5
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: pip binary
- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: see below

### Describe the problem


### Source code / logs

Example code:

`tf.constant(3, shape=(3, 1, tf.constant(3) * 5))`

Will throw an error (very unrelated / confusing: `ValueError: setting an array element with a sequence.`).

Corresponding `tf.zeros` or `tf.ones` works, so it is a counter-intuitive that `tf.constant` does not.

(Workaround for me, but this seems a bit ugly: `tf.tile(tf.constant(value, shape=[1] * len(shape)), shape)`)"
21266,[tflite][android]deeplab-v3+ runtime error on Pad Ops,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: BlackBerry KEY2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master branch last commit is 78f58629ae4a47a799e88da7a78ecfcb6a6115cc
- **Python version**:3.6.3
- **Bazel version (if compiling from source)**:0.15.2
- **GCC/Compiler version (if compiling from source)**: NDK r17 toolchain
- **CUDA/cuDNN version**:NA
- **GPU model and memory**:NA
- **Exact command to reproduce**:

### Describe the problem
I download the pre-trained modals with MobileNet-v2 from [mobilenetv2_coco_voc_trainaug](http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz). covered model to tflite then load into android application, I see internal error at OP Pad on prepare.

### Source code / logs
using below command to cover the model without any errors:
```
bazel run //tensorflow/contrib/lite/toco:toco -- \
  --input_file=/tmp/frozen_inference_graph.pb \
  --output_file=/tmp/optimized_graph.tflite \
  --input_format=TENSORFLOW_GRAPHDEF \
  --output_format=TFLITE \
  --input_type=QUANTIZED_UINT8 \
  --input_arrays=ImageTensor \
  --output_arrays=SemanticPredictions \
  --input_shapes=1,513,513,3
```
below command to build tensorflow-lite.aar
```
bazel build --cxxopt='--std=c++11' -c opt        \
  --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   \
  //tensorflow/contrib/lite/java:tensorflow-lite
```
Then I load optimized_graph.tflite and tensorflow-lite.aar into Android application project
```
  private static final int DIM_PIXEL_SIZE = 3;
  static final int DIM_IMG_SIZE_X = 513;
  static final int DIM_IMG_SIZE_Y = 513;

    tflite = new Interpreter(loadModelFile(activity));
    imgData =
        ByteBuffer.allocateDirect(
            DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y * DIM_PIXEL_SIZE);
    imgData.order(ByteOrder.nativeOrder());
    outputs = new int[DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y];
```
```
  /** Memory-map the model file in Assets. */
  private MappedByteBuffer loadModelFile(Activity activity) throws IOException {
    AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODEL_PATH);
    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());
    FileChannel fileChannel = inputStream.getChannel();
    long startOffset = fileDescriptor.getStartOffset();
    long declaredLength = fileDescriptor.getDeclaredLength();
    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
  }
```
run Interpreter
```
tflite.run(imgData, outputs);
```
Error Logs:
```
07-31 16:20:36.144 25819-25974/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground
    Process: android.example.com.tflitecamerademo, PID: 25819
    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/contrib/lite/kernels/pad.cc:96 op_context.dims != 4 (3 != 4)Node number 24 (PAD) failed to prepare.
    
        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:130)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:168)
        at org.tensorflow.lite.Interpreter.run(Interpreter.java:145)
```
"
21265,//tensorflow/python/kernel_tests:conv_ops_test fails on AVX512 builds,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: commit ecd8decac3d9f3c7cd772e1561b9c2d3f23aa830 on the master branch
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**: [bazel release 0.15.0]
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**:

bazel test --config=opt --cache_test_results=no  -- //tensorflow/python/kernel_tests:conv_ops_test

### Describe the problem

The //tensorflow/python/kernel_tests:conv_ops_test test case consistently fails on AVX512 builds of tensorflow

### Source code / logs
```
======================================================================
FAIL: testConv2D3x3FilterStride1x1Same (__main__.DeepConv2DTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py"", line 1796, in testConv2D3x3FilterStride1x1Same
    self._RunTestCases([1, 1], ""SAME"")
  File ""/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py"", line 1790, in _RunTestCases
    self._CompareFwdConv2D(input_shape, filter_shape, conv_strides, padding)
  File ""/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py"", line 1782, in _CompareFwdConv2D
    self.assertAllClose(values_expect, values_test, rtol=1e-5, atol=1e-5)
  File ""/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1381, in assertAllClose
    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
  File ""/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1331, in _assertAllCloseRecursive
    (path_str, path_str, msg))
  File ""/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1286, in _assertArrayLikeAllClose
    a, b, rtol=rtol, atol=atol, err_msg=msg, equal_nan=True)
  File ""/home/user/.local/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py"", line 1396, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File ""/home/user/.local/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py"", line 779, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=1e-05, atol=1e-05
Mismatched value: a is different from b. 
(mismatch 96.60625%)
 x: array([[[[[1257.2019, 1264.4261, 1258.311 , ..., 1258.0616, 1228.0815,
           1245.9379],
          [1875.4814, 1885.9357, 1868.8992, ..., 1865.7366, 1842.2179,...
 y: array([[[[[1256.2858, 1263.7555, 1257.8694, ..., 1256.4175, 1226.5343,
           1244.2854],
          [1874.3364, 1884.7977, 1868.6038, ..., 1864.8274, 1841.5417,...

----------------------------------------------------------------------
Ran 73 tests in 5.034s

FAILED (failures=1)
not close where =  (array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 4, 4, 4]), array([0, 0, 0, ..., 4, 4, 4]), array([0, 0, 0, ..., 2, 2, 2]), array([  0,   1,   2, ..., 125, 126, 127]))
not close lhs =  [1257.2019 1264.4261 1258.311  ... 1840.9    1845.5197 1843.9106]
not close rhs =  [1256.2858 1263.7555 1257.8694 ... 1839.0669 1843.0771 1841.3849]
not close dif =  [0.9161377 0.6706543 0.4416504 ... 1.8331299 2.442505  2.5257568]
not close tol =  [0.01257286 0.01264755 0.01258869 ... 0.01840067 0.01844077 0.01842385]
dtype = float32, shape = (1, 5, 5, 5, 128)
```


```
FAIL: testConv2D3x3FilterStride1x1Valid (__main__.DeepConv2DTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py"", line 1793, in testConv2D3x3FilterStride1x1Valid
    self._RunTestCases([1, 1], ""VALID"")
  File ""/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py"", line 1790, in _RunTestCases
    self._CompareFwdConv2D(input_shape, filter_shape, conv_strides, padding)
  File ""/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py"", line 1782, in _CompareFwdConv2D
    self.assertAllClose(values_expect, values_test, rtol=1e-5, atol=1e-5)
  File ""/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1381, in assertAllClose
    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
  File ""/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1331, in _assertAllCloseRecursive
    (path_str, path_str, msg))
  File ""/home/user/.cache/bazel/_bazel_markus/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1286, in _assertArrayLikeAllClose
    a, b, rtol=rtol, atol=atol, err_msg=msg, equal_nan=True)
  File ""/home/user/.local/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py"", line 1396, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File ""/home/user/.local/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py"", line 779, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=1e-05, atol=1e-05
Mismatched value: a is different from b. 
(mismatch 94.3576388889%)
 x: array([[[[[2802.1404, 2796.723 , 2792.0303, ..., 2813.227 , 2780.2148,
           2796.4958],
          [2797.6106, 2792.4019, 2813.2893, ..., 2790.9302, 2788.8625,...
 y: array([[[[[2802.357 , 2796.4634, 2792.4202, ..., 2812.9595, 2780.5022,
           2796.4705],
          [2797.8032, 2792.7297, 2813.6294, ..., 2791.9949, 2789.6606,...

----------------------------------------------------------------------
Ran 73 tests in 7.059s

FAILED (failures=1)
not close where =  (array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 4, 4, 4]), array([0, 0, 0, ..., 2, 2, 2]), array([0, 0, 0, ..., 1, 1, 1]), array([  0,   1,   2, ..., 125, 126, 127]))
not close lhs =  [2802.1404 2796.723  2792.0303 ... 2763.8906 2751.459  2762.4377]
not close rhs =  [2802.357  2796.4634 2792.4202 ... 2764.6665 2753.163  2764.2102]
not close dif =  [0.21655273 0.25952148 0.38989258 ... 0.7758789  1.7041016  1.7724609 ]
not close tol =  [0.02803357 0.02797463 0.0279342  ... 0.02765667 0.02754163 0.0276521 ]
dtype = float32, shape = (1, 5, 3, 3, 128)
```"
21264,Tensorflow Lite Speech Recognition Android App not able to recognize,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: from source : https://github.com/tensorflow/tensorflow
- **Python version**: Python 2.7.6
- **Bazel version (if compiling from source)**: 0.15.1
- **GCC/Compiler version (if compiling from source)**: gcc 4.8.4
- **Exact command to reproduce**: bazel build -c opt --cxxopt='--std=c++11' --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   //tensorflow/contrib/lite/examples/android:tflite_demo

### the problem
The TFLite Speech Recognition App from the path https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android is not able to Recognize spoken words at all. From my debug, I have got that the below ""if"" condition is failing:

if ((currentTopScore > detectionThreshold) && (timeSinceLastTop > suppressionMs)) {
      previousTopLabel = currentTopLabel;
      previousTopLabelTime = currentTimeMS;
      previousTopLabelScore = currentTopScore;
      isNewCommand = true;
    } else {
      isNewCommand = false;
    }

I'm getting currentTopScore < detectionThreshold always and I'm not able to recognize the spoken words.

### My own added debug logs:
07-27 10:34:15.106  7240  7262 D My_Debug: >>>>>>>>>>arraycopy of recordingBuffer
07-27 10:34:15.106  7240  7262 D My_Debug: >>>>>>>>>>recordingBuffer.unlock
07-27 10:34:15.135  7240  7262 D My_Debug: >>>>>>>>>>inside processLatestResults
07-27 10:34:15.136  7240  7262 D My_Debug: >>>>>>>>>currentTopLabel:_silence_
**07-27 10:34:15.136  7240  7262 D My_Debug: >>>>>>>>>currentTopScore:1.3075984
07-27 10:34:15.136  7240  7262 D My_Debug: >>>>>>>>>detectionThreshold:0.7**
07-27 10:34:15.136  7240  7262 D My_Debug: >>>>>>>>>timeSinceLastTop:9223372036854775807
07-27 10:34:15.136  7240  7262 D My_Debug: >>>>>>>>>suppressionMs:1500
07-27 10:34:15.136  7240  7262 D My_Debug: >>>>>>>>>>result:org.tensorflow.demo.RecognizeCommands$RecognitionResult@32219e3
07-27 10:34:15.136  7240  7240 D My_Debug: >>>>>>>>>result.isNewCommand:true
07-27 10:34:15.136  7240  7240 D My_Debug: >>>>>>>>>result.foundCommand.startsWith:true"
21263,C:/users/******/_bazel_quickride/obpmm2rb/external/double_conversion/BUILD.bazel:7:1: C++ compilation of rule '@double_conversion//:double-conversion' failed (Exit -1),"I stuck with this problem i had installed cmake also but iam not able to build the the tensarflow demo application.

C:/users/*****/_bazel_quickride/obpmm2rb/external/double_conversion/BUILD.bazel:7:1: C++ compilation of rule '@double_conversion//:double-conversion' failed (Exit -1)"
21262,Toco gives core dump on using the Speech model for conversion to tflite,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.9
- **Python version**:2.7.12
- **Bazel version (if compiling from source)**: 0.15.1
- **GCC/Compiler version (if compiling from source)**:c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
i am trying to convert the  wavenet tensorflow model to tflite using toco with the below command.
the link for the pre-trained  model is here https://github.com/buriburisuri/speech-to-text-wavenet 


model info is here below
Tensor(""import/wav_data:0"", dtype=string)
Tensor(""import/output:0"", shape=(?, ?), dtype=int64)

used the below command to convert to toco

bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=q_wavenet_mobile.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=q_wavenet_mobile.tflite --inference_type=QUANTIZED_UINT8 --input_arrays=wav_data --output_arrays=output  --allow_custom_ops

### Source code / logs

2018-07-31 11:25:22.075935: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: DecodeWav
2018-07-31 11:25:22.076164: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: AudioSpectrogram
2018-07-31 11:25:22.076206: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Mfcc
2018-07-31 11:25:22.076270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.076371: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.076416: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.076559: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.076655: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.076698: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.076827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.076925: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.076970: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.077080: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.077176: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.077221: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.077391: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.077627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.077674: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.077838: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.078074: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.078123: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.078236: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.078340: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.078386: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.078566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.078813: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.078862: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.079054: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.079290: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.079339: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.079447: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.079552: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.079598: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.079791: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.080031: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.080081: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.080238: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.080475: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.080522: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.080628: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.080733: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.080778: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.080978: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.081225: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.081272: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.081434: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.081702: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.081751: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.081854: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.081959: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.082004: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.082195: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.082299: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.082342: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.082514: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.082621: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.082667: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.082777: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.082881: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.082926: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.083127: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.083374: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.083421: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.083584: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.083835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.083882: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.083992: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.084100: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.084152: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.084352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.084607: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.084654: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.084813: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.085060: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.085108: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.085216: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.085330: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.085374: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.085568: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.085818: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.085866: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.086028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.086281: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.086329: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.086447: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.086562: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.086606: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.086802: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.087109: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.087157: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.087352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.087615: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.087662: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.087774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.087894: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.087939: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.088103: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.088219: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.088264: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.088453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.088570: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.088664: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.088773: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.088891: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.088935: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.089102: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.089358: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.089576: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.089782: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.090062: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.090112: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.090224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.090346: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.090391: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.090571: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.090836: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.090884: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.091077: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.091996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.092046: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.092158: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.092943: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.092993: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.093199: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.093484: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.093534: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.093699: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.093976: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.094024: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.094136: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.094267: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.094310: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.094512: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.094795: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.094844: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.095003: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.095276: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.095325: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.095435: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.095564: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.095609: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.095719: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.095845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: SquaredDifference
2018-07-31 11:25:22.095889: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Reciprocal
2018-07-31 11:25:22.095987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: Dequantize
2018-07-31 11:25:22.096080: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1048] Converting unsupported operation: CTCBeamSearchDecoder
2018-07-31 11:25:22.244485: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2797 operators, 4442 arrays (0 quantized)
2018-07-31 11:25:22.379827: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2797 operators, 4442 arrays (0 quantized)
2018-07-31 11:25:22.400884: I tensorflow/contrib/lite/toco/graph_transformations/identify_dilated_conv.cc:161] Identified sub-network emulating dilated convolution.
2018-07-31 11:25:22.401095: I tensorflow/contrib/lite/toco/graph_transformations/identify_dilated_conv.cc:209] Replaced with Dilated Conv2D op outputting ""block_0_2/conv_filter/atrous_conv2d"".
terminate called after throwing an instance of 'std::out_of_range'
  what():  _Map_base::at
Aborted (core dumped)

"
21261,"Tensorflow 1.9 for CPU, without GPU still requires cudNN - Windows","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: pip 18.0 - pip install tensorflow
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: none
- **GPU model and memory**: none
- **Exact command to reproduce**: import tensorflow after pip install tensorflow

### Describe the problem
I am working on a Win10 machine, with python 3.6.3 and using tensorflow 1.9, pip 18.0. I did not provide an option to install tensorflow with gpu, (i.e.), according to this

https://www.tensorflow.org/install/install_windows, 

I used

        pip install tensorflow

and did not provide option for using GPU. However, when trying to import tensorflow, I am faced with the following error

        ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

After following various links,
 
https://stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso,
https://github.com/tensorflow/tensorflow/issues/8385,
 
I installed the **Visual studio update 3** and also used the script provided for **tensorflow self check**,  

https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c, 

and came across this following error:

        Could not load 'cudart64_80.dll'. .....
        Could not load 'nvcuda.dll' .......
        Could not load 'cudnn64_5.dll' ........

Why is my Tensorflow looking for these packages, when I installed it without GPU? MY system doesn't house a GPU at the moment. I tried uninstall and reinstalling with the **upgraded pip 18.0**, but the issue persists. How can this be rectified.?

### Source code / logs

--------------------------------------------------------------------------------------
Errors in Detail. in Jupyter notebook while trying import - 

ImportError                               Traceback (most recent call last)
c:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     13         try:
---> 14             return importlib.import_module(mname)
     15         except ImportError:

c:\python36\lib\importlib\__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 

c:\python36\lib\importlib\_bootstrap.py in _gcd_import(name, package, level)

c:\python36\lib\importlib\_bootstrap.py in _find_and_load(name, import_)

c:\python36\lib\importlib\_bootstrap.py in _find_and_load_unlocked(name, import_)

c:\python36\lib\importlib\_bootstrap.py in _load_unlocked(spec)

c:\python36\lib\importlib\_bootstrap.py in module_from_spec(spec)

c:\python36\lib\importlib\_bootstrap_external.py in create_module(self, spec)

c:\python36\lib\importlib\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)

ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

ModuleNotFoundError                       Traceback (most recent call last)
c:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

c:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     16             return importlib.import_module('_pywrap_tensorflow_internal')
---> 17     _pywrap_tensorflow_internal = swig_import_helper()
     18     del swig_import_helper

c:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     15         except ImportError:
---> 16             return importlib.import_module('_pywrap_tensorflow_internal')
     17     _pywrap_tensorflow_internal = swig_import_helper()

c:\python36\lib\importlib\__init__.py in import_module(name, package)
    125             level += 1
--> 126     return _bootstrap._gcd_import(name[level:], package, level)
    127 

ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-28ca7626b6fd> in <module>()
      1 get_ipython().run_line_magic('pylab', 'inline')
----> 2 from keras import regularizers, activations
      3 from keras.models import Sequential, load_model
      4 from keras.utils import np_utils,plot_model,to_categorical
      5 import pandas

c:\python36\lib\site-packages\keras\__init__.py in <module>()
      1 from __future__ import absolute_import
      2 
----> 3 from . import utils
      4 from . import activations
      5 from . import applications

c:\python36\lib\site-packages\keras\utils\__init__.py in <module>()
      4 from . import data_utils
      5 from . import io_utils
----> 6 from . import conv_utils
      7 
      8 # Globally-importable utils.

c:\python36\lib\site-packages\keras\utils\conv_utils.py in <module>()
      7 from six.moves import range
      8 import numpy as np
----> 9 from .. import backend as K
     10 
     11 

c:\python36\lib\site-packages\keras\backend\__init__.py in <module>()
     87 elif _BACKEND == 'tensorflow':
     88     sys.stderr.write('Using TensorFlow backend.\n')
---> 89     from .tensorflow_backend import *
     90 else:
     91     # Try and load external backend.

c:\python36\lib\site-packages\keras\backend\tensorflow_backend.py in <module>()
      3 from __future__ import print_function
      4 
----> 5 import tensorflow as tf
      6 from tensorflow.python.framework import ops as tf_ops
      7 from tensorflow.python.training import moving_averages

c:\python36\lib\site-packages\tensorflow\__init__.py in <module>()
     20 
     21 # pylint: disable=g-bad-import-order
---> 22 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     23 from . import app
     24 from . import bitwise

c:\python36\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

c:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""c:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""c:\python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""c:\python36\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

--------------------------------------------------------------------------------------
Tensorflow Self Check output

ERROR: Failed to import the TensorFlow module.

WARNING! This script is no longer maintained! 
=============================================
Since TensorFlow 1.4, the self-check has been integrated with TensorFlow itself,
and any missing DLLs will be reported when you execute the `import tensorflow`
statement. The error messages printed below refer to TensorFlow 1.3 and earlier,
and are inaccurate for later versions of TensorFlow.

- Python version is 3.6.

- TensorFlow is installed at: C:\Python36\lib\site-packages\tensorflow

- Could not load 'cudart64_80.dll'. The GPU version of TensorFlow
  requires that this DLL be installed in a directory that is named in
  your %PATH% environment variable. Download and install CUDA 8.0 from
  this URL: https://developer.nvidia.com/cuda-toolkit

- Could not load 'nvcuda.dll'. The GPU version of TensorFlow requires that
  this DLL be installed in a directory that is named in your %PATH%
  environment variable. Typically it is installed in 'C:\Windows\System32'.
  If it is not present, ensure that you have a CUDA-capable GPU with the
  correct driver installed.

- Could not load 'cudnn64_5.dll'. The GPU version of TensorFlow
  requires that this DLL be installed in a directory that is named in
  your %PATH% environment variable. Note that installing cuDNN is a
  separate step from installing CUDA, and it is often found in a
  different directory from the CUDA DLLs. You may install the
  necessary DLL by downloading cuDNN 5.1 from this URL:
  https://developer.nvidia.com/cudnn

- Could not find cuDNN.

--------------------------------------------------------------------------------------"
21260,"Using Dataset api with Estimator in MirroredStrategy, Dst tensor is not initialized","### **System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):1.9.0
- Python version: python 2.7
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source):4.8.5
- CUDA/cuDNN version: 9.0
- GPU model and memory:GeForce GTX 1080Ti * 4
- Exact command to reproduce:
### **Describe the problem**
 In order to solve the problem here:https://github.com/tensorflow/tensorflow/issues/19588, I git clone the master branch of tensorflow and compile it with bazel 0.15.0, then the  ""Non-DMA-safe string tensor error"" disappeared ,  the training went well , but when the evaluation part start, it ran into this: 
 **'Dst tensor is not initialized'**
it should be a “Not enough memory” problem , what i have tried:

1. tune the batchsize to a very small one
2. set config.gpu_options.per_process_gpu_memory_fraction to a smaller fraction like 0.6
3. set config.gpu_options.allow_growth to True
4. use a small dataset

But none of them can help .

### **Source code / logs**
![image](https://user-images.githubusercontent.com/31854634/43434228-5b6e6356-94ad-11e8-9e8d-e00a2c64bb1a.png)

![image](https://user-images.githubusercontent.com/31854634/43434328-c4f53642-94ad-11e8-8113-554a1c03c5a9.png)


LOG：
(Jet_env_3) [lijiaji@m7-model-gpu05 classification]$ python demo.py 
2018-07-30 11:49:33,994:INFO:tensorflow:Begin configure dataset.
2018-07-30 11:49:34,210:INFO:tensorflow:End configure dataset.
2018-07-30 11:49:34,210:INFO:tensorflow:Mission hyper parameters
2018-07-30 11:49:34,210:INFO:tensorflow:{'adagrad_initial_accumulator_value': 0.1, 'label_list': ['1', '0', '3', '2', '5', '4', '7', '6', '9', '8'], 'learning_rate_decay_factor': 0.94, 'learning_rate_decay_type': 'exponential', 'save_steps': 1000, 'ftrl_initial_accumulator_value': 0.1, 'ftrl_learning_rate_power': -0.5, 'num_train_samples': 48000, 'rmsprop_decay': 0.9, 'weight_decay': 4e-05, 'end_learning_rate': 0.0001, 'num_epochs': 2, 'trainable_scopes': None, 'checkpoint_path': None, 'finetune_flag': 'False', 'rmsprop_momentum': 0.9, 'model_dir': '/home/lijiaji/test_models/multi_gou_test', 'replicas_to_aggregate': 1, 'learning_rate': 0.01, 'momentum': 0.9, 'opt_epsilon': 1.0, 'optimizer': 'momentum', 'num_epochs_per_decay': 2.0, 'label_smoothing': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'batch_size': 32, 'num_classes': 10, 'ftrl_l1': 0.0, 'ftrl_l2': 0.0, 'train_image_size': 224, 'train_dir': '/home/lijiaji/project/augments/mnist', 'moving_average_decay': None, 'sync_replicas': False, 'checkpoint_exclude_scopes': None, 'ignore_missing_vars': False, 'model_name': 'inception_resnet_v2', 'adadelta_rho': 0.95}
2018-07-30 11:49:34,211:INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0dcd846ed0>, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': 912, '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f0dcd846b90>, '_session_config': gpu_options {
  allow_growth: true
}
allow_soft_placement: true
, '_global_id_in_cluster': 0, '_is_chief': True, '_protocol': None, '_save_checkpoints_steps': 1000, '_save_summary_steps': 1000, '_model_dir': '/home/lijiaji/test_models/multi_gou_test', '_master': ''}
2018-07-30 11:49:34,212:INFO:tensorflow:Starting epoch 1 / 2
2018-07-30 11:49:42,557:INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0
2018-07-30 11:49:42,559:INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0
2018-07-30 11:49:42,559:INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0
2018-07-30 11:49:42,559:INFO:tensorflow:Device is available but not used by distribute strategy: /device:GPU:0
2018-07-30 11:49:42,559:INFO:tensorflow:Device is available but not used by distribute strategy: /device:GPU:1
2018-07-30 11:49:42,559:INFO:tensorflow:Configured nccl all-reduce.
2018-07-30 11:49:42,655:INFO:tensorflow:Calling model_fn.
2018-07-30 11:50:09,603:INFO:tensorflow:Calling model_fn.
2018-07-30 11:50:22,764:INFO:tensorflow:batch_all_reduce invoked for batches size = 496 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
2018-07-30 11:50:34,055:INFO:tensorflow:Done calling model_fn.
2018-07-30 11:50:34,067:INFO:tensorflow:Done calling model_fn.
2018-07-30 11:50:39,149:INFO:tensorflow:Create CheckpointSaverHook.
2018-07-30 11:51:11,985:INFO:tensorflow:Graph was finalized.
2018-07-30 11:51:37,259:INFO:tensorflow:Running local_init_op.
2018-07-30 11:51:38,022:INFO:tensorflow:Done running local_init_op.
2018-07-30 11:52:26,489:INFO:tensorflow:Saving checkpoints for 0 into /home/lijiaji/test_models/multi_gou_test/model.ckpt.
18/07/30 11:53:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-07-30 11:53:21,956:INFO:tensorflow:loss = 11.345272, step = 0
2018-07-30 11:55:20,779:INFO:tensorflow:global_step/sec: 0.841569
2018-07-30 11:55:20,780:INFO:tensorflow:loss = 1.7503239, step = 100 (118.825 sec)
2018-07-30 11:56:24,385:INFO:tensorflow:global_step/sec: 1.5722
2018-07-30 11:56:24,385:INFO:tensorflow:loss = 1.5974646, step = 200 (63.605 sec)
2018-07-30 11:57:28,862:INFO:tensorflow:global_step/sec: 1.55093
2018-07-30 11:57:28,863:INFO:tensorflow:loss = 1.3371584, step = 300 (64.478 sec)
2018-07-30 11:58:18,151:INFO:tensorflow:Saving checkpoints for 375 into /home/lijiaji/test_models/multi_gou_test/model.ckpt.
2018-07-30 11:58:28,894:INFO:tensorflow:Loss for final step: 1.3792272.
2018-07-30 11:58:28,900:INFO:tensorflow:Begin evaluation of epoch 1
2018-07-30 11:58:28,973:INFO:tensorflow:Calling model_fn.
2018-07-30 11:58:36,515:INFO:tensorflow:Done calling model_fn.
2018-07-30 11:58:36,539:INFO:tensorflow:Starting evaluation at 2018-07-30-03:58:36
2018-07-30 11:58:41,904:INFO:tensorflow:Graph was finalized.
2018-07-30 11:58:41,907:INFO:tensorflow:Restoring parameters from /home/lijiaji/test_models/multi_gou_test/model.ckpt-375
Traceback (most recent call last):
  File ""demo.py"", line 22, in <module>
    run(config)
  File ""/mnt/disk0/home/lijiaji/test_cvalgo/cv_algo/classification/run_classification.py"", line 473, in run
    run_impl(args)   
  File ""/mnt/disk0/home/lijiaji/test_cvalgo/cv_algo/classification/run_classification.py"", line 435, in run_impl
    steps = args.save_steps * 2
  File ""/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 464, in evaluate
    output_dir=self.eval_dir(name))
  File ""/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 1521, in _evaluate_run
    config=self._session_config)
  File ""/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/evaluation.py"", line 209, in _evaluate_once
    session_creator=session_creator, hooks=hooks) as session:
  File ""/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 832, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 555, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1018, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1023, in _create_session
    return self._sess_creator.create_session()
  File ""/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 712, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 483, in create_session
    init_fn=self._scaffold.init_fn)
  File ""/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session
    config=config)
  File ""/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint
    saver.restore(sess, checkpoint_filename_with_path)
  File ""/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1727, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 886, in run
    run_metadata_ptr)
  File ""/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1109, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1281, in _do_run
    run_metadata)
  File ""/home/lijiaji/anaconda2/envs/Jet_env_3/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1300, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.
	 [[Node: save/RestoreV2/_1311 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_1315_save/RestoreV2"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
	 [[Node: save/RestoreV2/_1438 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_1443_save/RestoreV2"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](save/RestoreV2:719)]]
"
21259,tf.data.Dataset.padded_batch() doesn't work with dataset.map,"System information
Have I written custom code: yes
OS Platform and Distribution:  Ubuntu Linux 
TensorFlow installed from: conda
TensorFlow version (use command below): 1.4.1
Python version: 3.6.5
Describe the problem
It's a image classification task to read variable-length images from png files, to map them and to padd them. But Dataset.padded_batch() doesn't work with tf.dataset which uses map 
    
    # source codes
    data.load_data() # load data from disk
    usable_data = data.get_data(use_percent=use_percent, tail=tail)
    # usable_data is a tuple for features and labels
    init_hook = IteratorInitHook()

    def input_fn():
        dataset = tf.data.Dataset.from_tensor_slices(usable_data)
        dataset = dataset.shuffle(params.shuffle_buffer_size)
        dataset = dataset.map(data.instance_as_tensor) # the instance_as_tensor method transfer an image into feature and label tensors, the shape of feature is (?,?,?) and label shape is ()
        dataset = dataset.repeat(epochs)
        dataset = dataset.padded_batch(params.batch_size, padded_shapes=([None, None, None], []))

        iterator = dataset.make_initializable_iterator()
        init_hook.iterator_init = iterator.initializer
        next_example, next_label = iterator.get_next()
        return {'sgram': next_example}, next_label

Exception stack is 

Traceback (most recent call last):
  File ""main.py"", line 414, in <module>
    tf.app.run(main=train_or_predict)
  File ""/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""main.py"", line 405, in train_or_predict
    hparams=params,
  File ""/data0/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py"", line 218, in run
    return _execute_schedule(experiment, schedule)
  File ""/data0/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py"", line 46, in _execute_schedule
    return task()
  File ""/data0/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 625, in train_and_evaluate
    self.train(delay_secs=0)
  File ""/data0/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 367, in train
    hooks=self._train_monitors + extra_hooks)
  File ""/data0/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 807, in _call_train
    hooks=hooks)
  File ""/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 708, in _train_model
    input_fn, model_fn_lib.ModeKeys.TRAIN)
  File ""/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 577, in _get_features_and_labels_from_input_fn
    result = self._call_input_fn(input_fn, mode)
  File ""/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 663, in _call_input_fn
    return input_fn(**kwargs)
  File ""main.py"", line 218, in input_fn
    padded_shapes=([None, None, None],[])
  File ""/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 695, in padded_batch
    return PaddedBatchDataset(self, batch_size, padded_shapes, padding_values)
  File ""/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1292, in __init__
    input_dataset.output_shapes, _partial_shape_to_tensor, padded_shapes)
  File ""/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py"", line 512, in map_structure_up_to
    assert_shallow_structure(shallow_tree, input_tree)
  File ""/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py"", line 372, in assert_shallow_structure
    check_types=check_types)
  File ""/data0/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py"", line 356, in assert_shallow_structure
    ""Input has type: %s."" % type(input_tree))
TypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: <class 'list'>."
21257,Request for Multi Graph composition,"```

In [29]: with tf.Session() as sess:
   ....:     x = tf.placeholder(tf.int32, [None])
   ....:     a = x + 1
   ....:     y = tf.placeholder(tf.int32, [None])
   ....:     b = y + 1
   ....:     print(sess.run(b, feed_dict={y:a, x:[1]}))
   ....:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-29-d72479c69760> in <module>()
      4     y = tf.placeholder(tf.int32, [None])
      5     b = y + 1
----> 6     print(sess.run(b, feed_dict={y:a, x:[1]}))
      7

/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    887     try:
    888       result = self._run(None, fetches, feed_dict, options_ptr,
--> 889                          run_metadata_ptr)
    890       if run_metadata:
    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1068
   1069           if isinstance(subfeed_val, ops.Tensor):
-> 1070             raise TypeError('The value of a feed cannot be a tf.Tensor object. '
   1071                             'Acceptable feed values include Python scalars, '
   1072                             'strings, lists, numpy ndarrays, or TensorHandles.')

TypeError: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles.

```
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

➜  ~ python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.4.0-19-ga52c8d9b01', '1.4.1')

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21255,Building TF with custom op support,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.8
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:  0.14.1
- **GCC/Compiler version (if compiling from source)**:  5.4.0 20160609
- **CUDA/cuDNN version**: 9/7.1
- **GPU model and memory**: 1080ti, 11gb, dual
- **Exact command to reproduce**:

1. Copied my custom op definitions in tensorflow/user_ops
2. Modified BUILD script as follows:
` tf_custom_op_library(
    name = ""libsbnet.so"",
    srcs = [""reduce_mask.cu,""reduce_mask.cu.h"",""zero_block_counters.cu.h"",""sparse_gather.cu"", ""sparse_blocks.cu.h"",""sparse_gather.cc"",""reduce_mask.cc""],
)`
3. `bazel build --config=opt --config=cuda --config=monolithic //tensorflow:libtensorflow_cc.so`

When running the graph with the new .so file in Tensorflow C++, i still get an error that the custom op is not found. Is there some documentation that i can follow to see if i am missing anything?

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21254,"InvalidArgumentError: assertion failed: [predictions must be in [0, 1]] [Condition x >= y did not hold element-wise:x (linear/head/predictions/logistic:0) = ]","### Describe the problem
I am using TF 1.9. My model trains successfully but when I try to evaluate I get the error below. I was even thinking my create_test_input_fn() could be the problem so I tried to evaluate on the training data set but I got the same error.

### Source code / logs

def create_train_input_fn(): 
    return tf.estimator.inputs.pandas_input_fn(
        x=X_train.drop('indiv_key',axis=1),
        y=y_train, 
        batch_size=32,
        num_epochs=None, # Repeat forever
        shuffle=True)

def create_test_input_fn():
    return tf.estimator.inputs.pandas_input_fn(
        x=X_test.drop('indiv_key',axis=1),
        y=y_test, 
        num_epochs=1, # Just one epoch
        shuffle=False) # Don't shuffle so we can compare to census_test_labels later


outdir = 'graphs/linear'
shutil.rmtree(outdir, ignore_errors = True) # start fresh each time
myopt = tf.train.AdamOptimizer(1e-4)

train_input_fn = create_train_input_fn()
estimator = tf.estimator.LinearClassifier(feature_columns ,
            optimizer=myopt, 
            model_dir=outdir, 
            n_classes=2)
estimator.train(train_input_fn, steps=5000)


test_input_fn = create_test_input_fn()
estimator.evaluate(test_input_fn)

=============
ERROR
=============
InvalidArgumentError: assertion failed: [predictions must be in [0, 1]] [Condition x >= y did not hold element-wise:x (linear/head/predictions/logistic:0) = ] [[0.0246640872][0.0240472779][0.0353220813]...] [y (linear/metrics/auc_precision_recall/Cast/x:0) = ] [0]
	 [[Node: linear/metrics/auc_precision_recall/assert_greater_equal/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_FLOAT, DT_STRING, DT_FLOAT], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](linear/metrics/auc_precision_recall/assert_greater_equal/Assert/AssertGuard/Assert/Switch, linear/metrics/auc/assert_greater_equal/Assert/AssertGuard/Assert/data_0, linear/metrics/auc/assert_greater_equal/Assert/AssertGuard/Assert/data_1, linear/metrics/auc/assert_greater_equal/Assert/AssertGuard/Assert/Switch_1, linear/metrics/auc_precision_recall/assert_greater_equal/Assert/AssertGuard/Assert/data_3, linear/metrics/auc_precision_recall/assert_greater_equal/Assert/AssertGuard/Assert/Switch_2)]]

During handling of the above exception, another exception occurred:
"
21252,tensorflow/contrib/lite/Makefile doesn't build,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian Linux rodete
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.9.0
- **Python version**: 3.5.3
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: 6.4.0
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: Quadro M2000 4GB
- **Exact command to reproduce**:

```
git clone https://github.com/tensorflow/tensorflow
cd tensorflow
git checkout v1.9.0
./tensorflow/contrib/lite/download_dependencies.sh
make -j $(nproc) -f tensorflow/contrib/lite/Makefile
```


### Describe the problem

The build fails with:

```
./tensorflow/contrib/lite/schema/schema_generated.h: In member function ‘bool tflite::QuantizationParameters::Verify(flatbuffers::Verifier&) const’:
./tensorflow/contrib/lite/schema/schema_generated.h:1480:33: error: no matching function for call to ‘flatbuffers::Verifier::Verify(const flatbuffers::Vector<float>*)’
```

The problem appears to be that `./tensorflow/contrib/lite/download_dependencies.sh` downloads the latest version of flatbuffers instead of a fixed version and [this](https://github.com/google/flatbuffers/commit/79f2adc50a23949d14b0da7d12a5d1451748e82c) commit to the flatbuffers repo renamed the `flatbuffers::Verifier::Verify` vector<T> overload to `flatbuffers::Verifier::VerifyVector`.

Here's the full build error: [compile_error.txt](https://github.com/tensorflow/tensorflow/files/2242668/compile_error.txt)

All dependencies downloaded by `./tensorflow/contrib/lite/download_dependencies.sh` should use explicit versions."
21251,TF_LoadLibrary Usage in C++,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/a
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**:9/7.1
- **GPU model and memory**:1080 ti, 11gb dual
- **Exact command to reproduce**:
1. Create a .so file with custom ops
2. Freeze the graph(with custom ops) by calling tf.load_op_library at top of the freezing graph function
3. Load graph in C++ `    status = session_->Create(graph_def);`
4. Load custom ops by importing .so file `    TF_Status* status_2 = TF_NewStatus();
    TF_Library* lib =TF_LoadLibrary(""libcustomOp.so"", status_2);
    TF_Buffer op_list_buf = TF_GetOpList(lib);`

Question: How do I attach these custom ops with the graph, as I still get an error that the custom op is not in the graph?


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21250,tf.Print support of tf.complex types,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a (cpu execution)
- **GPU model and memory**: n/a (cpu execution)
- **Exact command to reproduce**: cf test case below

### Problem description
When using the `tf.Print` operation on complex data, the printed output only shows question marks (`?`) in place of the complex values. The documentation does not indicate a limitation on the type of tensors of the data argument. It would be good to add that limitation in the documentation or even better, support tf.complex types. In the meanwhile, one can work around and use tf.real and tf.imag. The test case below shows the problem and the work around.
Note that the problem was already present with Tensorflow 1.7.

### Source code / logs
Sources:
```
import numpy as np
import tensorflow as tf


a = np.array([[1, 2], [3, 4]])

input1 = tf.placeholder(tf.complex64, shape=[2,2], name=""input1"")
input2 = tf.placeholder(tf.float32, shape=[2, 2], name=""input2"")

input1 = tf.Print(input1, [input1], ""input1: "")
input1 = tf.Print(input1, [tf.real(input1)], ""input1.real: "")
input2 = tf.Print(input2, [input2], ""input2: "")

output1 = tf.identity(input1, name=""output1"")
output2 = tf.identity(input2, name=""output2"")

sess = tf.InteractiveSession()

out1, out2 = sess.run([""output1:0"", ""output2:0""], {""input1:0"": a, ""input2:0"": a})
```
Console output:
```
input2: [[1 2][3]...]input1: [? ? ?...]

input1.real: [[1 2][3]...]
```
By the way, the formatting of the printed messages (line returns) looks weird. This appeared when going from TF1.7 to 1.9, but this is not the point of that issue."
21248,[Bug] TensorRT conversion error with conv2d_transpose,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.9.0rc0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.14.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0.176/7.0.5
- **GPU model and memory**: GTX 1080 with 8GiB memory
- **Exact command to reproduce**: python ./minimal_graph.py

### Describe the problem
When I use Tensorflow's TensorRT integration from tensorflow.contrib to transform my model into tensorRT compatible graph, I notice that if I have conv2d_transpose layer (which is not supported for conversion yet) following some convolution layer (or other layer that can be optimized), the latter would be transformed into a tensorRT operation during graph optimization and eventually cause Cudnn to Segfault. 

Below is a minimal example I figured out to trigger this bug, which describe the process to transform a simple randomly initialized network into tensorRT inference graph, and then execute it with some dummy input.

The Tensorflow version I use comes from dockerhub's nightly build (tensorflow/tensorflow:nightly-devel-gpu) with build configuration changed to enable tensorRT. I also notice that previous tensorflow version (like 1.8.0) has the same issue. On 1.9.0 there's a bug that prevents tensorRT to run correctly so I use 1.9.0rc0 instead.

The tensorRT version I use is 3.0.4. I notice another issue from [this link](https://github.com/tensorflow/tensorflow/issues/20157) claiming conv2d_transpose is not supported, but here the problem is that the model won't correctly executed instead of not transformed.


### Source code / logs

#### Code
minimal_graph.py
```
import numpy as np
import tensorflow as tf
from tensorflow.contrib import tensorrt as trt


def build_graph_from_def(graph_def, input_nodes, output_nodes):
    """"""
    build the actual graph from definition
    """"""
    tf.reset_default_graph()
    graph = tf.Graph()
    with graph.as_default():
        return_tensors = [operation_name + "":0"" for operation_name in input_nodes + output_nodes]
        tensors = tf.import_graph_def(graph_def=graph_def, name="""",
                                      return_elements=return_tensors)
        input_tensor_list = tensors[:len(input_nodes)]
        output_tensor_list = tensors[len(input_nodes):]

    return graph, input_tensor_list, output_tensor_list


def main():
    with tf.variable_scope(""Net""):
        inp = tf.placeholder(tf.float32, shape=(1, 28, 28, 3), name=""input_image"")
        deconv1 = tf.layers.conv2d_transpose(inp, filters=8, kernel_size=(3, 3), strides=(2, 2))
        output = tf.layers.conv2d(deconv1, filters=8, kernel_size=(3, 3), name=""output"")
    input_nodes = [""Net/input_image""]
    output_nodes = [""Net/output/BiasAdd""]
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        const_graph_def = tf.graph_util.convert_variables_to_constants(
            sess, sess.graph.as_graph_def(), output_nodes)

    optimized_graph_def = trt.create_inference_graph(
        input_graph_def=const_graph_def,
        outputs=output_nodes,
        max_batch_size=1,
        max_workspace_size_bytes=1 << 25)
    graph, input_tensors, output_tensors = build_graph_from_def(
        optimized_graph_def, input_nodes, output_nodes)
    with tf.Session(graph=graph) as sess:
        sess.run(output_tensors[0], feed_dict={input_tensors[0]: np.zeros((1, 28, 28, 3))})

if __name__ == ""__main__"":
    main()
```
#### Log
```$ python ./minimal_graph.py
2018-07-30 10:30:11.355872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-07-30 10:30:11.356407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1404] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 7.92GiB freeMemory: 6.07GiB
2018-07-30 10:30:11.356418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Adding visible gpu devices: 0
2018-07-30 10:30:11.604399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:964] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-30 10:30:11.604422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:970]      0 
2018-07-30 10:30:11.604427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0:   N 
2018-07-30 10:30:11.604573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5853 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-07-30 10:30:11.604697: E tensorflow/core/common_runtime/gpu/gpu_device.cc:228] Illegal GPUOptions.experimental.num_dev_to_dev_copy_streams=0 set to 1 instead.
2018-07-30 10:30:11.669166: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1
2018-07-30 10:30:11.671202: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope 'Net/', converted to graph
2018-07-30 10:30:11.674008: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0
2018-07-30 10:30:12.200384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Adding visible gpu devices: 0
2018-07-30 10:30:12.200405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:964] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-30 10:30:12.200411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:970]      0 
2018-07-30 10:30:12.200414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0:   N 
2018-07-30 10:30:12.200490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5853 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-07-30 10:30:12.212534: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger Parameter check failed at: cudnnEngine.cpp::enqueue::140, condition: bindings[x] != nullptr
2018-07-30 10:30:12.212549: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:401] Failed to enqueue batch for TRT engine: Net/my_trt_op_0
[1]    18050 segmentation fault (core dumped)
```
"
21247,TensorFlow Lite - Sample Android app returns Invalid handle to Interpreter on AOSP,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
Android.mk added for AOSP build
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
Generic Android 8 device
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**:1.7.1
- **Python version**:2.7
- **Bazel version (if compiling from source)**:0.14.1
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
TF-Lite sample Android app returns error when it is built as a part of AOSP system app. 
I have run Android sample TF-Lite app and it works with no error, but if I try to build the same sample app (SpeechActivity part) in AOSP using Android.mk, I've got the ""java.lang.IllegalArgumentException: Invalid handle to Interpreter"".

I have added same downloaded conv_actions_frozen.tflite and conv_action_labels.txt to ""/assets"" directory and specify it in Android.mk as follows.

LOCAL_ASSET_FILES += $(call find-subdir-assets)

### Source code / logs
java.lang.RuntimeException: Unable to start activity ComponentInfo{com.mycompany.mmiservice/com.mycompany.mmiservice.SpeechActivity}: java.lang.IllegalArgumentException: Invalid handle to Interpreter.
    at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2820)
    at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2895)
    at android.app.ActivityThread.-wrap11(Unknown Source:0)
    at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1596)
    at android.os.Handler.dispatchMessage(Handler.java:105)
    at android.os.Looper.loop(Looper.java:164)
    at android.app.ActivityThread.main(ActivityThread.java:6565)
    at java.lang.reflect.Method.invoke(Native Method)
    at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)
    at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)
 Caused by: java.lang.IllegalArgumentException: Invalid handle to Interpreter.
    at org.tensorflow.lite.NativeInterpreterWrapper.resizeInput(Native Method)
    at org.tensorflow.lite.NativeInterpreterWrapper.resizeInput(NativeInterpreterWrapper.java:155)
    at org.tensorflow.lite.Interpreter.resizeInput(Interpreter.java:191)
    at com.mycompany.mmiservice.SpeechActivity.onCreate(DummyActivity.java:143)"
21245,TFLite Android: Model file will not load. startOffset and declaredLength problems,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Pixel 2XL, Android studio emulator
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: Loading TFLite model in an Android application

Posted as an issue here based on comment from https://stackoverflow.com/questions/51341554/tflite-android-model-file-will-not-load-startoffset-and-declaredlength-problem?noredirect=1#comment90097492_51341554

I'm having issues with loading a TFLite model using the MappedByteBuffer method from the Tensorflow-for-poets-2 TFLite tutorial.

```
private MappedByteBuffer loadModelFile(Activity activity,String MODEL_FILE) throws IOException {
    AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODEL_FILE);
    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());
    FileChannel fileChannel = inputStream.getChannel();
    long startOffset = fileDescriptor.getStartOffset();
    long declaredLength = fileDescriptor.getDeclaredLength();
    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
```

In particular the model I have converted with the tflite_convert (formerly toco) tools fails when the fileChannel.map is returned. The model is a floating point TFLite mode.

The problem seems to be caused by the startOffset and declaredLength variables. The error I receive in the logcat is
```
7525-7525/dp.thexor A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 7525 (dp.thexor), pid 7525 (dp.thexor)
If I fix these values to ones from a model that successfully loads then the method successfully returns the fileChannel.map. The values from my model are startOffset = 2846788 declaredLength = 45525464
```

I know that my TFlite model can successfully be mapped to memory as the tensorflow/contrib/lite/tools/benchmark:benchmark_model is able to benchmark it.

I'd try to load a quantized TFLite model but currently my model contains ops that do not have quantized equivalents (transpose_conv).

What could be causing this?

My .tflite model can be found here: https://github.com/andrewginns/CycleGAN-Tensorflow-PyTorch/releases/download/tf1.7-py3.6.4/float.tflite
"
21243,aggregation of sparse gradient and dense gradient is unexpected,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX 10.13.5
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: pip binary
- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: see below

### Describe the problem


### Source code / logs

Example code:

```
    var = tf.get_variable(""var"", (3, 5), initializer=tf.ones_initializer())
    session.run(var.initializer)
    loss = tf.reduce_sum(tf.nn.embedding_lookup(var, [1]) ** 2) + tf.reduce_sum(var ** 2)
    grad, = tf.gradients(loss, var)
    print(""grad:"", grad)  # It is an IndexedSlices.
    grad_np = session.run(grad)
    print(""grad value:"")
    print(grad_np)
```

Output:
```
...
grad value:
IndexedSlicesValue(values=array([[2., 2., 2., 2., 2.],
       [2., 2., 2., 2., 2.],
       [2., 2., 2., 2., 2.],
       [2., 2., 2., 2., 2.]], dtype=float32), indices=array([0, 1, 2, 1], dtype=int32), dense_shape=array([3, 5], dtype=int32))
```

The logic in TF is implemented in `_AggregatedGrads` and `_AggregateIndexedSlicesGradients`.
As there is one dense gradient, I would have expected also a final dense gradient.
I think that the dense var updates of the optimizers (e.g. `ApplyAdam`) also perform more efficient compared to the corresponding sparse updates (implemented in `_apply_sparse`).

Regarding how to accumulate the dense and sparse gradients: Probably it can stay like it is, via first accumulating them in `IndexedSlices`, and an additional flag whether there was a dense gradient, and if so, then do a final dense conversion, via `tf.convert_to_tensor`.
"
21241,Feature request: matmul operation for int64 datatype,"For an open source project on [sMPC](https://en.wikipedia.org/wiki/Secure_multi-party_computation) in Tensorflow I am using the int64 datatype [to represent decimal numbers](https://www1.cs.fau.de/filepool/publications/octavian_securescm/secfp-fc10.pdf). However, the matmul operations is currently not supported for this datatype. Is there any chance this could be enabled in the future? Currently we are using the [Chinese remainder theorem](https://en.wikipedia.org/wiki/Chinese_remainder_theorem) to represent the numbers, but as a result all operations are significantly slower.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A


"
21239,GPU doesn't work when I request PoseNet API (@tensorflow-models/posenet) with node.js and express,"### Problem
Excuse me.
I implemented demo app with tensorflow-models/posenet, node.js and express.
I replaced the sample code as follows, but GPU may not work when executing estimateMultiplePoses().
Could you tell me how to run my code on GPU?

For example, I got a very low response when I post my API to a localhost server.

```
$curl -X POST -F thumbnail=@./picture.png localhost:3000/upload
```

[At server log] 
I want to get the response less than 100 ms.
I got the response less than 30 ms when I didn't use PoseNet API, so thought that this low response was caused by not running on GPU.

```
POST /upload 200 2110.888 ms - 9372
```

However,  the process may be alive when building the server.

```
$ nvidia-smi
Tue Jul 31 10:27:38 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.37                 Driver Version: 396.37                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 107...  On   | 00000000:01:00.0  On |                  N/A |
| 27%   40C    P8    13W / 180W |   7732MiB /  8116MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 107...  On   | 00000000:02:00.0 Off |                  N/A |
| 27%   38C    P8    12W / 180W |   7723MiB /  8119MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1252      G   /usr/lib/xorg/Xorg                           281MiB |
|    0      2068      G   compiz                                       226MiB |
|    0      2435      G   ...-token=...                                 61MiB |
|    0      2697      G   unity-control-center                          15MiB |
|    0      2945      G   ...-token=...                                 92MiB |
|    0     14062      G   ...-token=...                                 66MiB |
|    0     26892      C   node                                        6983MiB |
|    1     26892      C   node                                        7711MiB |
+-----------------------------------------------------------------------------+

```

regards.

----
### Custom Code

./routes/index.js

``` 

var posenet = require('@tensorflow-models/posenet');
var tf = require('@tensorflow/tfjs');
require('@tensorflow/tfjs-node-gpu');

var express = require('express');
var router = express.Router();
var multer = require('multer');
var upload = multer({dest: ""./uploads/""}).single('thumbnail');
var Canvas = require('canvas');
var Image = Canvas.Image;

const state = {
  input: {
    imageScaleFactor: 0.5,
    flipHorizontal: false,
    outputStride: 16,
  },
  detection: {
    maxPoseDetections: 5,
    minPoseConfidence: 0.15,
    minPartConfidence: 0.1,
    nmsRadius: 30.0,
  },
};

function createHTMLCanvasElementFromRequest(req){
    const img = new Image;
    img.src = req.file.path;
    const canvas = new Canvas(img.width,img.height); 
    const ctx = canvas.getContext('2d');
    ctx.drawImage(img, 0, 0, img.width, img.height);
    return canvas;
}

function detectPoses(req, res){
  const canvas = createHTMLCanvasElementFromRequest(req);
  async function usePoseNet(req){
    const net = await posenet.load();
    const poses = await net.estimateMultiplePoses(  canvas, 
                                                  state.input.imageScaleFactor, 
                                                  state.input.flipHorizontal, 
                                                  state.input.outputStride, 
                                                  state.detection.maxPoseDetections,
                                                  state.detection.minPartConfidence, 
                                                  state.detection.nmsRadius);
    return poses;
  }
  usePoseNet(req).then( json_res =>{
    res.send({
        ""OriginalName"": req.file.originalname,
        ""FileName"": req.file.filename,
        ""Size"": req.file.size,
        ""Width"": canvas.width,
        ""Height"": canvas.height,
        ""Context"": json_res
    })
  }).catch(e => {
    res.send({
        ""Error"": e
    });
  });
}

function failedToGetImages(req,res, err){
  res.send({
        ""Destination"": req.file.destination,
        ""Error"": err
      });
}

router.post('/upload', function(req, res){
  upload(req, res, function(err){
    res.header('Content-Type', 'application/json; charset=utf-8');
    err? failedToGetImages(req,res,err) : detectPoses(req, res);
  });
});

module.exports = router;

```

----

./app.js

```

var createError = require('http-errors');
var express = require('express');
var path = require('path');
var cookieParser = require('cookie-parser');
var logger = require('morgan');

var indexRouter = require('./routes/index');

var app = express();

app.set('views', path.join(__dirname, 'views'));
app.set('view engine', 'pug');

app.use(logger('dev'));
app.use(express.json());
app.use(cookieParser());
app.use(express.urlencoded({ extended: false }));
app.use(express.static(path.join(__dirname, 'public')));

app.use('/', indexRouter);

app.use(function (req, res, next) {
  res.header('Access-Control-Allow-Origin', req.headers.origin);
  res.header('Access-Control-Allow-Headers', 'X-Requested-With, X-HTTP-Method-Override, Content-Type, Accept');
  res.header('Access-Control-Allow-Methods', 'POST, GET, PUT, DELETE, OPTIONS');
  res.header('Access-Control-Allow-Credentials', true);
  res.header('Access-Control-Max-Age', '86400');
  next();
});

app.options('*', function (req, res) {
  res.sendStatus(200);
});


// catch 404 and forward to error handler
app.use(function(req, res, next) {
  next(createError(404));
});

// error handler
app.use(function(err, req, res, next) {
  // set locals, only providing error in development
  res.locals.message = err.message;
  res.locals.error = req.app.get('env') === 'development' ? err : {};

  // render the error page
  res.status(err.status || 500);
  res.render('error');
});


module.exports = app;

```

---

./bin/www

```

#!/usr/bin/env node

var app = require('../app');
var debug = require('debug')('roboserver:server');
var http = require('http');

var port = normalizePort(process.env.PORT || '3000');
app.set('port', port);

var server = http.createServer(app);

server.listen(port);
server.on('error', onError);
server.on('listening', onListening);

function normalizePort(val) {
  var port = parseInt(val, 10);

  if (isNaN(port)) {
    // named pipe
    return val;
  }

  if (port >= 0) {
    // port number
    return port;
  }

  return false;
}

function onError(error) {
  if (error.syscall !== 'listen') {
    throw error;
  }

  var bind = typeof port === 'string'
    ? 'Pipe ' + port
    : 'Port ' + port;

  // handle specific listen errors with friendly messages
  switch (error.code) {
    case 'EACCES':
      console.error(bind + ' requires elevated privileges');
      process.exit(1);
      break;
    case 'EADDRINUSE':
      console.error(bind + ' is already in use');
      process.exit(1);
      break;
    default:
      throw error;
  }
}

function onListening() {
  var addr = server.address();
  var bind = typeof addr === 'string'
    ? 'pipe ' + addr
    : 'port ' + addr.port;
  debug('Listening on ' + bind);
}

```

-----

### System information
- OS Platform and Distribution:  ubuntu 16.04.4 LTS
- TensorFlow installed from: https://github.com/tensorflow/tfjs
- TensorFlow version: 0.12.0
- Bazel version: N/A
- CUDA/ cuDNN: 8.0/6.0
- GPU model and memory: GeForce GTX 1070 Ti 8GB * 2
- Memory:
SMBIOS 3.0.0 present.
Handle 0x0043, DMI type 16, 23 bytes
Physical Memory Array
	Location: System Board Or Motherboard
	Use: System Memory
	Error Correction Type: None
	Maximum Capacity: 32 GB
	Error Information Handle: Not Provided
	Number Of Devices: 2

Handle 0x0044, DMI type 17, 40 bytes
Memory Device
	Array Handle: 0x0043
	Error Information Handle: Not Provided
	Total Width: Unknown
	Data Width: Unknown
	Size: No Module Installed
	Form Factor: Unknown
	Set: None
	Locator: ChannelA-DIMM1
	Bank Locator: BANK 0
	Type: Unknown
	Type Detail: None
	Speed: Unknown
	Manufacturer: Not Specified
	Serial Number: Not Specified
	Asset Tag: Not Specified
	Part Number: Not Specified
	Rank: Unknown
	Configured Clock Speed: Unknown
	Minimum Voltage: Unknown
	Maximum Voltage: Unknown
	Configured Voltage: Unknown

Handle 0x0045, DMI type 17, 40 bytes
Memory Device
	Array Handle: 0x0043
	Error Information Handle: Not Provided
	Total Width: 64 bits
	Data Width: 64 bits
	Size: 16384 MB
	Form Factor: DIMM
	Set: None
	Locator: ChannelA-DIMM2
	Bank Locator: BANK 1
	Type: DDR4
	Type Detail: Synchronous Unbuffered (Unregistered)
	Speed: 2133 MHz
	Manufacturer: Corsair
	Serial Number: 00000000
	Asset Tag: 9876543210
	Part Number: CMK32GX4M2B3000C15  
	Rank: 2
	Configured Clock Speed: 2133 MHz
	Minimum Voltage: Unknown
	Maximum Voltage: Unknown
	Configured Voltage: 1.2 V

Handle 0x0046, DMI type 17, 40 bytes
Memory Device
	Array Handle: 0x0043
	Error Information Handle: Not Provided
	Total Width: Unknown
	Data Width: Unknown
	Size: No Module Installed
	Form Factor: Unknown
	Set: None
	Locator: ChannelB-DIMM1
	Bank Locator: BANK 2
	Type: Unknown
	Type Detail: None
	Speed: Unknown
	Manufacturer: Not Specified
	Serial Number: Not Specified
	Asset Tag: Not Specified
	Part Number: Not Specified
	Rank: Unknown
	Configured Clock Speed: Unknown
	Minimum Voltage: Unknown
	Maximum Voltage: Unknown
	Configured Voltage: Unknown

Handle 0x0047, DMI type 17, 40 bytes
Memory Device
	Array Handle: 0x0043
	Error Information Handle: Not Provided
	Total Width: 64 bits
	Data Width: 64 bits
	Size: 16384 MB
	Form Factor: DIMM
	Set: None
	Locator: ChannelB-DIMM2
	Bank Locator: BANK 3
	Type: DDR4
	Type Detail: Synchronous Unbuffered (Unregistered)
	Speed: 2133 MHz
	Manufacturer: Corsair
	Serial Number: 00000000
	Asset Tag: 9876543210
	Part Number: CMK32GX4M2B3000C15  
	Rank: 2
	Configured Clock Speed: 2133 MHz
	Minimum Voltage: Unknown
	Maximum Voltage: Unknown
	Configured Voltage: 1.2 V

- Exact command to reproduce: node ./bin/www
- Mobile device: N/A
- Browser: Google Chrome 
- npm: 6.1.0
- node: 10.6.0
- @tensorflow-models/posenet: 0.2.2
- @tensorflow/tfjs-node: 0.1.9
- @tensorflow/tfjs-node-gpu: 0.1.9
- gcc: 5.4.0"
21238,DecodeBase64 is not supported for Android TensorFlowInferenceInterface,"### System information

== cat /etc/issue ===============================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.1.0 (clang-902.0.39.1)
Target: x86_64-apple-darwin17.5.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy                              1.14.2
protobuf                           3.5.2.post1
tensorflow                         1.8.0
tensorflow-hub                     0.1.0
tensorflow-model-analysis          0.6.0
tensorflow-serving-api             1.8.0
tensorflow-tensorboard             1.5.0
tensorflow-transform               0.6.0
tensorflowjs                       0.1.0
tensorflowonspark                  1.0.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================


Have I written custom code:
No

OS Platform and Distribution:
MacOS

TensorFlow installed from:
`pip install tensorflow`

TensorFlow version:
1.8.0

Bazel version:
0.5.4

CUDA/cuDNN version
N/A

GPU model and memory:
N/A

Exact command to reproduce:
Follow the official and deploy the model with DecodeBase64 operator.

Mobile device:
Xiaomi MIX 2

### Describe the problem

We have the problem to deploy the image model to Android platform. It throws `No OpKernel was registered to support Op 'DecodeBase64' with these attrs`. And we have tried feeding string or byte array like these.

```
image_string = ""foo""

byte[] image_bytes = image_string.getBytes();

String inputName = ""model_input_b64_images"";
 
inferenceInterface.feedString(inputName, image_bytes);

inferenceInterface.feed(inputName, image_bytes, 1, 4860);
```

Is it possible to implement and registry the kernel for `DecodeBase64` which is really important for image models? Or we have another way to feed the data for this Operator.

### Source code / logs

It is easy to re-produce the issue. Try exporting the model which has the `DecodeBase64` operator and use the `freeze_graph.py` to generated the Android model file.

Here is the complete log when trying to run the inference in Android.

```

07-30 16:58:35.943 14616-14616/com.tobe.androidclient E/AndroidRuntime: FATAL EXCEPTION: main
Process: com.tobe.androidclient, PID: 14616
java.lang.RuntimeException: Unable to start activity ComponentInfo{com.tobe.androidclient/com.tobe.androidclient.MainActivity}: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'DecodeBase64' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[Node: DecodeBase64 = DecodeBase64[](model_input_b64_images)]]
    at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2855)
    at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2930)
    at android.app.ActivityThread.-wrap11(Unknown Source:0)
    at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1619)
    at android.os.Handler.dispatchMessage(Handler.java:105)
    at android.os.Looper.loop(Looper.java:171)
    at android.app.ActivityThread.main(ActivityThread.java:6684)
    at java.lang.reflect.Method.invoke(Native Method)
    at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:246)
    at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:783)
 Caused by: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'DecodeBase64' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[Node: DecodeBase64 = DecodeBase64[](model_input_b64_images)]]
    at org.tensorflow.Session.run(Native Method)
    at org.tensorflow.Session.access$100(Session.java:48)
    at org.tensorflow.Session$Runner.runHelper(Session.java:298)
    at org.tensorflow.Session$Runner.runAndFetchMetadata(Session.java:260)
    at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:220)
    at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)
    at com.tobe.androidclient.MainActivity.onCreate(MainActivity.java:98)
    at android.app.Activity.performCreate(Activity.java:7057)
    at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1214)
    at android.app.ActivityThread.performLaun
```
"
21237,how to get reg_loss  in eager mode,"https://github.com/tensorflow/tensorflow/blob/9e0b05bbc4bb88d1b34fb2147429dc4ad7bd25cd/tensorflow/contrib/eager/python/examples/densenet/densenet_test.py#L105

In this demo, the Conv2D have regluarizer configure but when it computed loss, there is only softmax_loss!!
where reg_loss?????"
21236,fashion-mnist url is error.,"TensorFlow website error url:https://www.tensorflow.org/tutorials/keras/basic_classification
it should downloads the fashion-mnist,but it really downloads the origin-mnist which is digital.
please modify this error.
I download the right datasets from GitHub:https://github.com/zalandoresearch/fashion-mnist"
21234,Default model signatures of Estimator can not be used by TensorFlow Serving RESTful APIs,"
### System information

== cat /etc/issue ===============================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.1.0 (clang-902.0.39.1)
Target: x86_64-apple-darwin17.5.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy                              1.14.2
protobuf                           3.5.2.post1
tensorflow                         1.8.0
tensorflow-hub                     0.1.0
tensorflow-model-analysis          0.6.0
tensorflow-serving-api             1.8.0
tensorflow-tensorboard             1.5.0
tensorflow-transform               0.6.0
tensorflowjs                       0.1.0
tensorflowonspark                  1.0.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 106: nvidia-smi: command not found

== cuda libs  ===================================================

== cat /etc/issue ===============================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.1.0 (clang-902.0.39.1)
Target: x86_64-apple-darwin17.5.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin mbp-2.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy                              1.14.2
protobuf                           3.5.2.post1
tensorflow                         1.8.0
tensorflow-hub                     0.1.0
tensorflow-model-analysis          0.6.0
tensorflow-serving-api             1.8.0
tensorflow-tensorboard             1.5.0
tensorflow-transform               0.6.0
tensorflowjs                       0.1.0
tensorflowonspark                  1.0.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 106: nvidia-smi: command not found

== cuda libs  ===================================================

### Describe the problem

Now we can use `TensorFlow Estimator` to train and export the `SavedModel` which can be loaded by `TensorFlow Serving`. However, the default model signature of Estimators only accept an byte array of input Tensor. These byte arrays can be encoded in `UTF-8` or other formats.

Therefore, if we use `TensorFlow Serving` to serve Estimator models, the gRPC APIs works but the RESTful APIs doesn't. We have fixed that by modifying the model signature of the graph to accept base64 string which can be encoded in `UTF-8` and send to the RESTful servers.

### Source code / logs

It is easy to re-produce the issue. We can export the SavedModel by this [estimator example](https://github.com/tobegit3hub/tensorflow_examples/blob/master/estimator_example/export_estimator_savedmodel.py). Then start the TensorFlow Serving and try to send the RESTful requests.

We have this [script](https://github.com/tobegit3hub/simple_tensorflow_serving/blob/master/tools/tensorflow_estimator_tool/generate_estimator_string.py) to generated the byte array of requested Tensor.

"
21233,report the tf.boolean_mask runtime problem,"System information

    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.5 LTS (Xenial Xerus)
    Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
    TensorFlow installed from (source or binary): pip (binary)
    TensorFlow version (use command below): (tf.GIT_VERSION, tf.VERSION) == ('v1.9.0-0-g25c197e023', '1.9.0')
    Python version: 2.7.12
    Bazel version (if compiling from source): N/A
    GCC/Compiler version (if compiling from source): N/A
    CUDA/cuDNN version: 9.0/6.1
    GPU model and memory: NVIDIA GeForce GTX 1080Ti 11177MiB
    Exact command to reproduce: tf.boolean_mask

Describe the problem
    I found the tf.boolean_mask() espect the same dimension of input, but when the shape of data passed to the function is None, the graph will be built successfully. But when shape of input data is different, it runs successfully. I think it may cause some problems.

Code.
```import tensorflow as tf
import numpy as np

x = tf.placeholder(tf.float32, [None, 3])
y = tf.placeholder(tf.float32, [None, 3])

mask = x[:, 0] > 0.1
x_gather = tf.boolean_mask(x, mask)
y_gather = tf.boolean_mask(y, mask)

data_x = np.array([
  [0, 1, 2],
  [1., 2., 3.],
  [4., 2, 3.]])
data_y = np.array([
  [0, 1, 2],
  [11., 2., 3.] )
with tf.Session() as sess:
  print(sess.run(y_gather, feed_dict={x: data_x, y: data_y}))
  print(sess.run(x_gather, feed_dict={x: data_x, y: data_y}))

it outputs
 [[11.  2.  3.]
 [ 0.  0.  0.]]
[[1. 2. 3.]
 [4. 2. 3.]] rather than throw a shape error
    
"
21230,tensorflow to tensorflow-lite conversion: unsupported operation: NonMaxSuppressionV2,"Hello all,

### System information

== cat /etc/issue ===============================================
Linux master 4.15.0-29-generic #31~16.04.1-Ubuntu SMP Wed Jul 18 08:54:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.5.0-12ubuntu1~16.04) 5.5.0 20171010
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux master 4.15.0-29-generic #31~16.04.1-Ubuntu SMP Wed Jul 18 08:54:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                         1.14.5                
protobuf                      3.6.0                 
tensorflow                    1.8.0                 
tensorflow-tensorboard        0.4.0                 

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = v1.8.0-0-g93bc2e2072
tf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072
Sanity check: array([1], dtype=int32)
/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/lib64/:/usr/local/cuda/lib64/:/usr/local/cuda-9.0/lib64:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Mon Jul 30 11:49:23 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 396.45                 Driver Version: 396.45                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro P600         Off  | 00000000:03:00.0  On |                  N/A |
| 34%   43C    P8    N/A /  N/A |    574MiB /  1997MiB |      1%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1152      G   /usr/lib/xorg/Xorg                           237MiB |
|    0      2213      G   compiz                                        79MiB |
|    0      2589      G   ...-token=224D99F526E00AE3A3C0EF4D5E6D103A   113MiB |
|    0      5305      G   ...-token=53BA9EE005E85645FEB6537828E37D64   141MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.2/doc/man/man7/libcudart.7
/usr/local/cuda-9.2/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148
 

### Describe the problem
I am trying to test tensorflow lite on android and following the instructions given in this link: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md
however, after executing the command 

> tensorflow$ bazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=""tflite_graph.pb"" --output_file=""mobilenetSSD.tflite"" --input_shapes=1,300,300,3 --input_arrays='image_tensor' --output_arrays='detection_boxes','detection_scores','detection_classes' --inferente_type=FLOAT --allow_custom_ops

### Source code / logs

INFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).
INFO: Found 1 target...
Target //tensorflow/contrib/lite/toco:toco up-to-date:
  bazel-bin/tensorflow/contrib/lite/toco/toco
INFO: Elapsed time: 0.327s, Critical Path: 0.00s
INFO: 0 processes.
INFO: Build completed successfully, 1 total action
INFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/tflite_graph.pb' '--output_file=mobilenetSSD.tflite' '--input_shapes=1,300,300,3' '--input_arrays=image_tensor' '--output_arraysINFO: Build completed successfully, 1 total action
2018-07-30 11:05:10.064158: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Unpack
2018-07-30 11:05:10.064255: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.064302: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.064321: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.064349: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.064388: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.064405: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.064432: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.064469: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.064487: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.064513: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.064550: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.064567: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.064593: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.064629: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.064646: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.064671: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.064707: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.064724: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.064749: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.064785: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.064801: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.064826: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.064861: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.064877: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.064905: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.064942: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.064959: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.064983: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.065020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.065045: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.065071: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.065109: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.065125: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.065151: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.065187: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.065204: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.065229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.065264: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.065281: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.065305: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.065341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.065358: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.065383: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.065418: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.065435: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.065459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.065495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.065511: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.065535: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.065571: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.065588: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.065613: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.065649: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.065666: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.065690: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.065726: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.065743: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.065768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.065804: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.065821: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.065845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Where
2018-07-30 11:05:10.065880: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: NonMaxSuppressionV2
2018-07-30 11:05:10.065897: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: ZerosLike
2018-07-30 11:05:10.065946: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: Size
2018-07-30 11:05:10.074173: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 748 operators, 1262 arrays (0 quantized)
2018-07-30 11:05:10.097717: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 742 operators, 1250 arrays (0 quantized)
2018-07-30 11:05:10.126346: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 742 operators, 1250 arrays (0 quantized)
2018-07-30 11:05:10.169961: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 444 operators, 813 arrays (0 quantized)
2018-07-30 11:05:10.181204: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 444 operators, 813 arrays (0 quantized)
2018-07-30 11:05:10.188689: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:329] Total transient array allocated size: 11520000 bytes, theoretical optimal value: 8640000 bytes.
2018-07-30 11:05:10.191222: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191247: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191267: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191278: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191295: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191307: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191322: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191333: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191348: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191361: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191377: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191390: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191420: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191434: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191450: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191461: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191475: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191486: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191501: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191511: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191528: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191539: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191556: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191566: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191581: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191592: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191608: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191621: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191637: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191648: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191663: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191674: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191688: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191699: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191713: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191724: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191738: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191749: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191764: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191774: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191789: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191799: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'
2018-07-30 11:05:10.191814: W tensorflow/contrib/lite/toco/tflite/operator.cc:1148] Ignoring unsupported attribute type with key 'T'

**note:** the model that I am using is converted from caffe mobilenet-ssd and it works on PC.
I converted existing tensorflow mobilenet-ssd model and uploaded the apk on android and detection results show so many overlapping boxes over image. because of this reason, I am thinking that those models run without nms algorithm. 
my questions is:
1. Do existing tensorflow models (e.g., mobilenetssd) run without nms algorithm?
2. If (1)==yes, how to convert NonMaxSuppressionV2 into tf-lite?

I also checked the: https://github.com/tensorflow/tensorflow/issues/19263
but not exact answer here.

this is the android debug log:

Caused by: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find custom op for name 'NonMaxSuppressionV2' with version 1
07-27 18:26:14.809   872   872 E AndroidRuntime: Didn't find custom op for name 'Unpack' with version 1
07-27 18:26:14.809   872   872 E AndroidRuntime: Didn't find custom op for name 'Where' with version 1
07-27 18:26:14.809   872   872 E AndroidRuntime: Didn't find custom op for name 'ZerosLike' with version 1
07-27 18:26:14.809   872   872 E AndroidRuntime: Registration failed.

Thanks,



"
21229,Default behavior of tf.layers.batch_normalization,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.5 LTS (Xenial Xerus)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: pip (binary)
- **TensorFlow version (use command below)**: (tf.GIT_VERSION, tf.VERSION) == ('v1.9.0-0-g25c197e023', '1.9.0')
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/6.1
- **GPU model and memory**: NVIDIA GeForce GTX 1080Ti 11177MiB
- **Exact command to reproduce**: tf.layers.batch_normalization
### Describe the problem
I was trying to train my network using multiple batch normalization layers(`tf.layers.batch_normalization`), only to fail the training process even though I was basically copying a published paper. I had skimmed through the documentation as it looked fairly straightforward, especially after already having had added multiple layers using `tf.layers`. Only after a couple days (as training takes quite a long time) did I realize I had missed the note part of the documentation. This is of course my fault for not reading the documentation thoroughly, but it really strikes me odd that the default behavior of this layer is not to update the ops right away. I guess it might be due to performance reasons, but if that is the case why not just have `defer_updates` argument which defaults to False? I think most beginners would expect the layer to update everything right away, and maybe not even expect everything to be fast anyway. Only power users would really want to fine-tune and optimize their network, in which case then they can set `defer_updates` to True.

I think I just rambled a lot here; my point is that I think it would be better if `tf.layers.batch_normalization` would update ops without the need of `update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)` and `with tf.control_dependencies(update_ops)`, and for people who do not want this, throw `defer_updates=True`.

Edit: formatting



"
21228,"Cloud MLEngine: Check failed: DeviceNameUtils::ParseFullName(new_base, &parsed_name)","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Raspbian GNU/Linux 9
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: binary (from https://github.com/lhelontra/tensorflow-on-arm/releases/download/v1.8.0/tensorflow-1.8.0-cp35-none-linux_armv7l.whl)
- **TensorFlow version (use command below)**: v1.8.0-1g0d1f389b6c 1.8.0
- **Python version**: 3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: (Running on gcloud)
```
gcloud ml-engine jobs submit training object_detection_`date +%m_%d_%Y_%H_%M_%S` \
--runtime-version 1.8 \     
--job-dir=gs://aka_b2/train/ \     
--packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz \
--module-name object_detection.model_main \
--region us-central1 \
--config object_detection/samples/cloud/cloud.yml \
-- \
--model_dir=gs://aka_b2/train/ \
--pipeline_config_path=gs://aka_b2/data/ssd_mobilenet_v1_coco.config
```

### Describe the problem
I have successfully trained one object detection model on cloud. I created another bucket and and used same tfrecords, and a different model to do another training. But training doesn't start and following errors are given.

### Source code / logs
```
Check failed: DeviceNameUtils::ParseFullName(new_base, &parsed_name) 
{
 insertId:  ""1am4lt7g2ytgyip""  
 jsonPayload: {
  created:  1532870862.316736   
  levelname:  ""CRITICAL""   
  lineno:  27   
  message:  ""Check failed: DeviceNameUtils::ParseFullName(new_base, &parsed_name) ""   
  pathname:  ""tensorflow/core/common_runtime/renamed_device.cc""   
 }
 labels: {
  compute.googleapis.com/resource_id:  ""8188383009228980271""   
  compute.googleapis.com/resource_name:  ""cmle-training-ps-1d73aafb3a-0-7bjnw""   
  compute.googleapis.com/zone:  ""us-central1-a""   
  ml.googleapis.com/job_id:  ""object_detection_07_29_2018_14_17_36""   
  ml.googleapis.com/job_id/log_area:  ""root""   
  ml.googleapis.com/task_name:  ""ps-replica-0""   
  ml.googleapis.com/trial_id:  """"   
 }
 logName:  ""projects/object-detection-210310/logs/ps-replica-0""  
 receiveTimestamp:  ""2018-07-29T13:27:48.515404065Z""  
 resource: {
  labels: {
   job_id:  ""object_detection_07_29_2018_14_17_36""    
   project_id:  ""object-detection-210310""    
   task_name:  ""ps-replica-0""    
  }
  type:  ""ml_job""   
 }
 severity:  ""CRITICAL""  
 timestamp:  ""2018-07-29T13:27:42.316735982Z""  
}

Followed by,
-ps-replica-1
Command '['python', '-m', u'object_detection.model_main', u'-- 
model_dir=gs://aka_b2/train/', u'-- 
pipeline_config_path=gs://aka_b2/data/ssd_mobilenet_v1_coco.config', '--job- 
dir', u'gs://aka_b2/train/']' returned non-zero exit status -6

{
 insertId:  ""1d4klnfg3ihl2be""  
 jsonPayload: {
  created:  1532870863.971174   
  levelname:  ""ERROR""   
  lineno:  879   
  message:  ""Command '['python', '-m', u'object_detection.model_main', u'--model_dir=gs://aka_b2/train/', u'--pipeline_config_path=gs://aka_b2/data/ssd_mobilenet_v1_coco.config', '--job-dir', u'gs://aka_b2/train/']' returned non-zero exit status -6""   
  pathname:  ""/runcloudml.py""   
 }
 labels: {
  compute.googleapis.com/resource_id:  ""7345648913232166992""   
  compute.googleapis.com/resource_name:  ""cmle-training-ps-1d73aafb3a-1-tjx4f""   
  compute.googleapis.com/zone:  ""us-central1-a""   
  ml.googleapis.com/job_id:  ""object_detection_07_29_2018_14_17_36""   
  ml.googleapis.com/job_id/log_area:  ""root""   
  ml.googleapis.com/task_name:  ""ps-replica-1""   
  ml.googleapis.com/trial_id:  """"   
 }
 logName:  ""projects/object-detection-210310/logs/ps-replica-1""  
 receiveTimestamp:  ""2018-07-29T13:27:47.591698250Z""  
 resource: {
  labels: {
   job_id:  ""object_detection_07_29_2018_14_17_36""    
   project_id:  ""object-detection-210310""    
   task_name:  ""ps-replica-1""    
  }
  type:  ""ml_job""   
 }
 severity:  ""ERROR""  
 timestamp:  ""2018-07-29T13:27:43.971174001Z""  
}

And finally,
2018-07-29 23:10:05.645 IST
worker-replica-2
CreateSession still waiting for response from worker: /job:ps/replica:0/task:1
{
 insertId:  ""iltasog3l3wajo""  
 jsonPayload: {
  created:  1532886005.645953   
  levelname:  ""INFO""   
  lineno:  224   
  message:  ""CreateSession still waiting for response from worker: /job:ps/replica:0/task:1""   
  pathname:  ""tensorflow/core/distributed_runtime/master.cc""   
 }
 labels: {
  compute.googleapis.com/resource_id:  ""3481433417880454925""   
  compute.googleapis.com/resource_name:  ""cmle-training-worker-f58b439b46-2-96k8w""   
  compute.googleapis.com/zone:  ""us-central1-c""   
  ml.googleapis.com/job_id:  ""object_detection_07_29_2018_18_29_49""   
  ml.googleapis.com/job_id/log_area:  ""root""   
  ml.googleapis.com/task_name:  ""worker-replica-2""   
  ml.googleapis.com/trial_id:  """"   
 }
 logName:  ""projects/object-detection-210310/logs/worker-replica-2""  
 receiveTimestamp:  ""2018-07-29T17:40:10.955863033Z""  
 resource: {
  labels: {
   job_id:  ""object_detection_07_29_2018_18_29_49""    
   project_id:  ""object-detection-210310""    
   task_name:  ""worker-replica-2""    
  }
  type:  ""ml_job""   
 }
 severity:  ""INFO""  
 timestamp:  ""2018-07-29T17:40:05.645952939Z""  
}
```
After that, the job fails with the following log entry.

```
The replica ps 0 exited with a non-zero status of 6(SIGABRT). Termination reason: Error. The replica ps 1 exited with a non-zero status of 6(SIGABRT). Termination reason: Error. The replica ps 2 exited with a non-zero status of 6(SIGABRT). Termination reason: Error. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=26926246216&resource=ml_job%2Fjob_id%2Fobject_detection_07_30_2018_03_57_25&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22object_detection_07_30_2018_03_57_25%22
```

Thank you."
21227,Confused in Distributed Training in Tensorflow,"Hi,I have tried several methods to build my distributed training system , but the training speed is fallacious. As I want, several machines with one master and muti ps(3 ps),the training speed depends on the slowest one of the ps. But what I got it is almost 200s every 100steps in En-De training, I'm very confused about this. What's more ,the t2t defines the master and ps , but in conventional tensorflow distributed training tutorial, there should be worker and ps. The ps plays the role of updating variables like master in T2T , and the worker computes the gradients parallel like ps in T2T. This really makes me amazing! I have ask questions in gitter ,but no one answer me . I hope you can give me some advice !"
21226,FP16 Batch Matmul still does not run.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Slightly.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:  Binary
- **TensorFlow version (use command below)**:  v1.9.0-0-g25c197e023 1.9.0
- **Python version**:  3.6
- **CUDA/cuDNN version**:  9.1
- **GPU model and memory**:  V100 on GCE
- **Exact command to reproduce**:

```python
with tf.device(""/gpu:0""):
    a = tf.random_normal(dtype=tf.float16, shape=[5, 2, 3], name='a')
    b = tf.random_normal(dtype=tf.float16, shape=[5, 3, 2], name='b')
    c = tf.matmul(a, b)
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=False))
print(sess.run(c).shape)
```

### Describe the problem
The above operation yields a InvalidArgumentError saying the kernel is not registered for DT_HALF. I've copied the full error below.

When the operation is pinned to `cpu:0`, it runs with no problems.

This issue is referenced in #18123, and was apparently addressed in #18436, then committed to master in commit `f08f24cd559b5824a1874a0e76d339875e43f366`, which ought to be included in v1.9.0, but the above code still errors for me when I try it.

### Source code / logs
```
InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'MatMul_1': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
Registered kernels:
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]

         [[Node: MatMul_1 = BatchMatMul[T=DT_HALF, adj_x=false, adj_y=false, _device=""/device:GPU:0""](a_1, b_1)]]
```"
21225,Running tensorboard with numpy 1.15.0 gives a few RuntimeWarning,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Any
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.5.3
- **Bazel version (if compiling from source)**: 1.12
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: 9.2
- **GPU model and memory**: Titan XP
- **Exact command to reproduce**:
1. Make sure you have numpy 1.15.0 installed:
2. run tensorboard
3. A few RuntimeWarning appear
If you have version 1.14.5 installed instead, no RuntimeWarnings appear.

### Describe the problem
Running tensorboard with numpy 1.15.0 gives a few RuntimeWarning (see below). All is fine with numpy 1.14.5 and below.

### Source code / logs
```/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)```
"
21224,"tf.while_loop - Eager mode allows a changing input size by default, while Graph mode does not","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Window10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Unknown
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: CPU version
- **Exact command to reproduce**:

### Describe the problem
With tf.while_loop - Eager mode allows a changing input size by default, while Graph mode does not.
This seems to me like a consistency issue.
When eager and graph mode are consistent it allows to debug in eager, and when done run as graph. Inconsistency between eager and graph modes requires an extra debugging step in the graph mode. 

The error message in graph mode:
""ValueError: Input tensor 'while/Const_1:0' enters the loop with shape (), but has shape (2, 1) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.""
But works perfectly in eager mode.
### Source code / logs
*** EAGER (WORKS) ***:
import tensorflow as tf
import numpy as np
tf.enable_eager_execution()

batch = 2
time = 3
input_depth = 5
inpt = tf.constant(np.random.normal(size=(batch, input_depth)).astype(np.float32))

def timestep(t, dns_output):
    dns_output = dns(inpt)
    return t+1, dns_output


dns = tf.layers.Dense(units=1)
_, dns_output = tf.while_loop(lambda t, dns_output: t < time, timestep, (0, 0))

print(dns_output)

***Graph (Error)***:
import tensorflow as tf
import numpy as np

batch = 2
time = 3
input_depth = 5
dense_units = 1
inpt = tf.constant(np.random.normal(size=(batch, input_depth)).astype(np.float32))

def timestep(t, dns_output):
    dns_output = dns(inpt)
    print(dns_output)
    return t+1, dns_output


dns = tf.layers.Dense(units=dense_units)

_, dns_output1 = tf.while_loop(lambda t, dns_output: t < time, timestep, (0, 0))

sess = tf.Session()
sess.run(tf.global_variables_initializer())
out = sess.run(dns_output1)
print(out)

***Graph (No Error, works)***:
import tensorflow as tf
import numpy as np

batch = 2
time = 3
input_depth = 5
dense_units = 1
inpt = tf.constant(np.random.normal(size=(batch, input_depth)).astype(np.float32))

def timestep(t, dns_output):
    dns_output = dns(inpt)
    return t+1, dns_output


dns = tf.layers.Dense(units=dense_units)
dns_output_temp = tf.zeros(shape=[batch, dense_units])
_, dns_output1 = tf.while_loop(lambda t, dns_output: t < time, timestep, (0, dns_output_temp))

sess = tf.Session()
sess.run(tf.global_variables_initializer())
out = sess.run(dns_output1)
print(out)
"
21223,Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.,"I get the following error

An error ocurred while starting the kernel
2018󈚫󈛁 18:04:30.223185: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018󈚫󈛁 18:04:30.275857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (𔂫), but there must be at least one NUMA node, so returning NUMA node zero
2018󈚫󈛁 18:04:30.276812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: GeForce GT 750M major: 3 minor: 0 memoryClockRate(GHz): 0.967
pciBusID: 0000:01:00.0
totalMemory: 1.95GiB freeMemory: 1.57GiB
2018󈚫󈛁 18:04:30.306371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (𔂫), but there must be at least one NUMA node, so returning NUMA node zero
2018󈚫󈛁 18:04:30.307350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: 
name: GeForce GT 750M major: 3 minor: 0 memoryClockRate(GHz): 0.967
pciBusID: 0000:07:00.0
totalMemory: 1.95GiB freeMemory: 1.93GiB
2018󈚫󈛁 18:04:30.307702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1
2018󈚫󈛁 18:04:30.751898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018󈚫󈛁 18:04:30.751936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958] 0 1 
2018󈚫󈛁 18:04:30.751943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0: N Y 
2018󈚫󈛁 18:04:30.751947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1: Y N 
2018󈚫󈛁 18:04:30.752166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1346 MB memory) ‑> physical GPU (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0, compute capability: 3.0)
2018󈚫󈛁 18:04:30.767241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 1709 MB memory) ‑> physical GPU (device: 1, name: GeForce GT 750M, pci bus id: 0000:07:00.0, compute capability: 3.0)
2018󈚫󈛁 18:15:26.140459: E tensorflow/stream_executor/cuda/cuda_driver.cc:1078] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_INSTRUCTION
2018󈚫󈛁 18:15:26.140506: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 0x55b5818bb6d0: CUDA_ERROR_ILLEGAL_INSTRUCTION
2018󈚫󈛁 18:15:26.140514: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 0x55b5818bb6d0: CUDA_ERROR_ILLEGAL_INSTRUCTION
2018󈚫󈛁 18:15:26.140532: F tensorflow/stream_executor/cuda/cuda_dnn.cc:189] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.

my system information
Nvidi-smi RESULTS

 NVIDIA-SMI 390.77                 Driver Version: 390.77                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GT 750M     Off  | 00000000:01:00.0 N/A |                  N/A |
| N/A   50C    P8    N/A /  N/A |    432MiB /  1999MiB |     N/A      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GT 750M     Off  | 00000000:07:00.0 N/A |                  N/A |
| N/A   38C    P8    N/A /  N/A |      2MiB /  1999MiB |     N/A      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0                    Not Supported                                       |
|    1                    Not Supported                                       |


CUDA Results
CUDA Version 9.2.148

CUDNN version Results

#define CUDNN_MAJOR 7
#define CUDNN_MINOR 1
#define CUDNN_PATCHLEVEL 4
--
#define CUDNN_VERSION    (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)

#include ""driver_types.h""
what could be the error here

thanks"
21221,toco convert quantized mobilenet_v1_ppn failed,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:None of them
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.9.0-rc2-623-geb04124bb7' 1.9.0-rc0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.15
- **GCC/Compiler version (if compiling from source)**: gcc-7.1
- **CUDA/cuDNN version**: cuda-9.2,cudnn-0.71
- **GPU model and memory**: 16G
- **Exact command to reproduce**:

I trained model by  `models/research/object_detection/model_main.py` with backbone `samples/configs/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync.config`.The difference from official is that I added quantization code in this config file like 
`
graph_rewriter {
  quantization {
    delay: 48000
    activation_bits: 8
    weight_bits: 8
  }
}
` 
**Everything works fine on PC(both train and eval)**  then I tried to export it to tf-lite.
1) `export_tflite_ssd_graph.py` seems ok for I **got tflite_graph.pb and tflite_graph.pbtxt**.
2) `bazel run -c opt tensorflow/contrib/lite/toco:toco -- \
--input_file=$OUTPUT_DIR/tflite_graph.pb \
--output_file=$OUTPUT_DIR/detect.tflite \
--input_shapes=1,300,300,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \
--inference_type=QUANTIZED_UINT8 \
--mean_values=128 \
--std_values=128 \
--change_concat_input_ranges=false \
--allow_custom_ops
`
**failed with ""tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:116] Check failed: dim_x == dim_y (512 vs. 24)Dimensions must match""**

Any idea?"
21220,ls,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21219,please help me,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21217,"cuda 9.0 on windows 10 goes extremely unstable, prebuilt should be migrated to cuda 9.2","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Windows 10 x64
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: 
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.0rc0
- **Python version**: 3.6.6 x64
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: MSVC15
- **CUDA/cuDNN version**: 9.2/7.1.5
- **GPU model and memory**: GTX1050Ti GDDR5 4GB
- **Exact command to reproduce**:
CUDA 9.0 no longer supported


This is not a direct tensorflow issue. This is a suggestion for all windows 10 tensorflow-gpu user

These days NVIDIA geforce driver starts to reject CUDA 9.0 since this version is too old and deprecated



There is no reason tensorflow-gpu prebuilt binary is provided with cuda 9.0 and cudnn 7.0


Prebuild binary sholud be built with latest cuda and cudnn version"
21216,Eager notebook on Neural Machine Translation does not restore weights (using tfe.Checkpoint),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Fedora 28
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

Hello,

the excellent notebook on Neural Machine Translation 

https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb

does not include code to save and restore the model. 
I've added checkpointing code as per the eager execution guide

https://www.tensorflow.org/guide/eager

and also compared the code from the eager GAN example

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/gan

However, when I apply the same principles to the NMT example, the network does not restore the saved weights correctly.
I am attaching a complete executable file that shows the problem after training for just 5 epochs and loading only 100 lines of data, to allow for quick testing. 

There a two modes of execution, restore == False and restore == True.

When not restoring, the network produces a longish, ""random translation"" in epoch 1, and by epoch 5 has learned to just produce the end symbol.
In restore mode, I would expect that last behavior, but instead, it produces the random output.

The checkpointing-relative code looks like this (but see complete file below)

```
restore = True # resp False
checkpoint_dir = ""./ckpt-py/""
restore_from = ""./ckpt-py/-1""

checkpoint = tfe.Checkpoint(optimizer=optimizer,
                            encoder = encoder,
                            decoder = decoder,
                            optimizer_step=tf.train.get_or_create_global_step())

if restore == False:
  for epoch in range(EPOCHS):
      ...
      save_path = checkpoint.save(checkpoint_dir)
      metadata = tf.contrib.checkpoint.object_metadata(save_path)
      with open(""save_python.txt"", ""w"") as f: 
        f.write(str(metadata))
else: 
  rst = checkpoint.restore(restore_from)
  # or 
  #rst = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))
  metadata = tf.contrib.checkpoint.object_metadata(restore_from)
  with open(""restore_python.txt"", ""w"") as f: 
    f.write(str(metadata))
  #print(rst.assert_consumed())
```

If I uncomment the above 

```
rst.assert_consumed()
```

I see

```
  AssertionError: Unresolved object in checkpoint: attributes {
  name: ""VARIABLE_VALUE""
  full_name: ""decoder/embedding_1/embeddings""
  checkpoint_key: ""decoder/embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE""
```

but I don't know if this is useful information.

In any case, it would be great if you could advise how to successfully save an NMT as other users too will probably very interested in doing this. 
Thank you!

"
21214,tensorflow softmax_cross_entropy_with_logits_v2 throws ValueError,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:No
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.5.0
- **Python version**:2.7.12
- **Bazel version (if compiling from source)**: No
- **GCC/Compiler version (if compiling from source)**: No
- **CUDA/cuDNN version**: No
- **GPU model and memory**: CPU
- **Exact command to reproduce**:
```

import tensorflow as tf
import numpy as np

num_columns=24
num_classes=4
train_steps = 2


def model():

   ground_truth_input = tf.placeholder(tf.float32,[None,num_classes]) #onehotencoded with depth 4
   bottleneck_input = tf.placeholder(tf.float32,[None,num_columns])  #num_columns=24 keypoint features

   net = tf.layers.dense(inputs=bottleneck_input,units=100,activation=tf.nn.relu)
   logits = tf.layers.dense(inputs = net,units=4,activation=None)

   # loss function 
   loss_mean = tf.nn.softmax_cross_entropy_with_logits_v2(labels=ground_truth_input, logits=logits)#,dim=-1)

   with tf.name_scope('train'):
      optimizer = tf.train.MomentumOptimizer(
        learning_rate=0.1,
        use_nesterov=True,
        momentum=0.9)
      train_op = optimizer.minimize(loss_mean, global_step=tf.train.get_global_step())

   with tf.name_scope('SoftMax_Layer'):
      final_tensor = tf.nn.softmax(logits,name='Softmax')

   return train_op, ground_truth_input, bottleneck_input, loss_mean


trainStep, cross_entropy, features, ground_truth = model()

with tf.Session() as sess:
   for i in range(2):
       Label = np.eye(4)[np.random.choice(4,32)]
       Features = np.random.rand(32,24)
       train_summary, _ = sess.run([trainStep],feed_dict = {ground_truth : Label, features :Features})
```


### Describe the problem
features are of shape **32x24** and onehot labels are **32x4**. while running the code the softmax cross entropy raises ValueError. have asked [this](https://stackoverflow.com/questions/51574218/tensorflow-softmax-cross-entropy-with-logits-v2-throws-valueerror) on stackoverflow.

Originally i feed data from tfrecords generated from csv files. csv file contains 24 columns and one label column. I convert the label to onehot before fedding it to session.
### Source code / logs
Error logs 
Traceback (most recent call last):
 > File ""dummy.py"", line 57, in <module>
    train_summary, _ = sess.run([trainStep],feed_dict = {ground_truth : Label, features :Features})
 > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
 > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1104, in _run
    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))
>ValueError: Cannot feed value of shape (32, 4) for Tensor u'softmax_cross_entropy_with_logits/Reshape_2:0', which has shape '(?,)'
"
21213,Passing/Retrieving String data to/from TFLite model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
     [Yes, here's the source code](https://github.com/the-dagger/MLKitAndroid/blob/master/app/src/main/java/io/github/the_dagger/mlkit/activity/SmartReplyActivity.kt)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX, High Sierra
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Pixel 2XL
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:  1.9.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.15.2-homebrew

### Describe the problem
I am using the smartReply TFLite model with Firebase but couldn't figure out how to pass and retrieve the input data as String to and from my tflite Interpreter.
The default data types are limited to FLOAT32, INT32, UINT8 and INT64 and the documentation nowhere specifies how to provide string input or retrieving String output from a model.

https://firebase.google.com/docs/ml-kit/android/use-custom-models

### Source code / logs

Source code of the app : [https://github.com/the-dagger/MLKitAndroid/blob/master/app/src/main/java/io/github/the_dagger/mlkit/activity/SmartReplyActivity.kt](https://github.com/the-dagger/MLKitAndroid/blob/master/app/src/main/java/io/github/the_dagger/mlkit/activity/SmartReplyActivity.kt)

Stacktrace
```
07-29 00:14:26.286 6489-6489/? W/System.err: Caused by: java.lang.IllegalArgumentException: DataType error: DataType 5 is not recognized in Java (version 3)
        at org.tensorflow.lite.DataType.fromNumber(DataType.java:54)
        at org.tensorflow.lite.Tensor.<init>(Tensor.java:211)
        at org.tensorflow.lite.Tensor.fromHandle(Tensor.java:32)
        at org.tensorflow.lite.NativeInterpreterWrapper.getInputTensor(NativeInterpreterWrapper.java:245)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:110)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:168)
        at com.google.android.gms.internal.firebase_ml.zzho.runForMultipleInputsOutputs(Unknown Source:2)
        at com.google.android.gms.internal.firebase_ml.zzhn.zza(Unknown Source:459)
        at com.google.android.gms.internal.firebase_ml.zzhn.zza(Unknown Source:2)
        at com.google.android.gms.internal.firebase_ml.zzgu.call(Unknown Source:15)
        at com.google.android.gms.internal.firebase_ml.zzgp.zza(Unknown Source:0)
07-29 00:19:17.979 7895-7895/? W/System.err: com.google.firebase.ml.common.FirebaseMLException: Internal error has occurred when executing Firebase ML tasks
        at com.google.android.gms.internal.firebase_ml.zzgp.zza(Unknown Source:15)
        at com.google.android.gms.internal.firebase_ml.zzgq.run(Unknown Source:4)
        at android.os.Handler.handleCallback(Handler.java:873)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loop(Looper.java:193)
        at android.os.HandlerThread.run(HandlerThread.java:65)
    Caused by: java.lang.IllegalArgumentException: DataType error: DataType 5 is not recognized in Java (version 3)
        at org.tensorflow.lite.DataType.fromNumber(DataType.java:54)
        at org.tensorflow.lite.Tensor.<init>(Tensor.java:211)
```
"
21211,"tflite: ""Ensure that weights and inputs have the same channel dimension."" in transpose_conv.cc.  It seems that the input and output must have same channel dimension?","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21205,tensorflow on Python 3.7,"I upgraded to python 3.7. 

Now when I try to run it in jupyter notebook or in terminal, I receive following errors: 

Traceback (most recent call last):

  File ""/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 2963, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)

  File ""<ipython-input-1-39d68f4585c2>"", line 4, in <module>
    import tensorflow as tf

  File ""/usr/local/lib/python3.7/site-packages/tensorflow/__init__.py"", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *

  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 114
    def TFE_ContextOptionsSetAsync(arg1, async):
                                             ^
SyntaxError: invalid syntax"
21204,Retrieve 'feature_importances` from canned BoostedTreesRegressor and BoostedTreesClassifier,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Feature request

When using the Premade BoostedTreesRegressor or Classifier in TensorFlow to train a model, it will be nice to have a way to retrieve the `feature_imporances` or feature ranking to know which variables have the most effect in the resulting model as seen in Scikit-learn's implementation.

Perhaps it can be something like, `trees.get_variable_value('feature_imporances')` to retrieve the feature rank in the order of the inputs."
21200,the weights of tf.contrib.rnn.BasicLSTMCell can't be updated,"Environment
OS: Ubuntu 16.04
Tensorflow-gpu: 1.8

```
    lstm_cell = tf.contrib.rnn.BasicLSTMCell(
        num_units=self.config.num_lstm_units, state_is_tuple=True)
    if self.mode == ""train"":
      lstm_cell = tf.contrib.rnn.DropoutWrapper(
          lstm_cell,
          input_keep_prob=self.config.lstm_dropout_keep_prob,
          output_keep_prob=self.config.lstm_dropout_keep_prob)

    with tf.variable_scope(""lstm"", initializer=self.initializer) as lstm_scope:
      # Feed the image embeddings to set the initial LSTM state.
      zero_state = lstm_cell.zero_state(
          batch_size=self.image_embeddings.get_shape()[0], dtype=tf.float32)

      # Allow the LSTM variables to be reused.
      #lstm_scope.reuse_variables()

      ........

      scores = tf.Variable(tf.random_normal(shape=[K, self.config.batch_size, C]), name=""scores"")

      M = tf.Variable(tf.random_normal(shape=[K+1, self.config.batch_size, 2, 3]), name=""M"")
      tf.assign(M[0], tf.convert_to_tensor([[1., 0., 0.], [0., 1., 0.]]))

      lstm_input_size = 14
      zk_size = 4096

      hidden = zero_state

      for k in range(0, K+1):
          .......

          lstm_outputs, hidden = lstm_cell(f_k, hidden) 
```

M and Scores can be updated. Why BasicLSTMCell is not in the optimizeLoss module？


![opimizeloss](https://user-images.githubusercontent.com/7899459/43352992-ce472a2a-925f-11e8-9f48-08c9d64fa503.png)"
21199,Documentation issue ,"![screenshot 25 _li](https://user-images.githubusercontent.com/25861787/43352882-816db4d2-9244-11e8-8f0a-3af1eef9b29f.jpg)
It's a Fashion MNIST dataset Tesorflow tutorial.What are digits doing here?"
21198,'toco_from_protos: not found' error found using prebuilt tensorflow binary for Raspbian Stretch 9 on Raspberry Pi 3 Model B,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Raspbian Stretch 9
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Raspberry Pi 3 Model B
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:
tf.GIT_VERSION returns v1.9.0-0-g25c197e
- **Python version**:3.5.3
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:N/A
- **GPU model and memory**:Please refer to Raspberry Pi 3 Model B specs, but I use the cpu only version of tensorflow.
- **Exact command to reproduce**:
use example code in tf-lite wiki... One sec...
# Converting a GraphDef from file. converter = lite.TocoConverter.from_frozen_graph( graph_def_file, input_arrays, output_arrays) tflite_model = converter.convert() open(""converted_model.tflite"", ""wb"").write(tflite_model)
This was taken from tf.contrib.lite.TocoConverter's API webpage.

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
tl;dw2r run the script on the above specs and you get 'toco_from_protos: not found'
long story:
I am trying to convert a frozen pb file into a tflite format.  I have a Windows 10 system, but tflite isn't available yet on the Windows 10 binaries.  The only other system that I have available is a Raspberry Pi so I figured that its version of tensorflow should have access to the tflite segment of the api.  Sure enough, it was there.  But when I ran the script, the following error occurred.  I can try to use Ubuntu, but it's been a bit of a learning curve trying to manage all the systems I have built on my Windows computer with Linux alternatives.  If at all possible, I would like to have a solution on either of these devices.  

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21197,tensorflow conflicts with PCL,"I want to build a .so file using tensorflow and pcl in ubuntu16.04LTS, but can't succeed. In order to find what arise the errors, I deleted all codes but INCLUDE sentence, and found the header of tensorflow conflicts with pcl'. How should I do?
---------------------------------------------------------------------------------------------------------------------------------
cpp file:
#include <pcl/point_types.h>
#include <pcl/registration/icp.h>
#include <pcl/registration/lum.h>
#include ""tensorflow/core/framework/common_shape_fns.h""
#include ""tensorflow/core/framework/op_kernel.h""
#include ""tensorflow/core/framework/register_types.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/util/work_sharder.h""
CMakeLists.txt:
cmake_minimum_required(VERSION 2.8)
project(icp_op)
set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -std=c++11"")
add_definitions(-D_GLIBCXX_USE_CXX11_ABI=0)
set(PCL_DIR /home/XXX/software/pcl181/share/pcl-1.8)
find_package(PCL REQUIRED)
include_directories(${PCL_INCLUDE_DIRS})
include_directories(
	/home/XXX/software/eigen/include/eigen3
	/usr/local/lib/python2.7/dist-packages/tensorflow/include/external/eigen_archive
	/usr/local/lib/python2.7/dist-packages/tensorflow/include
	/usr/local/lib/python2.7/dist-packages/tensorflow/include/external/nsync/public
)
link_directories(
	/usr/local/lib/python2.7/dist-packages/tensorflow
)
add_library(${PROJECT_NAME} SHARED
	icp_op_kernel.cpp
)
target_link_libraries(${PROJECT_NAME}
	${PCL_LIBRARIES}
	tensorflow_framework
)"
21196,MobileNet v2 slower than v1 when loading from Frozen GraphDef,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04.3 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
Doesn't apply
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
v1.9.0-0-g25c197e023 1.9.0
- **Python version**:
Python3
- **Bazel version (if compiling from source)**:
Doesn't apply
- **GCC/Compiler version (if compiling from source)**:
Doesn't apply
- **CUDA/cuDNN version**:
CUDA: 9.0 ; cuDNN: 7.1.4
- **GPU model and memory**:
GeForce GTX 1080 Ti
- **Exact command to reproduce**:

### Describe the problem

### Summary
MobileNet v2 is faster than v1 only when loading from the checkpoint format i.e, Variable Ops (meta, index, data) whereas it is slower than v1 when running in the frozen graph format (Const Ops) (.pb)

### Description
I have two TensorFlow trained models that are in the checkpoint format (meta, index, data) namely mobilenetv1_0.75.ckpt and mobilenetv2_0.75_6.ckpt. 

The model definitions are as described in the [MobileNetv1](https://arxiv.org/abs/1704.04861) and [MobileNetv2](https://arxiv.org/pdf/1801.04381.pdf) papers. Both models are trained with a width_multiplier of 0.75. MobileNetv2 has an expansion factor of 6. This means that MAC wise, v2 is better than v1 (v1: 26.5 Mil, v2: 20.6Mil) and is expected to be slightly faster than v1.

To compare how the models actually perform, I evaluated them in two ways
Method 1: After Training - Inference by loading the models from checkpoints
Method 2: During Deployment - Inference by loading the models from frozen GraphDefs ([freeze_graph](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) for converting Variables to Consts)

**Tools Used for Testing:**
- TF's Timeline Trace Tool
- benchmark_model Tool
- Naive Python time module

### Timeline Tool
I used the TensorFlow's [timeline](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/timeline.py) tool to view the execution times in the Chrome Trace format. 

**Method 1:** 
MobileNetv1 Trace

<img width=""1440"" alt=""image"" src=""https://user-images.githubusercontent.com/7610546/43350531-b0ad3f8e-9225-11e8-95da-d21ac7cdc1e6.png"">

MobileNetv2 Trace

<img width=""1440"" alt=""image"" src=""https://user-images.githubusercontent.com/7610546/43350534-bc7a644a-9225-11e8-8032-ec10bddf52a9.png"">

Eyeballing it, V2 is only slightly faster than V1; measuring from Conv1 to SoftMax,

v1: 9.73 ms
v2: 8.153 ms

**Method 2:**

MobileNetv1 Trace

<img width=""1440"" alt=""image"" src=""https://user-images.githubusercontent.com/7610546/43350556-ea35af48-9225-11e8-8f2d-56ccd7e600d3.png"">

MobileNet v2 Trace

<img width=""1437"" alt=""image"" src=""https://user-images.githubusercontent.com/7610546/43350562-f60ebda0-9225-11e8-992d-5c7ae66a5eb4.png"">

As you can clearly see, when the models are loaded from a frozen format, v2 is slower than v1. This affects the performance timings for v2 especially while deployment since models are usually exported in the frozen format.


### Time Measurements with Python's time module

Apart from the time traces above, I calculated the FPS by measuring the time between session.run. Even though the frozen models are faster than the checkpoint format, v2 is slower than v1 in the frozen format. Why is that so?

Model | Checkpoint - FPS | Frozen (FPS)
-- | -- | --
MobileNetv1 | 97.6 | 254.86
MobileNetv2 | 159.22 | 185.04


### Benchmark Model Tool Results


**MobileNet_v1_75**

Command Used:
`bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=MobileNet_V1_75.pb --input_layer=""input/Placeholder"" --input_layer_shape=""1,64,64,3"" --input_layer_type=""float"" --output_layer=""output/Softmax_1"" --show_run_order=false --show_time=false --show_memory=false --show_summary=true --show_flops=true`

[Node type] | [count] | [avg ms] | [avg %] | [cdf %] | [mem KB] | [times called]
-- | -- | -- | -- | -- | -- | --
Conv2D | 15 | 1.044 | 21.20% | 21.20% | 8658.432 | 15
gpu:Conv2D | 15 | 0.571 | 11.60% | 32.80% | 0 | 35
BiasAdd | 28 | 0.557 | 11.31% | 44.11% | 0 | 28
Add | 27 | 0.493 | 10.01% | 54.12% | 0 | 27
Mul | 27 | 0.48 | 9.75% | 63.87% | 0 | 27
Relu6 | 27 | 0.414 | 8.41% | 72.28% | 0 | 27
DepthwiseConv2dNative | 13 | 0.392 | 7.96% | 80.24% | 540.672 | 13
Const | 113 | 0.37 | 7.51% | 87.75% | 0 | 113
gpu:Mul | 27 | 0.082 | 1.67% | 89.42% | 0 | 27
gpu:Add | 27 | 0.081 | 1.65% | 91.06% | 0 | 27
gpu:DepthwiseConv2dNative | 13 | 0.061 | 1.24% | 92.30% | 0 | 13
NoOp | 1 | 0.061 | 1.24% | 93.54% | 0 | 2
gpu:BiasAdd | 28 | 0.059 | 1.20% | 94.74% | 0 | 28
Transpose | 2 | 0.055 | 1.12% | 95.86% | 49.152 | 2
gpu:Relu6 | 27 | 0.054 | 1.10% | 96.95% | 0 | 27
gpu:MEMCPYHtoD | 1 | 0.043 | 0.87% | 97.83% | 0 | 1
Softmax | 1 | 0.04 | 0.81% | 98.64% | 0.512 | 1
AvgPool | 1 | 0.033 | 0.67% | 99.31% | 3.072 | 1
gpu:Softmax | 1 | 0.009 | 0.18% | 99.49% | 0 | 3
gpu:AvgPool | 1 | 0.006 | 0.12% | 99.61% | 0 | 1
_Arg | 1 | 0.006 | 0.12% | 99.74% | 0 | 1
gpu:Transpose | 1 | 0.005 | 0.10% | 99.84% | 0 | 1
_Retval | 1 | 0.004 | 0.08% | 99.92% | 0 | 1
Reshape | 1 | 0.003 | 0.06% | 99.98% | 0 | 1
gpu:MEMCPYDtoH | 1 | 0.001 | 0.02% | 100.00% | 0 | 1
Total |   | 4.924 |   |   |  

**MobileNet_v2_75**

Command Used:
`bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=MobileNet_V2_75.pb --input_layer=""input/Placeholder"" --input_layer_shape=""1,64,64,3"" --input_layer_type=""float"" --output_layer=""output/Softmax_1"" --show_run_order=false --show_time=false --show_memory=false --show_summary=true --show_flops=true`

[Node type] | [count] | [avg ms] | [avg %] | [cdf %] | [mem KB] | [times called]
-- | -- | -- | -- | -- | -- | --
Conv2D | 39 | 2.967 | 37.97% | 37.97% | 7955.2 | 39
Add | 53 | 1.217 | 15.58% | 53.55% | 0 | 53
gpu:Conv2D | 39 | 1.036 | 13.26% | 66.80% | 0 | 93
Relu6 | 36 | 0.554 | 7.09% | 73.89% | 0 | 36
DepthwiseConv2dNative | 17 | 0.408 | 5.22% | 79.11% | 959.232 | 17
Const | 129 | 0.403 | 5.16% | 84.27% | 0 | 129
Mul | 17 | 0.304 | 3.89% | 88.16% | 0 | 17
AddN | 12 | 0.264 | 3.38% | 91.54% | 0 | 12
gpu:Add | 53 | 0.161 | 2.06% | 93.60% | 0 | 53
NoOp | 1 | 0.076 | 0.97% | 94.57% | 0 | 2
gpu:Relu6 | 36 | 0.073 | 0.93% | 95.51% | 0 | 36
gpu:DepthwiseConv2dNative | 17 | 0.07 | 0.90% | 96.40% | 0 | 17
Transpose | 2 | 0.053 | 0.68% | 97.08% | 49.152 | 2
gpu:Mul | 17 | 0.052 | 0.67% | 97.75% | 0 | 17
gpu:MEMCPYHtoD | 1 | 0.044 | 0.56% | 98.31% | 0 | 1
Softmax | 1 | 0.041 | 0.53% | 98.84% | 0.512 | 1
AvgPool | 1 | 0.031 | 0.40% | 99.23% | 3.84 | 1
gpu:AddN | 12 | 0.025 | 0.32% | 99.55% | 0 | 12
gpu:Softmax | 1 | 0.009 | 0.12% | 99.67% | 0 | 3
gpu:AvgPool | 1 | 0.007 | 0.09% | 99.76% | 0 | 1
_Arg | 1 | 0.006 | 0.08% | 99.83% | 0 | 1
gpu:Transpose | 1 | 0.005 | 0.06% | 99.90% | 0 | 1
_Retval | 1 | 0.004 | 0.05% | 99.95% | 0 | 1
Reshape | 1 | 0.003 | 0.04% | 99.99% | 0 | 1
gpu:MEMCPYDtoH | 1 | 0.001 | 0.01% | 100.00% | 0 | 1
Total |   | 7.814 |   |   |  


**Avg Time
v1: 4.924 ms
v2: 7.814 ms**

### Source code / logs

How I load the model with method 1:

```
    def __load_model(self):
        latest_checkpoint = tf.train.latest_checkpoint(self.args.checkpoint_dir)
        if latest_checkpoint:
            print(""Loading model checkpoint {} ...\n"".format(latest_checkpoint))
            self.saver.restore(self.sess, latest_checkpoint)
            print(""Checkpoint loaded\n\n"")
        else:
            print(""No checkpoints available!\n\n"")
```

How I load the model with method 2:

```
def create_graph(checkpoint_path):
   with tf.gfile.FastGFile(checkpoint_path, 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        _ = tf.import_graph_def(graph_def, name='')
```

I'm interested to know the details on why v2's performance is only half as good as v1 when in the paper it is discussed that v2 is supposed to be 35% faster.

Note:
I've cross checked this with other hardware such as P40, an i5 7th Gen CPU and an alternative TF version (1.5.0) and this pattern is the same."
21194,errors after installing pydot-ng  ImportError: DLL load failed: The specified module could not be found. and SyntaxError: invalid syntax,"I followed the instructions given in the link. 
http://www.codesofinterest.com/2017/02/visualizing-model-structures-in-keras.html

then I did:
conda install pydot-ng 

Then i installed graphviz from here http://www.graphviz.org/download/ and added C:\Program Files (x86)\Graphviz2.38\bin to PATH

  after that it gives me errors if i made any "" conda"" instruction in command window or anaconda prompt
and also give the same error when writing ""  activate tensorflow  "" in command window



Error:

(base) C:\Users\BioHelwan>activate tensorflow
Traceback (most recent call last):
  File ""C:\Users\BioHelwan\Anaconda3\lib\site-packages\conda\cli\main.py"", line 98, in main
    return activator_main()
  File ""C:\Users\BioHelwan\Anaconda3\lib\site-packages\conda\activate.py"", line 632, in main
    print(activator.execute(), end='')
  File ""C:\Users\BioHelwan\Anaconda3\lib\site-packages\conda\activate.py"", line 166, in execute
    return getattr(self, self.command)()
  File ""C:\Users\BioHelwan\Anaconda3\lib\site-packages\conda\activate.py"", line 152, in activate
    return self._finalize(self._yield_commands(self.build_activate(self.env_name_or_prefix)),
  File ""C:\Users\BioHelwan\Anaconda3\lib\site-packages\conda\activate.py"", line 231, in build_activate
    prefix = locate_prefix_by_name(env_name_or_prefix)
  File ""C:\Users\BioHelwan\Anaconda3\lib\site-packages\conda\base\context.py"", line 951, in locate_prefix_by_name
    envs_dirs = context.envs_dirs
  File ""C:\Users\BioHelwan\Anaconda3\lib\site-packages\conda\base\context.py"", line 381, in envs_dirs
    join(self._user_data_dir, 'envs'),
  File ""C:\Users\BioHelwan\Anaconda3\lib\site-packages\conda\base\context.py"", line 416, in _user_data_dir
    return user_data_dir(APP_NAME, APP_NAME)
  File ""C:\Users\BioHelwan\Anaconda3\lib\site-packages\conda\_vendor\appdirs.py"", line 67, in user_data_dir
    path = os.path.join(_get_win_folder(const), appauthor, appname)
  File ""C:\Users\BioHelwan\Anaconda3\lib\site-packages\conda\_vendor\appdirs.py"", line 284, in _get_win_folder_with_pywin32
    from win32com.shell import shellcon, shell
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\BioHelwan\Anaconda3\Scripts\conda-script.py"", line 10, in <module>
    sys.exit(main())
  File ""C:\Users\BioHelwan\Anaconda3\lib\site-packages\conda\cli\main.py"", line 108, in main
    init_loggers()
  File ""C:\Users\BioHelwan\Anaconda3\lib\site-packages\conda\cli\main.py"", line 55, in init_loggers
    from ..gateways.logging import initialize_logging, set_verbosity
  File ""C:\Users\BioHelwan\Anaconda3\lib\site-packages\conda\gateways\logging.py"", line 12, in <module>
    from ..common.io import attach_stderr_handler
  File ""C:\Users\BioHelwan\Anaconda3\lib\site-packages\conda\common\io.py"", line 5, in <module>
    from concurrent.futures import ThreadPoolExecutor, _base, as_completed
  File ""C:\Users\BioHelwan\Anaconda3\envs\tensorflow\Lib\site-packages\concurrent\futures\__init__.py"", line 8, in <module>
    from concurrent.futures._base import (FIRST_COMPLETED,
  File ""C:\Users\BioHelwan\Anaconda3\envs\tensorflow\Lib\site-packages\concurrent\futures\_base.py"", line 414
    raise exception_type, self._exception, self._traceback
                        ^
SyntaxError: invalid syntax



Note: any instruction related to anaconda gives me an  error also i can't remove anaconda to start setup from beginning to get rid of from this problem

i'm using 
windows10 (64)  - Anaconda 4.2 and python 3.5.2
i installed tensor flow gpu version from instructions in this link https://www.tensorflow.org/install/install_windows 
tensorflow version=1.9.0
cuda versions installed 8.0 /9.0
GPU : NVIDIA GeForce GTX 960M   specs link: https://www.geforce.com/hardware/notebook-gpus/geforce-gtx-960m/specifications

Exact command to reproduce : activate tensorflow (also any code related to anaconda i can't use ""conda"" or ""pip"" instructions in my command prompt because they give errors too)"
21193,please how i can convert string input to numbers,"hi ..
please give me a wey to convert a string input like user information ( contry, name ...) to use it in the functions
thank you :)
"
21192,Unimplemented cast int64 to string is not supported,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04.4
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.5.2
- Bazel version: N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A
- Exact command to reproduce: described below
- Mobile device: N/A

### Describe the problem
After following the steps in Using SavedModel with Estimators: When sending a classification request from client, the server is not able to cast it into serialized tf example that model is expecting.

The same error comes up even when I used ```
tf.contrib.predictor.from_saved_model```

So far, I have checked StackOverflow, Tf documentation, issues on Tf and Tf/serving. No one has ever encountered this error. Hence reporting here. This [issue {#1017}](https://github.com/tensorflow/serving/issues/1017) was rejected by @chrisolston in Tf/serving because the error trace indicates it is an issue in one of the core modules on main Tf repo and it had looked like there was no serialization of tf example proto in the client side code: But that task is already accomplished by [prediction_service_pb2.py](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/prediction_service_pb2.py) file. 

There are no other pointers online to what is going wrong here, please help!

### Source code / error logs
TensorFlow model server **error** trace:
```
2018-07-27 14:54:49.685755: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1295] OP_REQUIRES failed at cast_op.cc:77 : Unimplemented: Cast int64 to string is not supported
2018-07-27 14:54:49.685822: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:696] Executor failed to create kernel. Unimplemented: Cast int64 to string is not supported
         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[2]], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ArgMax)]]
```

**Error** on running client side code:
```
Traceback (most recent call last):
  File ""/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py"", line 193, in _blocking_unary_unary
    credentials=_credentials(protocol_options))
  File ""/home/mldev/venv/lib/python3.5/site-packages/grpc/_channel.py"", line 500, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File ""/home/mldev/venv/lib/python3.5/site-packages/grpc/_channel.py"", line 434, in _end_unary_response_blocking
    raise _Rendezvous(state, None, None, deadline)
grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.UNIMPLEMENTED, Cast int64 to string is notsupported
         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[2]], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ArgMax)]])>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/mldev/intelligent_sp/train_scripts/client.py"", line 63, in <module>
    run(args.host, args.port, args.input, args.model, args.signature_name)
  File ""/home/mldev/intelligent_sp/train_scripts/client.py"", line 42, in run
    result = stub.Classify(request, 10.0)
  File ""/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py"", line 309, in __call__
    self._request_serializer, self._response_deserializer)
  File ""/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py"", line 195, in _blocking_unary_unary
    raise _abortion_error(rpc_error_call)
grpc.framework.interfaces.face.face.LocalError: LocalError(code=StatusCode.UNIMPLEMENTED, details=""Cast int64 to string isnot supported
         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[2]], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ArgMax)]]"")
```

**Relevant Client Code**:
```
    channel = implementations.insecure_channel(host, port)
    stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)

    # Pre-processing
    prediction_input = [json.dumps(eval(input_str))]
    
    ink, classname = creat.parse_line(prediction_input[0])

    classnames = ['doodle', 'expression', 'symbols']
    features = {}
    features[""class_index""] = tf.train.Feature(int64_list=tf.train.Int64List(value=[classnames.index(""doodle"")]))
    features[""ink""] = tf.train.Feature(float_list=tf.train.FloatList(value=ink.flatten()))
    features[""shape""] = tf.train.Feature(int64_list=tf.train.Int64List(value=ink.shape))
    f = tf.train.Features(feature=features)
    example = tf.train.Example(features=f)
    final_req = [example]
    start = time.time()

    # Call classification model to make prediction
    request = classification_pb2.ClassificationRequest()
    request.model_spec.name = model
    request.model_spec.signature_name = signature_name
    request.input.example_list.examples.extend(final_req)
    
    result = stub.Classify(request, 10.0)
```

Imports used in Client Code:
```
from tensorflow_serving.apis import classification_pb2
from tensorflow_serving.apis import prediction_service_pb2
from tensorflow_serving.apis import input_pb2 as final_inp
```

Code used to **export saved model** (runs successfully):
```
      feature_spec = {
      ""ink"": tf.VarLenFeature(dtype=tf.float32),
      ""shape"": tf.FixedLenFeature([2], dtype=tf.int64),
      ""class_index"": tf.FixedLenFeature([1], dtype=tf.int64)
      }
      
      # defining serving input receiver function
      def serving_input_receiver_fn():
        """"""An input receiver that expects a serialized tf.Example.""""""
        serialized_tf_example = tf.placeholder(dtype=tf.string, shape=[None], name='input_example_tensor')
        receiver_tensors = {'examples': serialized_tf_example}
        parsed_features = tf.parse_example(serialized_tf_example, feature_spec)
        parsed_features[""ink""] = tf.sparse_tensor_to_dense(parsed_features[""ink""])
        return tf.estimator.export.ServingInputReceiver(parsed_features, receiver_tensors)

      # export saved model
      estimator.export_savedmodel(FLAGS.model_dir+""/serve/"", serving_input_receiver_fn, strip_default_attrs=True)
      print(""done exporting"")
```

The output from **saved_model_cli** (used to inspect saved model) looks like this:
```
MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['output']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['inputs'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: input_example_tensor:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['classes'] tensor_info:
        dtype: DT_STRING
        shape: (2)
        name: Cast:0
    outputs['scores'] tensor_info:
        dtype: DT_FLOAT
        shape: (2, 348)
        name: dense/BiasAdd:0
  Method name is: tensorflow/serving/classify

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['inputs'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: input_example_tensor:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['classes'] tensor_info:
        dtype: DT_STRING
        shape: (2)
        name: Cast:0
    outputs['scores'] tensor_info:
        dtype: DT_FLOAT
        shape: (2, 348)
        name: dense/BiasAdd:0
  Method name is: tensorflow/serving/classify
```

"
21190,[Bug] tf.map_fn cannot work with tf.histogram_fixed_width when using float,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: Quardo M4000
- **Exact command to reproduce**: N/A

### Describe the problem
I have tested the tf.map_fn to produce a list of histograms for a tensor. My example codes are as below.
My findings: 

1. When running tf.histogram_fixed_width for single floating tensor, there is no problem. (see test0() below)
2. When running tf.map_fn together with tf.histogram_fixed_width for integer tensor, there is no problem (see test2() below)
3. When running tf.map_fn together with tf.histogram_fixed_width for floating tensor, the program crashed (see test1() below)

### Source code / logs
	import tensorflow as tf
	import numpy as np
	def test0():
	  V = tf.placeholder(tf.float32, [None, 3])
	  hists = tf.histogram_fixed_width(V[0], [0.0,10.0], 10)
	  A = [[1.0,2.0,3.0], [4.0,5.0,6.0]]
	  with tf.Session() as sess:
		print(sess.run(hists, {V:A}))
		
	def test1():
	  V = tf.placeholder(tf.float32, [None, 3])
	  hists = tf.map_fn(
		lambda x: tf.histogram_fixed_width(x, [0.0,10.0], 10), V)
	  A = [[1.0,2.0,3.0], [4.0,5.0,6.0]]
	  with tf.Session() as sess:
		print(sess.run(hists, {V:A}))

	def test2():
	  V = tf.placeholder(tf.int32, [None, 3])
	  hists = hists = tf.map_fn(
		lambda x: tf.histogram_fixed_width(x, [0,10], 10), V)
	  A = [[1,2,3], [4,5,6]]
	  with tf.Session() as sess:
		print(sess.run(hists, {V:A}))


test0() and test2() work fine. test1() produce the following error. Therefore, I think this may be a bug in tensorflow.

	Traceback (most recent call last):
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1322, in _do_call
		return fn(*args)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1307, in _run_fn
		options, feed_dict, fetch_list, target_list, run_metadata)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1409, in _call_tf_sessionrun
		run_metadata)
	tensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray dtype is float but Op is trying to write dtype int32.
			 [[Node: map/while/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](map/while/TensorArrayWrite/TensorArrayWriteV3/Enter, map/while/Switch_1/_31, map/while/histogram_fixed_width, map/while/Switch_2/_33)]]
			 [[Node: map/while/TensorArrayWrite/TensorArrayWriteV3/_35 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_66_map/while/TensorArrayWrite/TensorArrayWriteV3"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](^_cloopmap/while/NextIteration_2/_6)]]

	During handling of the above exception, another exception occurred:

	Traceback (most recent call last):
	  File ""train_model.py"", line 196, in <module>
		main()
	  File ""train_model.py"", line 193, in main
		test1()
	  File ""train_model.py"", line 166, in test1
		print(sess.run(hists, {V:A}))
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 900, in run
		run_metadata_ptr)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1135, in _run
		feed_dict_tensor, options, run_metadata)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1316, in _do_run
		run_metadata)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1335, in _do_call
		raise type(e)(node_def, op, message)
	tensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray dtype is float but Op is trying to write dtype int32.
			 [[Node: map/while/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](map/while/TensorArrayWrite/TensorArrayWriteV3/Enter, map/while/Switch_1/_31, map/while/histogram_fixed_width, map/while/Switch_2/_33)]]
			 [[Node: map/while/TensorArrayWrite/TensorArrayWriteV3/_35 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_66_map/while/TensorArrayWrite/TensorArrayWriteV3"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](^_cloopmap/while/NextIteration_2/_6)]]

	Caused by op 'map/while/TensorArrayWrite/TensorArrayWriteV3', defined at:
	  File ""train_model.py"", line 196, in <module>
		main()
	  File ""train_model.py"", line 193, in main
		test1()
	  File ""train_model.py"", line 163, in test1
		lambda x: tf.histogram_fixed_width(x, [0.0,10.0], 10), V)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\functional_ops.py"", line 459, in map_fn
		maximum_iterations=n)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 3209, in while_loop
		result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2941, in BuildLoop
		pred, body, original_loop_vars, loop_vars, shape_invariants)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2878, in _BuildLoop
		body_result = body(*packed_vars_for_body)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 3179, in <lambda>
		body = lambda i, lv: (i + 1, orig_body(*lv))
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\functional_ops.py"", line 451, in compute
		tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\functional_ops.py"", line 451, in <listcomp>
		tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\util\tf_should_use.py"", line 118, in wrapped
		return _add_should_use_warning(fn(*args, **kwargs))
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\tensor_array_ops.py"", line 842, in write
		return self._implementation.write(index, value, name=name)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\util\tf_should_use.py"", line 118, in wrapped
		return _add_should_use_warning(fn(*args, **kwargs))
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\tensor_array_ops.py"", line 276, in write
		name=name)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\ops\gen_data_flow_ops.py"", line 7870, in tensor_array_write_v3
		flow_in=flow_in, name=name)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
		op_def=op_def)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 3414, in create_op
		op_def=op_def)
	  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1740, in __init__
		self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

	InvalidArgumentError (see above for traceback): TensorArray dtype is float but Op is trying to write dtype int32.
			 [[Node: map/while/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](map/while/TensorArrayWrite/TensorArrayWriteV3/Enter, map/while/Switch_1/_31, map/while/histogram_fixed_width, map/while/Switch_2/_33)]]
			 [[Node: map/while/TensorArrayWrite/TensorArrayWriteV3/_35 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_66_map/while/TensorArrayWrite/TensorArrayWriteV3"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](^_cloopmap/while/NextIteration_2/_6)]]

"
21189,"How to know the right values of X and Y for strip_unused_nodes(type=X, shape=""y0,y1,y3,3"") to use in the transofrm_graph tool?",
21188,TypeError: Cannot interpret feed_dict key as Tensor: Can not convert a NoneType into a Tensor.,"Error:
/usr/lib64/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-07-27 08:35:51.967246: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: FMA
Traceback (most recent call last):
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1075, in _run
    subfeed, allow_tensor=True, allow_operation=False)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3590, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3679, in _as_graph_element_locked
    types_str))
TypeError: Can not convert a NoneType into a Tensor.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""--------------.py"", line 397, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""---------------------.py"", line 88, in main
    print(prediction(e))
  File ""---------------------.py"", line 361, in prediction
    model_prediction = sess.run(pred_label, feed_dict={x: subject_npy, y: y_const_npy, keep_prob: 1.0})
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1078, in _run
    'Cannot interpret feed_dict key as Tensor: ' + e.args[0])
TypeError: Cannot interpret feed_dict key as Tensor: Can not convert a NoneType into a Tensor.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

I've tried what some stackoverflow posts recommended, e.g. keeping the feed_dict placeholder names as x and y, when either saving & restoring the graph or just making new placeholders.

My objective: I'm trying to restore and run an lstm.

My code:
""""""setup for training-------(no issue with training)""""""
  out_weights = tf.Variable(tf.random_normal([num_units,n_classes]), name = ""out_weights"")
  out_bias = tf.Variable(tf.random_normal([n_classes]), name = ""out_bias"")
  x = tf.placeholder(""float"",[None,time_steps,n_input])
  y = tf.placeholder(""float"",[None,n_classes])
  keep_prob = tf.placeholder(""float"", shape=())
input = tf.unstack(x ,time_steps,1, name = ""inputs"")
lstm_layer = rnn.BasicLSTMCell(num_units,forget_bias=1)
outputs, _ = rnn.static_rnn(lstm_layer,input,dtype=""float32"")
  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))
  tf.summary.scalar('cross_entrophy', loss)
  opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)
  pred_label = tf.argmax(prediction,1)
  actual_label = tf.argmax(y,1)
  correct_prediction = tf.equal(pred_label, actual_label)
  accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    saver = tf.train.Saver()
    for i in range(N):
        sess.run(, feed_dict = {x: batch[0], y: batch[1], keep_prob: 0.5})
        if i%100==0:
                    saver.save(sess, ""path/model.ckpt"")
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
""""""LOADING  lstm""""""
  with tf.Session() as sess:
    from_saver = tf.train.import_meta_graph('path/model.ckpt.meta')
    from_saver.restore(sess, tf.train.latest_checkpoint('path_to_summaries_dir'))
   lstm_graph = tf.get_default_graph()
    out_weights = lstm_graph.get_tensor_by_name(""out_weights:0"")
    out_bias = lstm_graph.get_tensor_by_name(""out_bias:0"")
    x = tf.placeholder(""float"",[time_steps,n_input])
    y = tf.placeholder(""float"",[n_classes])
    keep_prob = keep_prob = tf.placeholder(""float"", shape=())
    inputs = lstm_graph.get_tensor_by_name(""inputs:0"")
    prediction = lstm_graph.get_tensor_by_name(""prediction:0"") + out_bias
    pred_label = tf.argmax(prediction,1)

""""""RUNNING MODEL""""""
  with tf.Session(graph=lstm_graph) as sess:
    sess.run(tf.global_variables_initializer())
    model_prediction = sess.run(pred_label, feed_dict={x: subject_npy, y: y_const_npy, keep_prob: 1.0})

------------------------


Have I written custom code:   yes
OS Platform and Distribution:    centos 6
TensorFlow installed from:    pip3
TensorFlow version:    v1.9
Bazel version:    N/A
CUDA/cuDNN version:    N/A
GPU model and memory:    N/A
Exact command to reproduce:    N/A
Mobile device:    N/A"
21185,tf.shape output is wrong when net input shape is changed during import,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.8.0 **(still present in 2.6.0, updating the code accordingly)**
- **Python version**: 3.6.6
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Bazel version**: N/A
- **Mobile device**: N/A
- **Exact command to reproduce**: see below

### Describe the problem

`tf.shape` returns an inconsistent result when a network is imported from file and its input is changed during the import. Let me create a simple net with a `batch_size` of 128, and save it to disk

    import tensorflow.compat.v1 as tf
    tf.disable_v2_behavior()

    batch_size = 128
    x = tf.placeholder(tf.float32, shape=(batch_size, 10), name='x')
    b = tf.Variable(tf.zeros((10)))
    y = tf.add(x, b, name='y')

    saver = tf.train.Saver()
    with tf.Session() as sess:
      tf.global_variables_initializer().run()
      saver.save(sess, './foo')

Later, I reload this model, and replace the input placeholder with a more flexible one, with an undefined `batch_size`. 

    import numpy as np
    import tensorflow.compat.v1 as tf
    tf.disable_v2_behavior()

    x = tf.placeholder(tf.float32, shape=(None, 10))
    restorer = tf.train.import_meta_graph('./foo.meta', input_map={'x:0': x})
    y = tf.get_default_graph().get_tensor_by_name('y:0')
    y_shape = tf.shape(y)
    sess = tf.Session()
    restorer.restore(sess, './foo')
    [y_, y_shape_] = sess.run(['y:0', y_shape], {x: np.zeros((1, 10), np.float32)})
    assert np.all(y_.shape == y_shape_), 'inconsistent sizes'

This results in an `AssertionError: inconsistent sizes`, because `y_shape_` still returns the old batch size of 128, despite the output `y` being computed as expected with a batch size of 1."
21184,ValueError: No op named QuantizedAdd in defined operations.,"python version:2.7.5 tensorflow version:1.2.0 bazel version:

i want to use tensorflow bazel to quantize my trained model,the quantize code is follow:

`bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
  --in_graph=/home/oup/zzl/zzl/AdversarialJPEG/JPEGFinalXu/Discriminative145000.pb \
  --out_graph=/home/oup/zzl/zzl/AdversarialJPEG/JPEGFinalXu/quantized8_Discriminative145000.pb \
  --outputs=Group_22/y_ \
   --transforms='
  strip_unused_nodes(type=float, shape=""1,253,253,1"")
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms
  quantize_nodes
  quantize_weights(minimum_size=0)'`

but when i try to get my trained model to test it accuracy, i get the error:

`tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0)
2018-07-27 14:01:15.643854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0)
2018-07-27 14:01:15.643863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P100-PCIE-16GB, pci bus id: 0000:8a:00.0)
Traceback (most recent call last):
  File ""testpb2.py"", line 82, in <module>
    _ = tf.import_graph_def(graph_def,name="""")
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 283, in import_graph_def
    raise ValueError('No op named %s in defined operations.' % node.op)
ValueError: No op named QuantizedAdd in defined operations.`

and the code snippet to lead the error:

`with tf.Session(config=config) as sess:
    #load the quantity model
     with open(path+'quantized64_noDCT_retrained5000_Pruned_Discriminative145000.pb', 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        **_ = tf.import_graph_def(graph_def,name="""")**
     y_ = sess.graph.get_tensor_by_name('Group_22/y_:0')
     correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

     init = tf.global_variables_initializer()
     sess.run(init)`


i don't konw why i delete the params 'quantize_nodes' in the quantize code it work normal,but add 'quantize_nodes' will raise the error above,is't my tensorflow version is too low lead this error?i have google that error but i can't found any solution,please give me some suggestion for that error,thank you very much!"
21182,if you install tensorflow v1.9 through wheel，you should install tensorboard v1.9.0 but not v1.10.0,"as title i typed, is this a bug of tf1.9?"
21181,Lambda Layer issue when receiving a list of DeferredTensor with a TensorShape shape,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
MacOS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
v1.9.0-0-g25c197e023 1.9.0
- **Python version**:
Python 3.6.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:
See below.

### Describe the problem
Following [this issue](https://github.com/tensorflow/tensorflow/issues/20338), I used the work-around and wrapped the result of `compute_output_shape` with tf.TensorShape.
However when using a Lambda layer with multiple inputs, the shape isn't handled properly and throws the exception:

`TypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorShape'`

### Source code / logs
Here is a basic set-up to reproduce it:

    import tensorflow as tf

    class MyLayer(tf.layers.Layer):

        def __init__(self, **kwargs):
            super(MyLayer, self).__init__(**kwargs)

        def call(self, x):
            return x

        def compute_output_shape(self, input_shape):
            return tf.TensorShape(input_shape[0])

    tf.enable_eager_execution()

    a = tf.keras.layers.Input(shape=[1])
    b = tf.keras.layers.Input(shape=[1])
    layer_1 = MyLayer()(a)
    layer_2 = MyLayer()(b)

    layer_3 = tf.keras.layers.Lambda(lambda x: x[0])([layer_1, layer_2])


And the stack-trace:

```

Traceback (most recent call last):
  File ""/Users/jnd/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-C/ch-0/181.5087.37/PyCharm CE.app/Contents/helpers/pydev/pydevd.py"", line 1664, in <module>
    main()
  File ""/Users/jnd/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-C/ch-0/181.5087.37/PyCharm CE.app/Contents/helpers/pydev/pydevd.py"", line 1658, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/Users/jnd/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-C/ch-0/181.5087.37/PyCharm CE.app/Contents/helpers/pydev/pydevd.py"", line 1068, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/Users/jnd/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-C/ch-0/181.5087.37/PyCharm CE.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/Users/jnd/dev/mask_rcnn/bug_replication.py"", line 23, in <module>
    layer_3 = tf.keras.layers.Lambda(lambda x: x[0])([layer_1, layer_2])
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 714, in __call__
    output_shapes = self.compute_output_shape(input_shapes)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py"", line 677, in compute_output_shape
    input_shape = tuple(tensor_shape.TensorShape(input_shape).as_list())
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 541, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 541, in <listcomp>
    self._dims = [as_dimension(d) for d in dims_iter]
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 482, in as_dimension
    return Dimension(value)
  File ""/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py"", line 37, in __init__
    self._value = int(value)
TypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorShape'

```"
21180,(MirroredStrategy) AttributeError: 'NoneType' object has no attribute 'merge_call',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.11.1
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/7.1
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See below.

### Describe the problem
I have several GPUs that I'd like to train a model with. I'm using `tf.estimator.train_and_evaluate` because I'd like to do early stopping (checking loss on a validation set every now and then during training, and stopping the training loop when the model stops improving on the validation data). I'm currently doing this with `tf.contrib.estimator.stop_if_no_decrease_hook`. This crashes when the evaluation starts (because `tf.metrics` cannot run distributed yet?).

This is my current workaround for local training:
```python
best = float('inf')
for epoch in range(100):
    estimator.train(training_input_fn)
    metrics = estimator.evaluate(validation_input_fn)
    if metrics['loss'] < best:
        best = metrics['loss']
    else:
        tf.logging.info(f'Early stopping after {epoch} epochs.')
        break
```

### Source code / logs
This seems to reproduce the error consistently:

```python
import tensorflow as tf


def input_fn():
    dataset = (
        tf.data.Dataset
        .range(100)
        .map(lambda x: tf.random_normal([100]))
        .batch(32)
        .map(lambda x: ({'input': x}, {'output': x}))
        .repeat()
    )
    return dataset


def model_fn(features, labels, mode, params):

    units = features['input'].shape[-1]
    predictions = tf.layers.dense(features['input'], units)
    labels = labels['output']

    loss = tf.losses.mean_squared_error(labels, predictions)
    tf.losses.add_loss(loss)
    loss = tf.losses.get_total_loss()

    if mode == tf.estimator.ModeKeys.EVAL:
        metrics = {'mse': tf.metrics.mean_squared_error(labels, predictions)}

        return tf.estimator.EstimatorSpec(
            mode,
            loss=loss,
            eval_metric_ops=metrics,
        )

    if mode == tf.estimator.ModeKeys.TRAIN:
        step = tf.train.get_or_create_global_step()
        optimizer = tf.train.AdamOptimizer()
        train_op = optimizer.minimize(loss, step)

        return tf.estimator.EstimatorSpec(
            mode,
            loss=loss,
            train_op=train_op,
        )


estimator = tf.estimator.Estimator(
    model_fn,
    config=tf.estimator.RunConfig(
        save_checkpoints_steps=100,
        train_distribute=tf.contrib.distribute.MirroredStrategy()),
)

tf.estimator.train_and_evaluate(
    estimator,
    train_spec=tf.estimator.TrainSpec(input_fn),
    eval_spec=tf.estimator.EvalSpec(input_fn),
)
```

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-2-b27091873f02> in <module>()
     56     estimator,
     57     train_spec=tf.estimator.TrainSpec(input_fn),
---> 58     eval_spec=tf.estimator.EvalSpec(input_fn),
     59 )

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)
    449         '(with task id 0).  Given task id {}'.format(config.task_id))
    450 
--> 451   return executor.run()
    452 
    453 

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in run(self)
    588         config.task_type != run_config_lib.TaskType.EVALUATOR):
    589       logging.info('Running training and evaluation locally (non-distributed).')
--> 590       return self.run_local()
    591 
    592     # Distributed case.

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in run_local(self)
    689         max_steps=self._train_spec.max_steps,
    690         hooks=train_hooks,
--> 691         saving_listeners=saving_listeners)
    692 
    693     eval_result = listener_for_eval.eval_result or _EvalResult(

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    374 
    375       saving_listeners = _check_listeners_type(saving_listeners)
--> 376       loss = self._train_model(input_fn, hooks, saving_listeners)
    377       logging.info('Loss for final step: %s.', loss)
    378       return self

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1141   def _train_model(self, input_fn, hooks, saving_listeners):
   1142     if self._distribution:
-> 1143       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1144     else:
   1145       return self._train_model_default(input_fn, hooks, saving_listeners)

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)
   1366         return self._train_with_estimator_spec(estimator_spec, worker_hooks,
   1367                                                hooks, global_step_tensor,
-> 1368                                                saving_listeners)
   1369 
   1370   def _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks,

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)
   1449       loss = None
   1450       while not mon_sess.should_stop():
-> 1451         _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
   1452     return loss
   1453 

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)
    581                           feed_dict=feed_dict,
    582                           options=options,
--> 583                           run_metadata=run_metadata)
    584 
    585   def run_step_fn(self, step_fn):

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)
   1057                               feed_dict=feed_dict,
   1058                               options=options,
-> 1059                               run_metadata=run_metadata)
   1060       except _PREEMPTION_ERRORS as e:
   1061         logging.info('An error was raised. This may be due to a preemption in '

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)
   1148         raise six.reraise(*original_exc_info)
   1149       else:
-> 1150         raise six.reraise(*original_exc_info)
   1151 
   1152 

~/.miniconda3/lib/python3.6/site-packages/six.py in reraise(tp, value, tb)
    691             if value.__traceback__ is not tb:
    692                 raise value.with_traceback(tb)
--> 693             raise value
    694         finally:
    695             value = None

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)
   1133   def run(self, *args, **kwargs):
   1134     try:
-> 1135       return self._sess.run(*args, **kwargs)
   1136     except _PREEMPTION_ERRORS:
   1137       raise

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)
   1213               results=outputs[hook] if hook in outputs else None,
   1214               options=options,
-> 1215               run_metadata=run_metadata))
   1216     self._should_stop = self._should_stop or run_context.stop_requested
   1217 

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py in after_run(self, run_context, run_values)
    462       if self._timer.should_trigger_for_step(global_step):
    463         self._timer.update_last_triggered_step(global_step)
--> 464         if self._save(run_context.session, global_step):
    465           run_context.request_stop()
    466 

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py in _save(self, session, step)
    487     should_stop = False
    488     for l in self._listeners:
--> 489       if l.after_save(session, step):
    490         logging.info(
    491             ""A CheckpointSaverListener requested that training be stopped. ""

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in after_save(***failed resolving arguments***)
    495       return True
    496     if self._timer.should_trigger_for_step(global_step_value):
--> 497       self._evaluate(global_step_value)  # updates self.eval_result
    498       if not self._continuous_eval_listener.after_eval(self.eval_result):
    499         logging.info('Exiting evaluation, as requested by '

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in _evaluate(self, global_step_value)
    515     self._timer.update_last_triggered_step(global_step_value)
    516     self.eval_result, self.export_results = (
--> 517         self._evaluator.evaluate_and_export())
    518     if self.eval_result.status != _EvalStatus.EVALUATED:
    519       #  This is unexpected; should never happen.

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in evaluate_and_export(self)
    882           name=self._eval_spec.name,
    883           checkpoint_path=latest_ckpt_path,
--> 884           hooks=self._eval_spec.hooks)
    885 
    886       # _EvalResult validates the metrics.

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in evaluate(self, input_fn, steps, hooks, checkpoint_path, name)
    461         (scaffold, update_op,
    462          eval_dict, all_hooks) = self._evaluate_build_graph(
--> 463              input_fn, hooks, checkpoint_path)
    464         return self._evaluate_run(
    465             checkpoint_path=checkpoint_path,

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _evaluate_build_graph(self, input_fn, hooks, checkpoint_path)
   1461                                                     model_fn_lib.ModeKeys.EVAL))
   1462     estimator_spec = self._call_model_fn(
-> 1463         features, labels, model_fn_lib.ModeKeys.EVAL, self.config)
   1464 
   1465     # Call to warm_start has to be after model_fn is called.

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
   1131 
   1132     logging.info('Calling model_fn.')
-> 1133     model_fn_results = self._model_fn(features=features, **kwargs)
   1134     logging.info('Done calling model_fn.')
   1135 

<ipython-input-2-b27091873f02> in model_fn(features, labels, mode, params)
     26 
     27     if mode == tf.estimator.ModeKeys.EVAL:
---> 28         metrics = {'mse': tf.metrics.mean_squared_error(labels, predictions)}
     29 
     30         return tf.estimator.EstimatorSpec(

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py in mean_squared_error(labels, predictions, weights, metrics_collections, updates_collections, name)
   1297   squared_error = math_ops.square(labels - predictions)
   1298   return mean(squared_error, weights, metrics_collections, updates_collections,
-> 1299               name or 'mean_squared_error')
   1300 
   1301 

~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py in mean(values, weights, metrics_collections, updates_collections, name)
    374       return mean_t
    375 
--> 376     mean_t = distribute_lib.get_tower_context().merge_call(
    377         aggregate_across_towers, total, count)
    378     update_op = _safe_div(update_total_op, update_count_op, 'update_op')

AttributeError: 'NoneType' object has no attribute 'merge_call'
```
"
21178,Support placeholders without shape in build_raw_serving_input_receiver_fn,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, a small demo.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian-based Linux distribution.
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0
- **Python version**: 3.5.3
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A


### Describe the problem
```tf.estimator.export.build_raw_serving_input_receiver_fn``` crashes on ```tf.placeholder```s that do not have a shape specified.

### Source code / logs
Below is a minimal demo that causes the crash:
```
# Attempts to learn x+y.

import tensorflow as tf

def input_fn():
  x = tf.random_uniform(shape=[1])
  y = tf.random_uniform(shape=[1])
  return {'x': x, 'y': y}, tf.add(x, y)


def main(_):
  estimator = tf.estimator.LinearRegressor([
      tf.contrib.layers.real_valued_column('x'),
      tf.contrib.layers.real_valued_column('y'),
  ])

  train_spec = tf.estimator.TrainSpec(input_fn)

  phs = { 'x': tf.placeholder(tf.float32), 'y': tf.placeholder(tf.float32) }
  #receiver_fn = lambda: tf.estimator.export.ServingInputReceiver(phs, phs)
  receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(phs)

  eval_spec = tf.estimator.EvalSpec(
      input_fn,
      exporters=[tf.estimator.LatestExporter('latest', receiver_fn)]
  )

  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)


if __name__ == ""__main__"":
  tf.app.run()
```

Attempting to execute this code produces the following error and stacktrace (after 10mins, when model export is attempted):
```
  File ""bug_demo.py"", line 26, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
  File ""tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/training.py"", line 447, in train_and_evaluate
    return executor.run()
  File ""tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/training.py"", line 531, in run
    return self.run_local()
  File ""tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/training.py"", line 681, in run_local
    eval_result, export_results = evaluator.evaluate_and_export()
  File ""tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/training.py"", line 898, in evaluate_and_export
    is_the_final_export)
  File ""tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/training.py"", line 931, in _export_eval_result
    is_the_final_export=is_the_final_export))
  File ""tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/exporter.py"", line 472, in export
    is_the_final_export)
  File ""tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/exporter.py"", line 126, in export
    strip_default_attrs=self._strip_default_attrs)
  File ""tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 650, in export_savedmodel
    mode=model_fn_lib.ModeKeys.PREDICT)
  File ""tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 703, in _export_saved_model_for_mode
    strip_default_attrs=strip_default_attrs)
  File ""tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 811, in _export_all_saved_models
    mode=model_fn_lib.ModeKeys.PREDICT)
  File ""tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py"", line 872, in _add_meta_graph_for_mode
    input_receiver = input_receiver_fn()
  File ""tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/export/export.py"", line 335, in serving_input_receiver_fn
    features, default_batch_size)
  File ""tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/export/export.py"", line 312, in _placeholders_from_receiver_tensors_dict
    for name, t in input_vals.items()
  File ""tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/export/export.py"", line 312, in <dictcomp>
    for name, t in input_vals.items()
  File ""tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/export/export.py"", line 298, in _placeholder_from_tensor
    shape_list = t.get_shape().as_list()
  File ""tf_public/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 903, in as_list
    raise ValueError(""as_list() is not defined on an unknown TensorShape."")
```

Replacing ```receiver_fn``` as per the commented-out line or adding shape parameters to ```tf.placeholder``` prevents this crash. "
21175,Unable to convert keras_model.h5 to .tflite,"I enter this command : tflite_convert \ --output_file=/tmp/foo.tflite \ --keras_model_file=C:/Users/n.muthuraj/Desktop/cnn/02/dataset/ashik.h5

The Output is : Traceback (most recent call last):

 File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper return importlib.import_module(mname) File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\importlib_init_.py"", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File """", line 986, in _gcd_import File """", line 969, in _find_and_load File """", line 958, in _find_and_load_unlocked File """", line 666, in _load_unlocked File """", line 577, in module_from_spec File """", line 906, in create_module File """", line 222, in _call_with_frames_removed ImportError: DLL load failed: The specified module could not be found. During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in from tensorflow.python.pywrap_tensorflow_internal import * File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in _pywrap_tensorflow_internal = swig_import_helper() File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper return importlib.import_module('pywrap_tensorflow_internal') File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\importlib_init.py"", line 126, in import_module return _bootstrap.gcd_import(name[level:], package, level) ImportError: No module named 'pywrap_tensorflow_internal' During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\runpy.py"", line 184, in run_module_as_main ""main"", mod_spec) File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\runpy.py"", line 85, in run_code exec(code, run_globals) File ""C:\Users\n.muthuraj\AppData\Local\Programs\Python\Python35\Scripts\tflite_convert.exe_main.py"", line 5, in File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\site-packages\tensorflow_init.py"", line 22, in from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python_init.py"", line 49, in from tensorflow.python import pywrap_tensorflow File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in raise ImportError(msg) ImportError: Traceback (most recent call last): File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper return importlib.import_module(mname) File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\importlib_init.py"", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File """", line 986, in _gcd_import File """", line 969, in _find_and_load File """", line 958, in _find_and_load_unlocked File """", line 666, in _load_unlocked File """", line 577, in module_from_spec File """", line 906, in create_module File """", line 222, in _call_with_frames_removed ImportError: DLL load failed: The specified module could not be found. During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in from tensorflow.python.pywrap_tensorflow_internal import * File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in _pywrap_tensorflow_internal = swig_import_helper() File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper return importlib.import_module('pywrap_tensorflow_internal') File ""c:\users\n.muthuraj\appdata\local\programs\python\python35\lib\importlib_init.py"", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) ImportError: No module named '_pywrap_tensorflow_internal' Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/install_sources#common_installation_problems for some common reasons and solutions. Include the entire stack trace above this error message when asking for help.

I am not sure where i am going wrong"
21174,"Allow Keras Callbacks to access predictions on_batch_end, on_epoch_end","### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.9.0-rc0-35-g17d6639b55 1.9.0-rc1
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

### Describe the problem

I wanted to call attention to an issue and a PR on the keras-team/keras project so that TF could track and integrate with TF-core Keras. I believe these issues are of high importance to the community; for my project at least they are just short of critical. In summary:

* PR: Keras Callbacks should be able to access predictions made during training in `on_batch_end()` and `on_epoch_end()` to eliminate expensive duplicate calls to `predict()` in Callbacks that require prediction results -- https://github.com/keras-team/keras/pull/10513
* Writing a Keras metric should not require a graph -- https://github.com/keras-team/keras/issues/4506#issuecomment-405452870

The reason these are related is because a viable solution to the second issue depends on the first; in particular, if you wanted to write a ""fancy"" metric function that did calcs in Python, for example, you could leverage already-made predictions from the API described in issue 1 and circumvent having to `predict()` all over again, which is prohibitively expensive in all but the simplest models."
21173,tf.nn.top_k returns wrong indices,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes?
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9.0 / 1.8.0 / 1.5.0
- **Python version**: 2.7 / 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN 7.0.5
- **GPU model and memory**: GeForce GTX 1050Ti 4GB (Laptop)
- **Exact command to reproduce**: [tf.nn.top_k](https://www.tensorflow.org/api_docs/python/tf/nn/top_k)

### Describe the problem
The [tf.nn.top_k](https://www.tensorflow.org/api_docs/python/tf/nn/top_k) command return wrong ranked indices. 
E.g.
if I run the following commands
```
import tensorflow as tf

a = tf.constant([[5,6,4],[0.1,2,3]])
v, r = tf.nn.top_k(a, 3)

sess.run(a)
sess.run(r)
sess.run(v)
```
the results are as expected 

```
>>> sess.run(a)
array([[5. , 6. , 4. ],
       [0.1, 2. , 3. ]], dtype=float32)
>>> sess.run(r)
array([[1, 0, 2],
       [2, 1, 0]], dtype=int32)
>>> sess.run(v)
array([[6. , 5. , 4. ],
       [3. , 2. , 0.1]], dtype=float32)
```
but if I try the following example
```
import tensorflow as tf

a = tf.constant([[-5,0.8,0.4],[0.5,0.3,2.6]])
v, r = tf.nn.top_k(a, 3)

sess.run(a)
sess.run(r)
sess.run(v)
```
the results of the indices tensor are totally wrong 
```
>>> sess.run(a)
array([[-5. ,  0.8,  0.4],
       [ 0.5,  0.3,  2.6]], dtype=float32)
>>> sess.run(r)
array([[1, 2, 0],
       [2, 0, 1]], dtype=int32)
>>> sess.run(v)
array([[ 0.8,  0.4, -5. ],
       [ 2.6,  0.5,  0.3]], dtype=float32)
```
Same thing happens for higher rank tensors and more complex ordering of the input values.
"
21172,request wrapper tutorial,"hi all,

it would be really great to have a tutorial on how to use the many awesome wrappers like dropoutwrapper, attentionwrapper, etc. "
21170,Converting the model to TFLite format getting “Check failed: is_rnn_state_array ”,"toco \--input_file=$TRAINING_DIR/retrained_graph.pb \
--input_format=TENSORFLOW_GRAPHDEF \
--output_format=TFLITE \
--output_file=/$TRAINING_DIR/${ARCHITECTURE}.tflite \
--inference_type=QUANTIZED_UINT8 \
--input_arrays=input \
--output_arrays=final_result \ 
--input_shapes=1,224,224,3 \inference_input_type=QUANTIZED_UNIT8 \
--mean_values=128 \
--std_values=128 \
--default_ranges_min=0 \
--quantize_weights=true \
--default_ranges_max=6 "
21169,tf.Estimator can't load weights at runtime,"This is not a StackOverflow question because it simply can't be done the way tf.Estimator is currently designed.  This is a design flaw.

Language: Anaconda Python 3.6
OS Platform and Distribution: Ubuntu 17.10
TensorFlow version: 1.8
Cuda compilation tools, release 9.0, V9.0.176 /
CUDNN 7.0.5
GPU model and memory: NVIDIA Titan V, Driver Version: 387.34, 12057MiB

I'm working on building a distributed TensorFlow model for semantic segmentation (pixel-based, not bounding-box-based) using the https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator example.  The code fragments below are based on that example. 

This model is for production, not research so as part of the input pipeline, I have to extract training images from production images and calculate training class weights on the fly.  However, the current design of tf.Estimator appears to make this impossible.

If the class weights are uploaded into a static variable and then I attempt to collate them in _resnet_model_fn to use in the loss function I get errors telling me that the class weights are in the wrong class.

Runtime class weights assembly:
```
class MyDataSet(object):
    _class_weights = None

    def __init__(self, data_dir):
        self.data_dir = data_dir
        if MyDataSet._class_weights is None:
            MyDataSet._class_weights = MyDataSet.map_weights(data_dir)

    @staticmethod
    def train_id_to_weight_map_func(label):
        return MyDataSet._class_weights[label]

    @staticmethod
    def map_weights(data_dir):
        weights = pickle.load(open(os.path.join(data_dir, 'weight.pkl'), 'rb'))
        class_weights = [0] * weights.size
        for i, w in enumerate(weights):
            class_weights[i] = w
        return tf.convert_to_tensor(class_weights, np.float32)

```
 
cyfar10_main.py:

```
def _resnet_model_fn(features, labels, mode, params):
    tower_labels = labels
    tower_weight = []
    for label in tower_labels:
        tower_weight.append(tf.map_fn(lambda x: MyDataSet.train_id_to_weight_map_func(x), label, np.float32)
```
Error:
```
ValueError: Tensor(""map/while/TensorArrayReadV3:0"", shape=(256, 512), dtype=int32) must be from the same graph as Tensor(""Const:0"", shape=(25,), dtype=float32)
print(label.graph)
<tensorflow.python.framework.ops.Graph object at 0x7f9a206630f0>
print(LinkNetDataSet._class_weights.graph)
<tensorflow.python.framework.ops.Graph object at 0x7f9a2a8b1cf8>`
```

If I next try to load the weights into input_fn to get them into the same graph, I am unable to do so because Estimator is only expecting two lists of Tensors from input_fn: features and labels.  And features is already taken up with the training image so there's no place to put the class weights."
21167,Rebuilding the docker devel container fails,"### System information
- **Have I written custom code**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.4 (Tensorflow is build inside the devel docker container executed by parameterized_docker_build.sh, which is based on Ubuntu 16.04)
- **Docker version**: 18.06.0-ce, build 0ffa825
- **TensorFlow installed from**: Docker devel container build (i.e. source) executed by  by parameterized_docker_build.sh.
- **TensorFlow version**: 1.9
- **Python version**: 3.5 (inside executed docker container) 
- **Bazel version (if compiling from source)**: 0.15.0 (inside executed docker container  by parameterized_docker_build.sh) 
- **GCC/Compiler version (if compiling from source)**: 5.4.0 (inside executed docker container by parameterized_docker_build.sh)
- **Exact command to reproduce**:

```
mkdir tensorflow && cd tensorflow
git clone https://github.com/tensorflow/tensorflow.git .
export TF_DOCKER_BUILD_IS_DEVEL=YES
export TF_DOCKER_BUILD_TYPE=CPU
export TF_DOCKER_BUILD_PYTHON_VERSION=PYTHON3
export TF_DOCKER_BUILD_DEVEL_BRANCH=r1.9
tensorflow/tools/docker/parameterized_docker_build.sh
```


### Describe the problem
The rebuild of the devel docker container fails after the compilation of tensorflow, when trying to install the whl file

### Source code / logs

```
Installing collected packages: setuptools, termcolor, absl-py, gast, protobuf, grpcio, werkzeug, markdown, tensorboard, astor, tensorflow
  Found existing installation: setuptools 40.0.0
    Uninstalling setuptools-40.0.0:
      Successfully uninstalled setuptools-40.0.0
  Running setup.py install for termcolor: started
    Running setup.py install for termcolor: finished with status 'error'
    Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-lv_39b1o/termcolor/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-gjq_zk43/install-record.txt --single-version-externally-managed --compile:
    usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]
       or: -c --help [cmd1 cmd2 ...]
       or: -c --help-commands
       or: -c cmd --help

    error: option --single-version-externally-managed not recognized

    ----------------------------------------
Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-lv_39b1o/termcolor/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-gjq_zk43/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-install-lv_39b1o/termcolor/
The command '/bin/sh -c tensorflow/tools/ci_build/builds/configured CPU     bazel build -c opt --copt=-mavx --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""         tensorflow/tools/pip_package:build_pip_package &&     bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip3 &&     pip3 --no-cache-dir install --upgrade /tmp/pip3/tensorflow-*.whl &&     rm -rf /tmp/pip3 &&     rm -rf /root/.cache' returned a non-zero code: 1
```

### Probable cause

The dockerfile installs pip, setuptools and wheel with:

```
RUN curl -fSsL -O https://bootstrap.pypa.io/get-pip.py && \
    python get-pip.py && \
    rm get-pip.py
```
On 22-Jul-2018 this package was updated, so the installed versions are now: `pip-18.0` , `setuptools-40.0.0` and `wheel-0.31.1` . Proved using the previous version, i.e.:

```
RUN curl -fSsL -O https://bootstrap.pypa.io/3.3/get-pip.py && \
    python get-pip.py && \
    rm get-pip.py
```

Which installs `pip-10.0.1` , `setuptools-40.0.0` and `wheel-0.29.0`,  and the rebuild worked. The exact command to reproduce the working build was:

```
mkdir tensorflow && cd tensorflow
git clone https://github.com/tensorflow/tensorflow.git .
sed -i 's/bootstrap.pypa.io\/get-pip.py/bootstrap.pypa.io\/3.3\/get-pip.py/' tensorflow/tools/docker/Dockerfile.devel
export TF_DOCKER_BUILD_IS_DEVEL=YES
export TF_DOCKER_BUILD_TYPE=CPU
export TF_DOCKER_BUILD_PYTHON_VERSION=PYTHON3
export TF_DOCKER_BUILD_DEVEL_BRANCH=master
tensorflow/tools/docker/parameterized_docker_build.sh
```

### Pull request: #21168
"
21166,Numpy with Tensorflow Eager as backend?,"It would be really cool if the eager execution would not be exclusive in one session.

If we could use default and eager execution in parallel, it would be possible to use Tensorflow Eager Execution as backend to numpy.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: None
- **TensorFlow installed from (source or binary)**: None
- **TensorFlow version (use command below)**: None
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: None
"
21165,"[BUG] About your Android App, problem with fragment lifecycle.",
21164,Default floating point values in single_image_random_dot_stereograms_ops.cc cause syntax error,"Have I written custom code: yes
OS Platform and Distribution: Ubuntu 18.04
TensorFlow installed from: origin master
TensorFlow version: 1.9
Bazel version 0.15.2
CUDA/cuDNN version: 9.1/7.1
GPU model and memory:  NVIDIA-SMI 390.48
Exact command to reproduce: use code below in python3 script 
Mobile device: N/A
     
     import tensorflow as tf
      vol = tf.contrib.util.make_tensor_proto([256, 256])

Import of /localhome/local/projects/lme_custom_ops/tensorflow/contrib/image/__init__.py causes syntax error on my system (see issue https://github.com/tensorflow/serving/issues/421).

    Traceback (most recent call last):
    File ""main.py"", line 6, in <module>
        import geometry as geometry
    File ""/localhome/local/projects/deep-iterative-reco/geometry.py"", line 20, in <module>
        volume_origin = tf.contrib.util.make_tensor_proto([-((_volume_xy-1)/2 * _volume_spacing),-((_volume_xy-1)/2 * _volume_spacing)], tf.float32 )
    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lazy_loader.py"", line 53, in __getattr__
        module = self._load()
    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lazy_loader.py"", line 42, in _load
        module = importlib.import_module(self.__name__)
    File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/__init__.py"", line 48, in <module>
        from tensorflow.contrib import image
    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/image/__init__.py"", line 70, in <module>
        from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms
    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/image/python/ops/single_image_random_dot_stereograms.py"", line 27, in <module>
        ""_single_image_random_dot_stereograms.so""))
    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/util/loader.py"", line 56, in load_op_library
        ret = load_library.load_op_library(path)
    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/load_library.py"", line 73, in load_op_library
        exec(wrappers, module.__dict__)
    File ""<string>"", line 28
        def single_image_random_dot_stereograms(depth_values, hidden_surface_removal=True, convergence_dots_size=8, dots_per_inch=72, eye_separation=3, mu=0,333299994, normalize=True, normalize_max=-100, normalize_min=100, border_level=0, number_colors=256, output_image_shape=[1024, 768, 1], output_data_window=[1022, 757], name=None):
                                                                                                                                                                    ^
    SyntaxError: invalid syntax

Works again if I change the default kwargs float arguments to integer values. Reason maybe localization option (I am from Germany which uses , to indicate decimal point. E.g. 1.2 == 1,2 in Germany). Though I think I am using the US standard.

Using this code in /tensorflow/contrib/image/ops/single_image_random_dot_stereograms_ops.cc solves the problem (not using floating values).

    REGISTER_OP( ""SingleImageRandomDotStereograms"" )
        .Attr( ""T: {double,float,int64,int32}"" )
        .Input( ""depth_values: T"" )
        .Output( ""image: uint8"" )
        .Attr( ""hidden_surface_removal: bool = true"" )
        .Attr( ""convergence_dots_size: int = 8"" )
        .Attr( ""dots_per_inch: int = 72"" )
        .Attr( ""eye_separation: float = 3"" )
        .Attr( ""mu: float = 3333"" )
        .Attr( ""normalize: bool = true"" )
        .Attr( ""normalize_max: float = -100.0"" )
        .Attr( ""normalize_min: float = 100.0"" )
        .Attr( ""border_level: float = 0.0"" )
        .Attr( ""number_colors: int = 256"" )


On python2 executing the following...  

      Python 2.7.15rc1 (default, Apr 15 2018, 21:51:34) 
      [GCC 7.3.0] on linux2
      Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
      >>> import locale
      >>> locale.setlocale(locale.LC_ALL, '')
            'LC_CTYPE=en_US.UTF-8;LC_NUMERIC=de_DE.UTF-8;LC_TIME=de_DE.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=de_DE.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=de_DE.UTF-8;LC_NAME=de_DE.UTF-8;LC_ADDRESS=de_DE.UTF-8;LC_TELEPHONE=de_DE.UTF-8;LC_MEASUREMENT=de_DE.UTF-8;LC_IDENTIFICATION=de_DE.UTF-8'

Though I compiled for python3:

    Python 3.6.5 (default, Apr  1 2018, 05:46:30) 
    Type ""copyright"", ""credits"" or ""license"" for more information.

    IPython 5.5.0 -- An enhanced Interactive Python.
    ?         -> Introduction and overview of IPython's features.
    %quickref -> Quick reference.
    help      -> Python's own help system.
    object?   -> Details about 'object', use 'object??' for extra details.

    In [1]: 
    ...: 
    ...: import locale

    In [2]: locale.localeconv()
    Out[2]: 
    {'currency_symbol': '',
    'decimal_point': '.',
    'frac_digits': 127,
    'grouping': [],
    'int_curr_symbol': '',
    'int_frac_digits': 127,
    'mon_decimal_point': '',
    'mon_grouping': [],
    'mon_thousands_sep': '',
    'n_cs_precedes': 127,
    'n_sep_by_space': 127,
    'n_sign_posn': 127,
    'negative_sign': '',
    'p_cs_precedes': 127,
    'p_sep_by_space': 127,
    'p_sign_posn': 127,
    'positive_sign': '',
    'thousands_sep': ''}

I am using tensorflow revision 6d71d3fc659b317a38586f71ae94410ad3261f55 on Ubuntu 18.08. cuDNN 7.1, CUDA 9.1

This seems to be related: https://github.com/tensorflow/tensorflow/issues/2974"
21163,FAILED: Build did NOT complete successfully (101 packages loaded),"lyq@lyq-virtual-machine:~/yqli/tensorflow$ sudo bazel build tensorflow/tools/quantization:quantize_graph
ERROR: /home/lyq/yqli/tensorflow/tensorflow/BUILD:449:1: no such package '@grpc//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/grpc/grpc/archive/v1.13.0.tar.gz, https://github.com/grpc/grpc/archive/v1.13.0.tar.gz] to /home/lyq/.cache/bazel/_bazel_root/aa56a649b479b5a19f4ce65a6e36da7d/external/grpc/v1.13.0.tar.gz: Checksum was 8ed4a3a6eeed153cc54921c8f3aebb9a5fe2a9d9923b5bbbce7e2989c42f2039 but wanted 50db9cf2221354485eb7c3bd55a4c27190caef7048a2a1a15fbe60a498f98b44 and referenced by '//tensorflow:grpc++'
ERROR: Analysis of target '//tensorflow/tools/quantization:quantize_graph' failed; build aborted: Analysis failed
INFO: Elapsed time: 775.107s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
"
21162,Eager execution support in tf.keras fit_generator breaks Keras callbacks,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 7.5 ppc64le but would occur on others
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**: Anaconda3, Python 3.6.4
- **Bazel version (if compiling from source)**:  N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.2
- **GPU model and memory**: NVIDIA Volta V100
- **Exact command to reproduce**: complicated, see below

### Describe the problem
When using tf.keras fit_generator with Keras callbacks the backward phase of the model is not present when the set_model, set_params, and on_train_begin callback methods are called.  This breaks any callback what needs the gradients, ops, etc present in the model before training begins.

This change was injected by this commit for eager execution support:
https://github.com/tensorflow/tensorflow/commit/1b67ccbe8006eacffd268553abd01310e8b187d6#diff-abfd39a5a5f655f8d92c482e91081f8c

The _make_train_function that was removed calls the optimizer's get_updates() method which adds all the operations under the training/<optimizer class name>/ scope. 

Once possible solution to this may be to put the calls back in but guard them with `if context.executing_eagerly()` as is done here:
https://github.com/tensorflow/tensorflow/blob/939237af1479cf61a7c16ad4a1ead521da65a3a6/tensorflow/python/keras/engine/training.py#L1640-L1650

### Source code / logs
An example of a Keras callback that depends on the full model being present when the set_model function is called is this model graph analysis / rewriting callback:
https://github.com/tensorflow/tensorflow/pull/19845/files#diff-ae5805990d1ab77911507c9ae0b1cda2"
21160, W T:\src\github\tensorflow\tensorflow\core\framework\op_kernel.cc:1318] OP_REQUIRES failed at save_restore_tensor.cc:170 ,"System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No. I am trying to run the Speech_Commands example
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): 1.9.0
Python version: 3.5.4
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:
Describe the problem
The training of 18000 steps is over successfully and I have got both the confusion matrix and the accuracy. I am trying to freeze the model by running freeze.py. I am getting errors.

Source code / logs
The command I typed is as follows: as given in the tutorial.
C:\>python C:\Users\EDDY\tensorflow\tensorflow\examples\speech_commands\freeze.py
The output I got is as follows:
 I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
 W T:\src\github\tensorflow\tensorflow\core\framework\op_kernel.cc:1318] OP_REQUIRES failed at save_restore_tensor.cc:170 : Invalid argument: Unsuccessful TensorSliceReader constructor: Failed to get matching files on : Unknown: FindFirstFile failed for: ./Documents and Settings : 存取被拒。
; Input/output error"
21159,The name 'import/Mul' refers to an Operation not in the graph,"#12736  This did not work for me is there any other solution for this ?
I changed my code as following
  ```
input_height = 299
  input_width = 299
  input_mean = 0
  input_std = 299
  input_layer = ""Mul""
  output_layer = ""final_result""

  input_name = ""import/"" + input_layer
  output_name = ""import/"" + output_layer
  input_operation = graph.get_operation_by_name(input_name)
  output_operation = graph.get_operation_by_name(output_name)

```
Please suggest me a solution
Thank you"
21158,'ConfigProto' object has no attribute 'name',"Dear guys!!

I have encountered an issue when I train the neural network. It shows 'ConfigProto' object has no attribute 'name' in the terminal and I have no idea what's going on. Does anyone face similar problem and how to solve it. Thanks a lot. 
"
21157,DecodeError: truncated message,"retrain.py ran successfully and saved_model.pb and output_labels.txt are been created inside the ""tmp"" folder. When I try to use it inside label_image.py I get the following error

```
  File ""C:\Users\Srikanth1.R\AppData\Local\Continuum\anaconda3\lib\site-packages\google\protobuf\internal\decoder.py"", line 814, in _SkipFixed32
    raise _DecodeError('Truncated message.')

DecodeError: Truncated message.
```

Has anyone else faced this issue ? How to solve this ?"
21156,"Feature Idea: Checkpoint API ""split"" assert_consumed","I would like to see a function that is called something like `not_consumed()` for the checkpoint API here (https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restore) that returns all objects in the dependency graph that were not found in the checkpoint but without raising a python assertion like `assert_consumed` does.

I would use it because sometimes I want to make sure that I restored certain variables but about others I do not care if they are restored or not.

In general I believe this would be a minor change to the code (e.g. splitting `assert_consumed` into two) that would greatly help to write tests to validate that variables are indeed restored and probably also be helpful in other scenarios as well.

What is your take on this @allenlavoie ?
"
21154,tf.Print() print wrong value,"environment
ubuntu 16.04
tf-gpu 1.8

`tf_print = tf.Print(labels_0_new, [labels_0_new])`

The shape of labels_0_new is [16, 64]
But the output of tf_print is [[0 1 0]...]. The shape is [16?, 3]

What's the problem?"
21153,tf.contrib.image.sparse_image_warp,"
System:

== cat /etc/issue ===============================================
Linux george-OMEN-by-HP-Laptop 4.10.0-38-generic #42~16.04.1-Ubuntu SMP Tue Oct 10 16:32:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""18.3 (Sylvia)""
VERSION_ID=""18.3""
VERSION_CODENAME=sylvia

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux george-OMEN-by-HP-Laptop 4.10.0-38-generic #42~16.04.1-Ubuntu SMP Tue Oct 10 16:32:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy                              1.14.5            
numpydoc                           0.7.0             
protobuf                           3.6.0             
tensorflow-gpu                     1.9.0             
tensorflow-lattice                 0.9.6             
tensorflow-probability-gpu         0.2.0             
tensorflow-tensorboard             1.5.1             

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.9.0
tf.GIT_VERSION = v1.9.0-0-g25c197e023
tf.COMPILER_VERSION = v1.9.0-0-g25c197e023
Sanity check: array([1], dtype=int32)
/home/george/anaconda3/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda-9.1/lib64:/usr/local/cuda/lib64:/usr/local/cuda-9.1/lib64:/usr/local/cuda/extras/CUPTI/lib64/:
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Thu Jul 26 11:04:57 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1050    Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   54C    P0    N/A /  N/A |    355MiB /  4038MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1451      G   /usr/lib/xorg/Xorg                           187MiB |
|    0      2194      G   cinnamon                                      79MiB |
|    0      2864      G   ...-token=685C4795125F550AF469D7ED70DAE1ED    81MiB |
|    0      3149      G   /usr/lib/firefox/firefox                       3MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart.so.9.1.85
/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.1/lib64/libcudart.so.9.1.85
/usr/local/cuda-9.1/lib64/libcudart_static.a
/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.1/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-9.0/lib64/libcudart_static.a
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7


VERSION: v1.9.0-0-g25c197e023 1.9.0

Code:
def warp(image, R0, C0, R, C):
    A = tf.reshape(tf.stack([R0, C0], axis=-1), (1, -1, 2))
    B = tf.reshape(tf.stack([R, C], axis=-1), (1, -1, 2))
    image = tf.expand_dims(tf.cast(image, tf.float32), axis=0)
    print(image, A, B)
    image = tf.contrib.image.sparse_image_warp(image, A, B)[0]
    print(image)
    image = tf.cast(image, tf.uint8)
    return image
Error:

Traceback (most recent call last):
  File ""rotate.py"", line 167, in <module>
    test_warp()
  File ""rotate.py"", line 154, in test_warp
    new_image = sess.run(warp(image, R0, C0, R, C))
  File ""rotate.py"", line 70, in warp
    image = tf.contrib.image.sparse_image_warp(image, A, B)[0]
  File ""/home/george/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py"", line 53, in __getattr__
    module = self._load()
  File ""/home/george/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py"", line 42, in _load
    module = importlib.import_module(self.__name__)
  File ""/home/george/anaconda3/envs/py36/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/george/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/__init__.py"", line 47, in <module>
    from tensorflow.contrib import image
  File ""/home/george/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/image/__init__.py"", line 70, in <module>
    from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms
  File ""/home/george/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/image/python/ops/single_image_random_dot_stereograms.py"", line 27, in <module>
    ""_single_image_random_dot_stereograms.so""))
  File ""/home/george/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py"", line 56, in load_op_library
    ret = load_library.load_op_library(path)
  File ""/home/george/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py"", line 73, in load_op_library
    exec(wrappers, module.__dict__)
  File ""<string>"", line 27
    def single_image_random_dot_stereograms(depth_values, hidden_surface_removal=True, convergence_dots_size=8, dots_per_inch=72, eye_separation=2,5, mu=0,333299994, normalize=True, normalize_max=-100, normalize_min=100, border_level=0, number_colors=256, output_image_shape=[1024, 768, 1], output_data_window=[1022, 757], name=None):
                                                                                                                                                   ^
SyntaxError: invalid syntax
"
21152,Native TF operations drop Keras tensor metadata,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution: Ubuntu 16.04
- **Mobile device**: N/A
- **TensorFlow installed from**: from pip
- **TensorFlow version**: ('v1.9.0-0-g25c197e023', '1.9.0')
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA-9.0/cuDNN-7.1.4
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```
$ python
import tensorflow as tf
from tensorflow.keras.layers import Input

x = Input((28, 28, 3))
y = Input((28, 28, 3))
z = x + y

model = tf.keras.Model([x, y], z)
```

### Description
The code mentioned above throws the error (see logs). As it turned out, almost all native tf operations , such padding, convolution, etc., drop tensor metadata.

I do understand that these operations can be replaced with similar Keras layers or wrapped with Lambda. However, since Keras is officially moved to tensorflow package, it would be nice to preserve tensor compatibility between tf and Keras.

### Source code / logs
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/oleg/tf/venv/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py"", line 111, in __init__
    super(Model, self).__init__(*args, **kwargs)
  File ""/home/oleg/tf/venv/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/network.py"", line 78, in __init__
    self._init_graph_network(*args, **kwargs)
  File ""/home/oleg/tf/venv/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/network.py"", line 201, in _init_graph_network
    '(thus holding past layer metadata). Found: ' + str(x))
ValueError: Output tensors to a Model must be the output of a TensorFlow `Layer` (thus holding past layer metadata). Found: Tensor(""add:0"", shape=(?, 28, 28, 3), dtype=float32)
```
"
21150,How can use mobileNet Classification Model in opencv(c++ platform)?,"I am trying to using mobileNet model in opencv(c++),the object-dection Model is working.but the classification Model can not work.Hope to get help .So many thanks."
21145,InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'MklConv2DWithBias' with these attrs.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: debian 8
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.1.0
- **Python version**: Python 2.7.13
- **Bazel version (if compiling from source)**: 0.4.2
- **GCC/Compiler version (if compiling from source)**: gcc version 4.9.2 (Debian 4.9.2-10)
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

tf_env_collect.sh:

```
== cat /etc/issue ===============================================
Linux 4be8a8788f34 4.9.60-linuxkit-aufs #1 SMP Mon Nov 6 16:00:12 UTC 2017 x86_64 GNU/Linux
VERSION_ID=""8""
VERSION=""8 (jessie)""

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (Debian 4.9.2-10) 4.9.2
Copyright (C) 2014 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 4be8a8788f34 4.9.60-linuxkit-aufs #1 SMP Mon Nov 6 16:00:12 UTC 2017 x86_64 GNU/Linux

== check pips ===================================================
intel-numpy            1.13.3.10
numpy                  1.14.5
protobuf               3.6.0
tensorflow             1.1.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.1.0
tf.GIT_VERSION = v1.1.0-0-g1ec6ed5
tf.COMPILER_VERSION = v1.1.0-0-g1ec6ed5
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH /code/:/code//lib:/usr/local/lib
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
/tensor/tensorflow/tools/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```

### Describe the problem
I posted a stackoverflow issue: https://stackoverflow.com/questions/51530592/invalidargumenterror-see-above-for-traceback-no-opkernel-was-registered-to-su

### How to Generate the problem

I found a way to reproduce this problem.

1. download tensorflow zip and unzip. 
  https://drive.google.com/file/d/1k616LEvgTUXpHuX6Fz7EDXukaG5tGZwQ/view

2. mount tensorflow folder to container:
  docker run --rm -it -v $(pwd):/code econtal/numpy-mkl  bash

3. create the test script:

  ```
  $ cd /code
  $ vi test_conv2d.py

  # demo from https://www.jianshu.com/p/a70c1d931395
  import tensorflow as tf 
  import tensorflow.contrib.slim as slim
  import numpy as np
 
  x1 = tf.ones(shape=[1, 64, 64, 3]) 
  w = tf.fill([5, 5, 3, 64], 1)
  # print(""rank is"", tf.rank(x1))

  # x1 = tf.cast(x1, tf.float32)  
  w = tf.cast(w, tf.float32)

  print('-----debugging-----')
  print(type(x1))
  print(x1.dtype.base_dtype)

  print(type(w))
  print(w.dtype.base_dtype)
  print('-------------------')

  # x1 = tf.cast(x1, tf.float16)

  y1 = tf.nn.conv2d(x1, w, strides=[1, 1, 1, 1], padding='SAME')
  y2 = slim.conv2d(x1, np.float32(64.0), np.array([5.0, 5.0], dtype=np.float32), 
  weights_initializer=tf.ones_initializer, padding='SAME')

  with tf.Session() as sess: 
      sess.run(tf.global_variables_initializer()) 
      y1_value,y2_value,x1_value=sess.run([y1,y2,x1])
      print(""shapes are"", y1_value.shape, y2_value.shape)
      print(y1_value==y2_value)
      print(y1_value)
      print(y2_value)
  ```

  4. run the test script

  ```
  $ python test_conv2d.py
  ```

Can anyone give some advice on how to find the problem? Thanks.

"
21142,Feature Request: Provide tf.pow with supporting  broadcasting?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21141,ValueError: No variables to save | Tensorflow,"Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""C:\Users\uthir\Anaconda3\envs\qwerty\lib\site-packages\tensorflow\python\platform\app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""C:\tensorflow12345\models\research\object_detection\trainer.py"", line 392, in train
    init_saver = tf.train.Saver(available_var_map)
  File ""C:\Users\uthir\Anaconda3\envs\qwerty\lib\site-packages\tensorflow\python\training\saver.py"", line 1338, in __init__
    self.build()
  File ""C:\Users\uthir\Anaconda3\envs\qwerty\lib\site-packages\tensorflow\python\training\saver.py"", line 1347, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""C:\Users\uthir\Anaconda3\envs\qwerty\lib\site-packages\tensorflow\python\training\saver.py"", line 1372, in _build
    raise ValueError(""No variables to save"")
ValueError: No variables to save

I am trying to train my own model using ssd_mobile_v2_coco with ssd_mobilenet_v1_pets.config. I got this error. Please help me to solve this."
21137,KNN classifier.setClassifierDataset,"hi

can you help me to save and retrieve the model I've created based on KNN Classifier?
I've seen that I need to use classifier.setClassifierDataset and classifier.getClassifierDataset, but I cannot make it workj

thanks"
21136,Feature Request: tf.contrib.image.interpolate_spline partially known TensorShape,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13.5
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
When using tf.contrib.image.interpolate_spline(), the input cannot have partially known shape, e.g., a placeholder. I wonder if new features to allow inputs with partially known shape can be added. The following is a code snippet to demonstrate the problem. Thank you for helping!

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

import tensorflow as tf

X = tf.placeholder(tf.float32, [None])
Y = tf.constant([4,5,6], tf.float32)
x2 = tf.constant([2.5,2.3,2.9], tf.float32)

x = tf.reshape([[X]], [1, -1, 1])
y = tf.reshape([[Y]], [1, -1, 1])
x2 = tf.reshape([[x2]], [1, -1, 1])
y2 = tf.contrib.image.interpolate_spline(x, y, x2, order=1)

with tf.Session() as session:
    result = session.run(y2, feed_dict={X: [1, 2, 3]})
    print(result[0,:,0])"
21135,[Bug] Conflict between Tensorboard and Tensorflow Training ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: Quadro M4000
- **Exact command to reproduce**: N/A

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Using the new Tensorflow v1.9 framework's keras interfaces, I just add Tensorboard callback in the example Keras classification codes. However, when tensorboard is activated, the training crashed. The error is 

2018-07-24 16:08:28.615130: W T:\src\github\tensorflow\tensorflow\core\framework\op_kernel.cc:1318] OP_REQUIRES failed at save_restore_v2_ops.cc:137 : Unknown: Failed to rename: E:\projects\keras1\sc_weight.ckpt.index.tempstate893681564054566973 to: E:\projects\keras1\sc_weight.ckpt.index : Access is denied.

When tensorboard is not used, training has no problem. 

It seems that the Tensorflow needs to rename the files every time when saving occurs. The tensorboard people thinks they could do nothing on their end, and the solution should be based the Tensorflow end. (See the last response in the link below)
https://github.com/tensorflow/tensorboard/issues/892#issuecomment-407901434

Could people on Tensorflow's end fix this ""renaming"" issue? Thank you."
21134,apply_gradients does not work with var_list,"I use Windows 10, Tensorflow 1.9.0rc0, Python 3 with latest pycharm.

In a project I want to use compute_gradients and apply_gradients. I'm actually following this example https://gist.github.com/tillahoffmann/c04dd6ca9259ef68f743e44c74e3d095 to solve this problem https://github.com/tensorflow/tensorflow/issues/6692. The problem is that the solution does not work when we want to use  var_list with compute_gradients a simple example is
`

    loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))
    optimizer = tf.train.RMSPropOptimizer(learning_rate)
    grads = optimizer.compute_gradients(loss, var_list=[w])    
    train_op = optimizer.apply_gradients(grads)
`
which gives me the following error, unless I remove the var_list which is a critical part of my network design.

`File ""G:\python-venv\lib\site-packages\tensorflow\python\training\optimizer.py"", line 602, in apply_gradients
    update_ops.append(processor.update_op(self, grad))
  File ""G:\python-venv\lib\site-packages\tensorflow\python\training\optimizer.py"", line 188, in update_op
    raise NotImplementedError(""Trying to update a Tensor "", self._v)
NotImplementedError: ('Trying to update a Tensor ', <tf.Tensor 'mul:0' shape=(2560, 256) dtype=float32>)`


"
21130,Unexpected behavior of tf.hessians on graphs with tf.reduce_prod,"### System information

TensorFlow 1.9 CPU version installed via pip on Linux. Replicated the error on both Python 2.7.13 and 3.5.3 on Linux. Did not try with GPU version.

Details:
Have I written custom code: no
OS Platform and Distribution: Ubuntu 18.04
TensorFlow installed from: pip
TensorFlow version: 1.9 CPU version
Bazel version: n/a
CUDA/cuDNN version: n/a
GPU model and memory: n/a
Exact command to reproduce: see code below
Mobile device: no

### Describe the problem
``tf.hessians`` fails on a very simple function that uses ``tf.reduce_prod``. The error only occurs at the minimum of the function, where the Hessian is zero. Here's a minimal example:

    import numpy as np
    import tensorflow as tf
    
    x = tf.placeholder(tf.float32, shape=[3,])
    y = tf.reduce_prod(x**2)
    H = tf.hessians(y, x)[0]
    
    with tf.Session() as sess:
      print(sess.run(H, feed_dict={x: np.ones(3)}))
      print(sess.run(H, feed_dict={x: np.zeros(3)}))
      
    # Produces
    # [[2. 4. 4.]
    #  [4. 2. 4.]
    #  [4. 4. 2.]]
    # [[nan nan nan]
    #  [nan nan nan]
    #  [nan nan nan]]

The Hessian at ``x=[0, 0, 0]`` is well-defined and should evaluate to a zero-matrix. This behavior is unexpected. If ``tf.hessians`` can't handle ``tf.reduce_prod`` it should raise an exception.

### Source code / logs
See above for minimal example.
"
21129,"Feature request: Add a `LMDBDataset` class, that inherits from `tf.data.Dataset`.","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
N/A
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
N/A
- **TensorFlow installed from (source or binary)**:
N/A
- **TensorFlow version (use command below)**:
1.9
- **Python version**:
3.6
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
N/A
- **GPU model and memory**:
N/A
- **Exact command to reproduce**:
N/A

### Describe the problem

**Feature request: Add a `LMDBDataset` class, that inherits from `tf.data.Dataset`.**
Currently TF only supports LMDB datasets with the method [`LMDBReader`](https://www.tensorflow.org/api_docs/python/tf/LMDBReader). Implementing a `LMDBDataset` class that inherits from `tf.data.Dataset` will allow users to load LMDB data with the friendly tf.data API, and also use [`tf.contrib.data.make_batched_features_dataset`](https://www.tensorflow.org/api_docs/python/tf/contrib/data/make_batched_features_dataset), which applies the most up-to-data performance optimization for loading data.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21128,No contrib directory after installation via virtualenv,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.4
- **TensorFlow installed from (source or binary)**: https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.9.0-py3-none-any.whl
- **TensorFlow version (use command below)**: 1.9.0 (v1.9.0-0-g25c197e023)
- **Python version**: 3.6.4
- **CUDA/cuDNN version**: -
- **GPU model and memory**: Intel Iris Graphics 6100 / 1536 MB
- **Exact command to reproduce**: bazel build tensorflow/contrib/lite/toco:toco

### Describe the problem
Hi, I've successfully installed the latest version of tensorflow with virtualenv and even created the frozen graph of my model with help of it, but then I needed the .tflite file and I found that ```bazel run -c opt tensorflow/contrib/lite/toco:toco``` returns ```no such package 'tensorflow/contrib/lite/toco'``` error (at first run there was ```The 'run' command is only supported from within a workspace.``` and I've created WORKSPACE file, because I saw some issue with such an advice, but problem was not in this file).
Then I've admitted that directory structure differs from one from the github. I had only include, bin and lib directories and pip-selfcheck.json
I've reinstalled tensorflow few times, I've deleted environment in Anaconda, when I had another build of tensorflow and reinstalled it again. Nothing changes. Also I haven't found any issues with this problem in github or stackoverflow. Maybe I've missed something (despite I followed the docs very precisely)? Any help would be very appreciated

### Source code / logs
I've installed When I'm trying to run 
```
bazel run -c opt tensorflow/contrib/lite/toco:toco -- \
--input_file=/Users/kate/python/codes2/tflite_res/tflite_graph.pb \
--output_file=/Users/kate/python/codes2/tflite_res/detect.tflite \
--input_shapes=1,300,300,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \
--inference_type=QUANTIZED_UINT8 \
--mean_values=128 \
--std_values=128 \
--change_concat_input_ranges=false \
--allow_custom_ops
```
I get 
```
ERROR: Skipping 'tensorflow/contrib/lite/toco:toco': no such package 'tensorflow/contrib/lite/toco': BUILD file not found on package path
WARNING: Target pattern parsing failed.
ERROR: no such package 'tensorflow/contrib/lite/toco': BUILD file not found on package path
INFO: Elapsed time: 0.370s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
FAILED: Build did NOT complete successfully (0 packages loaded)
```

Bazel was installed with homebrew. Build command returns the same. 
Also here is the info I've collected with the help of your script
```
== cat /etc/issue ===============================================
Darwin MacBook-Pro-Ekaterina.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64
Mac OS X 10.13.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.1.0 (clang-902.0.39.2)
Target: x86_64-apple-darwin17.5.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== uname -a =====================================================
Darwin MacBook-Pro-Ekaterina.local 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy                              1.15.0
numpydoc                           0.7.0
protobuf                           3.6.0
tensorflow                         1.9.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.9.0
tf.GIT_VERSION = v1.9.0-0-g25c197e023
tf.COMPILER_VERSION = v1.9.0-0-g25c197e023
Sanity check: array([1], dtype=int32)
/Users/kate/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/Users/kate/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
/Users/kate/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/Users/kate/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```

There are some warnings about deprecation in python libs but them seem to be harmless. Also I don't understand why check for virtualenv is false (I've tried to run the command with both activated and deactivated tensorflow environment but it is always false).

Thanks
"
21126,Estimator model folder format,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: no
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- **CUDA/cuDNN version**: 9.2
- **GPU model and memory**: 1080TI 11GB
- **Exact command to reproduce**: python example.py

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I tried to make a working code for Estimator and Dataset APIs together. The `Estimator` constructor depends on `model_dir` parameter like this: `tf.estimator.Estimator(model_fn, model_dir, params={})`. I've noticed that `model_dir` string should not be ended with '/'. Step to reproduce:
1. Run `python example.py` -> fail
2. Replace line 11 by
```
experiment_folder = '/output'
```
3. Run `python example.py` -> success

I think it is a bug. It is simple and easy to avoid but I investigated not so much time to verify is it a symptom of some bigger problem. Hope someone will pay attention to it. 

### Source code / logs
Both `example.py` and console output could be found [here](https://gist.github.com/RomanSteinberg/840d58b9333a3359cec43bbf4c464e5a)."
21125,TensorRT loses defined shapes ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **Bazel version**: N/A
- **GPU model and memory**: Titan 12Gb
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10rc0
- **Python version**: 3.5
- **CUDA/cuDNN version**: 9 / 7.0.15

### Describe the problem
When `tf.contrib.tensorrt` compiles a graph which can be fully compiled it loses shape information.

For example if you compile a graph which has no activations, for example a model which ends in an average pool so that you can get [N, C] then the shape information is lost.

If you load up that optimised model and try to add an operation on the end which required a shape definition, for example a `tf.unstack` it will fail out stating there isn't any shape information.

This seems like the output shape should be kept.

"
21124,The mobilenet predict results are different,"**Problem Description:**

I meet a very strange problem,the problem is that:
My job is train a pb file that can predict face's five key points,he network model is mobilenet v2,when I trained to get a small loss pb file, the loss is about 0.7,that's ok,no problem.But when I predict with trained pb file, there's a problem. When the input image shape is [100,224,224,3], get the output is [100,10],(ten means five key point coordinates),that's ok, the output is accurate,tes is right,but when I change input image shape[1,224,224,3](there's only one input image),the output [1,10] is different from privous result,the two cases of  input[0] is the same, but output[0] is different,  the input shape is  [100,224,224,3] more accurate, why? 

the exact coordinates of output[0]:
[38 46 71 54 62 70 26 81 56 88]

input shape [100,224,224,3], get input[0]:
[39.381622 45.988384 73.15548  55.10713  64.56505  70.48474  29.348461
 82.159615 53.589706 87.59885 ]

input shape [1,224,224,3],get input[0]:
[[40.17595  47.69543  85.54374  50.631523 64.47522  71.54203  36.73784
  85.07918  79.665985 87.313995]]


the exact coordinates of output[0]:
[39 46 88 35 76 65 51 80 94 70]

input shape [100,224,224,3], get input[0]:
[38.898895 45.90179  86.28276  34.750797 76.82557  65.86458  51.15553
 79.85125  95.181885 70.00042 ]

input shape [1,224,224,3],get input[0]:
[[39.191376 46.032986 88.266106 39.030933 72.28544  64.2173   49.861656
  80.540855 94.46614  74.561905]]

-------------------------------------

System information
Linux Ubuntu 16.04:
TensorFlow installed from source:
TensorFlow version 1.8.0:
Python version:3.6:
Bazel version 0.11.1:
GCC/Compiler version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) :
CUDA/cuDNN 9.0/7.0.5:
GPU 1060-6G:
"
21123,"Unresolved External Symbol: public: double __cdecl double_conversion::StringToDoubleConverter::StringToDouble(char const *,int,int *)const","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: r1.9
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: Cmake 3.10.2, swigwin 3.0.12, Visual studio 2015 v140
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:  I followed to CMake tutorial here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake TensorFlow CMake build

### Describe the problem
Error when compiling a sample program in Visual Studio:

- Severity	Code	Description	Project	File	Line	Suppression State
Error	LNK2019	unresolved external symbol ""public: double __cdecl double_conversion::StringToDoubleConverter::StringToDouble(char const *,int,int *)const "" (?StringToDouble@StringToDoubleConverter@double_conversion@@QEBANPEBDHPEAH@Z) referenced in function ""bool __cdecl tensorflow::strings::safe_strtod(char const *,double *)"" (?safe_strtod@strings@tensorflow@@YA_NPEBDPEAN@Z)	TensorFlow r1.9	D:\GIT\All\TensorFlow.plugin\tf_core_lib.lib(numbers.obj)	1	
- Severity	Code	Description	Project	File	Line	Suppression State
Error	LNK2019	unresolved external symbol ""public: float __cdecl double_conversion::StringToDoubleConverter::StringToFloat(char const *,int,int *)const "" (?StringToFloat@StringToDoubleConverter@double_conversion@@QEBAMPEBDHPEAH@Z) referenced in function ""unsigned __int64 __cdecl tensorflow::strings::FloatToBuffer(float,char *)"" (?FloatToBuffer@strings@tensorflow@@YA_KMPEAD@Z)	TensorFlow r1.9	D:\GIT\All\TensorFlow.plugin\tf_core_lib.lib(numbers.obj)	1	

Is there any additional dependency that I am missing? Check below for the list. Thanks

### Source code / logs

Sample program:

```
#include ""stdafx.h""

#include <vector>
#include <eigen/Dense>

#include ""matmul.h""
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/cc/ops/standard_ops.h""

using namespace tensorflow;


int main()
{
	Scope root = `Scope::NewRootScope();
}
```

Additional Dependencies:

```
TensorFlow\r1.9\tensorflow\contrib\cmake\build\zlib\install\lib\zlibstatic.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\gif\install\lib\giflib.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\png\install\lib\libpng16_static.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\jpeg\install\lib\libjpeg.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\lmdb\install\lib\lmdb.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\jsoncpp\src\jsoncpp\src\lib_json\Release\jsoncpp.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\farmhash\install\lib\farmhash.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\fft2d\\src\lib\fft2d.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\highwayhash\install\lib\highwayhash.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\nsync\src\nsync\Release\nsync.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\snappy\src\snappy\Release\snappy.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\protobuf\src\protobuf\Release\libprotobuf.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\tf_string_ops.dir\Release\tf_string_ops.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\tf_cc.dir\Release\tf_cc.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\tf_cc_ops.dir\Release\tf_cc_ops.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\tf_cc_framework.dir\Release\tf_cc_framework.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\tf_core_lib.dir\Release\tf_core_lib.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\tf_core_cpu.dir\Release\tf_core_cpu.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\tf_core_direct_session.dir\Release\tf_core_direct_session.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\tf_core_framework.dir\Release\tf_core_framework.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\tf_core_kernels.dir\Release\tf_core_kernels.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\tf_core_ops.dir\Release\tf_core_ops.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\tf_cc_while_loop.dir\Release\tf_cc_while_loop.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\Release\tf_protos_cc.lib
TensorFlow\r1.9\tensorflow\contrib\cmake\build\sqlite\install\lib\sqlite.lib
```"
21121,add_to_collection does not update trainable attribute,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution**: ArchLinux
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:1.9.0-2
- **Python version**:3.6.6
- **CUDA/cuDNN version**: 9.2.148-1 / 7.1.4-1
- **GPU model and memory**: GTX 1080 Ti
- **Exact command to reproduce**: python test.py

### Describe the problem
Use `tf.add_to_collection` to add nontrainable variable to `tf.GraphKeys.TRAINABLE_VARIABLES` collection. The `trainable` attribute of variable is stiil False.


### Source code / logs
```python
import tensorflow as tf

with tf.name_scope('some_scope1'):
    a = tf.Variable(initial_value=1, name='a')
    b = tf.Variable(initial_value=2, name='b', trainable=False)
    c = tf.Variable(initial_value=3, name='c')

with tf.name_scope('some_scope2'):
    d = tf.Variable(initial_value=4, name='d', trainable=False)
    e = tf.Variable(initial_value=5, name='e', trainable=False)
    f = tf.Variable(initial_value=6, name='f')

h = tf.Variable(initial_value=8, name='h')

print('-------GLOBAL_VARIABLES-------')
for i in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='some_scope'):
    print(i.name, i)   # i.name if you want just a name

print('-------TRAINABLE_VARIABLES 1-------')
for i in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='some_scope'):
    print(i.name, i)   # i.name if you want just a name

print('-------Set ALL to Trainable------')
for i in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='some_scope'):
    tf.add_to_collection(tf.GraphKeys.TRAINABLE_VARIABLES, i)

print('-------TRAINABLE_VARIABLES 2-------')
for i in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='some_scope'):
    print(i.name, i, i.trainable)   # i.name if you want just a name

'''
-------GLOBAL_VARIABLES-------
some_scope1/a:0 <tf.Variable 'some_scope1/a:0' shape=() dtype=int32_ref>
some_scope1/b:0 <tf.Variable 'some_scope1/b:0' shape=() dtype=int32_ref>
some_scope1/c:0 <tf.Variable 'some_scope1/c:0' shape=() dtype=int32_ref>
some_scope2/d:0 <tf.Variable 'some_scope2/d:0' shape=() dtype=int32_ref>
some_scope2/e:0 <tf.Variable 'some_scope2/e:0' shape=() dtype=int32_ref>
some_scope2/f:0 <tf.Variable 'some_scope2/f:0' shape=() dtype=int32_ref>
-------TRAINABLE_VARIABLES 1-------
some_scope1/a:0 <tf.Variable 'some_scope1/a:0' shape=() dtype=int32_ref>
some_scope1/c:0 <tf.Variable 'some_scope1/c:0' shape=() dtype=int32_ref>
some_scope2/f:0 <tf.Variable 'some_scope2/f:0' shape=() dtype=int32_ref>
-------Set ALL to Trainable------
-------TRAINABLE_VARIABLES 2-------
some_scope1/a:0 <tf.Variable 'some_scope1/a:0' shape=() dtype=int32_ref> True
some_scope1/c:0 <tf.Variable 'some_scope1/c:0' shape=() dtype=int32_ref> True
some_scope2/f:0 <tf.Variable 'some_scope2/f:0' shape=() dtype=int32_ref> True
some_scope1/a:0 <tf.Variable 'some_scope1/a:0' shape=() dtype=int32_ref> True
some_scope1/b:0 <tf.Variable 'some_scope1/b:0' shape=() dtype=int32_ref> False
some_scope1/c:0 <tf.Variable 'some_scope1/c:0' shape=() dtype=int32_ref> True
some_scope2/d:0 <tf.Variable 'some_scope2/d:0' shape=() dtype=int32_ref> False
some_scope2/e:0 <tf.Variable 'some_scope2/e:0' shape=() dtype=int32_ref> False
some_scope2/f:0 <tf.Variable 'some_scope2/f:0' shape=() dtype=int32_ref> True
'''
```"
21120,Log_probabilities returned by tf.nn.ctc_beam_search_decoder,"**System Info**

- Have I written custom code=Yes
- OS Platform and Distribution :: Linux ( CentOS version 6.6 )
- TensorFlow installed from: binary (pip install)
- TensorFlow version : 1.3.0
- Bazel version: N/A
- CUDA/cuDNN version : v8.0.44
- GPU model and memory: NVIDIA Quadro P5000 (16272MiB)
- Exact command to reproduce :N/A 
- Mobile device: N/A

**Problem**
I am training a LSTM-CTC speech recognition system with using beam search decoding in the following configuration:

`decoded, log_prob =
tf.nn.ctc_beam_search_decoder(
    inputs,
    sequence_length,
    beam_width=100,
    top_paths=3,
    merge_repeated=True
)`


The output of log_probabilities for a batch by the above decoder are like-
 
`     

       [ [ 20.45407486,  20.44991684,  20.41798401],

        [ 14.9961853 ,  14.925807  ,  14.88066769],
        ..., 
        [ 18.89863396,  18.85992241,  18.85712433],

        [  3.93567419,   3.92791557,   3.89198923],

        [ 14.56258488,  14.55923843,  14.51092243]],`


So how these scores represent log probabilities and if I want to compare confidence for top paths among examples then what will be the normalisation factor???


"
21118,use custom operators  Failed to allocate tensors.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:iphone 6s
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:1.9
- **Python version**:3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I follow [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/custom_operators.md) custom Abs operator,but I ran my code throw error:
tensorflow/contrib/lite/simple_memory_arena.cc:82 it->size != alloc.size (52272 != 0)
2018-07-25 14:03:50.795332+0800 testTF[1477:647442] Failed to allocate tensors.

### Source code / logs
    TfLiteStatus AbsPrepare(TfLiteContext* context, TfLiteNode* node) {
    using namespace tflite;
    TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
    TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
    
    const TfLiteTensor* input = GetInput(context, node, 0);
    TfLiteTensor* output = GetOutput(context, node, 0);
    
    int num_dims = NumDimensions(input);
    
    TfLiteIntArray* output_size = TfLiteIntArrayCreate(num_dims);
    for (int i=0; i<num_dims; ++i) {
        output_size->data[i] = input->dims->data[i];
    }
    
    return context->ResizeTensor(context, output, output_size);
    }

    TfLiteStatus AbsEval(TfLiteContext* context, TfLiteNode* node) {
    using namespace tflite;
    const TfLiteTensor* input = GetInput(context, node,0);
    TfLiteTensor* output = GetOutput(context, node,0);
    
    float* input_data = input->data.f;
    float* output_data = output->data.f;
    
    size_t count = 1;
    int num_dims = NumDimensions(input);
    for (int i = 0; i < num_dims; ++i) {
        count *= input->dims->data[i];
    }
    
    for (size_t i=0; i<count; ++i) {
        output_data[i] = std::abs(input_data[i]);
    }
    return kTfLiteOk;
    }

    TfLiteRegistration* Register_ABS() {
    static TfLiteRegistration r = {nullptr, nullptr, AbsPrepare, AbsEval};
    return &r;
    }
    int width = 24;
     int height = 24;
     NSString* RunInferenceOnImage() {
     NSString* graph = @""tf_Rnet"";
     const int num_threads = 4;
     std::string input_layer_type = ""float"";
     std::vector<int> sizes = {1, height, width, 3};
     //std::vector<int> sizes = {1, 224, 224, 3};
    
    const NSString* graph_path = FilePathForResourceName(graph, @""tflite"");
    
    std::unique_ptr<tflite::FlatBufferModel> model(
                                                   tflite::FlatBufferModel::BuildFromFile([graph_path UTF8String]));
    if (!model) {
        NSLog(@""Failed to mmap model %@."", graph);
        exit(-1);
    }
    NSLog(@""Loaded model %@."", graph);
    model->error_reporter();
    NSLog(@""Resolved reporter."");
    
     #ifdef TFLITE_CUSTOM_OPS_HEADER
    tflite::MutableOpResolver resolver;
    RegisterSelectedOps(&resolver);
    #else
    tflite::ops::builtin::BuiltinOpResolver resolver;
    resolver.AddCustom(""Abs"", Register_ABS());
    #endif
    
    std::unique_ptr<tflite::Interpreter> interpreter;
    tflite::InterpreterBuilder(*model, resolver)(&interpreter);
    if (!interpreter) {
        NSLog(@""Failed to construct interpreter."");
        exit(-1);
    }
    
    if (num_threads != -1) {
        interpreter->SetNumThreads(num_threads);
    }
    
    int input = interpreter->inputs()[0];
    std::vector<int> tensor_input =interpreter->inputs();
    for (int index = 0; index < tensor_input.size(); index++)
    {
        TfLiteTensor* tensor = interpreter->tensor(tensor_input[index]);
        TfLiteIntArray* dims = tensor->dims;
        NSLog(@""fi"");
    }
    if (input_layer_type != ""string"") {
        interpreter->ResizeInputTensor(input, sizes);
    }
    
    if (interpreter->AllocateTensors() != kTfLiteOk) {
        NSLog(@""Failed to allocate tensors."");
        exit(-1);
    }
    NSString* image_path = FilePathForResourceName(@""face"", @""jpg"");
    int image_width;
    int image_height;
    int image_channels;
    std::vector<uint8_t> image_data =
    LoadImageFromFile([image_path UTF8String], &image_width, &image_height, &image_channels);
    const int wanted_width = width;
    const int wanted_height = height;
    const int wanted_channels = 3;
    const float input_mean = 127.5f;
    const float input_std = 127.5f;
    //  const float input_mean = 127.5f;
    //  const float input_std = 127.5f;
    assert(image_channels >= wanted_channels);
    uint8_t* in = image_data.data();
    float* out = interpreter->typed_tensor<float>(input);
    for (int y = 0; y < wanted_height; ++y) {
        const int in_y = (y * image_height) / wanted_height;
        uint8_t* in_row = in + (in_y * image_width * image_channels);
        float* out_row = out + (y * wanted_width * wanted_channels);
        for (int x = 0; x < wanted_width; ++x) {
            const int in_x = (x * image_width) / wanted_width;
            uint8_t* in_pixel = in_row + (in_x * image_channels);
            float* out_pixel = out_row + (x * wanted_channels);
            for (int c = 0; c < wanted_channels; ++c) {
                out_pixel[c] = (in_pixel[c] - input_mean) / input_std;
            }
        }
    }
    
    if (interpreter->Invoke() != kTfLiteOk) {
        NSLog(@""Failed to invoke!"");
        exit(-1);
    }
    
    std::vector<int> tensor_out =interpreter->outputs();

    size_t size = tensor_out.size();
    for (int index = 0; index < size; index++)
    {
        TfLiteTensor* tensor = interpreter->tensor(tensor_out[index]);
        TfLiteIntArray* dims = tensor->dims;
        float* output = interpreter->typed_output_tensor<float>(index);
        for (int i = 0; i<dims->data[1]; i++)
        {
            NSLog(@""%@"",@(output[i]));
        }
        NSLog(@""finish"");
    }
    return NULL;
    }"
21117,"mpi build failed, wrong filename","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04 x64
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
no
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.10.0rc0
- **Python version**:
3.6.6 x64
- **Bazel version (if compiling from source)**:
0.15.0
- **GCC/Compiler version (if compiling from source)**:
5.4.0
- **CUDA/cuDNN version**: 
9.2/7.1.5
- **GPU model and memory**:
GTX1080Ti GDDR5X 11GB  X  6
- **Exact command to reproduce**:
build with mpi support

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

![image](https://user-images.githubusercontent.com/4515120/43182126-1e4d20e8-901b-11e8-9414-2bc216fbd49f.png)


there is no such file actually

![image](https://user-images.githubusercontent.com/4515120/43182165-4789549a-901b-11e8-8086-f2da98551316.png)
"
21116,tensorflow lite only one output,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **Mobile device (e.g iphone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:iphone 6s
- **TensorFlow installed from (source or binary)**:no
- **TensorFlow version (use command below)**:1.9
- **Python version**:3.5
- **Bazel version (if compiling from source)**:0.12
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I use my model in tensorflowlite ,it have two outputs,but in my iphone6s only get one output!

(py35) ljg@ljg-linux:~/facenet/src/align/pb$ /home/ljg/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=Pnet.pb
Found 1 possible inputs: (name=pnet/input, type=float(1), shape=[?,?,?,3]) 
No variables spotted.
Found 2 possible outputs: (name=pnet/conv4-1/BiasAdd, op=BiasAdd) (name=pnet/conv4-2/BiasAdd, op=BiasAdd) 
Found 6632 (6.63k) const parameters, 0 (0) variable parameters, and 0 control_edges
Op types used: 13 Const, 13 Identity, 6 Neg, 6 Relu, 5 BiasAdd, 5 Conv2D, 3 Add, 3 Mul, 1 MaxPool, 1 Placeholder
To use with tensorflow/tools/benchmark:benchmark_model try these arguments:
bazel run tensorflow/tools/benchmark:benchmark_model -- --graph=Pnet.pb --show_flops --input_layer=pnet/input --input_layer_type=float --input_layer_shape=-1,-1,-1,3 --output_layer=pnet/conv4-1/BiasAdd,pnet/conv4-2/BiasAdd

/home/ljg/tensorflow-1.9.0/bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=Pnet.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=Pnet.tflite  --inference_type=FLOAT --inference_input_type=FLOAT --input_shape=1,12,12,3 --input_array=pnet/input --output_array=pnet/conv4-1/BiasAdd pnet/conv4-2/BiasAdd

![problem](https://user-images.githubusercontent.com/13581022/43180218-6e6282b0-9009-11e8-8bd4-f1d6046ba7e5.png)

### Source code / logs
    `class PNet(Network):
        def setup(self):
             (self.feed('data') 
             .conv(3, 3, 10, 1, 1, padding='VALID', relu=False, name='conv1')
             .prelu(name='PReLU1')
             .max_pool(2, 2, 2, 2, name='pool1')
             .conv(3, 3, 16, 1, 1, padding='VALID', relu=False, name='conv2')
             .prelu(name='PReLU2')
             .conv(3, 3, 32, 1, 1, padding='VALID', relu=False, name='conv3')
             .prelu(name='PReLU3')
             .conv(1, 1, 2, 1, 1, relu=False, name='conv4-1'))
             .softmax(3,name='prob1'))

             (self.feed('PReLU3') 
             .conv(1, 1, 4, 1, 1, relu=False, name='conv4-2'))
        `   
      `model_path='./'
      export_dir='./pb/'
      g1 = tf.Graph()
      sess1 = tf.Session(graph=g1)
      with sess1.as_default(): 
          with g1.as_default():
              with tf.variable_scope('pnet'):
                  data = tf.placeholder(tf.float32, (None,None,None,3), 'input')
                  pnet = PNet({'data':data})
                 pnet.load(os.path.join(model_path, 'det1.npy'), sess1)
            
    frozen_graphdef = tf.graph_util.convert_variables_to_constants(
          sess1, sess1.graph_def, ['pnet/conv4-2/BiasAdd', 'pnet/conv4-1/BiasAdd'])
    tf.train.write_graph(frozen_graphdef,export_dir, 'Pnet.pb', as_text=False)  
    print ('finis')
`     

"
21115,#Distributed Tensorflow,"* Overview: I'm struggling  with ""Distributed Tensorflow"" and the error is ""Could not satisfy explicit device specification '/job:worker/task:0' because the node was colocated with a group of nodes that required incompatible device '/job:ps/task:0'"". I also try to research on StackOverflow,.. but it doesn't help much. 

The code I used to work with ""Distributed Tensorflow"":  https://imgur.com/Wna7Yhz

Thank you so much 
"
21112,failed to build unit test for arm64,"I would like to run unit test on arm64, with command,

	bazel build --cxxopt=--std=c++11 //tensorflow/core/kernels:quantized_matmul_op_test --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a --verbose_failures

But got errors like

	ERROR: /home/local/SPREADTRUM/ben.shi/git-repo/tensorflow/tensorflow/core/kernels/BUILD:2879:1: C++ compilation of rule '//tensorflow/core/kernels:cwise_op' failed (Exit 1): aarch64-linux-android-gcc failed: error executing command 
	  (cd /home/local/SPREADTRUM/ben.shi/.cache/bazel/_bazel_spreadtrum.com/41de783a40029e841491a00f78b435ef/execroot/org_tensorflow && \
	  exec env - \
	    PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin:/usr/lib/jvm/java-8-openjdk-amd64/jre/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
	    PWD=/proc/self/cwd \
	    PYTHON_BIN_PATH=/usr/bin/python \
	    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
	    TF_DOWNLOAD_CLANG=0 \
	    TF_NEED_CUDA=0 \
	    TF_NEED_OPENCL_SYCL=0 \
	  external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64/bin/aarch64-linux-android-gcc -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -fno-canonical-system-headers -O2 -g -DNDEBUG -MD -MF bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/cwise_op/tensorflow/core/kernels/cwise_op_bessel.d '-frandom-seed=bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/cwise_op/tensorflow/core/kernels/cwise_op_bessel.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' '-DS_IEXEC=S_IXUSR' -DTF_USE_SNAPPY -iquote . -iquote bazel-out/arm64-v8a-opt/genfiles -iquote external/nsync -iquote bazel-out/arm64-v8a-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/arm64-v8a-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/arm64-v8a-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/arm64-v8a-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/arm64-v8a-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/arm64-v8a-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/arm64-v8a-opt/genfiles/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/arm64-v8a-opt/genfiles/external/zlib_archive -iquote external/double_conversion -iquote bazel-out/arm64-v8a-opt/genfiles/external/double_conversion -isystem external/nsync/public -isystem bazel-out/arm64-v8a-opt/genfiles/external/nsync/public -isystem bazel-out/arm64-v8a-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/arm64-v8a-opt/genfiles/external/eigen_archive -isystem bazel-out/arm64-v8a-opt/bin/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/arm64-v8a-opt/genfiles/external/gif_archive/lib -isystem bazel-out/arm64-v8a-opt/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/arm64-v8a-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/arm64-v8a-opt/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/arm64-v8a-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/arm64-v8a-opt/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/arm64-v8a-opt/genfiles/external/zlib_archive -isystem bazel-out/arm64-v8a-opt/bin/external/zlib_archive -isystem external/double_conversion -isystem bazel-out/arm64-v8a-opt/genfiles/external/double_conversion -isystem bazel-out/arm64-v8a-opt/bin/external/double_conversion '--std=c++11' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-std=c++11' -DTF_LEAN_BINARY -Wno-narrowing -fomit-frame-pointer -O2 '--sysroot=external/androidndk/ndk/platforms/android-24/arch-arm64' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/arm64-v8a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c tensorflow/core/kernels/cwise_op_bessel.cc -o bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/cwise_op/tensorflow/core/kernels/cwise_op_bessel.o)
	In file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:387:0,
	                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,
	                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
	                 from ./tensorflow/core/kernels/cwise_ops.h:23,
	                 from ./tensorflow/core/kernels/cwise_ops_common.h:29,
	                 from tensorflow/core/kernels/cwise_op_bessel.cc:16:
	external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In instantiation of 'static Scalar Eigen::internal::i1e_impl<Scalar>::run(Scalar) [with Scalar = Eigen::half]':
	external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:2132:49:   required from 'typename Eigen::internal::i1e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::i1e(const Scalar&) [with Scalar = Eigen::half; typename Eigen::internal::i1e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = Eigen::half]'
	external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsFunctors.h:323:17:   required from 'const Scalar Eigen::internal::scalar_i1e_op<Scalar>::operator()(const Scalar&) const [with Scalar = Eigen::half]'
	external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:341:44:   required from 'Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>::CoeffReturnType Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>::coeff(Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>::Index) const [with UnaryOp = Eigen::internal::scalar_i1e_op<Eigen::half>; ArgType = const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer>; Device = Eigen::ThreadPoolDevice; Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>::CoeffReturnType = Eigen::half; Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<UnaryOp, ArgType>, Device>::Index = long int]'
	external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:137:28:   required from 'void Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::evalScalar(Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::Index) [with LeftArgType = Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer>; RightArgType = const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_i1e_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer> >; Device = Eigen::ThreadPoolDevice; Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::Index = long int]'
	external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:93:7:   required from 'static void Eigen::internal::EvalRange<Evaluator, Index, Vectorizable>::run(Evaluator*, Index, Index) [with Evaluator = Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_i1e_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::ThreadPoolDevice>; Index = long int; bool Vectorizable = false]'
	external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:157:98:   [ skipping 2 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
	external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:154:7:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_i1e_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer> > >; bool Vectorizable = false]'
	external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:79:   required from 'Eigen::TensorDevice<ExpressionType, DeviceType>& Eigen::TensorDevice<ExpressionType, DeviceType>::operator=(const OtherDerived&) [with OtherDerived = Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_i1e_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer> >; ExpressionType = Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer>; DeviceType = Eigen::ThreadPoolDevice]'
	./tensorflow/core/kernels/cwise_ops_common.h:277:17:   required from 'void tensorflow::functor::Assign(const D&, Out, Rhs) [with D = Eigen::ThreadPoolDevice; Out = Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer>; Rhs = Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_i1e_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer> >]'
	./tensorflow/core/kernels/cwise_ops_common.h:525:58:   required from 'void tensorflow::functor::UnaryFunctor<Eigen::ThreadPoolDevice, Functor>::operator()(const CPUDevice&, typename Functor::tout_type, typename Functor::tin_type) [with Functor = tensorflow::functor::bessel_i1e<Eigen::half>; tensorflow::functor::CPUDevice = Eigen::ThreadPoolDevice; typename Functor::tout_type = Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer>; typename Functor::tin_type = Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long int>, 16, Eigen::MakePointer>]'
	./tensorflow/core/kernels/cwise_ops_common.h:250:72:   required from 'void tensorflow::UnaryOp<Device, Functor>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; Functor = tensorflow::functor::bessel_i1e<Eigen::half>]'
	tensorflow/core/kernels/cwise_op_bessel.cc:29:1:   required from here
	external/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1896:5: error: static assertion failed: THIS_TYPE_IS_NOT_SUPPORTED
	     EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),
	     ^
	Target //tensorflow/core/kernels:quantized_matmul_op_test failed to build
	INFO: Elapsed time: 860.780s, Critical Path: 51.03s
	INFO: 483 processes: 483 local.
	FAILED: Build did NOT complete successfully
	

"
21111,Tensorflow-GPU on Docker freezes after calling tf.Session(),"### System information
- **Have I written custom code**: No
- **OS Platform and Distribution** : Centos 7 and Ubuntu 16.04 running Nvidia-docker
- **TensorFlow installed from (source or binary)**: `pip install tensorflow-gpu`
- **TensorFlow version**: all versions from 1.6 to 1.9
- **Python version**: both 2.7 and 3.4
- **CUDA/cuDNN version**: `nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04` (On Centos 9.1/7.1.3, On Ubuntu driver only)
- **Exact command to reproduce**:
`>>import tensorflow as tf`
`>>sess = tf.Session()`
- **Run on native pip**:
I have successfully built tensorflow outside docker and it run properly
- **Bazel version**: N/A
- **GPU model and memory**: GTX 745/ 4GB
- **Mobile device**: N/A


### Describe the problem
Hi, i am struggling with the problem of using tensorflow inside nvidia-docker. I have successfully installed nvidia-docker and tested with nvidia-smi.
I used the docker image from nvidia, it works on Ubuntu but in Centos, whenever it runs `tf.Session` for both files or intepreter (type in `python` in terminal) it stucks even if I interupted by Control+C, Control+Z. The only way to get out is close the terminal window.
It seems that there are some problems with stdout.

Same problems:
https://stackoverflow.com/questions/45068455/tensorflow-app-freezes-in-docker-container
https://github.com/tensorflow/tensorflow/issues/1947

Thanks in advance!
"
21110,run with big data breakdown sometimes,"I use .pb file to prediction . if I use sample image is good .  if the I use big data，sometime while breakdown.   I use  tensorflow1.3 1.4 1.5 is OK,  but the version is higher than 1.5 have the problem.The pb is producted with tensorflow 1.4. 

CUB segmented reduce errorinvalid configuration argument
   [[Node: inception_v3/logits/predictions = Softmax[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](inception_v3/logits/logits/xw_plus_b)]]"
21108,Higher iterations_per_loop values in TPU training lead to NaN gradients!,"Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Cloud Platform (Linux Debian)
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
TensorFlow installed from (source or binary):Binary
TensorFlow version (use command below):1.9
Python version: 2.7
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: NA (TPU)
GPU model and memory: NA (TPU)
Exact command to reproduce:

### Describe the problem
I am using TPU and my model trains properly with `tpu_config=tf.contrib.tpu.TPUConfig(iterations_per_loop=20)` but when I increase `iterations_per_loop` to say 50 or 100, the gradients becomes NaN during the training (see the error below). I consistently see this behavior with higher values of `iterations_per_loop` while lower values (e.g <20) never lead to NaN gradients. 

I tried to set `num_shards=1` but the problem is the same.

 
### Source code / logs
```
InvalidArgumentError (see above for traceback): Gradient for ic_enc/ic_enc_2_ics/logvar/b:0 is NaN : Tensor had NaN values
	 [[Node: CheckNumerics_11 = CheckNumerics[T=DT_FLOAT, message=""Gradient for ic_enc/ic_enc_2_ics/logvar/b:0 is NaN"", _device=""/job:worker/replica:0/task:0/device:CPU:0""](Read_13/ReadVariableOp)]]
```
"
21104,Feature Request: 5D rot90 (for voxel grid rotations),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Amazon Deep Learning AMI w/ TF 1.9
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Pip
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9
- **GPU model and memory**: K80
- **Exact command to reproduce**: N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

It would be great to have a 5D [batch, height, width, depth, channels] version of tf.image.rot90. This is useful for data augmentation when using voxel grids. I suspect that this would be an extension of the existing rot90.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21103,tf.gradients should return an error if the xs list is empty,"You can currently call tf.gradients(ys, xs) where xs is an empty list. The returned result is None. I feel like it should return an error, though, to keep in line with the definition of the gradient -- dx/dy is undefined if x is undefined. 

Example code:

    a = tf.Variable([100])
    b = tf.reduce_sum(a)
    grads = tf.gradients(b, [])
    with tf.Session() as sess:
        print(sess.run(grads))

`None`

This is somewhat related to issue #783 (Gradient computation erroneously returns None), at least in the sense that the tf.gradients implementation deviates from the mathematical definition. I also understand that there may be downstream complications that make this unreasonable to implement. I looked around for an issue regarding this but couldn't find one... sorry if I missed anything!

EDIT: filling out template
Platform - Ubuntu 16.04
Tensorflow installed with pip3 in venv
Tensorflow version 1.8.0
N/A for the rest (they don't really apply to the question)
"
21099,Tensorflow Optimizer cannot optimize gradients when they are the output of tf.cond,"I have the following code in tensorflow where I cam trying to compute the gradients in 2 ways:

    def optimize(loss):
        
        with tf.name_scope('Optimizer'):
            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
        
            with tf.control_dependencies(update_ops):
                optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
			
                def prev_grads_are_not_None():
                
                    ....... (Not important for now)....
                    return grads_and_vars, grads_wrt_initial_state
            
                # This is executed when feeding the first batch only. => there is no previously collected gradients.
                def prev_grads_are_None():
                
                    # This will compute the local gradient given the loss function (taking into account all trainable variables)
                    local_grads = tf.gradients(loss, tf.trainable_variables())
                    grads_wrt_initial_state = tf.gradients(loss, initial_state, grad_ys=None)
                
                    grads_and_vars = list(zip(local_grads, tf.trainable_variables()))
                    return grads_and_vars, grads_wrt_initial_state
            
                grads_and_vars, grads_wrt_initial_state = tf.cond(this_is_last_batch, lambda: prev_grads_are_None(), lambda: prev_grads_are_None())
            
                train_step = optimizer.apply_gradients(grads_and_vars)            
                return train_step, grads_wrt_initial_state
Please note that I have used the same methods 2 times on purpose just for testing tf.cond.
and this will throw the following error:

    ~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\training\optimizer.py in update_op(self, optimizer, g)
        199 
        200   def update_op(self, optimizer, g):
    --> 201     raise NotImplementedError(""Trying to update a Tensor "", self._v)
        202 
        203 

    NotImplementedError: ('Trying to update a Tensor ', <tf.Tensor 'Optimizer/cond/Merge_1:0' shape=(264, 128) dtype=float32>)

so what is the reason for this bug?
Please note that I am trying to train a 2 layer GRU.

I have tried the following code as well:

def optimize(loss):
        
    with tf.name_scope('Optimizer'):
        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
        
        with tf.control_dependencies(update_ops):
            optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
            
            # When training we start from the end and proceed back ward. while we are in the middle of the training
            # then we will have previously collected gradients to be used. 
            def prev_grads_are_not_None():
                
                # ..... Not needed for now .....
                return grads, grads_wrt_initial_state
            
            # This is executed when feeding the first batch only. => there is no previously collected gradients.
            def prev_grads_are_None():
                
                # This will compute the local gradient given the loss function (taking into account all trainable variables)
                local_grads_ = tf.gradients(loss, tf.trainable_variables())
                grads_wrt_initial_state_ = tf.gradients(loss, initial_state)
                
                local_grads = [g for g in local_grads_]
                grads_wrt_initial_state = [g for g in grads_wrt_initial_state_]
                
                print(""---<<<"")
                for h in local_grads:
                    print(h)
                    
                return local_grads, grads_wrt_initial_state
            
            grads_r, grads_wrt_initial_state = tf.cond(this_is_last_batch, prev_grads_are_None, prev_grads_are_None)
            
            print(""---->>"")
            for r in grads_r:
                print(r)
            
            grads_and_vars = list(zip(grads_r, tf.trainable_variables()))

            train_step = optimizer.apply_gradients(grads_and_vars)            
            return train_step, grads_wrt_initial_state

But that will run and then stall, but showing kernel is busy in jupyter notebook. 

The printed tensors shows:

	---<<<
	Tensor(""Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/MatMul/Enter_grad/b_acc_3:0"", shape=(264, 128), dtype=float32)
	Tensor(""Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/BiasAdd/Enter_grad/b_acc_3:0"", shape=(128,), dtype=float32)
	Tensor(""Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/MatMul_1/Enter_grad/b_acc_3:0"", shape=(264, 64), dtype=float32)
	Tensor(""Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/BiasAdd_1/Enter_grad/b_acc_3:0"", shape=(64,), dtype=float32)
	Tensor(""Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/MatMul/Enter_grad/b_acc_3:0"", shape=(96, 64), dtype=float32)
	Tensor(""Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/BiasAdd/Enter_grad/b_acc_3:0"", shape=(64,), dtype=float32)
	Tensor(""Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/MatMul_1/Enter_grad/b_acc_3:0"", shape=(96, 32), dtype=float32)
	Tensor(""Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/BiasAdd_1/Enter_grad/b_acc_3:0"", shape=(32,), dtype=float32)
	Tensor(""Optimizer/case/cond/gradients/output_layer/MatMul_grad/MatMul_1:0"", shape=(32, 3), dtype=float32)
	Tensor(""Optimizer/case/cond/gradients/output_layer/add_grad/Reshape_1:0"", shape=(3,), dtype=float32)
	---->>
	Tensor(""Optimizer/case/cond/Merge:0"", shape=(264, 128), dtype=float32)
	Tensor(""Optimizer/case/cond/Merge_1:0"", shape=(128,), dtype=float32)
	Tensor(""Optimizer/case/cond/Merge_2:0"", shape=(264, 64), dtype=float32)
	Tensor(""Optimizer/case/cond/Merge_3:0"", shape=(64,), dtype=float32)
	Tensor(""Optimizer/case/cond/Merge_4:0"", shape=(96, 64), dtype=float32)
	Tensor(""Optimizer/case/cond/Merge_5:0"", shape=(64,), dtype=float32)
	Tensor(""Optimizer/case/cond/Merge_6:0"", shape=(96, 32), dtype=float32)
	Tensor(""Optimizer/case/cond/Merge_7:0"", shape=(32,), dtype=float32)
	Tensor(""Optimizer/case/cond/Merge_8:0"", shape=(32, 3), dtype=float32)
	Tensor(""Optimizer/case/cond/Merge_9:0"", shape=(3,), dtype=float32)

Therefore, is the difference in the type of the tensor considered the reason for this problem?
Any help is much appreciated!! 

**System Information**

- Python version: 3.6
- My tensorflow version is 1.8
- CUDA: 7/ cuDNN: 9
- GPU Model: GeForce 1080 Ti/ Memory: 12 GB
- tensorflow installed from binaries 
- OS: windows 10.
- Have I written custom code: N/A
- Bazel Version: N/A
- Mobile Device N/A
- Exact command to reproduce: N/A"
21096,documentation request: Prebuilt binary build configuration,"There are many flags/options for tensorflow build, some of them are performance related.
User may want to build a whl from source (with some modifications) , using **exactly the same** build configuration that prebuilt binary used

With previous Jenkins CI server, we can still figure out the build configuration from log of a release-xxxx build job.
But now with the new internal build system, it is not quite clear what build configuration is used for prebuilt binary anymore.
e.g. 
cuda compute capability: default value in configure.py (3.5,7.0) seems not complete.
cpu simd option: one may have to search release note to figure out whether it is still AVX.

It will be great to document the build setting somewhere, or introduce some script to produce prebuilt binary for a release.
Thanks.
"
21095,TFRecord documentation is still lacking,"1. It must be a bug, a feature request, or **a significant problem with documentation** (for small docs fixes please send a PR instead).
------------------------

### System information (N/A)

### Describe the problem
#1749 was closed with the note that ""We revamped our docs. Feel free to open a new issue if this is still missing from our new docs."" 

Despite being the ""recommended format"" for input data, I still can't find any official documentation on creating TFRecords, and what the best practices are surrounding this task.

It seems strange that blogs and the source code are the only sources of information on this. I think it would be a good idea to have an official guide showing how to go from a directory of images to some TFRecords. Maybe using [this script from tensorflow/models](https://github.com/tensorflow/models/blob/master/research/inception/inception/data/build_imagenet_data.py) as an example.

### Source code / logs (N/A)"
21094,libnccl-dev is not installed in Dockerfile.gpu,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master branch
- **Python version**: not related
- **Bazel version (if compiling from source)**: 0.14.1
- **GCC/Compiler version (if compiling from source)**: not related
- **CUDA/cuDNN version**: CUDA9, cuDNN7
- **GPU model and memory**: not related
- **Exact command to reproduce**:
tensorflow/tools/ci_build/ci_build.sh GPU bazel test //tensorflow/...


### Describe the problem
when build from source inside docker container, configure.py will fail with error message below due to nccl.h is missing.

```
Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:

Invalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2
Traceback (most recent call last):
  File ""configure.py"", line 1586, in <module>
    main()
  File ""configure.py"", line 1525, in main
    set_tf_nccl_install_path(environ_cp)
  File ""configure.py"", line 1156, in set_tf_nccl_install_path
    _DEFAULT_PROMPT_ASK_ATTEMPTS)
__main__.UserInputError: Invalid TF_NCCL setting was provided 10 times in a row. Assuming to be a scripting mistake.
```


### Source code / logs
https://github.com/tensorflow/tensorflow/blob/226831aab92a395a26824a08caa9d43f0c3d604e/tensorflow/tools/ci_build/Dockerfile.gpu#L18-L20
libnccl-dev is not installed, so nccl.h is missing."
21093,tf.metrics.*_at_thresholds inconsistent with other metrics which return a single value,"when I try to solve a binary classification problem with a custom tf.Estimator,
I made a model_fn refer to https://www.tensorflow.org/guide/custom_estimators,
but I want add 2 eval metrics like this:
...
tn = tf.metrics.true_positives(labels, predictions=predicted_classes)
tp_5 = tf.metrics.true_positives_at_thresholds(labels, predicted_classes, [0.2, 0.4, 0.6, 0.8, 1.0])
metrics = {'tn': tn, 'tp_5': tp_5}
if mode == tf.estimator.ModeKeys.EVAL:
  return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)
...

It will report TypeError: eval_metric_ops[tp_5] must be Operation or Tensor, given: <tf.Variable 'true_positives/true_positives:0' shape=(5,) dtype=float32_ref>

a workaround 
metrics = {'tn': tn, 'tp_5': tuple([tf.convert_to_tensor(tp_5[0]), tp_5[1]])}
will as expected.

Is this API design intentional?"
21091,iOS session create crash ,"Hi there,
I follow the iOS example ""simple"" to build an iOS cocoa touch static library.
Using camera to detect something and square it to image, then output feature data array. 
It seems crash when ''session creating'' sometimes and I don't know how to fix it.
here is the link about the static library:([click me](https://github.com/AlysonQ/GetFaceFeatureData))
Is that possible crash because of sending images too fast?
<img width=""1680"" alt=""2018-07-24 5 48 47"" src=""https://user-images.githubusercontent.com/9098498/43138691-4571cabe-8f82-11e8-9331-4792f47ef8ce.png"">

I tried #7108 and #2927 before, don't work for me.

Sorry for missing some information as below.
Have I written custom code: yes
OS Platform and Distribution: xcode 9.4.1 
TensorFlow installed from : TensorFlow static library build from source.
TensorFlow version: 1.08
Bazel version: 0.14.1-homebrew
CUDA/cuDNN version : none
GPU model and memory : none
Exact command to reproduce : ([click me](https://github.com/AlysonQ/GetFaceFeatureData))
Mobile device: iPhone8"
21090,Is there a benchmark that can be used on GPU?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21089,[Building Error] No rule to make target,"+ set -e
+++ dirname tensorflow/contrib/lite/build_ios_universal_lib.sh
++ cd tensorflow/contrib/lite
++ pwd
+ SCRIPT_DIR=/Users/ctr-daben/desktop/tensorflow-master/tensorflow/contrib/lite
+ cd /Users/ctr-daben/desktop/tensorflow-master/tensorflow/contrib/lite/../../..
+ make_library libtensorflow-lite.a
+ for arch in x86_64 armv7 armv7s arm64
+ make -f tensorflow/contrib/lite/Makefile TARGET=IOS IOS_ARCH=x86_64 -j 8 /Users/ctr-daben/desktop/tensorflow-master/tensorflow/contrib/lite/gen/lib/ios_x86_64/libtensorflow-lite.a
/Users/ctr-daben/Desktop/tensorflow-master/tensorflow/contrib/lite/gen/bin/ios_x86_64/benchmark_model
make: *** No rule to make target `/Users/ctr-daben/desktop/tensorflow-master/tensorflow/contrib/lite/gen/lib/ios_x86_64/libtensorflow-lite.a'.  Stop."
21088,3*cycle_length readers are created when using tf.data.Dataset? ,"Hello, I am creating a tensorflow dataset using the following code.
```
files = 'hdfs://default/testset/*'
ds = tf.data.Dataset.list_files(files)
ds = ds.apply(interleave_ops.parallel_interleave(tf.data.TextLineDataset, cycle_length=1))
ds = ds.shuffle(buffer_size=10000)
ds = ds.map(parse_line)
ds = ds.repeat()
ds = ds.batch(128)
ds = ds.prefetch(1)
iterator = ds.make_one_shot_iterator()
next_element = iterator.get_next()
```
Since the first line of each file is an empty line, I can notice it read 3*cycle_length(here cycle_length=1) files at the beginning from the the following output. After that, it still read files in order(not recurrently).
```
18/07/24 11:07:44 WARN hdfs.DFSClient: zero
18/07/24 11:07:44 WARN hdfs.DFSClient: zero
18/07/24 11:07:44 WARN hdfs.DFSClient: zero
```
I could not find a argument whose default value is 3. Does anyone know why it is 3? Thank you."
21085,Cannot compile toco on windows,"### Error Message
Target //tensorflow/contrib/lite/toco:toco failed to build saying Cannot open include file: 'sys/time.h'

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: none
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.9
- **Python version**:  3.5.1-32 bit
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: nothing since i have AMD radeon
- **GPU model and memory**:  Intel(R) HD Graphics 620
- **Exact command to reproduce**:

`bazel run -c opt tensorflow/contrib/lite/toco:toco -- \ --input_file=maonani/tflite_graph.pb \ --output_file=maonani/detect.tflite \ --input_shapes=1,300,300,3 \ --input_arrays=normalized_input_image_tensor \ --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \ --inference_type=QUANTIZED_UINT8 \ --mean_values=128 \ --std_values=128 \ --change_concat_input_ranges=false \ --allow_custom_ops`

### Describe the problem
i have been trying for weeks to convert custom object detection api models to tflite and i have found this article [https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193](url) but i just skip to the part where it converts the resulting frozen graph (tflite_graph.pb) to the TensorFlow Lite flatbuffer format (detect.tflite) and when i run the command that i specified above i get an error saying

```
ERROR: C:/users/lenovo-pc/tensorflow/tensorflow/contrib/lite/profiling/BUILD:37:1: C++ compilation of rule '//tensorflow/contrib/lite/profiling:time' failed (Exit 2)
tensorflow/contrib/lite/profiling/time.cc(17): fatal error C1083: Cannot open include file: 'sys/time.h': No such file or directory
Target //tensorflow/contrib/lite/toco:toco failed to build
```

### Source code / logs

C:\Users\LENOVO-PC\tensorflow> bazel run -c opt tensorflow/contrib/lite/toco:toco -- \ --input_file=maonani/tflite_graph.pb \ --output_file=maonani/detect.tflite \ --input_shapes=1,300,300,3 \ --input_arrays=normalized_input_image_tensor \ --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \ --inference_type=QUANTIZED_UINT8 \ --mean_values=128 \ --std_values=128 \ --change_concat_input_ranges=false \ --allow_custom_ops
INFO: Build options have changed, discarding analysis cache.
INFO: Analysed target //tensorflow/contrib/lite/toco:toco (60 packages loaded).
INFO: Found 1 target...
INFO: From Compiling external/protobuf_archive/src/google/protobuf/generated_message_reflection.cc [for host]:
external/protobuf_archive/src/google/protobuf/generated_message_reflection.cc(2429): warning C4506: no definition for inline function 'google::protobuf::Message *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::NewFromPrototype(const GenericType *,google::protobuf::Arena *)'
        with
        [
            GenericType=google::protobuf::Message
        ]
external/protobuf_archive/src/google/protobuf/generated_message_reflection.cc(2429): warning C4506: no definition for inline function 'google::protobuf::Arena *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::GetArena(GenericType *)'
        with
        [
            GenericType=google::protobuf::Message
        ]
external/protobuf_archive/src/google/protobuf/generated_message_reflection.cc(2429): warning C4506: no definition for inline function 'void *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::GetMaybeArenaPointer(GenericType *)'
        with
        [
            GenericType=google::protobuf::Message
        ]
INFO: From Compiling external/protobuf_archive/src/google/protobuf/map_field.cc [for host]:
external/protobuf_archive/src/google/protobuf/map_field.cc(465): warning C4506: no definition for inline function 'google::protobuf::Message *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::NewFromPrototype(const GenericType *,google::protobuf::Arena *)'
        with
        [
            GenericType=google::protobuf::Message
        ]
external/protobuf_archive/src/google/protobuf/map_field.cc(465): warning C4506: no definition for inline function 'google::protobuf::Arena *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::GetArena(GenericType *)'
        with
        [
            GenericType=google::protobuf::Message
        ]
external/protobuf_archive/src/google/protobuf/map_field.cc(465): warning C4506: no definition for inline function 'void *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::GetMaybeArenaPointer(GenericType *)'
        with
        [
            GenericType=google::protobuf::Message
        ]
INFO: From Compiling tensorflow/core/platform/default/logging.cc [for host]:
c:\users\lenovo-pc\_bazel_lenovo-pc\buizwhxs\execroot\org_tensorflow\tensorflow\core\platform\default\logging.cc(147) : warning C4722: 'tensorflow::internal::LogMessageFatal::~LogMessageFatal': destructor never returns, potential memory leak
INFO: From Compiling tensorflow/core/platform/default/logging.cc:
c:\users\lenovo-pc\_bazel_lenovo-pc\buizwhxs\execroot\org_tensorflow\tensorflow\core\platform\default\logging.cc(147) : warning C4722: 'tensorflow::internal::LogMessageFatal::~LogMessageFatal': destructor never returns, potential memory leak
INFO: From Compiling external/protobuf_archive/src/google/protobuf/generated_message_reflection.cc:
external/protobuf_archive/src/google/protobuf/generated_message_reflection.cc(2429): warning C4506: no definition for inline function 'google::protobuf::Message *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::NewFromPrototype(const GenericType *,google::protobuf::Arena *)'
        with
        [
            GenericType=google::protobuf::Message
        ]
external/protobuf_archive/src/google/protobuf/generated_message_reflection.cc(2429): warning C4506: no definition for inline function 'google::protobuf::Arena *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::GetArena(GenericType *)'
        with
        [
            GenericType=google::protobuf::Message
        ]
external/protobuf_archive/src/google/protobuf/generated_message_reflection.cc(2429): warning C4506: no definition for inline function 'void *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::GetMaybeArenaPointer(GenericType *)'
        with
        [
            GenericType=google::protobuf::Message
        ]
INFO: From Compiling external/protobuf_archive/src/google/protobuf/map_field.cc:
external/protobuf_archive/src/google/protobuf/map_field.cc(465): warning C4506: no definition for inline function 'google::protobuf::Message *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::NewFromPrototype(const GenericType *,google::protobuf::Arena *)'
        with
        [
            GenericType=google::protobuf::Message
        ]
external/protobuf_archive/src/google/protobuf/map_field.cc(465): warning C4506: no definition for inline function 'google::protobuf::Arena *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::GetArena(GenericType *)'
        with
        [
            GenericType=google::protobuf::Message
        ]
external/protobuf_archive/src/google/protobuf/map_field.cc(465): warning C4506: no definition for inline function 'void *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::GetMaybeArenaPointer(GenericType *)'
        with
        [
            GenericType=google::protobuf::Message
        ]
INFO: From Compiling external/flatbuffers/src/util.cpp [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/src/reflection.cpp [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/src/flatc.cpp [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/src/idl_parser.cpp [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/src/idl_gen_fbs.cpp [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/src/code_generators.cpp [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/grpc/src/compiler/go_generator.cc [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/src/idl_gen_python.cpp [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/src/idl_gen_php.cpp [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/src/idl_gen_js.cpp [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/src/idl_gen_cpp.cpp [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/grpc/src/compiler/cpp_generator.cc [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/src/idl_gen_go.cpp [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/src/idl_gen_json_schema.cpp [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/src/flatc_main.cpp [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/src/idl_gen_text.cpp [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/grpc/src/compiler/java_generator.cc [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
INFO: From Compiling external/flatbuffers/src/idl_gen_grpc.cpp [for host]:
cl : Command line warning D9002 : ignoring unknown option '-fexceptions'
ERROR: C:/users/lenovo-pc/tensorflow/tensorflow/contrib/lite/profiling/BUILD:37:1: C++ compilation of rule '//tensorflow/contrib/lite/profiling:time' failed (Exit 2)
tensorflow/contrib/lite/profiling/time.cc(17): fatal error C1083: Cannot open include file: 'sys/time.h': No such file or directory
Target //tensorflow/contrib/lite/toco:toco failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 287.186s, Critical Path: 15.02s
INFO: 295 processes: 295 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
"
21084,"Not found: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_12 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Reshape_6, stack_12)","

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.5
- **Python version**:2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:5.4
- **CUDA/cuDNN version**: cuda9.0 cudnn 7 nvidia- 384driver 
- **GPU model and memory**: 12G
- **Exact command to reproduce**:
python object_detection/model_main.py     --pipeline_config_path=${PIPELINE_CONFIG_PATH}     --model_dir=${MODEL_DIR}     --num_train_steps=${NUM_TRAIN_STEPS}     --num_eval_steps=${NUM_EVAL_STEPS}     --alsologtostderr
 TITANX  python object_detection/model_main.py \
>     --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
>     --model_dir=${MODEL_DIR} \
>     --num_train_steps=${NUM_TRAIN_STEPS} \
>     --num_eval_steps=${NUM_EVAL_STEPS} \
>     --alsologtostderr
WARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x7eff51d9a6e0>) includes params argument, but params are not passed to Estimator.
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /mnt/object_detection/core/box_predictor.py:407: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
WARNING:tensorflow:From /mnt/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2037: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.get_or_create_global_step
WARNING:tensorflow:From /mnt/object_detection/core/losses.py:317: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See tf.nn.softmax_cross_entropy_with_logits_v2.

WARNING:tensorflow:From /mnt/object_detection/meta_architectures/faster_rcnn_meta_arch.py:1930: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
2018-07-24 16:38:16.076136: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-07-24 16:38:16.529363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-07-24 16:38:16.532926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:00:06.0
totalMemory: 11.90GiB freeMemory: 11.75GiB
2018-07-24 16:38:16.532987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:00:06.0, compute capability: 6.1)
2018-07-24 16:38:34.429255: E tensorflow/core/common_runtime/executor.cc:651] Executor failed to create kernel. Not found: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_12 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Reshape_6, stack_12)
         (OpKernel was found, but attributes didn't match)
        .  Registered:  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT64]
"
21081,Tensort RT Engine Object Detection,"tensorflow.python.framework.errors_impl.ResourceExhaustedError: Requested batch size is not available and engine cache is full
	 [[Node: import/my_trt_op_16 = TRTEngineOp[InT=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[1], calibration_data="""", fixed_input_size=true, input_shapes=[[?,1], [?,1], [?,1], [?,1]], max_cached_engines_count=1, output_shapes=[[?,4]], precision_mode=""FP32"", segment_funcdef_name=""my_trt_op_16_native_segment"", serialized_segment=""\2007\000\...00\000\000"", static_engine=true, workspace_size_bytes=65075264, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](import/MultiClassNonMaxSuppression/ClipToWindow/split, import/MultiClassNonMaxSuppression/ClipToWindow/split:2, import/MultiClassNonMaxSuppression/ClipToWindow/split:1, import/MultiClassNonMaxSuppression/ClipToWindow/split:3)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[Node: import/SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField/strided_slice/_89 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_1900_...ided_slice"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info."
21077,Nasnet-mobile tfhub module issue.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Raspbian Stretch 9(frozen model user), Windows 10 (trainer)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Raspberry Pi 3 Model B
- **TensorFlow installed from (source or binary)**:Uhhh not sure.  I installed from the binary in June.
- **TensorFlow version (use command below)**: b'v1.9.0-0-g25c197e023' 1.9.0
- **Python version**:3.6.6 on Windows PC, 3.5.3 on Raspberry Pi
- **CUDA/cuDNN version**:9.0, 7.1.4
- **GPU model and memory**: GTX1060 6GB
- **Exact command to reproduce**:
python retrain.py --image_dir=""C:\fake\data\directory"" --learning_rate 0.0005 --how_many_training_steps=200 --tfhub_module=""https://tfhub.dev/google/imagenet/nasnet_mobile/classification/1"" --random_brightness 25 --flip_left_right --print_misclassified_test_images

I wrote a lot of custom code and used alot of custom data (Cue Debugger's facepalm) both of which I am not sure I can release because both are technically company property.  And this is my first Github submission, so I don't know what I need to mention or do.  Disclaimer/Excusing over.

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

When I produce the frozen model after training off of nasnet_mobile, I get a really large variable/parameter count of around 1100.  That's larger than a regular non-mobile friendly 800 variable count.  However, the model size was still a magnitude less than resnet-152.  It was about the same size as a typical tf-lite mobilenet v2 sized model though a little larger.  I decided to try the model out, by transferring it on a chimeric version of label_image.py.  Then, it gave the printouts nearly identical to any model.  After an exceptionally long time, it just prints out 

'Bus error'

The large variable count leads me to suspect that it's not being prepared for mobile properly and nothing to do with my modified code.  

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

I used a modified version of label_image.py and pretty much an exact copy of retrain.py in the image retraining tutorial.  I only changed the squeeze_dims() to axis() in the args for retrain.py.  Just to be clear.  I am running retrain.py on a Windows 10 PC with the aforementioned specs, then running label_image.py on a Raspberry Pi 3 Model B after flashdriving over the model and code.INFO:tensorflow:Initialize variable module/reduction_cell_1/beginning_bn/moving_mean:0 from checkpoint 
'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/beginning_bn/moving_mean
INFO:tensorflow:Initialize variable module/reduction_cell_1/beginning_bn/moving_variance:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/beginning_bn/moving_variance
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/beta:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/beta
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/gamma:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/gamma
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/moving_mean:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/moving_mean
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/moving_variance:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/moving_variance
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/beta:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/beta
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/gamma:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/gamma
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/moving_mean:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/moving_mean
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/moving_variance:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/moving_variance
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/separable_5x5_1/depthwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/left/separable_5x5_1/depthwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/separable_5x5_1/pointwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/left/separable_5x5_1/pointwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/separable_5x5_2/depthwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/left/separable_5x5_2/depthwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/separable_5x5_2/pointwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/left/separable_5x5_2/pointwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/beta:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/beta
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/gamma:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/gamma
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/moving_mean:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/moving_mean
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/moving_variance:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/moving_variance
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/beta:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/beta
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/gamma:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/gamma
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/moving_mean:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/moving_mean
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/moving_variance:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/moving_variance
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/separable_7x7_1/depthwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/right/separable_7x7_1/depthwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/separable_7x7_1/pointwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/right/separable_7x7_1/pointwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/separable_7x7_2/depthwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/right/separable_7x7_2/depthwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/separable_7x7_2/pointwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_0/right/separable_7x7_2/pointwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/beta:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/beta
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/gamma:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/gamma
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/moving_mean:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/moving_mean
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/moving_variance:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/moving_variance
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/beta:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/beta
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/gamma:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/gamma
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/moving_mean:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/moving_mean
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/moving_variance:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/moving_variance
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/separable_7x7_1/depthwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_1/right/separable_7x7_1/depthwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/separable_7x7_1/pointwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_1/right/separable_7x7_1/pointwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/separable_7x7_2/depthwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_1/right/separable_7x7_2/depthwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/separable_7x7_2/pointwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_1/right/separable_7x7_2/pointwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/beta:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/beta
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/gamma:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/gamma
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/moving_mean:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/moving_mean
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/moving_variance:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/moving_variance
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/beta:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/beta
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/gamma:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/gamma
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/moving_mean:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/moving_mean
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/moving_variance:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/moving_variance
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/separable_5x5_1/depthwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_2/right/separable_5x5_1/depthwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/separable_5x5_1/pointwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_2/right/separable_5x5_1/pointwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/separable_5x5_2/depthwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_2/right/separable_5x5_2/depthwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/separable_5x5_2/pointwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_2/right/separable_5x5_2/pointwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/beta:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/beta
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/gamma:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/gamma
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/moving_mean:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/moving_mean
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/moving_variance:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/moving_variance
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/beta:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/beta
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/gamma:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/gamma
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/moving_mean:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/moving_mean
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/moving_variance:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/moving_variance
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/separable_3x3_1/depthwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_4/left/separable_3x3_1/depthwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/separable_3x3_1/pointwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_4/left/separable_3x3_1/pointwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/separable_3x3_2/depthwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_4/left/separable_3x3_2/depthwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/separable_3x3_2/pointwise_weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/comb_iter_4/left/separable_3x3_2/pointwise_weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/prev_1x1/weights:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/prev_1x1/weights
INFO:tensorflow:Initialize variable module/reduction_cell_1/prev_bn/beta:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/prev_bn/beta
INFO:tensorflow:Initialize variable module/reduction_cell_1/prev_bn/gamma:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/prev_bn/gamma
INFO:tensorflow:Initialize variable module/reduction_cell_1/prev_bn/moving_mean:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/prev_bn/moving_mean
INFO:tensorflow:Initialize variable module/reduction_cell_1/prev_bn/moving_variance:0 from checkpoint b'C:\\Users\\wontae\\AppData\\Local\\Temp\\tfhub_modules\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\variables\\variables' with reduction_cell_1/prev_bn/moving_variance
2018-07-24 11:34:25.376617: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1471] Adding visible gpu devices: 0
2018-07-24 11:34:25.379309: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-24 11:34:25.382994: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:958]      0
2018-07-24 11:34:25.386328: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   N
2018-07-24 11:34:25.389811: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4734 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:09:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from /tmp/_retrain_checkpoint
INFO:tensorflow:Froze 1128 variables.
INFO:tensorflow:Converted 1128 variables to const ops.
"
21076,ERROR: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Exit 1),"Hi, I am trying to build TF from source. But I met an error which is listed below. Does anyone know how to fix it? Thanks a lot!

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.15.2
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.2 / 7.1
- **GPU model and memory**: GTX 1080Ti
- **Exact command to reproduce**: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`


### Source code / logs

```
INFO: From Compiling tensorflow/core/kernels/padding_fifo_queue.cc:
In file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,
                 from ./tensorflow/core/framework/resource_mgr.h:24,
                 from ./tensorflow/core/framework/queue_interface.h:23,
                 from ./tensorflow/core/kernels/queue_op.h:22,
                 from ./tensorflow/core/kernels/fifo_queue.h:26,
                 from ./tensorflow/core/kernels/padding_fifo_queue.h:27,
                 from tensorflow/core/kernels/padding_fifo_queue.cc:26:
./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':
./tensorflow/core/util/tensor_format.h:500:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {
                                             ^
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/core/refcount.h:22,
                 from ./tensorflow/core/platform/tensor_coding.h:21,
                 from ./tensorflow/core/framework/resource_handle.h:19,
                 from ./tensorflow/core/framework/register_types.h:21,
                 from tensorflow/core/kernels/padding_fifo_queue.cc:22:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int]':
./tensorflow/core/util/tensor_format.h:452:47:   required from here
./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attributes.size())
                             ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attributes.size())
   ^
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int]':
./tensorflow/core/util/tensor_format.h:461:54:   required from here
./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
                             ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
ERROR: /home/xxx/tensorflow-git/tensorflow/BUILD:581:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Exit 1)
Traceback (most recent call last):
  File ""/home/xxx/.cache/bazel/_bazel_root/10dd4b2d41234e5022064db22e215668/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/xxx/.cache/bazel/_bazel_root/10dd4b2d41234e5022064db22e215668/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/home/xxx/.cache/bazel/_bazel_root/10dd4b2d41234e5022064db22e215668/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""/usr/local/lib/python3.5/dist-packages/google/protobuf/__init__.py"", line 37, in <module>
    __import__('pkg_resources').declare_namespace(__name__)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2927, in <module>
    @_call_aside
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2913, in _call_aside
    f(*args, **kwargs)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2952, in _initialize_master_working_set
    add_activation_listener(lambda dist: dist.activate())
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 956, in subscribe
    callback(dist)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2952, in <lambda>
    add_activation_listener(lambda dist: dist.activate())
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2515, in activate
    declare_namespace(pkg)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2097, in declare_namespace
    _handle_ns(packageName, path_item)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2047, in _handle_ns
    _rebuild_mod_path(path, packageName, module)
  File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2066, in _rebuild_mod_path
    orig_path.sort(key=position_in_sys_path)
AttributeError: '_NamespacePath' object has no attribute 'sort'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 112.766s, Critical Path: 89.56s
INFO: 323 processes: 323 local.
FAILED: Build did NOT complete successfully
```
"
21074,ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory,"I have tensorflow-gpu 1.9 installed for python3.5 system environment. I have cuda 9.0 installed. I do not have any other versions of cuda installed. I verified that libcublas.so.9.0 is located in /usr/local/cuda-9.0/lib64. I added the following to my ~/.bashrc file, as per the NVIDIA docs:
```
export PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
```
I still get this error. Please help. Thank you.

ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory
"
21070,Documentation request: tf.contrib.data.AUTOTUNE,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.9
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: tf.contrib.data.AUTOTUNE

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
According to release notes for 1.8, there is a new parameter called tf.contrib.data.AUTOTUNE. It's functionality is described in release notes as follows:
 'Add tf.contrib.data.AUTOTUNE, which allows the tf.data runtime to automatically tune the prefetch buffer sizes based on your system and environment.'
But I couldn't find any other documentation associated with it in [tf.dataset ](https://www.tensorflow.org/api_docs/python/tf/data)

I used it in my pipeline as shown below and it didn't give any errors. But I have no idea how it works and which circumstances I should use it. Please provide some documentation on how to use this feature.

### Source code / logs
```
#Make dataset for training
dataset_train = tf.data.Dataset.from_tensor_slices((file_ids_training,file_names_training))
dataset_train = dataset_train.flat_map(lambda file_id,file_name: tf.data.Dataset.from_tensor_slices(
    tuple (tf.py_func(_get_data_for_dataset, [file_id,file_name], [tf.float32,tf.float32]))))

dataset_train= dataset_train.shuffle(buffer_size=train_buffer_size)
dataset_train= dataset_train.repeat()
dataset_train= dataset_train.batch(train_batch_size) #Make dataset, shuffle, and create batches
dataset_train = dataset_train.prefetch(tf.contrib.data.AUTOTUNE)

dataset_train_iterator = dataset_train.make_one_shot_iterator()
get_train_batch = dataset_train_iterator.get_next()
```"
21069,writing tfrecord with multithreading is not fast as expected,"System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04 and MacOS High Sierra 10.13.4
TensorFlow installed from (source or binary): docker and virtualenv
TensorFlow version (use command below): v1.8.0-0-g93bc2e2072
Python version: 2.7.12
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce: N/A

### Describe the problem
Tried to write tfrecord w/ and w/o multithreading, and found the speed difference is not much (w/ 4 threads: 434 seconds; w/o multithread 590 seconds).  Not sure if I used it correctly. Is there any better way to write tfrecord faster?

### Source code / logs

```
import tensorflow as tf 
import numpy as np 
import threading 
import time 


def generate_data(shape=[15,28,60,1]):
	return np.random.uniform(size=shape)


def _bytes_feature(value):
	return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))


def _int64_feature(value):
	return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))


def write_instances_to_tfrecord(tfrecord_file, filenames):
	tfrecord_writer = tf.python_io.TFRecordWriter(tfrecord_file)
	for i, filename in enumerate(filenames):
		curr_MFCC = generate_data()
		curr_MFCC_raw = curr_MFCC.tostring()
		curr_filename_raw = str(filename)+'-'+str(i)
		example = tf.train.Example(features=tf.train.Features(
			feature={
			'MFCC': _bytes_feature(curr_MFCC_raw),
			'filename': _bytes_feature(curr_filename_raw)
			})
		)
		tfrecord_writer.write(example.SerializeToString())
	tfrecord_writer.close()


def test():
	threading_start = time.time()
	coord = tf.train.Coordinator()
	threads = []
	for thread_index in xrange(4):
		args = (str(thread_index), range(200000))
		t = threading.Thread(target=write_instances_to_tfrecord, args=args)
		t.start()
		threads.append(t)
	coord.join(threads)
	print 'w/ threading takes', time.time()-threading_start

	start = time.time()
	write_instances_to_tfrecord('5', range(800000))
	print 'w/o threading takes', time.time()-start

if __name__ == '__main__':
	test()
```
"
21067,AttributeError: 'Series' object has no attribute 'columns',"Hi,

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
NO
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Hugh Sierra
TensorFlow installed from (source or binary): I did pip install tensor flow
TensorFlow version (use command below): v1.9.0-0-g25c197e023 1.9.0
Python version: 3.6
Bazel version (if compiling from source):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:
Exact command to reproduce:
Please find below the error am having.


/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
updated
Performing function: predict
on outcome: continuous_static_outcome_occupancy
Predicting for date 2017-01-01 00:00:00
Traceback (most recent call last):
  File ""./src/NeirbiLSTM-occupancy.py"", line 52, in <module>
    predict.run()
  File ""/Users/jasonachonu/git/SeqHub_Summer2018/src/nblib/predict.py"", line 51, in run
    [rawdata, var_type,var_timing,var_use,key] = preprocess.preprocess(rawdata,checks=False)
  File ""/Users/jasonachonu/git/SeqHub_Summer2018/src/nblib/preprocess.py"", line 29, in preprocess
    meta_data = np.core.defchararray.split(np.asarray(rawdata.columns).astype(str),sep=""_"")
  File ""/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py"", line 4372, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'Series' object has no attribute 'columns'"
21062,Quantize nodes with Graph Transform tool,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a
- **TensorFlow installed from (source or binary)**:bazel
- **TensorFlow version (use command below)**:latest commit
- **Python version**:2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:0.14.1
- **CUDA/cuDNN version**:9/7.1
- **GPU model and memory**:1080ti
- **Exact command to reproduce**:
`
bazel build tensorflow/tools/graph_transforms:transform_graph
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=model.pb \
--out_graph=optimized_model.pb \
--inputs='Input/input' \
--outputs='fc2/Relu' \
--transforms='
  add_default_attributes
  remove_nodes(op=Identity, op=CheckNumerics)
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms
  quantize_weights
  quantize_nodes
  strip_unused_nodes
  sort_by_execution_order'
`


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

ValueError: Node 'conv2/act_quant/AssignMinEma/conv2/act_quant/min/AssignAdd/value' expects to be colocated with unknown node 'conv2/act_quant/min'

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21060,Fixed Point Quantization on GPU,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:n/a
- **GCC/Compiler version (if compiling from source)**:n/a
- **CUDA/cuDNN version**:9/7.1
- **GPU model and memory**:1080ti
- **Exact command to reproduce**: n/a

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I want to run a fixed point quantized model on a GPU, but looking at the documentation of TF lite, it only allows you to run the flat buffer file on ios, android or x86 cpu. Is there a way to utilize the quantization speedup on a GPU ?

https://www.tensorflow.org/performance/quantization

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
21059,there is still an isue with cuda 9.1,the problem in isue CUDA 9.1 Support #18961 is still unsolved is the any good workaround
21058,Mobilenet v1 with cifar10 unexpected behavior ,"
### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:NA
- **TensorFlow installed from (source or binary)**:SOURCE
- **TensorFlow version (use command below)**:1.8
- **Python version**:3.5

Hi,

I am using mobilenet_v1_eval code, instead of the imagenet dataset, i change the data to cifar10 training from scratch. The only change for the architecture is 1st conv layer with stride 1 instead of 2.


_CONV_DEFS = [
    Conv(kernel=[3, 3], stride=1, depth=32),
    DepthSepConv(kernel=[3, 3], stride=1, depth=64),
    DepthSepConv(kernel=[3, 3], stride=2, depth=128),
    DepthSepConv(kernel=[3, 3], stride=1, depth=128),
    DepthSepConv(kernel=[3, 3], stride=2, depth=256),
    DepthSepConv(kernel=[3, 3], stride=1, depth=256),
    DepthSepConv(kernel=[3, 3], stride=2, depth=512),
    DepthSepConv(kernel=[3, 3], stride=1, depth=512),
    DepthSepConv(kernel=[3, 3], stride=1, depth=512),
    DepthSepConv(kernel=[3, 3], stride=1, depth=512),
    DepthSepConv(kernel=[3, 3], stride=1, depth=512),
    DepthSepConv(kernel=[3, 3], stride=1, depth=512),
    DepthSepConv(kernel=[3, 3], stride=2, depth=1024),
    DepthSepConv(kernel=[3, 3], stride=1, depth=1024)
] 

Training was no problem, loss decrease and prediction seems good. But in the evaluation, I use the same code as the mobilenet_v1_eval with input data as cifar10, I am gettting the same output for each image I pass in to the model. I have double checked the my input is definitely different every time, but it is very weird to get an exact same output for different images.

[[-0.11333117 -0.5380551   0.18907356  0.7664664   0.07711207  0.04618246
   0.13568665  0.1360816  -0.36744678 -0.33176792]]
[[-0.11333117 -0.5380551   0.18907356  0.7664664   0.07711207  0.04618246
   0.13568665  0.1360816  -0.36744678 -0.33176792]]
[[-0.11333118 -0.5380551   0.18907356  0.7664664   0.07711206  0.04618246
   0.13568665  0.1360816  -0.36744678 -0.33176792]]
[[-0.11333118 -0.5380551   0.18907356  0.7664664   0.07711206  0.04618246
   0.13568665  0.1360816  -0.36744678 -0.33176792]]
[[-0.11333118 -0.5380551   0.18907356  0.7664664   0.07711206  0.04618246
   0.13568665  0.1360816  -0.36744678 -0.33176792]]

Please help, any suggestion will be helpful! Thank you in advance!
"
21057,Introduce support for tf.float16 in tf.contrib.image,"Hi,

Would be nice to use these helper functions with tf.float16 tensors to avoid having to cast them back.

https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/image/python/ops/image_ops.py





"
21056,"Build failed on windows 10 MSB3073: :VCEnd"" exited with code 1","
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64 bit
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: Visual Studio 2017
- **CUDA/cuDNN version**: 9.2/7.1
- **GPU model and memory**: GTX 1080 8GB
- **Exact command to reproduce**: MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj

### Describe the problem
Build failed with error MSB6006: ""cmd.exe"" exited with code 1. Before that, there is an ModuleNotFound Error: No module named 'tensorflow.python.tools.api' while running create_python_api, even though I do find it from the specified location.

### Source code / logs
```
Task Parameter:BuildSuffix=
                     :VCEnd (TaskId:7050)
                     Task Parameter:TrackerLogDirectory=x64\Release\estimator_python_api\estimato.8A00678D.tlog\ (TaskId:7050)
                     Task Parameter:MinimalRebuildFromTracking=True (TaskId:7050)
                     Task Parameter:TrackFileAccess=True (TaskId:7050)
                     Task Parameter:ToolArchitecture=Native32Bit (TaskId:7050)
                     Write Tracking Logs: (TaskId:7050)
                     	x64\Release\estimator_python_api\estimato.8A00678D.tlog\custombuild.write.1.tlog (TaskId:7050)
                     Read Tracking Logs: (TaskId:7050)
                     	x64\Release\estimator_python_api\estimato.8A00678D.tlog\custombuild.read.1.tlog (TaskId:7050)
                     No output for C:\WORK\LIBRARIES_LOCAL\TENSORFLOW\TENSORFLOW\CONTRIB\CMAKE\BUILD\CMAKEFILES\C75366AEF4FA6332EF71118C646F7D1C\__INIT__.PY.RULE was found in the tracking log; source compilation required. (TaskId:7050)
                     C:\work\libraries_local\tensorflow\tensorflow\contrib\cmake\build\CMakeFiles\c75366aef4fa6332ef71118c646f7d1c\__init__.py.rule will be compiled because it was not found in the tracking log. (TaskId:7050)
                     setlocal
                     cd C:\work\libraries_local\tensorflow\tensorflow\contrib\cmake\build\tf_python
                     if %errorlevel% neq 0 goto :cmEnd
                     C:
                     if %errorlevel% neq 0 goto :cmEnd
                     C:\work\tools\cmake-3.12.0-win32-x86\bin\cmake.exe -E env PYTHONPATH=C:/work/libraries_local/tensorflow/tensorflow/contrib/cmake/build/tf_python C:/Anaconda3/envs/python36/python.exe C:/work/libraries_local/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/python/tools/api/generator/create_python_api.py --apidir=C:/work/libraries_local/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/python/estimator/api --package=tensorflow.python.estimator --apiname=estimator --output_package=tensorflow.python.estimator.api C:/work/libraries_local/tensorflow/estimator_api_init_files_list.txt
                     if %errorlevel% neq 0 goto :cmEnd
                     :cmEnd
                     endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone
                     :cmErrorLevel
                     exit /b %1
                     :cmDone
                     if %errorlevel% neq 0 goto :VCEnd (TaskId:7050)
                     Generating __init__.py files for Python API. (TaskId:7050)
                     Traceback (most recent call last): (TaskId:7050)
                       File ""C:/work/libraries_local/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module> (TaskId:7050)
                         from tensorflow.python.tools.api.generator import doc_srcs (TaskId:7050)
                     ModuleNotFoundError: No module named 'tensorflow.python.tools.api' (TaskId:7050)
15:52:23.015     2>C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\VC\VCTargets\Microsoft.CppCommon.targets(171,5): error MSB6006: ""cmd.exe"" exited with code 1. [C:\work\libraries_local\tensorflow\tensorflow\contrib\cmake\build\estimator_python_api.vcxproj]
                   Done executing task ""CustomBuild"" -- FAILED. (TaskId:7050)
15:52:23.016     2>Done building target ""CustomBuild"" in project ""estimator_python_api.vcxproj"" -- FAILED.: (TargetId:18239)
15:52:23.016     2>Done Building Project ""C:\work\libraries_local\tensorflow\tensorflow\contrib\cmake\build\estimator_python_api.vcxproj"" (default targets) -- FAILED.
15:52:23.016     1>Done executing task ""MSBuild"" -- FAILED. (TaskId:8)
15:52:23.016     1>Done building target ""ResolveProjectReferences"" in project ""tf_python_build_pip_package.vcxproj"" -- FAILED.: (TargetId:12)
15:52:23.016     1>Done Building Project ""c:\work\libraries_local\tensorflow\tensorflow\contrib\cmake\build\tf_python_build_pip_package.vcxproj"" (default targets) -- FAILED.
```
"
21055,Can not install tensorflow,"Hello, 

I am new to python and tensorflow. I have downloaded PyCharm student version and working in virtual env with python 3.5

I am trying to install tensorflow gpu. I am getting following error:
""Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: '/home/smst/PycharmProjects/Project_Trials/venv/lib/python3.5/site-packages/tfp_nightly-0.0.1.dev20180629.dist-info/METADATA'""

not just tensorflow. for that matter I not able to install any package.tfp nightly is neither getting installed nor uninstalled. I am stuck. 

GPU : NVIDIA TITAN X

cat /usr/local/cuda-9.2/include/cudnn.h | grep CUDNN_MAJOR -A 2
#define CUDNN_MAJOR 7
#define CUDNN_MINOR 1
#define CUDNN_PATCHLEVEL 4

The above shows where cuda is installed and cudnn version.

Please help 


"
21054,gru_ops.so not found,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No using code provided on the website. Here it is the link to it.
https://www.tensorflow.org/versions/r1.0/get_started/mnist/beginners



- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:

- **TensorFlow version (use command below)**:
- **Python version**:
3.5.5
- **GCC/Compiler version (if compiling from source)**:
Using Anaconda and Jupyter Notebook
- **CUDA/cuDNN version**:
CUDA 9.0
cuDNN 7.0
- **GPU model and memory**:
Nvidia GEFORCE 940M
- **Exact command to reproduce**:


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**### Describe the problem**
I am trying to run MNIST (TensorFlow) code basic hand-drawn digit recognition on the Jupyter Notebook. When I run the first cell of it I just give me an error. In the first cell, I actually try to import MNISTR data file this is the error I am getting.

**This is the code:**
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)

**Error:**
NotFoundError                             Traceback (most recent call last)
<ipython-input-15-8bf8ae5a5303> in <module>()
----> 1 from tensorflow.examples.tutorials.mnist import input_data
      2 mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)

~\Anaconda3\envs\codcus\lib\site-packages\tensorflow\examples\tutorials\mnist\__init__.py in <module>()
     19 from __future__ import print_function
     20 
---> 21 from tensorflow.examples.tutorials.mnist import input_data
     22 from tensorflow.examples.tutorials.mnist import mnist

~\Anaconda3\envs\codcus\lib\site-packages\tensorflow\examples\tutorials\mnist\input_data.py in <module>()
     28 from six.moves import xrange  # pylint: disable=redefined-builtin
     29 import tensorflow as tf
---> 30 from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets
     31 # pylint: enable=unused-import

~\Anaconda3\envs\codcus\lib\site-packages\tensorflow\contrib\__init__.py in <module>()
     31 from tensorflow.contrib import copy_graph
     32 from tensorflow.contrib import crf
---> 33 from tensorflow.contrib import cudnn_rnn
     34 from tensorflow.contrib import data
     35 from tensorflow.contrib import deprecated

~\Anaconda3\envs\codcus\lib\site-packages\tensorflow\contrib\cudnn_rnn\__init__.py in <module>()
     32 
     33 # pylint: disable=unused-import,wildcard-import
---> 34 from tensorflow.contrib.cudnn_rnn.python.layers import *
     35 # pylint: enable=unused-import,wildcard-import
     36 

~\Anaconda3\envs\codcus\lib\site-packages\tensorflow\contrib\cudnn_rnn\python\layers\__init__.py in <module>()
     21 
     22 # pylint: disable=unused-import,wildcard-import
---> 23 from tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn import *
     24 # pylint: enable=unused-import,wildcard-import
     25 

~\Anaconda3\envs\codcus\lib\site-packages\tensorflow\contrib\cudnn_rnn\python\layers\cudnn_rnn.py in <module>()
     18 from __future__ import print_function
     19 
---> 20 from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops
     21 from tensorflow.python.framework import dtypes
     22 from tensorflow.python.framework import ops

~\Anaconda3\envs\codcus\lib\site-packages\tensorflow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py in <module>()
     19 
     20 from tensorflow.contrib.eager.python import checkpointable_utils
---> 21 from tensorflow.contrib.rnn.python.ops import lstm_ops
     22 from tensorflow.python.framework import common_shapes
     23 from tensorflow.python.framework import dtypes

~\Anaconda3\envs\codcus\lib\site-packages\tensorflow\contrib\rnn\__init__.py in <module>()
     86 
     87 from tensorflow.contrib.rnn.python.ops.fused_rnn_cell import *
---> 88 from tensorflow.contrib.rnn.python.ops.gru_ops import *
     89 from tensorflow.contrib.rnn.python.ops.lstm_ops import *
     90 from tensorflow.contrib.rnn.python.ops.rnn import *

~\Anaconda3\envs\codcus\lib\site-packages\tensorflow\contrib\rnn\python\ops\gru_ops.py in <module>()
     31 
     32 _gru_ops_so = loader.load_op_library(
---> 33     resource_loader.get_path_to_datafile(""_gru_ops.so""))
     34 
     35 LayerRNNCell = rnn_cell_impl.LayerRNNCell  # pylint: disable=invalid-name

~\Anaconda3\envs\codcus\lib\site-packages\tensorflow\contrib\util\loader.py in load_op_library(path)
     54       return None
     55   path = resource_loader.get_path_to_datafile(path)
---> 56   ret = load_library.load_op_library(path)
     57   assert ret, 'Could not load %s' % path
     58   return ret

~\Anaconda3\envs\codcus\lib\site-packages\tensorflow\python\framework\load_library.py in load_op_library(library_filename)
     54     RuntimeError: when unable to load the library or get the python wrappers.
     55   """"""
---> 56   lib_handle = py_tf.TF_LoadLibrary(library_filename)
     57 
     58   op_list_str = py_tf.TF_GetOpList(lib_handle)

NotFoundError: C:\Users\Abdul\Anaconda3\envs\codcus\lib\site-packages\tensorflow\contrib\rnn\python\ops\_gru_ops.so not found
"
21053,Session.run() on Operation return None. Design question,"- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes, see this issue. https://github.com/tensorflow/tensorflow/issues/20990

The documentation of Session.run from tensorflow:
```
run(
    fetches,
    feed_dict=None,
    options=None,
    run_metadata=None
)
```
> Runs operations and evaluates tensors in fetches.
> 
> This method runs one ""step"" of TensorFlow computation, by running the necessary graph fragment to execute every Operation and evaluate every Tensor in fetches, substituting the values in feed_dict for the corresponding input values.
> 
> The fetches argument may be a single graph element, or an arbitrarily nested list, tuple, namedtuple, dict, or OrderedDict containing graph elements at its leaves. A graph element can be one of the following types:
> 
>     **An tf.Operation. The corresponding fetched value will be None.**
>     A tf.Tensor. The corresponding fetched value will be a numpy ndarray containing the value of that tensor.
>     A tf.SparseTensor. The corresponding fetched value will be a tf.SparseTensorValue containing the value of that sparse tensor.
>     A get_tensor_handle op. The corresponding fetched value will be a numpy ndarray containing the handle of that tensor.
>     A string which is the name of a tensor or operation in the graph.

I was stuck for quite a while on this issue, due to a` sess.run() `returning `None` all the time and not raising any exception.
I don't quite get the design intention under such a behaviour:
`Session.run()` is expected to return values. But it return `None` for `tf.operation`
Not raising any exception, nor outputing any information make you think it is okay to call `sess.run()` on your parameter and that the issue is not on this function call.
Why is it okay to allow user to call this method with parameter that will result in no effect?
Also, as `tf.Operation` is a tensorflow type, wouldn't it be possible to automatically call for its `.values()` in the `Session.run()` instead of returning a `None`?
"
21051,Doc: ctc_beam_search_decoder,"The [TF documentation ](https://www.tensorflow.org/api_docs/python/tf/nn/ctc_beam_search_decoder) says: _""Note The ctc_greedy_decoder is a special case of the ctc_beam_search_decoder with top_paths=1 and beam_width=1 (but that decoder is faster for this special case).""_

This implies that the result of the greedy and beam decoder is equal if beam width is set to 1.

However, this is not the case. Even with beam width set to 1, beam search uses more information by distinguishing paths ending with blank and non-blank.
Therefore, it is possible to create inputs from which beam search can extract enough information to produce the correct result, while best path decoding fails.

Here is such an input matrix with 3 time-steps. It contains a label '0' and the blank '-'. The best path ""0-0"" is marked by a red line:
![matrix_best](https://user-images.githubusercontent.com/15148095/43075266-908f4740-8e80-11e8-85a5-91e9ffd08015.png)

 


And here is the code which produces different results, depending on the decoder:

```
import tensorflow as tf
import numpy as np


batchSize=1
numClasses=2
numTimesteps=3


def createGraph():
	""create tensors""
	tinputs=tf.placeholder(tf.float32, [numTimesteps, batchSize, numClasses])
	tseqLen=tf.placeholder(tf.int32, [None]) # list of sequence length in batch
	tbeam=tf.nn.ctc_beam_search_decoder(tinputs, tseqLen, beam_width=1, merge_repeated=False)
	tbest=tf.nn.ctc_greedy_decoder(tinputs, tseqLen, merge_repeated=True)

	return (tinputs, tseqLen, tbeam, tbest)


def getData():
	""get data matrix of size TxBxC with T=3, B=1 and C=2 (one label + blank)""
	seqLen=[numTimesteps]
	inputs=np.asarray([ [[0.51, 0.49]], [[0.49, 0.51]], [[0.51, 0.49]] ], np.float32)
	return (inputs, seqLen) 


def toLabelString(decoderOutput):
	""map sparse tensor from decoder to label string""
	decoded=decoderOutput[0][0]
	idxDict={b:[] for b in range(batchSize)}
	encodedLabels=[[] for i in range(batchSize)]
	for (idxVal, idx2d) in enumerate(decoded.indices):
		value=decoded.values[idxVal]
		batch=idx2d[0]
		encodedLabels[batch].append(value)

	return encodedLabels[0]

def main():
	# initialize
	(tinputs, tseqLen, tbeam, tbest)=createGraph()
	sess=tf.Session()
	sess.run(tf.global_variables_initializer())

	# compute decoded result
	(inputs, seqLen)=getData()
	[retBeam, retBest]=sess.run([tbeam, tbest], {tinputs:inputs, tseqLen:seqLen } )
	print('Beam Search Decoding :', toLabelString(retBeam))
	print('Best Path Decoding   :', toLabelString(retBest))


if __name__ == '__main__':
	main()
```


Output:
```
Beam Search Decoding : [0]
Best Path Decoding   : [0, 0]
```

---

Have I written custom code: code see above
OS Platform and Distribution: N/A
TensorFlow installed from: N/A
TensorFlow version: tested with 1.3 and 1.6
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: code see above, doc is linked
Mobile device: N/A
"
21048,"High loss of accuracy after coverting "".pb"" to "".lite"" on Android","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I only modified the ImageClassifier.java to make it compatible with float and quant.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.04.3 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
LG G4
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.7.1
- **Python version**:2.7.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:

### Describe the problem
Hi,
I retrained a module refer to [tensorflow-for-poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0), and its retrain.py was replaced by [this one](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/examples/image_retraining/retrain.py).

I did two experiments:

**1. Choose the module: mobilenet_1.0_224_quant**
I retrained the module with the command below:

> python -m scripts.retrain \
  --architecture=mobilenet_1.0_224_quant \
  --bottleneck_dir=tf_files/bottlenecks \
  --how_many_training_steps=500 \
  --model_dir=tf_files/models/ \
  --summaries_dir=tf_files/training_summaries/mobilenet_1.0_224_quant \
  --output_graph=tf_files/retrained_graph.pb \
  --output_labels=tf_files/retrained_labels.txt \
  --image_dir=tf_files/flower_photos

Then I converted "".pb"" to "".lite"" with this command:

> toco \
  --input_file=tf_files/retrained_graph.pb \
  --output_file=tf_files/optimized_graph_quant.lite \
  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \
  --inference_type=QUANTIZED_UINT8 \
  --input_shape=""1,224,224,3"" \
  --input_array=input \
  --output_array=final_result \
  --std_value=128 --mean_value=128

I tested _optimized_graph_quant.lite_ with the _TfLiteCameraDemo_ which provided by tensorflow-for-poets.
And only once inference, I found that the accuracy is very good:

> 01-01 01:12:37.942 12117 12217 D TfLiteCameraDemo: Timecost to run model inference: 354
01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: textToShow = 354ms
01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: roses: 1.00
01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: sunflowers: 0.00
01-01 01:12:37.946 12117 12217 D TfLiteCameraDemo: tulips: 0.00


**2. Choose the module: mobilenet_1.0_224**
I retrained the module with the command below:

> python -m scripts.retrain \
  --architecture=mobilenet_1.0_224 \
  --bottleneck_dir=tf_files/bottlenecks \
  --how_many_training_steps=500 \
  --model_dir=tf_files/models/ \
  --summaries_dir=tf_files/training_summaries/mobilenet_1.0_224 \
  --output_graph=tf_files/retrained_graph.pb \
  --output_labels=tf_files/retrained_labels.txt \
  --image_dir=tf_files/flower_photos

Then I converted "".pb"" to "".lite"" with this command:

> toco \
  --input_file=tf_files/retrained_graph.pb \
  --output_file=tf_files/optimized_graph.lite \
  --input_format=TENSORFLOW_GRAPHDEF \
  --output_format=TFLITE \
  --input_shape=""1,224,224,3"" \
  --input_array=input \
  --output_array=final_result \
  --inference_type=FLOAT \
  --input_data_type=FLOAT

I also tested optimized_graph.lite with the _TfLiteCameraDemo_ which provided by tensorflow-for-poets.
And after multiple inferences, the accuracy increased to an acceptable value :

> 01-01 01:29:23.221 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 510
01-01 01:29:23.226 13207 13224 D TfLiteCameraDemo: roses: 0.06
01-01 01:29:23.483 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 238
01-01 01:29:23.488 13207 13224 D TfLiteCameraDemo: roses: 0.18
01-01 01:29:23.741 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 221
01-01 01:29:23.746 13207 13224 D TfLiteCameraDemo: roses: 0.31
01-01 01:29:24.162 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 398
01-01 01:29:24.167 13207 13224 D TfLiteCameraDemo: roses: 0.45
01-01 01:29:24.527 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 336
01-01 01:29:24.533 13207 13224 D TfLiteCameraDemo: roses: 0.58
01-01 01:29:24.898 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 339
01-01 01:29:24.907 13207 13224 D TfLiteCameraDemo: roses: 0.68
01-01 01:29:25.274 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 326
01-01 01:29:25.280 13207 13224 D TfLiteCameraDemo: roses: 0.76
01-01 01:29:25.646 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 340
01-01 01:29:25.652 13207 13224 D TfLiteCameraDemo: roses: 0.83
01-01 01:29:26.032 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 354
01-01 01:29:26.041 13207 13224 D TfLiteCameraDemo: roses: 0.87
01-01 01:29:26.433 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 365
01-01 01:29:26.438 13207 13224 D TfLiteCameraDemo: roses: 0.91
01-01 01:29:26.831 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 365
01-01 01:29:26.838 13207 13224 D TfLiteCameraDemo: roses: 0.93
01-01 01:29:27.288 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 391
01-01 01:29:27.296 13207 13224 D TfLiteCameraDemo: roses: 0.95
01-01 01:29:27.658 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 333
01-01 01:29:27.663 13207 13224 D TfLiteCameraDemo: roses: 0.97
01-01 01:29:28.005 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 313
01-01 01:29:28.010 13207 13224 D TfLiteCameraDemo: roses: 0.97
01-01 01:29:28.381 13207 13224 D TfLiteCameraDemo: Timecost to run model inference: 341
01-01 01:29:28.385 13207 13224 D TfLiteCameraDemo: roses: 0.98

**Please help to analyze that why it needs multiple inferences to get an good accuracy here? Thanks!**
"
21045,Building TensorFlow failed,"### Issue
I'm trying to build the .so file using bazel
`bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a`  
Getting an error:

```
tensorflow/core/BUILD:2306:1: C++ compilation of rule '//tensorflow/core:protos_all_proto_cc_impl' failed (Exit 1)
In file included from bazel-out/armeabi-v7a-opt/genfiles/tensorflow/core/framework/tensor_shape.pb.cc:4:
In file included from bazel-out/armeabi-v7a-opt/genfiles/tensorflow/core/framework/tensor_shape.pb.h:9:
In file included from external/protobuf_archive/src/google/protobuf/stubs/common.h:52:
In file included from external/protobuf_archive/src/google/protobuf/stubs/mutex.h:33:
In file included from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/mutex:35:
external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/c++0x_warning.h:32:2: error: This file requires compiler and library support for the ISO C++ 2011 standard. This support is currently experimental, and must be enabled with the -std=c++11 or -std=gnu++11 compiler options.
#error This file requires compiler and library support for the \
 ^
In file included from bazel-out/armeabi-v7a-opt/genfiles/tensorflow/core/framework/tensor_shape.pb.cc:4:
In file included from bazel-out/armeabi-v7a-opt/genfiles/tensorflow/core/framework/tensor_shape.pb.h:9:
In file included from external/protobuf_archive/src/google/protobuf/stubs/common.h:52:
external/protobuf_archive/src/google/protobuf/stubs/mutex.h:58:8: error: no type named 'mutex' in namespace 'std'
  std::mutex mu_;

In file included from bazel-out/armeabi-v7a-opt/genfiles/tensorflow/core/framework/tensor_shape.pb.cc:4:
In file included from bazel-out/armeabi-v7a-opt/genfiles/tensorflow/core/framework/tensor_shape.pb.h:9:
In file included from external/protobuf_archive/src/google/protobuf/stubs/common.h:53:
external/protobuf_archive/src/google/protobuf/stubs/callback.h:346:25: error: no type named 'remove_reference' in namespace 'std'
  typedef typename std::remove_reference<T>::type base_type;
 
external/protobuf_archive/src/google/protobuf/stubs/callback.h:346:41: error: expected member name or ';' after declaration specifiers
  typedef typename std::remove_reference<T>::type base_type;

external/protobuf_archive/src/google/protobuf/stubs/callback.h:347:17: error: unknown type name 'base_type'
  typedef const base_type& type;
                
external/protobuf_archive/src/google/protobuf/stubs/callback.h:401:17: error: no type named 'remove_reference' in namespace 'std'
  typename std::remove_reference<P1>::type p1_;

external/protobuf_archive/src/google/protobuf/stubs/callback.h:401:33: error: expected member name or ';' after declaration specifiers
  typename std::remove_reference<P1>::type p1_;

external/protobuf_archive/src/google/protobuf/stubs/callback.h:402:17: error: no type named 'remove_reference' in namespace 'std'
  typename std::remove_reference<P2>::type p2_;

external/protobuf_archive/src/google/protobuf/stubs/callback.h:402:33: error: expected member name or ';' after declaration specifiers
  typename std::remove_reference<P2>::type p2_;

external/protobuf_archive/src/google/protobuf/stubs/callback.h:403:17: error: no type named 'remove_reference' in namespace 'std'
  typename std::remove_reference<P3>::type p3_;

external/protobuf_archive/src/google/protobuf/stubs/callback.h:403:33: error: expected member name or ';' after declaration specifiers
  typename std::remove_reference<P3>::type p3_;

external/protobuf_archive/src/google/protobuf/stubs/callback.h:404:17: error: no type named 'remove_reference' in namespace 'std'
  typename std::remove_reference<P4>::type p4_;

external/protobuf_archive/src/google/protobuf/stubs/callback.h:404:33: error: expected member name or ';' after declaration specifiers
  typename std::remove_reference<P4>::type p4_;

external/protobuf_archive/src/google/protobuf/stubs/callback.h:405:17: error: no type named 'remove_reference' in namespace 'std'
  typename std::remove_reference<P5>::type p5_;

external/protobuf_archive/src/google/protobuf/stubs/callback.h:405:33: error: expected member name or ';' after declaration specifiers
  typename std::remove_reference<P5>::type p5_;

external/protobuf_archive/src/google/protobuf/stubs/callback.h:383:9: error: member initializer 'p1_' does not name a non-static data member or base class
        p1_(p1),

external/protobuf_archive/src/google/protobuf/stubs/callback.h:384:9: error: member initializer 'p2_' does not name a non-static data member or base class
        p2_(p2),

external/protobuf_archive/src/google/protobuf/stubs/callback.h:385:9: error: member initializer 'p3_' does not name a non-static data member or base class
        p3_(p3),

external/protobuf_archive/src/google/protobuf/stubs/callback.h:386:9: error: member initializer 'p4_' does not name a non-static data member or base class
        p4_(p4)
```

### System information
OS:
 macOS High Sierra 10.13.3

bazel: 
`Build label: 0.15.0-homebrew
Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Jun 26 12:42:27 2018 (1530016947)
Build timestamp: 1530016947
Build timestamp as int: 1530016947`

ndk:
android-ndk-r16b

I looked similar problems, like [#3924](https://github.com/bazelbuild/bazel/issues/3924) or [#17046](https://github.com/tensorflow/tensorflow/issues/17046), but unfortunately did not find a solution for himself. 

Thank you in advance.
"
21044,"Reproducing when setting 'max_to_keep=0', then the get_checkpoint_state(ckpt_dir).all_model_checkpoint_paths only record the recent 1 model path.","I produced the closed issue #13381, below is my model saving code snippets:

```python
 def do_training(self):
        saver = tf.train.Saver(max_to_keep = 0)

        epoch_size = (os.path.getsize(self.config.data_path) - 1) // self.config.batch_size // self.config.max_length
        resume_ckpt_dir = self.config.resume_ckpt_dir
        with tf.Session() as sess:
            restored = False
            start_epoch = 0
            if resume_ckpt_dir:
                ckpt = tf.train.get_checkpoint_state(resume_ckpt_dir)
                if ckpt and ckpt.model_checkpoint_path:
                    saver.restore(sess, ckpt.model_checkpoint_path)
                    restored = True
                    print ('Restore from %s ... ' % ckpt.model_checkpoint_path )
                    resume_global_step = sess.run(self.model.global_step)
                    start_epoch = resume_global_step // epoch_size
                    print ('Restore global step is %d' % resume_global_step)
            if not restored:    
                sess.run(tf.global_variables_initializer())
            
            writer = tf.summary.FileWriter(self.config.summary_path, sess.graph)

            for epoch in range(start_epoch, start_epoch + self.config.num_epochs):
                gs, compl = self.run_epoch(sess, epoch, writer)
                
                if epoch % self.config.save_epoch_interval == 0:
                    print ('Save model at global step (%d) / Epoch (%d)' % (gs, epoch))
                    saver.save(sess, os.path.join(self.config.save_ckpt_dir, 'ckpt'), global_step = gs)
                if compl:
                    print (""Val loss has reach the expected entropy within %f"" % self.config.comp_stop_width)
                    if (self.config.comp_stop):
                        break
            writer.close()
```

The I run the follow code:
![image](https://user-images.githubusercontent.com/12935189/43059769-a6292a3a-8e80-11e8-914b-287082387e71.png)

I saved the model every 39 iterations, but I can only see the most recent 1 in all_model_checkpoint_paths

My versions:
1. tensorflow-gpu         1.3.0
2. Ubuntu 16.04.4 LTS"
21041,tensorflow_lite  Prelu unsupport dims->size = 2,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes, I write own model define, loss and training codes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: iPhone 6s
- **TensorFlow installed from (source or binary)**: binary, pip install
- **TensorFlow version (use command below)**:1.3.0
- **Python version**:3.5
- **Bazel version (if compiling from source)**:0.10.0
- **GCC/Compiler version (if compiling from source)**: 4.9.4
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**:GeForce GTX 1060
- **Exact command to reproduce**:
### Describe the problem
 In tensorflow_lite demo tflite_simple_example,I use my Rnet.tflite ,I get error as follow
  if (interpreter->AllocateTensors() != kTfLiteOk) {
    NSLog(@""Failed to allocate tensors."");
    exit(-1);
  }
error:tensorflow/contrib/lite/kernels/activations.cc:171 input->dims->size != 4 (2 != 4)

In activations.cc I find  PreluPrepare()  TF_LITE_ENSURE_EQ(context, input->dims->size, 4); my  input->dims->size =2

  TfLiteStatus PreluPrepare(TfLiteContext* context, TfLiteNode* node) {
  TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);
  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  const TfLiteTensor* input = GetInput(context, node, 0);
  TfLiteTensor* output = GetOutput(context, node, 0);
  const TfLiteTensor* alpha = GetInput(context, node, 1);

  output->type = input->type;

  // Currently only Float32 is supported
  // TODO(ycling): Support other data types.
  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteFloat32);
  TF_LITE_ENSURE_EQ(context, alpha->type, kTfLiteFloat32);

  // Currently, only support 4D `input` and 3D `alpha` with shape
  // (1, 1, channels).
  // TODO(impjdi): Support other cases where `alpha` is broadcastable
  // to `input`.
  TF_LITE_ENSURE_EQ(context, input->dims->size, 4);
  TF_LITE_ENSURE_EQ(context, alpha->dims->size, 3);
  TF_LITE_ENSURE_EQ(context, alpha->dims->data[0], 1);
  TF_LITE_ENSURE_EQ(context, alpha->dims->data[1], 1);
  TF_LITE_ENSURE_EQ(context, alpha->dims->data[2], input->dims->data[3]);

  return context->ResizeTensor(context, output,
                               TfLiteIntArrayCopy(input->dims));
}

### Source code / logs
input = Input(shape=[24, 24, 3],batch_shape=[1,24, 24, 3])
x = Conv2D(28, (3, 3), strides=1, padding='valid', name='conv1')(input)
x = PReLU(shared_axes=[1, 2], name='prelu1')(x)
x = MaxPool2D(pool_size=3,strides=2, padding='same')(x)

x = Conv2D(48, (3, 3), strides=1, padding='valid', name='conv2')(x)
x = PReLU(shared_axes=[1, 2], name='prelu2')(x)
x = MaxPool2D(pool_size=3, strides=2)(x)

x = Conv2D(64, (2, 2), strides=1, padding='valid', name='conv3')(x)
x = PReLU(shared_axes=[1, 2], name='prelu3')(x)
x = Permute((3, 2, 1))(x)
x = Flatten()(x)
x = Dense(128, name='conv4')(x)
x = PReLU( name='prelu4')(x)
classifier = Dense(2, activation='softmax', name='conv5-1')(x)
bbox_regress = Dense(4, name='conv5-2')(x)
model = Model([input], [classifier, bbox_regress])
model.load_weights(weight_path, by_name=True)


x = PReLU( name='prelu4')(x)    input x->dims->size =2

How can I slove this problem?
"
21040,How to calculate backward pass with feeding custom gradients.,"Currently I am replacing Pythorch's code with tensorflow implementation code.

The reference code has the following line.
#####
XXX.backward(grad_output, retain_graph=True)
YYY = input.grad.detach().sum(1).clone().clamp(min=0)
#####
XXX corresponds to ResNet's N*N*C map before fc layers.
In this code, the directly defined gradient is entered instead of the loss value in the calculation of the backward path.

How can I describe this to implement with tensorflow?"
21037,tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory,"can anyone help me??

Traceback (most recent call last):
  File ""/Users/meow/generate_tfrecord.py"", line 99, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/Users/meow/generate_tfrecord.py"", line 85, in main
    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/lib/io/tf_record.py"", line 112, in __init__
    compat.as_bytes(path), compat.as_bytes(compression_type), status)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 519, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory"
21036,No temporal sample weight in metrics,"Hi,

the sample weight option is ignored for the metrics. There seems to be a weights option for the metrics, but that does not seem to be able to take a 2D tensor into account for the temporal sample weight mode.

Best,
Noshaba"
21034,`get_weights` will not cover extra variables for subclassed `tf.keras.Model` ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: nightly 07 22
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: None
- **GPU model and memory**: None
- **Exact command to reproduce**: None

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

`get_weights` will not cover extra variables for subclassed `tf.keras.Model` 


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf
from tensorflow.python.ops.resource_variable_ops import ResourceVariable


class Model(tf.keras.Model):
    def __init__(self):
        super(Model, self).__init__()
        self.d = tf.keras.layers.Dense(2, use_bias=False)
        self.v = ResourceVariable([1.])

    def call(self, inputs, training=True, mask=None):
        return self.v + self.d(inputs)

m = Model()
o = m(tf.random_uniform((3, 4)))
# only one element(m.d.kernel), m.v is ignored
print(m.get_weights())
```

Extra variables will be ignored by `Model.get_weights` and `Model.set_weights` is it a bug or intention?

if it is a bug, I would like to send a PR to fix this issue.
"
21033,‘tf.train.shuffle_batch’ problem,"When I train my face_detection model，I set 0 as my non-face label ,and 1 as face label .After building TFRecord dataset，I use 'image_batch, label_batch = tf.train.shuffle_batch ' ,it  is clear  that ‘image_batch ’ corresponds to ‘label_batch’   .However,I found that ‘image_batch ’ is not corresponding to ‘label_batch’. some non-face pictures'label is 1,and some face pictures' label is 0,why?
here is my code:`import tensorflow as tf
import time
import matplotlib.pyplot as plt


begin=time.time()
filename_queue = tf.train.string_input_producer(['C:\\Users\\312\\Desktop\\Face_Detection\\ai.tfrecords'],shuffle=False)

reader = tf.TFRecordReader()
_, serialized_example = reader.read(filename_queue)

features = tf.parse_single_example(serialized_example,
                               features={
                               'label':tf.FixedLenFeature([],tf.int64),
                               'img_raw':tf.FixedLenFeature([],tf.string),
                               })
img=tf.decode_raw(features['img_raw'],tf.uint8)
img = tf.reshape(img, [227,227,3])
label=tf.cast(features['label'],tf.int32)

min_after_dequeue = 200
batch_size = 128
capacity = min_after_dequeue + 10 * batch_size
image_batch, label_batch = tf.train.batch([img, label], batch_size=batch_size, capacity=capacity)

image_batch, label_batch = tf.train.shuffle_batch(
      [img, label],
      batch_size=32,
      num_threads=7,
      capacity=500,
      min_after_dequeue=200)

with tf.Session() as sess:
    sess.run((tf.global_variables_initializer(),
              tf.local_variables_initializer()))
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess,coord=coord)
    for i in range(1): 
        label=sess.run(label)
        label_batch=sess.run(label_batch)
        image_batch=sess.run(image_batch) 

        for j in range(8):
            plt.imshow(image_batch[j])
            print(label_batch[j])
            plt.show()
    coord.request_stop()
    coord.join(threads)
print(time.time()-begin)
    `"
21032,Got different matrix eigenvalues by tensorflow.self_adjoint_eig(A) than by numpy.linalg.eig(A),"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: None
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.5
- **TensorFlow installed from (source or binary)**: anaconda
- **TensorFlow version (use command below)**: 1.1.0
- **Python version**: Python 3.6.5 :: Anaconda, Inc.
- **GCC/Compiler version (if compiling from source)**: [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
- **CUDA/cuDNN version**: None 

### Describe the problem
Got different matrix eigenvalues by tensorflow.self_adjoint_eig(A) than by numpy.linalg.eig(A) when A is
[[2., 3.], 
[2., 1.]]

### Source code / logs
```python 
import os
import tensorflow as tf 
import numpy as np 
sess = tf.Session()
A = np.array([[2., 3.],[2., 1.]])
e1,v1=np.linalg.eig(A) 
print(e1)

B = tf.convert_to_tensor(A, dtype=tf.float64)
e2,v2 = sess.run(tf.self_adjoint_eig(B))
print(e2)
```

#### output
```
[ 4. -1.]
[-0.56155281  3.56155281]
```
"
21030,Real time object detection api using tensorflow in android BUT where to change the NDK and SDK version on workspace?,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: none
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.9
- **Python version**:  3.5.1-32 bit
- **Bazel version (if compiling from source)**: 0.15.0
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: nothing since i have AMD radeon
- **GPU model and memory**:
- **Exact command to reproduce**:
### Describe the problem
Im following this tutorial [https://www.skcript.com/svr/realtime-object-and-face-detection-in-android-using-tensorflow-object-detection-api/](url) since i want to make a realtime detection using tensorflow and when i got to the step where changing the SDK and NDK version in tensorflow workspace file, i couldnt find similar codes like `Uncomment and update the paths in these entries to build the Android demo.
android_sdk_repository(
    name = ""androidsdk"",
    api_level = 23,
    build_tools_version = ""25.0.1"",
     Replace with path to Android SDK on your system
    path = ""<PATH_TO_SDK>"",
)

android_ndk_repository(
    name=""androidndk"",
   path=""<PATH_TO_NDK>"",
    api_level=14)` im not sure how the new implementation on android works since it keeps on changing the repository for tensorflow. should i just ignore changing the versions and continue following the steps after it?

"
21029,TF 1.8-gpu runs on only one Ubuntu 18.04 account,"About six months ago I succeeded in installing TF 1.4.0-gpu on top of Ubuntu 17.10.  I have multiple user accounts on the system, and I needed all accounts to have TF access.  TF needed to be accessible whether the Python script was launched from a terminal or from a GUI.  The procedure that worked for me was documented [here](https://ubuntuforums.org/showthread.php?t=2386704).

I decided to upgrade to Ubuntu 18.04 and TF 1.8.0-gpu.  The installation procedure I followed was essentially the same as shown above.  My primary account has full access to TF, and is functioning normally.  However, the secondary account (which has admin privileges) is unable to load TF.  I get this:

> Python 3.6.5 (default, Apr  1 2018, 05:46:30) 
> [GCC 7.3.0] on linux
> Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
> ˃˃˃ import tensorflow as tf
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
>   File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
>     return load_dynamic(name, filename, file)
>   File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
>     return _load(spec)
> ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py"", line 24, in <module>
>     from tensorflow.python import *
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
>     from tensorflow.python import pywrap_tensorflow
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 72, in <module>
>     raise ImportError(msg)
> ImportError: Traceback (most recent call last):
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
>   File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
>     return load_dynamic(name, filename, file)
>   File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
>     return _load(spec)
> ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory
> 
> 
> Failed to load the native TensorFlow runtime.
> 
> See https://www.tensorflow.org/install/install_sources#common_installation_problems
> 
> for some common reasons and solutions.  Include the entire stack trace
> above this error message when asking for help.

I searched for libcublas.so* files on my system.  The screenshot of the search result is attached.  I don't have 8.0 files.  I do have 9.0 and 9.1 files, I'm not sure how I ended up with both.  I have what appear to be manual pages with a version 7 suffix.

My main account is not looking for libcublas.so.8.0.  My secondary account is.  Any thoughts as to why?  I reviewed my .bashrc and .profile in each account for discrepancies.  I have none.  The modifications that I make to /etc/profile and ldconfig should be shared between all accounts on the system."
21026,tried to convert t to a tensor and failed error none values not supported,"When I want to set a trainable variable, and use 
```
alpha = tf.Variable(initial_value=1.0)
beta= tf.Variable(initial_value=1.0)

loss = alpha*loss_1 + beta*loss_2
```
and use it with a gradient clip, problem happened,
errors reported like: 
```
tried to convert t to a tensor and failed error none values not supported
```
What should I do?"
