Issue Number,Issue Title,Issue Body
57859,ValueError: Python inputs incompatible with input_signature:,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuda 11.0.3/cudnn 8

### GPU model and memory

yes

### Current Behaviour?

```shell
A bug happened!
ValueError: Python inputs incompatible with input_signature:
      inputs: (
        Tensor(""args_0:0"", shape=(), dtype=string))
      input_signature: (
        TensorSpec(shape=(None,), dtype=tf.float32, name=None)).
```


### Standalone code to reproduce the issue

```shell
# Using yamnet transfer learning model for sound classification

yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'
yamnet_model = hub.load(yamnet_model_handle)

filenames = filtered_pd['Filename']
targets = filtered_pd['Target']
split = filtered_pd['Split']

main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, split))
print(main_ds.element_spec)

#@tf.function
def load_wav_16k_mono(filename):
    try:
        file_contents = tf.io.read_file(filename)
        wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)
        wav = tf.squeeze(wav, axis=-1)
        sample_rate = tf.cast(sample_rate, dtype=tf.int64)
        wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)
        return wav
    except Exception as e:
        print(filename)
        return str(e)

def load_wav_for_map(filename, label, split):
    print(filename)
    return load_wav_16k_mono(filename), label, split

main_ds = main_ds.map(load_wav_for_map)
#print([x for x in main_ds])

def extract_embedding(wav_data, label, split):
    scores, embeddings, spectrogram = yamnet_model(wav_data)
    num_embeddings = tf.shape(embeddings)[0]
    return (embeddings,
            tf.repeat(label, num_embeddings),
            tf.repeat(split, num_embeddings))

# extract embedding
main_ds = main_ds.map(extract_embedding).unbatch()
main_ds.element_spec

cached_ds = main_ds.cache()
train_ds = cached_ds.filter(lambda embedding, label, split: split == 'train')
val_ds = cached_ds.filter(lambda embedding, label, split: split == 'val')
```


### Relevant log output

```shell
(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))
Tensor(""args_0:0"", shape=(), dtype=string)
WARNING:tensorflow:AutoGraph could not transform <function resample at 0x7f2700550430> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: unable to open file: libtensorflow_io.so, from paths: ['/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']
caused by: ['/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZNK10tensorflow10FileSystem8BasenameESt17basic_string_viewIcSt11char_traitsIcEE']
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function resample at 0x7f2700550430> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: unable to open file: libtensorflow_io.so, from paths: ['/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']
caused by: ['/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZNK10tensorflow10FileSystem8BasenameESt17basic_string_viewIcSt11char_traitsIcEE']
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Tensor(""args_0:0"", shape=(), dtype=string)
Cleaning up all outstanding Run operations, waiting 300.0 seconds
1 items cleaning up...
Cleanup took 0.5498418807983398 seconds
Traceback (most recent call last):
  File ""uc3_train_data_prep.py"", line 187, in <module>
    main_ds = main_ds.map(extract_embedding).unbatch()
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2048, in map
    return MapDataset(self, map_func, preserve_cardinality=True, name=name)
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 5243, in __init__
    self._map_func = structured_function.StructuredFunctionWrapper(
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py"", line 271, in __init__
    self._function = fn_factory()
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2567, in get_concrete_function
    graph_function = self._get_concrete_function_garbage_collected(
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2533, in _get_concrete_function_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2711, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2627, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1141, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py"", line 248, in wrapped_fn
    ret = wrapper_helper(*args)
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py"", line 177, in wrapper_helper
    ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py"", line 692, in wrapper
    raise e.ag_error_metadata.to_exception(e)
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py"", line 689, in wrapper
    return converted_call(f, args, kwargs, options=options)
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py"", line 439, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/__autograph_generated_filedlxifabg.py"", line 10, in tf__extract_embedding
    (scores, embeddings, spectrogram) = ag__.converted_call(ag__.ld(yamnet_model), (ag__.ld(wav_data),), None, fscope)
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py"", line 377, in converted_call
    return _call_unconverted(f, args, kwargs, options)
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py"", line 459, in _call_unconverted
    return f(*args)
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 686, in _call_attribute
    return instance.__call__(*args, **kwargs)
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/azureml-envs/azureml_0231a8f3aec1f5ab1653fe92c14fe093/lib/python3.8/site-packages/tensorflow/python/eager/function_spec.py"", line 531, in _convert_inputs_to_signature
    raise ValueError(""Python inputs incompatible with input_signature:\n""
ValueError: in user code:

    File ""uc3_train_data_prep.py"", line 177, in extract_embedding  *
        scores, embeddings, spectrogram = yamnet_model(wav_data)

    ValueError: Python inputs incompatible with input_signature:
      inputs: (
        Tensor(""args_0:0"", shape=(), dtype=string))
      input_signature: (
        TensorSpec(shape=(None,), dtype=tf.float32, name=None)).
```
</details>"
57858,Clarify functionality of deserialized subclassed models in docs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Source

binary

### Tensorflow Version

tf 2.9 and 2.10

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In the `SavedModel` format documentation [here](https://www.tensorflow.org/guide/keras/save_and_serialize#how_savedmodel_handles_custom_objects), it says:


> In the absence of the model/layer config, the call function is used to create a model that exists like the original model which can be trained, evaluated, and used for inference.

and:

> The first loaded model is loaded using the config and CustomModel class. The second model is loaded by dynamically creating the model class that acts like the original model.

I am finding that when a subclassed model is saved in the `SavedModel` format, upon re-loading it can be trained, evaluated, and used for inference in the same way as the original model. However, other functionality such as the custom `from_config` and `get_config` methods do not work. For example, attempting to clone the loaded model with `loaded_model_clone = loaded_model.from_config(loaded_model.get_config())` leads to errors.

I assume this is because loading a `CustomModel` without passing `custom_objects` (or registering it) gives a `keras.saving.saved_model.load.CustomModel`, which does not contain the full functionality of the original `CustomModel`? It would be helpful if the docs clarified the functional limitations of a `SavedModel` loaded without the `custom_objects` being provided.

The documentation (and example) in [SavedModel format](https://www.tensorflow.org/guide/keras/save_and_serialize#savedmodel_format) also seems to contradict the guidance in [Custom Objects](https://www.tensorflow.org/guide/keras/save_and_serialize#custom_objects). In the former, it suggests a custom `SavedModel` can be loaded without providing the `custom_objects`, albeit with limited functionality, whereas the latter suggests you should always register or pass in the `custom_objects` (i.e. ""_Additionally, you should use register the custom object so that Keras is aware of it._"").


### Standalone code to reproduce the issue

Extended from example in [SavedModel format](https://www.tensorflow.org/guide/keras/save_and_serialize#savedmodel_format).

Colab notebook gist: https://gist.github.com/ascillitoe/42702d4cab382880b55ab7344c694a23

Code:

```python
import tensorflow as tf
from tensorflow import keras
import numpy as np

class CustomModel(keras.Model):
    def __init__(self, hidden_units):
        #super(CustomModel, self).__init__()
        super().__init__()
        self.hidden_units = hidden_units
        self.dense_layers = [keras.layers.Dense(u) for u in hidden_units]

    def call(self, inputs):
        x = inputs
        for layer in self.dense_layers:
            x = layer(x)
        return x

    def get_config(self):
        return {""hidden_units"": self.hidden_units}

    @classmethod
    def from_config(cls, config):
        return cls(**config)


model = CustomModel([16, 16, 10])
# Build the model by calling it
input_arr = tf.random.uniform((1, 5))
outputs = model(input_arr)
model.save(""my_model"")

# Option 1: Load with the custom_object argument.
loaded_1 = keras.models.load_model(
    ""my_model"", custom_objects={""CustomModel"": CustomModel}
)

# Option 2: Load without the CustomModel class.

# Delete the custom-defined model class to ensure that the loader does not have
# access to it.
del CustomModel

loaded_2 = keras.models.load_model(""my_model"")
np.testing.assert_allclose(loaded_1(input_arr), outputs)
np.testing.assert_allclose(loaded_2(input_arr), outputs)

print(""\nInitial Save/Load\n################################################"")
print(""Original model:"", model)
print(""Model Loaded with custom objects:"", loaded_1)
print(""Model loaded without the custom object class:"", loaded_2)

print(""\nCloning\n################################################"")
cloned_loaded_1 = loaded_1.__class__.from_config(loaded_1.get_config())
print(""Clone of Model Loaded with custom objects:"", cloned_loaded_1)

cloned_loaded_2 = loaded_2.__class__.from_config(loaded_2.get_config())
print(""Clone of Model loaded without the custom object class:"", cloned_loaded_2)
```


### Relevant log output

```shell
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.

Initial Save/Load
###################################
Original model: <__main__.CustomModel object at 0x7fbf613e1950>
Model Loaded with custom objects: <__main__.CustomModel object at 0x7fbf60f34e10>
Model loaded without the custom object class: <keras.saving.saved_model.load.CustomModel object at 0x7fbf60ea7b10>

Cloning
################################################
Clone of Model Loaded with custom objects: <__main__.CustomModel object at 0x7fbf613e89d0>
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in from_config(cls, config, custom_objects)
   3049                 try:
-> 3050                     model = cls(**config)
   3051                 except TypeError as e:

4 frames
TypeError: ('Keyword argument not understood:', 'hidden_units')

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in from_config(cls, config, custom_objects)
   3051                 except TypeError as e:
   3052                     raise TypeError(
-> 3053                         ""Unable to revive model from config. When overriding ""
   3054                         ""the `get_config()`, make sure that the returned ""
   3055                         ""config contains all items used as arguments in the ""

TypeError: Unable to revive model from config. When overriding the `get_config()`, make sure that the returned config contains all items used as arguments in the constructor to <class 'keras.saving.saved_model.load.CustomModel'>, which is the default behavior. You can override this default behavior by defining a `from_config` method to specify how to create an instance of CustomModel from the config. 

Error encountered during deserialization:
('Keyword argument not understood:', 'hidden_units')
```
</details>"
57857,how can i get node name,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

ubuntu 18.04

### Mobile device

ubuntu 18.04

### Python version

3.6

### Bazel version

3.7

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I can use node_id to get node, use context->GetNodeAndRegistration(context, node_id, &node, &reg)).But i cant not get node_name in node ,how can i get node name
```


### Standalone code to reproduce the issue

```shell
I dont have test case
```


### Relevant log output

_No response_</details>"
57856,Tensorflow nor working in rasperrypi 4,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.4.0

### Custom Code

Yes

### OS Platform and Distribution

Rasbian os

### Mobile device

Ubuntu 18.04

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Tensorflow not getting installed with rasbian os 32 bit ,rasperrypi4- tried to install tensorflow using .whl, virtual environment, git clone but still not able install/import .please help.
```


### Standalone code to reproduce the issue

```shell
ImportError                               Traceback (most recent call last)
File ~/.local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py:58
     56   sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_LOCAL)
---> 58 from tensorflow.python.pywrap_tensorflow_internal import *
     59 from tensorflow.python.pywrap_tensorflow_internal import __version__

File ~/.local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:28
     27         return _mod
---> 28 _pywrap_tensorflow_internal = swig_import_helper()
     29 del swig_import_helper

File ~/.local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:24, in swig_import_helper()
     23 try:
---> 24     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25 finally:

File /usr/lib/python3.9/imp.py:242, in load_module(name, file, filename, details)
    241     else:
--> 242         return load_dynamic(name, filename, file)
    243 elif type_ == PKG_DIRECTORY:

File /usr/lib/python3.9/imp.py:342, in load_dynamic(name, path, file)
    340 spec = importlib.machinery.ModuleSpec(
    341     name=name, loader=loader, origin=path)
--> 342 return _load(spec)

ImportError: /home/pi/.local/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
File <timed exec>:21

File ~/.local/lib/python3.9/site-packages/tensorflow/__init__.py:24
     21 import os as _os
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     26 try:
     27   # Add `estimator` attribute to allow access to estimator APIs via
     28   # ""tf.estimator...""
     29   from tensorflow.python.estimator.api import estimator  # pylint: disable=g-import-not-at-top

File ~/.local/lib/python3.9/site-packages/tensorflow/python/__init__.py:49
     32 # TODO(drpng): write up instructions for editing this file in a doc and point to
     33 # the doc instead.
     34 # If you want to edit this file to expose modules in public tensorflow API, you
   (...)
     44 # go/tf-wildcard-import
     45 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
     47 import numpy as np
---> 49 from tensorflow.python import pywrap_tensorflow
     51 from tensorflow.python.tools import component_api_helper
     52 component_api_helper.package_hook(
     53     parent_package_str='tensorflow.python',
     54     child_package_str=(
     55         'tensorflow_estimator.python.estimator'))

File ~/.local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py:74
     69 except ImportError:
     70   msg = """"""%s\n\nFailed to load the native TensorFlow runtime.\n
     71 See https://www.tensorflow.org/install/errors\n
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""/home/pi/.local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/pi/.local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/pi/.local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.9/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.9/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /home/pi/.local/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header
```


### Relevant log output

```shell
ImportError                               Traceback (most recent call last)
File ~/.local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py:58
     56   sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_LOCAL)
---> 58 from tensorflow.python.pywrap_tensorflow_internal import *
     59 from tensorflow.python.pywrap_tensorflow_internal import __version__

File ~/.local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:28
     27         return _mod
---> 28 _pywrap_tensorflow_internal = swig_import_helper()
     29 del swig_import_helper

File ~/.local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:24, in swig_import_helper()
     23 try:
---> 24     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25 finally:

File /usr/lib/python3.9/imp.py:242, in load_module(name, file, filename, details)
    241     else:
--> 242         return load_dynamic(name, filename, file)
    243 elif type_ == PKG_DIRECTORY:

File /usr/lib/python3.9/imp.py:342, in load_dynamic(name, path, file)
    340 spec = importlib.machinery.ModuleSpec(
    341     name=name, loader=loader, origin=path)
--> 342 return _load(spec)

ImportError: /home/pi/.local/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
File <timed exec>:21

File ~/.local/lib/python3.9/site-packages/tensorflow/__init__.py:24
     21 import os as _os
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     26 try:
     27   # Add `estimator` attribute to allow access to estimator APIs via
     28   # ""tf.estimator...""
     29   from tensorflow.python.estimator.api import estimator  # pylint: disable=g-import-not-at-top

File ~/.local/lib/python3.9/site-packages/tensorflow/python/__init__.py:49
     32 # TODO(drpng): write up instructions for editing this file in a doc and point to
     33 # the doc instead.
     34 # If you want to edit this file to expose modules in public tensorflow API, you
   (...)
     44 # go/tf-wildcard-import
     45 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
     47 import numpy as np
---> 49 from tensorflow.python import pywrap_tensorflow
     51 from tensorflow.python.tools import component_api_helper
     52 component_api_helper.package_hook(
     53     parent_package_str='tensorflow.python',
     54     child_package_str=(
     55         'tensorflow_estimator.python.estimator'))

File ~/.local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py:74
     69 except ImportError:
     70   msg = """"""%s\n\nFailed to load the native TensorFlow runtime.\n
     71 See https://www.tensorflow.org/install/errors\n
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""/home/pi/.local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/pi/.local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/pi/.local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.9/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.9/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: /home/pi/.local/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header
```
</details>"
57853,[Keras] `Conv2DTranspose` constfolding issue with variable batch size.,"### TF Version
TF 2.10

## Gist of the issue

The `Conv2DTranspose` does not behave the same when batch_size is fixed (e.g. batch_size=32) and when it's variable (e.g. batch_size=None). The problem seems to reside in the way `constant_folding` is working or in the way `input_sizes` is managed (which in this context is actually meaning the output shape).

## Google Colab for Repro

Full repro case in detail: https://colab.research.google.com/drive/1sv3c8KqcK1lcUKmvJAAQheSU9VPiJH6N?usp=sharing

## A picture is worth a thousands word

### 1. Let's create the model

Picture the most basic model using `Conv2DTranspose`

```python
def create_model(batch_size=None):
  inputs = keras.layers.Input((128, 128, 3), dtype=tf.float32, batch_size=batch_size)

  net = keras.layers.Conv2DTranspose(filters=32, kernel_size=(3, 3))(inputs)

  model = keras.models.Model(inputs=inputs, outputs=net)
  return model
```

**Fixed BS:**
![image](https://user-images.githubusercontent.com/10923599/192428905-7ac02c41-7acf-4ff1-b254-d511c5fcb860.png)

**Variable BS:**
![image](https://user-images.githubusercontent.com/10923599/192428961-49602fab-72f8-432e-997a-976cdc0ae359.png)

So far so good, everything looks the same ...

### 2. Freeze + ConstFold + Plot

Now let's freeze the graph and plot the resulting graphdef:

**Fixed BS:**
![image](https://user-images.githubusercontent.com/10923599/192429189-04d2f615-a486-46ac-929e-1fd447470b2d.png)

**Variable BS:**
![image](https://user-images.githubusercontent.com/10923599/192429232-7315ce94-ad2a-4a7a-b718-36ac58428d97.png)

## Conclusion

I hope the difference is fairly self explanatory, and that's a problem because that's a very different graph for inference and a lot more complicated graph to optimize by compilers (e.g. TF-TRT, XLA, TF Lite, TVM, etc).

The `input_sizes` tensor **should** have been const folded, it's perfectly independent of input batch size. There's no reason to the best of my knowledge that the const folding behavior changes by changing the BS value. 

My guess is that the shape tensor is seen as variable because of one value being None, however the None dimension is not being used and therefore the slice of the Shape tensor is perfectly static in this case and should have been const folded.

## To go further

It should not be runtime computed when the shape are perfectly static, we should not need to constfold this graph in the first place. This could be perfectly computed in python when all the shapes are defined. This is very expensive to compute over and over at each iteration even in training.

A very similar issue was fixed with a different layer here, we should consider a similar fix here: https://github.com/tensorflow/tensorflow/pull/31450/files
"
57847,Custom Op failure due to Status symbol resolution in docker image tensorflow/tensorflow:2.10.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

No

### OS Platform and Distribution

host: ubuntu 20.04, docker image: tensorflow/tensorflow:2.10.0-gpu 

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Attempting to use a custom operator that works with the tensorflow docker image tag (tensorflow/tensorflow:2.9.1-gpu) no longer works with image tag (tensorflow/tensorflow:2.10.0-gpu) due to a undefined symbol error:

tensorflow.python.framework.errors_impl.NotFoundError: ./example.so: undefined symbol: _ZN10tensorflow6StatusC1ENS_5error4CodeEN4absl12lts_2022062311string_viewENS_14SourceLocationE

This seems to have something to do with the deprecation notice in the 2.10 release notes regarding a transition in Status becoming an alias for Abseil Status.
```


### Standalone code to reproduce the issue

```shell
1) Copy the two c++ files (kernel_example.h and kernel_example.cc) from the custom op guide (https://www.tensorflow.org/guide/create_op):


// kernel_example.h
#ifndef KERNEL_EXAMPLE_H_
#define KERNEL_EXAMPLE_H_

#include <unsupported/Eigen/CXX11/Tensor>

template <typename Device, typename T>
struct ExampleFunctor {
  void operator()(const Device& d, int size, const T* in, T* out);
};

#if GOOGLE_CUDA
// Partially specialize functor for GpuDevice.
template <typename T>
struct ExampleFunctor<Eigen::GpuDevice, T> {
  void operator()(const Eigen::GpuDevice& d, int size, const T* in, T* out);
};
#endif

#endif KERNEL_EXAMPLE_H_
```

```
// kernel_example.cc
#include ""kernel_example.h""

#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""

using namespace tensorflow;

using CPUDevice = Eigen::ThreadPoolDevice;
using GPUDevice = Eigen::GpuDevice;

REGISTER_OP(""Example"")
    .Attr(""T: numbertype"")
    .Input(""input: T"")
    .Output(""input_times_two: T"")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      c->set_output(0, c->input(0));
      return Status::OK();
    });

// CPU specialization of actual computation.
template <typename T>
struct ExampleFunctor<CPUDevice, T> {
  void operator()(const CPUDevice& d, int size, const T* in, T* out) {
    for (int i = 0; i < size; ++i) {
      out[i] = 2 * in[i];
    }
  }
};

// OpKernel definition.
// template parameter <T> is the datatype of the tensors.
template <typename Device, typename T>
class ExampleOp : public OpKernel {
 public:
  explicit ExampleOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& input_tensor = context->input(0);

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                                                     &output_tensor));

    // Do the computation.
    OP_REQUIRES(context, input_tensor.NumElements() <= tensorflow::kint32max,
                errors::InvalidArgument(""Too many elements in tensor""));
    ExampleFunctor<Device, T>()(
        context->eigen_device<Device>(),
        static_cast<int>(input_tensor.NumElements()),
        input_tensor.flat<T>().data(),
        output_tensor->flat<T>().data());
  }
};

// Register the CPU kernels.
#define REGISTER_CPU(T)                                          \
  REGISTER_KERNEL_BUILDER(                                       \
      Name(""Example"").Device(DEVICE_CPU).TypeConstraint<T>(""T""), \
      ExampleOp<CPUDevice, T>);
REGISTER_CPU(float);
REGISTER_CPU(int32);

// Register the GPU kernels.
#ifdef GOOGLE_CUDA
#define REGISTER_GPU(T)                                          \
  /* Declare explicit instantiations in kernel_example.cu.cc. */ \
  extern template class ExampleFunctor<GPUDevice, T>;            \
  REGISTER_KERNEL_BUILDER(                                       \
      Name(""Example"").Device(DEVICE_GPU).TypeConstraint<T>(""T""), \
      ExampleOp<GPUDevice, T>);
REGISTER_GPU(float);
REGISTER_GPU(int32);
#endif  // GOOGLE_CUDA
```

1) run an interactive terminal and share the files as a volume with image 2.9.1, compile the op, and run it through python:

docker run -v$HOME/src/op_failure:/op --rm -it tensorflow/tensorflow:2.9.1-gpu

cd /op
TF_CFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') )
TF_LFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') )
g++ -std=c++14 -shared kernel_example.cc -o example.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2
python -c 'import tensorflow as tf; example_module = tf.load_op_library(""./example.so""); print(example_module.example([[1, 2], [3, 4]]).numpy())'

yields a successful output:
[[2 4]
 [6 8]]

3) Do the same exact set of commands with image 2.10.0:

docker run -v$HOME/src/op_failure:/op --rm -it tensorflow/tensorflow:2.10.0-gpu

cd /op
TF_CFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))') )
TF_LFLAGS=( $(python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))') )
g++ -std=c++14 -shared kernel_example.cc -o example.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2
python -c 'import tensorflow as tf; example_module = tf.load_op_library(""./example.so""); print(example_module.example([[1, 2], [3, 4]]).numpy())'

output is:
tensorflow.python.framework.errors_impl.NotFoundError: ./example.so: undefined symbol: _ZN10tensorflow6StatusC1ENS_5error4CodeEN4absl12lts_2022062311string_viewENS_14SourceLocationE
```


### Relevant log output

_No response_</details>"
57844,Master build failure on s390x due to TensorFlow Runtime,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

master

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

NA

### Python version

Python 3.8.10

### Bazel version

5.1.1

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

NA

### GPU model and memory

NA

### Current Behaviour?

Build from source fails with error: `external/tf_runtime/include/tfrt/host_context/attribute_utils.h:55:5: error: static assertion failed: big-endian not yet supported here
   55 |     ASSERT_LITTLE_ENDIAN();`

**Confirmed that this was caused due to commit f8145fac4**

Since TensorFlow Runtime doesn't support big endian yet and XLA Runtime compilation is optional as mentioned in the commit description, is there a way to build without TensorFlow Runtime?

Appreciate any inputs on this. Thank you!


### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow
cd tensorflow
yes """" | ./configure
bazel --host_jvm_args=""-Xms1024m"" --host_jvm_args=""-Xmx2048m"" build  --define=tensorflow_mkldnn_contraction_kernel=0 --define tflite_with_xnnpack=false //tensorflow/tools/pip_package:build_pip_package
```

### Relevant log output

```shell
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded, 998 targets configured).
INFO: Found 1 target...
ERROR: /home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/external/tf_runtime/BUILD:138:16: Compiling lib/host_context/kernel_frame.cc failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 102 arguments skipped)
In file included from external/tf_runtime/include/tfrt/host_context/attribute_utils.h:30,
                 from external/tf_runtime/include/tfrt/host_context/kernel_frame.h:32,
                 from external/tf_runtime/lib/host_context/kernel_frame.cc:20:
external/tf_runtime/include/tfrt/host_context/attribute_utils.h: In constructor 'tfrt::Attribute<T>::Attribute(const void*)':
external/tf_runtime/include/tfrt/host_context/attribute_utils.h:55:5: error: static assertion failed: big-endian not yet supported here
   55 |     ASSERT_LITTLE_ENDIAN();
      |     ^~~~~~~~~~~~~~~~~~~~
external/tf_runtime/include/tfrt/host_context/attribute_utils.h: In constructor 'tfrt::CompilationUnitAttribute::CompilationUnitAttribute(const void*)':
external/tf_runtime/include/tfrt/host_context/attribute_utils.h:130:5: error: static assertion failed: big-endian not yet supported here
  130 |     ASSERT_LITTLE_ENDIAN();
      |     ^~~~~~~~~~~~~~~~~~~~
At global scope:
cc1plus: warning: unrecognized command line option '-Wno-unused-local-typedef'
cc1plus: warning: unrecognized command line option '-Wno-array-parameter'
cc1plus: warning: unrecognized command line option '-Wno-unknown-warning'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 3144.838s, Critical Path: 163.72s
INFO: 4287 processes: 19 internal, 4268 local.
FAILED: Build did NOT complete successfully
```
</details>"
57843,How to implement state with dynamic shape? How about a VariableArray?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8.2

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I try to build a module that has dynamic shaped state and I struggle to do it in Tensorflow with minimal changes to the models.

I have a custom training pipeline and build the model mostly from scratch. My use-case is a Transformer encoder-decoder model used for machine translation. In search it is enough to apply the encoder once, and the decoder for every new input, except for the decoder self-attention module. This module needs to work on the current input and all previous.

Since these models are basically nested modules it would be annoying to pass the state as an argument to __call__ functions. I would need to modify many lines of code and, if I want to use the same search code for models without state, have an unused argument. A better solution would be to have the state as a member variable which can be cleared/reshaped from the outside.

As far as I know tf.Variables are the only object capable of being a member variable? But since they have fixed shape after construction I cannot use them.

Since TensorFlow is huge, I dont know if there are already features or tricks that can do a dynamically shaped state. Does anyone know anything about this? If it is not possible, my feature request is a VariableArray that basically wraps a TensorArray similar to Variables and Tensors.


### Standalone code to reproduce the issue

To illustrate the issue I made a minimal example

```python
import tensorflow as tf
class ModuleLevel2(tf.Module):
    
    def __init__(self):
        self.state = tf.TensorArray(size=0, dynamic_size=True, clear_after_read=False, dtype=tf.float32)

    def __call__(self, x):
        self.state = self.state.write(self.state.size(), x)
        x = self.state.stack()
        x = tf.identity(x) # Some op that requires all saved inputs
        tf.print(x)

class ModuleLevel1(tf.Module):

    def __init__(self):
        self.module2 = ModuleLevel2()

    def __call__(self, x):
        self.module2(x)

class Model(tf.Module):

    def __init__(self):
        self.module1 = ModuleLevel1()

    def __call__(self, x):
        self.module1(x)

model = Model()

@tf.function
def search():
    for i in range(10):
        x = (i*5)+tf.constant([1,2,3,4,5], dtype=tf.float32)
        model(x)

search()
```

The snippet generates the error seen below in ""Relevant log output"".

If search is not a tf.function the snippet works which is in my opinion undesirable behavior.

What is possible is to create the state outside of the model and pass it as an argument but as seen in the below snippet I have to modify all arguments and return values of modules on upper abstraction levels:

```python
import tensorflow as tf

class ModuleLevel2(tf.Module):
    
    def __init__(self):
        pass

    def __call__(self, x, state):
        state = state.write(state.size(), x)
        x = state.stack()
        x = tf.identity(x) # Some op that requires all saved inputs
        tf.print(x)
        return state

class ModuleLevel1(tf.Module):

    def __init__(self):
        self.module2 = ModuleLevel2()

    def __call__(self, x, state):
        state = self.module2(x, state)
        return state

class Model(tf.Module):

    def __init__(self):
        self.module1 = ModuleLevel1()

    def __call__(self, x, state):
        state = self.module1(x, state)
        return state

model = Model()

@tf.function
def search():
    state = tf.TensorArray(size=0, dynamic_size=True, clear_after_read=False, dtype=tf.float32)
    for i in range(10):
        x = (i*5)+tf.constant([1,2,3,4,5], dtype=tf.float32)
        state = model(x, state)

search()
```


### Relevant log output
```shell
Traceback (most recent call last):
  File ""~/snippets/tf_test.py"", line 37, in <module>
    search()
  File ""~/lib/Python3.8.2/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""~/lib/Python3.8.2/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1127, in autograph_handler
    raise e.ag_error_metadata.to_exception(e)
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in user code:

    File ""~/snippets/tf_test.py"", line 35, in search  *
        model(x)
    File ""~/snippets/tf_test.py"", line 27, in __call__  *
        self.module1(x)
    File ""~/snippets/tf_test.py"", line 19, in __call__  *
        self.module2(x)
    File ""~/snippets/tf_test.py"", line 8, in __call__  *
        self.state = self.state.write(self.state.size(), x)

    OperatorNotAllowedInGraphError: Using a symbolic `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.
```
</details>"
57840,"Build smaller AAR files locally (tensorflow-lite.aar, tensorflow-lite-select-tf-ops.aar) fail.","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8.2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.3.1

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Trying to follow the tutorial at https://www.tensorflow.org/lite/android/lite_build#build_and_install to build smaller .aar files using a .tflite model with TensorFlow ops the procedure fails with unknowk error. This happens when it tries to load the native TensorFlow runtime.
```


### Standalone code to reproduce the issue

```shell
Giving you the link to download the colab notebook and see all the output logs.
https://colab.research.google.com/drive/1D_mdWel9Pk4zVbW1Lxk9yT8P5MZdjeGg?usp=sharing
```


### Relevant log output

```shell
...........................
    Compiling tensorflow/core/common_runtime/session_state.cc; 7s local
[15,079 / 16,006] 24 actions running
    //tensorflow/core:portable_tensorflow_lib_lite; 24s local
    Compiling tensorflow/core/common_runtime/partitioning_utils.cc; 12s local
    Compiling tensorflow/core/common_runtime/ring_gatherer.cc; 10s local
    Compiling tensorflow/core/common_runtime/ring_reducer.cc; 10s local
    Compiling tensorflow/core/common_runtime/collective_util.cc; 9s local
    //tensorflow/core:portable_tensorflow_lib_lite; 8s local
    Compiling tensorflow/core/common_runtime/session_state.cc; 7s local
ERROR: /tensorflow_src/tensorflow/BUILD:1419:19: Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)
2022-09-26 04:46:44.851959: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow10checkpoint26OpenTableTensorSliceReaderERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEPPNS0_17TensorSliceReader5TableE

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 22, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 78, in <module>
    f'{traceback.format_exc()}'
ImportError: Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow10checkpoint26OpenTableTensorSliceReaderERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEPPNS0_17TensorSliceReader5TableE


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
[15,081 / 16,006] 23 actions running
    //tensorflow/core:portable_tensorflow_lib_lite; 24s local
    Compiling tensorflow/core/common_runtime/ring_gatherer.cc; 10s local
    Compiling tensorflow/core/common_runtime/ring_reducer.cc; 10s local
    Compiling tensorflow/core/common_runtime/collective_util.cc; 9s local
    //tensorflow/core:portable_tensorflow_lib_lite; 8s local
    Compiling tensorflow/core/common_runtime/session_state.cc; 7s local
    Compiling tensorflow/core/common_runtime/lower_case_op.cc; 7s local
Target //tmp:tensorflow-lite-select-tf-ops failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /tensorflow_src/tensorflow/python/tools/BUILD:281:10 Middleman _middlemen/tensorflow_Spython_Stools_Sprint_Uselective_Uregistration_Uheader-runfiles failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)
INFO: Elapsed time: 4769.260s, Critical Path: 564.99s
INFO: 13392 processes: 135 internal, 13257 local.
FAILED: Build did NOT complete successfully
```
</details>"
57838,Conv2D layer fails to run with XLA on CUDA,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0.dev20220921, 2.8.2

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Conv2D layer with ""same"" padding fails to run with XLA on CUDA, though it works well with XLA on CPU.
I guess there's some issue with XLA for CUDA to compile Conv2D.
```


### Standalone code to reproduce the issue

https://colab.research.google.com/drive/1o9NX4ZzhyuhBSI5MFX-REgaBOxrCOpuM?usp=sharing

Reproduced on 2.8.2, 2.11.0.dev20220921

```python
import tensorflow as tf
print(tf.__version__)
from keras import layers

class MyModule(tf.Module):
    def __init__(self):
        super().__init__()
        self.conv = layers.Conv2D(
            filters=2, kernel_size=1, padding='same',
            dtype=tf.float32, autocast=False,
        )

    @tf.function(jit_compile=True)
    def __call__(self, x):
        y = self.conv(x)
        return y



inp = {
    ""x"": tf.constant(1.2, shape=[1,2,2,2], dtype=tf.float32),
}
m = MyModule()

with tf.device('CPU:0'):
    out = m(**inp)
    print(f'{out}')

with tf.device('GPU:0'):
    out = m(**inp) # <--- exception!
    print(f'{out}')
```


### Relevant log output

```python
2.8.2
[[[[-1.9236538 -2.2042007]
   [-1.9236538 -2.2042007]]

  [[-1.9236538 -2.2042007]
   [-1.9236538 -2.2042007]]]]

---------------------------------------------------------------------------

InvalidArgumentError                      Traceback (most recent call last)

<ipython-input-3-ee06c892b128> in <module>
     28 
     29 with tf.device('GPU:0'):
---> 30     out = m(**inp) # <--- exception!
     31     print(f'{out}')


/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     53     ctx.ensure_initialized()
     54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:
     57     if name is not None:

InvalidArgumentError: Trying to access resource Resource-2-at-0x234d2850 located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0 [Op:__inference___call___77]
```
</details>"
57837,TFLite gives wrong result after reshape with bool tensor,"### 1. System information

tf 2.10.0

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

https://colab.research.google.com/drive/1yySdWF9yfLYIGlnVCqAEWpnLX2HiZD0w?usp=sharing

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```python
import tensorflow as tf
print(tf.__version__)
from keras import layers

def get_tflite_callable(model, inp_dict):
    converter = tf.lite.TFLiteConverter.from_concrete_functions(
        funcs=[model.__call__.get_concrete_function(**inp_dict)],
        trackable_obj=model,
    )
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
        tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
    ]
    tflite_bytes = converter.convert()
    interpreter = tf.lite.Interpreter(model_content=tflite_bytes)
    runner = interpreter.get_signature_runner()
    return runner

class MyModule(tf.Module):
    def __init__(self):
        super().__init__()
        self.const = tf.constant(True, shape=[2,2], dtype=tf.bool)

    @tf.function
    def __call__(self, x):
        x = tf.logical_or(self.const, x) # works fine
        x = tf.reshape(x, [2,2,1,1]) # after reshape the result is wrong
        return x


inp = {
    ""x"": tf.constant(True, shape=[2], dtype=tf.bool),
}
m = MyModule()

out = m(**inp)
print(f'{out}')

runner = get_tflite_callable(m, inp) # Error!
out = runner(**inp)
print(f'{out}')
```

```python
2.10.0
[[[[ True]]

  [[ True]]]


 [[[ True]]

  [[ True]]]]
{'output_0': array([[[[ True]],

        [[False]]],


       [[[False]],

        [[False]]]])} # <--- wrong result with TFLite
```
"
57836,Adan optimizer,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Adan seems to be a superior optimizer compared to Adam and its variants. Can we introduce this in Tf/Keras optimizers?

Please find a few references below:

Adan paper: https://arxiv.org/abs/2208.06677

Pytorch implementation:
- https://github.com/sail-sg/Adan
- https://github.com/lucidrains/Adan-pytorch

Blog article:
- https://wandb.ai/capecape/adan_optimizer/reports/Adan-A-New-Optimizer-That-Challenges-Adam--VmlldzoyNTQ5NjQ5
```


### Standalone code to reproduce the issue

```shell
NA
```


### Relevant log output

_No response_</details>"
57835,Inconsistencies between 2d and 3d ops,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf2.10

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

3.7.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Inspired by the document of tf.nn.conv1d which writes ""Internally, this op reshapes the input tensors and invokes tf.nn.conv2d"", I want to check if the 2d operators can be implemented using their 3d versions by reshaping the input tensors and some arguments. In the following colab link, I compared between Conv2D and Conv3D, MaxPool2D and MaxPool3D, AveragePooling2D and AveragePooling3D. 

I added an extra dimension to the input, strides, and kernel_size/pool_size. The original 2d operators should be equivalent to the version that implements a 2d operator using its 3d version. However, when feeding them with those specific arguments and input, I detect inconsistencies in the output when using the latest version of tensorflow. 

I also checked previous versions, and found in old versions such as tf2.4 and before, those codes will show no inconsistencies between the original 2d version and the version that implemented 2d ops using 3d ops. I believe this may be a bug because of this change between versions.
```


### Standalone code to reproduce the issue

```shell
Colab link: https://colab.research.google.com/drive/15vYN__3x0BT0c5co83k38w7CMCqIqWtC?usp=sharing
```


### Relevant log output

_No response_</details>"
57834,CMakeFile.txt build failure for mlir-hlo,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.11.0 (sha 551852a9ea9bf4e99856ce75c63516ad6d372239)

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04.3 

### Mobile device

_No response_

### Python version

3.8

### Bazel version

not using bazel

### GCC/Compiler version

10.0

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?

```shell
Following the instructions at https://github.com/tensorflow/tensorflow/tree/551852a9ea9bf4e99856ce75c63516ad6d372239/tensorflow/compiler/xla/mlir_hlo#mlir-hlo-a-standalone-hlo-mlir-based-compiler

Building LLVM with:

mkdir build && cd build
cmake -G Ninja ../llvm -DLLVM_ENABLE_PROJECTS=mlir -DLLVM_EXTERNAL_PROJECTS=""mlir-hlo"" -DLLVM_EXTERNAL_MLIR_HLO_SOURCE_DIR=../tensorflow/tensorflow/compiler/xla/mlir_hlo ...
ninja all
```

Results in a linking error (see below).

The fix is this patch:
```
diff --git a/tensorflow/compiler/xla/mlir_hlo/lib/Dialect/mhlo/transforms/CMakeLists.txt b/tensorflow/compiler/xla/mlir_hlo/lib/Dialect/mhlo/transforms/CMakeLists.txt
index 47657444236..21fe2bf7bcb 100644
--- a/tensorflow/compiler/xla/mlir_hlo/lib/Dialect/mhlo/transforms/CMakeLists.txt
+++ b/tensorflow/compiler/xla/mlir_hlo/lib/Dialect/mhlo/transforms/CMakeLists.txt
@@ -94,6 +94,7 @@ add_mlir_library(MhloToThloConversion
   MLIRMhloUtils
   MLIRPass
   MLIRTransformUtils
+  THLODialect
 )
 
 add_mlir_library(MhloToLhloConversion
```
```


### Standalone code to reproduce the issue

```shell
FAILED: tools/mlir-hlo/python_packages/mlir_hlo/mlir/_mlir_libs/libMLIRHLOCAPI.so.16git 
: && /usr/bin/g++-10 -fPIC -fPIC -fno-semantic-interposition -fvisibility-inlines-hidden -Werror=date-time -Wall -Wextra -Wno-unused-parameter -Wwrite-strings -Wcast-qual -Wno-missing-field-initializers -pedantic -Wno-long-long -Wimplicit-fallthrough -Wno-maybe-uninitialized -Wno-class-memaccess -Wno-redundant-move -Wno-pessimizing-move -Wno-noexcept-type -Wdelete-non-virtual-dtor -Wsuggest-override -Wno-comment -Wmisleading-indentation -fdiagnostics-color -ffunction-sections -fdata-sections -fPIC -fno-semantic-interposition -fvisibility-inlines-hidden -Werror=date-time -Wall -Wextra -Wno-unused-parameter -Wwrite-strings -Wcast-qual -Wno-missing-field-initializers -pedantic -Wno-long-long -Wimplicit-fallthrough -Wno-maybe-uninitialized -Wno-class-memaccess -Wno-redundant-move -Wno-pessimizing-move -Wno-noexcept-type -Wdelete-non-virtual-dtor -Wsuggest-override -Wno-comment -Wmisleading-indentation -fdiagnostics-color -ffunction-sections -fdata-sections -O3 -DNDEBUG  -Wl,-z,defs -Wl,-z,nodelete -Wl,-z,defs -Wl,-z,nodelete   -Wl,-rpath-link,/compiler/boyana.norris/developer/llvm/build/./lib  -Wl,--gc-sections  -Wl,-z,defs -shared -Wl,-soname,libMLIRHLOCAPI.so.16git -o tools/mlir-hlo/python_packages/mlir_hlo/mlir/_mlir_libs/libMLIRHLOCAPI.so.16git tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPIAsync.dir/Async.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPIAsync.dir/AsyncPasses.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPIGPU.dir/GPU.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPIGPU.dir/GPUPasses.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/AffineExpr.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/AffineMap.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/BuiltinAttributes.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/BuiltinTypes.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/Diagnostics.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/DialectHandle.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/IntegerSet.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/IR.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/Pass.cpp.o tools/mlir/lib/CAPI/IR/CMakeFiles/obj.MLIRCAPIIR.dir/Support.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPILinalg.dir/Linalg.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPILinalg.dir/LinalgPasses.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPIQuant.dir/Quant.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPIPDL.dir/PDL.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPISparseTensor.dir/SparseTensor.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPISparseTensor.dir/SparseTensorPasses.cpp.o tools/mlir/lib/CAPI/Debug/CMakeFiles/obj.MLIRCAPIDebug.dir/Debug.cpp.o tools/mlir/lib/CAPI/Interfaces/CMakeFiles/obj.MLIRCAPIInterfaces.dir/Interfaces.cpp.o tools/mlir/lib/CAPI/Dialect/CMakeFiles/obj.MLIRCAPIFunc.dir/Func.cpp.o tools/mlir/lib/CAPI/ExecutionEngine/CMakeFiles/obj.MLIRCAPIExecutionEngine.dir/ExecutionEngine.cpp.o tools/mlir/lib/CAPI/Conversion/CMakeFiles/obj.MLIRCAPIConversion.dir/Passes.cpp.o tools/mlir/lib/CAPI/Transforms/CMakeFiles/obj.MLIRCAPITransforms.dir/Passes.cpp.o tools/mlir/lib/CAPI/RegisterEverything/CMakeFiles/obj.MLIRCAPIRegisterEverything.dir/RegisterEverything.cpp.o tools/mlir-hlo/lib/CAPI/CMakeFiles/obj.MLIRHLOCAPIDialects.dir/Attributes.cc.o tools/mlir-hlo/lib/CAPI/CMakeFiles/obj.MLIRHLOCAPIDialects.dir/Dialects.cc.o tools/mlir-hlo/lib/CAPI/CMakeFiles/obj.MLIRHLOCAPIDialects.dir/Types.cc.o tools/mlir-hlo/lib/CAPI/CMakeFiles/obj.MLIRHLOCAPIDialects.dir/Passes.cc.o  -Wl,-rpath,""\$ORIGIN:/compiler/boyana.norris/miniconda3/envs/manglir/lib:""  lib/libMLIRAsyncDialect.a  lib/libMLIRAsyncTransforms.a  lib/libMLIRPass.a  lib/libLLVMSupport.a  lib/libMLIRGPUTransforms.a  lib/libMLIRPass.a  lib/libLLVMSupport.a  lib/libMLIRIR.a  lib/libMLIRParser.a  lib/libMLIRSupport.a  lib/libMLIRPass.a  lib/libLLVMSupport.a  lib/libMLIRLinalgDialect.a  lib/libMLIRPass.a  lib/libMLIRLinalgTransforms.a  lib/libLLVMSupport.a  lib/libMLIRQuantDialect.a  lib/libLLVMSupport.a  lib/libMLIRPDLDialect.a  lib/libLLVMSupport.a  lib/libMLIRSparseTensorDialect.a  lib/libMLIRSparseTensorTransforms.a  lib/libLLVMSupport.a  lib/libMLIRSupport.a  lib/libLLVMSupport.a  lib/libMLIRInferTypeOpInterface.a  lib/libLLVMSupport.a  lib/libMLIRFuncDialect.a  lib/libLLVMSupport.a  lib/libMLIRExecutionEngine.a  lib/libMLIRLLVMToLLVMIRTranslation.a  lib/libLLVMSupport.a  lib/libLLVMX86CodeGen.a  lib/libLLVMX86Desc.a  lib/libLLVMX86Info.a  lib/libLLVMX86CodeGen.a  lib/libLLVMX86AsmParser.a  lib/libLLVMX86Desc.a  lib/libLLVMX86Disassembler.a  lib/libLLVMX86Info.a  lib/libMLIRAffineToStandard.a  lib/libMLIRAMDGPUToROCDL.a  lib/libMLIRArithmeticToLLVM.a  lib/libMLIRArithmeticToSPIRV.a  lib/libMLIRArmNeon2dToIntr.a  lib/libMLIRAsyncToLLVM.a  lib/libMLIRBufferizationToMemRef.a  lib/libMLIRComplexToLLVM.a  lib/libMLIRComplexToLibm.a  lib/libMLIRComplexToStandard.a  lib/libMLIRControlFlowToLLVM.a  lib/libMLIRControlFlowToSPIRV.a  lib/libMLIRFuncToLLVM.a  lib/libMLIRFuncToSPIRV.a  lib/libMLIRGPUToGPURuntimeTransforms.a  lib/libMLIRGPUToNVVMTransforms.a  lib/libMLIRGPUToROCDLTransforms.a  lib/libMLIRGPUToSPIRV.a  lib/libMLIRGPUToVulkanTransforms.a  lib/libMLIRLinalgToLLVM.a  lib/libMLIRLinalgToSPIRV.a  lib/libMLIRLinalgToStandard.a  lib/libMLIRLLVMCommonConversion.a  lib/libMLIRMathToLibm.a  lib/libMLIRMathToLLVM.a  lib/libMLIRMathToSPIRV.a  lib/libMLIRMemRefToLLVM.a  lib/libMLIRMemRefToSPIRV.a  lib/libMLIRNVGPUToNVVM.a  lib/libMLIROpenACCToLLVM.a  lib/libMLIROpenACCToSCF.a  lib/libMLIROpenMPToLLVM.a  lib/libMLIRPDLToPDLInterp.a  lib/libMLIRReconcileUnrealizedCasts.a  lib/libMLIRSCFToControlFlow.a  lib/libMLIRSCFToGPU.a  lib/libMLIRSCFToOpenMP.a  lib/libMLIRSCFToSPIRV.a  lib/libMLIRShapeToStandard.a  lib/libMLIRSPIRVToLLVM.a  lib/libMLIRTensorToLinalg.a  lib/libMLIRTensorToSPIRV.a  lib/libMLIRTosaToArith.a  lib/libMLIRTosaToLinalg.a  lib/libMLIRTosaToSCF.a  lib/libMLIRTosaToTensor.a  lib/libMLIRVectorToLLVM.a  lib/libMLIRVectorToGPU.a  lib/libMLIRVectorToSCF.a  lib/libMLIRVectorToSPIRV.a  lib/libLLVMSupport.a  lib/libMLIRTransforms.a  lib/libLLVMSupport.a  lib/libMLIRAffineAnalysis.a  lib/libMLIRAffineDialect.a  lib/libMLIRAffineTransforms.a  lib/libMLIRAffineUtils.a  lib/libMLIRAMDGPUDialect.a  lib/libMLIRArithmeticDialect.a  lib/libMLIRArithmeticTransforms.a  lib/libMLIRArithmeticUtils.a  lib/libMLIRArmNeonDialect.a  lib/libMLIRArmSVEDialect.a  lib/libMLIRArmSVETransforms.a  lib/libMLIRAsyncDialect.a  lib/libMLIRAsyncTransforms.a  lib/libMLIRAMXDialect.a  lib/libMLIRAMXTransforms.a  lib/libMLIRBufferizationDialect.a  lib/libMLIRBufferizationTransformOps.a  lib/libMLIRBufferizationTransforms.a  lib/libMLIRComplexDialect.a  lib/libMLIRControlFlowDialect.a  lib/libMLIRDLTIDialect.a  lib/libMLIREmitCDialect.a  lib/libMLIRFuncDialect.a  lib/libMLIRFuncTransforms.a  lib/libMLIRGPUOps.a  lib/libMLIRGPUTransforms.a  lib/libMLIRLinalgAnalysis.a  lib/libMLIRLinalgDialect.a  lib/libMLIRLinalgTransformOps.a  lib/libMLIRLinalgTransforms.a  lib/libMLIRLinalgUtils.a  lib/libMLIRLLVMIRTransforms.a  lib/libMLIRLLVMDialect.a  lib/libMLIRNVVMDialect.a  lib/libMLIRROCDLDialect.a  lib/libMLIRMathDialect.a  lib/libMLIRMathTransforms.a  lib/libMLIRMemRefDialect.a  lib/libMLIRMemRefTransforms.a  lib/libMLIRMemRefUtils.a  lib/libMLIRMLProgramDialect.a  lib/libMLIRNVGPUDialect.a  lib/libMLIRNVGPUTransforms.a  lib/libMLIROpenACCDialect.a  lib/libMLIROpenMPDialect.a  lib/libMLIRPDLDialect.a  lib/libMLIRPDLInterpDialect.a  lib/libMLIRQuantDialect.a  lib/libMLIRQuantUtils.a  lib/libMLIRSCFDialect.a  lib/libMLIRSCFTransformOps.a  lib/libMLIRSCFTransforms.a  lib/libMLIRSCFUtils.a  lib/libMLIRShapeDialect.a  lib/libMLIRShapeOpsTransforms.a  lib/libMLIRSparseTensorDialect.a  lib/libMLIRSparseTensorTransforms.a  lib/libMLIRSparseTensorPipelines.a  lib/libMLIRSparseTensorUtils.a  lib/libMLIRSPIRVDialect.a  lib/libMLIRSPIRVModuleCombiner.a  lib/libMLIRSPIRVConversion.a  lib/libMLIRSPIRVTransforms.a  lib/libMLIRSPIRVUtils.a  lib/libMLIRTensorDialect.a  lib/libMLIRTensorInferTypeOpInterfaceImpl.a  lib/libMLIRTensorTilingInterfaceImpl.a  lib/libMLIRTensorTransforms.a  lib/libMLIRTensorUtils.a  lib/libMLIRTosaDialect.a  lib/libMLIRTosaTransforms.a  lib/libMLIRTransformDialect.a  lib/libMLIRTransformDialectTransforms.a  lib/libMLIRVectorDialect.a  lib/libMLIRVectorTransforms.a  lib/libMLIRVectorUtils.a  lib/libMLIRX86VectorDialect.a  lib/libMLIRX86VectorTransforms.a  lib/libMLIRTargetCpp.a  lib/libMLIRSPIRVDeserialization.a  lib/libMLIRSPIRVSerialization.a  lib/libMLIRSPIRVBinaryUtils.a  lib/libMLIRSPIRVTranslateRegistration.a  lib/libMLIRArmNeonToLLVMIRTranslation.a  lib/libMLIRArmSVEToLLVMIRTranslation.a  lib/libMLIRAMXToLLVMIRTranslation.a  lib/libMLIRLLVMToLLVMIRTranslation.a  lib/libMLIRNVVMToLLVMIRTranslation.a  lib/libMLIROpenACCToLLVMIRTranslation.a  lib/libMLIROpenMPToLLVMIRTranslation.a  lib/libMLIRROCDLToLLVMIRTranslation.a  lib/libMLIRX86VectorToLLVMIRTranslation.a  lib/libMLIRTargetLLVMIRExport.a  lib/libMLIRToLLVMIRTranslationRegistration.a  lib/libMLIRTargetLLVMIRImport.a  lib/libMLIRAffineToStandard.a  lib/libMLIRAMDGPUToROCDL.a  lib/libMLIRArithmeticToLLVM.a  lib/libMLIRArithmeticToSPIRV.a  lib/libMLIRArmNeon2dToIntr.a  lib/libMLIRAsyncToLLVM.a  lib/libMLIRBufferizationToMemRef.a  lib/libMLIRComplexToLLVM.a  lib/libMLIRComplexToLibm.a  lib/libMLIRComplexToStandard.a  lib/libMLIRControlFlowToLLVM.a  lib/libMLIRControlFlowToSPIRV.a  lib/libMLIRFuncToLLVM.a  lib/libMLIRFuncToSPIRV.a  lib/libMLIRGPUToGPURuntimeTransforms.a  lib/libMLIRGPUToNVVMTransforms.a  lib/libMLIRGPUToROCDLTransforms.a  lib/libMLIRGPUToSPIRV.a  lib/libMLIRGPUToVulkanTransforms.a  lib/libMLIRLinalgToLLVM.a  lib/libMLIRLinalgToSPIRV.a  lib/libMLIRLinalgToStandard.a  lib/libMLIRLLVMCommonConversion.a  lib/libMLIRMathToLibm.a  lib/libMLIRMathToLLVM.a  lib/libMLIRMathToSPIRV.a  lib/libMLIRMemRefToLLVM.a  lib/libMLIRMemRefToSPIRV.a  lib/libMLIRNVGPUToNVVM.a  lib/libMLIROpenACCToLLVM.a  lib/libMLIROpenACCToSCF.a  lib/libMLIROpenMPToLLVM.a  lib/libMLIRPDLToPDLInterp.a  lib/libMLIRReconcileUnrealizedCasts.a  lib/libMLIRSCFToControlFlow.a  lib/libMLIRSCFToGPU.a  lib/libMLIRSCFToOpenMP.a  lib/libMLIRSCFToSPIRV.a  lib/libMLIRShapeToStandard.a  lib/libMLIRSPIRVToLLVM.a  lib/libMLIRTensorToLinalg.a  lib/libMLIRTensorToSPIRV.a  lib/libMLIRTosaToArith.a  lib/libMLIRTosaToLinalg.a  lib/libMLIRTosaToSCF.a  lib/libMLIRTosaToTensor.a  lib/libMLIRVectorToLLVM.a  lib/libMLIRVectorToGPU.a  lib/libMLIRVectorToSCF.a  lib/libMLIRVectorToSPIRV.a  lib/libMLIRLLVMToLLVMIRTranslation.a  lib/libLLVMSupport.a  lib/libChloDialect.a  lib/libMhloDialect.a  lib/libTHLODialect.a  lib/libChloPasses.a  lib/libMhloPasses.a  lib/libMhloToLhloConversion.a  lib/libMhloToArithmeticConversion.a  lib/libMhloToMemrefConversion.a  lib/libMhloToStandard.a  lib/libMhloToThloConversion.a  lib/libMhloToLinalg.a  lib/libMhloShapeOpsToStandard.a  lib/libLLVMSupport.a  lib/libLLVMSupport.a  lib/libLLVMAsmPrinter.a  lib/libLLVMGlobalISel.a  lib/libLLVMSelectionDAG.a  lib/libLLVMCodeGen.a  lib/libLLVMCFGuard.a  lib/libLLVMOrcJIT.a  lib/libLLVMExecutionEngine.a  lib/libLLVMRuntimeDyld.a  lib/libLLVMWindowsDriver.a  lib/libLLVMJITLink.a  lib/libLLVMOrcTargetProcess.a  lib/libLLVMOrcShared.a  lib/libLLVMOption.a  lib/libLLVMMCDisassembler.a  lib/libMLIRAMDGPUToROCDL.a  lib/libMLIRAMDGPUDialect.a  lib/libMLIRGPUToGPURuntimeTransforms.a  lib/libMLIRAsyncToLLVM.a  lib/libMLIRMemRefToSPIRV.a  lib/libMLIRGPUTransforms.a  lib/libMLIRAsyncDialect.a  lib/libMLIRExecutionEngineUtils.a  lib/libLLVMPasses.a  lib/libLLVMCoroutines.a  lib/libLLVMTarget.a  lib/libLLVMipo.a  lib/libLLVMVectorize.a  lib/libLLVMLinker.a  lib/libLLVMInstrumentation.a  lib/libLLVMObjCARCOpts.a  lib/libMLIRArithmeticToSPIRV.a  lib/libMLIRFuncToSPIRV.a  lib/libMLIRTosaTransforms.a  lib/libMLIRTosaDialect.a  lib/libMLIRQuantUtils.a  lib/libMLIRNVGPUDialect.a  lib/libMLIRSparseTensorTransforms.a  lib/libMLIRLinalgTransforms.a  lib/libMLIRFuncToLLVM.a  lib/libMLIRArithmeticToLLVM.a  lib/libMLIRControlFlowToLLVM.a  lib/libMLIRVectorToSCF.a  lib/libMLIRLinalgAnalysis.a  lib/libMLIRLinalgUtils.a  lib/libMLIRTensorUtils.a  lib/libMLIRTensorTilingInterfaceImpl.a  lib/libMLIRSCFTransforms.a  lib/libMLIRSCFUtils.a  lib/libMLIRSparseTensorUtils.a  lib/libMLIRAffineToStandard.a  lib/libMLIRTensorTransforms.a  lib/libMLIRSPIRVConversion.a  lib/libMLIRSPIRVUtils.a  lib/libMLIRTransformDialect.a  lib/libMLIREmitCDialect.a  lib/libMLIRSPIRVDeserialization.a  lib/libMLIRSPIRVSerialization.a  lib/libMLIRSPIRVBinaryUtils.a  lib/libMLIRSPIRVDialect.a  lib/libMLIRLLVMToLLVMIRTranslation.a  lib/libMLIRArmNeonToLLVMIRTranslation.a  lib/libMLIRArmSVEToLLVMIRTranslation.a  lib/libMLIRAMXToLLVMIRTranslation.a  lib/libMLIRNVVMToLLVMIRTranslation.a  lib/libMLIROpenACCToLLVMIRTranslation.a  lib/libMLIROpenACCToLLVM.a  lib/libMLIRMemRefToLLVM.a  lib/libMLIROpenACCDialect.a  lib/libMLIROpenMPToLLVMIRTranslation.a  lib/libMLIROpenMPDialect.a  lib/libMLIRROCDLToLLVMIRTranslation.a  lib/libMLIRROCDLDialect.a  lib/libMLIRVectorToLLVM.a  lib/libMLIRArmNeonDialect.a  lib/libMLIRArmSVETransforms.a  lib/libMLIRArmSVEDialect.a  lib/libMLIRAMXTransforms.a  lib/libMLIRAMXDialect.a  lib/libMLIRVectorTransforms.a  lib/libMLIRLinalgDialect.a  lib/libMLIRTilingInterface.a  lib/libMLIRAffineUtils.a  lib/libMLIRGPUOps.a  lib/libMLIRVectorUtils.a  lib/libMLIRAffineAnalysis.a  lib/libMLIRPresburger.a  lib/libMLIRX86VectorTransforms.a  lib/libMLIRLLVMCommonConversion.a  lib/libMLIRVectorDialect.a  lib/libMLIRVectorInterfaces.a  lib/libMLIRX86VectorToLLVMIRTranslation.a  lib/libMLIRX86VectorDialect.a  lib/libMLIRTargetLLVMIRExport.a  lib/libMLIRLLVMIRTransforms.a  lib/libMLIRNVVMDialect.a  lib/libLLVMFrontendOpenMP.a  lib/libLLVMScalarOpts.a  lib/libLLVMAggressiveInstCombine.a  lib/libLLVMInstCombine.a  lib/libLLVMTransformUtils.a  lib/libMLIRDLTIDialect.a  lib/libMLIRTranslateLib.a  lib/libMLIRParser.a  lib/libMLIRAsmParser.a  lib/libLLVMIRReader.a  lib/libGmlStDialect.a  lib/libChloDialect.a  lib/libMLIRFuncTransforms.a  lib/libMLIRArithmeticTransforms.a  lib/libMLIRSCFDialect.a  lib/libMLIRShapeOpsTransforms.a  lib/libMLIRBufferizationTransforms.a  lib/libMLIRTransforms.a  lib/libMLIRCopyOpInterface.a  lib/libMLIRMathDialect.a  lib/libHloToLinalgUtils.a  lib/libMLIRBufferizationDialect.a  lib/libMLIRSparseTensorDialect.a  lib/libMLIRAffineDialect.a  lib/libMLIRTransformUtils.a  lib/libMLIRRewrite.a  lib/libMLIRPDLToPDLInterp.a  lib/libMLIRPDLInterpDialect.a  lib/libMLIRPDLDialect.a  lib/libMhloTypeConversion.a  lib/libLmhloDialect.a  lib/libMhloDialect.a  lib/libMLIRQuantDialect.a  lib/libMLIRMhloUtils.a  lib/libMLIRPass.a  lib/libMLIRAnalysis.a  lib/libMLIRLoopLikeInterface.a  lib/libMLIRLLVMDialect.a  lib/libMLIRDataLayoutInterfaces.a  lib/libLLVMAsmParser.a  lib/libLLVMBitWriter.a  lib/libLLVMAnalysis.a  lib/libLLVMProfileData.a  lib/libLLVMSymbolize.a  lib/libLLVMDebugInfoDWARF.a  lib/libLLVMDebugInfoPDB.a  lib/libLLVMObject.a  lib/libLLVMMCParser.a  lib/libLLVMMC.a  lib/libLLVMBitReader.a  lib/libLLVMTextAPI.a  lib/libLLVMDebugInfoCodeView.a  lib/libLLVMDebugInfoMSF.a  lib/libMLIRMemRefDialect.a  lib/libMLIRShapeDialect.a  lib/libMLIRFuncDialect.a  lib/libMLIRControlFlowDialect.a  lib/libMLIRTensorDialect.a  lib/libMLIRArithmeticUtils.a  lib/libMLIRComplexDialect.a  lib/libMLIRArithmeticDialect.a  lib/libMLIRInferTypeOpInterface.a  lib/libMLIRInferIntRangeInterface.a  lib/libLLVMCore.a  lib/libLLVMBinaryFormat.a  lib/libLLVMRemarks.a  lib/libLLVMBitstreamReader.a  lib/libMLIRDialectUtils.a  lib/libMLIRViewLikeInterface.a  lib/libMLIRParallelCombiningOpInterface.a  lib/libMLIRControlFlowInterfaces.a  lib/libMLIRDialect.a  lib/libMLIRSideEffectInterfaces.a  lib/libMLIRCallInterfaces.a  lib/libMLIRCastInterfaces.a  lib/libHloOpsCommon.a  lib/libLmhloStructuredInterface.a  lib/libMLIRIR.a  lib/libMLIRSupport.a  lib/libLLVMSupport.a  -lrt  -ldl  -lm  /usr/lib/x86_64-linux-gnu/libz.so  /compiler/boyana.norris/miniconda3/envs/manglir/lib/libzstd.so.1.5.2  -pthread  /usr/lib/x86_64-linux-gnu/libtinfo.so  lib/libLLVMDemangle.a  -lpthread && :
/usr/bin/ld: lib/libMhloToThloConversion.a(legalize_mhlo_to_thlo.cc.o): in function `mlir::mhlo::(anonymous namespace)::LegalizeMHLOToTHLOPass::getDependentDialects(mlir::DialectRegistry&) const':
legalize_mhlo_to_thlo.cc:(.text._ZNK4mlir4mhlo12_GLOBAL__N_122LegalizeMHLOToTHLOPass20getDependentDialectsERNS_15DialectRegistryE+0x2c): undefined reference to `mlir::detail::TypeIDResolver<mlir::thlo::THLODialect, void>::id'
/usr/bin/ld: lib/libMhloToThloConversion.a(legalize_mhlo_to_thlo.cc.o): in function `std::_Function_handler<mlir::Dialect* (mlir::MLIRContext*), mlir::DialectRegistry::insert<mlir::thlo::THLODialect>()::{lambda(mlir::MLIRContext*)#1}>::_M_invoke(std::_Any_data const&, mlir::MLIRContext*&&)':
legalize_mhlo_to_thlo.cc:(.text._ZNSt17_Function_handlerIFPN4mlir7DialectEPNS0_11MLIRContextEEZNS0_15DialectRegistry6insertINS0_4thlo11THLODialectEEEvvEUlS4_E_E9_M_invokeERKSt9_Any_dataOS4_[_ZNSt17_Function_handlerIFPN4mlir7DialectEPNS0_11MLIRContextEEZNS0_15DialectRegistry6insertINS0_4thlo11THLODialectEEEvvEUlS4_E_E9_M_invokeERKSt9_Any_dataOS4_]+0x13): undefined reference to `mlir::detail::TypeIDResolver<mlir::thlo::THLODialect, void>::id'
/usr/bin/ld: lib/libMhloToThloConversion.a(legalize_mhlo_to_thlo.cc.o): in function `std::unique_ptr<mlir::Dialect, std::default_delete<mlir::Dialect> > llvm::function_ref<std::unique_ptr<mlir::Dialect, std::default_delete<mlir::Dialect> > ()>::callback_fn<mlir::MLIRContext::getOrLoadDialect<mlir::thlo::THLODialect>()::{lambda()#1}>(long)':
legalize_mhlo_to_thlo.cc:(.text._ZN4llvm12function_refIFSt10unique_ptrIN4mlir7DialectESt14default_deleteIS3_EEvEE11callback_fnIZNS2_11MLIRContext16getOrLoadDialectINS2_4thlo11THLODialectEEEPT_vEUlvE_EES6_l[_ZN4llvm12function_refIFSt10unique_ptrIN4mlir7DialectESt14default_deleteIS3_EEvEE11callback_fnIZNS2_11MLIRContext16getOrLoadDialectINS2_4thlo11THLODialectEEEPT_vEUlvE_EES6_l]+0x23): undefined reference to `mlir::thlo::THLODialect::THLODialect(mlir::MLIRContext*)'
/usr/bin/ld: lib/libMhloToThloConversion.a(legalize_mhlo_to_thlo.cc.o): in function `mlir::mhlo::(anonymous namespace)::ConcatenateOpPattern::matchAndRewrite(mlir::mhlo::ConcatenateOp, mlir::PatternRewriter&) const':
legalize_mhlo_to_thlo.cc:(.text._ZNK4mlir4mhlo12_GLOBAL__N_120ConcatenateOpPattern15matchAndRewriteENS0_13ConcatenateOpERNS_15PatternRewriterE+0xfcd): undefined reference to `mlir::thlo::ConcatenateOp::build(mlir::OpBuilder&, mlir::OperationState&, mlir::Type, mlir::ValueRange, mlir::Value, unsigned long)'
/usr/bin/ld: legalize_mhlo_to_thlo.cc:(.text._ZNK4mlir4mhlo12_GLOBAL__N_120ConcatenateOpPattern15matchAndRewriteENS0_13ConcatenateOpERNS_15PatternRewriterE+0xff3): undefined reference to `mlir::detail::TypeIDResolver<mlir::thlo::ConcatenateOp, void>::id'
/usr/bin/ld: lib/libMhloToThloConversion.a(legalize_mhlo_to_thlo.cc.o): in function `mlir::mhlo::(anonymous namespace)::DynamicBroadcastInDimOpPattern::matchAndRewrite(mlir::mhlo::DynamicBroadcastInDimOp, mlir::PatternRewriter&) const':
legalize_mhlo_to_thlo.cc:(.text._ZNK4mlir4mhlo12_GLOBAL__N_130DynamicBroadcastInDimOpPattern15matchAndRewriteENS0_23DynamicBroadcastInDimOpERNS_15PatternRewriterE+0xe83): undefined reference to `mlir::thlo::DynamicBroadcastInDimOp::build(mlir::OpBuilder&, mlir::OperationState&, mlir::TypeRange, mlir::Value, mlir::Value, mlir::detail::DenseArrayAttr<long>, mlir::detail::DenseArrayAttr<long>, mlir::detail::DenseArrayAttr<long>)'
/usr/bin/ld: legalize_mhlo_to_thlo.cc:(.text._ZNK4mlir4mhlo12_GLOBAL__N_130DynamicBroadcastInDimOpPattern15matchAndRewriteENS0_23DynamicBroadcastInDimOpERNS_15PatternRewriterE+0xeab): undefined reference to `mlir::detail::TypeIDResolver<mlir::thlo::DynamicBroadcastInDimOp, void>::id'
/usr/bin/ld: lib/libMhloToThloConversion.a(legalize_mhlo_to_thlo.cc.o): in function `mlir::thlo::GatherOp mlir::OpBuilder::create<mlir::thlo::GatherOp, mlir::TensorType, mlir::Value, mlir::Value, mlir::linalg::InitTensorOp&>(mlir::Location, mlir::TensorType&&, mlir::Value&&, mlir::Value&&, mlir::linalg::InitTensorOp&)':
legalize_mhlo_to_thlo.cc:(.text._ZN4mlir9OpBuilder6createINS_4thlo8GatherOpEJNS_10TensorTypeENS_5ValueES5_RNS_6linalg12InitTensorOpEEEET_NS_8LocationEDpOT0_[_ZN4mlir9OpBuilder6createINS_4thlo8GatherOpEJNS_10TensorTypeENS_5ValueES5_RNS_6linalg12InitTensorOpEEEET_NS_8LocationEDpOT0_]+0xcd): undefined reference to `mlir::thlo::GatherOp::build(mlir::OpBuilder&, mlir::OperationState&, mlir::TypeRange, mlir::Value, mlir::Value, mlir::Value)'
/usr/bin/ld: legalize_mhlo_to_thlo.cc:(.text._ZN4mlir9OpBuilder6createINS_4thlo8GatherOpEJNS_10TensorTypeENS_5ValueES5_RNS_6linalg12InitTensorOpEEEET_NS_8LocationEDpOT0_[_ZN4mlir9OpBuilder6createINS_4thlo8GatherOpEJNS_10TensorTypeENS_5ValueES5_RNS_6linalg12InitTensorOpEEEET_NS_8LocationEDpOT0_]+0xf3): undefined reference to `mlir::detail::TypeIDResolver<mlir::thlo::GatherOp, void>::id'
/usr/bin/ld: lib/libMhloToThloConversion.a(legalize_mhlo_to_thlo.cc.o): in function `mlir::thlo::ScatterOp mlir::OpBuilder::create<mlir::thlo::ScatterOp, mlir::ShapedType&, mlir::Value, mlir::Value, mlir::Value>(mlir::Location, mlir::ShapedType&, mlir::Value&&, mlir::Value&&, mlir::Value&&)':
legalize_mhlo_to_thlo.cc:(.text._ZN4mlir9OpBuilder6createINS_4thlo9ScatterOpEJRNS_10ShapedTypeENS_5ValueES6_S6_EEET_NS_8LocationEDpOT0_[_ZN4mlir9OpBuilder6createINS_4thlo9ScatterOpEJRNS_10ShapedTypeENS_5ValueES6_S6_EEET_NS_8LocationEDpOT0_]+0xc6): undefined reference to `mlir::thlo::ScatterOp::build(mlir::OpBuilder&, mlir::OperationState&, mlir::TypeRange, mlir::Value, mlir::Value, mlir::Value)'
/usr/bin/ld: legalize_mhlo_to_thlo.cc:(.text._ZN4mlir9OpBuilder6createINS_4thlo9ScatterOpEJRNS_10ShapedTypeENS_5ValueES6_S6_EEET_NS_8LocationEDpOT0_[_ZN4mlir9OpBuilder6createINS_4thlo9ScatterOpEJRNS_10ShapedTypeENS_5ValueES6_S6_EEET_NS_8LocationEDpOT0_]+0xec): undefined reference to `mlir::detail::TypeIDResolver<mlir::thlo::ScatterOp, void>::id'
collect2: error: ld returned 1 exit status
ninja: build stopped: subcommand failed.
make: *** [Makefile:103: llvm] Error 1
```
```


### Relevant log output

_No response_</details>"
57833,"Error getting persistent gradients from model converted from tf1 to tf2 with tfa.seq2seq layers: ""'NoneType' object has no attribute 'outer_context'""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.10

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

None

### GPU model and memory

_No response_

### Current Behaviour?

After converting a model from tf1 to tf2, I cannot use the tf2 model with tf.GradientTape(persistent=True).

The model uses tfa.seq2seq layers, which are most likely the source of the error (e.g. replacing the content of the function network below with a single tf.compat.v1.layers.Dense works fine)

If anyone could help me that would be great, thank you!

### Standalone code to reproduce the issue
Here are 2 snippets to reproduce the error:
- First run this snippet to create/save the model

```shell
import functools
import tensorflow as tf
import tensorflow_addons as tfa

MODEL_FOLDER = ""model""
LSTM_UNITS = 3

def network(input):
    attention_mechanism = tfa.seq2seq.BahdanauAttention(
        1, input, memory_sequence_length=None
    )
    cell = tf.compat.v1.nn.rnn_cell.LSTMCell(
      LSTM_UNITS,
    )
    attention_cell = tfa.seq2seq.AttentionWrapper(
        cell, attention_mechanism, output_attention=False
    )

    embedding_fn = functools.partial(tf.compat.v1.one_hot, depth=10)
    output_layer = tf.compat.v1.layers.Dense(
        10,
    )

    train_helper = tfa.seq2seq.GreedyEmbeddingSampler(embedding_fn)
    decoder = tfa.seq2seq.BasicDecoder(
            cell=attention_cell, sampler=train_helper, output_layer=output_layer
    )
    init_kwargs = {}
    init_kwargs[""start_tokens""] = tf.compat.v1.fill([1], 1)
    init_kwargs[""end_token""] = 2
    init_kwargs[""initial_state""] = attention_cell.get_initial_state(
        batch_size=1, dtype=tf.compat.v1.float32
    )
    outputs, _, output_lengths = tfa.seq2seq.dynamic_decode(
        decoder=decoder,
        output_time_major=False,
        impute_finished=False,
        maximum_iterations=1,
        decoder_init_kwargs=init_kwargs,
    )
    return outputs


def main(_):
    input = tf.compat.v1.placeholder(dtype=tf.compat.v1.float32, shape=[1, 1, LSTM_UNITS])

    fetches = {
        ""output"": network(input).rnn_output,
    }

    with tf.compat.v1.Session() as sess:
        builder = tf.compat.v1.saved_model.Builder(MODEL_FOLDER)

        sess.run(
            [
                tf.compat.v1.global_variables_initializer(),
                tf.compat.v1.local_variables_initializer(),
                tf.compat.v1.tables_initializer(),
            ]
        )

        sig_def = tf.compat.v1.saved_model.predict_signature_def(
            inputs={""input"": input}, outputs=fetches
        )

        builder.add_meta_graph_and_variables(
            sess,
            tags=[""serve""],
            signature_def_map={
                tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY: sig_def
            },
        )
        builder.save()

if __name__ == ""__main__"":

    # Save model
    tf.compat.v1.disable_eager_execution()
    tf.compat.v1.disable_v2_behavior()
    tf.compat.v1.app.run()
```

- Then run this snippet to load/use the model

```
import tensorflow as tf

MODEL_FOLDER = ""model""
LSTM_UNITS = 3

if __name__ == ""__main__"":

    # Load model and use it
    input = tf.ones((1, 1, LSTM_UNITS))
    model = tf.saved_model.load(
        MODEL_FOLDER, tags=""serve""
    ).signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]

    with tf.GradientTape():
        print(model(input)) # This works
    with tf.GradientTape(persistent=True):
        print(model(input)) # AttributeError: 'NoneType' object has no attribute 'outer_context'
```


### Relevant log output

_No response_</details>"
57828,max_pool2d is over 10x slower than it should be for large pooling size,"... At least on CPU, as demonstrated in [this notebook](https://colab.research.google.com/drive/1e6Kx7WMwkL_FH8UX8cdbCzetuYlr-6Gn#scrollTo=CIbmTsSZ8fGE&uniqifier=1)

<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.8.2

### Custom Code

No

### OS Platform and Distribution

Any

### Mobile device

-

### Python version

3.7

### Bazel version

(whatever is default for colab notebooks)

### GCC/Compiler version

(whatever is default for colab notebooks)

### CUDA/cuDNN version

(whatever is default for colab notebooks)

### GPU model and memory

(whatever is default for colab notebooks)

### Current Behaviour?

```shell
Tensorflow's max_pool2d operation is much slower than it should be for large pooling sizes, as demonstrated by the fact that we can compute the same result in 6x the speed using reduce_max for a pooling region of size 80.
```


### Standalone code to reproduce the issue

https://colab.research.google.com/drive/1e6Kx7WMwkL_FH8UX8cdbCzetuYlr-6Gn#scrollTo=CIbmTsSZ8fGE&line=17&uniqifier=1



### Relevant log output

```shell
Max-Pooling a shape (1280, 720) heatmap by factor 2... 
Using max_pool2d - 17.93ms
Using reduce_max - 16.31ms
---
Max-Pooling a shape (1280, 720) heatmap by factor 5... 
Using max_pool2d - 19.59ms
Using reduce_max - 8.22ms
---
Max-Pooling a shape (1280, 720) heatmap by factor 10... 
Using max_pool2d - 20.71ms
Using reduce_max - 4.94ms
---
Max-Pooling a shape (1280, 720) heatmap by factor 20... 
Using max_pool2d - 18.00ms
Using reduce_max - 3.67ms
---
Max-Pooling a shape (1280, 720) heatmap by factor 40... 
Using max_pool2d - 18.71ms
Using reduce_max - 2.45ms
---
Max-Pooling a shape (1280, 720) heatmap by factor 80... 
Using max_pool2d - 18.75ms
Using reduce_max - 1.64ms
```
</details>"
57826,Failing to build libtensorflow_cc.so for v2.10.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

5.3.1

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

11.2.152

### GPU model and memory

NVIDIA RTX 3090 TI (24GB)

### Current Behaviour?

I am trying to build the TensorFlow C++ library `libtensorflow_cc.so` from source. The same build commands that succeeded for versions below 2.10.0 now fail with the latest 2.10.0 tag of the TensorFlow repository.

I'm building inside a Docker container of the official `devel-gpu` image, which is [defined in devel-gpu.Dockerfile](https://github.com/tensorflow/tensorflow/blob/v2.10.0/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile).

For the build, I am calling the following. The build output is attached as the log in this issue.

```shell
bazel build --jobs ${JOBS} --config=cuda --config=opt --config=monolithic --verbose_failures tensorflow:libtensorflow_cc.so tensorflow:install_headers
```

The error message reads:

```
ERROR: /tensorflow/tensorflow/BUILD:1156:21: in cc_shared_library rule //tensorflow:libtensorflow_cc.so.2.10.0: 
Traceback (most recent call last):
        File ""/virtual_builtins_bzl/common/cc/experimental_cc_shared_library.bzl"", line 416, column 105, in _cc_shared_library_impl
        File ""/virtual_builtins_bzl/common/cc/experimental_cc_shared_library.bzl"", line 340, column 37, in _filter_inputs
        File ""/virtual_builtins_bzl/common/cc/experimental_cc_shared_library.bzl"", line 360, column 9, in _throw_error_if_unaccounted_libs
Error in fail: The following libraries cannot be linked either statically or dynamically:
@nccl_archive//:nccl
@nccl_archive//:device
@nccl_archive//:net
@nccl_archive//:include_hdrs
@nccl_archive//:src_hdrs
To ignore which libraries get linked statically for now, add the following to 'static_deps':
        ""@nccl_archive//:__subpackages__"",
ERROR: /tensorflow/tensorflow/BUILD:1156:21: Analysis of target '//tensorflow:libtensorflow_cc.so.2.10.0' failed
ERROR: Analysis of target '//tensorflow:libtensorflow_cc.so' failed; build aborted: 
INFO: Elapsed time: 41.228s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (277 packages loaded, 28806 targets configured)
FAILED: Build did NOT complete successfully (277 packages loaded, 28806 targets configured)
```


### Standalone code to reproduce the issue

First build the official GPU devel image with [devel-gpu.Dockerfile](https://github.com/tensorflow/tensorflow/blob/v2.10.0/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile).

```shell
docker build -t tensorflow/tensorflow:2.10.0-devel-gpu -f devel-gpu.Dockerfile .
```

Then try to build the `libtensorflow_cc.so` with the following Dockerfile.

```docker
ARG TF_VERSION=2.10.0
FROM tensorflow/tensorflow:${TF_VERSION}-devel-gpu AS build

ARG TF_VERSION
ARG JOBS=""auto""

# clone TensorFlow
RUN git clone --branch v${TF_VERSION} --depth=1 https://github.com/tensorflow/tensorflow.git /tensorflow
WORKDIR /tensorflow

# configure compilation
ENV PYTHON_BIN_PATH=/usr/bin/python3
ENV PYTHON_LIB_PATH=/usr/lib/python3/dist-packages
ENV TF_NEED_ROCM=0
ENV TF_CUDA_COMPUTE_CAPABILITIES=5.3,6.0,6.1,7.0,7.2,7.5,8.0,8.6
ENV TF_CUDA_CLANG=0
ENV GCC_HOST_COMPILER_PATH=/usr/bin/gcc
ENV CC_OPT_FLAGS=""-march=native -Wno-sign-compare""
ENV TF_SET_ANDROID_WORKSPACE=0
RUN ./configure

# build C++ library
RUN bazel build --jobs ${JOBS} --config=cuda --config=opt --config=monolithic --verbose_failures tensorflow:libtensorflow_cc.so tensorflow:install_headers
```

```shell
docker build -t tensorflow/tensorflow:2.10.0-libtensorflow-cc .
```


### Relevant log output

```shell
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from /tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false
INFO: Reading rc options for 'build' from /tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=tensorrt --action_env TF_CUDA_VERSION=11.2 --action_env TF_CUDNN_VERSION=8 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=5.3,6.0,6.1,7.0,7.2,7.5,8.0,8.6 --action_env LD_LIBRARY_PATH=/usr/local/cuda-11.0/targets/x86_64-linux/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/include/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.2/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 --config=cuda
INFO: Reading rc options for 'build' from /tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:tensorrt in file /tensorflow/.bazelrc: --repo_env TF_NEED_TENSORRT=1
INFO: Found applicable config definition build:cuda in file /tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda in file /tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file /tensorflow/.tf_configure.bazelrc: --copt=-march=native --host_copt=-march=native --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --experimental_link_static_libraries_once=false
INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/6ca793b5d862ef6c50f242d77a811f06cce9b60a.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/0538e5431afdb1fa05bdcedf70ee502ccfcd112a.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
DEBUG: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
Loading: 
Loading: 1 packages loaded
Analyzing: 2 targets (2 packages loaded, 0 targets configured)
Analyzing: 2 targets (46 packages loaded, 34 targets configured)
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/nvidia/nccl/archive/v2.12.12-1.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
Analyzing: 2 targets (275 packages loaded, 26073 targets configured)
ERROR: /tensorflow/tensorflow/BUILD:1156:21: in cc_shared_library rule //tensorflow:libtensorflow_cc.so.2.10.0: 
Traceback (most recent call last):
        File ""/virtual_builtins_bzl/common/cc/experimental_cc_shared_library.bzl"", line 416, column 105, in _cc_shared_library_impl
        File ""/virtual_builtins_bzl/common/cc/experimental_cc_shared_library.bzl"", line 340, column 37, in _filter_inputs
        File ""/virtual_builtins_bzl/common/cc/experimental_cc_shared_library.bzl"", line 360, column 9, in _throw_error_if_unaccounted_libs
Error in fail: The following libraries cannot be linked either statically or dynamically:
@nccl_archive//:nccl
@nccl_archive//:device
@nccl_archive//:net
@nccl_archive//:include_hdrs
@nccl_archive//:src_hdrs
To ignore which libraries get linked statically for now, add the following to 'static_deps':
        ""@nccl_archive//:__subpackages__"",
ERROR: /tensorflow/tensorflow/BUILD:1156:21: Analysis of target '//tensorflow:libtensorflow_cc.so.2.10.0' failed
ERROR: Analysis of target '//tensorflow:libtensorflow_cc.so' failed; build aborted: 
INFO: Elapsed time: 41.228s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (277 packages loaded, 28806 targets configured)
FAILED: Build did NOT complete successfully (277 packages loaded, 28806 targets configured)
```
</details>"
57825,Empty wheel package on pypi for version 2.10,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.10

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
win amd64 wheels are only 2kB and no python file inside
```


### Standalone code to reproduce the issue

```shell
none
```


### Relevant log output

_No response_</details>"
57824,SSD mobilenet v2 trained using OD API can't run on Raspberry pi,"### 1. System information

- OS Platform and Distribution: Raspbian Bullseye
- TensorFlow installation : 
pip package, 2.8.0 for conversion
training tensorflow-gpu 1.15.0
tflite-runtime 2.10.0

### 2. Code

```
import tflite_runtime.interpreter as tflite
interpreter = tflite.Interpreter(model_path,num_threads=4)
interpreter.resize_tensor_input(0,[1,300,300,3])
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model works using tensoflow 2.8.0 on my laptop using intel i5
- Model doesn't work on the raspberry pi using tflite-runtime

![image](https://user-images.githubusercontent.com/47783157/191978841-0234f50e-573e-4fb7-8305-bf759d4e3546.png)


### when running

```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model('saved_model/')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.experimental_new_converter = True
```

output

```
INFO:tensorflow:Saver not created because there are no variables in the graph to restore
2022-09-23 15:07:39.557389: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.
2022-09-23 15:07:39.557422: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.
2022-09-23 15:07:39.558537: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: saved_model/saved_model/
2022-09-23 15:07:39.572864: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }
2022-09-23 15:07:39.572909: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: saved_model/saved_model/
2022-09-23 15:07:39.627548: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 69018 microseconds.
2022-09-23 15:07:39.809761: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
loc(fused[""TensorArrayV3:"", ""Preprocessor/map/TensorArray""]): error: 'tf.TensorArrayV3' op is neither a custom op nor a flex op
loc(fused[""TensorArrayV3:"", ""Preprocessor/map/TensorArray_1""]): error: 'tf.TensorArrayV3' op is neither a custom op nor a flex op
.
.
.
.
	tf.TensorArrayWriteV3(tensor<2x!tf_type.resource<tensor<*xf32>>>, tensor<i32>, tensor<?xf32>, tensor<f32>) -> (tensor<f32>) : {_class = [""loc:@Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Pad_1""], device = """"}
	tf.TensorArrayWriteV3(tensor<2x!tf_type.resource<tensor<*xf32>>>, tensor<i32>, tensor<?xf32>, tensor<f32>) -> (tensor<f32>) : {_class = [""loc:@Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Pad_5""], device = """"}
	tf.TensorArrayWriteV3(tensor<2x!tf_type.resource<tensor<*xi32>>>, tensor<i32>, tensor<3xi32>, tensor<f32>) -> (tensor<f32>) : {_class = [""loc:@Preprocessor/map/while/ResizeImage/stack_1""], device = """"}
	tf.TensorArrayWriteV3(tensor<2x!tf_type.resource<tensor<*xi32>>>, tensor<i32>, tensor<i32>, tensor<f32>) -> (tensor<f32>) : {_class = [""loc:@Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Select_11""], device = """"}
```

(ps : output was loo long)

#### when using converter.allow_custom_ops = True

the conversion works on tensorflow but not on tflite-runtime

#### the question is:

**why a model provided by google not work on tflite-runtime ?**

I have tested many ssd mobilenet v2 tflite files on the raspberry pi and they work, and they are really good. But when trying to train them on custom dataset and follow the guide provided by Google, they just don't work.
I can't use tensorflow on the raspberry pi !!!


#### appreciate any help !

Thanks



"
57823,Front camera opening in reverse position,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

Android Q

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened! opening front camera in reverse mode. under cameraSource.kt file
```


### Standalone code to reproduce the issue

```shell
A bug happened! opening front camera in reverse mode. under cameraSource.kt file
```


### Relevant log output

```shell
A bug happened! opening front camera in reverse mode. under cameraSource.kt file
```
</details>"
57822,TF Lite enable Flex delegate C++ API (with CMake of Bazel),"**System information**
- Linux Ubuntu 18.04):
- TensorFlow installed from source:
- TensorFlow version commit 47e07ba0d68c55dba62bff5b8486291086840097:

Hi, I want to run simple conv2d layer training using C++ API of TF Lite. I’ve used examples/minimal project as reference. Calling trainer->invoke() leads to next error:

`ERROR: TensorFlow Lite Error: Select TensorFlow op(s), included in the given model is(are) not supported by this interpreter. Make sure you apply/link Flex delegate before inference. For the Android, it can be resolved by adding “org.tensorflow:tensorflow-lite-select-tf-ops” dependency…`
`ERROR: Node number 48 (FlexConv2DBackpropFilter) failed to prepare.`

I didn’t find any good tutorial on Flex delegate. Please, tell me, how to build TF Lite with Flex using CMake and how to make this interpreter use Flex’s implementation of this node. Thanks




"
57820,"RoBERTa example from tfhub produces error ""During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8.2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7.14 

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
I would like to use the roberta-base model from tfhub. I am trying to run the example below, although I get an error when I try to feed sentences to model as input. I get the following error Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string. I am using python 3.7, tensorflow 2.8
```


### Standalone code to reproduce the issue

```shell
# define a text embedding model
text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)
preprocessor = hub.KerasLayer(""https://tfhub.dev/jeongukjae/roberta_en_cased_preprocess/1"")
encoder_inputs = preprocessor(text_input)

encoder = hub.KerasLayer(""https://tfhub.dev/jeongukjae/roberta_en_cased_L-12_H-768_A-12/1"", trainable=True)
encoder_outputs = encoder(encoder_inputs)
pooled_output = encoder_outputs[""pooled_output""]      # [batch_size, 768].
sequence_output = encoder_outputs[""sequence_output""]  # [batch_size, seq_length, 768].

model = tf.keras.Model(text_input, pooled_output)

# You can embed your sentences as follows
sentences = tf.constant([""(your text here)""])
print(model(sentences))
```


### Relevant log output

```shell
InvalidArgumentError: Graph execution error:

2 root error(s) found.
  (0) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
	 [[{{node map/TensorArrayUnstack/TensorListFromTensor}}]]
	 [[model/preprocessing/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bpe_sentencepiece_tokenizer/StatefulPartitionedCall/RaggedFromRowSplits_1/RowPartitionFromRowSplits/assert_non_negative/assert_less_equal/Assert/AssertGuard/else/_18720/RaggedFromRowSplits_1/RowPartitionFromRowSplits/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert/data_0/_135]]
  (1) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
	 [[{{node map/TensorArrayUnstack/TensorListFromTensor}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_534484]
```
</details>"
57818,TFLite produce wrong results when add follows a leakyrelu,"### 1. System information

Reproduced on 2.11.0.dev20220914 and 2.8.2 .

### 2. Code

#### Option A: Reference colab notebooks

https://colab.research.google.com/drive/1VfnLOX68h0rMrTA3Fqk4moIOTLOL-P22?usp=sharing

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```python
import tensorflow as tf

def get_tflite_callable(model, inp_dict):
    converter = tf.lite.TFLiteConverter.from_concrete_functions(
        funcs=[model.__call__.get_concrete_function(**inp_dict)],
        trackable_obj=model,
    )
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
        tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
    ]
    tflite_bytes = converter.convert()
    interpreter = tf.lite.Interpreter(model_content=tflite_bytes)
    runner = interpreter.get_signature_runner()
    return runner

class MyModule(tf.Module):
    def __init__(self):
        super().__init__()
        self.const = tf.constant([-10.43154963850975037], dtype=tf.float64)

    @tf.function
    def __call__(self, x):
        c = tf.raw_ops.LeakyRelu(
            features=self.const, alpha=0.1,
        )
        x = tf.add(c, x)
        return x


inp = {
    ""x"": tf.constant([3.12363398], dtype=tf.float64),
}
m = MyModule()

out = m(**inp)
print(f'{out}') # t = <tf.Tensor: shape=(1,), dtype=float64, numpy=array([2.080479])>

runner = get_tflite_callable(m, inp)
out = runner(**inp)['output_0']
print(f'{out}\n{out.dtype}') # out = array([3.12363398])  out.dtype = dtype('float64')

```

### 3. Failure after conversion

Model produces wrong results and/or has lesser accuracy.

Actually, in this case, TFLite just outputs the input. It seems that the const number after leakyrelu and then added to the input is omitted.

"
57817,How to get mobilenetV2 normalization values?,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.9.2

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

Same problem as #47559:

Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.

So I added metadata:  
```
from tflite_support.metadata_writers import object_detector
from tflite_support.metadata_writers import writer_utils

# Create metadata writer
writer = object_detector.MetadataWriter.create_for_inference(
    writer_utils.load_file(tflite_file),
    [127.5],
    [127.5],
    label_file_paths=[label_file])

print(""Identified metadata: "", writer.get_metadata_json())

# Write metadata
writer_utils.save_file(writer.populate(), tflite_file)
```

### 3. Failure after conversion

When running, it still complains about not having normalization values. Tried both [127.5],[127.5] and [255],[0] as values, but neither work. How can i find out what the models expects as normalization values?  
I loaded the saved model in the netron.app and the input just says `type: float32[1,640,640,3]`
"
57816,infinite loop in ConstantFolding::SimplifySwitch function,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 1.15

### Custom Code

No

### OS Platform and Distribution

centos 7

### Mobile device

centos 7

### Python version

2.7

### Bazel version

0.24.1

### GCC/Compiler version

5.3.1 20160406 (Red Hat 5.3.1-6) (GCC)

### CUDA/cuDNN version

10.0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I run tf_optimizer.OptimizeGraph function with savedmodel, constant_folding optimization loops infinitely.At present, it is found through the log that it is caused by the SimplifySwitch function.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import pickle

def load_pb(filename):
  with tf.gfile.FastGFile(filename,'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
  return graph_def

def load_reserve(filename):
  with open (filename, 'rb') as fp:
    nodes_to_reserve = pickle.load(fp)
  return nodes_to_reserve

def convert(graph_def_file, nodes_to_reserve_file):
  graph_def = load_pb(graph_def_file)
  nodes_to_reserve = load_reserve(nodes_to_reserve_file)

  from tensorflow.python.compiler.tensorrt import trt_convert
  graph_def = trt_convert.create_inference_graph(input_graph_def=graph_def, minimum_segment_size=10, outputs=nodes_to_reserve)

if __name__ == ""__main__"":
  import sys
  graph_def_file = sys.argv[1]
  nodes_to_reserve_file = sys.argv[2]
  convert(graph_def_file, nodes_to_reserve_file)
```


### Relevant log output

```shell
[root@zf-data-hdp-dn-gpu0016 tvm_convert]# CUDA_VISIBLE_DEVICES=""1"" python convert.py ./concated.pb nodes_to_reserve.txt
tensorflow version: 1.15.0, monolithic_build: 0, compiler_version: 5.3.1 20160406 (Red Hat 5.3.1-6), git_version: v1.15.0-308-g88465a7b6a3, cxx11_abi_flag: 0
2022-09-23 15:43:35,530: WARNING tensorflow 139636683290432 From convert.py:5: __init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.gfile.GFile.
2022-09-23 15:43:35,531: WARNING tensorflow 139636683290432 From convert.py:6: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.

2022-09-23 15:43:36,625: WARNING tensorflow 139636683290432
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

2022-09-23 15:43:36.645697: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.7
2022-09-23 15:43:36.646504: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.7
2022-09-23 15:43:36.648082: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.7
2022-09-23 15:43:39.147402: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-09-23 15:43:39.182206: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2022-09-23 15:43:39.182429: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2022-09-23 15:43:39.182950: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F
2022-09-23 15:43:39.225819: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2022-09-23 15:43:39.229217: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x637e130 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-09-23 15:43:39.229244: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-09-23 15:43:39.320987: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x636b920 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-09-23 15:43:39.321028: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5
2022-09-23 15:43:39.322414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1623] Found device 0 with properties:
name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59
pciBusID: 0000:5f:00.0
2022-09-23 15:43:39.322485: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-09-23 15:43:39.322508: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-09-23 15:43:39.324166: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-09-23 15:43:39.324488: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-09-23 15:43:39.326971: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-09-23 15:43:39.328908: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-09-23 15:43:39.328969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-09-23 15:43:39.331277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1751] Adding visible gpu devices: 0
2022-09-23 15:43:39.331312: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-09-23 15:43:41.214554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1164] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-09-23 15:43:41.214600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1170]      0
2022-09-23 15:43:41.214613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1183] 0:   N
2022-09-23 15:43:41.217579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1309] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14045 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:5f:00.0, compute capability: 7.5)
2022-09-23 15:43:42.315064: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:516] current optimizer: constant_folding, deadline_usec: 1663919322306153
2022-09-23 15:48:42.656131: W tensorflow/core/grappler/optimizers/meta_optimizer.cc:536] constant_folding failed: Deadline exceeded: constant_folding exceeded deadline., time = 300341.062ms.
2022-09-23 15:48:42.682801: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:516] current optimizer: layout, deadline_usec: 1663919322306153
2022-09-23 15:48:43.173973: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:516] current optimizer: constant_folding, deadline_usec: 1663919322306153
2022-09-23 15:48:43.191756: W tensorflow/core/grappler/optimizers/meta_optimizer.cc:536] constant_folding failed: Deadline exceeded: constant_folding exceeded deadline., time = 19.814ms.
2022-09-23 15:48:43.193674: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:516] current optimizer: TensorRTOptimizer, deadline_usec: 1663919322306153
2022-09-23 15:48:43.823627: I tensorflow/compiler/tf2tensorrt/segment/segment.cc:460] There are 3450 ops of 45 different types in the graph that are not converted to TensorRT: Mul, SplitV, Tile, Sigmoid, FloorMod, BiasAdd, Identity, UniqueFlatmap, Slice, Shape, GatherV2, Select, MatMul, Less, StringSplit, Sub, Minimum, NotEqual, Fill, GatherNd, Reshape, ExpandDims, Where, ConcatV2, RegexReplace, Prod, Add, Placeholder, Elu, NoOp, StridedSlice, Cast, Greater, Switch, Pack, ZerosLike, SparseSlice, SparseFillEmptyRows, Const, Substr, SparseSegmentSqrtN, StringToHashBucketStrong, Split, Merge, SparseToDense, (For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops).
2022-09-23 15:48:44.088002: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:637] Number of TensorRT candidate segments: 4
2022-09-23 15:48:44.100991: W tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:741] TensorRT node TRTEngineOp_0 added for segment 0 consisting of 20 nodes failed: Internal: Input shapes must be fully defined when in static mode. Please try is_dynamic_op=True (shape was [?,?,1]). Fallback to TF...
2022-09-23 15:48:44.101020: W tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:741] TensorRT node TRTEngineOp_1 added for segment 1 consisting of 12 nodes failed: Internal: Input shapes must be fully defined when in static mode. Please try is_dynamic_op=True (shape was [?,1,?]). Fallback to TF...
2022-09-23 15:48:44.101281: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.7
2022-09-23 15:48:44.120034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.7
2022-09-23 15:48:46.894562: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:738] TensorRT node TRTEngineOp_2 added for segment 2 consisting of 121 nodes succeeded.
2022-09-23 15:48:47.143088: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:738] TensorRT node TRTEngineOp_3 added for segment 3 consisting of 16 nodes succeeded.
2022-09-23 15:48:47.171434: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:516] current optimizer: constant_folding, deadline_usec: 1663919322306153
2022-09-23 15:48:47.193557: W tensorflow/core/grappler/optimizers/meta_optimizer.cc:536] constant_folding failed: Deadline exceeded: constant_folding exceeded deadline., time = 26.225ms.
2022-09-23 15:48:47.225926: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:795] Optimization results for grappler item: tf_graph
2022-09-23 15:48:47.225966: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797]   constant_folding: Deadline exceeded: constant_folding exceeded deadline., time = 300341.062ms.
2022-09-23 15:48:47.225973: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797]   layout: Graph size after: 3694 nodes (0), 7048 edges (0), time = 515.378ms.
2022-09-23 15:48:47.226264: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797]   constant_folding: Deadline exceeded: constant_folding exceeded deadline., time = 19.814ms.
2022-09-23 15:48:47.226275: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797]   TensorRTOptimizer: Graph size after: 3559 nodes (-135), 6892 edges (-156), time = 3975.24609ms.
2022-09-23 15:48:47.226281: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797]   constant_folding: Deadline exceeded: constant_folding exceeded deadline., time = 26.225ms.
[root@zf-data-hdp-dn-gpu0016 tvm_convert]#
```
</details>"
57812,Cannot set inter/intra_op_parallelism_threads after tfds.load,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Cannot set inter/intra_op_parallelism_threads after running tfds.load. Error message is ""RuntimeError: Inter op parallelism cannot be modified after initialization.""
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow_datasets as tfds

(train_ds, test_ds), info = tfds.load('mnist',
                                      data_dir='/tmp/data',
                                      split=['train', 'test'],
                                      with_info=True,
                                      as_supervised=True)

print(tf.config.threading.get_inter_op_parallelism_threads()) # returns 0
tf.config.threading.set_inter_op_parallelism_threads(1)
```
```


### Relevant log output

```shell
WARNING:absl:Found a different version 3.0.1 of dataset mnist in data_dir /tmp/data. Using currently defined version 3.0.0.
2022-09-22 14:02:51.738490: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-22 14:02:51.738942: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.
WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.
WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.
WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.
0
Traceback (most recent call last):
  File ""tf_test.py"", line 11, in <module>
    tf.config.threading.set_inter_op_parallelism_threads(1)
  File ""/home/yang/miniconda3/envs/nano-dev/lib/python3.7/site-packages/tensorflow/python/framework/config.py"", line 144, in set_inter_op_parallelism_threads
    context.context().inter_op_parallelism_threads = num_threads
  File ""/home/yang/miniconda3/envs/nano-dev/lib/python3.7/site-packages/tensorflow/python/eager/context.py"", line 1842, in inter_op_parallelism_threads
    ""Inter op parallelism cannot be modified after initialization."")
RuntimeError: Inter op parallelism cannot be modified after initialization.
```
</details>"
57807,No registered 'MatrixDeterminant' OpKernel for XLA_GPU_JIT devices,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

v1.12.1-81213-g53844b49ea4 2.11.0-dev20220912

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
>>> tf.function(tf.linalg.det,jit_compile=True).get_concrete_function(tf.constant([[1,0],[0,1]],dtype=tf.float32))(tf.constant([[1,0],[0,1]],dtype=tf.float32))
2022-09-22 20:11:03.425889: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:327 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph __inference_matrix_determinant_26[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263] on XLA_GPU_JIT: MatrixDeterminant (No registered 'MatrixDeterminant' OpKernel for XLA_GPU_JIT devices compatible with node {{node MatrixDeterminant}}){{node MatrixDeterminant}}
The op is created at: 
File ""<stdin>"", line 1, in <module>
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/gpfs/jlse-fs0/users/xjin/gpu/pyvenv_nightly/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1473, in __call__
    return self._call_impl(args, kwargs)
  File ""/gpfs/jlse-fs0/users/xjin/gpu/pyvenv_nightly/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1482, in _call_impl
    return self._call_with_structured_signature(args, kwargs,
  File ""/gpfs/jlse-fs0/users/xjin/gpu/pyvenv_nightly/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1563, in _call_with_structured_signature
    return self._call_flat(
  File ""/gpfs/jlse-fs0/users/xjin/gpu/pyvenv_nightly/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1744, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/gpfs/jlse-fs0/users/xjin/gpu/pyvenv_nightly/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 377, in call
    outputs = execute.execute(
  File ""/gpfs/jlse-fs0/users/xjin/gpu/pyvenv_nightly/lib/python3.10/site-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_matrix_determinant_26[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263] on XLA_GPU_JIT: MatrixDeterminant (No registered 'MatrixDeterminant' OpKernel for XLA_GPU_JIT devices compatible with node {{node MatrixDeterminant}}){{node MatrixDeterminant}}
The op is created at: 
File ""<stdin>"", line 1, in <module> [Op:__inference_matrix_determinant_26]
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
@tf.function(jit_compile=True)
def f(x):
    return tf.linalg.det(x)
tf.print(f(tf.constant([[1,0],[0,1]],dtype=tf.float32)))
```


### Relevant log output

_No response_</details>"
57806,Asset file vanishes after loading & saving the model twice,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.4

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


When I created the tf.Module with an asset file inside the TextFileInitializer, and save & load the module twice, the asset file will vanish after the 2nd save, and the loading will fail, complaining the absolute path of that file cannot be found. 

```
import shutil
import tensorflow as tf


class PrimaryModule(tf.Module):
    def __init__(self, name=None):
        super(PrimaryModule, self).__init__(name=name)
        initializer = tf.lookup.TextFileInitializer(
            ""chzhu_vocab.txt"",
            key_dtype=tf.int64,
            key_index=0,
            value_dtype=tf.int64,
            value_index=1,
            delimiter="" "",
        )
        self.table = tf.lookup.StaticVocabularyTable(initializer, 1)

    @tf.function
    def __call__(self, inputs):
        return self.table.lookup(inputs)


model = PrimaryModule()
print(model(tf.constant([509323409], dtype=tf.int64)))
tf.saved_model.save(model, ""asset_model"")
imported_new = tf.saved_model.load(""asset_model"")
print(imported_new(tf.constant([509323409], dtype=tf.int64)))
tf.saved_model.save(imported_new, ""wrapped_assets_model"")

# Error when loading the model for the second time, it will be trying to find the original path
# instead of path in the asset folder inside model
shutil.rmtree(""asset_model"")
shutil.rm(""chzhu_vocab.txt"")
tf.saved_model.load(""wrapped_assets_model"")

```
Error: see full stack trace below 
```
FileNotFoundError:  chzhu_vocab.txt; No such file or directory
```

When changing `initializer` to be `self.initializer` in the object constructor, it will work as normal
```
import tensorflow as tf

class PrimaryModule(tf.Module):
    def __init__(self, name=None):
        super(PrimaryModule, self).__init__(name=name)
        self.initializer = tf.lookup.TextFileInitializer(
            ""chzhu_vocab.txt"",
            key_dtype=tf.int64,
            key_index=0,
            value_dtype=tf.int64,
            value_index=1,
            delimiter="" "",
        )
        self.table = tf.lookup.StaticVocabularyTable(self.initializer, 1)

    @tf.function
    def __call__(self, inputs):
        return self.table.lookup(inputs)

# ..same saving logics

```

I did a deep dive into the model saving logic in TF 2.4, and found the difference here:

When saving the model for the first time, the model saver will save the object dependencies into the SavedObjectGraph:

object_graph_def in the meta_graph_def:
```
object_graph_def {
  nodes {
    children {
      node_id: 1
      local_name: ""table""
    }
    children {
      node_id: 2
      local_name: ""signatures""
    }
    children {
      node_id: 5
      local_name: ""__call__""
    }
    user_object {
      identifier: ""_generic_user_object""
      version {
        producer: 1
        min_consumer: 1
      }
    }
  }
  nodes {
    children {
      node_id: 3
      local_name: ""_initializer""
    }
    children {
      node_id: 6
      local_name: ""_create_resource""
    }
    children {
      node_id: 7
      local_name: ""_initialize""
    }
    children {
      node_id: 8
      local_name: ""_destroy_resource""
    }
    resource {
    }
  }
…
```

As we can see above, `_initializer` is not the attribute of the top-level object and is the child of `VocabularyTable`, which is a resource object. The dependency will look like object -> table -> initializer -> Asset.


During the first time loading

* The Loader will try to recreate all the objects in the SavedObjectGraph, 

* The resource object will be recreated as _RestoredResource, which inherits the base.Trackable (not AutoTrackable). 

* When calling `_setattr_` in the _add_object_graph_edges(), it will not update _checkpoint_dependencies (unlike normal object inheriting AutoTrackable, which override the _setattr_ function to update the attribute dependencies.) Therefore, the table -> initializer -> Asset dependency is not entirely recovered.

During the second time saving, it will 

* find out the Asset objects in the object dependencies via Breadth First Traversal, and add that into asset_info. 

* in the _fill_meta_graph_def(), we will create asset_initializer_ops for all Assets in asset_info.

* in the _fill_meta_graph_def(), the asset_initializer and its downstream variables will be written into init_ops, and the main graph will be updated to initialize the variable from the corresponding asset_initializer.


However, since in the previous loading stage, the _checkpoint_dependencies will be empty for the “table” attribute, and we cannot access the Asset object via Breadth First Traversal of the object graph. Thus we are getting empty asset_info, and 2, and 3 above will not happen.



### Standalone code to reproduce the issue

```shell
import shutil
import tensorflow as tf


class PrimaryModule(tf.Module):
    def __init__(self, name=None):
        super(PrimaryModule, self).__init__(name=name)
        initializer = tf.lookup.TextFileInitializer(
            ""chzhu_vocab.txt"",
            key_dtype=tf.int64,
            key_index=0,
            value_dtype=tf.int64,
            value_index=1,
            delimiter="" "",
        )
        self.table = tf.lookup.StaticVocabularyTable(initializer, 1)

    @tf.function
    def __call__(self, inputs):
        return self.table.lookup(inputs)


model = PrimaryModule()
print(model(tf.constant([509323409], dtype=tf.int64)))
tf.saved_model.save(model, ""asset_model"")
imported_new = tf.saved_model.load(""asset_model"")
print(imported_new(tf.constant([509323409], dtype=tf.int64)))
tf.saved_model.save(imported_new, ""wrapped_assets_model"")

# Error when loading the model for the second time
shutil.rmtree(""asset_model"")
tf.saved_model.load(""wrapped_assets_model"")
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/chzhu/tf2-trainer/tf2-trainer-component-training/test/asset_testing.py"", line 32, in <module>
    tf.saved_model.load(""custom_model/wrapped_assets_model"")
  File ""/home/chzhu/tf2-trainer/build/tf2-trainer-component-training/environments/development-venv/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py"", line 859, in load
    return load_internal(export_dir, tags, options)[""root""]
  File ""/home/chzhu/tf2-trainer/build/tf2-trainer-component-training/environments/development-venv/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py"", line 893, in load_internal
    str(err) + ""\n If trying to load on a different device from the ""
FileNotFoundError:  custom_model/asset_model/assets/chzhu_vocab.txt; No such file or directory
         [[{{node StatefulPartitionedCall/text_file_init/InitializeTableFromTextFileV2}}]] [Op:__inference_restored_function_body_375]
```
</details>"
57805,Python3.11 support please,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

No

### OS Platform and Distribution

blackPanther OS

### Mobile device

_No response_

### Python version

3.11

### Bazel version

5.3.1

### GCC/Compiler version

11.2.1

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
Build fail with Python 3.11
```


### Standalone code to reproduce the issue

```shell
Standalone code from git
```


### Relevant log output

```shell
/tensorflow/python/BUILD:398:27: Compiling tensorflow/python/lite/toco_python_api_wrapper.cc failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 47 arguments skipped)
In file included from bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/cast.h:16,
                 from bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/attr.h:13,
                 from bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:13,
                 from tensorflow/python/lite/toco_python_api_wrapper.cc:19:
bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/detail/type_caster_base.h: In function 'std::string pybind11::detail::error_string()':
bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/detail/type_caster_base.h:482:26: error: invalid use of incomplete type 'PyFrameObject' {aka 'struct _frame'}
  482 |             frame = frame->f_back;
      |                          ^~
In file included from bazel-out/k8-opt/bin/external/local_config_python/python_include/Python.h:42,
                 from bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/detail/common.h:186,
                 from bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pytypes.h:12,
                 from bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/cast.h:13,
                 from bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/attr.h:13,
                 from bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:13,
                 from tensorflow/python/lite/toco_python_api_wrapper.cc:19:
bazel-out/k8-opt/bin/external/local_config_python/python_include/pytypedefs.h:22:16: note: forward declaration of 'PyFrameObject' {aka 'struct _frame'}
   22 | typedef struct _frame PyFrameObject;
      |                ^~~~~~
In file included from bazel-out/k8-opt/bin/external/local_config_python/python_include/tupleobject.h:39,
                 from bazel-out/k8-opt/bin/external/local_config_python/python_include/Python.h:59,
                 from bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/detail/common.h:186,
                 from bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pytypes.h:12,
                 from bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/cast.h:13,
                 from bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/attr.h:13,
                 from bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:13,
                 from tensorflow/python/lite/toco_python_api_wrapper.cc:19:
bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h: In function 'pybind11::function pybind11::detail::get_type_override(const void*, const pybind11::detail::type_info*, const char*)':
bazel-out/k8-opt/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:2348:54: error: 'PyCodeObject' {aka 'struct PyCodeObject'} has no member named 'co_varnames'; did you mean 'co_names'?
 2348 |                     locals, PyTuple_GET_ITEM(f_code->co_varnames, 0)
      |                                                      ^~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 7.900s, Critical Path: 6.54s
INFO: 111 processes: 11 internal, 100 local.
FAILED: Build did NOT complete successfully
```
</details>"
57803,"Failed precondition: Could not find variable Variable. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized.","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

binary

### Tensorflow Version

tensorflow                2.6.2

### Custom Code

Yes

### OS Platform and Distribution

win10

### Mobile device

_No response_

### Python version

3.6

### Bazel version

None

### GCC/Compiler version

None

### CUDA/cuDNN version

11.6

### GPU model and memory

None

### Current Behaviour?

```shell
raceback (most recent call last):
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\client\session.py"", line 1375, in _do_call
    return fn(*args)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\client\session.py"", line 1360, in _run_fn
    target_list, run_metadata)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\client\session.py"", line 1453, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.FailedPreconditionError: 2 root error(s) found.
  (0) Failed precondition: Could not find variable Variable. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status=Not found: Resource localhost/Variable/class tensorflow::Var does not exist.
	 [[{{node image-0/GatherV2/ReadVariableOp}}]]
	 [[strided_slice_54/_289]]
  (1) Failed precondition: Could not find variable Variable. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status=Not found: Resource localhost/Variable/class tensorflow::Var does not exist.
	 [[{{node image-0/GatherV2/ReadVariableOp}}]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:/PycharmProjects/4dFaceOptimizerIK/approximation_v2.py"", line 166, in <module>
    verts0=approx.fk(conbtrol0)
  File ""E:/PycharmProjects/4dFaceOptimizerIK/approximation_v2.py"", line 146, in fk
    final_refineMesh=self.sess.run([self.final_refineMesh])
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\client\session.py"", line 968, in run
    run_metadata_ptr)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\client\session.py"", line 1191, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\client\session.py"", line 1369, in _do_run
    run_metadata)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\client\session.py"", line 1394, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.FailedPreconditionError: 2 root error(s) found.
  (0) Failed precondition: Could not find variable Variable. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status=Not found: Resource localhost/Variable/class tensorflow::Var does not exist.
	 [[node image-0/GatherV2/ReadVariableOp (defined at E:\PycharmProjects\4dFaceOptimizerIK\cnnModel\cnnModel.py:98) ]]
	 [[strided_slice_54/_289]]
  (1) Failed precondition: Could not find variable Variable. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status=Not found: Resource localhost/Variable/class tensorflow::Var does not exist.
	 [[node image-0/GatherV2/ReadVariableOp (defined at E:\PycharmProjects\4dFaceOptimizerIK\cnnModel\cnnModel.py:98) ]]
0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node image-0/GatherV2/ReadVariableOp:
 Variable (defined at E:/PycharmProjects/4dFaceOptimizerIK/approximation_v2.py:67)

Input Source operations connected to node image-0/GatherV2/ReadVariableOp:
 Variable (defined at E:/PycharmProjects/4dFaceOptimizerIK/approximation_v2.py:67)

Original stack trace for 'image-0/GatherV2/ReadVariableOp':
  File ""E:/PycharmProjects/4dFaceOptimizerIK/approximation_v2.py"", line 162, in <module>
    approx=Approximator(sess)
  File ""E:/PycharmProjects/4dFaceOptimizerIK/approximation_v2.py"", line 78, in __init__
    mesh = buildModel(baseConfig, rigControls)
  File ""E:/PycharmProjects/4dFaceOptimizerIK/approximation_v2.py"", line 58, in buildModel
    model = cnnModel.buildModel(data,dataset,neutral,config)
  File ""E:\PycharmProjects\4dFaceOptimizerIK\cnnModel\cnnModel.py"", line 98, in buildModel
    imageInput = tf.gather(pose,index,axis=1)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\util\deprecation.py"", line 549, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\util\dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 5052, in gather
    params, indices, axis, batch_dims=batch_dims, name=name)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 3821, in gather_v2
    batch_dims=batch_dims, name=name)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 522, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\profiler\trace.py"", line 163, in wrapped
    return func(*args, **kwargs)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\framework\ops.py"", line 1566, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 2023, in _dense_var_to_tensor
    return var._dense_var_to_tensor(dtype=dtype, name=name, as_ref=as_ref)  # pylint: disable=protected-access
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 1421, in _dense_var_to_tensor
    return self.value()
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 576, in value
    return self._read_variable_op()
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 683, in _read_variable_op
    result = read_and_set_handle()
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 674, in read_and_set_handle
    self.handle, self._dtype)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\ops\gen_resource_variable_ops.py"", line 484, in read_variable_op
    ""ReadVariableOp"", resource=resource, dtype=dtype, name=name)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 750, in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\framework\ops.py"", line 3569, in _create_op_internal
    op_def=op_def)
  File ""C:\Users\raphaelmeng\.conda\envs\fdfd\lib\site-packages\tensorflow\python\framework\ops.py"", line 2045, in __init__
    self._traceback = tf_stack.extract_stack_for_node(self._c_op)


Process finished with exit code 1
```


### Standalone code to reproduce the issue

```shell
# Build the approximation model
import os
import pickle
import sys
from pathlib import Path

import numpy as np
import yaml
import tensorflow.compat.v1 as tf
from collections import namedtuple

sys.path.append(os.path.join('..','cnnModel'))
sys.path.append('..')
from cnnModel import cnnModel,rigidDeformer

def buildModel(config,pose,addNeutral=True):

    with open(config['data_params']['cache_file'],'rb') as file:
        data = pickle.load(file)
    cache = data
    parts = data['vCharts']
    faces = data['faces']
    neutral = data['neutral'][data['active']].astype('float32')
    uvs = data['uv']
    if 'parameter_mask' in data:
        mask = data['parameter_mask']
    else:
        mask = None

    # Create the model
    partCount = np.max(parts)+1
    data = {'pose':pose}
    usedVerts = []
    usedUVs = []
    for i in range(partCount):
        if np.sum(parts==i) > 0:
            data['image-'+str(i)] = tf.ones(1)
        else:
            data['image-'+str(i)] = None
        ref = faces.reshape(-1)
        idx = np.arange(len(neutral))[parts==i]
        if len(idx) == 0:
            continue
        usedFaces = [True if v in idx else False for v in ref]
        usedFaces = np.sum(np.asarray(usedFaces).reshape((-1,3)),-1) == 3
        faceIdx = np.arange(len(faces))[usedFaces]
        uv = uvs[idx]
        usedUVs.append(uv)
        usedVerts.append(idx)
    idx = np.concatenate(usedVerts)
    linear = np.zeros(neutral.shape,dtype='float32')
    if addNeutral:
        linear[idx] = neutral[idx]
    else:
        neutral = linear
    data['linear'] = linear
    dataset = namedtuple('Dataset','mask usedUVs usedVerts')(mask,usedUVs,usedVerts)
    model = cnnModel.buildModel(data,dataset,neutral,config)
    model['parts'] = parts
    model['cache'] = cache
    return model

class Approximator():

    def __init__(self,  sess):
        ####
        rigControls = tf.Variable(tf.zeros([1, 172], tf.float32))

        approximation_config = 'experiments/v00_refine_model_leaky.yaml'
        with open(approximation_config) as file:
            approximationConfig = yaml.load(file)
        with tf.variable_scope('refine'):
            refineMesh = buildModel(approximationConfig, rigControls, addNeutral=False)

        base_config = 'experiments/v00_base_model_leaky.yaml'
        with open(base_config) as file:
            baseConfig = yaml.load(file)
        mesh = buildModel(baseConfig, rigControls)

        have_refineMesh = True
        refineMesh['output'] = mesh['output'] + refineMesh['output']

        print(refineMesh['output'])
        print(type(refineMesh['output']))

        # Apply ridig deformer
        # Load info about the mesh
        with open(os.path.join(baseConfig['data_params']['cache_file']), 'rb') as file:
            data = pickle.load(file)
        parts = data['vCharts']
        neutral = data['neutral'][data['active']]
        print(""data['neutral'].shape="", data['neutral'].shape)
        print('neutral.shape=', neutral.shape)
        faces = data['faces']
        mask = np.arange(len(parts))[parts > -1]

        if 'rigid_files' in baseConfig['data_params']:

            cur_rigidDeformer = rigidDeformer.RigidDeformer(neutral, [f for f in
                                                                      baseConfig['data_params']['rigid_files']], mask)
            final_base_mesh = cur_rigidDeformer.deformTF(mesh['output'][0])[np.newaxis]
            if have_refineMesh:
                final_refineMesh = cur_rigidDeformer.deformTF(refineMesh['output'][0])[np.newaxis]
        else:
            final_base_mesh = mesh['output']
            if have_refineMesh:
                final_refineMesh = refineMesh['output']

        # print('final_refineMesh.shape =', final_refineMesh.eval())
        print('type(final_refineMesh) =', type(final_refineMesh))

        ####
        vars = tf.trainable_variables()
        print(vars)
        vars_to_train = vars[0]
        vars_to_load = vars[1:]
        self.saver = tf.train.Saver(vars_to_load)

        checkpoint_dir = 'E:\\PycharmProjects\\FDFD-Metahuman-Y-up\\output\\v00_refine_model_leaky'
        self.checkpointFile = tf.train.latest_checkpoint(checkpoint_dir)
        self.saver.restore(sess, self.checkpointFile)

        self.rigControls=rigControls
        self.sess=sess
        self.final_refineMesh=final_refineMesh


    def fk(self,control):
        # vars = tf.trainable_variables()
        # print(""after_vars ="",vars)
        self.rigControls= control
        # print('self.rigControls=',self.rigControls)
        #
        # # init = tf.global_variables_initializer()
        # # self.sess.run(init)
        # vars = tf.trainable_variables()
        # print(vars)
        # vars_to_train = vars[0]
        # vars_to_load = vars[1:]
        # saver = tf.train.Saver(vars_to_load)
        #
        # checkpoint_dir = 'E:\\PycharmProjects\\FDFD-Metahuman-Y-up\\output\\v00_refine_model_leaky'
        # checkpointFile = tf.train.latest_checkpoint(checkpoint_dir)
        # saver.restore(sess, checkpointFile)

        final_refineMesh=self.sess.run([self.final_refineMesh])
        return final_refineMesh

    def ik(self,verts):
        pass








if __name__ == '__main__':
    # Load the model from file
    with tf.Session() as sess:
        approx=Approximator(sess)


        conbtrol0=tf.zeros([1, 172], tf.float32)
        verts0=approx.fk(conbtrol0)
        print('verts0=',verts0)

        conbtrol1 = tf.ones([1, 172], tf.float32)
        verts1 = approx.fk(conbtrol1)
        print('verts1=', verts1)
```


### Relevant log output

```shell
None
```
</details>"
57802,How to parse MetaGraphDef with C API,"Hi,

I have a model with multiple outputs. Using the C API, I need to select outputs according to their names, for example 'detection_boxes' in the following image:
![image](https://user-images.githubusercontent.com/5577772/191764668-ff8a2486-7065-4521-9a3d-b878471c32a5.png)

I found these names are present in the meta_graph_def I get from the [TF_LoadSessionFromSavedModel()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h#L1240) function. Is there a supported way to parse this meta_graph_def to get the names? If not, is it planned? [This](https://github.com/serizba/cppflow/pull/203) is how I do it for now.

Regards,

Carl"
57798,Process crash caused by check failed in `UnsortedSegmentProd`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0.dev20220914

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Passing a large number to UnsortedSegmentProd will trigger an i32 overflow, which leads to check failure later and the process will crash.
As a public API, it's better to check the number and throw an exception.
Note that the same issue happens in UnsortedSegmentMax, UnsortedSegmentMin, UnsortedSegmentSum, UnsortedSegmentProd .
```


### Standalone code to reproduce the issue

https://colab.research.google.com/drive/1BIm3xyoJaGMD_YkGyKvDR8Xp9kZKk3OX?usp=sharing

```python
import tensorflow as tf
print(tf.__version__)

class MyModule(tf.Module):
    def __init__(self):
        super().__init__()

    @tf.function
    def __call__(self, x, y, z):
        return tf.raw_ops.UnsortedSegmentProd(
            data=x, segment_ids=y, num_segments=z,
        )


inp = {
    ""x"": tf.constant([3]),
    ""y"": tf.constant([1], dtype=tf.int64),
    ""z"": tf.constant(0x7fffffff + 1, dtype=tf.int64),
}
m = MyModule()

out = m(**inp)  # Error!
print(out)
print(out.shape)
```


### Relevant log output

```shell
2022-09-22 08:41:10.937141: F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. -2147483648)
/bin/bash: line 1:   138 Aborted                 (core dumped) python unsorted_seg_bug.py 2>&1
```
</details>"
57797,https://github.com/tensorflow/tensorflow/issues/new,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
57796,can i use graph_transforms where the input model is saved model,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
57795,Saving to an existing directory does not overwrite assets,"
 ### Issue Type

Bug

### Tensorflow Version

2.10

### Python version

3.8.10


### Current Behaviour?

Using `tf.saved_model.save` to save a model to a directory where an older version of the model was previously saved only partially overwrites the older SavedModel. Specifically, assets are not overwritten.

I would expect that while saving the model we either completely overwrite the old model, or produce a warning. As is, saving and reloading a model can produce an unexpected state.

It looks like this behavior was previously changed in [d42a6a948](https://github.com/tensorflow/tensorflow/commit/d42a6a948de44954fac3884466e969776dc4480e), but later reverted.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.training.tracking import autotrackable

_FILENAME = ""file.txt""
_SAVEDIR = ""model""

# Write contents to asset file
with open(_FILENAME, 'w') as f:
    f.write(""foo"")

# First Save
trackable = autotrackable.AutoTrackable()
asset = tf.saved_model.Asset(_FILENAME)
trackable.asset = asset
tf.saved_model.save(trackable, _SAVEDIR)

# Change asset contents
with open(_FILENAME, 'w') as f:
    f.write(""bar"")

# Second Save
tf.saved_model.save(trackable, _SAVEDIR)

# Load and compare with in-memory trackable
loaded_trackable = tf.saved_model.load(_SAVEDIR)
print(""Expected Result: {}"".format(tf.io.read_file(trackable.asset)))      # bar
print(""Loaded Result: {}"".format(tf.io.read_file(loaded_trackable.asset))) # foo
```


### Relevant log output

```
Expected Result: b'bar'
Loaded Result: b'foo'
```"
57786,Wrong NaN gradient for `tf.acos`," ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.10

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

`tf.math.acos` is an element-wise operation, thus the gradient should be`0` when the input and output elements have different indices. However, in the example below, the gradient of `tf.acos(x)[0]` w.r.t. `x[2]` is computed as `nan`, which is wrong. It should be `0`.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([-0.5, 0.5, 2.0])
with tf.GradientTape(persistent=True) as tape:
    tape.watch(x)
    y = tf.acos(x)[0:1]
print(tape.gradient(y, x))
```


### Relevant log output

```shell
tf.Tensor([-1.1547005 -0.               nan], shape=(3,), dtype=float32)
```"
57784,Framework NotFoundError building sagemaker-tensorflow-extensions to support TF 2.10.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7,3.8,3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When we try to build/test https://github.com/aws/sagemaker-tensorflow-extensions to support the new tensorflow release, the following error is thrown:
E   tensorflow.python.framework.errors_impl.NotFoundError: //tmp/test/.tox/py39/lib/python3.9/site-packages/sagemaker_tensorflow/libPipeModeOp.so: undefined symbol: _ZNK10tensorflow20OpKernelConstruction7HasAttrEN4absl12lts_2022062311string_viewE (more in log output)
In 2.9.1, we have found this string to be _ZNK10tensorflow20OpKernelConstruction7HasAttrEN4absl12lts_2021110211string_viewE and everything works as expected.
This is run on 2.10.0, with this patch (https://github.com/tensorflow/tensorflow/pull/56691) to fix cuBLAS issue we were having before. Can't find anything in the release notes suggesting something that would need to be changed in our project to prevent this. Can anyone help us out on what might be causing the issue or a potential solution?
Thank you!
```


### Standalone code to reproduce the issue

```shell
We run tox tests on the code changes included in this pull request
https://github.com/aws/sagemaker-tensorflow-extensions/pull/144
```


### Relevant log output

```shell
_________________________________________________________________________________________________________________ ERROR collecting test/unit/test_pipemode.py __________________________________________________________________________________________________________________
test/unit/test_pipemode.py:20: in <module>
    from sagemaker_tensorflow import PipeModeDataset, PipeModeDatasetException
.tox/py39/lib/python3.9/site-packages/sagemaker_tensorflow/__init__.py:15: in <module>
    from sagemaker_tensorflow.pipemode import PipeModeDataset, PipeModeDatasetException
.tox/py39/lib/python3.9/site-packages/sagemaker_tensorflow/pipemode.py:38: in <module>
    class PipeModeDataset(dataset_ops.Dataset):
.tox/py39/lib/python3.9/site-packages/sagemaker_tensorflow/pipemode.py:41: in PipeModeDataset
    _tf_plugin = _load_plugin()
.tox/py39/lib/python3.9/site-packages/sagemaker_tensorflow/pipemode.py:29: in _load_plugin
    return tf.load_op_library(tf_plugin_path)
.tox/py39/lib/python3.9/site-packages/tensorflow/python/framework/load_library.py:54: in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
E   tensorflow.python.framework.errors_impl.NotFoundError: //tmp/test/.tox/py39/lib/python3.9/site-packages/sagemaker_tensorflow/libPipeModeOp.so: undefined symbol: _ZNK10tensorflow20OpKernelConstruction7HasAttrEN4absl12lts_2022062311string_viewE
```
</details>"
57781,TensorFlow Lite: Not detecting `DataType` for UINT8 `TensorImage`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

Android 12, target SDK 30

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When trying to run a UINT8 quantized model using the TensorFlow Lite `Interpreter`, the runtime is not able to determine the correct data type for inference. The code is using the `TensorImage` type from the support library with an explicitly set `DataType` of `DataType.UINT8`. 
I've confirmed the shape and type of the input in Netron: ""type: uint8[1,224,224,3]""
Inspecting the code, it seems to fail to resolve the type in `TensorImpl.dataTypeOf()`, throwing the above Exception.
```


### Standalone code to reproduce the issue

```shell
val imageProcessor = ImageProcessor.Builder()
	.add(ResizeOp(inputSize.width, inputSize.height, ResizeOp.ResizeMethod.NEAREST_NEIGHBOR))
	.build()

val inputBuffer: TensorImage = run {
	val imageTensorIndex = 0
	val imageDataType = interpreter.getInputTensor(imageTensorIndex).dataType()
	TensorImage(imageDataType)
}

inputBuffer.load(bitmap)
val buffer = imageProcessor.process(inputBuffer)

// passing TensorImage to runtime
interpreter.run(buffer, outputBuffer)

// passing a raw buffer and resizing according to the docs
interpreter.resizeInput(0, intArrayOf(1, inputSize.width, inputSize.height, 3))
interpreter.run(buffer.buffer, outputBuffer)
```


### Relevant log output

```shell
java.lang.IllegalArgumentException: DataType error: cannot resolve DataType of org.tensorflow.lite.support.image.TensorImage
	at org.tensorflow.lite.TensorImpl.dataTypeOf(TensorImpl.java:319)
	at org.tensorflow.lite.TensorImpl.throwIfTypeIsIncompatible(TensorImpl.java:390)
	at org.tensorflow.lite.TensorImpl.getInputShapeIfDifferent(TensorImpl.java:252)
	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:237)
	at org.tensorflow.lite.InterpreterImpl.runForMultipleInputsOutputs(InterpreterImpl.java:133)
	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:80)
	at org.tensorflow.lite.InterpreterImpl.run(InterpreterImpl.java:126)
	at org.tensorflow.lite.Interpreter.run(Interpreter.java:80)
```
</details>"
57780,tflite gesture_classification int8 model,"@PaulTR 

For tflite examples, we are seeking for the dataset used to train the gesture classification model.
https://github.com/tensorflow/examples/tree/master/lite/examples/gesture_classification/android

if the dataset is unavailable for some reason, Is the int8 tflite model for gesture classification available for sharing?

Thanks,
Niranjan

"
57779,tf.cast python float to tf.float64 or tf.complex128 leads to loss of precision,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.10.0-rc3-6-g359c3cdfc5f 2.10.0

### Custom Code

No

### OS Platform and Distribution

tested on Linux and macOS 

### Mobile device

_No response_

### Python version

tested with 3.9 and 3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
>>> tf.cast(0.2, tf.float64)
<tf.Tensor: shape=(), dtype=float64, numpy=0.20000000298023224>
>>> tf.cast(0.2, tf.complex128)
<tf.Tensor: shape=(), dtype=complex128, numpy=(0.20000000298023224+0j)>
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.print(tf.cast(0.2, tf.float64))
tf.print(tf.math.real(tf.cast(0.2, tf.complex128)))
```


### Relevant log output

_No response_</details>"
57777,flatbuffer -> ModelT -> flatbuffer:  zero length Vectors -> null pointer.,"Folks,

I'm seeing zero length Vectors being converted to null pointers when converting
a tflite model to a flatbuffer tflite ModelT object and back again.

Netron shows the correct graph for the final model, and the tflite runtime seems to execute correctly,
but tools like `tensorflow/lite/python/analyzer_wrapper/model_analyzer.cc` fail when they encounter a null pointer rather
than an empty vector. For instance, this happens when the inputs and outputs of the CALL_ONCE operator are inspected.
The ModelT object seems to have the correct zero length vector data immediately prior to serialization.

My question: Is there a more ""correct"" way to export a  model in TFLite with empty vectors?  I'm using the pretty simple code shown below.  I'm also using schema_v3b.fbs (2.9.2 branch) and flatbuffers 1.12

git_repository(
    name = ""com_github_google_flatbuffers"",
    commit = ""6df40a2471737b27271bdd9b900ab5f3aec746c7"",
    remote = ""https://github.com/google/flatbuffers"",
    shallow_since = ""1584052419 -0700"",
)

Backstory - we're splitting TFLite models so that NN components can run on a delegate after which the remainder run on cpu.


### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 22.04
- TensorFlow installation (pip package or built from source):   v2.9.2 
- TensorFlow library (version, if pip package or github SHA, if built from source):   18960c4

### 2. Code
```
#include ""flatbuffers/flatbuffers.h""

namespace chain_saw {

tflite::ModelT read_model(const std::string& model_path) {
  std::ifstream infile;
  infile.open(model_path.c_str(), std::ios::binary | std::ios::in);
  infile.seekg(0, std::ios::end);
  auto length = infile.tellg();
  infile.seekg(0, std::ios::beg);
  char *data = new char[length];
  infile.read(data, length);
  infile.close();

  const tflite::Model *m = tflite::GetModel(data);
  tflite::ModelT model_obj;
  m->UnPackTo(&model_obj);
  // not using original model representation anymore, so tidying up.
  delete[] data;

  return model_obj;
}

void write_model(const tflite::ModelT &model, const std::string& model_path) {
  // Serialize into new flatbuffer.
  flatbuffers::FlatBufferBuilder fbb;
  fbb.Finish(tflite::Model::Pack(fbb, &model), ""TFL3"");
  const uint8_t *buf = fbb.GetBufferPointer();
  int size = fbb.GetSize();  // Returns the size of the buffer that
  std::ofstream outfile;
  outfile.open(model_path.c_str(), std::ios::binary | std::ios::out);
  if (!outfile) {
    LOG(FATAL);
  }
  outfile.write(reinterpret_cast<const char *>(buf), size);
  LOG(INFO) << ""Wrote model to "" << model_path;
}

#include ""schema/schema_v3b_generated.h""

namespace {
const char kSrcModel[] = ""models/src.tflite"";
const char kDstModel[] = ""/tmp/dst.tflite"";
}  // namespace

// nominally write back what we read.
int main(int argc, char* argv[]) {

  tflite::ModelT model_obj = chain_saw::read_model(kSrcModel);
  chain_saw::write_model(model_obj, kDstModel);
}
```
"
57775,"TESTs Failed: kernels:unsorted_segment_{prod, max, min, sum}_test","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

latest master branch

### Custom Code

No

### OS Platform and Distribution

Ubuntu 9.4.0-1ubuntu1~18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

</details>

### Current Behaviour?

```shell
TFLite kernel TESTs failing:

//tensorflow/lite/kernels:unsorted_segment_prod_test

//tensorflow/lite/kernels:unsorted_segment_max_test

//tensorflow/lite/kernels:unsorted_segment_min_test

//tensorflow/lite/kernels:unsorted_segment_sum_test
```


### Standalone code to reproduce the issue

```shell
bazel test //tensorflow/lite/kernels:unsorted_segment_min_test --test_output=all
```


### Relevant log output

```shell
(unsorted_segment_min_test as example, similar log for other TESTs)

...
ERROR: /media/WORK/mirror-tensorflow/tensorflow/lite/kernels/BUILD:2850:8: Compiling tensorflow/lite/kernels/unsorted_segment_min_test.cc failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 197 arguments skipped)
tensorflow/lite/kernels/unsorted_segment_min_test.cc:136:67: error: expected constructor, destructor, or type conversion before ';' token
  136 | GTEST_ALLOW_UNINSTANTIATED_PARAMETERIZED_TEST(UnsortedSegmentTest);
      |                                                                   ^
cc1plus: warning: unrecognized command line option '-Wno-array-parameter'
cc1plus: warning: unrecognized command line option '-Wno-unknown-warning'
Target //tensorflow/lite/kernels:unsorted_segment_min_test failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 7.023s, Critical Path: 6.42s
INFO: 12 processes: 12 internal.
FAILED: Build did NOT complete successfully
```"
57774,How can i use 4 GPUS with tensorflow2.10,"How can i use 4 GPUS with tensorflow2.10

sorry for my poor english

who can show me an example."
57773,[MSVC][permissive-] Tensorflow failed to build,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf2.9

### Custom Code

No

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.9

### Bazel version

bazel 5.2.0

### GCC/Compiler version

VS2019 Version 16.11.18

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The MSVC team uses Tensorflow as part of RWC testing to detect compiler regressions. We found the following errors when building TensorFlow with the ""/permissive-"" option. Can you help look?

Actually, I added some workarounds to unblock the tests, but how do I patch these external libraries? It looks like the external library is downloaded online when building and I can't change the source code before downloading it.

Error info:

external/com_github_grpc_grpc\src/core/lib/security/credentials/credentials.h(205): error C7626: unnamed class used in typedef name cannot declare members other than non-static data members, member enumerations, or member classes

external/com_github_grpc_grpc\src/core/lib/security/security_connector/ssl/ssl_security_connector.h(60): error C7626: unnamed class used in typedef name cannot declare members other than non-static data members, member enumerations, or member classes

external/com_github_grpc_grpc\src/core/ext/transport/chttp2/transport/internal.h(109): error C7626: unnamed class used in typedef name cannot declare members other than non-static data members, member enumerations, or member classes

external/highwayhash\highwayhash/state_helpers.h(49): error C2760: syntax error: unexpected token 'alignas', expected 'declaration'

external/com_google_protobuf/python/google/protobuf/pyext/descriptor_pool.cc(179): error C2440: 'initializing': cannot convert from 'const char [14]' to 'char *'
external/com_google_protobuf/python/google/protobuf/pyext/descriptor_pool.cc(179): note: Conversion from string literal loses const qualifier (see /Zc:strictStrings)
external/com_google_protobuf/python/google/protobuf/pyext/descriptor_pool.cc(246): error C2664: 'PyObject *google::protobuf::python::cdescriptor_pool::SetErrorFromCollector(google::protobuf::DescriptorPool::ErrorCollector *,char *,char *)': cannot convert argument 3 from 'const char [8]' to 'char *'
external/com_google_protobuf/python/google/protobuf/pyext/descriptor_pool.cc(248): note: Conversion from string literal loses const qualifier (see /Zc:strictStrings)

depthwise_conv.cc
.\tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv.h(1432): error C3861: '__PRETTY_FUNCTION__': identifier not found
.\tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv.h(1432): error C2065: '__PRETTY_FUNCTION__': undeclared identifier
.\tensorflow/lite/kernels/internal/optimized/depthwiseconv_float.h(771): error C3861: '__PRETTY_FUNCTION__': identifier not found
.\tensorflow/lite/kernels/internal/optimized/depthwiseconv_float.h(771): error C2065: '__PRETTY_FUNCTION__': undeclared identifier
.\tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h(1522): error C3861: '__PRETTY_FUNCTION__': identifier not found
.\tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h(1522): error C2065: '__PRETTY_FUNCTION__': undeclared identifier

F:\bazeltemp\2powapgf\execroot\org_tensorflow>link.exe @bazel-out/x64_windows-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params
LINK : warning LNK4044: unrecognized option '/lm'; ignored
philox.lo.lib(random_distributions.obj) : error LNK2005: ""private: void __cdecl tensorflow::random::SingleSampleAdapter<class tensorflow::random::PhiloxRandom>::SkipFromGenerator(unsigned __int64)"" (?SkipFromGenerator@?$SingleSampleAdapter@VPhiloxRandom@random@tensorflow@@@random@tensorflow@@AEAAX_K@Z) already defined in shuffle_dataset_op.lo.lib(shuffle_dataset_op.obj)
   Creating library bazel-out/x64_windows-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so.if.lib and object bazel-out/x64_windows-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so.if.exp
bazel-out\x64_windows-opt\bin\tensorflow\python\_pywrap_tensorflow_internal.so : fatal error LNK1169: one or more multiply defined symbols found
```


### Standalone code to reproduce the issue

```shell
Build steps:

git clone https://github.com/tensorflow/tensorflow.git F:\gitP\tensorflow\tensorflow
cd F:\gitP\tensorflow\tensorflow
pip3 install --upgrade ""setuptools"" pip wheel
pip3 install -r tensorflow/tools/ci_build/release/requirements_common.txt
set PATH=F:\gitP\tensorflow\tools;%path% // add bazel
set PATH=F:\gitP\tensorflow\tools\msys64\usr\bin;%path%
yes """" 2>nul | python ./configure.py
set BAZEL_VC=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC
set BAZEL_VC_FULL_VERSION=14.29.30133
bazel --output_user_root F:\bazelTemp build --jobs 8 --config=opt --cxxopt=""/permissive-"" --subcommands //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_</details>"
57768,GPU detection src location,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

2

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

Tesla V100

### Current Behaviour?

```shell
Hello, 

I'm a student researcher and I am only posting this as I got no response on the Tensorflow Google group. (Please direct me to next place to the answer).

I'm trying to develop a feature where if tensorflow does not detect a local GPU, I direct the binary to my custom application (a GPU virtualization framework) for it to talk to a remote GPU execute the calls.

I want to know, in the large codebase of tensorflow where should I start looking for this? I need source file where it decides ""no GPU so use CPU"" or something similar.
```


### Standalone code to reproduce the issue

```shell
NA
```


### Relevant log output

```shell
NA
```
</details>"
57767,ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none) ERROR: No matching distribution found for tensorflow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

Latest as of 9/20/22 (trying to pip install)

### Custom Code

No

### OS Platform and Distribution

MacOS Monterey version 12.6

### Mobile device

_No response_

### Python version

3.8.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Unable to pip install tensorflow following the documentation here: https://www.tensorflow.org/install/pip. 

```shell
 venv  ~/bg  python --version                                                                                      
Python 3.8.13
 venv  ~/bg  pip --version                                                                                         
pip 22.2.2 from /Users/jennyl/adb/venv/lib/python3.8/site-packages/pip (python 3.8)
 venv  ~/bg  pip install tensorflow                                                                                
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```
"
57766,Unhandle exception,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
57765,SparseFillEmptyRowsGrad crash,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0.dev20220914

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When giving empty list into SparseFillEmptyRowsGrad, it crashes with abort. As a public API, it should check it earlier and throw an python exception.
```


### Standalone code to reproduce the issue

Reproduced on CoLab with CUDA backend: https://colab.research.google.com/drive/1nUyX5iKxKRWR2m3NHOZdoeXaUh8s3P6Z?usp=sharing

```python
import tensorflow as tf
print(tf.__version__)

tf.raw_ops.SparseFillEmptyRowsGrad(reverse_index_map=[], grad_values=[])
```


### Relevant log output

```shell
import tensorflow as tf
print(tf.__version__)

tf.raw_ops.SparseFillEmptyRowsGrad(reverse_index_map=[], grad_values=[])

2.8.2
2022-09-20 14:44:40.812917: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-09-20 14:44:40.847990: F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)
/bin/bash: line 1:   124 Aborted                 (core dumped) python bug.py 2>&1
```
</details>"
57764,tensorflow 2.9.1-gpu error kernel driver does not appear to be running on this host,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.9.1-gpu

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.1

### Mobile device

n/a

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Tensorflow 2.9.1-gpu is installed from Dockerfile.

Why is the kernel driver missing in the host if the image is gpu support ""tensorflow/tensorflow:2.9.1-gpu""?
```


### Standalone code to reproduce the issue

```shell
tf-docker /app > python3
Python 3.8.10 (default, Jun 22 2022, 20:18:18) 
[GCC 9.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

>>> import tensorflow as tf
2022-09-20 14:11:26.407642: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.

>>> print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
2022-09-20 14:11:47.727435: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: UNKNOWN ERROR (34)
2022-09-20 14:11:47.727516: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mv-inference-5f7d446b77-kmsxn): /proc/driver/nvidia/version does not exist
Num GPUs Available:  0

>>> print(tf.__version__)
2.9.1
```


### Relevant log output

_No response_</details>"
57763,A flag to enable the previous RNG behavior for `tf.keras.initializers`,"### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

TF 2.10

### Custom Code

No

### Current Behaviour?

According to [release note](https://github.com/tensorflow/tensorflow/releases/tag/v2.10.0), RNG behavior change for `tf.keras.initializers` since `TF 2.10`.

In particular, `tf.random.set_seed(seed)` is not enough to get the same model weights - we need to set explicit seeds in the initializers.

In general, this is NOT an issue. However, for **testing purpose**, this makes things much more difficult. For example, let's say we have `MyCustomModel`, implemented with layers without specifying any initializer (therefore, no explicit seed with initializer).

However, if we want to have a CI test that verifies the model produces the same result, it is impossible with TF 2.10 due to the new change. With TF < 2.10, we can set `tf.random.set_seed(seed=0)`, and the model weights will be the same, so we can check if the output matches the expected values.

It would be great if there is a way to have previous behavior for **testing purpose**."
57761,Tensorflow failed to build with Download from archive/13c6828bedeb815ee7748f82ca36073dbd55a9db.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

master branch commit a098414

### Custom Code

No

### OS Platform and Distribution

windows server 2019

### Mobile device

No response

### Python version

No response

### Bazel version

_No response_

### GCC/Compiler version

No response

### CUDA/cuDNN version

No response

### GPU model and memory

No response

### Current Behaviour?

```shell
WARNING:c
Loading: 0 packages loaded
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded, 0 targets configured)
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/9ca259d5092e9cf2c1fa0788a470df6a4fc95f0a.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (272 packages loaded, 4050 targets configured)
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (528 packages loaded, 28314 targets configured)
ERROR: F:/gitp/tensorflow/tensorflow/tensorflow/python/BUILD:3654:8: in cmd attribute of genrule rule //tensorflow/python:pywrap_tensorflow_import_lib_file: variable '$<' : no input file
ERROR: F:/gitp/tensorflow/tensorflow/tensorflow/python/BUILD:3654:8: Analysis of target '//tensorflow/python:pywrap_tensorflow_import_lib_file' failed
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: 
INFO: Elapsed time: 533.961s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (529 packages loaded, 29595 targets configured)
FAILED: Build did NOT complete successfully (529 packages loaded, 29595 targets configured)
```


### Standalone code to reproduce the issue

```shell
1. git clone https://github.com/tensorflow/tensorflow F:\gitP\tensorflow\tensorflow
2. cd F:\gitP\tensorflow\tensorflow
3. set VSCMD_SKIP_SENDTELEMETRY=1 & ""C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\Tools\VsDevCmd.bat"" -host_arch=amd64 -arch=amd64
4. python.exe -m pip install evergreen.py 2>&1
5. pip3 install six numpy wheel 2>&1
6. pip3 install keras_applications==1.0.6 --no-deps 2>&1
7. pip3 install keras_preprocessing==1.0.5 --no-deps 2>&1
8. pip3 install requests==2.26.0 --no-deps 2>&1
9. pip3 install packaging --no-deps 2>&1
10.set PATH=F:\gitP\tensorflow\tensorflow\..\tools;%path%
11.set PATH=F:\gitP\tensorflow\tensorflow\..\tools\msys64\usr\bin;%path%
12.yes """" 2>nul | python ./configure.py 2>&1
13.cd F:\gitP\tensorflow\tensorflow
14.set BAZEL_VC=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC
15.set BAZEL_VC_FULL_VERSION=14.29.30133
16.set PATH=F:\gitP\tensorflow\tensorflow\..\tools;%path%
17.set PATH=F:\gitP\tensorflow\tensorflow\..\tools\msys64\usr\bin;%path%
18.bazel --output_user_root F:\bazelTemp build --jobs 8 --config=opt --subcommands //tensorflow/tools/pip_package:build_pip_package 2>&1
```


### Relevant log output

```shell
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'build' from f:\gitp\tensorflow\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Python39/python.exe
INFO: Reading rc options for 'build' from f:\gitp\tensorflow\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_ui_max_stdouterr_bytes=-1 --experimental_cc_shared_library --experimental_link_static_libraries_once=true
INFO: Reading rc options for 'build' from f:\gitp\tensorflow\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Python39/python.exe --action_env PYTHON_LIB_PATH=C:/Python39/lib/site-packages --python_path=C:/Python39/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from f:\gitp\tensorflow\tensorflow\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file f:\gitp\tensorflow\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file f:\gitp\tensorflow\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file f:\gitp\tensorflow\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX
INFO: Found applicable config definition build:windows in file f:\gitp\tensorflow\tensorflow\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file f:\gitp\tensorflow\tensorflow\.bazelrc: --define framework_shared_object=false --experimental_link_static_libraries_once=false
Loading: 
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/5c2c5ceec2c0019a4f011538b7fb023f771f1a82.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
```
</details>"
57760,ERROR:tensorflow:Remote function on worker /job:worker/replica:0/task:4 failed with UnavailableError():Graph execution error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


During the distributed training of Tensorflow, an error is reported: the pods cannot communicate with each other, and this phenomenon occurs after the step or batch size is slightly larger。


Any suggestions?


### Standalone code to reproduce the issue


```shell

variable_partitioner = tf.distribute.experimental.partitioners.MinSizePartitioner(min_shard_bytes=(256 << 10),max_shards=n_ps)
strategy = tf.distribute.ParameterServerStrategy(cluster_resolver, variable_partitioner=variable_partitioner)
coordinator = tf.distribute.coordinator.ClusterCoordinator(strategy)
 with coordinator.strategy.scope():
                preprocessor = self.get_preprocessor()
                optimizer = self.get_optimizer()
ds_train_distributed = coordinator.create_per_worker_dataset(per_worker_dataset_fn(preprocessor=preprocessor))
ds_valid_distributed = coordinator.create_per_worker_dataset(ds_validation)

steps_per_epoch = 10
for i_batch in range(steps_per_epoch):
        coordinator.schedule(train_step_fn, args=(it_train_distributed,))
coordinator.join()
```





### Relevant log output


ERROR:

```shell
ERROR:tensorflow:Remote function on worker /job:worker/replica:0/task:4 failed with UnavailableError():Graph execution error:

2 root error(s) found.
  (0) UNAVAILABLE:  Socket closed
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638747.951542134"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}
         [[{{node GroupCrossDeviceControlEdges_0/Identity/_1905}}]]
  (1) UNAVAILABLE:  Socket closed
Additional GRPC error information from remote target /job:ps/replica:0/task:0/device:CPU:0:
:{""created"":""@1663638747.951514658"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}
0 successful operations.
0 derived errors ignored.
Additional GRPC error information from remote target /job:worker/replica:0/task:4:
:{""created"":""@1663638747.953756298"",""description"":""Error received from peer ipv4:10.244.14.62:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""2 root error(s) found.\n  (0) UNAVAILABLE:  Socket closed\nAdditional GRPC error information from remote target /job:ps/replica:0/task:0:\n:{""created"":""@1663638747.951542134"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}\n\t [[{{node GroupCrossDeviceControlEdges_0/Identity/_1905}}]]\n  (1) UNAVAILABLE:  Socket closed\nAdditional GRPC error information from remote target /job:ps/replica:0/task:0/device:CPU:0:\n:{""created"":""@1663638747.951514658"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}\n0 successful operations.\n0 derived errors ignored."",""grpc_status"":14} [Op:__inference_train_step_fn_459862]
It is treated as a transient connectivity failure for now.
ERROR:tensorflow:Remote function on worker /job:worker/replica:0/task:6 failed with UnavailableError():Graph execution error:

2 root error(s) found.
  (0) UNAVAILABLE:  Socket closed
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638747.950653934"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}
         [[{{node GroupCrossDeviceControlEdges_0/Identity/_1905}}]]
  (1) UNAVAILABLE:  Socket closed
Additional GRPC error information from remote target /job:ps/replica:0/task:0/device:CPU:0:
:{""created"":""@1663638747.950634099"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}
0 successful operations.
0 derived errors ignored.
Additional GRPC error information from remote target /job:worker/replica:0/task:6:
:{""created"":""@1663638747.954198209"",""description"":""Error received from peer ipv4:10.244.17.171:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""2 root error(s) found.\n  (0) UNAVAILABLE:  Socket closed\nAdditional GRPC error information from remote target /job:ps/replica:0/task:0:\n:{""created"":""@1663638747.950653934"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}\n\t [[{{node GroupCrossDeviceControlEdges_0/Identity/_1905}}]]\n  (1) UNAVAILABLE:  Socket closed\nAdditional GRPC error information from remote target /job:ps/replica:0/task:0/device:CPU:0:\n:{""created"":""@1663638747.950634099"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}\n0 successful operations.\n0 derived errors ignored."",""grpc_status"":14} [Op:__inference_train_step_fn_459862]
It is treated as a transient connectivity failure for now.
ERROR:tensorflow:/job:worker/task:7 encountered the following error when processing closure: UnavailableError():Graph execution error:

Socket closed
Additional GRPC error information from remote target /job:ps/replica:0/task:0/device:CPU:0:
:{""created"":""@1663638747.951031130"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}
Additional GRPC error information from remote target /job:worker/replica:0/task:7:
:{""created"":""@1663638747.953744606"",""description"":""Error received from peer ipv4:10.244.12.45:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":"" Socket closed\nAdditional GRPC error information from remote target /job:ps/replica:0/task:0/device:CPU:0:\n:{""created"":""@1663638747.951031130"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"",""grpc_status"":14} [Op:__inference_train_step_fn_459862]
ERROR:tensorflow:/job:worker/task:1 encountered the following error when processing closure: UnavailableError():Graph execution error:

Bad address
Additional GRPC error information from remote target /job:worker/replica:0/task:1:
:{""created"":""@1663638746.887806160"",""description"":""Error received from peer ipv4:10.244.4.222:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Bad address"",""grpc_status"":14}
         [[{{node gradient_tape/esmm/d7_day_sort_books_start_ts_seq_10/d7_day_sort_books_start_ts_seq_10_2/layer_normalization_12/batchnorm/mul/Reshape_1/_1654}}]]
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638746.919809197"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Bad address\nAdditional GRPC error information from remote target /job:worker/replica:0/task:1:\n:{""created"":""@1663638746.887806160"",""description"":""Error received from peer ipv4:10.244.4.222:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Bad address"",""grpc_status"":14}\n\t [[{{node gradient_tape/esmm/d7_day_sort_books_start_ts_seq_10/d7_day_sort_books_start_ts_seq_10_2/layer_normalization_12/batchnorm/mul/Reshape_1/_1654}}]]"",""grpc_status"":14}
         [[GroupCrossDeviceControlEdges_0/Identity/_1905]]
Additional GRPC error information from remote target /job:worker/replica:0/task:1:
:{""created"":""@1663638747.953384471"",""description"":""Error received from peer ipv4:10.244.4.222:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":"" Bad address\nAdditional GRPC error information from remote target /job:worker/replica:0/task:1:\n:{""created"":""@1663638746.887806160"",""description"":""Error received from peer ipv4:10.244.4.222:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Bad address"",""grpc_status"":14}\n\t [[{{node gradient_tape/esmm/d7_day_sort_books_start_ts_seq_10/d7_day_sort_books_start_ts_seq_10_2/layer_normalization_12/batchnorm/mul/Reshape_1/_1654}}]]\nAdditional GRPC error information from remote target /job:ps/replica:0/task:0:\n:{""created"":""@1663638746.919809197"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Bad address\nAdditional GRPC error information from remote target /job:worker/replica:0/task:1:\n:{""created"":""@1663638746.887806160"",""description"":""Error received from peer ipv4:10.244.4.222:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Bad address"",""grpc_status"":14}\n\t [[{{node gradient_tape/esmm/d7_day_sort_books_start_ts_seq_10/d7_day_sort_books_start_ts_seq_10_2/layer_normalization_12/batchnorm/mul/Reshape_1/_1654}}]]"",""grpc_status"":14}\n\t [[GroupCrossDeviceControlEdges_0/Identity/_1905]]"",""grpc_status"":14} [Op:__inference_train_step_fn_459862]
ERROR:tensorflow:/job:worker/task:3 encountered the following error when processing closure: UnavailableError():Graph execution error:

Socket closed
Additional GRPC error information from remote target /job:ps/replica:0/task:0/device:CPU:0:
:{""created"":""@1663638747.951631068"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}
Additional GRPC error information from remote target /job:worker/replica:0/task:3:
:{""created"":""@1663638747.968857695"",""description"":""Error received from peer ipv4:10.244.9.234:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":"" Socket closed\nAdditional GRPC error information from remote target /job:ps/replica:0/task:0/device:CPU:0:\n:{""created"":""@1663638747.951631068"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"",""grpc_status"":14} [Op:__inference_train_step_fn_459862]
ERROR:tensorflow:/job:worker/task:2 encountered the following error when processing closure: UnavailableError():Graph execution error:

Socket closed
Additional GRPC error information from remote target /job:ps/replica:0/task:0/device:CPU:0:
:{""created"":""@1663638747.953042461"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}
Additional GRPC error information from remote target /job:worker/replica:0/task:2:
:{""created"":""@1663638747.953651467"",""description"":""Error received from peer ipv4:10.244.5.120:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":"" Socket closed\nAdditional GRPC error information from remote target /job:ps/replica:0/task:0/device:CPU:0:\n:{""created"":""@1663638747.953042461"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}"",""grpc_status"":14} [Op:__inference_train_step_fn_459862]
ERROR:tensorflow:/job:worker/task:0 encountered the following error when processing closure: UnavailableError():Graph execution error:

2 root error(s) found.
  (0) UNAVAILABLE:  Socket closed
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638747.951607493"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}
         [[{{node esmm/binary_tower/mlp/batch_normalization_9/AssignMovingAvg_1/ReadVariableOp/_1240}}]]
  (1) UNAVAILABLE:  Socket closed
Additional GRPC error information from remote target /job:ps/replica:0/task:0/device:CPU:0:
:{""created"":""@1663638747.951584419"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}
0 successful operations.
0 derived errors ignored.
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{""created"":""@1663638747.962150123"",""description"":""Error received from peer ipv4:10.244.7.96:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""2 root error(s) found.\n  (0) UNAVAILABLE:  Socket closed\nAdditional GRPC error information from remote target /job:ps/replica:0/task:0:\n:{""created"":""@1663638747.951607493"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}\n\t [[{{node esmm/binary_tower/mlp/batch_normalization_9/AssignMovingAvg_1/ReadVariableOp/_1240}}]]\n  (1) UNAVAILABLE:  Socket closed\nAdditional GRPC error information from remote target /job:ps/replica:0/task:0/device:CPU:0:\n:{""created"":""@1663638747.951584419"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}\n0 successful operations.\n0 derived errors ignored."",""grpc_status"":14} [Op:__inference_train_step_fn_459862]
ERROR:tensorflow:/job:worker/task:5 encountered the following error when processing closure: UnavailableError():Graph execution error:

2 root error(s) found.
  (0) UNAVAILABLE:  Socket closed
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638747.952328566"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}
         [[{{node esmm/binary_tower/mlp/batch_normalization_1/Cast/ReadVariableOp/_1156}}]]
  (1) UNAVAILABLE:  Socket closed
Additional GRPC error information from remote target /job:ps/replica:0/task:0/device:CPU:0:
:{""created"":""@1663638747.952274779"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}
0 successful operations.
0 derived errors ignored.
Additional GRPC error information from remote target /job:worker/replica:0/task:5:
:{""created"":""@1663638747.969053922"",""description"":""Error received from peer ipv4:10.244.13.94:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""2 root error(s) found.\n  (0) UNAVAILABLE:  Socket closed\nAdditional GRPC error information from remote target /job:ps/replica:0/task:0:\n:{""created"":""@1663638747.952328566"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}\n\t [[{{node esmm/binary_tower/mlp/batch_normalization_1/Cast/ReadVariableOp/_1156}}]]\n  (1) UNAVAILABLE:  Socket closed\nAdditional GRPC error information from remote target /job:ps/replica:0/task:0/device:CPU:0:\n:{""created"":""@1663638747.952274779"",""description"":""Error received from peer ipv4:10.244.14.61:2222"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14}\n0 successful operations.\n0 derived errors ignored."",""grpc_status"":14} [Op:__inference_train_step_fn_459862]
ERROR:tensorflow:/job:worker/task:4 encountered the following error when processing closure: UnavailableError():Graph execution error:
```
```shell
This could happen if the remote target has been disconnected from the client.
2022-09-20 09:52:31.126506: E tensorflow/core/common_runtime/eager/context.cc:856] Failed to register function remotely due to failed to connect to all addresses
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638751.126446506"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3940,""referenced_errors"":[{""created"":""@1663638750.666195604"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":392,""grpc_status"":14}]}
This could happen if the remote target has been disconnected from the client.
2022-09-20 09:52:31.136138: E tensorflow/core/common_runtime/eager/context.cc:856] Failed to register function remotely due to failed to connect to all addresses
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638751.136078375"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3940,""referenced_errors"":[{""created"":""@1663638750.666195604"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":392,""grpc_status"":14}]}
This could happen if the remote target has been disconnected from the client.
2022-09-20 09:52:31.144724: E tensorflow/core/common_runtime/eager/context.cc:856] Failed to register function remotely due to failed to connect to all addresses
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638751.144684949"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3940,""referenced_errors"":[{""created"":""@1663638750.666195604"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":392,""grpc_status"":14}]}
This could happen if the remote target has been disconnected from the client.
2022-09-20 09:52:31.153434: E tensorflow/core/common_runtime/eager/context.cc:856] Failed to register function remotely due to failed to connect to all addresses
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638751.153396204"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3940,""referenced_errors"":[{""created"":""@1663638750.666195604"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":392,""grpc_status"":14}]}
This could happen if the remote target has been disconnected from the client.
2022-09-20 09:52:31.162113: E tensorflow/core/common_runtime/eager/context.cc:856] Failed to register function remotely due to failed to connect to all addresses
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638751.162074801"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3940,""referenced_errors"":[{""created"":""@1663638750.666195604"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":392,""grpc_status"":14}]}
This could happen if the remote target has been disconnected from the client.
2022-09-20 09:52:31.170673: E tensorflow/core/common_runtime/eager/context.cc:856] Failed to register function remotely due to failed to connect to all addresses
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638751.170635882"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3940,""referenced_errors"":[{""created"":""@1663638750.666195604"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":392,""grpc_status"":14}]}
This could happen if the remote target has been disconnected from the client.
2022-09-20 09:52:31.179101: E tensorflow/core/common_runtime/eager/context.cc:856] Failed to register function remotely due to failed to connect to all addresses
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638751.179053769"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3940,""referenced_errors"":[{""created"":""@1663638750.666195604"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":392,""grpc_status"":14}]}
This could happen if the remote target has been disconnected from the client.
2022-09-20 09:52:31.187572: E tensorflow/core/common_runtime/eager/context.cc:856] Failed to register function remotely due to failed to connect to all addresses
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638751.187531278"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3940,""referenced_errors"":[{""created"":""@1663638750.666195604"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":392,""grpc_status"":14}]}
This could happen if the remote target has been disconnected from the client.
2022-09-20 09:52:31.196391: E tensorflow/core/common_runtime/eager/context.cc:856] Failed to register function remotely due to failed to connect to all addresses
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638751.196351443"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3940,""referenced_errors"":[{""created"":""@1663638750.666195604"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":392,""grpc_status"":14}]}
This could happen if the remote target has been disconnected from the client.
2022-09-20 09:52:31.205141: E tensorflow/core/common_runtime/eager/context.cc:856] Failed to register function remotely due to failed to connect to all addresses
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638751.205082316"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3940,""referenced_errors"":[{""created"":""@1663638750.666195604"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":392,""grpc_status"":14}]}
This could happen if the remote target has been disconnected from the client.
2022-09-20 09:52:31.213731: E tensorflow/core/common_runtime/eager/context.cc:856] Failed to register function remotely due to failed to connect to all addresses
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638751.213689625"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3940,""referenced_errors"":[{""created"":""@1663638750.666195604"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":392,""grpc_status"":14}]}
This could happen if the remote target has been disconnected from the client.
2022-09-20 09:52:31.222359: E tensorflow/core/common_runtime/eager/context.cc:856] Failed to register function remotely due to failed to connect to all addresses
Additional GRPC error information from remote target /job:ps/replica:0/task:0:
:{""created"":""@1663638751.222320395"",""description"":""Failed to pick subchannel"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3940,""referenced_errors"":[{""created"":""@1663638750.666195604"",""description"":""failed to connect to all addresses"",""file"":""external/com_github_grpc_grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":392,""grpc_status"":14}]}
This could happen if the remote target has been disconnected from the client.
```
</details>"
57759,TFLite's max operator has wrong broadcasting behavior,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.4 LTS
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.11.0.dev20220914

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

https://colab.research.google.com/drive/1SWl6-cWwk2zRkiNYifUXtzojSz_uUCxg?usp=sharing

#### Option B: Paste your code here or provide a link to a custom end-to-end colab


### 3. Failure after conversion

After converting to TFLite, the model produces tensor with wrong shape. This is inconsistent with normal TF runtime, and numpy behavior.
"
57758,Some conv2d operations remain float32 after post training full integer quantization,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Monteley 12.5.1, Android 12
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.3

### 2. Code

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

Some of the conversion codes are as follows.  

```
converter.experimental_new_quantizer = True
converter._experimental_disable_per_channel = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
converter.representative_dataset = representative_dataset_gen
tflite_model = converter.convert()
```

### 3. Failure after conversion

- The conversion is successful, but some conv2d operations remain float32 after post training full integer quantization. Some conv2d operations are sandwiched between dequantize and quantize operation, and these conv2d operations has float32 weights and bias. All conv2d should have int8 weights and int32 biases.  
- I tried running this model on Pixel 6 NNAPI delegate with TFLite model benchmark tool. Some operations fallbacks to the XNNPACK delegate. I suspect that the fallback to XNNPACK delegate is due to the float32 conv2d layers and it causes a slight degradation in inference speed.
- How can I quantize all operations to int8 weights and int32 biases? Also, will quantizing all operations to int8 improve inference speed?

Quantized tflite model is here.  

[model_full_integer_quant.tflite.zip](https://github.com/tensorflow/tensorflow/files/9603395/model_full_integer_quant.tflite.zip)

### 4. (optional) RNN conversion support  

None.  

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.  

The command to run model benchmark tool on Pixel 6 is as follows. I found specifying `nnapi_accelerator_name=""google-edgetpu""` accelerates the inference speed the most.  

`android_aarch64_benchmark_model --graph=./model_full_integer_quant.tflite --use_nnapi=true --nnapi_accelerator_name=""google-edgetpu"" --enable_op_profiling=true`  

The log is here.  

<details>
<summary>Click here to expand the log.</summary>

```
STARTING!
Log parameter values verbosely: [0]
Graph: [./exp_2/model_full_integer_quant.tflite]
Enable op profiling: [1]
Use NNAPI: [1]
NNAPI accelerator name: [google-edgetpu]
NNAPI accelerators available: [google-edgetpu,google-armnn,nnapi-reference]
Loaded model ./exp_2/model_full_integer_quant.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for NNAPI.
NNAPI delegate created.
WARNING: NNAPI SL driver did not implement SL_ANeuralNetworksDiagnostic_registerCallbacks!
VERBOSE: Replacing 198 node(s) with delegate (TfLiteNnapiDelegate) node, yielding 7 partitions.
WARNING: NNAPI SL driver did not implement SL_ANeuralNetworksDiagnostic_registerCallbacks!
WARNING: NNAPI SL driver did not implement SL_ANeuralNetworksDiagnostic_registerCallbacks!
WARNING: NNAPI SL driver did not implement SL_ANeuralNetworksDiagnostic_registerCallbacks!
WARNING: NNAPI SL driver did not implement SL_ANeuralNetworksDiagnostic_registerCallbacks!
Explicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 4 delegate kernels.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
VERBOSE: Replacing 18 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 7 partitions.
The input model file size (MB): 3.56052
Initialized session in 1620.78ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=7 first=114164 curr=68436 min=67413 max=114164 avg=75216.3 std=15972

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=50 first=68293 curr=66811 min=66170 max=70650 avg=68201.1 std=1021

Inference timings in us: Init: 1620775, First inference: 114164, Warmup (avg): 75216.3, Inference (avg): 68201.1
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Memory footprint delta from the start of the tool (MB): init=9.42969 overall=51.1094
Profiling Info for Benchmark Initialization:
============================== Run Order ==============================
	             [node type]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	 ModifyGraphWithDelegate	 1614.218	  808.024	 50.129%	 50.129%	  1252.000	        2	ModifyGraphWithDelegate/0
	         AllocateTensors	 1607.685	  803.867	 49.871%	100.000%	     0.000	        2	AllocateTensors/0

============================== Top by Computation Time ==============================
	             [node type]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	 ModifyGraphWithDelegate	 1614.218	  808.024	 50.129%	 50.129%	  1252.000	        2	ModifyGraphWithDelegate/0
	         AllocateTensors	 1607.685	  803.867	 49.871%	100.000%	     0.000	        2	AllocateTensors/0

Number of nodes executed: 2
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	 ModifyGraphWithDelegate	        1	  1616.047	    50.129%	    50.129%	  1252.000	        2
	         AllocateTensors	        1	  1607.735	    49.871%	   100.000%	     0.000	        2

Timings (microseconds): count=1 curr=3223782
Memory (bytes): count=0
2 nodes observed



Operator-wise Profiling Info for Regular Benchmark Runs:
============================== Run Order ==============================
	             [node type]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	     TfLiteNnapiDelegate	    6.458	    6.441	  9.446%	  9.446%	     0.000	        1	[Identity]:219
	   TfLiteXNNPackDelegate	    5.023	    5.078	  7.448%	 16.894%	     0.000	        1	[tfl.quantize4]:222
	     TfLiteNnapiDelegate	   10.697	   10.688	 15.675%	 32.569%	     0.000	        1	[model/tf.math.add_33/Add;model/conv2d_12/Conv2D;model/conv2d_32/Conv2D;model/tf.math.add_33/Add/y1, model/tf.nn.relu_24/Relu;model/tf.math.add_35/Add;model/conv2d_12/Conv2D;model/conv2d_34/Conv2D;model/tf.math.add_35/Add/y, model/tf.math.add_71/Add;model/conv2d_9/Conv2D;model/conv2d_66/Conv2D;model/tf.math.add_71/Add/y1, model/tf.math.add_5/Add;model/conv2d_12/Conv2D;model/conv2d_5/Conv2D;model/tf.math.add_5/Add/y11, model/tf.math.add_56/Add;model/conv2d_12/Conv2D;model/conv2d_53/Conv2D;model/tf.math.add_56/Add/y1]:218
	   TfLiteXNNPackDelegate	   10.060	    9.159	 13.432%	 46.001%	     0.000	        1	[tfl.quantize1, tfl.quantize3]:221
	     TfLiteNnapiDelegate	   15.087	   15.096	 22.140%	 68.141%	     0.000	        1	[model/tf.math.add_14/Add;model/conv2d_12/Conv2D;model/conv2d_14/Conv2D;model/tf.math.add_14/Add/y11, model/tf.math.add_29/Add;model/conv2d_12/Conv2D;model/conv2d_28/Conv2D;model/tf.math.add_29/Add/y1, model/tf.math.add_99/Add;model/conv2d_transpose/stack;model/conv2d_transpose_7/conv2d_transpose;model/tf.math.add_99/Add/y1, model/tf.math.add_2/Add;model/conv2d_9/Conv2D;model/conv2d_2/Conv2D;model/tf.math.add_2/Add/y1, model/tf.math.add_2/Add;model/conv2d_9/Conv2D;model/conv2d_2/Conv2D;model/tf.math.add_2/Add/y11, model/tf.math.add_75/Add;model/conv2d_12/Conv2D;model/conv2d_70/Conv2D;model/tf.math.add_75/Add/y1]:217
	     TfLiteNnapiDelegate	    8.404	    8.342	 12.235%	 80.376%	     0.000	        1	[tfl.dequantize, model/tf.strided_slice/StridedSlice31, model/tf.math.add_11/Add;model/conv2d_9/Conv2D;model/conv2d_11/Conv2D;model/tf.math.add_11/Add/y11, model/tf.math.add_47/Add;model/conv2d_9/Conv2D;model/conv2d_44/Conv2D;model/tf.math.add_47/Add/y1]:216
	   TfLiteXNNPackDelegate	   12.543	   13.381	 19.624%	100.000%	     0.000	        1	[tfl.quantize, model/tf.math.add_94/Add;model/conv2d_9/Conv2D;model/conv2d_79/Conv2D;model/tf.math.add_94/Add/y11, tfl.quantize2]:220

============================== Top by Computation Time ==============================
	             [node type]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	     TfLiteNnapiDelegate	   15.087	   15.096	 22.140%	 22.140%	     0.000	        1	[model/tf.math.add_14/Add;model/conv2d_12/Conv2D;model/conv2d_14/Conv2D;model/tf.math.add_14/Add/y11, model/tf.math.add_29/Add;model/conv2d_12/Conv2D;model/conv2d_28/Conv2D;model/tf.math.add_29/Add/y1, model/tf.math.add_99/Add;model/conv2d_transpose/stack;model/conv2d_transpose_7/conv2d_transpose;model/tf.math.add_99/Add/y1, model/tf.math.add_2/Add;model/conv2d_9/Conv2D;model/conv2d_2/Conv2D;model/tf.math.add_2/Add/y1, model/tf.math.add_2/Add;model/conv2d_9/Conv2D;model/conv2d_2/Conv2D;model/tf.math.add_2/Add/y11, model/tf.math.add_75/Add;model/conv2d_12/Conv2D;model/conv2d_70/Conv2D;model/tf.math.add_75/Add/y1]:217
	   TfLiteXNNPackDelegate	   12.543	   13.381	 19.624%	 41.764%	     0.000	        1	[tfl.quantize, model/tf.math.add_94/Add;model/conv2d_9/Conv2D;model/conv2d_79/Conv2D;model/tf.math.add_94/Add/y11, tfl.quantize2]:220
	     TfLiteNnapiDelegate	   10.697	   10.688	 15.675%	 57.439%	     0.000	        1	[model/tf.math.add_33/Add;model/conv2d_12/Conv2D;model/conv2d_32/Conv2D;model/tf.math.add_33/Add/y1, model/tf.nn.relu_24/Relu;model/tf.math.add_35/Add;model/conv2d_12/Conv2D;model/conv2d_34/Conv2D;model/tf.math.add_35/Add/y, model/tf.math.add_71/Add;model/conv2d_9/Conv2D;model/conv2d_66/Conv2D;model/tf.math.add_71/Add/y1, model/tf.math.add_5/Add;model/conv2d_12/Conv2D;model/conv2d_5/Conv2D;model/tf.math.add_5/Add/y11, model/tf.math.add_56/Add;model/conv2d_12/Conv2D;model/conv2d_53/Conv2D;model/tf.math.add_56/Add/y1]:218
	   TfLiteXNNPackDelegate	   10.060	    9.159	 13.432%	 70.871%	     0.000	        1	[tfl.quantize1, tfl.quantize3]:221
	     TfLiteNnapiDelegate	    8.404	    8.342	 12.235%	 83.106%	     0.000	        1	[tfl.dequantize, model/tf.strided_slice/StridedSlice31, model/tf.math.add_11/Add;model/conv2d_9/Conv2D;model/conv2d_11/Conv2D;model/tf.math.add_11/Add/y11, model/tf.math.add_47/Add;model/conv2d_9/Conv2D;model/conv2d_44/Conv2D;model/tf.math.add_47/Add/y1]:216
	     TfLiteNnapiDelegate	    6.458	    6.441	  9.446%	 92.552%	     0.000	        1	[Identity]:219
	   TfLiteXNNPackDelegate	    5.023	    5.078	  7.448%	100.000%	     0.000	        1	[tfl.quantize4]:222

Number of nodes executed: 7
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	     TfLiteNnapiDelegate	        4	    40.564	    59.495%	    59.495%	     0.000	        4
	   TfLiteXNNPackDelegate	        3	    27.616	    40.505%	   100.000%	     0.000	        3

Timings (microseconds): count=50 first=68272 curr=66794 min=66156 max=70634 avg=68184.7 std=1022
Memory (bytes): count=0
7 nodes observed

Delegate internal: 
============================== Run Order ==============================
	             [node type]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	        DelegateOpInvoke	    0.265	    0.283	  1.025%	  1.025%	     0.000	        1	Delegate/Convert (NC, F32, QS8):5
	        DelegateOpInvoke	    1.737	    2.685	 29.222%	 30.247%	     0.000	        3	Delegate/Convolution (NHWC, F32) IGEMM:0
	        DelegateOpInvoke	    0.035	    0.035	  0.382%	 30.629%	     0.000	        3	Delegate/Convert (NC, F32, QS8):1
	        DelegateOpInvoke	    0.978	    4.353	 31.581%	 62.210%	     0.000	        2	Delegate/Convolution (NHWC, F32) IGEMM:2
	        DelegateOpInvoke	    0.133	    0.149	  1.085%	 63.295%	     0.000	        2	Delegate/Convert (NC, F32, QS8):3
	        DelegateOpInvoke	    9.371	   10.118	 36.705%	100.000%	     0.000	        1	Delegate/Convolution (NHWC, F32) IGEMM:4

============================== Top by Computation Time ==============================
	             [node type]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	        DelegateOpInvoke	    9.371	   10.118	 36.705%	 36.705%	     0.000	        1	Delegate/Convolution (NHWC, F32) IGEMM:4
	        DelegateOpInvoke	    0.978	    4.353	 31.581%	 68.286%	     0.000	        2	Delegate/Convolution (NHWC, F32) IGEMM:2
	        DelegateOpInvoke	    1.737	    2.685	 29.222%	 97.508%	     0.000	        3	Delegate/Convolution (NHWC, F32) IGEMM:0
	        DelegateOpInvoke	    0.265	    0.283	  1.025%	 98.533%	     0.000	        1	Delegate/Convert (NC, F32, QS8):5
	        DelegateOpInvoke	    0.133	    0.149	  1.085%	 99.618%	     0.000	        2	Delegate/Convert (NC, F32, QS8):3
	        DelegateOpInvoke	    0.035	    0.035	  0.382%	100.000%	     0.000	        3	Delegate/Convert (NC, F32, QS8):1

Number of nodes executed: 6
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	        DelegateOpInvoke	        6	    27.562	   100.000%	   100.000%	     0.000	       12

Timings (microseconds): count=50 first=27574 curr=26304 min=26187 max=29665 avg=27565.3 std=973
Memory (bytes): count=0
6 nodes observed
```
</details>

"
57757,pow operation gives valid output even the input is invalid,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.2

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Most of the TF operators can produce nan if input contains nan. However, pow cannot do that given certain inputs when compiling with XLA, making it hard to find problems in code.
```


### Standalone code to reproduce the issue

Code below in colab [here](https://colab.research.google.com/drive/1UIPnyZqR_xAfgRzT7zizVrGOww7HUyZE?authuser=1)

```shell

import tensorflow as tf
print(tf.__version__)
from keras import layers


class Network(tf.Module):
    def __init__(self):
        super().__init__()

    @tf.function(jit_compile=True)
    def __call__(self, x):
        y0 = tf.raw_ops.Softmax(logits=x)
        y1 = tf.sqrt(x)
        y2 = tf.pow(y0, y1)
        return y2


net = Network()
x = tf.constant(
    [-0.3523314], dtype=tf.float32,
)

tf.config.run_functions_eagerly(True)
res = net(x)
print(res)
# tf.Tensor([1.], shape=(1,), dtype=float32)
tf.config.run_functions_eagerly(False)

res = net(x)
print(res)
# tf.Tensor([nan], shape=(1,), dtype=float32)
```


### Relevant log output

```shell
2.8.2
tf.Tensor([1.], shape=(1,), dtype=float32)
tf.Tensor([nan], shape=(1,), dtype=float32)
```

_No response_</details>"
57756,Problem with @tf.function decoration: tensor out of scope ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Current behavior is I get error message 'The tensor cannot be accessed from FuncGraph, because it was defined in FuncGraph which is out of scope.' due to line 'ghat[j] = ghat[j] + ghat_update[j]' (line 35). This error only occurs if decorated with tf.function. 

I would expect the behavior to work in tf.function mode the same as eager mode.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf


class Outer(tf.keras.Model):
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.mem = None
        self.loss_mem = None

    def loss(self, x):
        return 2*x-1

    @tf.function
    def gradient(self, x):
        self.mem = tf.TensorArray(tf.float32, size=10)
        self.loss_mem = tf.TensorArray(tf.float32, size=10)
        for i in tf.range(10):
            out_i = self.model.call(x)
            loss_i = self.loss(out_i)
            self.mem = self.mem.write(i, out_i)
            self.loss_mem = self.loss_mem.write(i, loss_i)

        ghat = [tf.zeros_like(i, dtype=tf.float32) for i in self.model.trainable_variables]
        for i in tf.reverse(tf.range(10), [0]):
            with tf.GradientTape() as g:
                g.watch(x)
                out_i = self.model.call(x)
            with tf.GradientTape() as g_loss:
                g_loss.watch(out_i)
                loss_i = self.loss(out_i)
            output_grad = g_loss.gradient(loss_i, out_i)
            ghat_update = g.gradient(out_i, self.model.trainable_variables, output_gradients=output_grad)
            for j in range(len(ghat)):
                ghat[j] = ghat[j] + ghat_update[j]
        return ghat

class Inner(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.out = tf.keras.layers.Dense(10)

    def call(self, x):
        return self.out(x[0])


x = [tf.zeros((32, 4, 16), tf.float32), tf.zeros((64, 10), tf.float32)]
mytest = Outer(Inner())
mytest.gradient(x)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\Users\tawit\AppData\Roaming\JetBrains\PyCharmCE2022.2\scratches\scratch.py"", line 49, in <module>
    mytest.gradient(x)
  File ""C:\Users\tawit\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\util\traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\tawit\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 790, in capture
    raise errors.InaccessibleTensorError(
tensorflow.python.framework.errors_impl.InaccessibleTensorError: <tf.Tensor 'while_1/add:0' shape=(16, 10) dtype=float32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.
Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.

<tf.Tensor 'while_1/add:0' shape=(16, 10) dtype=float32> was defined here:
    File ""C:\Users\tawit\AppData\Roaming\JetBrains\PyCharmCE2022.2\scratches\scratch.py"", line 49, in <module>
      mytest.gradient(x)
    File ""C:\Users\tawit\AppData\Roaming\JetBrains\PyCharmCE2022.2\scratches\scratch.py"", line 25, in gradient
      for i in tf.reverse(tf.range(10), [0]):
    File ""C:\Users\tawit\AppData\Roaming\JetBrains\PyCharmCE2022.2\scratches\scratch.py"", line 34, in gradient
      for j in range(len(ghat)):
    File ""C:\Users\tawit\AppData\Roaming\JetBrains\PyCharmCE2022.2\scratches\scratch.py"", line 35, in gradient
      ghat[j] = ghat[j] + ghat_update[j]

The tensor <tf.Tensor 'while_1/add:0' shape=(16, 10) dtype=float32> cannot be accessed from FuncGraph(name=gradient, id=1901677038992), because it was defined in FuncGraph(name=while_1_body_143, id=1902270847184), which is out of scope.
```
</details>"
57755,tf.image.combined_non_max_suppression crash with segmentation fault,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0-dev20220916

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.4 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.7.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.image.combined_non_max_suppression crash with segmentation fault when `max_total_size` is given a large value
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
tf.image.combined_non_max_suppression(max_output_size_per_class=10, max_total_size=1916847725, scores=np.ones((2,2,1)), boxes=np.ones((2,2,1,4)))
```


### Relevant log output

```shell
2022-09-19 20:56:42.880536: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2022-09-19 20:56:42.880586: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)
2022-09-19 20:56:42.880661: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist
2022-09-19 20:56:42.881322: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-19 20:56:42.924370: W tensorflow/core/kernels/image/non_max_suppression_op.cc:995] Detected a large value for `max_total_size`. This may cause OOM error. (max_total_size: 1916847725)
Segmentation fault (core dumped)
```
</details>"
57754,tf.image.crop_and_resize crash (abort) when given num_boxes=0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0-dev20220916

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.4 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.7.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.image.crop_and_resize crash (abort) when given num_boxes=0
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
tf.image.crop_and_resize(crop_size=[1,1], box_indices=np.ones((0,1)), boxes=np.ones((0,4)), image=np.ones((2,2,2,2)))
```


### Relevant log output

```shell
2022-09-19 20:55:05.906144: F tensorflow/core/framework/tensor_shape.cc:45] Check failed: NDIMS == dims() (1 vs. 2)Asking for tensor of 1 dimensions from a tensor of 2 dimensions
Aborted (core dumped)
```
</details>"
57753,`UnimplementedError` when calling Conv2D in TF 2.10," 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.10, tf-nightly

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The example below is from the [Con2D documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D?hl=en). It fails to execute the basic Conv2D in TensorFlow 2.10 and tf-nightly (on CUDA). This can be reproduced in Colab.

This issue does not happen in TensorFlow 2.9 and 2.8.


### Standalone code to reproduce the issue

```shell
# In Colab:
# !pip install tensorflow==2.10
input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
```


### Relevant log output

```shell
UnimplementedError: Exception encountered when calling layer ""conv2d_2"" ""                 f""(type Conv2D).

{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} DNN library is not found. [Op:Conv2D]

Call arguments received by layer ""conv2d_2"" ""                 f""(type Conv2D):
  • inputs=tf.Tensor(shape=(4, 28, 28, 3), dtype=float32)
```"
57752,session crashes (CPU->GPU Memcpy failed) when running BatchNormalization," 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tfnightly

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When I run the code snippet below (`relu` and `BatchNormalization`) for the first time, I encounter `InternalError`. Then I try to run it again and session crashes.


### Standalone code to reproduce the issue
1. Run for a single time: throws InternalError
```shell
import tensorflow as tf
import numpy as np
print(tf.__version__)
input_data = np.random.rand(1, 3, 3, 1).astype(np.float32)
output_data = tf.keras.activations.relu(
    tf.keras.layers.BatchNormalization(axis=-1)(input_data))
```
Log output:

```shell
2.11.0-dev20220919
InternalError: Exception encountered when calling layer 'batch_normalization' (type BatchNormalization).

{{function_node __wrapped__FusedBatchNormV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} cuDNN launch failure : input shape ([1,3,3,1]) [Op:FusedBatchNormV3]

Call arguments received by layer 'batch_normalization' (type BatchNormalization):
  • inputs=tf.Tensor(shape=(1, 3, 3, 1), dtype=float32)
  • training=None
```

2. Run for a second time: crash
```
import tensorflow as tf
import numpy as np
try:
  input_data = np.random.rand(1, 3, 3, 1).astype(np.float32)
  output_data = tf.keras.activations.relu(
    tf.keras.layers.BatchNormalization(axis=-1)(input_data))
except:
  pass

input_data = np.random.rand(1, 3, 3, 1).astype(np.float32)
output_data = tf.keras.activations.relu(
    tf.keras.layers.BatchNormalization(axis=-1)(input_data)) # crash here
```
Relevant logs:
```
F tensorflow/core/common_runtime/gpu/gpu_util.cc:386] CPU->GPU Memcpy failed
```
"
57751,Wrong shape broadcasting during tensor equal check,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tfnightly

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

when comparing two shape-incompatible tensors (with shape `(3,)` and `(5,)`) using `==` on cpu, tf fails to check shape and silently produce an output. if I switch to CUDA then it has the correct behavior: throws error because the shapes cannot broadcast.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
print(tf.__version__)
with tf.device(""cpu""):
  tensor = tf.constant([1, 2, 3, 4, 5])
  print(tf.experimental.numpy.isreal(tensor) == np.array([True, False, False])) # Pass
with tf.device(""gpu""):
  tensor = tf.constant([1, 2, 3, 4, 5])
  print(tf.experimental.numpy.isreal(tensor).shape)
  print(np.array([True, False, False]).shape)
  print(tf.experimental.numpy.isreal(tensor) == np.array([True, False, False])) # Fail
```


### Relevant log output

```shell
2.11.0-dev20220919
tf.Tensor(False, shape=(), dtype=bool)
(5,)
(3,)
InvalidArgumentError: {{function_node __wrapped__Equal_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:Equal]
```
</details>"
57749,Integer divide-by-0 during fused convolution with oneDNN on CPUs supporting AVX512 instructions,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1, 2.10.0

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

No GPU

### Current Behaviour?

With oneDNN optimisations enabled and a CPU supporting AVX512 instructions, an _Integer divide-by-zero_ exception occurs during execution of fused convolutions with certain input sizes.

To reproduce:

1. Use a machine equipped with a CPU supporting AVX512 instructions (e.g. Intel i7-1165G7).
2. Enable oneDNN optimisations by setting the `TF_ENABLE_ONEDNN_OPTS` environment variable to `1` and disable GPU usage by setting the `CUDA_VISIBLE_DEVICES` environment variable to `-1`.
3. Run the script below.
4. The `python.exe` process will crash with an _Integer divide-by-zero_ exception.

The problem can be worked around by setting the `DNNL_MAX_CPU_ISA` variable to e.g. `AVX2` in order to prevent oneDNN from using AVX512 instructions.

The issue occurs in TensorFlow 2.9.1 and 2.10.0 installed using `pip`. To debug it, I've built 2.9.1 TensorFlow from source with (some) debugging symbols and obtained the following stack trace:

```
[0x0]   _pywrap_tensorflow_internal!dnnl::impl::cpu::x64::brgemm_convolution_utils::brg_blocking_t::est_eff_1x1 + 0x38d   
[0x1]   _pywrap_tensorflow_internal!dnnl::impl::cpu::x64::brgemm_convolution_utils::brg_blocking_t::calc_blocks_1x1 + 0x9fc   
[0x2]   _pywrap_tensorflow_internal!dnnl::impl::cpu::x64::brgemm_convolution_utils::init_1x1_conf + 0x434   
[0x3]   _pywrap_tensorflow_internal!dnnl::impl::cpu::x64::brgemm_1x1_convolution_fwd_t<71>::pd_t::init + 0x2f4   
[0x4]   _pywrap_tensorflow_internal!dnnl::impl::primitive_desc_t::create<dnnl::impl::cpu::x64::brgemm_1x1_convolution_fwd_t<71>::pd_t> + 0x17c   
[0x5]   _pywrap_tensorflow_internal!dnnl_primitive_desc_iterator::operator++ + 0x2c1   
[0x6]   _pywrap_tensorflow_internal!dnnl_primitive_desc_iterator_create + 0x1de   
[0x7]   _pywrap_tensorflow_internal!dnnl::primitive_desc::primitive_desc + 0x73   
[0x8]   _pywrap_tensorflow_internal!tensorflow::MklConvFwdPrimitive<float,float,float,float>::Setup + 0x6d7   
[0x9]   _pywrap_tensorflow_internal!tensorflow::MklConvFwdPrimitive<float,float,float,float>::MklConvFwdPrimitive<float,float,float,float> + 0x1a8   
[0xa]   _pywrap_tensorflow_internal!tensorflow::MklConvFwdPrimitiveFactory<float,float,float,float>::Get + 0x14c   
[0xb]   _pywrap_tensorflow_internal!tensorflow::MklConvOp<Eigen::ThreadPoolDevice,float,float,float,float,float,int,1,0,0,1>::Compute + 0x1ba0   
[0xc]   _pywrap_tensorflow_internal!tensorflow::ThreadPoolDevice::Compute + 0x4a   
[0xd]   _pywrap_tensorflow_internal!tensorflow::`anonymous namespace'::ExecutorState<tensorflow::SimplePropagatorState>::ProcessSync + 0x13f   
[0xe]   _pywrap_tensorflow_internal!tensorflow::`anonymous namespace'::ExecutorState<tensorflow::SimplePropagatorState>::Process + 0xf58   
[0xf]   _pywrap_tensorflow_internal!std::_Func_class<void>::operator() + 0xf   
[0x10]   _pywrap_tensorflow_internal!tensorflow::thread::EigenEnvironment::ExecuteTask + 0x13   
[0x11]   _pywrap_tensorflow_internal!Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop + 0x4b6   
[0x12]   _pywrap_tensorflow_internal!std::_Func_class<void>::operator() + 0xf   
[0x13]   _pywrap_tensorflow_internal!tensorflow::thread::EigenEnvironment::CreateThread::__l2::<lambda_fe7aa395b13fe170862dcdb4d85eb030>::operator() + 0x38   
[0x14]   _pywrap_tensorflow_internal!std::invoke + 0x38   
[0x15]   _pywrap_tensorflow_internal!std::_Invoker_ret<void,1>::_Call + 0x38   
[0x16]   _pywrap_tensorflow_internal!std::_Func_impl_no_alloc<<lambda_fe7aa395b13fe170862dcdb4d85eb030>,void>::_Do_call + 0x41   
[0x17]   _pywrap_tensorflow_internal!std::thread::_Invoke<std::tuple<std::function<void __cdecl(void)> >,0> + 0x18   
[0x18]   ucrtbase!thread_start<unsigned int (__cdecl*)(void *),1> + 0x42   
```


### Standalone code to reproduce the issue

```python
import tensorflow as tf

conv2d = tf.keras.layers.Conv2D(filters=8, kernel_size=(1, 1), padding='same')
relu = tf.keras.layers.ReLU()

@tf.function
def fused_conv_bias_relu(x):
    y = conv2d(x)
    y = relu(y)
    return y

x_shape = [1, 2048, 2048, 8]
inputs = tf.random.normal(x_shape)
outputs = fused_conv_bias_relu(inputs)
print(""Success!"")
```


### Relevant log output

_No response_</details>"
57748, Conv2D with XLA jit_compile=True fails to run,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0.dev20220914

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


The following code works well without `jit_compile=True`. However, if we enable XLA compilation by adding `jit_compile=True`, it will throw an error. Reproduced in CoLab notebook [here](https://colab.research.google.com/drive/12EtMiLTuXzxFsnJ2IT86JR3wwY6FekE3?usp=sharing).


### Standalone code to reproduce the issue

```python
import tensorflow as tf
from keras import layers

class MyModule(tf.Module):
    def __init__(self):
        super().__init__()
        self.conv = layers.Conv2D(2, 1, padding='valid', dtype=tf.float64, autocast=False)

    @tf.function(jit_compile=True) # without jit_compile=True works fine
    def __call__(self, i0):
        o0 = tf.floor(i0)
        o1 = self.conv(o0)
        o2 = tf.add(o1, o0)
        return o2

def simple():
    inp = {
        ""i0"": tf.constant(
            3.14, shape=[1,1,3,2], dtype=tf.float64
        ),
    }
    m = MyModule()

    out = m(**inp) # Error!

    print(out)
    print(out.shape)

if __name__ == ""__main__"":
    simple()
```


### Relevant log output

```python
2022-09-18 01:33:53.096156: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x55a9ace73180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-09-18 01:33:53.096176: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6
2022-09-18 01:33:53.098645: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-09-18 01:33:53.537659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100
2022-09-18 01:33:54.161249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:5341] Disabling cuDNN frontend for the following convolution:
  input: {count: 1 feature_map_count: 2 spatial: 1 3  value_min: 0.000000 value_max: 0.000000 layout: BatchDepthYX}
  filter: {output_feature_map_count: 2 input_feature_map_count: 2 layout: OutputInputYX shape: 1 1 }
  {zero_padding: 0 0  pad_alignment: default filter_strides: 1 1  dilation_rates: 1 1 }
  ... because it uses an identity activation.
2022-09-18 01:33:54.749772: I tensorflow/compiler/jit/xla_compilation_cache.cc:476] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
Traceback (most recent call last):
  File ""/home/colin/code/test_proj/scripts/tflite2.py"", line 41, in simple
    out = m(**inp)
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnknownError: CUDNN_STATUS_NOT_SUPPORTED
in tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc(5151): 'status' [Op:__inference___call___46]
```
</details>"
57747,pow operator output nan for valid inputs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.2, 2.11.0-dev20220914

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


Pow operator outputs nan although the input is a valid one.
This issue happens when using XLA with CUDA as the device.



### Standalone code to reproduce the issue


CoLab to reproduce it here: https://colab.research.google.com/drive/19FwLPPKeCRy1Xx5gwbj7M6hwF2RGb42i?usp=sharing

```python
import tensorflow as tf
print(tf.__version__)
from keras import layers


class MyModule(tf.Module):
    def __init__(self):
        super().__init__()

    @tf.function(jit_compile=True)
    def __call__(self, x):
        y = tf.divide(x, x)
        z = tf.pow(x, y)
        return z



sys_details = tf.sysconfig.get_build_info()
print(f'cuda_version: {sys_details[""cuda_version""]}')
print(f'cudnn_version: {sys_details[""cudnn_version""]}')
print(f'cuda_compute_capabilities: {sys_details[""cuda_compute_capabilities""]}')

m = MyModule()
x = tf.constant(
    [-0.1982182], dtype=tf.float32,
)
with tf.device('/CPU:0'):
    tf.config.run_functions_eagerly(True)
    out = m(x)
    print(out) # NOTE: RIGHT! tf.Tensor([-0.1982182], shape=(1,), dtype=float32)
    tf.config.run_functions_eagerly(False)

with tf.device('/CPU:0'):
    out = m(x)
    print(out) # NOTE: RIGHT! tf.Tensor([-0.1982182], shape=(1,), dtype=float32)

with tf.device('/GPU:0'): # NOTE: GPU needed!
    out = m(x)
    print(out) # NOTE: WRONG! tf.Tensor([nan], shape=(1,), dtype=float32)
```


### Relevant log output

```python
2.8.2
cuda_version: 11.1
cudnn_version: 8
cuda_compute_capabilities: ['compute_60', 'compute_70', 'compute_75', 'compute_80']
tf.Tensor([-0.1982182], shape=(1,), dtype=float32)
tf.Tensor([-0.1982182], shape=(1,), dtype=float32)
tf.Tensor([nan], shape=(1,), dtype=float32)
```

</details>"
57746,LRN operator outputs wrong results with `jit_compile=True`,"<details><summary>Click to expand!</summary> 
 
### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8.2

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The operator LRN outputs a valid real number like 0.35355338 although according to its definition it should give nan because the input have nan. This issue will not happen w/o XLA, so XLA should have consistent behavior.

### Standalone code to reproduce the issue

CoLab here: https://colab.research.google.com/drive/16QQ5ZUDLCoc7we7TDXvVPrb7OW_fiMNM?usp=sharing

```python
import tensorflow as tf
print(tf.__version__)
from keras import layers


class MyModule(tf.Module):
    def __init__(self):
        super().__init__()

    @tf.function(jit_compile=True)
    def __call__(self, x):
        x = tf.sqrt(x)
        x = tf.raw_ops.LRN(
            input=x,
            depth_radius=1,
            bias=1,
            alpha=1,
            beta=1,
        ) # input / (bias + alpha * sqr_sum) ** beta
        return x


m = MyModule()
x = tf.constant(
    [[[[-0.5,  0.5,  0.5]]]], dtype=tf.float32,
)
with tf.device('/CPU:0'):
    tf.config.run_functions_eagerly(True)
    out = m(x)
    print(out) # tf.Tensor([[[[nan nan nan]]]], shape=(1, 1, 1, 3), dtype=float32)
    tf.config.run_functions_eagerly(False)

with tf.device('/CPU:0'):
    out = m(x)
    print(out) # NOTE: WRONG! tf.Tensor([[[[       nan        nan 0.35355338]]]], shape=(1, 1, 1, 3), dtype=float32)
```


### Relevant log output

```python
2.8.2
tf.Tensor([[[[nan nan nan]]]], shape=(1, 1, 1, 3), dtype=float32)
tf.Tensor([[[[       nan        nan 0.35355338]]]], shape=(1, 1, 1, 3), dtype=float32)
```


</details>"
57745,"convert tflite model quantized by per tensor, accuracy benchmark with sample app(labelimg), accuracy rate is much lower that quantized by per axies","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ARMv8 mpu 
- TensorFlow installation (pip package or built from source): tensorflow 2.9.1

### 2. Code

Accuracy benchmark with labelimg, verify accuracy rate by Imagenet50k dataset(5k out of 50k),
quantzied model by per tensor (converter_experimental_disable_per_channel=True)


### 3. Benchmark
for inference time, can get about ~50% improvement (from ~6ms to ~3ms).
for accuracy rate, decrease a lot (from ~69%(per channel) to~10%(per tensor).

I wonder if there are some other convertor parameters that can be used for combination?
or is there any official benchmark data of per tensor models published to public for reference?"
57744,log operator outputs wrong results with XLA compilation,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

With XLA compilation, the operator log will output a normal real number for nan as an input. This may hide numeric issues in the network, or break numeric checking like `is_nan`, making it hard for developers to debug or handle corner cases. The XLA should have the same behavior as the eager mode.


### Standalone code to reproduce the issue

I can reproduce this issue on version `2.11.0-dev20220914`. I can also reproduce it in CoLab.

https://colab.research.google.com/drive/1KH1slOkHrd-sFgPpHTqvFC54noiaRJUO?usp=sharing

```python
import tensorflow as tf
print(tf.__version__)
from keras import layers

class MyModule(tf.Module):
    def __init__(self):
        super().__init__()

    @tf.function(jit_compile=True)
    def __call__(self, x):
        x = tf.pow(x, x)
        x = tf.math.log(x)
        # NOTE: tf.experimental.numpy.log2 will also output wrong result with XLA
        return x


def simple_diff():
    m = MyModule()
    x = tf.constant(
        -1.5, shape=[1], dtype=tf.float32,
    )
    with tf.device('/CPU:0'):
        tf.config.run_functions_eagerly(True)
        out = m(x)
        print(out) # RIGHT! tf.Tensor([nan], shape=(1,), dtype=float32)
        tf.config.run_functions_eagerly(False)
    
    with tf.device('/CPU:0'):
        out = m(x)
        print(out) # NOTE: WRONG! tf.Tensor([-0.8774437], shape=(1,), dtype=float32)


simple_diff()
```


### Relevant log output

```python
2.8.2
tf.Tensor([nan], shape=(1,), dtype=float32) # <-- right
tf.Tensor([-0.6081976], shape=(1,), dtype=float32) # <-- wrong
```
</details>"
57743,is it possible to use tf.distribute.MirroredStrategy on Intel Xeon Scalable Processors (with 2 sockets),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.4

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am using Intel Xeon Scalable Processor(with 2 sockets - 20 Cores per each Socket). My batch_size = 2048 and total batches = 2600. Is it possible to split the total batches and run 1300 batches in parallel on each CPU socket using tf.distribute.MirroredStrategy?

Here in the below code, even though my server has 2 CPU sockets, the no.of devices getting detected is just 1.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

import numpy as np
import os

print(tf.__version__)

strategy = tf.distribute.MirroredStrategy()
print(""strategy:"",strategy)

print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
```


### Relevant log output

_No response_</details>"
57742,custom op  error,"To replace the tf.py_func, I wrote custom op by c++ and use tf.load_op_library to load the op in python.And get Segmentation fault (core dumped).And when  I comment  SetShapeFn, there is no error. But the output shape of the custom op is unknow.Could someone can help me ?  Another info is using pb model to inference by c++ custom op is correct. Below is the error and code
######################################################################################################
This is the error.
######################################################################################################
WARNING:tensorflow:From` /home/zhj/Py/frustum-pointnets-master/models/tf_util.py:384: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

WARNING:tensorflow:From /home/zhj/Py/frustum-pointnets-master/models/tf_util.py:613: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /home/zhj/Py/frustum-pointnets-master/models/model_util.py:235: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From /home/zhj/Py/frustum-pointnets-master/models/model_util.py:236: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
mask.shape---------------------------- (1, 1024)
Segmentation fault (core dumped)

######################################################################################################
This is the custom op 

```
#include ""tensorflow/core/framework/common_shape_fns.h""
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/op_kernel.h""
#include ""tensorflow/core/framework/shape_inference.h""

namespace tensorflow {

using shape_inference::DimensionHandle;
using shape_inference::InferenceContext;
using shape_inference::ShapeHandle;
using shape_inference::UnchangedShape;

REGISTER_OP(""Zhj"")
    .Input(""input: float"")
    .Output(""output: int32"")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
        DimensionHandle batch = c->Dim(c->input(0), 0);
        ShapeHandle output_shape = c->MakeShape({batch,c->MakeDim(512),c->MakeDim(2)});
        c->set_output(0, output_shape);
        return Status::OK();
    });

class PyFuncZhj : public tensorflow::OpKernel {
 public:
  explicit PyFuncZhj(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& tensor_in = context->input(0);
    const TensorShape& input_shape = tensor_in.shape();
    auto inputEigen = tensor_in.shaped<float, 2>({input_shape.dim_size(0), input_shape.dim_size(1)});
    // Create an output tensor
    Tensor* output_tensor;
    OP_REQUIRES_OK(context, context->allocate_output(
                                0, TensorShape({input_shape.dim_size(0), 512,2}),
                                &output_tensor));
    auto outputEigen = output_tensor->shaped<int32, 3>({input_shape.dim_size(0), 512, 2});
    std::vector<std::vector<int>> indicesVec;
    for (int i = 0; i < input_shape.dim_size(0); i++)
    {
      std::vector<int> tmpIndices;
      for (int j = 0; j < input_shape.dim_size(1); j++)//output_shape.dim_size(1)
      {
        if(inputEigen(i,j) > 0.1)
        {
          tmpIndices.push_back(j);
        }
      }
      int lengthIndices = tmpIndices.size();
      if(lengthIndices > 0)
      {
        while(tmpIndices.size() < 512)
        {
          int rand_num = rand() % lengthIndices;
          tmpIndices.push_back(tmpIndices[rand_num]);
        }
        while (tmpIndices.size() > 512)
        {
          int rand_num = rand() % tmpIndices.size();
          tmpIndices.erase(tmpIndices.begin() + rand_num);
        }
      }
      indicesVec.push_back(tmpIndices);
    }
    for(int i = 0; i < indicesVec.size(); i++)
    {
      if(0 == indicesVec[i].size())
      {
        for(int j = 0; j < 512; j++)
        {
          outputEigen(i, j, 0) = i;  // batchsize
          outputEigen(i, j, 1) = 0;
        }
      }
      else
      {
        for(int j = 0; j < indicesVec[i].size(); j++)
        {
          outputEigen(i, j, 0) = i;  // batchsize
          outputEigen(i, j, 1) = indicesVec[i][j];
        }
      }
    }
  }
};
REGISTER_KERNEL_BUILDER(Name(""Zhj"").Device(DEVICE_CPU), PyFuncZhj);
}
```

######################################################################################################
This is the python code
```
Zhj = tf.load_op_library(""/home/zhj/Py/frustum-pointnets-master/train/log_v1/zhj.so"")
def tf_gather_object_pc(point_cloud, mask, npoints=512):
    ''' Gather object point clouds according to predicted masks.
    Input:
        point_cloud: TF tensor in shape (B,N,C)
        mask: TF tensor in shape (B,N) of 0 (not pick) or 1 (pick)
        npoints: int scalar, maximum number of points to keep (default: 512)
    Output:
        object_pc: TF tensor in shape (B,npoint,C)
        indices: TF int tensor in shape (B,npoint,2)
    '''
    def mask_to_indices(mask):
        indices = np.zeros((mask.shape[0], npoints, 2), dtype=np.int32)
        for i in range(mask.shape[0]):
            pos_indices = np.where(mask[i,:]>0.5)[0]
            # skip cases when pos_indices is empty
            if len(pos_indices) > 0: 
                if len(pos_indices) > npoints:
                    choice = np.random.choice(len(pos_indices),
                        npoints, replace=False)
                else:
                    choice = np.random.choice(len(pos_indices),
                        npoints-len(pos_indices), replace=True)
                    choice = np.concatenate((np.arange(len(pos_indices)), choice))
                np.random.shuffle(choice)
                indices[i,:,1] = pos_indices[choice]
            indices[i,:,0] = i
        return indices

    print(""mask.shape----------------------------"", mask.shape)  # mask.shape: (1,1024)
    # indices = tf.py_func(mask_to_indices, [mask], tf.int32)
    indices = Zhj.zhj([mask])
    object_pc = tf.gather_nd(point_cloud, indices)
    return object_pc, indices
```
"
57741,save preprocessor is too slow for large vocabulary size: 2500e4 lookup table size ~ 0.50 hour,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

tf 2.10.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


It takes near **an hour(too long)** to save the preprocessor for a lookup size ~ 2500e4.

Maybe related to `saved_model/save.py: list_children()`. 

It seems like it iterate the lookup table for each of 2500e4 elements. (Not sure)

Any suggestions?


### Standalone code to reproduce the issue

```shell
#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""demo of preprcoessing large layers
""""""

# pylint:disable=no-member
import time
from typing import Dict

import tensorflow as tf


@tf.keras.utils.register_keras_serializable()
def split_vertical(x):
    """"""
    用于TextVectorization的split入参, split by `|`
    """"""
    return tf.strings.split(x, sep=""|"")


# 25242127 -> 3323 seconds
class TestPreprocessor(tf.keras.Model):
    def __init__(self, name=""test_preprocessor"", vocab_size: int = 100, **kwargs):
        super().__init__(name=name, trainable=False, **kwargs)
        self._lookup = {}
        self.vocab_size = vocab_size
        print(f""\nvocab_size={vocab_size}"")

    def adapt(self):
        vocab = [str(i) for i in range(self.vocab_size)]

        tic = time.time()
        text_vectorizer = tf.keras.layers.TextVectorization(
            vocabulary=vocab,
            output_mode=""int"",
            split=split_vertical,
            standardize=None,
            output_sequence_length=7,
            name=""text_vectorization_"" + ""faked_id"",
        )
        toc = time.time()
        print(f""Construct TextVectorization: {toc - tic:.2f} seconds"")
        self._lookup[""faked_id""] = text_vectorizer

    def save(self, filepath: str = ""/tmp/debug_large_vocab""):
        ds = tf.data.Dataset.from_tensor_slices({""faked_id"": [str(e) for e in range(100)]}).batch(3).map(self)
        _ = next(iter(ds))
        tic = time.time()
        super().save(filepath)
        toc = time.time()
        print(f""Save TextVectorzation:       {toc-tic:.2f} seconds"")

    def call(self, batch: Dict):
        ans = {}
        for f, lookup in self._lookup.items():
            ans[f] = lookup(batch[f])
        return ans


def main():
    vsize = [1000, 10000, 100000, 1000000, 25000000]
    for V in vsize:
        preprocessor = TestPreprocessor(vocab_size=V)
        preprocessor.adapt()
        preprocessor.save()


if __name__ == ""__main__"":
    main()
```


### Relevant log output

```shell
vocab_size=1000
Construct TextVectorization: 0.01 seconds
Save TextVectorzation:       0.29 seconds

vocab_size=10000
Construct TextVectorization: 0.04 seconds
Save TextVectorzation:       0.68 seconds

vocab_size=100000
Construct TextVectorization: 0.35 seconds
Save TextVectorzation:       5.58 seconds

vocab_size=1000000
Construct TextVectorization: 3.58 seconds
Save TextVectorzation:       57.34 seconds

vocab_size=25000000
Construct TextVectorization: 90.79 seconds
Save TextVectorzation:       1612.00 seconds
```
</details>"
57740,MKL Tensorflow and Softmax Layer,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

tf 2.8.2

### Custom Code

No

### OS Platform and Distribution

SUSE Linux Enterprise Server 15 SP3, Kernel 5.3.18-59.19

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

While evaluating the performance benefits of using the Intel MKL / oneDNN-accelerated `intel-tensorflow` package, I noticed that the Softmax implementation used in `tf.nn.softmax` (and `tf.keras.layers.nn.Softmax`) does not seem to use oneDNN acceleration, even though there are references to such an implementation in [1]. 

On a system with an Intel Xeon Platinum 8360Y and no GPU, the Eigen implementation of Softmax underperforms and bottlenecks a model which is otherwise almost entirely accelerated by oneDNN-primitives.

Are oneDNN-accelerated, standalone Softmax layers currently supported? 

[1] https://github.com/tensorflow/tensorflow/blob/v2.8.2/tensorflow/core/kernels/mkl/mkl_softmax_op.cc


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

@tf.function
def softmax_tffunction(x):
    return tf.nn.softmax(x)

if __name__ == ""__main__"":
    softmax_tffunction(tf.random.normal([10, 10]))
```


### Relevant log output

_No response_</details>"
57739,"2022-09-17 23:49:02.600018: I tensorflow/stream_executor/gpu/asm_compiler.cc:323] ptxas warning : Registers are spilled to local memory in function 'fusion_24', 8 bytes spill stores, 16 bytes spill loads ptxas warning : Registers are spilled to local memory in function '__internal_trig_reduction_slowpathd', 4 bytes spill stores, 4 bytes spill loads","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.2

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Trying to run deepxde with tensorflow (TF2) backend.  
I think this is related to
https://github.com/tensorflow/tensorflow/issues/33375
This question partly relates to the answer provided by there

In that issue, the following answer is given
> Hi @kleyersoma. The workaround for this particular problem on unix-based machines is to link your cuda bin to your working directory. Go to the directory, where you launch your python code and create the link:
> `ln -s /full/path/to/your/cuda/installation/bin .`
> This sovles the problem. The point is that TF first tries to load the ptxas from ./bin directory, then from /usr/local/cuda/bin. Unfortunately, it completely ignores the environment variables (which I consider to be a bug).

It is not clear to me what ""the directory, where you launch your python code"" refers to

I am running Ubuntu 22.04, and 
which python3
gives
/usr/bin/python3
Is this the directory you're referring to?
If so, in my case I would do
ln -s /usr/local/cuda/bin /usr/bin/python3
Is that correct? Thanks.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1rYD_GMLAWJ6uTx76RfYvLWXB2nf9NP7Q?usp=sharing
```


### Relevant log output

```shell
U instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-17 23:48:43.139452: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-09-17 23:48:43.139501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10126 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1
mexclusions 
 []
Compiling model...
'compile' took 0.000393 s

Warning: epochs is deprecated and will be removed in a future version. Use iterations instead.
Training model...

2022-09-17 23:48:46.762887: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x560ea9633610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-09-17 23:48:46.762922: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1
2022-09-17 23:48:46.842763: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-09-17 23:48:51.944301: I tensorflow/stream_executor/gpu/asm_compiler.cc:323] ptxas warning : Registers are spilled to local memory in function 'input_fusion_reduce_4', 33468 bytes spill stores, 38624 bytes spill loads
ptxas warning : Registers are spilled to local memory in function '__internal_accurate_pow', 132 bytes spill stores, 132 bytes spill loads
ptxas warning : Registers are spilled to local memory in function '__internal_trig_reduction_slowpathd', 56 bytes spill stores, 48 bytes spill loads

2022-09-17 23:48:51.953246: I tensorflow/compiler/jit/xla_compilation_cache.cc:478] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-09-17 23:48:57.307073: I tensorflow/stream_executor/gpu/asm_compiler.cc:323] ptxas warning : Registers are spilled to local memory in function 'input_fusion_reduce_4', 33468 bytes spill stores, 38624 bytes spill loads
ptxas warning : Registers are spilled to local memory in function '__internal_accurate_pow', 132 bytes spill stores, 132 bytes spill loads
ptxas warning : Registers are spilled to local memory in function '__internal_trig_reduction_slowpathd', 56 bytes spill stores, 48 bytes spill loads

Step      Train loss                                                                                    Test loss                                                                                     Test metric
0         [1.98e+03, 3.67e+02, 3.39e-02, 2.04e-01, 3.69e-02, 2.30e-01, 9.99e-02, 2.29e-01, 7.95e-02]    [1.98e+03, 3.67e+02, 3.39e-02, 2.04e-01, 3.69e-02, 2.30e-01, 9.99e-02, 2.29e-01, 7.95e-02]    []  
2022-09-17 23:49:02.600018: I tensorflow/stream_executor/gpu/asm_compiler.cc:323] ptxas warning : Registers are spilled to local memory in function 'fusion_24', 8 bytes spill stores, 16 bytes spill loads
ptxas warning : Registers are spilled to local memory in function '__internal_trig_reduction_slowpathd', 4 bytes spill stores, 4 bytes spill loads
```
</details>"
57738,Crashes with no error message but exit code -1073740791 (0xC0000409) on CUDA 11.7,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.2, tf 2.10 both affected (did not test previous versions)

### Custom Code

Yes

### OS Platform and Distribution

Windows 10 19043

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7/8401

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Any python script that attempts to train a tensorflow model immediately crashes during model.fit call.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import *

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  # normalize to between 0-1

# model layers
xIn = Input((28, 28, 1))
x = Conv2D(32, (3, 3), activation='relu')(xIn)
x = Dropout(0.4)(x)
x = Conv2D(64, (3, 3), activation='relu')(x)
x = Dropout(0.4)(x)
x = Conv2D(128, (3, 3), activation='relu')(x)
x = Dropout(0.4)(x)
x = Flatten()(x)
x = Dense(128, activation='swish')(x)
x = Dropout(0.5)(x)
xOut = Dense(10)(x)

model = Model(inputs=xIn, outputs=xOut)

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy'])

callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=5, verbose=1)
]

model.summary()
model.fit(x_train, y_train, epochs=100, batch_size=16, validation_data=(x_test, y_test), callbacks=callbacks)
```


### Relevant log output

```shell
""C:\Program Files\Python39\python.exe"" C:\Users\alien\Documents\PyCharm-Projects\IRS-ML\test.py 
2022-09-18 12:12:09.933811: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-18 12:12:10.296700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9436 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6
Model: ""model""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 28, 28, 1)]       0         
                                                                 
 conv2d (Conv2D)             (None, 26, 26, 32)        320       
                                                                 
 dropout (Dropout)           (None, 26, 26, 32)        0         
                                                                 
 conv2d_1 (Conv2D)           (None, 24, 24, 64)        18496     
                                                                 
 dropout_1 (Dropout)         (None, 24, 24, 64)        0         
                                                                 
 conv2d_2 (Conv2D)           (None, 22, 22, 128)       73856     
                                                                 
 dropout_2 (Dropout)         (None, 22, 22, 128)       0         
                                                                 
 flatten (Flatten)           (None, 61952)             0         
                                                                 
 dense (Dense)               (None, 128)               7929984   
                                                                 
 dropout_3 (Dropout)         (None, 128)               0         
                                                                 
 dense_1 (Dense)             (None, 10)                1290      
                                                                 
=================================================================
Total params: 8,023,946
Trainable params: 8,023,946
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
2022-09-18 12:12:11.925564: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401

Process finished with exit code -1073740791 (0xC0000409)
```

![image](https://user-images.githubusercontent.com/20109683/190885518-bbaca094-59d3-43cc-8580-1e5a0636314f.png)
can see GPU memory being briefly taken up before the crash and GPU usage spikes to 100 for a second too.
</details>"
57737,Does op ADD support int8/int16 dtype? ,"Hi, I just got a issue about the op ADDV2. My model has the following part that I need to cast the float32 input to int8/int16/int32, then cast it back. When I cast the input to int8/int16, the final addition op changed from ADD to ADDV2 as I checked the graph. Hence, how can I avoid op ADDV2 and use op ADD for dtype int8/int16 (if possible), thanks!

```python
  def reshape_add(self, r, yy, method, name):
      """"""Match the dimension for residuals and add to activations.""""""
      if r is None:
          return yy
      r = tf.cast(r, tf.int16)
      out_features = yy.shape[3]
      in_features = r.shape[3]
      if in_features > out_features:
          r = r[:, :, :, :out_features]
      elif in_features < out_features:
          # assert method in ['tile', 'zeropad']
          if method == 'tile':
              ch_mult = out_features // in_features
              r = tf.tile(r, (1, 1, 1, ch_mult))
          elif method == 'zeropad':
              pad_size = out_features - in_features
              pad_width = ((0, 0), (0, 0), (0, 0), (0, pad_size))
              r = tf.pad(r, pad_width, mode = ""CONSTANT"")

      if yy.shape[1] != r.shape[1]:
          # r = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(r)
          r = r[:, :yy.shape[1], :yy.shape[1], :]

      yy_int = tf.cast(yy, tf.int16)
      sum_two = tf.add(yy_int, r)
      return tf.cast(sum_two, tf.float32)
```

### Environment

TensorFlow version: 2.9.1
python version:  3.8.13
"
57735,Vulnerability CVE-2022-37434,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.6.5

### Custom Code

No

### OS Platform and Distribution

EulerOS 2.5\EulerOS 2.10

### Mobile device

-

### Python version

3.9

### Bazel version

-

### GCC/Compiler version

-

### CUDA/cuDNN version

-

### GPU model and memory

-

### Current Behaviour?

```shell
Hi there, it seems that zlib 1.2.12 has a vulnerability CVE-2022-37434. Please fix it or confirm tf 2.6.5 is not affected by it.
Here is the detail:
https://nvd.nist.gov/vuln/detail/CVE-2022-37434
```


### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

_No response_</details>"
57734,Vulnerability CVE-2022-32207、CVE-2022-32205、CVE-2022-32206、CVE-2022-32208、CVE-2022-35252 ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.6.5

### Custom Code

No

### OS Platform and Distribution

EulerOS 2.5\EulerOS 2.10

### Mobile device

-

### Python version

3.9

### Bazel version

-

### GCC/Compiler version

-

### CUDA/cuDNN version

-

### GPU model and memory

-

### Current Behaviour?

```shell
Hi there, it seems that curl 7.83.1 has a vulnerability CVE-2022-32207 CVE-2022-32205 CVE-2022-32206 CVE-2022-32208 CVE-2022-35252. Please fix it or confirm tf 2.6.5 is not affected by it.
Here is the detail:
https://nvd.nist.gov/vuln/detail/CVE-2022-32207
https://nvd.nist.gov/vuln/detail/CVE-2022-32205
https://nvd.nist.gov/vuln/detail/CVE-2022-32206
https://nvd.nist.gov/vuln/detail/CVE-2022-32208
https://vulners.com/redhatcve/RH:CVE-2022-35252
```


### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

```shell
-
```
</details>"
57733,Clang-9 incompatibility with mkl_conv_ops.cc,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

HEAD

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Failure to build with clang-9 in mkl_conv_ops.cc, saying to insert `template` where some type inference fails.
```


### Standalone code to reproduce the issue

```shell
At head, build with clang-9.
```


### Relevant log output

```shell
For our project, we have cherry-picked this patch in, which fixes the issue. We need to land it upstream: https://github.com/iree-org/iree-tf-fork/commit/8667909c4243b8526bff6260ef6c810061d76070
```
</details>"
57731,error with building with bazel,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

No

### OS Platform and Distribution

pop_os

### Mobile device

_No response_

### Python version

3.9

### Bazel version

0.26.0

### GCC/Compiler version

11.2.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
crashes after 10-15 min of compiling. Im running first gen intel so some of the options i have selected cant be run?
```


### Standalone code to reproduce the issue

```shell
i dont know if is repeatable but i ran this: bazel build -c opt --copt=-mfma --copt=-msse4.2 //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /home/bryce/.cache/bazel/_bazel_bryce/bd1d213a6ffc70c3426679580b755b10/external/com_google_absl/absl/synchronization/BUILD.bazel:29:1: C++ compilation of rule '@com_google_absl//absl/synchronization:graphcycles_internal' failed (Exit 1)
```
</details>"
57730,Unable to import hdf5_format  from tensorflow.python.keras.saving,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.10.0-rc3-6-g359c3cdfc5f 2.10.0

### Custom Code

Yes

### OS Platform and Distribution

MacOS 12.5.1

### Mobile device

_No response_

### Python version

Python 3.8.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In the past I was able to load the `hdf5_format` module as follows:

```
from tensorflow.python.keras.saving import hdf5_format
```

Now this code seems not accessible anymore:

```
ModuleNotFoundError: No module named 'tensorflow.python'
``` 

How to import the `hdf5_format` module in TF2.10.0?


### Standalone code to reproduce the issue

```
from tensorflow.python.keras.saving import hdf5_format
```


### Relevant log output

```
ModuleNotFoundError: No module named 'tensorflow.python'
```
</details>"
57729,Silent update in google play for android mobile release crash tensorflowlite library #2,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
No possible , I just can see logs from play console.A lot  devices of vendor Samsung,Xiaomi,Sony,Oppo,Huawei,Google and etc
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
- implementation ""com.google.android.play:core:1.10.0""
    implementation 'com.google.android.play:core-ktx:1.8.1'
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):
22.26.15 or other version from users devices.
**Standalone code to reproduce the issue**
Install new version from google play. Update application from old build to new with new tensorflow files.
**Any other info / logs**
I attach all logs  and more details here
https://github.com/tensorflow/tensorflow/issues/57696"
57728,tf.random.poisson crash(abort),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0-dev20220914

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.4 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.7.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.random.poisson crash(abort)
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf 
tf.random.poisson(lam=np.ones((10,10,11,2)), shape=[27, 187, 229])
```


### Relevant log output

```shell
2022-09-16 19:45:10.220556: F tensorflow/core/util/work_sharder.cc:34] Check failed: total >= 0 (0 vs. -1751281096)
Aborted (core dumped)
```
</details>"
57725,"log, pow, div unsupported in 16x8 precision","It seems these ops are supported in 8x8 mode, but not in 16x8. The latter is very important for our organization, as it saves a lot of rounding error over 8x8. Is full 16x8 support on the current roadmap? If not, could we help to contribute this feature?

### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04
- TensorFlow installation (pip package or built from source): pip tf-nightly
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.11.0-dev20220916 

### 2. Code

```
import numpy as np
import tensorflow as tf
import keras

def data_generator():
    for x in range(1, 100):
        yield [np.array(x).astype(np.float32)]

def test_layer(layer):
    model = tf.keras.Model(inputs=[in_tensor], outputs=[layer])
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.representative_dataset = data_generator
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8
    ]
    converter.inference_input_type = tf.int16
    converter.inference_output_type = tf.int16
    try:
        tflite_model = converter.convert()
    except Exception as e:
        print(""Received exception:\n%s"" % str(e))

in_tensor = tf.keras.Input(shape=(1,))
test_layer(tf.math.log(in_tensor))
test_layer(in_tensor ** 3)
test_layer(in_tensor / in_tensor)
```

### 3. Failure after conversion

Conversion fails, with the following error messages:

> Quantization to 16x8-bit not yet supported for op: 'LOG'.
> Quantization to 16x8-bit not yet supported for op: 'POW'.
> Quantization to 16x8-bit not yet supported for op: 'DIV'.
"
57724,`tf.random.learned_unigram_candidate_sampler` crash with segmentation fault,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0-dev20220914

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.4 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.7.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current Behaviour?


`tf.random.learned_unigram_candidate_sampler` crash with segmentation fault when `batch_size` is large.

Also reproduced in the [gist](https://colab.research.google.com/drive/1sz2P703JMLaq_6DR92jf4R-d5jxs4I52?usp=sharing)



### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
tf.random.learned_unigram_candidate_sampler(true_classes=np.array([[1000000]]), num_true=1,num_sampled=1,unique=False,range_max=1)
```


### Relevant log output

```shell
Segmentation fault (core dumped)
```
</details>"
57722,Tensorflow Integration Issue with Cloudpickle for AdamW optimizer,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi team,
Tensorflow model with AdamWeightDecay optimizer does not load with cloudpickle. Here is the error when I load the pickled model: 
“ValueError: Unknown optimizer: AdamWeightDecay. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.""

The problem is only encountered with AdamWeightDecay as there are no issues when adam optimizer is used instead of AdamWeightDecay for the same code. Is there a way we can load the model with AdamW using cloudpickle?
```


### Standalone code to reproduce the issue

```shell
!pip install -q -U ""tensorflow-text==2.8.*""
!pip install cloudpickle
!pip install -q tf-models-official==2.7.0

import os
import shutil
import cloudpickle
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization  # to create AdamW optimizer


url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'

dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,
                                  untar=True, cache_dir='.',
                                  cache_subdir='')

dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')

train_dir = os.path.join(dataset_dir, 'train')


AUTOTUNE = tf.data.AUTOTUNE
batch_size = 32
seed = 42

raw_train_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/train',
    batch_size=batch_size,
    validation_split=0.2,
    subset='training',
    seed=seed)

class_names = raw_train_ds.class_names
train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)

val_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/train',
    batch_size=batch_size,
    validation_split=0.2,
    subset='validation',
    seed=seed)

val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

test_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/test',
    batch_size=batch_size)

test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)


tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'
tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'
bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)
bert_model = hub.KerasLayer(tfhub_handle_encoder)


def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
  encoder_inputs = preprocessing_layer(text_input)
  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
  outputs = encoder(encoder_inputs)
  net = outputs['pooled_output']
  net = tf.keras.layers.Dropout(0.1)(net)
  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
  return tf.keras.Model(text_input, net)


classifier_model = build_classifier_model()
loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
metrics = tf.metrics.BinaryAccuracy()
epochs = 1
steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()
num_train_steps = steps_per_epoch * epochs
num_warmup_steps = int(0.1*num_train_steps)

init_lr = 3e-5
optimizer = optimization.create_optimizer(init_lr=init_lr,
                                          num_train_steps=num_train_steps,
                                          num_warmup_steps=num_warmup_steps,
                                          optimizer_type='adamw')
classifier_model.compile(optimizer=optimizer,
                         loss=loss,
                         metrics=metrics)
classifier_model.fit(x=train_ds,
                               validation_data=val_ds,
                               epochs=epochs)
pickle = cloudpickle.dumps(classifier_model)
unpickle = cloudpickle.loads(pickle)
```


### Relevant log output

```shell
WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 124). These functions will not be directly callable after loading.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-16-58cfd5217ca1> in <module>
      1 import cloudpickle
      2 pickle = cloudpickle.dumps(classifier_model)
----> 3 unpickle = cloudpickle.loads(pickle)

2 frames
/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py in class_and_config_for_serialized_keras_object(config, module_objects, custom_objects, printable_module_name)
    561   if cls is None:
    562     raise ValueError(
--> 563         f'Unknown {printable_module_name}: {class_name}. Please ensure this '
    564         'object is passed to the `custom_objects` argument. See '
    565         'https://www.tensorflow.org/guide/keras/save_and_serialize'

ValueError: Unknown optimizer: AdamWeightDecay. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.
```
</details>"
57721,"Meaning of the ""FlexErf"" node? ","I'm using the vit_keras.vit.Vit_b32 and I am trying to understand what is the meaning of the ""FlexErf"" node. 
Can someone explain?

I think that Is probably linked with the ERF (error function) computation..."
57718,Memory leak when using Metal delegate,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

TFLite 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

iOS 15

### Mobile device

iPhone X

### Python version

N.A.

### Bazel version

5.0.0

### GCC/Compiler version

Apple clang 13.1.6

### CUDA/cuDNN version

N.A.

### GPU model and memory

iPhone X

### Current Behaviour?

```shell
There seems to be a memory leak while using Metal delegate. It doesn't go away no matter how I try to clean up (be it reusing the delegate or not). I have tried to use without the Metal delegate and the leak doesn't happen.
```


### Standalone code to reproduce the issue

```shell
TfLiteDelegate *CreateGPUDelegate(MetalDelegateWaitType waitType = MetalDelegateWaitType::PASSIVE, bool allowPrecisionLoss = false, bool enableQuantization = true)
{
    TFLGpuDelegateOptions options;
    switch(waitType)
    {
    case PASSIVE:
      options.wait_type = TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypePassive;
      break;
    case ACTIVE:
      options.wait_type = TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypeActive;
      break;
    case DO_NOT_WAIT:
      options.wait_type = TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypeDoNotWait;
      break;
    case AGGRESIVE:
      options.wait_type = TFLGpuDelegateWaitType::TFLGpuDelegateWaitTypeAggressive;
      break;
    }
    options.enable_quantization = enableQuantization;
    options.allow_precision_loss = allowPrecisionLoss;
    return TFLGpuDelegateCreate(&options);
}
void DeleteGPUDelegate(TfLiteDelegate *delegate)
{
  TFLGpuDelegateDelete(delegate);
}

// Somewhere in main
  std::thread([this]() {
    std::unique_ptr<tflite::FlatBufferModel> model;
    model = tflite::FlatBufferModel::BuildFromFile(path.c_str());
    while (true)
    {
      std::unique_ptr<tflite::Interpreter> interpreter;
      tflite::ops::builtin::BuiltinOpResolver resolver;
      const TfLiteStatus isInterpreterOk = tflite::InterpreterBuilder(*model, resolver)(&interpreter);
      TfLiteDelegate *delegate = CreateGPUDelegate();
      interpreter->ModifyGraphWithDelegate(delegate);
      interpreter->Invoke();
      interpreter = nullptr;
      DeleteGPUDelegate(delegate);
      std::this_thread::sleep_for(std::chrono::seconds(1));
    }
  }).detach();
```


### Relevant log output

_No response_</details>"
57717,Memory issue using Pose Estimation model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['CoreML', 'Metal']

### Custom Code

No

### OS Platform and Distribution

MacOS

### Mobile device

iPhone X

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
memory not deallocated once I back to root VC.

App crashes after some time.
```


### Standalone code to reproduce the issue

```shell
I used Tensorflow sample example, and added one empty page before that, on click button will redirect to next camera screen.

This process I done many time so after some time memory issue will be occurred & app crash.
```


### Relevant log output

_No response_</details>"
57716,tf.math.unsorted_segment_min (max/sum/prod) crash (abort),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0-dev20220914

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.4 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.7.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current Behaviour?

`tf.math.unsorted_segment_min`, `tf.math.unsorted_segment_max`, `tf.math.unsorted_segment_sum`, `tf.math.unsorted_segment_prod` crash with abortion.

Also reproduced in the [gist](https://colab.research.google.com/drive/1BM8HWcrSTH6qyPwujFl5QjhB_BlT8Nat?usp=sharing)

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
tf.math.unsorted_segment_min(data=np.ones((3)),segment_ids=898042203, num_segments=8327099846119777499)



import numpy as np
import tensorflow as tf
tf.math.unsorted_segment_max(data=np.ones((3)),segment_ids=898042203, num_segments=8327099846119777499)



import numpy as np
import tensorflow as tf
tf.math.unsorted_segment_sum(data=np.ones((3)),segment_ids=898042203, num_segments=8327099846119777499)



import numpy as np
import tensorflow as tf
tf.math.unsorted_segment_prod(data=np.ones((3)),segment_ids=898042203, num_segments=8327099846119777499)
```


### Relevant log output

```shell
2022-09-15 21:11:52.268123: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2022-09-15 21:11:52.268149: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)
2022-09-15 21:11:52.268205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist
2022-09-15 21:11:52.268549: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-15 21:11:52.305843: F tensorflow/core/framework/tensor_shape.cc:404] Check failed: 0 <= new_num_elements (0 vs. -1)
Aborted (core dumped)
```
</details>"
57715,tf.nn.conv2d_transpose crash with abort whtn `output_shape` has negative value,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0-dev20220914

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.4 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.7.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current Behaviour?


`tf.nn.conv2d_transpose` crash with abort whtn `output_shape` has negative value

Also reproduced in this [gist](https://colab.research.google.com/drive/1Yx5pvWM7HoVpwAmCfzU5DIi1Y9IRsHrP?usp=sharing)


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
tf.nn.conv2d_transpose(input=np.ones((1,1,1,1)), filters=np.ones((1,1,1,1)), output_shape=[2,-2], strides=[1])
```


### Relevant log output

```shell
2022-09-15 20:35:37.763729: F tensorflow/core/framework/tensor_shape.cc:186] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Expected shape dimensions to be non-negative, got -2
Aborted (core dumped)
```
</details>"
57714,tf.quantization.quantize_and_dequantize (and v2) crash (abort),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0-dev20220914

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.4 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.7.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current Behaviour?


`tf.quantization.quantize_and_dequantize` and `tf.quantization.quantize_and_dequantize_v2` crash with abortion

Also reproduced in the [gist](https://colab.research.google.com/drive/1gr2mX4G2qWQanate4tslrQNByU-sqI0s?usp=sharing)



### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
tf.quantization.quantize_and_dequantize_v2(input=np.ones((10)),input_min=-1,input_max=[-1,1], range_given=True)
```


### Relevant log output

```shell
2022-09-15 19:54:27.500788: F tensorflow/core/framework/tensor.cc:733] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor
Aborted (core dumped)
```
</details>"
57713,tf.linalg.diag crash (abort),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0-dev20220914

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.4 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.7.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current Behaviour?


tf.linalg.diag crash (abort)

Also reproduced in the [gist](https://colab.research.google.com/drive/18ixYmcwDLRbqTLbD4AHXq1Qqt74aDY5v?usp=sharing)



### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
tf.linalg.diag(k=1070828000000, diagonal=np.ones((2,2,2,2)))
```


### Relevant log output

```shell
2022-09-15 19:38:46.149294: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2022-09-15 19:38:46.149328: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)
2022-09-15 19:38:46.149367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist
2022-09-15 19:38:46.149684: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-15 19:38:46.188085: F tensorflow/core/framework/tensor_shape.cc:404] Check failed: 0 <= new_num_elements (0 vs. -3186289596827017184)
Aborted (core dumped)
```
</details>"
57712,tf.keras.layers.Reshape crash with segfault when layer input is empty,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0-dev20220914

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.4 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.7.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current Behaviour?


`tf.keras.layers.Reshape` crash with segfault when layer input is empty

Also reproduced in the [gist](https://colab.research.google.com/drive/16h_oWIEk7qUWu6hun2b7CoAQkA5dUUkS?usp=sharing)



### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
layer_input=np.ones((0,1,1,1))
layer=tf.keras.layers.Reshape((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100))
layer(layer_input)
```


### Relevant log output

```shell
2022-09-15 19:02:51.096968: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2022-09-15 19:02:51.096993: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)
2022-09-15 19:02:51.097022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist
2022-09-15 19:02:51.097296: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault (core dumped)
```
</details>"
57711,tf.keras.backend.eye crash (abort) with large input,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0-dev20220914

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.4 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.7.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current Behaviour?


`tf.keras.backend.eye` and `tf.eye` crash (abort) with large input

Also reproduced in this [gist](https://colab.research.google.com/drive/1UEeiUYXSKSU1cBqUs01d_M1RkXgSflEw?usp=sharing)



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.keras.backend.eye(size=2752212975)
```

```shell
import tensorflow as tf
tf.eye(2752212975)
```



### Relevant log output

```shell
2022-09-15 18:51:32.477313: F tensorflow/core/framework/tensor_shape.cc:572] Check failed: size >= 0 (0 vs. -1542754321)
Aborted (core dumped)
```
</details>"
57710,Failed to get registration from op code CUSTOM,"Hi developers, hi all!

I'm making some experiments with LSTM networks using Keras, TensorFlow, and I'm working on a Colab environment, using TF 2.11.0-dev20220915 installed via pip. 

On the model authoring side, I'm able to successfully build the model, apply compression techniques and get the equivalent Lite model, and the .cc file. Specifically, starting from the base model I apply the pruning technique first, and then fully integer quantization. During the conversion process to Lite model, I do not get any errors.

However, when it's time to flash the model onto the device (in this case,  an ESP32), I always get the following error **Failed to get registration from op code CUSTOM AllocateTensors() failed**. 

Is this error generated because there is something in the model not yet supported? is there a way to discover which is, or are, the OPs that are not supported?
I really would like to understand the reason for this error and how could I solve it.

Here attached you can find a zip folder with two .tflite files (one it's related to the model not quantized yet and the other to the quantized one), the third one is the .cc file of the quantized model.
[material_pruning-fullyintquant.zip](https://github.com/tensorflow/tensorflow/files/9577953/material_pruning-fullyintquant.zip)
"
57709,tf.keras.backend.conv2d_transpose crash abort in the nightly version,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0-dev20220914

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.4 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.7.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current Behaviour?

`tf.keras.backend.conv2d_transpose` crash with abortion.


Also reproduced in this [gist](https://colab.research.google.com/drive/15IXylJP_NgNuE1HBPw4P5QrMgbHQCFVd?usp=sharing)



### Standalone code to reproduce the issue

```
import numpy as np
import tensorflow as tf
tf.keras.backend.conv2d_transpose(x=np.ones((2,1,1,1)), kernel=1, output_shape=np.array([-3697127975084027074, 1],dtype=np.int32))

```



### Relevant log output


Output:
```
2022-09-15 17:30:45.974542: F tensorflow/core/framework/tensor_shape.cc:186] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Expected shape dimensions to be non-negative, got -470478018
Aborted (core dumped)

```
"
57708,TFLite throws an error with certain tensor value,"### 1. Problem

For the same model, just changing the value of a constant tensor will cause an error.

### 2. Code

See the ipynb [here](https://colab.research.google.com/drive/15pshTNxXLDObJZbwes5tCvW1RXgbAHGA?usp=sharing#scrollTo=UDfz52j1K62g)

Log:

```python
RuntimeError: Index out of range using input dim 1; input has only 1 dimsNode number 2 (TfLiteFlexDelegate) failed to invoke.
```


"
57706,Aligned alloc might not be defined,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.0+

### Custom Code

No

### OS Platform and Distribution

Centos 6.0

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.2.0

### GCC/Compiler version

7.1.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
INFO: Analyzed target //tensorflow/lite/c:tensorflowlite_c (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /root/tensorflow-2.10.0/tensorflow/lite/BUILD:505:11: Compiling tensorflow/lite/interpreter_builder.cc failed: (Exit 1): gcc failed: error executing command /usr/local/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 47 arguments skipped)
tensorflow/lite/interpreter_builder.cc: In member function 'virtual void* tflite::{anonymous}::MallocDataAllocator::Allocate(size_t, size_t)':
tensorflow/lite/interpreter_builder.cc:312:12: error: 'aligned_alloc' was not declared in this scope
     return aligned_alloc(used_alignment, used_size);
            ^~~~~~~~~~~~~
Target //tensorflow/lite/c:tensorflowlite_c failed to build
```


Aligned alloc is not defined for a gcc compiler of version 7.1.0 (stdc11 and c++14):

```shell
[root]# gcc -dM -E - < /dev/null | grep STDC_VERSION
#define __STDC_VERSION__ 201112L

[root]# echo '#include<iostream>

int main() {
    if (__cplusplus == 201703L) std::cout << ""C++17\n"";
    else if (__cplusplus == 201402L) std::cout << ""C++14\n"";
    else if (__cplusplus == 201103L) std::cout << ""C++11\n"";
    else if (__cplusplus == 199711L) std::cout << ""C++98\n"";
    else std::cout << ""pre-standard C++\n"";
}' | g++ -x c++ -

[root]# ./a.out 
C++14

```


If we refer to [your code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/interpreter_builder.cc#L312) you need `TFLITE_USE_STD_ALIGNED_ALLOC` to be defined which happens with [all those conditions](https://github.com/tensorflow/tensorflow/blob/00a049661641ff13e0dd666fa692c97fb079e4bd/tensorflow/lite/interpreter_builder.cc#L49-L57)
```c
// aligned_alloc is available (via cstdlib/stdlib.h) with C++17/C11.
#if __cplusplus >= 201703L || __STDC_VERSION__ >= 201112L
#if !defined(__ANDROID__) || __ANDROID_API__ >= 28
// Neither Apple nor Windows provide aligned_alloc.
#if !defined(__APPLE__) && !defined(_WIN32)
#define TFLITE_USE_STD_ALIGNED_ALLOC
#endif
#endif
#endif
```

However having c11 is not enough, aligned_malloc was mainly shipped with c++17 so the first condition should have been `#if __cplusplus >= 201703L && __STDC_VERSION__ >= 201112L` and not a simple or.

[This](https://github.com/tensorflow/tensorflow/blob/4ea1236a02160e0c9bd8c3673cfae66dfb3f1b9b/tensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc#L39) also need to change


### Standalone code to reproduce the issue

```shell
Just run the compilation with gcc 7.1.0
build -c opt //tensorflow/lite/c:tensorflowlite_c
```


### Relevant log output

```shell
INFO: Analyzed target //tensorflow/lite/c:tensorflowlite_c (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /root/tensorflow-2.10.0/tensorflow/lite/BUILD:505:11: Compiling tensorflow/lite/interpreter_builder.cc failed: (Exit 1): gcc failed: error executing command /usr/local/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 47 arguments skipped)
tensorflow/lite/interpreter_builder.cc: In member function 'virtual void* tflite::{anonymous}::MallocDataAllocator::Allocate(size_t, size_t)':
tensorflow/lite/interpreter_builder.cc:312:12: error: 'aligned_alloc' was not declared in this scope
     return aligned_alloc(used_alignment, used_size);
            ^~~~~~~~~~~~~
Target //tensorflow/lite/c:tensorflowlite_c failed to build
```
</details>"
57705,"Compute jacobian for `tf.image.rot90` throws error ""Encountered an exception while vectorizing the jacobian computation""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf-nightly

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Compute jacobian for `tf.image.rot90` fails and throws error ""Encountered an exception while vectorizing the jacobian computation"". This only happens when I set jit_compile=True.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
image = tf.random.uniform([2, 2, 1, 1], minval=0, maxval=1, dtype=tf.float64)
@tf.function(jit_compile=True)
def rot_func(image):
    return tf.image.rot90(image, k=3)
with tf.GradientTape(persistent=True) as tape:
    tape.watch(image)
    outputs = rot_func(image)
print(outputs.shape)
gradient = tape.jacobian(outputs, [image])
print(gradient)
```


### Relevant log output

```shell
(2, 1, 2, 1)

......

ValueError: in user code: 
ValueError: Dimension 2 in both shapes must be equal, but are 1 and 2. Shapes are [4,2,1,2,1] and [?,2,2,1,1].
Encountered an exception while vectorizing the jacobian computation. Vectorization can be disabled by setting experimental_use_pfor to False.
```
</details>"
57704,tensorflow build from source with bazel failed,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf2.7-rc0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.6.9

### Bazel version

3.7.2

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

N/A

### Current Behaviour?

```shell
I expected build success and I can get smaller size of tensorflow-lite.aar and tensorflow-lite-select-tf-ops.aar
```


### Standalone code to reproduce the issue

```shell
I want to reduce library size.
I checkout v2.7.0-rc0 tag and just followed this link (https://www.tensorflow.org/lite/guide/ops_select#building_the_android_aar).

## Configure
* No CUDA build
* Bazel Optimization flag : -D_GLIBCXX_USE_CXX11_ABI=0 -O3
* Android NDK version : 21.4.7075529
* Android NDK API Level : 21
* Android BuildTool Version : 30.0.3
* Android SDK API Level : 31
```


### Relevant log output

```shell
ERROR: /home/user/BIG/tensorflow/tmp/BUILD:19:28: C++ compilation of rule '//tmp:custom_tensorflowlite_flex_flex_delegate_tensorflow_lib' failed (Exit 1): clang failed: error executing command external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/x86_64-4.9/prebuilt/linux-x86_64 -target x86_64-none-linux-android -fPIC ... (remaining 154 argument(s) skipped)
In file included from tensorflow/core/kernels/list_kernels.cc:22:
In file included from ./tensorflow/core/kernels/list_kernels.h:50:
In file included from ./tensorflow/stream_executor/stream.h:32:
In file included from ./tensorflow/stream_executor/blas.h:46:
In file included from ./tensorflow/stream_executor/dnn.h:33:
./tensorflow/stream_executor/data_type.h:21:10: fatal error: 'tensorflow/stream_executor/dnn.pb.h' file not found
#include ""tensorflow/stream_executor/dnn.pb.h""
         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
1 error generated.
Target //tmp:tensorflow-lite-select-tf-ops failed to build
```
</details>"
57703,"Feature request for Cast, Minimum, RealDiv and ResizeNearestNeighbor operations in TensorFlow-Lite ","### 1. System information
-Linux Ubuntu 20.04
-TensorFlow installation-pip package
-TensorFlow library 2.10.0

### 2. Code

import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_saved_model(""exported-model-test"")

converter.optimizations = [tf.lite.Optimize.DEFAULT]

tflite_model = converter.convert()
open(""exported-model/TF-Lite/DeepLabv3Plus_model_new1.tflite"", ""wb"").write(tflite_model)


### 5. Info / logs

onverterError                            Traceback (most recent call last)
Input In [15], in <cell line: 13>()
      4 #converter.allow_custom_ops = True
      5 # converter.target_spec.supported_ops = [
      6 #   tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
      7 #   tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
      8 # ]
      9 #converter.experimental_new_converter = True
     11 converter.optimizations = [tf.lite.Optimize.DEFAULT]
---> 13 tflite_model = converter.convert()
     14 open(""exported-model/TF-Lite/DeepLabv3Plus_model_new1.tflite"", ""wb"").write(tflite_model)

File ~/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:930, in _export_metrics.<locals>.wrapper(self, *args, **kwargs)
    927 @functools.wraps(convert_func)
    928 def wrapper(self, *args, **kwargs):
    929   # pylint: disable=protected-access
--> 930   return self._convert_and_export_metrics(convert_func, *args, **kwargs)

File ~/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:908, in TFLiteConverterBase._convert_and_export_metrics(self, convert_func, *args, **kwargs)
    906 self._save_conversion_params_metric()
    907 start_time = time.process_time()
--> 908 result = convert_func(self, *args, **kwargs)
    909 elapsed_time_ms = (time.process_time() - start_time) * 1000
    910 if result:

File ~/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:1213, in TFLiteSavedModelConverterV2.convert(self)
   1208 else:
   1209   self._debug_info = _get_debug_info(
   1210       _convert_debug_info_func(self._trackable_obj.graph_debug_info),
   1211       graph_def)
-> 1213 return self._convert_from_saved_model(graph_def)

File ~/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:1096, in TFLiteConverterBaseV2._convert_from_saved_model(self, graph_def)
   1093 converter_kwargs.update(self._get_base_converter_args())
   1094 converter_kwargs.update(quant_mode.converter_flags())
-> 1096 result = _convert_saved_model(**converter_kwargs)
   1097 return self._optimize_tflite_model(
   1098     result, quant_mode, quant_io=self.experimental_new_quantizer)

File ~/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py:212, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)
    210   else:
    211     report_error_message(str(converter_error))
--> 212   raise converter_error from None  # Re-throws the exception.
    213 except Exception as error:
    214   report_error_message(str(error))

File ~/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py:205, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)
    202 @functools.wraps(func)
    203 def wrapper(*args, **kwargs):
    204   try:
--> 205     return func(*args, **kwargs)
    206   except ConverterError as converter_error:
    207     if converter_error.errors:

File ~/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:809, in convert_saved_model(**kwargs)
    807 model_flags = build_model_flags(**kwargs)
    808 conversion_flags = build_conversion_flags(**kwargs)
--> 809 data = convert(
    810     model_flags.SerializeToString(),
    811     conversion_flags.SerializeToString(),
    812     input_data_str=None,
    813     debug_info_str=None,
    814     enable_mlir_converter=True)
    815 return data

File ~/anaconda3/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:311, in convert(model_flags_str, conversion_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    309     for error_data in _metrics_wrapper.retrieve_collected_errors():
    310       converter_error.append_error(error_data)
--> 311     raise converter_error
    313 return _run_deprecated_conversion_binary(model_flags_str,
    314                                          conversion_flags_str, input_data_str,
    315                                          debug_info_str)

ConverterError: <unknown>:0: error: loc(callsite(callsite(fused[""Cast:"", ""truediv/Cast_1@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): 'tf.Cast' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""Cast:"", ""truediv/Cast_1@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""Cast:"", ""truediv_1/Cast_1@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): 'tf.Cast' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""Cast:"", ""truediv_1/Cast_1@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""RealDiv:"", ""truediv@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): 'tf.RealDiv' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""RealDiv:"", ""truediv@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""RealDiv:"", ""truediv_1@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): 'tf.RealDiv' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""RealDiv:"", ""truediv_1@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""Minimum:"", ""Minimum@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): 'tf.Minimum' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""Minimum:"", ""Minimum@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""Cast:"", ""Cast_1@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): 'tf.Cast' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""Cast:"", ""Cast_1@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: loc(callsite(callsite(fused[""ResizeNearestNeighbor:"", ""resize_align_corners_1/ResizeNearestNeighbor@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
<unknown>:0: note: loc(callsite(callsite(fused[""ResizeNearestNeighbor:"", ""resize_align_corners_1/ResizeNearestNeighbor@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: failed while converting: 'main': 
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: Cast, Minimum, RealDiv, ResizeNearestNeighbor
Details:
	tf.Cast(tensor<f64>) -> (tensor<f32>) : {Truncate = false, device = """"}
	tf.Cast(tensor<i32>) -> (tensor<f64>) : {Truncate = false, device = """"}
	tf.Minimum(tensor<f64>, tensor<f64>) -> (tensor<f64>) : {device = """"}
	tf.RealDiv(tensor<f64>, tensor<f64>) -> (tensor<f64>) : {device = """"}
	tf.ResizeNearestNeighbor(tensor<1x?x?x1xi32>, tensor<2xi32>) -> (tensor<1x?x?x1xi32>) : {align_corners = true, device = """", half_pixel_centers = false}



(callsite(fused[""Minimum:"", ""Minimum@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): error: 'tf.Minimum' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""Cast:"", ""Cast_1@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): error: 'tf.Cast' op is neither a custom op nor a flex op
loc(callsite(callsite(fused[""ResizeNearestNeighbor:"", ""resize_align_corners_1/ResizeNearestNeighbor@__inference___call___6210""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall@__inference_signature_wrapper_6889""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): error: 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op
error: failed while converting: 'main': 
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: Cast, Minimum, RealDiv, ResizeNearestNeighbor
Details:
	tf.Cast(tensor<f64>) -> (tensor<f32>) : {Truncate = false, device = """"}
	tf.Cast(tensor<i32>) -> (tensor<f64>) : {Truncate = false, device = """"}
	tf.Minimum(tensor<f64>, tensor<f64>) -> (tensor<f64>) : {device = """"}
	tf.RealDiv(tensor<f64>, tensor<f64>) -> (tensor<f64>) : {device = """"}
	tf.ResizeNearestNeighbor(tensor<1x?x?x1xi32>, tensor<2xi32>) -> (tensor<1x?x?x1xi32>) : {align_corners = true, device = """", half_pixel_centers = false}
"
57702,`tf.broadcast_to` crashes with JIT,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.4

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Session crashes when running the (compiled) `tf.broadcast_to` with an invalid shape, and can potentially trigger a denial of service attack. 
This bug only happens with using `@tf.function(jit_compile=True) `.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
input_tensor = tf.zeros([1, 1], dtype=tf.int32)

@tf.function(jit_compile=True) 
def f(input):
    shape = [-1, 1]
    return tf.broadcast_to(input, shape, ) 

f(input_tensor)
```


### Relevant log output

```shell
F tensorflow/core/framework/tensor_shape.cc:186] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Expected shape dimensions to be non-negative, got -1
abort
```
</details>"
57700,[Linking Issue] Undefined reference when using cross compiled TFLite C library for ARM64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf v2.8.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

Linux Ubuntu 18.04

### Python version

3.6.9

### Bazel version

_No response_

### GCC/Compiler version

Cross compilation with gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I got linking issue undefined reference to tflite APIs when using cross compiled TFLite C library for ARM64.

/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/src/tflitetest.cpp.o: in function `test::Fnv4VXUnitTest_TensorFlowLite_ImageClassifierEfficientNet_Test::TestBody()':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1636: undefined reference to `TfLiteModelCreateFromFile'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1640: undefined reference to `TfLiteInterpreterOptionsCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1641: undefined reference to `TfLiteInterpreterOptionsSetNumThreads'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1644: undefined reference to `TfLiteInterpreterCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1647: undefined reference to `TfLiteInterpreterAllocateTensors'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1648: undefined reference to `TfLiteInterpreterGetInputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1651: undefined reference to `TfLiteInterpreterGetOutputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1658: undefined reference to `TfLiteInterpreterInvoke'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1711: undefined reference to `TfLiteInterpreterDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1712: undefined reference to `TfLiteInterpreterOptionsDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1713: undefined reference to `TfLiteModelDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/src/tflitetest.cpp.o: in function `test::Fnv4VXUnitTest_TensorFlowLite_PoseDetectMoveNetSingleLight_Test::TestBody()':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1350: undefined reference to `TfLiteModelCreateFromFile'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1354: undefined reference to `TfLiteInterpreterOptionsCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1355: undefined reference to `TfLiteInterpreterOptionsSetNumThreads'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1358: undefined reference to `TfLiteInterpreterCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1361: undefined reference to `TfLiteInterpreterAllocateTensors'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1362: undefined reference to `TfLiteInterpreterGetInputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1365: undefined reference to `TfLiteInterpreterGetOutputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1372: undefined reference to `TfLiteInterpreterInvoke'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1423: undefined reference to `TfLiteInterpreterDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1424: undefined reference to `TfLiteInterpreterOptionsDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1425: undefined reference to `TfLiteModelDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/src/tflitetest.cpp.o: in function `test::Fnv4VXUnitTest_TensorFlowLite_ObjDetectMobileNetV1_Test::TestBody()':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1200: undefined reference to `TfLiteVersion'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1229: undefined reference to `TfLiteModelCreateFromFile'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1234: undefined reference to `TfLiteInterpreterOptionsCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1235: undefined reference to `TfLiteInterpreterOptionsSetNumThreads'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1238: undefined reference to `TfLiteInterpreterCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1241: undefined reference to `TfLiteInterpreterAllocateTensors'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1242: undefined reference to `TfLiteInterpreterGetInputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1245: undefined reference to `TfLiteInterpreterGetOutputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1246: undefined reference to `TfLiteInterpreterGetOutputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1247: undefined reference to `TfLiteInterpreterGetOutputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1248: undefined reference to `TfLiteInterpreterGetOutputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1256: undefined reference to `TfLiteInterpreterInvoke'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1303: undefined reference to `TfLiteInterpreterDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1304: undefined reference to `TfLiteInterpreterOptionsDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1305: undefined reference to `TfLiteModelDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `com::ford::fordos::fnv4vx::FordVxTfliteKernel::kernelFunction(_vx_node*, _vx_reference* const*)':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:302: undefined reference to `TfLiteInterpreterInvoke'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `com::ford::fordos::fnv4vx::FordVxTfliteKernel::validateTensor(_vx_tensor*, TfLiteTensor const*)':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:78: undefined reference to `TfLiteTensorType'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:79: undefined reference to `TfLiteTensorNumDims'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:91: undefined reference to `TfLiteTensorDim'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `com::ford::fordos::fnv4vx::FordVxTfliteKernel::FordVxTfliteKernel(_vx_node*, _vx_reference* const*)':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:175: undefined reference to `TfLiteModelCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `std::_Function_base::_Base_manager<void (*)(TfLiteModel*)>::_M_init_functor(std::_Any_data&, void (*&&)(TfLiteModel*), std::integral_constant<bool, true>)':
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/aarch64-none-linux-gnu/include/c++/9.2.1/new:174: undefined reference to `TfLiteModelDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/aarch64-none-linux-gnu/include/c++/9.2.1/new:174: undefined reference to `TfLiteModelDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `com::ford::fordos::fnv4vx::FordVxTfliteKernel::FordVxTfliteKernel(_vx_node*, _vx_reference* const*)':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:179: undefined reference to `TfLiteInterpreterOptionsCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `std::_Function_base::_Base_manager<void (*)(TfLiteInterpreterOptions*)>::_M_init_functor(std::_Any_data&, void (*&&)(TfLiteInterpreterOptions*), std::integral_constant<bool, true>)':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:183: undefined reference to `TfLiteInterpreterOptionsDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `std::_Function_base::_Base_manager<void (*)(TfLiteInterpreterOptions*)>::_M_init_functor(std::_Any_data&, void (*&&)(TfLiteInterpreterOptions*), std::integral_constant<bool, true>)':
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/aarch64-none-linux-gnu/include/c++/9.2.1/bits/std_function.h:676: undefined reference to `TfLiteInterpreterOptionsDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `com::ford::fordos::fnv4vx::FordVxTfliteKernel::FordVxTfliteKernel(_vx_node*, _vx_reference* const*)':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:200: undefined reference to `TfLiteInterpreterCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `std::_Function_base::_Base_manager<void (*)(TfLiteInterpreter*)>::_M_init_functor(std::_Any_data&, void (*&&)(TfLiteInterpreter*), std::integral_constant<bool, true>)':
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/aarch64-none-linux-gnu/include/c++/9.2.1/new:174: undefined reference to `TfLiteInterpreterDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/aarch64-none-linux-gnu/include/c++/9.2.1/new:174: undefined reference to `TfLiteInterpreterDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `com::ford::fordos::fnv4vx::FordVxTfliteKernel::FordVxTfliteKernel(_vx_node*, _vx_reference* const*)':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:205: undefined reference to `TfLiteInterpreterAllocateTensors'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:209: undefined reference to `TfLiteInterpreterGetInputTensorCount'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:217: undefined reference to `TfLiteInterpreterGetInputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:226: undefined reference to `TfLiteInterpreterGetOutputTensorCount'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:234: undefined reference to `TfLiteInterpreterGetOutputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:196: undefined reference to `TfLiteInterpreterOptionsAddDelegate'
collect2: error: ld returned 1 exit status
ninja: build stopped: subcommand failed.
```


### Standalone code to reproduce the issue

I built tflite C library for ARM64 as follows:
```shell
git clone https://github.com/tensorflow/tensorflow.git tensorflow_src

cd tensorflow_src/

git checkout v2.8.0

mkdir tflite_build_arm

cd tflite_build_arm

ARMCC_PREFIX=/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/aarch64-none-linux-gnu-

ARMCC_FLAGS=""-funsafe-math-optimizations""

cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc  -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++  -DCMAKE_C_FLAGS=""${ARMCC_FLAGS}""  -DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}""  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON  -DCMAKE_SYSTEM_NAME=Linux  -DTFLITE_ENABLE_XNNPACK=ON  -DCMAKE_SYSTEM_PROCESSOR=aarch64  -DBUILD_SHARED_LIBS=ON  ../tensorflow/lite/c

cmake --build . -j
```

Then, I modified CMakeLists.txt files to link tflite C library to target as follows:

***Parent CMakeLists.txt***
```
set(TFLITE_INCLUDE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}/tensorflow_src/)

include_directories(${TFLITE_INCLUDE_DIRS})

if(CROSS_BUILD_TARGET_ARM)
        set(TFLITE_LIB ${FNV4VX_BASE_PATH}/tensorflow_src/tflite_build_arm/libtensorflowlite_c.so)
        file(GLOB_RECURSE TFLITE_DEPS_LIBS CONFIGURE_DEPENDS ${FNV4VX_BASE_PATH}/tensorflow_src/tflite_build_arm/*.so)
else()
        set(TFLITE_LIB ${FNV4VX_BASE_PATH}/tensorflow_src/tflite_build/libtensorflowlite_c.so)
endif()

    target_link_libraries(fnv4vxbundle
        ${TFLITE_LIB}
        ${TFLITE_DEPS_LIBS}
)
```

***Child CMakeLists.txt***
```
target_link_libraries(fnv4vxtest
        ${TFLITE_LIB}
        ${TFLITE_DEPS_LIBS}
    )
```

***Child CMakeLists.txt***
```
target_link_libraries(forktest
        ${TFLITE_LIB}
        ${TFLITE_DEPS_LIBS}
    )
```

***Child CMakeLists.txt***
```
target_link_libraries(gstrtptest
        ${TFLITE_LIB}
        ${TFLITE_DEPS_LIBS}
    )
```

***Child CMakeLists.txt***
```
target_link_libraries(gsttest
        ${TFLITE_LIB}
        ${TFLITE_DEPS_LIBS}
    )
```


### Relevant log output

```shell
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/src/tflitetest.cpp.o: in function `test::Fnv4VXUnitTest_TensorFlowLite_ImageClassifierEfficientNet_Test::TestBody()':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1636: undefined reference to `TfLiteModelCreateFromFile'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1640: undefined reference to `TfLiteInterpreterOptionsCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1641: undefined reference to `TfLiteInterpreterOptionsSetNumThreads'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1644: undefined reference to `TfLiteInterpreterCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1647: undefined reference to `TfLiteInterpreterAllocateTensors'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1648: undefined reference to `TfLiteInterpreterGetInputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1651: undefined reference to `TfLiteInterpreterGetOutputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1658: undefined reference to `TfLiteInterpreterInvoke'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1711: undefined reference to `TfLiteInterpreterDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1712: undefined reference to `TfLiteInterpreterOptionsDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1713: undefined reference to `TfLiteModelDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/src/tflitetest.cpp.o: in function `test::Fnv4VXUnitTest_TensorFlowLite_PoseDetectMoveNetSingleLight_Test::TestBody()':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1350: undefined reference to `TfLiteModelCreateFromFile'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1354: undefined reference to `TfLiteInterpreterOptionsCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1355: undefined reference to `TfLiteInterpreterOptionsSetNumThreads'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1358: undefined reference to `TfLiteInterpreterCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1361: undefined reference to `TfLiteInterpreterAllocateTensors'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1362: undefined reference to `TfLiteInterpreterGetInputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1365: undefined reference to `TfLiteInterpreterGetOutputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1372: undefined reference to `TfLiteInterpreterInvoke'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1423: undefined reference to `TfLiteInterpreterDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1424: undefined reference to `TfLiteInterpreterOptionsDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1425: undefined reference to `TfLiteModelDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/src/tflitetest.cpp.o: in function `test::Fnv4VXUnitTest_TensorFlowLite_ObjDetectMobileNetV1_Test::TestBody()':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1200: undefined reference to `TfLiteVersion'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1229: undefined reference to `TfLiteModelCreateFromFile'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1234: undefined reference to `TfLiteInterpreterOptionsCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1235: undefined reference to `TfLiteInterpreterOptionsSetNumThreads'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1238: undefined reference to `TfLiteInterpreterCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1241: undefined reference to `TfLiteInterpreterAllocateTensors'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1242: undefined reference to `TfLiteInterpreterGetInputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1245: undefined reference to `TfLiteInterpreterGetOutputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1246: undefined reference to `TfLiteInterpreterGetOutputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1247: undefined reference to `TfLiteInterpreterGetOutputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1248: undefined reference to `TfLiteInterpreterGetOutputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1256: undefined reference to `TfLiteInterpreterInvoke'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1303: undefined reference to `TfLiteInterpreterDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1304: undefined reference to `TfLiteInterpreterOptionsDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tests/fnv4vxtest/src/tflitetest.cpp:1305: undefined reference to `TfLiteModelDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `com::ford::fordos::fnv4vx::FordVxTfliteKernel::kernelFunction(_vx_node*, _vx_reference* const*)':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:302: undefined reference to `TfLiteInterpreterInvoke'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `com::ford::fordos::fnv4vx::FordVxTfliteKernel::validateTensor(_vx_tensor*, TfLiteTensor const*)':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:78: undefined reference to `TfLiteTensorType'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:79: undefined reference to `TfLiteTensorNumDims'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:91: undefined reference to `TfLiteTensorDim'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `com::ford::fordos::fnv4vx::FordVxTfliteKernel::FordVxTfliteKernel(_vx_node*, _vx_reference* const*)':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:175: undefined reference to `TfLiteModelCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `std::_Function_base::_Base_manager<void (*)(TfLiteModel*)>::_M_init_functor(std::_Any_data&, void (*&&)(TfLiteModel*), std::integral_constant<bool, true>)':
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/aarch64-none-linux-gnu/include/c++/9.2.1/new:174: undefined reference to `TfLiteModelDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/aarch64-none-linux-gnu/include/c++/9.2.1/new:174: undefined reference to `TfLiteModelDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `com::ford::fordos::fnv4vx::FordVxTfliteKernel::FordVxTfliteKernel(_vx_node*, _vx_reference* const*)':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:179: undefined reference to `TfLiteInterpreterOptionsCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `std::_Function_base::_Base_manager<void (*)(TfLiteInterpreterOptions*)>::_M_init_functor(std::_Any_data&, void (*&&)(TfLiteInterpreterOptions*), std::integral_constant<bool, true>)':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:183: undefined reference to `TfLiteInterpreterOptionsDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `std::_Function_base::_Base_manager<void (*)(TfLiteInterpreterOptions*)>::_M_init_functor(std::_Any_data&, void (*&&)(TfLiteInterpreterOptions*), std::integral_constant<bool, true>)':
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/aarch64-none-linux-gnu/include/c++/9.2.1/bits/std_function.h:676: undefined reference to `TfLiteInterpreterOptionsDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `com::ford::fordos::fnv4vx::FordVxTfliteKernel::FordVxTfliteKernel(_vx_node*, _vx_reference* const*)':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:200: undefined reference to `TfLiteInterpreterCreate'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `std::_Function_base::_Base_manager<void (*)(TfLiteInterpreter*)>::_M_init_functor(std::_Any_data&, void (*&&)(TfLiteInterpreter*), std::integral_constant<bool, true>)':
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/aarch64-none-linux-gnu/include/c++/9.2.1/new:174: undefined reference to `TfLiteInterpreterDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/aarch64-none-linux-gnu/include/c++/9.2.1/new:174: undefined reference to `TfLiteInterpreterDelete'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: tests/fnv4vxtest/CMakeFiles/fnv4vxtest.dir/__/__/ford_platform/kernels/src/fordTflite.cpp.o: in function `com::ford::fordos::fnv4vx::FordVxTfliteKernel::FordVxTfliteKernel(_vx_node*, _vx_reference* const*)':
/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:205: undefined reference to `TfLiteInterpreterAllocateTensors'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:209: undefined reference to `TfLiteInterpreterGetInputTensorCount'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:217: undefined reference to `TfLiteInterpreterGetInputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:226: undefined reference to `TfLiteInterpreterGetOutputTensorCount'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:234: undefined reference to `TfLiteInterpreterGetOutputTensor'
/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/../../../../aarch64-none-linux-gnu/bin/ld: /home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/ford_platform/kernels/src/fordTflite.cpp:196: undefined reference to `TfLiteInterpreterOptionsAddDelegate'
collect2: error: ld returned 1 exit status
ninja: build stopped: subcommand failed.
```
</details>"
57698,Impossible to create dataset with non-infered labels ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CuDNN 8.5.0.96 with CUDA 11.7

### GPU model and memory

Nvidia A100, 40Gb VRAM

### Current Behaviour?

```shell
Hi,

I've been experimenting with the ILSVRC2012 Dataset lately, in order to do some transfer learning. 

I've used the tensorflow.keras.utils.image_dataset_from_directory function to create the training dataset, 
and let it infer the labels (since every class has its dedicated directory). 
However, for the validation dataset, the images are mixed in a single directory, so I need to label them. 
When feeding labels to the aforementioned function, the documentation states 

[The ""labels"" argument can be set as] Either ""inferred"" (labels are generated from the directory structure), 
None (no labels), or a list/tuple of integer labels of the same size as the number of image files found in 
the directory. Labels should be sorted according to the alphanumeric order of the image file paths 
(obtained via os.walk(directory) in Python). 

So I fed the function a list containing 50,000 integers for the classes of the val dataset. 
I made sure the list contains 50,000 integers ranging from 1 to 1000. However, when I attempt to create 
the dataset, the function returns ""Found 50,000 files belonging to 1 classes."" and no other output.

Despite this, my models infer on the dataset seamlessly.

I don't know if the problem stems from the documentation or the function, so I file it as a bug.

I thank you for your time and attention !
```


### Standalone code to reproduce the issue

```shell
path_val = './ILSVRC/Data/val/'
path_val_labels = './ILSVRC/Data/val.txt'
val_labels = list(np.loadtxt(path_val_labels).astype(int))

val_dataset_224 = tf.keras.utils.image_dataset_from_directory(
  path_val, 
  labels=val_labels, 
  image_size=(224, 224),
  batch_size=32, 
  color_mode='rgb', 
  shuffle=True, 
  seed=None, 
  validation_split=None, 
  subset=None,
  interpolation='bilinear', 
  follow_links=False,
  crop_to_aspect_ratio=False)
```


### Relevant log output

_No response_</details>"
57697,TFLite fails to run a model with a dense layer following an Add operator,"### 1. System information

```python
❯ lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 20.04.4 LTS
Release:        20.04
Codename:       focal

In [2]: tf.__version__
Out[2]: '2.11.0-dev20220914'
```

### 2. Code

```python
import tensorflow as tf
from keras import layers

def get_tflite_callable(model, inp_dict):
    converter = tf.lite.TFLiteConverter.from_concrete_functions(
        funcs=[model.__call__.get_concrete_function(**inp_dict)],
        trackable_obj=model,
    )
    tflite_bytes = converter.convert()
    interpreter = tf.lite.Interpreter(model_content=tflite_bytes)
    runner = interpreter.get_signature_runner()
    return runner

class MyModule(tf.Module):
    def __init__(self):
        super().__init__()
        self.dense_1 = layers.Dense(1)

    @tf.function
    def __call__(self, i0):
        o0 = tf.add(i0, tf.constant(1.1, shape=[1,1,1,1]))
        o0 = self.dense_1(o0) # a dense layer following an Add operator
        return o0


if __name__ == ""__main__"":
    inp = { ""i0"": tf.constant(0.2) }
    m = MyModule()
    runner = get_tflite_callable(m, inp)

    print(m(**inp)) # works fine
    
    print(runner(**inp)) # works fines
```

For TF, the model above works fine. But for TFLite, it raises an error as below.

```python
2022-09-14 10:46:53.593802: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-14 10:46:54.084941: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64::/home/colin/miniconda3/envs/py39/lib/
2022-09-14 10:46:54.085001: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64::/home/colin/miniconda3/envs/py39/lib/
2022-09-14 10:46:54.085008: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.

tf.Tensor([[[[0.3788812]]]], shape=(1, 1, 1, 1), dtype=float32) # <--- works fine here for TF !

Traceback (most recent call last):
  File ""/home/colin/code/test/scripts/tflite.py"", line 39, in <module>
    print(runner(**inp)) # works fines
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py"", line 251, in __call__
    self._interpreter_wrapper.AllocateTensors(self._subgraph_index)
RuntimeError: tensorflow/lite/kernels/fully_connected.cc:434 input->dims->data[input->dims->size - 1] != SizeOfDimension(filter, 1) (0 != 1)Node number 0 (FULLY_CONNECTED) failed to prepare.
```


"
57696,Silent update in google play for android mobile  release crash tensorflowlite library,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8.0

### Custom Code

Yes

### OS Platform and Distribution

Android arm OS 8-11

### Mobile device

Android devices

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!

After publish new update with new tensorflowlites ml files new crashes began.
It happening of tensorflow generated file that can not correct unpack new data.

Attach loggs below
```


### Standalone code to reproduce the issue

```shell
its not possible in debug mode or do it with manual tests
```


### Relevant log output

```shell
//JAVA LOG
android.content.res.AssetManager.nativeOpenAssetFd (AssetManager.java)
android.content.res.AssetManager.openFd (AssetManager.java:950)
org.tensorflow.lite.support.common.FileUtil.loadMappedFile (FileUtil.java:159)
org.tensorflow.lite.support.model.Model.createModel (Model.java:172)
Model.<init> (Model.java:23)
ModelnewInstance (Model.java:29)
java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1167)
java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:641)
java.lang.Thread.run (Thread.java:923) 

// IT CAN BE SEPARATE PROBLEMS..... 
// JNI LOG
==/split_config.armeabi_v7a.apk!libtensorflowlite_jni.so
libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run)
  #00  pc 0x00000000000d3bd5  /apex/com.android.art/lib/libart.so (art_quick_invoke_stub_internal)
  #00  pc 0x00000000004eb915  /apex/com.android.art/lib/libart.so (art_quick_invoke_static_stub)
  #00  pc 0x000000000012bf3f  /apex/com.android.art/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*))
  #00  pc 0x000000000023f6d7  /apex/com.android.art/lib/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*))
  #00  pc 0x0000000000237397  /apex/com.android.art/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*))
  #00  pc 0x00000000004df7a7  /apex/com.android.art/lib/libart.so (MterpInvokeStatic)
  #00  pc 0x00000000000ce794  /apex/com.android.art/lib/libart.so (mterp_op_invoke_static)
  #00  pc 0x0000000000538eb4  /data/app/~~fnfQMxtYKukpS_kOKZ4rqA==/-Y6IEuVzbvGcozyoeNelTZw==/oat/arm/base.vdex (org.tensorflow.lite.NativeInterpreterWrapper.run)
  #00  pc 0x00000000004e2e93  /apex/com.android.art/lib/libart.so (MterpInvokeVirtualQuick)
  #00  pc 0x00000000000d2394  /apex/com.android.art/lib/libart.so (mterp_op_invoke_virtual_quick)
  #00  pc 0x0000000000538894  /data/app/~~fnfQMxtYKukpS_kOKZ4rqA==/-Y6IEuVzbvGcozyoeNelTZw==/oat/arm/base.vdex (org.tensorflow.lite.InterpreterImpl.run)
  #00  pc 0x00000000004e2e93  /apex/com.android.art/lib/libart.so (MterpInvokeVirtualQuick)
  #00  pc 0x00000000000d2394  /apex/com.android.art/lib/libart.so (mterp_op_invoke_virtual_quick)
  #00  pc 0x000000000026ec90  /data/app/~~fnfQMxtYKukpS_kOKZ4rqA==/-Y6IEuVzbvGcozyoeNelTZw==/oat/arm/base.vdex (tensor.Classifier.recognizeImage)
  #00  pc 0x00000000004e2e93  /apex/com.android.art/lib/libart.so (MterpInvokeVirtualQuick)
  #00  pc 0x00000000000d2394  /apex/com.android.art/lib/libart.so (mterp_op_invoke_virtual_quick)
  #00  pc 0x0000000000262b88  /data/app/~~fnfQMxtYKukpS_kOKZ4rqA==/-Y6IEuVzbvGcozyoeNelTZw==/oat/arm/base.vdex (tensorflow.HairDetectorNew$$ExternalSyntheticLambda0.run)
  #00  pc 0x0000000000230391  /apex/com.android.art/lib/libart.so (art::interpreter::Execute(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame&, art::JValue, bool, bool) (.llvm.270406214262585567))
  #00  pc 0x0000000000236b01  /apex/com.android.art/lib/libart.so (art::interpreter::EnterInterpreterFromEntryPoint(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*))
  #00  pc 0x00000000004ce7db  /apex/com.android.art/lib/libart.so (artQuickToInterpreterBridge)
  #00  pc 0x00000000000d8761  /apex/com.android.art/lib/libart.so (art_quick_to_interpreter_bridge)
  #00  pc 0x00000000001113f9  /apex/com.android.art/javalib/arm/boot.oat (java.util.concurrent.Executors$RunnableAdapter.call)
  #00  pc 0x0000000000154463  /apex/com.android.art/javalib/arm/boot.oat (java.util.concurrent.FutureTask.run)
  #00  pc 0x0000000000187257  /apex/com.android.art/javalib/arm/boot.oat (java.util.concurrent.ThreadPoolExecutor.runWorker)
  #00  pc 0x0000000000184de7  /apex/com.android.art/javalib/arm/boot.oat (java.util.concurrent.ThreadPoolExecutor$Worker.run)
  #00  pc 0x00000000000dfad1  /apex/com.android.art/javalib/arm/boot.oat (java.lang.Thread.run)
  #00  pc 0x00000000000d3bd5  /apex/com.android.art/lib/libart.so (art_quick_invoke_stub_internal)
  #00  pc 0x00000000004eb7e3  /apex/com.android.art/lib/libart.so (art_quick_invoke_stub)
  #00  pc 0x000000000012bf2d  /apex/com.android.art/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*))
  #00  pc 0x00000000003f8b27  /apex/com.android.art/lib/libart.so (art::JValue art::InvokeVirtualOrInterfaceWithJValues<art::ArtMethod*>(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, art::ArtMethod*, jvalue const*))
  #00  pc 0x00000000003f8c37  /apex/com.android.art/lib/libart.so (art::JValue art::InvokeVirtualOrInterfaceWithJValues<_jmethodID*>(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, jvalue const*))
  #00  pc 0x000000000043a165  /apex/com.android.art/lib/libart.so (art::thread::CreateCallback(void*))
  #00  pc 0x00000000000aacf3  /apex/com.android.runtime/lib/bionic/libc.so (__pthread_start(void*))
  #00  pc 0x0000000000064063  /apex/com.android.runtime/lib/bionic/libc.so (__start_thread)
```
</details>"
57694,dataset_fn: Table not initialized when saved preprocessor is loaded,"# env

- Python 3.9.12 (main, Apr  5 2022, 01:53:17) 

- tf.__version__ == '2.10.0'


In distributed training,  I use tf.distribute.ParameterServerStrategy. 

- If I use the **saved** preprocessor(), an error raised. ""Table not initialized.""

- If I **construct** preprocessor(), no error.


The code is below, save it to demo.py: 

``` python

#!/usr/bin/env python
# -*- coding: utf-8 -*-

import argparse
import multiprocessing
from typing import Dict, List

import portpicker
import tensorflow as tf


class MyPreprocessor(tf.keras.Model):
    def __init__(self):
        super().__init__(trainable=False)
        self._lookup = {}
        self._lookup[""hour""] = tf.keras.layers.StringLookup(vocabulary=[""00"", ""01"", ""02"", ""03"", ""04"", ""05""])

    def call(self, batch: Dict):
        ans = {}
        ans[""lookup""] = self._lookup[""hour""](batch[""hour""])
        return ans

    def adapt(self, save_path=""/tmp/preprocessor""):
        ds = tf.data.Dataset.from_tensor_slices(
            {
                ""hour"": [""01"", ""03"", ""04"", ""05"", ""01""],
            }
        )
        ds = ds.batch(2).map(self)
        _ = next(iter(ds))
        self.save(""/tmp/preprocessor"", save_traces=True)


def create_in_process_cluster(n_worker: int, n_ps: int = 1) -> ""ClusterResolver"":
    """"""Creates and starts local servers and returns the cluster_resolver.""""""
    worker_ports = [portpicker.pick_unused_port() for _ in range(n_worker)]
    ps_ports = [portpicker.pick_unused_port() for _ in range(n_ps)]

    cluster_dict = {}
    cluster_dict[""worker""] = [""localhost:%s"" % port for port in worker_ports]
    if n_ps > 0:
        cluster_dict[""ps""] = [""localhost:%s"" % port for port in ps_ports]

    cluster_spec = tf.train.ClusterSpec(cluster_dict)

    # Workers need some inter_ops threads to work properly.
    worker_config = tf.compat.v1.ConfigProto()
    if multiprocessing.cpu_count() < n_worker + 1:
        worker_config.inter_op_parallelism_threads = n_worker + 1

    for i in range(n_worker):
        tf.distribute.Server(cluster_spec, job_name=""worker"", task_index=i, config=worker_config, protocol=""grpc"")

    for i in range(n_ps):
        tf.distribute.Server(cluster_spec, job_name=""ps"", task_index=i, protocol=""grpc"")

    cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(cluster_spec, rpc_layer=""grpc"")
    return cluster_resolver


def demo_ps(flag=""ok""):

    n_worker, n_ps = 3, 1
    cluster_resolver = create_in_process_cluster(n_worker=n_worker, n_ps=n_ps)
    variable_partitioner = tf.distribute.experimental.partitioners.MinSizePartitioner(min_shard_bytes=(256 << 10), max_shards=n_ps)
    strategy = tf.distribute.ParameterServerStrategy(
        cluster_resolver,
        variable_partitioner=variable_partitioner,
    )
    coordinator = tf.distribute.coordinator.ClusterCoordinator(strategy=strategy)

    with coordinator.strategy.scope():
        p_construct = MyPreprocessor()
        p_construct.adapt(""/tmp/preprocessor"")
        p_loaded = tf.keras.models.load_model(""/tmp/preprocessor"")

        def dataset_fn(input_context):
            ds = (
                tf.data.Dataset.from_tensor_slices(
                    {
                        ""hour"": [""01"", ""01"", ""01"", ""05"", ""05"", ""05""],
                    }
                )
                .batch(3)
                .repeat(16)
                .map(p_loaded if flag == ""bad"" else p_construct)
            )
            return ds

    @tf.function
    def per_worker_dataset_fn():
        return strategy.distribute_datasets_from_function(dataset_fn=dataset_fn, options=None)

    ds = coordinator.create_per_worker_dataset(per_worker_dataset_fn)
    dsit = iter(ds)

    @tf.function
    def train_step_fn(iterator):
        z = next(iterator)
        tf.print(z)
        return z

    for i in range(5):
        coordinator.schedule(train_step_fn, args=(dsit,))

    coordinator.join()


def check_preprocessor():
    p_construct = MyPreprocessor()
    p_construct.adapt(""/tmp/preprocessor"")
    p_loaded = tf.keras.models.load_model(""/tmp/preprocessor"")

    for p in [p_construct, p_loaded]:
        print(p._lookup[""hour""].get_vocabulary())


if __name__ == ""__main__"":

    parser = argparse.ArgumentParser()
    parser.add_argument(""--mode"", ""-m"", default=""demo_bad"")
    args = parser.parse_args()
    print(args)

    if args.mode == ""check_preprocessors"":
        check_preprocessor()
    elif args.mode == ""demo_ok"":
        demo_ps(""ok"")
    elif args.mode == ""demo_bad"":
        demo_ps(""bad"")
    else:
        raise ValueError(""only check_preprocessors/demo_ok/demo_bad supported"")

```

# Output of `python demo.py`

- `python demo.py -m check_preprocessor`: Make sure both preprocessors has the same vocabulary
- `python demo.py -m demo_bad`: Show the error
- `python demo.py -m demo_ok`: this works( but should not be used for very large data)

## python demo.py -m check_preprocessors

2022-09-15 12:15:37.962618: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Namespace(mode='check_preprocessors')
2022-09-15 12:15:41.228254: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.

['[UNK]', '00', '01', '02', '03', '04', '05']
['[UNK]', '00', '01', '02', '03', '04', '05']

## python demo.py -m demo_bad

2022-09-15 12:15:19.220947: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Namespace(mode='demo_bad')
2022-09-15 12:15:22.503514: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-15 12:15:22.506460: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52592}
2022-09-15 12:15:22.506480: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52589, 1 -> localhost:52590, 2 -> localhost:52591}
2022-09-15 12:15:22.506599: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:52589
2022-09-15 12:15:22.513135: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52592}
2022-09-15 12:15:22.513177: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52589, 1 -> localhost:52590, 2 -> localhost:52591}
2022-09-15 12:15:22.555869: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:52590
2022-09-15 12:15:22.569988: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52592}
2022-09-15 12:15:22.570037: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52589, 1 -> localhost:52590, 2 -> localhost:52591}
2022-09-15 12:15:22.570326: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:52591
2022-09-15 12:15:22.578150: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52592}
2022-09-15 12:15:22.578189: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52589, 1 -> localhost:52590, 2 -> localhost:52591}
2022-09-15 12:15:22.620104: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:52592
2022-09-15 12:15:22.639992: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52592}
2022-09-15 12:15:22.640025: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52589, 1 -> localhost:52590, 2 -> localhost:52591}
2022-09-15 12:15:22.640080: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job chief -> {0 -> localhost:34262}
2022-09-15 12:15:22.650400: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52592}
2022-09-15 12:15:22.650423: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52589, 1 -> localhost:52590, 2 -> localhost:52591}
2022-09-15 12:15:22.684019: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job chief -> {0 -> localhost:34262}
2022-09-15 12:15:22.684142: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52592}
2022-09-15 12:15:22.684202: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52592}
2022-09-15 12:15:22.684235: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52589, 1 -> localhost:52590, 2 -> localhost:52591}
2022-09-15 12:15:22.684282: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job chief -> {0 -> localhost:34262}
2022-09-15 12:15:22.747368: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52592}
2022-09-15 12:15:22.747487: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52589, 1 -> localhost:52590, 2 -> localhost:52591}
2022-09-15 12:15:22.747513: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job chief -> {0 -> localhost:34262}
2022-09-15 12:15:22.747548: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52589, 1 -> localhost:52590, 2 -> localhost:52591}
2022-09-15 12:15:22.747652: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job chief -> {0 -> localhost:34262}
2022-09-15 12:15:22.747679: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:275] Creating sync eager service context with rendezvous_id on host qimaodeMacBook-Pro.local /job:ps/replica:0/task:0
2022-09-15 12:15:22.814046: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:275] Creating sync eager service context with rendezvous_id on host qimaodeMacBook-Pro.local /job:worker/replica:0/task:0
2022-09-15 12:15:22.814158: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:275] Creating sync eager service context with rendezvous_id on host qimaodeMacBook-Pro.local /job:worker/replica:0/task:2
2022-09-15 12:15:22.814265: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:275] Creating sync eager service context with rendezvous_id on host qimaodeMacBook-Pro.local /job:worker/replica:0/task:1
2022-09-15 12:15:22.817835: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52592}
2022-09-15 12:15:22.817886: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52589, 1 -> localhost:52590, 2 -> localhost:52591}
2022-09-15 12:15:22.883315: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job chief -> {0 -> localhost:34262}
2022-09-15 12:15:22.884199: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:34262
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.
2022-09-15 12:15:23.511333: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
2022-09-15 12:15:23.545379: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at lookup_table_op.cc:929 : FAILED_PRECONDITION: Table not initialized.
2022-09-15 12:15:23.545648: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at lookup_table_op.cc:929 : FAILED_PRECONDITION: Table not initialized.
2022-09-15 12:15:23.545861: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at lookup_table_op.cc:929 : FAILED_PRECONDITION: Table not initialized.
2022-09-15 12:15:23.546399: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:58] Ignoring an error encountered when deleting remote tensors handles: INVALID_ARGUMENT: Unable to find the relevant tensor remote_handle: Op ID: 95, Output num: 0
Additional GRPC error information from remote target /job:worker/replica:0/task:2:
:{""created"":""@1663215323.546318000"",""description"":""Error received from peer ipv6:[::1]:52591"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unable to find the relevant tensor remote_handle: Op ID: 95, Output num: 0"",""grpc_status"":3} [type.googleapis.com/tensorflow.core.platform.ErrorSourceProto='\x08\x05']
2022-09-15 12:15:23.550175: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at lookup_table_op.cc:929 : FAILED_PRECONDITION: Table not initialized.
2022-09-15 12:15:23.577872: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at lookup_table_op.cc:929 : FAILED_PRECONDITION: Table not initialized.
2022-09-15 12:15:23.578019: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at lookup_table_op.cc:929 : FAILED_PRECONDITION: Table not initialized.
2022-09-15 12:15:23.578038: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at lookup_table_op.cc:929 : FAILED_PRECONDITION: Table not initialized.
ERROR:tensorflow:/job:worker/task:2 encountered the following error when processing closure: FailedPreconditionError():Graph execution error:

Table not initialized.
	 [[{{node string_lookup/None_Lookup/LookupTableFindV2}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
Additional GRPC error information from remote target /job:worker/replica:0/task:2:
:{""created"":""@1663215323.545921000"",""description"":""Error received from peer ipv6:[::1]:52591"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":"" Table not initialized.\n\t [[{{node string_lookup/None_Lookup/LookupTableFindV2}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNextAsOptional]]"",""grpc_status"":9} [Op:__inference_train_step_fn_409]
2022-09-15 12:15:23.578253: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at lookup_table_op.cc:929 : FAILED_PRECONDITION: Table not initialized.
2022-09-15 12:15:23.643837: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at lookup_table_op.cc:929 : FAILED_PRECONDITION: Table not initialized.
ERROR:tensorflow:/job:worker/task:1 encountered the following error when processing closure: FailedPreconditionError():Graph execution error:

Table not initialized.
	 [[{{node string_lookup/None_Lookup/LookupTableFindV2}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
Additional GRPC error information from remote target /job:worker/replica:0/task:1:
:{""created"":""@1663215323.578301000"",""description"":""Error received from peer ipv6:[::1]:52590"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":"" Table not initialized.\n\t [[{{node string_lookup/None_Lookup/LookupTableFindV2}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNextAsOptional]]"",""grpc_status"":9} [Op:__inference_train_step_fn_409]
2022-09-15 12:15:23.650184: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at lookup_table_op.cc:929 : FAILED_PRECONDITION: Table not initialized.
ERROR:tensorflow:/job:worker/task:0 encountered the following error when processing closure: FailedPreconditionError():Graph execution error:

Table not initialized.
	 [[{{node string_lookup/None_Lookup/LookupTableFindV2}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{""created"":""@1663215323.578288000"",""description"":""Error received from peer ipv6:[::1]:52589"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":"" Table not initialized.\n\t [[{{node string_lookup/None_Lookup/LookupTableFindV2}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNextAsOptional]]"",""grpc_status"":9} [Op:__inference_train_step_fn_409]
ERROR:tensorflow:Start cancelling closures due to error FailedPreconditionError(): Graph execution error:

Table not initialized.
	 [[{{node string_lookup/None_Lookup/LookupTableFindV2}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
Additional GRPC error information from remote target /job:worker/replica:0/task:2:
:{""created"":""@1663215323.545921000"",""description"":""Error received from peer ipv6:[::1]:52591"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":"" Table not initialized.\n\t [[{{node string_lookup/None_Lookup/LookupTableFindV2}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNextAsOptional]]"",""grpc_status"":9} [Op:__inference_train_step_fn_409]
ERROR:tensorflow:/job:worker/task:2 encountered the following error when processing closure: FailedPreconditionError():Graph execution error:

Table not initialized.
	 [[{{node string_lookup/None_Lookup/LookupTableFindV2}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
Additional GRPC error information from remote target /job:worker/replica:0/task:2:
:{""created"":""@1663215323.649936000"",""description"":""Error received from peer ipv6:[::1]:52591"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":"" Table not initialized.\n\t [[{{node string_lookup/None_Lookup/LookupTableFindV2}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNextAsOptional]]"",""grpc_status"":9} [Op:__inference_train_step_fn_409]
Traceback (most recent call last):
  File ""/home/myname/myrepo/recommender/recommender/ranker/qmrs/demo.py"", line 130, in <module>
    demo_ps(""bad"")
  File ""/home/myname/myrepo/recommender/recommender/ranker/qmrs/demo.py"", line 106, in demo_ps
    coordinator.join()
  File ""/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/distribute/coordinator/cluster_coordinator.py"", line 1107, in join
    self._cluster.join()
  File ""/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/distribute/coordinator/cluster_coordinator.py"", line 914, in join
    self.closure_queue.wait()
  File ""/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/distribute/coordinator/cluster_coordinator.py"", line 447, in wait
    self._raise_if_error()
  File ""/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/distribute/coordinator/cluster_coordinator.py"", line 371, in _raise_if_error
    raise self._error  # pylint: disable=raising-bad-type
  File ""/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/distribute/coordinator/cluster_coordinator.py"", line 704, in _process_closure
    closure.execute_on(self)
  File ""/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/distribute/coordinator/cluster_coordinator.py"", line 248, in execute_on
    output_values = self._function(
  File ""/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py"", line 1892, in cancellable_call
    return self._call_impl(
  File ""/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py"", line 1613, in _call_impl
    return self._call_with_structured_signature(args, kwargs,
  File ""/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py"", line 1694, in _call_with_structured_signature
    return self._call_flat(
  File ""/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py"", line 1862, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py"", line 506, in call
    outputs = execute.execute_with_cancellation(
  File ""/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py"", line 106, in execute_with_cancellation
    tensors = pywrap_tfe.TFE_Py_ExecuteCancelable(ctx._handle, device_name,
tensorflow.python.framework.errors_impl.FailedPreconditionError: Graph execution error:

Table not initialized.
	 [[{{node string_lookup/None_Lookup/LookupTableFindV2}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
Additional GRPC error information from remote target /job:worker/replica:0/task:2:
:{""created"":""@1663215323.545921000"",""description"":""Error received from peer ipv6:[::1]:52591"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":"" Table not initialized.\n\t [[{{node string_lookup/None_Lookup/LookupTableFindV2}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNextAsOptional]]"",""grpc_status"":9} [Op:__inference_train_step_fn_409]

## python demo.py -m demo_ok
2022-09-15 12:15:27.024498: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Namespace(mode='demo_ok')
2022-09-15 12:15:30.312957: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-15 12:15:30.315815: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52605}
2022-09-15 12:15:30.315859: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52602, 1 -> localhost:52603, 2 -> localhost:52604}
2022-09-15 12:15:30.316004: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:52602
2022-09-15 12:15:30.323038: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52605}
2022-09-15 12:15:30.323062: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52602, 1 -> localhost:52603, 2 -> localhost:52604}
2022-09-15 12:15:30.365470: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:52603
2022-09-15 12:15:30.382867: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52605}
2022-09-15 12:15:30.382900: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52602, 1 -> localhost:52603, 2 -> localhost:52604}
2022-09-15 12:15:30.383070: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:52604
2022-09-15 12:15:30.392620: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52605}
2022-09-15 12:15:30.392657: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52602, 1 -> localhost:52603, 2 -> localhost:52604}
2022-09-15 12:15:30.434817: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:52605
2022-09-15 12:15:30.452386: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52605}
2022-09-15 12:15:30.452423: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52602, 1 -> localhost:52603, 2 -> localhost:52604}
2022-09-15 12:15:30.452432: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job chief -> {0 -> localhost:34276}
2022-09-15 12:15:30.463669: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52605}
2022-09-15 12:15:30.463701: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52605}
2022-09-15 12:15:30.502447: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52605}
2022-09-15 12:15:30.502497: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52602, 1 -> localhost:52603, 2 -> localhost:52604}
2022-09-15 12:15:30.502526: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job chief -> {0 -> localhost:34276}
2022-09-15 12:15:30.502585: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52605}
2022-09-15 12:15:30.502626: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52602, 1 -> localhost:52603, 2 -> localhost:52604}
2022-09-15 12:15:30.502639: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job chief -> {0 -> localhost:34276}
2022-09-15 12:15:30.561039: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52602, 1 -> localhost:52603, 2 -> localhost:52604}
2022-09-15 12:15:30.561155: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job chief -> {0 -> localhost:34276}
2022-09-15 12:15:30.561203: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52602, 1 -> localhost:52603, 2 -> localhost:52604}
2022-09-15 12:15:30.561313: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job chief -> {0 -> localhost:34276}
2022-09-15 12:15:30.561383: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:275] Creating sync eager service context with rendezvous_id on host qimaodeMacBook-Pro.local /job:ps/replica:0/task:0
2022-09-15 12:15:30.628756: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:275] Creating sync eager service context with rendezvous_id on host qimaodeMacBook-Pro.local /job:worker/replica:0/task:2
2022-09-15 12:15:30.628934: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:275] Creating sync eager service context with rendezvous_id on host qimaodeMacBook-Pro.local /job:worker/replica:0/task:1
2022-09-15 12:15:30.629041: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:275] Creating sync eager service context with rendezvous_id on host qimaodeMacBook-Pro.local /job:worker/replica:0/task:0
2022-09-15 12:15:30.632171: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> localhost:52605}
2022-09-15 12:15:30.632264: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:52602, 1 -> localhost:52603, 2 -> localhost:52604}
2022-09-15 12:15:30.696763: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job chief -> {0 -> localhost:34276}
2022-09-15 12:15:30.698215: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:34276
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.
2022-09-15 12:15:31.312783: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
{'lookup': [2 2 2]}
{'lookup': [2 2 2]}
{'lookup': [2 2 2]}
{'lookup': [6 6 6]}
{'lookup': [6 6 6]}





"
57693,"order of the inputs and the outputs, within the TF-lite model, is not according to the original TF model. How to solve it?","### 1. System information

- macOS 12.5.1
- TensorFlow 2.10.0

### 2. Code

```
input1 = tf.keras.layers.Input(shape=(3,3,2), name=""input1"")
input2 = tf.keras.layers.Input(shape=(3,3,4), name=""input2"")
input3 = tf.keras.layers.Input(shape=(3,3,6), name=""input3"")
input4 = tf.keras.layers.Input(shape=(3,3,8), name=""input4"")

input5 = tf.keras.layers.concatenate([input1, input2, input3, input4], axis=-1, name=""input5"")
model = tf.keras.Model(inputs=[input1, input2, input3, input4], outputs=[input1, input2, input3, input4, input5])
```

[Colab notebook with the model above and the prints of the input/output details + signatures](https://colab.research.google.com/drive/1Zegxc5kR8ckJYnVhg6lLpAX6dD54pSiz?usp=sharing)

### 3. Failure after conversion

The order of the inputs and the outputs, within the TF-lite model, is not according to the original TF model. I have seen similar issues such as [56985](https://github.com/tensorflow/tensorflow/issues/56985) and [57043](https://github.com/tensorflow/tensorflow/issues/57043) with suggestions related to the signature and the names' sorting. Nevertheless, I could not manage to find the order of inputs and the outputs, within the TF-lite model, with respect to the original TF model.
"
57691,TFlite conversion Doesn't support dense layers with inputs of rank greater than 2,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 64 bit
- TensorFlow installation (pip package or built from source): Python 3.8.2, pip: 22.1, pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): TF version:'2.9.0'

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
```
import tensorflow as tf
import numpy as np
import pathlib

def tflite_convert(model,data):
    def representative_data_gen():
            for input_value in data:
                input_value = input_value[np.newaxis, ...]
                yield [input_value] # shape should be (1, <data point size))
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = representative_data_gen
    # Ensure that if any ops can't be quantized, the converter throws an error
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]

    converter.inference_input_type = tf.int8
    converter.inference_output_type = tf.int8
    
    tflite_model = converter.convert()
    tflite_models_dir = pathlib.Path(""./output/tflite_models/"")
    tflite_models_dir.mkdir(exist_ok=True, parents=True)
    tflite_model_file = tflite_models_dir/""model_{0}.tflite"".format(model.name)
    tflite_model_file.write_bytes(tflite_model)

def tflite_explore(path):
    tflite_interpreter = tf.lite.Interpreter(model_path=path)
    tflite_interpreter.allocate_tensors()
    
    '''
    Check input/output details
    '''
    input_details = tflite_interpreter.get_input_details()
    output_details = tflite_interpreter.get_output_details()

    print(""== Input details =="")
    print(""name:"", input_details[0]['name'])
    print(""shape:"", input_details[0]['shape'])
    print(""type:"", input_details[0]['dtype'])
    print(""\n== Output details =="")
    print(""name:"", output_details[0]['name'])
    print(""shape:"", output_details[0]['shape'])
    print(""type:"", output_details[0]['dtype'])


def make_dense_model():
    input_layer = tf.keras.Input(shape=(1,5,12))
    dense_layer=tf.keras.layers.Dense(3)(input_layer)
    model=tf.keras.Model(input_layer,dense_layer)
    return model

def generate_Noise_Data(shape,batch_size):
    if None in shape:
        shape=list(shape)
        shape[0]=batch_size  
    noise=np.array(np.random.randint(0,255,shape).astype(np.float32))
    return noise/255

model=make_dense_model()
print(model.summary())
in_value=generate_Noise_Data(model.layers[0].input_shape[0],2)
print(model(in_value))
model.save(""./output/dense1layer.h5"")
tflite_convert(model,in_value)
tflite_explore(""./output/tflite_models/model_{0}.tflite"".format(model.name))

```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results
- here i use a keras model that is converted to .tflite file. the keras model's HDF5 save and its tflite converted '.tflite' are visualized in netron. their outputs don't match.
- in the keras model: the input is of shape (none,1,5,12) (shape is>2D ).the model has a single dense layer with 3 nodes. its kernal are of shape [12,3]. the model produces an output of shape (none,1,5,3). 
- in the .tflite converted file the output is of shape [1,1,1,3] (not [1,1,5,3]) and input is of shape [1,1,5,12] 



this is the output i have obtained with the above code:
![image](https://user-images.githubusercontent.com/94594473/190126454-8f0a86be-89f4-41eb-8121-1540016a1030.png)
its netrons with .h5 on the left and .tflite on the right
![image](https://user-images.githubusercontent.com/94594473/190126958-7f6d1077-66cd-4276-9c54-bebc6042418c.png)


"
57690,Datasets created by `from_generator` do not properly release objects,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When we create a dataset by using `tf.data.Dataset.from_generator`, it seems that Python objects captured in the generator are not properly released.

This causes crucial memory leak when conducting experiments that creates datasets with large memory footprint for multiple times (e.g., cross validation).


### Standalone code to reproduce the issue

We expect `__del__ called!` to be printed for three times, but it is never printed.

```python
from typing import Generator

import tensorflow as tf


class SomeObj:
    def __init__(self, x):
        self.x = x

    def __del__(self):
        print(""__del__ called!"")


def build_dataset(
    obj: SomeObj,
) -> tf.data.Dataset:
    def _generator() -> Generator[tf.Tensor, None, None]:
        while True:
            yield tf.convert_to_tensor(obj.x, dtype=tf.float32)

    return tf.data.Dataset.from_generator(
        _generator,
        output_signature=tf.TensorSpec(shape=(), dtype=tf.float32),
    )


def train_for_a_fold(fold):
    dataset = build_dataset(SomeObj(fold))
    for x in dataset.take(3):
        print(x)


def main():
    train_for_a_fold(0)
    train_for_a_fold(1)
    train_for_a_fold(2)


if __name__ == ""__main__"":
    main()
```
#### Output

```
tf.Tensor(0.0, shape=(), dtype=float32)
tf.Tensor(0.0, shape=(), dtype=float32)
tf.Tensor(0.0, shape=(), dtype=float32)
tf.Tensor(1.0, shape=(), dtype=float32)
tf.Tensor(1.0, shape=(), dtype=float32)
tf.Tensor(1.0, shape=(), dtype=float32)
tf.Tensor(2.0, shape=(), dtype=float32)
tf.Tensor(2.0, shape=(), dtype=float32)
tf.Tensor(2.0, shape=(), dtype=float32)
```

### Relevant log output

_No response_</details>"
57688,How do I call SparseMatrixZeros,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

gpu

### Mobile device

gpu

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.1

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I want to use SparseMatrixZeros on TensorFlow 2.9, but I can't call it. Could someone please tell me how to call it.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow import SparseMatrixZeros
```


### Relevant log output

_No response_</details>"
57685,Error while running Bazel Test on windows platform,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.10.0

### Custom Code

No

### OS Platform and Distribution

windows 10 

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.3.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Running Test Cases on the windows platform
some of sample test cases which give the error of DATA_LOSS: file is too short to be an sstable. A detailed log is available in Relevant log output.

//tensorflow/c/experimental/saved_model/core/ops:restore_ops_test
//tensorflow/cc/saved_model:saved_model_bundle_test
//tensorflow/cc/saved_model:saved_model_bundle_lite_test
//tensorflow/core/util/tensor_bundle:tensor_bundle_test
//tensorflow/cc/experimental/libtf:libtf_transform_test
```


### Standalone code to reproduce the issue

```shell
bazel ^
        --output_user_root=C:\tmp\ ^
        test ^
        --nodistinct_host_configuration ^
	--enable_runfiles ^
        --dynamic_mode=off ^
	--test_verbose_timeout_warnings ^
        --config=xla ^
        --config=short_logs ^
        --announce_rc ^
        --build_tag_filters=-no_windows,-no_oss ^
        --build_tests_only ^
        --config=monolithic ^
        --keep_going ^
        --test_output=errors ^
        --test_tag_filters=-no_windows,-no_oss,-gpu,-tpu ^
        --test_size_filters=small,medium ^
        --test_timeout=""300,450,1200,3600"" ^
        --verbose_failures ^
        --copt=/d2ReducedOptimizeHugeFunctions ^
        --host_copt=/d2ReducedOptimizeHugeFunctions ^
        -- ^
        //tensorflow/... ^
        -//tensorflow/java/... ^
        -//tensorflow/lite/... ^
        -//tensorflow/compiler/xla/python/tpu_driver/... ^
        -//tensorflow/compiler/...
```


### Relevant log output

```shell
================================================================================
==================== Test output for //tensorflow/cc/saved_model:bundle_v2_test:
[==========] Running 3 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 3 tests from BundleV2Test
[ RUN      ] BundleV2Test.LoadsVarsAndArithmeticObjectGraph
2022-08-25 21:47:51.967131: I tensorflow/cc/saved_model/bundle_v2.cc:41] Reading SavedModel from: C:/tmp/ockagzuo/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/cc/saved_model/bundle_v2_test.exe.runfiles/org_tensorflow/tensorflow/cc/saved_model/testdata/VarsAndArithmeticObjectGraph
2022-08-25 21:47:51.968820: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: C:/tmp/ockagzuo/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/cc/saved_model/bundle_v2_test.exe.runfiles/org_tensorflow/tensorflow/cc/saved_model/testdata/VarsAndArithmeticObjectGraph
tensorflow/cc/saved_model/bundle_v2_test.cc(78): error: Expected equality of these values:
  ::tensorflow::OkStatus()
    Which is: OK
  (SavedModelV2Bundle::Load(export_dir, &bundle))
    Which is: DATA_LOSS: file is too short to be an sstable
	Unable to load SavedModel variables checkpoint from C:/tmp/ockagzuo/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/cc/saved_model/bundle_v2_test.exe.runfiles/org_tensorflow/tensorflow/cc/saved_model/testdata/VarsAndArithmeticObjectGraph/variables/variables
[  FAILED  ] BundleV2Test.LoadsVarsAndArithmeticObjectGraph (3 ms)
[ RUN      ] BundleV2Test.LoadsCyclicModule
2022-08-25 21:47:51.970947: I tensorflow/cc/saved_model/bundle_v2.cc:41] Reading SavedModel from: C:/tmp/ockagzuo/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/cc/saved_model/bundle_v2_test.exe.runfiles/org_tensorflow/tensorflow/cc/saved_model/testdata/CyclicModule
2022-08-25 21:47:51.972252: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: C:/tmp/ockagzuo/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/cc/saved_model/bundle_v2_test.exe.runfiles/org_tensorflow/tensorflow/cc/saved_model/testdata/CyclicModule
tensorflow/cc/saved_model/bundle_v2_test.cc(91): error: Expected equality of these values:
  ::tensorflow::OkStatus()
    Which is: OK
  (SavedModelV2Bundle::Load(export_dir, &bundle))
    Which is: DATA_LOSS: file is too short to be an sstable
	Unable to load SavedModel variables checkpoint from C:/tmp/ockagzuo/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/cc/saved_model/bundle_v2_test.exe.runfiles/org_tensorflow/tensorflow/cc/saved_model/testdata/CyclicModule/variables/variables
[  FAILED  ] BundleV2Test.LoadsCyclicModule (3 ms)
[ RUN      ] BundleV2Test.UpdatesMetrics
2022-08-25 21:47:51.973963: I tensorflow/cc/saved_model/bundle_v2.cc:41] Reading SavedModel from: C:/tmp/ockagzuo/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/cc/saved_model/bundle_v2_test.exe.runfiles/org_tensorflow/tensorflow/cc/saved_model/testdata/VarsAndArithmeticObjectGraph
2022-08-25 21:47:51.975363: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: C:/tmp/ockagzuo/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/cc/saved_model/bundle_v2_test.exe.runfiles/org_tensorflow/tensorflow/cc/saved_model/testdata/VarsAndArithmeticObjectGraph
tensorflow/cc/saved_model/bundle_v2_test.cc(108): error: Expected equality of these values:
  ::tensorflow::OkStatus()
    Which is: OK
  (SavedModelV2Bundle::Load(export_dir, &bundle))
    Which is: DATA_LOSS: file is too short to be an sstable
	Unable to load SavedModel variables checkpoint from C:/tmp/ockagzuo/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/cc/saved_model/bundle_v2_test.exe.runfiles/org_tensorflow/tensorflow/cc/saved_model/testdata/VarsAndArithmeticObjectGraph/variables/variables
```
</details>"
57682,tf.profiler.experimental.ProfilerOptions.delay_ms does not work,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Debian 11 (bullseye)

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


I used tf.profiler.experimental.ProfilerOptions(...) and tf.profiler.experimental.client.trace(...) to profile google cloud TPU. I found TF started to profile immediately without delay for `delay_ms` milliseonds.



### Standalone code to reproduce the issue

```python
# pseudo-code for demonstration
import tensorflow as tf

opt = tf.profiler.experimental.ProfilerOptions(delay_ms=50000)

# some TPU asynchronous calls

service_addr = 'grpc://' + tpu_ip_address + profiler_port
tf.profiler.experimental.client.trace(service_addr=service_addr,
                                      logdir='gs://log',
                                      duration_ms=10000,
                                      options=opt)

```

As mentioned above, profiling starts immediately without waiting for 50s. Then I traced the calling stack, which looks like this:
```shell
tensorflow/core/profiler/rpc/client/capture_profile.cc:209
|-tensorflow/core/profiler/rpc/client/capture_profile.cc:121
  |-tensorflow/core/profiler/rpc/client/remote_profiler_session_manager.cc:69  // `session_created_time`
  |-tensorflow/core/profiler/rpc/client/remote_profiler_session_manager.cc:71
```
At tensorflow/core/profiler/rpc/client/remote_profiler_session_manager.cc:71, deadline of session is calculated by adding `duration_ms` to `session_created_time` without considering `delay_ms`.

I guess this causes the bug. Because I'm new to TF, I just point this out.

### Relevant log output

_No response_</details>"
57681,XLA makes copies of tensors instead of referencing them,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Wrapping just one tf.reshape operation into compiled @tf.function(jit_compile=True) works much more slower than without XLA compilation. It turns out that generated code contains copying of incoming tensor rather than referencing it with different shape.
Module after compilation looks like 

HloModule a_inference_foo_xla_9__XlaMustCompile_true_config_proto_6001324581131673121_executor_type_11160318154034397263_.8, alias_passthrough_params=true

ENTRY %a_inference_foo_xla_9__XlaMustCompile_true_config_proto_6001324581131673121_executor_type_11160318154034397263_.8 (arg0.1: f32[100,200,300]) -> f32[2,3000000] {
  %arg0.1 = f32[100,200,300]{2,1,0} parameter(0), parameter_replication={false}, metadata={op_name=""XLA_Args""}
  %bitcast = f32[2,3000000]{1,0} bitcast(f32[100,200,300]{2,1,0} %arg0.1), metadata={op_type=""Reshape"" op_name=""Reshape"" source_file=""<ipython-input-3-906cbb49fe55>"" source_line=6}
  ROOT %copy = f32[2,3000000]{1,0} copy(f32[2,3000000]{1,0} %bitcast)
}
``` and includes copy operation, while reshape can be done without any copies
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1Y3Sv-Ji8zf-0dpTuvSIYc-zyQBkC4HrL
```


### Relevant log output

```shell
[5] %timeit foo_xla(rnd)
34.5 ms ± 4.25 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
[6] %timeit foo_tf(rnd)
371 µs ± 60 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```
</details>"
57680,Multiplication following a int8 transposed convolutions isn't constant folded,"When converting a model that used int8 quantization aware training, conversion of transposed convolutions followed by a scalar multiplication fails.

The converter isn't able to correctly constant fold the per-tensor fake quantized weights and the scalar multiplication which is a common patter when using transposed convolutions followed by batch normalisation layers. This is a follow-up issue to #53766

### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS / Ubuntu
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.10.0 / 2.11.0-dev20220913

### 2. Code

A minimal reproduction of the issue is available in [this notebook](https://colab.research.google.com/drive/1XM12NAzoiWRJS12uYyzNMsfi571fiauA?usp=sharing). Re-run the notebook to show netron visualisations showing the conversion problem."
57679,TensorFlow 2.10.0 not compatible with TensorRT 8.4.3,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuda 11.7

### GPU model and memory

NVIDIA GeForce RTX 3090

### Current Behaviour?

```shell
I cannot use TensorRT 8 with the latest version of TensorFlow and CUDA.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

```shell
2022-09-13 11:06:57.075736: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2022-09-13 11:06:57.075769: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2022-09-13 11:06:57.075772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
```
</details>"
57678,"Include ""name"" parameter in tf.keras.models.clone_model() function to give a new name to the clone.","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.8.2

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When using the `tf.keras.models.clone_model(previous_model)` function, it assigns the same name as ""previous_model"" to the created clone. I want to assign a new name to the created clone. This can probably be done by passing a name parameter to the `clone_model()` function.
```


### Standalone code to reproduce the issue

```shell
`pre_m1 = tf.keras.Sequential([
    sentence_encoder_layer,
    layers.Dense(64,activation=""relu""),
    layers.Dense(1,activation=""sigmoid"")
],name=""pre_trained_m1"")

pre_m2 = tf.keras.models.clone_model(pre_m1)

# In the summary, the name of pre_m2 is printed as ""pre_trained_m1"" i.e. same as pre_m1.
print(pre_m2.summary())
`
```


### Relevant log output

_No response_</details>"
57677,Theoretical Question about model size (parameters),"Hi guys,

I would like to know if the model size of a model depends on the data set size, so that for each new training a new model size results?

The parameters in an architecture are fixed and do not change, but is it possible that if the training is bad, not all neurons will be activated and the model size will be smaller?

"
57675,WARNING:tensorflow: Failed to read source code from path: /tmp/ipykernel_2003167/1294704215.py,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.10.0-rc3-6-g359c3cdfc5f 2.10.0

### Custom Code

No

### OS Platform and Distribution

Linux Pop_OS! 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I was attempting to use Tensorboard and the `enable_dump_debug_info()` feature and now every time that I attempt to load in my dataset or run my models I get errors like the following:

WARNING:tensorflow:Failed to read source code from path: /tmp/ipykernel_2003167/1294704215.py. Reason: Source path neither exists nor can be loaded as a .par file: /tmp/ipykernel_2003167/1294704215.py
```

I was expecting to be able to use this feature to help me in hunting down an error in one of my models.
```


### Standalone code to reproduce the issue

```shell
Here is a link to a [Google Colab](https://colab.research.google.com/drive/1bj0T_rgTAIoSsN4J4An1VcPdiNETDrz1?usp=sharing)


import tensorflow as tf

tf.debugging.experimental.enable_dump_debug_info('logs/debug', tensor_debug_mode=""FULL_HEALTH"", circular_buffer_size=-1)

model = tf.keras.models.Sequential([])
```


### Relevant log output

```shell
INFO:tensorflow:Enabled dumping callback in thread MainThread (dump root: logs/debug, tensor debug mode: FULL_HEALTH)
INFO:tensorflow:Enabled dumping callback in thread MainThread (dump root: logs/debug, tensor debug mode: FULL_HEALTH)
WARNING:tensorflow:Failed to read source code from path: /tmp/ipykernel_2003167/1842616214.py. Reason: Source path neither exists nor can be loaded as a .par file: /tmp/ipykernel_2003167/1842616214.py
WARNING:tensorflow:Failed to read source code from path: /tmp/ipykernel_2003167/1842616214.py. Reason: Source path neither exists nor can be loaded as a .par file: /tmp/ipykernel_2003167/1842616214.py
```
</details>"
57672,Periodic-boundary padding for tensors,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.10

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
This issue is to request a feature for padding with ""wrap"", narrowing down suggestions from Issue #956 (https://github.com/tensorflow/tensorflow/issues/956).

This mode of padding is included as an option in numpy.pad()
https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html

It would be very beneficial for applications that require periodic boundary conditions, in particular for physics-based calculations.
```


### Standalone code to reproduce the issue

```shell
# From the original issue:

import numpy
matrix = numpy.arange(25).reshape(5,5)
paddings = [[1,1],[1,1]]
# periodic boundary conditions
padded_matrix = numpy.pad(matrix, pad_width=paddings, mode='wrap')
```


### Relevant log output

_No response_</details>"
57671,Tensorflow pypi wheel for Linux has outdated TensorRT dependency,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.10

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7/8.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
On Ubuntu 20.04 Tensorflow 2.10 as installed via pip tries to dlopen libnvinfer from TensorRT, which is different from the Windows pypi package (no TensorRT dependency) and more important it wants the outdated version 7 which is only available up to Ubuntu 18.04 and only compatible with CUDA up to version 11.2. Either the TensorRT dependency should not be there (as it was before and is still on Windows) or it should be against the current version 8.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.config.list_physical_devices(""GPU""))
```


### Relevant log output

```shell
2022-09-12 17:53:57.425679: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-12 17:53:57.566393: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-09-12 17:53:57.596092: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2022-09-12 17:53:58.180614: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2022-09-12 17:53:58.180675: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2022-09-12 17:53:58.180680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2022-09-12 17:53:58.968890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
2022-09-12 17:53:59.000480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-09-12 17:53:59.000632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
```
</details>"
57669,Segmentation Fault on importing Tensorflow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.4.1

### Custom Code

Yes

### OS Platform and Distribution

Debian GNU/Linux 10

### Mobile device

Debian GNU/Linux 10

### Python version

3.9.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The bug occurs when importing both tensorflow and torchvision in my current environment. When importing tensorflow before torchvision, there is no bug. However, when importing tensorflow _after_ importing torchvision, a segmentation fault occurs.

Conda environment YAML:
name: test_env
channels:
  - pytorch
  - nvidia
  - conda-forge
  - defaults
dependencies:
  - _libgcc_mutex=0.1=conda_forge
  - _openmp_mutex=4.5=1_llvm
  - anyio=3.3.0=py39hf3d152e_0
  - argon2-cffi=20.1.0=py39h3811e60_2
  - async_generator=1.10=py_0
  - attrs=21.2.0=pyhd8ed1ab_0
  - babel=2.9.1=pyh44b312d_0
  - backcall=0.2.0=pyh9f0ad1d_0
  - backports=1.0=py_2
  - backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0
  - blas=1.0=mkl
  - bleach=4.1.0=pyhd8ed1ab_0
  - brotlipy=0.7.0=py39h3811e60_1001
  - ca-certificates=2021.10.8=ha878542_0
  - certifi=2021.10.8=py39hf3d152e_0
  - cffi=1.14.6=py39h4bc2ebd_1
  - chardet=4.0.0=py39hf3d152e_1
  - charset-normalizer=2.0.0=pyhd8ed1ab_0
  - colorama=0.4.4=pyh9f0ad1d_0
  - cryptography=3.4.7=py39hbca0aa6_0
  - cudatoolkit=11.0.221=h6bb024c_0
  - cycler=0.10.0=py_2
  - dbus=1.13.6=h48d8840_2
  - debugpy=1.4.1=py39he80948d_0
  - decorator=5.1.0=pyhd8ed1ab_0
  - defusedxml=0.7.1=pyhd8ed1ab_0
  - entrypoints=0.3=pyhd8ed1ab_1003
  - expat=2.4.1=h9c3ff4c_0
  - fontconfig=2.13.1=hba837de_1005
  - freetype=2.10.4=h0708190_1
  - gettext=0.19.8.1=h73d1719_1007
  - glib=2.68.4=h9c3ff4c_1
  - glib-tools=2.68.4=h9c3ff4c_1
  - gst-plugins-base=1.14.0=hbbd80ab_1
  - gstreamer=1.14.0=h28cd5cc_2
  - icu=58.2=hf484d3e_1000
  - idna=3.1=pyhd3deb0d_0
  - importlib-metadata=4.8.1=py39hf3d152e_0
  - ipykernel=6.4.1=py39hef51801_0
  - ipython=7.27.0=py39hef51801_0
  - ipython_genutils=0.2.0=py_1
  - jedi=0.18.0=py39hf3d152e_2
  - jinja2=3.0.1=pyhd8ed1ab_0
  - joblib=1.0.1=pyhd8ed1ab_0
  - jpeg=9b=h024ee3a_2
  - json5=0.9.5=pyh9f0ad1d_0
  - jsonschema=3.2.0=pyhd8ed1ab_3
  - jupyter_client=7.0.3=pyhd8ed1ab_0
  - jupyter_core=4.8.1=py39hf3d152e_0
  - jupyter_server=1.11.0=pyhd8ed1ab_0
  - jupyterlab=3.1.12=pyhd8ed1ab_0
  - jupyterlab_pygments=0.1.2=pyh9f0ad1d_0
  - jupyterlab_server=2.8.1=pyhd8ed1ab_0
  - kiwisolver=1.3.2=py39h1a9c180_0
  - lcms2=2.12=h3be6417_0
  - ld_impl_linux-64=2.36.1=hea4e1c9_2
  - libblas=3.9.0=11_linux64_mkl
  - libcblas=3.9.0=11_linux64_mkl
  - libffi=3.4.2=h9c3ff4c_2
  - libgcc-ng=11.2.0=h1d223b6_8
  - libgfortran-ng=11.2.0=h69a702a_8
  - libgfortran5=11.2.0=h5c6108e_8
  - libglib=2.68.4=h174f98d_1
  - libiconv=1.16=h516909a_0
  - liblapack=3.9.0=11_linux64_mkl
  - libllvm11=11.1.0=hf817b99_2
  - libpng=1.6.37=h21135ba_2
  - libsodium=1.0.18=h36c2ea0_1
  - libstdcxx-ng=11.2.0=he4da1e4_8
  - libtiff=4.2.0=h85742a9_0
  - libuuid=2.32.1=h7f98852_1000
  - libwebp-base=1.2.1=h7f98852_0
  - libxcb=1.13=h7f98852_1003
  - libxml2=2.9.12=h03d6c58_0
  - llvm-openmp=12.0.1=h4bd325d_1
  - llvmlite=0.37.0=py39h1bbdace_0
  - lz4-c=1.9.3=h9c3ff4c_1
  - markupsafe=2.0.1=py39h3811e60_0
  - matplotlib=3.4.3=py39hf3d152e_1
  - matplotlib-base=3.4.3=py39h2fa2bec_1
  - matplotlib-inline=0.1.3=pyhd8ed1ab_0
  - mistune=0.8.4=py39h3811e60_1004
  - mkl=2021.3.0=h726a3e6_557
  - mkl-service=2.4.0=py39h3811e60_0
  - mkl_fft=1.3.0=py39h42c9631_2
  - mkl_random=1.2.2=py39hde0f152_0
  - nbclassic=0.3.2=pyhd8ed1ab_0
  - nbclient=0.5.4=pyhd8ed1ab_0
  - nbconvert=6.1.0=py39hf3d152e_1
  - nbformat=5.1.3=pyhd8ed1ab_0
  - ncurses=6.2=h58526e2_4
  - nest-asyncio=1.5.1=pyhd8ed1ab_0
  - ninja=1.10.2=h4bd325d_0
  - notebook=6.4.4=pyha770c72_0
  - numba=0.54.1=py39h56b8d98_0
  - numpy=1.20.3=py39hf144106_0
  - numpy-base=1.20.3=py39h74d4b33_0
  - olefile=0.46=pyh9f0ad1d_1
  - openjpeg=2.4.0=hb52868f_1
  - openssl=1.1.1l=h7f98852_0
  - packaging=21.0=pyhd8ed1ab_0
  - pandoc=2.14.2=h7f98852_0
  - pandocfilters=1.5.0=pyhd8ed1ab_0
  - parso=0.8.2=pyhd8ed1ab_0
  - pcre=8.45=h9c3ff4c_0
  - pexpect=4.8.0=pyh9f0ad1d_2
  - pickleshare=0.7.5=py_1003
  - pillow=8.3.1=py39h2c7a002_0
  - pip=21.2.4=pyhd8ed1ab_0
  - prometheus_client=0.11.0=pyhd8ed1ab_0
  - prompt-toolkit=3.0.20=pyha770c72_0
  - pthread-stubs=0.4=h36c2ea0_1001
  - ptyprocess=0.7.0=pyhd3deb0d_0
  - pycparser=2.20=pyh9f0ad1d_2
  - pygments=2.10.0=pyhd8ed1ab_0
  - pynndescent=0.5.4=pyh6c4a22f_0
  - pyopenssl=20.0.1=pyhd8ed1ab_0
  - pyparsing=2.4.7=pyh9f0ad1d_0
  - pyqt=5.9.2=py39h2531618_6
  - pyrsistent=0.17.3=py39h3811e60_2
  - pysocks=1.7.1=py39hf3d152e_3
  - python=3.9.7=hb7a2778_1_cpython
  - python-dateutil=2.8.2=pyhd8ed1ab_0
  - python_abi=3.9=2_cp39
  - pytorch=1.7.1=py3.9_cuda11.0.221_cudnn8.0.5_0
  - pytz=2021.1=pyhd8ed1ab_0
  - pyzmq=22.3.0=py39h37b5a0c_0
  - qt=5.9.7=h5867ecd_1
  - readline=8.1=h46c0cb4_0
  - requests=2.26.0=pyhd8ed1ab_0
  - requests-unixsocket=0.2.0=py_0
  - scikit-learn=0.24.2=py39h7c5d8c9_1
  - scipy=1.7.1=py39hee8e79c_0
  - send2trash=1.8.0=pyhd8ed1ab_0
  - setuptools=58.0.4=py39hf3d152e_1
  - sip=4.19.13=py39h2531618_0
  - six=1.16.0=pyh6c4a22f_0
  - sniffio=1.2.0=py39hf3d152e_1
  - sqlite=3.36.0=h9cd32fc_1
  - tbb=2021.3.0=h4bd325d_0
  - terminado=0.12.1=py39hf3d152e_0
  - testpath=0.5.0=pyhd8ed1ab_0
  - threadpoolctl=2.2.0=pyh8a188c0_0
  - tk=8.6.11=h27826a3_1
  - torchaudio=0.7.2=py39
  - torchvision=0.8.2=py39_cu110
  - tornado=6.1=py39h3811e60_1
  - tqdm=4.62.3=pyhd8ed1ab_0
  - traitlets=5.1.0=pyhd8ed1ab_0
  - typing_extensions=3.10.0.0=pyha770c72_0
  - tzdata=2021a=he74cb21_1
  - umap-learn=0.5.1=py39hf3d152e_1
  - urllib3=1.26.6=pyhd8ed1ab_0
  - wcwidth=0.2.5=pyh9f0ad1d_2
  - webencodings=0.5.1=py_1
  - websocket-client=0.57.0=py39hf3d152e_4
  - wheel=0.37.0=pyhd8ed1ab_1
  - xorg-libxau=1.0.9=h7f98852_0
  - xorg-libxdmcp=1.1.3=h7f98852_0
  - xz=5.2.5=h516909a_1
  - zeromq=4.3.4=h9c3ff4c_1
  - zipp=3.5.0=pyhd8ed1ab_0
  - zlib=1.2.11=h516909a_1010
  - zstd=1.4.9=ha95c52a_0
  - pip:
    - clip==1.0
    - ftfy==6.0.3
    - jupyterlab-vim==0.14.5
    - regex==2021.8.28
prefix: /home/ubuntu/anaconda3/envs/test_env
```


### Standalone code to reproduce the issue

```shell
#Need to import in this order to reproduce the bug
import torchvision
import tensorflow
```


### Relevant log output

_No response_</details>"
57668,Tensorflow block for FTC robots,"I would like help acquiring the tensor flow blocks needed to run FTC robot.  I am the director of the virtual robot simulator and we would like to bringing  tensorflow into our system https://powerplay.vrobotsim.online/programpage.html 
Thanks"
57667,"ModelCheckpoint used with `save_best_only` doesn't handle interruptions, even with BackupAndRestore","### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04
-   **TensorFlow installed from (source or binary)**: docker image tensorflow/tensorflow:2.10.0
-   **TensorFlow version (use command below)**: 2.10.0 (the issue is the same for 2.8 and 2.9)
-   **Python version**: 3.8.10

### Describe the problem
I am using ModelCheckpoint callback to save the best model, combined with BackupAndRestore callback to handle interruptions.
Steps leading to the issue: 
- I run a training script :heavy_check_mark: 
- The script gets interrupted :heavy_check_mark: 
- When running it again, the model is correctly restored by BackupAndRestore :heavy_check_mark: 
- However, when the model is restored and training resumes, the ModelCheckpoint doesn't behave as expected: on this new run, it saves the model on the first epoch not accounting if the loss improved or not. :negative_squared_cross_mark: 

### Source code to reproduce

#### First training
```python
import tensorflow as tf
from tensorflow import keras
import numpy as np

# Dummy datasets
np.random.seed(12)
train_ds = tf.data.Dataset.from_tensor_slices((np.random.rand(500, 10, 4), np.random.randint(0, 5, (500, 1))))
valid_ds = tf.data.Dataset.from_tensor_slices((np.random.rand(100, 10, 4), np.random.randint(0, 5, (100, 1))))

model = keras.Sequential(
    [
        keras.layers.Dense(40, activation=""relu""),
        keras.layers.Dense(100, activation=""relu""),
        keras.layers.Dense(400, activation=""relu""),
        keras.layers.Dense(10, activation=""relu""),
        keras.layers.Dense(3, activation=""relu""),
        keras.layers.Dense(1),
    ]
)

model.compile(loss='mean_squared_error',
              optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.005))

# Callbacks
backup_cb = keras.callbacks.BackupAndRestore(backup_dir='/tmp/backup_dir')
ckpt_cb = keras.callbacks.ModelCheckpoint('/tmp/best_model', save_best_only=True, monitor='val_loss', verbose=1)

# Callback that fakes an interruption
class InterruptingCallback(keras.callbacks.Callback):
    def on_epoch_begin(self, epoch, logs=None):
        if epoch == 4:
            raise RuntimeError('Interrupting!')

model.fit(train_ds, epochs=6, validation_data=valid_ds, verbose=1, callbacks=[backup_cb, ckpt_cb, InterruptingCallback()])
```
The output is:
```
Epoch 1/6
476/500 [===========================>..] - ETA: 0s - loss: 2.4351  
Epoch 1: val_loss improved from inf to 2.07992, saving model to /tmp/best_model
500/500 [==============================] - 2s 3ms/step - loss: 2.4528 - val_loss: 2.0799
Epoch 2/6
475/500 [===========================>..] - ETA: 0s - loss: 2.2506
Epoch 2: val_loss did not improve from 2.07992
500/500 [==============================] - 1s 1ms/step - loss: 2.2690 - val_loss: 2.0819
Epoch 3/6
476/500 [===========================>..] - ETA: 0s - loss: 2.2212
Epoch 3: val_loss did not improve from 2.07992
500/500 [==============================] - 1s 1ms/step - loss: 2.2440 - val_loss: 2.0859
Epoch 4/6
486/500 [============================>.] - ETA: 0s - loss: 2.2116
Epoch 4: val_loss did not improve from 2.07992
500/500 [==============================] - 1s 1ms/step - loss: 2.2372 - val_loss: 2.0894
Traceback (most recent call last):
  File ""1st_training.py"", line 36, in <module>
    model.fit(train_ds, epochs=6, validation_data=valid_ds, verbose=1, callbacks=[backup_cb, ckpt_cb, InterruptingCallback()])
  File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""1st_training.py"", line 33, in on_epoch_begin
    raise RuntimeError('Interrupting!')
RuntimeError: Interrupting!

```


#### 2nd training
```python
import tensorflow as tf
from tensorflow import keras
import numpy as np

# Dummy datasets
np.random.seed(12)
train_ds = tf.data.Dataset.from_tensor_slices((np.random.rand(500, 10, 4), np.random.randint(0, 5, (500, 1))))
valid_ds = tf.data.Dataset.from_tensor_slices((np.random.rand(100, 10, 4), np.random.randint(0, 5, (100, 1))))

model = keras.Sequential(
    [
        keras.layers.Dense(40, activation=""relu""),
        keras.layers.Dense(100, activation=""relu""),
        keras.layers.Dense(400, activation=""relu""),
        keras.layers.Dense(10, activation=""relu""),
        keras.layers.Dense(3, activation=""relu""),
        keras.layers.Dense(1),
    ]
)

model.compile(loss='mean_squared_error',
              optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.005))

# Callbacks
backup_cb = keras.callbacks.BackupAndRestore(backup_dir='/tmp/backup_dir')
ckpt_cb = keras.callbacks.ModelCheckpoint('/tmp/best_model', save_best_only=True, monitor='val_loss', verbose=1)

model.fit(train_ds, epochs=6, validation_data=valid_ds, verbose=1, callbacks=[backup_cb, ckpt_cb])
```
The output is:
```
Epoch 5/6
476/500 [===========================>..] - ETA: 0s - loss: 2.2097  
Epoch 5: val_loss improved from inf to 2.09097, saving model to /tmp/best_model
500/500 [==============================] - 2s 3ms/step - loss: 2.2322 - val_loss: 2.0910
Epoch 6/6
500/500 [==============================] - ETA: 0s - loss: 2.2200
Epoch 6: val_loss did not improve from 2.09097
500/500 [==============================] - 1s 1ms/step - loss: 2.2200 - val_loss: 2.0978
```

The problem lies at `val_loss improved from inf to 2.09097`, the model restored by BackupAndRetore doesn't restore the previous value of val_loss. The model is initialized with an `inf` value, thus ModelCheckpoint doesn't fulfill what it is supposed to do and it even overwrites the ""best"" model with a not as good model.
"
57666,Model Trains on CPU but not on GPU - TensorDot/MatMul error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA=11.2, cuDNN=8.1

### GPU model and memory

RTX 8000 48GB 

### Current Behaviour?

```shell
I am trying to train a model using the GPUs on my computer. The model trains as expected on the CPU but I get the following error when trying to use the GPU:

Node: 'model/final_output/Tensordot/MatMul'
2 root error(s) found.
  (0) INTERNAL:  Blas xGEMV launch failed : a.shape=[1,2628288,150], b.shape=[1,150,1], m=2628288, n=1, k=150
	 [[{{node model/final_output/Tensordot/MatMul}}]]
	 [[Nadam/Nadam/group_deps/_111]]
  (1) INTERNAL:  Blas xGEMV launch failed : a.shape=[1,2628288,150], b.shape=[1,150,1], m=2628288, n=1, k=150
	 [[{{node model/final_output/Tensordot/MatMul}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_29232]
```

I have tried multiple versions of tensorflow gpu (2.5 through 2.10) and get the same error. I would expect the network to train the same on GPU as on CPU.
```


### Standalone code to reproduce the issue

```shell
The network that I have created is a bit cumbersome for posting on this platform. I am happy to send files if needed.
```


### Relevant log output

```shell
Node: 'model/final_output/Tensordot/MatMul'
2 root error(s) found.
  (0) INTERNAL:  Blas xGEMV launch failed : a.shape=[1,2628288,150], b.shape=[1,150,1], m=2628288, n=1, k=150
	 [[{{node model/final_output/Tensordot/MatMul}}]]
	 [[Nadam/Nadam/group_deps/_111]]
  (1) INTERNAL:  Blas xGEMV launch failed : a.shape=[1,2628288,150], b.shape=[1,150,1], m=2628288, n=1, k=150
	 [[{{node model/final_output/Tensordot/MatMul}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_29232]
```
</details>"
57664,Inconsistant behavior of Conv2D between eager mode and tracing,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0-dev20220910

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

If the input to Conv2D is too small, in graph execution mode, a ValueError will be raised when tracing. However, if we only use eager mode to run, it will output a tensor with dim = 0. So there's an inconsistency under two modes.

If we only create a conv layer and directly use it to compute something, it works well as the eager mode.

```python
conv = layers.Conv2D(1, 2, 1, autocast=False)
x = tf.random.normal([2, 1, 2, 2])
print(conv(x)) # no error
```

I think the behavior should be consistent in all modes.


### Standalone code to reproduce the issue

```python
import tensorflow as tf
from keras import layers

class MyModule(tf.Module):
    def __init__(self):
        self.conv = layers.Conv2D(1, 2, 1, autocast=False)
    
    @tf.function
    def __call__(self, x):
        return self.conv(x)

if __name__ == '__main__':
    model = MyModule()

    tf.config.run_functions_eagerly(True)
    x = tf.random.normal([2, 1, 2, 2])
    print(model(x)) # tf.Tensor([], shape=(2, 0, 1, 1), dtype=float32)

    tf.config.run_functions_eagerly(False)
    x = tf.random.normal([2, 1, 2, 2])
    print(model(x)) # Error when tracing  
    model.__call__.get_concrete_function(x) # Same error if we call this instead of the last line
```


### Relevant log output

```python
tf.Tensor([], shape=(2, 0, 1, 1), dtype=float32) # eagerly running works fine
Traceback (most recent call last):
  File ""/home/colin/code/test/conv.py"", line 21, in <module>
    model.__call__.get_concrete_function(x)
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py"", line 1233, in get_concrete_function
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py"", line 1213, in _get_concrete_function_garbage_collected
    self._initialize(args, kwargs, add_initializers_to=initializers)
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py"", line 778, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 166, in _get_concrete_function_internal_garbage_collected
    concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 161, in _maybe_define_concrete_function
    return self._maybe_define_function(args, kwargs)
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 364, in _maybe_define_function
    concrete_function = self._create_concrete_function(args, kwargs)
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 288, in _create_concrete_function
    func_graph_module.func_graph_from_py_func(
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py"", line 1281, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py"", line 679, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 449, in bound_method_wrapper
    return wrapped_fn(*args, **kwargs)
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py"", line 1267, in autograph_handler
    raise e.ag_error_metadata.to_exception(e)
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py"", line 1256, in autograph_handler
    return autograph.converted_call(
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 439, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/__autograph_generated_file6e7qw11z.py"", line 12, in tf____call__
    retval_ = ag__.converted_call(ag__.ld(self).conv, (ag__.ld(x),), None, fscope)
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 377, in converted_call
    return _call_unconverted(f, args, kwargs, options)
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 459, in _call_unconverted
    return f(*args)
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 1967, in _create_c_op
    raise ValueError(e.message)
ValueError: in user code:

    File ""/home/colin/code/test/conv.py"", line 10, in __call__  *
        return self.conv(x)
    File ""/home/colin/miniconda3/envs/py39/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler  **
        raise e.with_traceback(filtered_tb) from None

    ValueError: Exception encountered when calling layer ""conv2d"" ""                 f""(type Conv2D).
    
    Negative dimension size caused by subtracting 2 from 1 for '{{node conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x, conv2d/Conv2D/ReadVariableOp)' with input shapes: [2,1,2,2], [2,2,2,1].
    
    Call arguments received by layer ""conv2d"" ""                 f""(type Conv2D):
      • inputs=tf.Tensor(shape=(2, 1, 2, 2), dtype=float32)
```
</details>"
57663,2.10.0 cuBLAS error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 7.5.0-3ubuntu1~18.04

### Mobile device

_No response_

### Python version

3.8.0

### Bazel version

_No response_

### GCC/Compiler version

gcc 7.5.0

### CUDA/cuDNN version

NVIDIA-SMI 450.102.04   Driver Version: 450.102.04   CUDA Version: 11.0

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I use `pip` to install tensorflow-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl`. After installation, when I import tensorflow, there is an error:
`E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered.
I noticed this is an error that occurred before and has an issue closed. But I install the latest 2.10.0 version tf and still meet this error. hope you can help me figure it out. thanks!
```


### Standalone code to reproduce the issue

```shell
pip install tensorflow-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
```


### Relevant log output

```shell
In [1]: import tensorflow as tf
2022-09-11 03:59:51.293241: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-11 03:59:51.436444: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-09-11 03:59:51.474370: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
```
```
</details>"
57662,Module not Found,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.10

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
After installing TF 2.10, my python can't find the module. even reverting back to tf 2.9 is not helping. I'm using anaconda and I've tried installing on different environment and still not helping.
```


### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

_No response_</details>"
57661,tensorflow Load model gives the random prediction in python new session.(LSTM.),"after train and test the model gives the good accuracy and perfect predictions.

model.save()
load_model()
once i save the model and testing in a new python session it gives random prediction.

i tried many methods but I'm not able to sole the issue. anyone have a solution?

"
57660,Model serialized with resize_with_pad called from map_fn produces different results after being loaded,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.7.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu Linux 20.04.4 under WSL2

### Mobile device

_No response_

### Python version

3.8.5

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A model created with a custom image processing layer produces different results after it is saved and loaded.

Specifically, the image preprocessing layer runs `tf.map_fn` to a function that contains a call to `tf.image.resize_with_pad`.

The expected behavior is that both the original model and the loaded model should produce identical results.
```


### Standalone code to reproduce the issue

```shell
from typing import Tuple
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt


class ResizeLayer(tf.keras.layers.Layer):
    def resize_impl(self, elems):
        im, xy_x2y2 = elems
        x = xy_x2y2[0]
        y = xy_x2y2[1]
        width=xy_x2y2[2]-x
        height=xy_x2y2[3]-y
        # Ultimately Id like to run this too
        #im = tf.image.crop_to_bounding_box(im, y, x, height, width)
        im = tf.image.resize_with_pad(im, 90, 90)
        return im
    def call(self, x):
        im, coords = x
        coords = tf.cast(coords, tf.int32)
        return tf.map_fn(fn=self.resize_impl, elems=[im, coords],
            fn_output_signature=tf.TensorSpec((90,90,3), dtype=tf.float32))


i_im = tf.keras.layers.Input(shape=(None, None, 3))
i_c = tf.keras.layers.Input(shape=(4))
o = ResizeLayer()((i_im, i_c))

model = tf.keras.Model(inputs=(i_im, i_c), outputs=o)
model.summary()

im = tf.random.normal((1, 1200, 1200, 3), seed=42)
np.random.seed(42)
xy_x2y2 = tf.constant([[np.random.normal(loc=100), np.random.normal(loc=100), np.random.normal(loc=1000), np.random.normal(loc=1000)]])
gt = model((im, xy_x2y2))
model.save('mymodel')
model2 = tf.keras.models.load_model('mymodel')
got = model2((im, xy_x2y2))
print(tf.reduce_max(tf.abs(gt-got)))
```


### Relevant log output

```shell
Model: ""model""
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 input_1 (InputLayer)           [(None, None, None,  0           []
                                 3)]

 input_2 (InputLayer)           [(None, 4)]          0           []

 resize_layer (ResizeLayer)     (None, 90, 90, 3)    0           ['input_1[0][0]',
                                                                  'input_2[0][0]']

==================================================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
__________________________________________________________________________________________________
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.
tf.Tensor(3.7141252, shape=(), dtype=float32)
```
</details>"
57659,Out Of Memory During Model Save,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.11.0-dev20220901

### Custom Code

No

### OS Platform and Distribution

Windows WSL

### Mobile device

_No response_

### Python version

Python 3.9.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

NVIDIA GeForce RTX 3090/21612 MB memory

### Current Behaviour?

```shell
2022-09-09 18:56:44.476100: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:798] failed to alloc 1073741824 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory

could not allocate pinned host memory of size: 1073741824

Both GPU and RAM show as having enough memory when this happens
```


### Standalone code to reproduce the issue

```shell
import os
os.environ[""TF_GPU_ALLOCATOR""] = ""cuda_malloc_async""
import tensorflow as tf
from tensorflow.python.framework.ops import disable_eager_execution

disable_eager_execution()
print(tf.config.optimizer.get_experimental_options())
for gpu in tf.config.list_physical_devices('GPU'):
    tf.config.experimental.set_memory_growth(gpu, True)

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import Dense, Flatten, Dropout
import shutil

import keras.backend
import numpy as np
import pandas
import matplotlib
from matplotlib import pyplot as plt
matplotlib.use(""svg"")

IMG_SIZE = 224
CHANNELS = 3
BATCH_SIZE = 1
LR_2 = 0.003
EPOCHS = 2

class VGG19(Sequential):
    def __init__(self, labels, input_shape):
        super().__init__()

        self.add(Conv2D(64, kernel_size=(3, 3), padding='same',
                        activation='relu', input_shape=input_shape))
        self.add(Conv2D(64, kernel_size=(3, 3), padding='same',
                        activation='relu'))
        self.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
        self.add(Conv2D(128, kernel_size=(3, 3), padding='same',
                        activation='relu'))
        self.add(Conv2D(128, kernel_size=(3, 3), padding='same',
                        activation='relu'))
        self.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
        self.add(Conv2D(256, kernel_size=(3, 3), padding='same',
                        activation='relu'))
        self.add(Conv2D(256, kernel_size=(3, 3), padding='same',
                        activation='relu'))
        self.add(Conv2D(256, kernel_size=(3, 3), padding='same',
                        activation='relu'))
        self.add(Conv2D(256, kernel_size=(3, 3), padding='same',
                        activation='relu'))
        self.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
        self.add(Conv2D(512, kernel_size=(3, 3), padding='same',
                        activation='relu'))
        self.add(Conv2D(512, kernel_size=(3, 3), padding='same',
                        activation='relu'))
        self.add(Conv2D(512, kernel_size=(3, 3), padding='same',
                        activation='relu'))
        self.add(Conv2D(512, kernel_size=(3, 3), padding='same',
                        activation='relu'))
        self.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
        self.add(Conv2D(512, kernel_size=(3, 3), padding='same',
                        activation='relu'))
        self.add(Conv2D(512, kernel_size=(3, 3), padding='same',
                        activation='relu'))
        self.add(Conv2D(512, kernel_size=(3, 3), padding='same',
                        activation='relu'))
        self.add(Conv2D(512, kernel_size=(3, 3), padding='same',
                        activation='relu'))
        self.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
        self.add(Flatten())
        self.add(Dense(4096, activation='relu'))
        self.add(Dropout(0.5))
        self.add(Dense(4096, activation='relu'))
        self.add(Dropout(0.5))
        self.add(Dense(labels, activation='sigmoid'))

        self.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR_2, decay=LR_2 / EPOCHS),
                     loss='binary_crossentropy',
                     metrics=['accuracy'])

model = VGG19(labels, (IMG_SIZE, IMG_SIZE, CHANNELS))
# training and validation is an assortment of images augmented by ImageDataGenerator

H = model.fit(train_generator,
              steps_per_epoch=STEP_SIZE_TRAIN,
              validation_data=validate_generator,
              validation_steps=STEP_SIZE_VALID,
              epochs=classify.EPOCHS,
              verbose=1
              )

model.save('data/model.h5', save_format=""h5"")
```


### Relevant log output

```shell
Model: ""vgg19""
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 conv2d (Conv2D)             (None, 224, 224, 64)      1792

 conv2d_1 (Conv2D)           (None, 224, 224, 64)      36928

 max_pooling2d (MaxPooling2D  (None, 112, 112, 64)     0
 )

 conv2d_2 (Conv2D)           (None, 112, 112, 128)     73856

 conv2d_3 (Conv2D)           (None, 112, 112, 128)     147584

 max_pooling2d_1 (MaxPooling  (None, 56, 56, 128)      0
 2D)

 conv2d_4 (Conv2D)           (None, 56, 56, 256)       295168

 conv2d_5 (Conv2D)           (None, 56, 56, 256)       590080

 conv2d_6 (Conv2D)           (None, 56, 56, 256)       590080

 conv2d_7 (Conv2D)           (None, 56, 56, 256)       590080

 max_pooling2d_2 (MaxPooling  (None, 28, 28, 256)      0
 2D)

 conv2d_8 (Conv2D)           (None, 28, 28, 512)       1180160

 conv2d_9 (Conv2D)           (None, 28, 28, 512)       2359808

 conv2d_10 (Conv2D)          (None, 28, 28, 512)       2359808

 conv2d_11 (Conv2D)          (None, 28, 28, 512)       2359808

 max_pooling2d_3 (MaxPooling  (None, 14, 14, 512)      0
 2D)

 conv2d_12 (Conv2D)          (None, 14, 14, 512)       2359808

 conv2d_13 (Conv2D)          (None, 14, 14, 512)       2359808

 conv2d_14 (Conv2D)          (None, 14, 14, 512)       2359808

 conv2d_15 (Conv2D)          (None, 14, 14, 512)       2359808

 max_pooling2d_4 (MaxPooling  (None, 7, 7, 512)        0
 2D)

 flatten (Flatten)           (None, 25088)             0

 dense (Dense)               (None, 4096)              102764544

 dropout (Dropout)           (None, 4096)              0

 dense_1 (Dense)             (None, 4096)              16781312

 dropout_1 (Dropout)         (None, 4096)              0

 dense_2 (Dense)             (None, 44)                180268

=================================================================
Total params: 139,750,508
Trainable params: 139,750,508
Non-trainable params: 0
2022-09-09 18:56:44.476100: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:798] failed to alloc 1073741824 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2022-09-09 18:56:44.476165: W ./tensorflow/core/common_runtime/device/device_host_allocator.h:46] could not allocate pinned host memory of size: 1073741824
2022-09-09 18:56:44.550245: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:798] failed to alloc 966367744 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2022-09-09 18:56:44.550308: W ./tensorflow/core/common_runtime/device/device_host_allocator.h:46] could not allocate pinned host memory of size: 966367744
2022-09-09 18:56:44.626489: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:798] failed to alloc 869731072 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2022-09-09 18:56:44.626563: W ./tensorflow/core/common_runtime/device/device_host_allocator.h:46] could not allocate pinned host memory of size: 869731072
2022-09-09 18:56:44.704140: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:798] failed to alloc 782758144 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2022-09-09 18:56:44.704206: W ./tensorflow/core/common_runtime/device/device_host_allocator.h:46] could not allocate pinned host memory of size: 782758144
2022-09-09 18:56:44.783625: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:798] failed to alloc 704482304 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2022-09-09 18:56:44.783691: W ./tensorflow/core/common_runtime/device/device_host_allocator.h:46] could not allocate pinned host memory of size: 704482304
2022-09-09 18:56:44.857180: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:798] failed to alloc 634034176 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2022-09-09 18:56:44.857261: W ./tensorflow/core/common_runtime/device/device_host_allocator.h:46] could not allocate pinned host memory of size: 634034176
2022-09-09 18:56:44.955964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:798] failed to alloc 570630912 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2022-09-09 18:56:44.956033: W ./tensorflow/core/common_runtime/device/device_host_allocator.h:46] could not allocate pinned host memory of size: 570630912
2022-09-09 18:56:45.032018: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:798] failed to alloc 513568000 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2022-09-09 18:56:45.032090: W ./tensorflow/core/common_runtime/device/device_host_allocator.h:46] could not allocate pinned host memory of size: 513568000
2022-09-09 18:56:45.106644: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:798] failed to alloc 462211328 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2022-09-09 18:56:45.106711: W ./tensorflow/core/common_runtime/device/device_host_allocator.h:46] could not allocate pinned host memory of size: 462211328
2022-09-09 18:56:45.182697: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:798] failed to alloc 415990272 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2022-09-09 18:56:45.182771: W ./tensorflow/core/common_runtime/device/device_host_allocator.h:46] could not allocate pinned host memory of size: 415990272
2022-09-09 18:56:45.304334: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:798] failed to alloc 1073741824 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2022-09-09 18:56:45.304399: W ./tensorflow/core/common_runtime/device/device_host_allocator.h:46] could not allocate pinned host memory of size: 1073741824
2022-09-09 18:56:55.388728: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:798] failed to alloc 1073741824 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2022-09-09 18:56:55.388791: W ./tensorflow/core/common_runtime/device/device_host_allocator.h:46] could not allocate pinned host memory of size: 1073741824
2022-09-09 18:56:55.482692: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:798] failed to alloc 1073741824 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2022-09-09 18:56:55.482759: W ./tensorflow/core/common_runtime/device/device_host_allocator.h:46] could not allocate pinned host memory of size: 1073741824
2022-09-09 18:56:55.482804: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (gpu_host_bfc) ran out of memory trying to allocate 392.00MiB (rounded to 411041792)requested by op SameWorkerRecvDone
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation.
Current allocation summary follows.
Current allocation summary follows.
2022-09-09 18:56:55.482837: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] BFCAllocator dump for gpu_host_bfc
2022-09-09 18:56:55.482869: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (256):  Total Chunks: 37, Chunks in use: 37. 9.2KiB allocated for chunks. 9.2KiB in use in bin. 752B client-requested in use in bin.
2022-09-09 18:56:55.482902: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (512):  Total Chunks: 3, Chunks in use: 3. 1.5KiB allocated for chunks. 1.5KiB in use in bin. 1.5KiB client-requested in use in bin.
2022-09-09 18:56:55.482933: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1024):         Total Chunks: 2, Chunks in use: 2. 2.0KiB allocated for chunks. 2.0KiB in use in bin. 2.0KiB client-requested in use in bin.
2022-09-09 18:56:55.482963: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2048):         Total Chunks: 12, Chunks in use: 12. 24.0KiB allocated for chunks. 24.0KiB in use in bin. 24.0KiB client-requested in use in bin.
2022-09-09 18:56:55.482992: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4096):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-09-09 18:56:55.483022: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8192):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-09-09 18:56:55.483055: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16384):        Total Chunks: 2, Chunks in use: 2. 32.0KiB allocated for chunks. 32.0KiB in use in bin. 32.0KiB client-requested in use in bin.
2022-09-09 18:56:55.483083: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (32768):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-09-09 18:56:55.483114: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (65536):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-09-09 18:56:55.483144: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (131072):       Total Chunks: 1, Chunks in use: 1. 144.0KiB allocated for chunks. 144.0KiB in use in bin. 144.0KiB client-requested in use in bin.
2022-09-09 18:56:55.483177: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (262144):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-09-09 18:56:55.483207: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (524288):       Total Chunks: 4, Chunks in use: 4. 2.48MiB allocated for chunks. 2.48MiB in use in bin. 2.22MiB client-requested in use in bin.
2022-09-09 18:56:55.483243: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1048576):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-09-09 18:56:55.483272: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2097152):      Total Chunks: 3, Chunks in use: 3. 6.75MiB allocated for chunks. 6.75MiB in use in bin. 6.75MiB client-requested in use in bin.
2022-09-09 18:56:55.483281: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4194304):      Total Chunks: 1, Chunks in use: 1. 4.50MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.
2022-09-09 18:56:55.483306: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8388608):      Total Chunks: 9, Chunks in use: 9. 83.50MiB allocated for chunks. 83.50MiB in use in bin. 81.00MiB client-requested in use in bin.
2022-09-09 18:56:55.483335: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16777216):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-09-09 18:56:55.483363: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (33554432):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-09-09 18:56:55.483375: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (67108864):     Total Chunks: 3, Chunks in use: 3. 192.00MiB allocated for chunks. 192.00MiB in use in bin. 192.00MiB client-requested in use in bin.
2022-09-09 18:56:55.483381: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (134217728):    Total Chunks: 1, Chunks in use: 0. 240.56MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-09-09 18:56:55.483386: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (268435456):    Total Chunks: 1, Chunks in use: 1. 512.00MiB allocated for chunks. 512.00MiB in use in bin. 392.00MiB client-requested in use in bin.
2022-09-09 18:56:55.483411: I tensorflow/core/common_runtime/bfc_allocator.cc:1056] Bin for 392.00MiB was 256.00MiB, Chunk State:
2022-09-09 18:56:55.483439: I tensorflow/core/common_runtime/bfc_allocator.cc:1069] Next region of size 2097152
2022-09-09 18:56:55.483470: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c00000 of size 256 next 1
2022-09-09 18:56:55.483498: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c00100 of size 256 next 2
2022-09-09 18:56:55.483506: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c00200 of size 256 next 3
2022-09-09 18:56:55.483527: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c00300 of size 256 next 4
2022-09-09 18:56:55.483555: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c00400 of size 256 next 7
2022-09-09 18:56:55.483581: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c00500 of size 256 next 8
2022-09-09 18:56:55.483608: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c00600 of size 256 next 9
2022-09-09 18:56:55.483634: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c00700 of size 256 next 10
2022-09-09 18:56:55.483671: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c00800 of size 256 next 11
2022-09-09 18:56:55.483699: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c00900 of size 256 next 12
2022-09-09 18:56:55.483706: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c00a00 of size 256 next 13
2022-09-09 18:56:55.483727: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c00b00 of size 256 next 14
2022-09-09 18:56:55.483755: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c00c00 of size 256 next 15
2022-09-09 18:56:55.483763: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c00d00 of size 256 next 16
2022-09-09 18:56:55.483766: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c00e00 of size 256 next 17
2022-09-09 18:56:55.483768: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c00f00 of size 256 next 18
2022-09-09 18:56:55.483790: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c01000 of size 256 next 19
2022-09-09 18:56:55.483817: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c01100 of size 256 next 20
2022-09-09 18:56:55.483845: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c01200 of size 256 next 21
2022-09-09 18:56:55.483855: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c01300 of size 256 next 22
2022-09-09 18:56:55.483858: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c01400 of size 256 next 23
2022-09-09 18:56:55.483862: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c01500 of size 256 next 24
2022-09-09 18:56:55.483883: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c01600 of size 256 next 25
2022-09-09 18:56:55.483913: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c01700 of size 256 next 26
2022-09-09 18:56:55.483940: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c01800 of size 256 next 27
2022-09-09 18:56:55.483966: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c01900 of size 256 next 28
2022-09-09 18:56:55.483992: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c01a00 of size 256 next 29
2022-09-09 18:56:55.484019: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c01b00 of size 256 next 30
2022-09-09 18:56:55.484047: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c01c00 of size 256 next 31
2022-09-09 18:56:55.484074: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c01d00 of size 256 next 32
2022-09-09 18:56:55.484083: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c01e00 of size 256 next 48
2022-09-09 18:56:55.484106: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c01f00 of size 256 next 38
2022-09-09 18:56:55.484158: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c02000 of size 256 next 44
2022-09-09 18:56:55.484173: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c02100 of size 256 next 35
2022-09-09 18:56:55.484176: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c02200 of size 2048 next 51
2022-09-09 18:56:55.484201: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c02a00 of size 1024 next 43
2022-09-09 18:56:55.484209: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c02e00 of size 2048 next 45
2022-09-09 18:56:55.484232: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c03600 of size 512 next 33
2022-09-09 18:56:55.484243: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c03800 of size 2048 next 41
2022-09-09 18:56:55.484246: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c04000 of size 512 next 37
2022-09-09 18:56:55.484270: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c04200 of size 589824 next 40
2022-09-09 18:56:55.484298: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c94200 of size 16384 next 50
2022-09-09 18:56:55.484327: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c98200 of size 2048 next 34
2022-09-09 18:56:55.484354: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205c98a00 of size 720896 next 39
2022-09-09 18:56:55.484382: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205d48a00 of size 2048 next 52
2022-09-09 18:56:55.484409: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205d49200 of size 2048 next 53
2022-09-09 18:56:55.484436: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205d49a00 of size 256 next 57
2022-09-09 18:56:55.484466: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205d49b00 of size 2048 next 59
2022-09-09 18:56:55.484492: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205d4a300 of size 2048 next 60
2022-09-09 18:56:55.484520: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205d4ab00 of size 2048 next 63
2022-09-09 18:56:55.484546: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205d4b300 of size 2048 next 65
2022-09-09 18:56:55.484572: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205d4bb00 of size 256 next 66
2022-09-09 18:56:55.484580: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205d4bc00 of size 147456 next 67
2022-09-09 18:56:55.484604: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205d6fc00 of size 2048 next 70
2022-09-09 18:56:55.484616: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205d70400 of size 16384 next 71
2022-09-09 18:56:55.484619: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205d74400 of size 572416 next 18446744073709551615
2022-09-09 18:56:55.484645: I tensorflow/core/common_runtime/bfc_allocator.cc:1069] Next region of size 16777216
2022-09-09 18:56:55.484682: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 205e00000 of size 2359296 next 42
2022-09-09 18:56:55.484690: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 206040000 of size 2359296 next 36
2022-09-09 18:56:55.484694: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 206280000 of size 12058624 next 18446744073709551615
2022-09-09 18:56:55.484697: I tensorflow/core/common_runtime/bfc_allocator.cc:1069] Next region of size 536870912
2022-09-09 18:56:55.484700: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 206e00000 of size 67108864 next 49
2022-09-09 18:56:55.484703: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 20ae00000 of size 4718592 next 46
2022-09-09 18:56:55.484707: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 20b280000 of size 9437184 next 47
2022-09-09 18:56:55.484732: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 20bb80000 of size 67108864 next 54
2022-09-09 18:56:55.484761: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 20fb80000 of size 9437184 next 55
2022-09-09 18:56:55.484791: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 210480000 of size 9437184 next 56
2022-09-09 18:56:55.484819: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 210d80000 of size 9437184 next 58
2022-09-09 18:56:55.484829: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 211680000 of size 9437184 next 61
2022-09-09 18:56:55.484833: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 211f80000 of size 9437184 next 62
2022-09-09 18:56:55.484856: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 212880000 of size 67108864 next 68
2022-09-09 18:56:55.484882: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 216880000 of size 9437184 next 69
2022-09-09 18:56:55.484910: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 217180000 of size 2359296 next 72
2022-09-09 18:56:55.484938: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 2173c0000 of size 720896 next 73
2022-09-09 18:56:55.484963: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 217470000 of size 2048 next 74
2022-09-09 18:56:55.484991: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 217470800 of size 9437184 next 75
2022-09-09 18:56:55.485019: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 217d70800 of size 256 next 76
2022-09-09 18:56:55.485046: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 217d70900 of size 512 next 77
2022-09-09 18:56:55.485075: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 217d70b00 of size 1024 next 78
2022-09-09 18:56:55.485101: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 217d70f00 of size 252244224 next 18446744073709551615
2022-09-09 18:56:55.485128: I tensorflow/core/common_runtime/bfc_allocator.cc:1069] Next region of size 536870912
2022-09-09 18:56:55.485138: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 227000000 of size 536870912 next 18446744073709551615
2022-09-09 18:56:55.485159: I tensorflow/core/common_runtime/bfc_allocator.cc:1094]      Summary of in-use Chunks by size:
2022-09-09 18:56:55.485188: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 37 Chunks of size 256 totalling 9.2KiB
2022-09-09 18:56:55.485217: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 3 Chunks of size 512 totalling 1.5KiB
2022-09-09 18:56:55.485260: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 1024 totalling 2.0KiB
2022-09-09 18:56:55.485290: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 12 Chunks of size 2048 totalling 24.0KiB
2022-09-09 18:56:55.485319: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 16384 totalling 32.0KiB
2022-09-09 18:56:55.485349: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 147456 totalling 144.0KiB
2022-09-09 18:56:55.485378: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 572416 totalling 559.0KiB
2022-09-09 18:56:55.485407: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 589824 totalling 576.0KiB
2022-09-09 18:56:55.485436: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 720896 totalling 1.38MiB
2022-09-09 18:56:55.485462: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 3 Chunks of size 2359296 totalling 6.75MiB
2022-09-09 18:56:55.485489: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 4718592 totalling 4.50MiB
2022-09-09 18:56:55.485517: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 8 Chunks of size 9437184 totalling 72.00MiB
2022-09-09 18:56:55.485546: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 12058624 totalling 11.50MiB
2022-09-09 18:56:55.485574: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 3 Chunks of size 67108864 totalling 192.00MiB
2022-09-09 18:56:55.485603: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 536870912 totalling 512.00MiB
2022-09-09 18:56:55.485632: I tensorflow/core/common_runtime/bfc_allocator.cc:1101] Sum Total of in-use chunks: 801.44MiB
2022-09-09 18:56:55.485663: I tensorflow/core/common_runtime/bfc_allocator.cc:1103] total_region_allocated_bytes_: 1092616192 memory_limit_: 68719476736 available bytes: 67626860544 curr_region_allocation_bytes_: 1073741824
2022-09-09 18:56:55.485695: I tensorflow/core/common_runtime/bfc_allocator.cc:1109] Stats:
Limit:                     68719476736
InUse:                       840371968
MaxInUse:                    840371968
NumAllocs:                       54045
MaxAllocSize:                536870912
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-09-09 18:56:55.485727: W tensorflow/core/common_runtime/bfc_allocator.cc:491] ****************************______________________***************************************xxxxxxxxxxx
Traceback (most recent call last):
  File ""/anaconda/lib/python3.9/site-packages/keras/callbacks.py"", line 1557, in _save_model
    self.model.save_weights(
  File ""/anaconda/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/anaconda/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1397, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

2 root error(s) found.
  (0) RESOURCE_EXHAUSTED: SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:GPU:0;0000000000000001;/job:localhost/replica:0/task:0/device:CPU:0;edge_164_dense/kernel/Read/ReadVariableOp;0:0
         [[{{node dense/kernel/Read/ReadVariableOp/_409}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

         [[training/Adam/decay/Read/ReadVariableOp/_426]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

  (1) RESOURCE_EXHAUSTED: SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:GPU:0;0000000000000001;/job:localhost/replica:0/task:0/device:CPU:0;edge_164_dense/kernel/Read/ReadVariableOp;0:0
         [[{{node dense/kernel/Read/ReadVariableOp/_409}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

0 successful operations.
0 derived errors ignored.
```
</details>"
57658,XNNPack Error configuring CMake to build an installable package ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

latest master (commit 9906c67e30c2def772d2a9057c98265d904d8b97)

### Custom Code

No

### OS Platform and Distribution

MacOS 12.4

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

Apple clang version 13.1.6 (clang-1316.0.21.2.5)
Target: arm64-apple-darwin21.5.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When configuring a build to produce an installable package per the instructions (https://www.tensorflow.org/lite/guide/build_cmake#build_installable_package), I get the following error: 


CMake Error: install(EXPORT ""tensorflow-liteTargets"" ...) includes target ""tensorflow-lite"" which requires target ""XNNPACK"" that is not in any export set.
```
```


### Standalone code to reproduce the issue

```shell
This is my command to generate my build files that produces the error I pasted above: 

cmake -GNinja -S ./third_party/tensorflow/tensorflow/lite -B ./deps/build/tensorflow \
  -DTFLITE_ENABLE_INSTALL:BOOL=ON \
  -DCMAKE_FIND_PACKAGE_PREFER_CONFIG:BOOL=ON \
  -DCMAKE_INSTALL_PREFIX:PATH=./deps/install/tensorflow \
  -Dcpuinfo_DIR:PATH=./deps/install/cpuinfo/share/cpuinfo \
  -Druy_DIR:PATH=./deps/install/ruy/lib/cmake/ruy \
  -Dabsl_DIR:PATH=./deps/install/abseil-cpp/lib/cmake/absl \
  -DEigen3_DIR:PATH=./deps/install/eigen/share/eigen3/cmake \
  -DNEON_2_SSE_DIR:PATH=./deps/install/ARM_NEON_2_x86_SSE/lib/cmake/NEON_2_SSE \
  -DFlatbuffers_DIR:PATH=./deps/install/flatbuffers/lib/cmake/flatbuffers
```
All the dependencies (cpuinfo, ruy, absl, eigen, NEON_2_SSE, Flatbuffers) are built from source and installed at the paths specified. 

If I add `-DTFLITE_ENABLE_XNNPACK:BOOL=OFF` then then everything works.
```


### Relevant log output

_No response_</details>"
57657,mkj,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
57652,How to disable eager mode?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04 in WSL2

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


Is there a way to disable the eager mode globally in TensorFLow 2? I do not want to use the following command:
```
""tf.compat.v1.disable_eager_execution()""
```
as my trained keras model is kind of corrupted after reload from cache in Streamlit (see #57645 report)


### Standalone code to reproduce the issue

```
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import Input
from tensorflow.keras.initializers import RandomNormal
from tensorflow.keras.optimizers import Adam

from datetime import datetime

import streamlit as st

# Do not use GPU
tf.config.set_visible_devices([], ""GPU"")
tf.compat.v1.disable_eager_execution()  # adding this line make keras training much slower

# Generate data
def f(x):
    return (x + 1) * np.sin(5 * x)


x_plot = np.arange(-1, 1 + 0.001, 0.001)
y_plot = f(x_plot)

x_train = np.arange(-1 + 0.05, 1, 0.2)
y_train = f(x_train)

x_val = np.arange(-1 + 0.15, 1, 0.2)
y_val = f(x_val)

# Plot the problem
plt.figure()
plt.plot(x_plot, y_plot, ""-"", label=""Orgininal function"")
plt.plot(x_train, y_train, ""o"", label=""Training points"")
plt.plot(x_val, y_val, ""s"", label=""Validation points"")
plt.xlim(-1, 1)
plt.ylim(-2, 2)
plt.xlabel(""x"")
plt.ylabel(""f"")
plt.grid()
plt.legend()
plt.show(block=False)

# Reshape
X_train = x_train.reshape(x_train.shape[0], 1)
Y_train = y_train.reshape(x_train.shape[0], 1)

X_val = x_val.reshape(x_val.shape[0], 1)
Y_val = y_val.reshape(x_val.shape[0], 1)

# Simple model
start_time = datetime.now()


@st.experimental_singleton
def train():
    tf.keras.utils.set_random_seed(1)
    model = Sequential()
    model.add(Input(shape=(1,)))  # Input layer
    model.add(
        Dense(
            4,
            activation=""sigmoid"",
            kernel_initializer=RandomNormal(mean=0.0, stddev=1.0, seed=1),
        )
    )
    model.add(Dense(1))

    model.compile(
        loss=""mean_squared_error"", optimizer=Adam(learning_rate=3e-1), run_eagerly=False
    )
    history = model.fit(
        X_train,
        Y_train,
        validation_split=0.0,
        validation_data=(X_val, Y_val),
        validation_freq=1,
        batch_size=X_train.shape[0],
        epochs=2000,
        verbose=0,
    )
    return model


run_time = datetime.now() - start_time

model2 = train()

print(""Training time : {:.4f} s"".format(run_time.total_seconds()))

st.write(""Please rerun the app to see the error"")

model2.predict(X_val)
```

### Relevant log output

_No response_</details>"
57650,Updating TFLite ConvTranspose to make it support dynamic inputs.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubunutu 21.02
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):  tensorflow 2.9.1

**Standalone code to reproduce the issue** 
Converting any ConvTranspose layer from ONNX to TensorflowLite.

**Any other info / logs**
The ONNX does not have requirement for output_shape argument for ConvTranspose layer.
[https://github.com/onnx/onnx/blob/main/docs/Operators.md#ConvTranspose]()
since that can be calculated. 

But in tflite the ConvTranspose layer requires mandatory requirement for output_shape.
[https://github.com/tensorflow/tensorflow/blob/18960c44ad3f5219c22dca55f842912dbce78a07/tensorflow/lite/kernels/transpose_conv.cc#L251-L253]()

So because of this the dynamic reshape happens in wrong way in tflite ConvTranpose, but for ONNX it can be done.
Is it possible to remove the output_shape tensor requirement in tflite and similarly be updated in tflite convertor? or Any other workaround will be appreciated.

"
57649,--tf_xla_persistent_cache_directory get segmentation fault in CpuCompiler,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.10

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.6

### Bazel version

5.3.0

### GCC/Compiler version

GCC 12.2

### CUDA/cuDNN version

none

### GPU model and memory

none

### Current Behaviour?

I turn on --tf_xla_persistent_cache_directory on a CPU version build.
It get Segmentation fault in `CpuCompiler::CompileAheadOfTime`

gdb bt:

```
#0  __memchr_avx2 () at ../sysdeps/x86_64/multiarch/memchr-avx2.S:65
#1  0x00007ffff2485eae in llvm::StringRef::split(llvm::SmallVectorImpl<llvm::StringRef>&, char, int, bool) const () from /home/xxx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007ffff24a9455 in llvm::Triple::normalize[abi:cxx11](llvm::StringRef) () from /home/xxx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007fffe2385dfe in xla::cpu::CpuCompiler::CompileAheadOfTime(std::unique_ptr<xla::HloModuleGroup, std::default_delete<xla::HloModuleGroup> >, xla::AotCompilationOptions const&) ()
   from /home/xxx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007fffe6e3ba7e in xla::Service::BuildAotResults(std::vector<xla::HloModuleProto const*, std::allocator<xla::HloModuleProto const*> > const&, std::vector<std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, std::allocator<std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> > > >, xla::Backend*, std::vector<std::vector<stream_executor::StreamExecutor*, std::allocator<stream_executor::StreamExecutor*> >, std::allocator<std::vector<stream_executor::StreamExecutor*, std::allocator<stream_executor::StreamExecutor*> > > >, xla::Compiler::CompileOptions const&, bool) ()
   from /home/xxx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007fffe6e2d3fc in xla::LocalService::CompileAotResults(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&) ()
   from /home/xxx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007fffe6e111e4 in xla::LocalClient::CompileAheadOfTime(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&) ()
   from /home/xxx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007fffe566f3da in tensorflow::XlaCompilationCache::BuildSerializedExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&) ()
   from /home/xxx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007fffe5675f57 in tensorflow::XlaCompilationCache::SerializeEntry(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationCache::Signature const&, tensorflow::XlaCompilationCache::Entry const&) ()
   from /home/xxx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007fffe5677a9e in tensorflow::XlaCompilationCache::CompileStrict(tensorflow::XlaCompilationCache::Signature const&, tensorflow::XlaCompilationCache::Entry*, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::OpKernelContext*, tensorflow::XlaCompilationCache::CompileScope) ()
   from /home/xxx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007fffe567ab23 in tensorflow::XlaCompilationCache::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::OpKernelContext*, tensorflow::XlaCompilationCache::CompileScope, tensorflow::XlaCompilationCache::CompileMode, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**) () from /home/xxx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007fffe567bb81 in tensorflow::XlaCompilationCache::Compile(tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompilationCache::CompileMode, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**) ()
   from /home/xxx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#12 0x00007fffe464cf67 in tensorflow::CompileToLocalExecutable(tensorflow::OpKernelContext*, tensorflow::NameAttrList const&, bool, tensorflow::XlaPlatformInfo const&, absl::lts_20220623::Span<tensorflow::Tensor const* const>, absl::lts_20220623::Span<tensorflow::VariableInfo const>, absl::lts_20220623::Span<int const>, tensorflow::XlaCompilationCache::CompileMode, bool, xla::LocalClient**, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**) ()
   from /home/xxx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#13 0x00007fffe465214e in tensorflow::XlaCompileOp::Compute(tensorflow::OpKernelContext*) () from /home/xxx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#14 0x00007fffdc88e24b in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) () from /home/xxx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#15 0x00007fffdc7ddad7 in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::PropagatorState>::Process(tensorflow::PropagatorState::TaggedNode, long) ()
   from /home/xxx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#16 0x00007fffdc7c405b in std::_Function_handler<void (), void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::PropagatorState>::RunTask<std::_Bind<void (tensorflow::(anonymous namespace)::ExecutorState<tensorflow::PropagatorState>::*(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::PropagatorState>*, tensorflow::PropagatorState::TaggedNode, long))(tensorflow::PropagatorState::TaggedNode, long)> >(std::_Bind<void (tensorflow::(anonymous namespace)::ExecutorState<tensorflow::PropagatorState>::*(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::PropagatorState>*, tensorflow::PropagatorState::TaggedNode, long))(tensorflow::PropagatorState::TaggedNode, long)>&&)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /home/xxx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#17 0x00007ffff21e2ff4 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) () from /home/xxx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#18 0x00007ffff21dfa53 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /home/xxx/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#19 0x00007fffdc5d6cda in tensorflow::(anonymous namespace)::PThread::ThreadFn(void*) () from /home/xxx/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#20 0x00007ffff7bbb6db in start_thread (arg=0x7fff98979700) at pthread_create.c:463
#21 0x00007ffff78e461f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95
```

I read the code CpuCompiler::CompileAheadOfTime, 
and found it convert AotCompilationOptions& aot_options to CpuAotCompilationOptions, [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/cpu_compiler.cc#L1372)
but the input aot_options is type AotCompilationOptions, which is added in d28f9ca1fc4d.
```


### Standalone code to reproduce the issue

I think that is the reason, so I didn't add standalone code to reproduce the issue.


### Relevant log output

_No response_</details>"
57647,Request: webp support,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

No

### OS Platform and Distribution

Archlinux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The .webp image format is not available for tf.keras.preprocessing.image.ImageDataGenerator().flow_from_directory()

The use of the .webp format is at least desirable, due to their lower size and better compression over .jpeg and the fact it can be loseless like .png while keeping the lower file size
```


### Standalone code to reproduce the issue

```shell
For what i see there is no support currently for .webp, not even in the io module. (I wrote this part here because it ask for something even if I am just making a feature request)
```


### Relevant log output

_No response_</details>"
57645,"Keras model training is slow without ""tf.compat.v1.disable_eager_execution()""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04 in WSL2

### Mobile device

_No response_

### Python version

3.8.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In documentation, keras.model.fit() runs in graph mode by default, even if eager mode is by default in TF2.x. So I expect that training a simple keras model (13 parameters) should be fast. But it is very slow on my computer (~30s). However, it will be 10 times faster (~3s) if I add this line in the code: tf.compat.v1.disable_eager_execution()
It seems that keras fit function do not run in graph mode(?). Adding ""run_eagerly=False"" option while compiling the model do not solve the problem. Is this a bug ?
As a workaround, is there another way to disable eager mode globally in TF2 ? The command ""tf.compat.v1.disable_eager_execution()"" works but changes something in TF behavior behind the scences. In my case, when using this command, I cannot use predict function of my trained model which was cached in the Streamlit framework (using @st.cache(allow_output_mutation=True) or @st.experimental_singleton). The following error arises:
""InvalidArgumentError: Tensor input_1:0, specified in either feed_devices or fetch_devices was not found in the Graph"" (traceback in log output)
This does not happen when disable_eager_execution() is not used.
Thank you!
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import Input
from tensorflow.keras.initializers import RandomNormal
from tensorflow.keras.optimizers import Adam

from datetime import datetime

# Do not use GPU
tf.config.set_visible_devices([], ""GPU"")
# tf.compat.v1.disable_eager_execution()  # adding this line make keras training much faster

# Generate data
def f(x):
    return (x + 1) * np.sin(5 * x)


x_plot = np.arange(-1, 1 + 0.001, 0.001)
y_plot = f(x_plot)

x_train = np.arange(-1 + 0.05, 1, 0.2)
y_train = f(x_train)

x_val = np.arange(-1 + 0.15, 1, 0.2)
y_val = f(x_val)

# Plot the problem
plt.figure()
plt.plot(x_plot, y_plot, ""-"", label=""Orgininal function"")
plt.plot(x_train, y_train, ""o"", label=""Training points"")
plt.plot(x_val, y_val, ""s"", label=""Validation points"")
plt.xlim(-1, 1)
plt.ylim(-2, 2)
plt.xlabel(""x"")
plt.ylabel(""f"")
plt.grid()
plt.legend()
plt.show(block=False)

# Reshape
X_train = x_train.reshape(x_train.shape[0], 1)
Y_train = y_train.reshape(x_train.shape[0], 1)

X_val = x_val.reshape(x_val.shape[0], 1)
Y_val = y_val.reshape(x_val.shape[0], 1)

# Simple model
tf.keras.utils.set_random_seed(1)
model = Sequential()
model.add(Input(shape=(1,)))  # Input layer
model.add(
    Dense(
        4,
        activation=""sigmoid"",
        kernel_initializer=RandomNormal(mean=0.0, stddev=1.0, seed=1),
    )
)
model.add(Dense(1))

model.compile(loss=""mean_squared_error"", optimizer=Adam(learning_rate=3e-1))

start_time = datetime.now()
history = model.fit(
    X_train,
    Y_train,
    validation_split=0.0,
    validation_data=(X_val, Y_val),
    validation_freq=1,
    batch_size=X_train.shape[0],
    epochs=2000,
    verbose=0,
)

run_time = datetime.now() - start_time
print(""Training time : {:.4f} s"".format(run_time.total_seconds()))
```


### Relevant log output

```shell
InvalidArgumentError: Tensor input_1:0, specified in either feed_devices or fetch_devices was not found in the Graph
Traceback:
File ""/home/vinh/miniconda3/envs/data/lib/python3.8/site-packages/streamlit/scriptrunner/script_runner.py"", line 557, in _run_script
    exec(code, module.__dict__)
File ""dashboard.py"", line 227, in <module>
    Y_train_pred = model.predict(X_train)
File ""/home/vinh/miniconda3/envs/data/lib/python3.8/site-packages/keras/engine/training_v1.py"", line 969, in predict
    return func.predict(
File ""/home/vinh/miniconda3/envs/data/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py"", line 700, in predict
    return predict_loop(
File ""/home/vinh/miniconda3/envs/data/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py"", line 377, in model_iteration
    batch_outs = f(ins_batch)
File ""/home/vinh/miniconda3/envs/data/lib/python3.8/site-packages/keras/backend.py"", line 4282, in __call__
    self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)
File ""/home/vinh/miniconda3/envs/data/lib/python3.8/site-packages/keras/backend.py"", line 4218, in _make_callable
    callable_fn = session._make_callable_from_options(callable_opts)
File ""/home/vinh/miniconda3/envs/data/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1513, in _make_callable_from_options
    return BaseSession._Callable(self, callable_options)
File ""/home/vinh/miniconda3/envs/data/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1471, in __init__
    self._handle = tf_session.TF_SessionMakeCallable(
```
</details>"
57643,Is it possible to use both cameras (FRONT AND BACK) with CameraX for object detection TFLITE *NO HARDWARE LIMITATION*,"As the title says i tried to implement an app that with the front detects the driver face and with the back detects cars. I modified the demo object detection for tensorflow lite inrtroducing a new camera fragment and adding the permissions to the new camerabinding. Only 1 camera appears to works, getting in the LOGS that the maximum of Cameras allowed to be open are 1. 

Ive seen apps with camera2  API working both cameras at the same time, so i have the next question:

1. is it possible to use camera2 API with tensorflow lite?
2. is it possible to use 2 cameras simultaniously  with CameraX api?

Thanks in advance.
"
57642,How can I save model with input/output signatures with TF2?,"How can I save model with input/output signatures for using model in TF serving like I used to saved in V1?

```
builder = tf.saved_model.builder.SavedModelBuilder(export_path)

model_input = tf.saved_model.utils.build_tensor_info(model.input)
model_output = tf.saved_model.utils.build_tensor_info(model.output)

prediction_signature = (
    tf.saved_model.signature_def_utils.build_signature_def(
        inputs={'inputs': model_input},
        outputs={'output': model_output},
        method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))

with K.get_session() as sess:
    builder.add_meta_graph_and_variables(
        sess=sess, tags=[tf.saved_model.tag_constants.SERVING],
        signature_def_map={
            'predict':
                prediction_signature,
        })

    builder.save()
```

I see that in TF2 I need use tf.function but I can't understand how create exactly same save signatures that I used in TF1

I can't use way with `tf.compat.v1.` cause TF serving wont work due to my previous issue
https://github.com/tensorflow/tensorflow/issues/56698

System information
python 3.9
tensorflow-serving-api==2.8.0
tensorflow==2.8.0

"
57641,Partial GPU Logical Device configurations fail on TF 2.10 ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.10

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
This is an issue to document a known issue with TensorFlow 2.10.

When the system is equipped with 2 or more GPUs, the following code snippet fails with TensorFlow 2.10:


gpus = tf.config.list_physical_devices('GPU')

# Error triggered if logical device configuration is only set 
# for gpus[0], not gpus[1].
tf.config.set_logical_device_configuration(gpus[0], [
    tf.config.LogicalDeviceConfiguration(memory_limit=100),
    tf.config.LogicalDeviceConfiguration(memory_limit=100)
    ])

# Fails
tf.config.list_logical_devices('GPU')

# Raises Error: tensorflow.python.framework.errors_impl.InvalidArgumentError: 
# Device ordinals must be set for all virtual devices or none. But the device_ordinal is specified for 1 while previous devices didn't have any set.
```

2 known workarounds:

- The user can hide the other physical GPUs from TensorFlow with `tf.config.set_visible_devices()`.
- The user can set Logical device configuration to all physical GPUs.

A fix will be included in the next release of TensorFlow.

Reference:
The bug was discovered during TensorFlow Addons 2.10 release. [Discussion Thread](
https://github.com/tensorflow/tensorflow/commit/6c6fe3d2f55d90ef59447c378ff7970559a5d3a6#r83220055)
```


### Standalone code to reproduce the issue

```shell
Refer to above.
```


### Relevant log output

_No response_</details>"
57637,code_check_full.bats failure on aarch64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.8.10

### Bazel version

5.1.1

### GCC/Compiler version

9.3.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
The code_check_full.bats test reports 3 failures
```


### Standalone code to reproduce the issue

```shell
docker exec tf bats /usertools/code_check_full.bats --timing --formatter junit
```


### Relevant log output

```shell
<?xml version=""1.0"" encoding=""UTF-8""?>
<testsuites time=""224.574"">
<testsuite name=""code_check_full.bats"" tests=""9"" failures=""3"" errors=""0"" skipped=""0"" time=""224.574"" timestamp=""2022-09-07T15:04:32"" hostname=""469027ae5b51"">
    <testcase classname=""code_check_full.bats"" name=""Pip package generated license includes all dependencies&#39; licenses"" time=""9.365"">
        <failure type=""failure"">(from function `do_external_licenses_check&#39; in file /usertools/code_check_full.bats, line 82,
 in test file /usertools/code_check_full.bats, line 87)
  `&quot;//tensorflow/tools/pip_package:build_pip_package&quot; \&#39; failed
Please remove the following extra licenses from //tensorflow/tools/pip_package:licenses:
@arm_neon_2_x86_sse//
@nasm//</failure>
    </testcase>
    <testcase classname=""code_check_full.bats"" name=""Libtensorflow generated license includes all dependencies&#39; licenses"" time=""1.171"">
        <failure type=""failure"">(from function `do_external_licenses_check&#39; in file /usertools/code_check_full.bats, line 82,
 in test file /usertools/code_check_full.bats, line 93)
  `&quot;//tensorflow:libtensorflow.so&quot; \&#39; failed
Please remove the following extra licenses from //tensorflow/tools/lib_package:clicenses_generate:
@nasm//</failure>
    </testcase>
    <testcase classname=""code_check_full.bats"" name=""Java library generated license includes all dependencies&#39; licenses"" time=""1.224"">
        <failure type=""failure"">(from function `do_external_licenses_check&#39; in file /usertools/code_check_full.bats, line 82,
 in test file /usertools/code_check_full.bats, line 99)
  `&quot;//tensorflow/java:libtensorflow_jni.so&quot; \&#39; failed
Please remove the following extra licenses from //tensorflow/tools/lib_package:jnilicenses_generate:
@nasm//</failure>
    </testcase>
    <testcase classname=""code_check_full.bats"" name=""Pip package includes all required //tensorflow dependencies"" time=""43.200"" />
    <testcase classname=""code_check_full.bats"" name=""Pip package doesn&#39;t depend on CUDA"" time=""157.946"" />
    <testcase classname=""code_check_full.bats"" name=""Pip package doesn&#39;t depend on CUDA for static builds (i.e. Windows)"" time=""2.179"" />
    <testcase classname=""code_check_full.bats"" name=""All tensorflow.org/code links point to real files"" time=""1.413"" />
    <testcase classname=""code_check_full.bats"" name=""No duplicate files on Windows"" time=""0.105"" />
    <testcase classname=""code_check_full.bats"" name=""bazel nobuild passes on all of TF except TF Lite and win toolchains"" time=""7.971"" />

</testsuite>
</testsuites>
```
</details>"
57636,"model.predict(train_ds = tf.data.dataset) messed dataset order, couldn't match labels sequence?","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

binary

### Tensorflow Version

2.8.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
model.predict(train_ds = tf.data.dataset) messed dataset order, couldn't match labels sequence?

The dataset is from a generator, I just wanted to extract features and save the features with their corresponding labels (x_features, y)
```


### Standalone code to reproduce the issue

```shell
test_ds = test_ds.shuffle(1, reshuffle_each_iteration=False).repeat(20).batch(batch_size=batch_size).prefetch(tf.data.AUTOTUNE)

test_features = model.predict(test_ds)

for im, labels in test_ds.take(1):
    test_batch1_feats = model.predict(im)
    print(labels)

test_features[:batch_size] == test_batch1_feats

test_features2 = model.predict(test_ds)
test_features == test_features2

Sadly both are False.. How could I match the features extracted with their corresponding labels?

Thank you very much!!
```


### Relevant log output

_No response_</details>"
57635,When Building Sequential model tensorflow gives multiple opKernel registrations,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.10.0

### Custom Code

No

### OS Platform and Distribution

Windows 11 Pro

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuda_11.7.r11.7

### GPU model and memory

RTX a4500 20GB

### Current Behaviour?

```shell
Trying to build this sequential model, but tensorflow gives me this error (installed with pip both tensorflow and tensorflow-gpu)
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus: 
    tf.config.experimental.set_memory_growth(gpu, True)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

test = Sequential([Dense(100)])
```


### Relevant log output

```shell
InvalidArgumentError                      Traceback (most recent call last)
d:\dev\projects\testGAN\Test.ipynb Cell 3 in <cell line: 1>()
----> 1 test = Sequential([Dense(100)])

File d:\Programs\Python\Python310\lib\site-packages\tensorflow\python\trackable\base.py:205, in no_automatic_dependency_tracking.<locals>._method_wrapper(self, *args, **kwargs)
    203 self._self_setattr_tracking = False  # pylint: disable=protected-access
    204 try:
--> 205   result = method(self, *args, **kwargs)
    206 finally:
    207   self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

File d:\Programs\Python\Python310\lib\site-packages\keras\utils\traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File d:\Programs\Python\Python310\lib\site-packages\tensorflow\python\framework\ops.py:7209, in raise_from_not_ok_status(e, name)
   7207 def raise_from_not_ok_status(e, name):
   7208   e.message += ("" name: "" + name if name is not None else """")
-> 7209   raise core._status_to_exception(e) from None

InvalidArgumentError: Multiple OpKernel registrations match NodeDef at the same priority '{{node AssignVariableOp}}': 'op: ""AssignVariableOp"" device_type: ""GPU"" constraint { name: ""dtype"" allowed_values { list { type: DT_INT64 } } } host_memory_arg: ""resource""' and 'op: ""AssignVariableOp"" device_type: ""GPU"" constraint { name: ""dtype"" allowed_values { list { type: DT_INT64 } } } host_memory_arg: ""resource""'
	 [[AssignVariableOp]] [Op:AssignVariableOp]
```
</details>"
57633,Can't excute ConvNeXt with input_tensor,"When tf.keras.applications.ConvNeXt is executed with input_tensor, it is not executed.
As you can see from [issue](https://github.com/keras-team/tf-keras/issues/437), it seems to be a problem inside the keras code.
Once the keras issue is resolved, I hope to apply it as soon as possible.
This article is just to inform you that there was such a problem.
Thanks."
57632,Illegal instruction (core dumped) in PYNQ-Z7,"### Tensorflow Version

- 2.5

### OS Platform and Distribution

- Linux in Xilinx


### Current Behaviour?

- In Jetson Nano and other edge devices, it was solved with the following command.

```shell
export OPENBLAS_CORETYPE=ARMV8 python
```
- But I don't know the solution after checking it on PYNQ.


### Relevant log output

```shell
(env) xilinx@pynq:~/glory/220902$ cat /proc/cpuinfo
processor       : 0
model name      : ARMv7 Processor rev 0 (v7l)
BogoMIPS        : 650.00
Features        : half thumb fastmult vfp edsp neon vfpv3 tls vfpd32 
CPU implementer : 0x41
CPU architecture: 7
CPU variant     : 0x3
CPU part        : 0xc09
CPU revision    : 0

processor       : 1
model name      : ARMv7 Processor rev 0 (v7l)
BogoMIPS        : 650.00
Features        : half thumb fastmult vfp edsp neon vfpv3 tls vfpd32 
CPU implementer : 0x41
CPU architecture: 7
CPU variant     : 0x3
CPU part        : 0xc09
CPU revision    : 0

Hardware        : Xilinx Zynq Platform
Revision        : 0003
Serial          : 0000000000000000
(env) xilinx@pynq:~/glory/220902$ export OPENBLAS_CORETYPE=ARMV7 python
(env) xilinx@pynq:~/glory/220902$ python3.7 3.py 
Training these words: snoring,no_snoring
Training steps in each stage: 25,25
Learning rate in each stage: 0.005,0.005
Total number of training steps: 50
Illegal instruction (core dumped)
```
</details>"
57631,Build error: Exec failed due to IOException: xcrun failed with code 1.,"### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

macOS 12.5.1 (Apple M1 Pro)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

5.1.1

### GCC/Compiler version

Apple Clang 13.1.6

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?

I'm unable to install TF 2.10.0 on macOS 12.5.1. I see the following errors during build:
```
ERROR: /private/var/folders/j1/68dlgpr91vlgs26vty2c8xk80000gn/T/spackynvkrim2/02aadb0d9aff009e669c11c7332b5f49/external/llvm-project/llvm/BUILD.bazel:172:11: Compiling llvm/lib/Support/AArch64TargetParser.cpp failed: Exec failed due to IOException: xcrun failed with code 1.
This most likely indicates that SDK version [12.5.1] for platform [MacOSX] is unsupported for the target version of xcode.
Process exited with status 1
stdout: stderr: 2022-09-06 20:02:19.765 xcodebuild[25302:641207] Requested but did not find extension point with identifier Xcode.IDEKit.ExtensionSentinelHostApplications for extension Xcode.DebuggerFoundation.AppExtensionHosts.watchOS of plug-in com.apple.dt.IDEWatchSupportCore
2022-09-06 20:02:19.765 xcodebuild[25302:641207] Requested but did not find extension point with identifier Xcode.IDEKit.ExtensionPointIdentifierToBundleIdentifier for extension Xcode.DebuggerFoundation.AppExtensionToBundleIdentifierMap.watchOS of plug-in com.apple.dt.IDEWatchSupportCore
xcodebuild: error: SDK ""macosx12.5.1"" cannot be located.
2022-09-06 20:02:20.133 xcodebuild[25305:641233] Requested but did not find extension point with identifier Xcode.IDEKit.ExtensionSentinelHostApplications for extension Xcode.DebuggerFoundation.AppExtensionHosts.watchOS of plug-in com.apple.dt.IDEWatchSupportCore
2022-09-06 20:02:20.133 xcodebuild[25305:641233] Requested but did not find extension point with identifier Xcode.IDEKit.ExtensionPointIdentifierToBundleIdentifier for extension Xcode.DebuggerFoundation.AppExtensionToBundleIdentifierMap.watchOS of plug-in com.apple.dt.IDEWatchSupportCore
xcodebuild: error: SDK ""macosx12.5.1"" cannot be located.
xcrun: error: unable to lookup item 'Path' in SDK 'macosx12.5.1'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /private/var/folders/j1/68dlgpr91vlgs26vty2c8xk80000gn/T/ajstewart/spack-stage/spack-stage-py-tensorflow-2.10.0-d66xhjmq75ezf46u2u6vid4sjarpijbo/spack-src/tensorflow/lite/python/BUILD:68:10 Middleman _middlemen/tensorflow_Slite_Spython_Stflite_Uconvert-runfiles failed: Exec failed due to IOException: xcrun failed with code 1.
This most likely indicates that SDK version [12.5.1] for platform [MacOSX] is unsupported for the target version of xcode.
Process exited with status 1
stdout: stderr: 2022-09-06 20:02:19.765 xcodebuild[25302:641207] Requested but did not find extension point with identifier Xcode.IDEKit.ExtensionSentinelHostApplications for extension Xcode.DebuggerFoundation.AppExtensionHosts.watchOS of plug-in com.apple.dt.IDEWatchSupportCore
2022-09-06 20:02:19.765 xcodebuild[25302:641207] Requested but did not find extension point with identifier Xcode.IDEKit.ExtensionPointIdentifierToBundleIdentifier for extension Xcode.DebuggerFoundation.AppExtensionToBundleIdentifierMap.watchOS of plug-in com.apple.dt.IDEWatchSupportCore
xcodebuild: error: SDK ""macosx12.5.1"" cannot be located.
2022-09-06 20:02:20.133 xcodebuild[25305:641233] Requested but did not find extension point with identifier Xcode.IDEKit.ExtensionSentinelHostApplications for extension Xcode.DebuggerFoundation.AppExtensionHosts.watchOS of plug-in com.apple.dt.IDEWatchSupportCore
2022-09-06 20:02:20.133 xcodebuild[25305:641233] Requested but did not find extension point with identifier Xcode.IDEKit.ExtensionPointIdentifierToBundleIdentifier for extension Xcode.DebuggerFoundation.AppExtensionToBundleIdentifierMap.watchOS of plug-in com.apple.dt.IDEWatchSupportCore
xcodebuild: error: SDK ""macosx12.5.1"" cannot be located.
xcrun: error: unable to lookup item 'Path' in SDK 'macosx12.5.1'
INFO: Elapsed time: 100.341s, Critical Path: 1.19s
INFO: 97 processes: 51 internal, 46 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```


### Standalone code to reproduce the issue

I'm [trying to update](https://github.com/spack/spack/pull/32544) the [Spack](https://spack.io) package manager with the latest TF release. Just like [all prior releases](https://github.com/tensorflow/tensorflow/issues/56799), we've unfortunately been unable to compile TF on Apple M1/M2. To reproduce the issue, you can use the branch where I'm trying to add TF 2.10:

```console
$ git clone https://github.com/adamjstewart/spack.git
$ cd spack
$ git checkout packages/py-tensorflow
$ . share/spack/setup-env.sh
$ spack install py-tensorflow
```

### Relevant log output

* [build log](https://github.com/tensorflow/tensorflow/files/9501838/spack-build-out.txt)
* [build env](https://github.com/tensorflow/tensorflow/files/9501840/spack-build-env-mods.txt)
"
57627,Unified Memory feature seems not working correctly,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04.1 LTS

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7/8.5.0

### GPU model and memory

_No response_

### Current Behaviour?

I am trying to use Unified Memory feature guided by the link below.
https://stackoverflow.com/questions/58025069/how-to-enable-cuda-unified-memory-in-tensorflow-v2

However, it results in either an incorrect result or an error. I would really appreciate it if someone could check if it is a bug or my miss use.

Amount of device memory is 40 GB and amount of host memory is 503 GiB.

An result of 40GiB.py is below: 1 + 1 becomes 0.
```
2022-09-06 19:47:39.927372: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-09-06 19:47:41.557960: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-06 19:47:42.842460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 323169 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:1f:00.0, compute capability: 8.0
2022-09-06 19:47:42.844257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 323169 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:8b:00.0, compute capability: 8.0
2022-09-06 19:47:42.872079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 323169 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:1f:00.0, compute capability: 8.0
2022-09-06 19:47:42.873286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 323169 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:8b:00.0, compute capability: 8.0
tf.Tensor([[1. 1. 1. ... 1. 1. 1.]], shape=(1, 10737418240), dtype=float32)
tf.Tensor([[1. 1. 1. ... 1. 1. 1.]], shape=(1, 10737418240), dtype=float32)
tf.Tensor([[0. 0. 0. ... 0. 0. 0.]], shape=(1, 10737418240), dtype=float32)
```

An result of 40GB.py is below: CUDA_ERROR_ILLEGAL_ADDRESS
```
2022-09-06 19:51:12.360869: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-09-06 19:51:14.110566: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-06 19:51:15.392251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 323169 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:1f:00.0, compute capability: 8.0
2022-09-06 19:51:15.394253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 323169 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:8b:00.0, compute capability: 8.0
2022-09-06 19:51:15.422149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 323169 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:1f:00.0, compute capability: 8.0
2022-09-06 19:51:15.423485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 323169 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:8b:00.0, compute capability: 8.0
tf.Tensor([[1. 1. 1. ... 1. 1. 1.]], shape=(1, 10000000000), dtype=float32)
tf.Tensor([[1. 1. 1. ... 1. 1. 1.]], shape=(1, 10000000000), dtype=float32)
Traceback (most recent call last):
  File ""/home/siwata/40GB.py"", line 17, in <module>
    print(c)
  File ""/home/siwata/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 1097, in __str__
    value_text(self, is_repr=False), self.shape, self.dtype.name)
  File ""/home/siwata/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 260, in value_text
    text = numpy_text(tensor, is_repr=is_repr)
  File ""/home/siwata/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 231, in numpy_text
    text = repr(tensor._numpy()) if is_repr else str(tensor._numpy())
  File ""/home/siwata/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 1127, in _numpy
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InternalError: Could not synchronize CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
```


### Standalone code to reproduce the issue

40GiB.py
```python
import tensorflow as tf
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 8
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

with tf.device(""/gpu:0""):
    size = 1024 ** 3 * 10 # 40 GiB
    a = tf.ones([1, size])
    print(a)
    b = tf.ones([1, size])
    print(b)
    c = tf.add(a, b)
    print(c)
```

40GB.py
```python
import tensorflow as tf
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 8
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

with tf.device(""/gpu:0""):
    size = 1000 ** 3 * 10 # 40 GB
    a = tf.ones([1, size])
    print(a)
    b = tf.ones([1, size])
    print(b)
    c = tf.add(a, b)
    print(c)
```


### Relevant log output

_No response_</details>"
57626,Android Tensorflow Lite 2.9.0 AAR InterpreterAPI not found Error ,"**System information**
- OS Platform and Distribution (: Ubuntu
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source): 2.9.0 AAR

**Issue details**
As per our requirement, I want to add TF Lite AAR file into our project.

So I have downloaded AAR file(2.9.0) from [Maven](https://jarcasting.in/artifacts/org.tensorflow/tensorflow-lite/2.9.0/) and added to my project.Then I can imports the Interpreter and other required things, but when I try to build and run the project I’m getting InterpreterAPI class not found error like this.([please refer the screenshots][2]](https://i.stack.imgur.com/RPMkM.png)[[3]](https://i.stack.imgur.com/Jj9JY.png))

class file for org.tensorflow.lite.InterpreterApi not found

If I add below 2.7.0 version aar files, I’m able to build and run the projects but those are deprecated, I want use latest AAR file only.

Can anyone please help me on this?

Note: I should use AAR only not gradle dependencies"
57625,ImportError: cannot import name 'test' from partially initialized module 'tensorflow._api.v2.__internal__',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.2

### Custom Code

Yes

### OS Platform and Distribution

Debian GNU/Linux 11 (bullseye)

### Mobile device

_No response_

### Python version

3.9.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'm using tensorflow in docker container, but i get this error when trying to import this module.
Error can be solved if i install another or same version of this module (but for production its bad solution)
I have installed this packages (pip freeze):
absl-py==1.2.0
anyio==3.4.0
asgi-lifespan==1.0.1
asgiref==3.4.1
astroid==2.11.7
astunparse==1.6.3
asyncio-dgram==2.1.2
attrs==22.1.0
autoflake==1.4
bandit==1.7.4
black==22.3.0
cachetools==5.2.0
certifi==2021.10.8
charset-normalizer==2.0.7
click==8.1.3
dataclasses==0.6
deap==1.3.1
dill==0.3.5.1
fastapi==0.73.0
flatbuffers==1.12
gast==0.4.0
gitdb==4.0.9
GitPython==3.1.27
google-auth==2.11.0
google-auth-oauthlib==0.4.6
google-pasta==0.2.0
grpcio==1.48.1
h11==0.12.0
h5py==3.7.0
httpcore==0.14.7
httptools==0.3.0
httpx==0.22.0
idna==3.3
importlib-metadata==4.12.0
iniconfig==1.1.1
isort==5.10.1
joblib==0.17.0
keras==2.9.0
Keras-Preprocessing==1.1.2
lazy-object-proxy==1.7.1
libclang==14.0.6
lxml==4.7.1
Markdown==3.4.1
MarkupSafe==2.1.1
mccabe==0.7.0
mypy==0.942
mypy-extensions==0.4.3
numpy==1.21.3
oauthlib==3.2.0
opt-einsum==3.3.0
packaging==21.3
pandas==1.3.4
pathspec==0.10.1
pbr==5.10.0
platformdirs==2.5.2
plotly==4.8.1
pluggy==1.0.0
protobuf==3.19.4
py==1.11.0
pyasn1==0.4.8
pyasn1-modules==0.2.8
pydantic==1.9.0
pyflakes==2.5.0
pylint==2.13.2
pyparsing==3.0.9
pytest==7.1.1
pytest-asyncio==0.18.3
pytest-timeout==2.1.0
python-dateutil==2.8.2
python-dotenv==0.19.2
pytz==2021.3
PyYAML==6.0
requests==2.26.0
requests-oauthlib==1.3.1
retrying==1.3.3
rfc3986==1.5.0
rsa==4.9
scapy==2.4.5
scikit-learn==0.24.2
scipy==1.7.2
six==1.16.0
smmap==5.0.0
sniffio==1.2.0
starlette==0.17.1
stevedore==4.0.0
stopit==1.1.2
tensorboard==2.9.1
tensorboard-data-server==0.6.1
tensorboard-plugin-wit==1.8.1
tensorflow==2.9.2
tensorflow-estimator==2.9.0
tensorflow-io-gcs-filesystem==0.26.0
termcolor==1.1.0
threadpoolctl==3.0.0
toml==0.10.2
tomli==2.0.1
TPOT==0.11.5
tqdm==4.62.3
types-PyYAML==6.0.5
typing-extensions==3.10.0.2
update-checker==0.18.0
urllib3==1.26.7
uvicorn==0.17.1
uvloop==0.16.0
vulture==2.3
watchgod==0.7
websockets==10.1
Werkzeug==2.2.2
wrapt==1.14.1
zipp==3.8.1
```


### Standalone code to reproduce the issue

```shell
root@c666258f6b9d:/app# python3
Python 3.9.9 (main, Mar 28 2022, 09:18:32) 
[GCC 10.2.1 20210110] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
```


### Relevant log output

```shell
2022-09-06 09:42:52.620866: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-09-06 09:42:52.620890: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.9/site-packages/tensorflow/__init__.py"", line 45, in <module>
    from ._api.v2 import __internal__
  File ""/usr/local/lib/python3.9/site-packages/tensorflow/_api/v2/__internal__/__init__.py"", line 22, in <module>
    from . import test
ImportError: cannot import name 'test' from partially initialized module 'tensorflow._api.v2.__internal__' (most likely due to a circular import) (/usr/local/lib/python3.9/site-packages/tensorflow/_api/v2/__internal__/__init__.py)
```
</details>"
57624,Failed to import tensorflow on jetson xavier NX development kit even after installing tensorflow module,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.10.0rc3

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

Jetson Xavier NX platform ubuntu 20.04

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
showing Failed to load the native TensorFlow runtime.
```


### Standalone code to reproduce the issue

```shell
https://learnopencv.com/understanding-multiple-object-tracking-using-deepsort/
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /usr/local/lib/python3.8/dist-packages/tensorflow/python/../../tensorflow_cpu_aws.libs/libgomp-d22c30c5.so.1.0.0: cannot allocate memory in static TLS block

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""detect_track.py"", line 34, in <module>
    from deep_sort.tools import generate_detections as gdet
  File ""/home/xaviernx/Desktop/pycharm projects/Detect_track/yolov5/deep_sort/tools/generate_detections.py"", line 7, in <module>
    import tensorflow.compat.v1 as tf
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 37, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 77, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /usr/local/lib/python3.8/dist-packages/tensorflow/python/../../tensorflow_cpu_aws.libs/libgomp-d22c30c5.so.1.0.0: cannot allocate memory in static TLS block


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
</details>"
57623,GPU memory usage depends on data size,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.6-2.10

### Custom Code

Yes

### OS Platform and Distribution

Linux Fedora 36

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

RTX 2070, 8GiB

### Current Behaviour?


Training a small model on small batches runs out of GPU memory if the total amount of data is large.

The same code works fine on TF 2.5, but fails on 2.6, 2.7, 2.8, and 2.9.



### Standalone code to reproduce the issue


Here is a handy script: https://gist.github.com/Dapid/3f58697edf91c6610ed5e7b681db440f

In short, a tiny model is trained on a fixed array of data. If the total amount of data is small, it runs; otherwise, it crashes.

Running `python benchmark.py` works, but `python benchmark.py --big` doesn't. Using the `tf.data` API doesn't make a difference.



### Relevant log output

```shell
-09-06 10:52:22.131172: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 15.26GiB (rounded to 16384000000)requested by op _EagerConst
```

Defining `TF_GPU_ALLOCATOR=cuda_malloc_async`:

```shell
2022-09-06 10:58:16.939018: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16384000000 exceeds 10% of free system memory.
2022-09-06 10:58:23.313713: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 16384000000 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 1111293952/8369799168
2022-09-06 10:58:23.313737: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                      6184304640
InUse:                        67126312
MaxInUse:                    201327628
NumAllocs:                          13
MaxAllocSize:                 67108864
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-09-06 10:58:23.313745: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-09-06 10:58:23.313749: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 5
2022-09-06 10:58:23.313753: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 2
2022-09-06 10:58:23.313755: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-09-06 10:58:23.313758: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16384, 1
2022-09-06 10:58:23.313761: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 67108864, 1
```

</details>"
57622,Building error in docker,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8.11

### Bazel version

_No response_

### GCC/Compiler version

clang 11.0.0 clang version 11.0.0 (https://github.com/llvm/llvm-project.git 60a25202a7dd1e00067fcfce512086ebf3788537) Target: x86_64-unknown-linux-gnu Thread model: posix InstalledDir: /tmp/llvm-110-install_O_D_A/bin

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to build tensorflow using clang inside a docker. but it is showing me import error. Even include guard is not working.
```


### Standalone code to reproduce the issue

```shell
export CPLUS_INCLUDE_PATH=/home/klee/tensorflow/
alias clang++='clang++ -I tensorflow/third_party/eigen3/'
clang++ -emit-llvm -g -c tensorflow/tensorflow/core/kernels/aggregate_ops.cc -o aggregate_ops.bc
```


### Relevant log output

```shell
In file included from tensorflow/tensorflow/core/kernels/aggregate_ops.cc:20:
In file included from /home/klee/tensorflow/tensorflow/core/kernels/aggregate_ops.h:21:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
In file included from tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:
tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:10: error: #include nested too deeply
#include ""unsupported/Eigen/CXX11/Tensor""
         ^
In file included from tensorflow/tensorflow/core/kernels/aggregate_ops.cc:20:
In file included from /home/klee/tensorflow/tensorflow/core/kernels/aggregate_ops.h:22:
In file included from /home/klee/tensorflow/tensorflow/core/framework/tensor_types.h:20:
In file included from /home/klee/tensorflow/tensorflow/core/platform/logging.h:19:
In file included from /home/klee/tensorflow/tensorflow/core/platform/types.h:19:
In file included from /home/klee/tensorflow/tensorflow/core/platform/bfloat16.h:21:
/home/klee/tensorflow/tensorflow/tsl/platform/bfloat16.h:24:9: error: use of undeclared identifier 'Eigen'
typedef Eigen::bfloat16 bfloat16;
        ^
In file included from tensorflow/tensorflow/core/kernels/aggregate_ops.cc:20:
In file included from /home/klee/tensorflow/tensorflow/core/kernels/aggregate_ops.h:22:
In file included from /home/klee/tensorflow/tensorflow/core/framework/tensor_types.h:20:
In file included from /home/klee/tensorflow/tensorflow/core/platform/logging.h:19:
In file included from /home/klee/tensorflow/tensorflow/core/platform/types.h:21:
In file included from /home/klee/tensorflow/tensorflow/core/platform/tstring.h:19:
In file included from /home/klee/tensorflow/tensorflow/core/platform/cord.h:25:
/home/klee/tensorflow/tensorflow/tsl/platform/default/cord.h:22:10: fatal error: 'absl/strings/cord.h' file not found
#include ""absl/strings/cord.h""
         ^~~~~~~~~~~~~~~~~~~~~
3 errors generated.
```
</details>"
57621,Removing Text from Images using CV2 and Keras-OCR ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

windows

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Removing Text from Images using CV2 and Keras-OCR -while removing text from image it's creating blurriness in background for some images.
```


### Standalone code to reproduce the issue

```shell
will send soon in comment
```


### Relevant log output

_No response_</details>"
57620,TF2 Keras OOM Training ImageNet with MobileNet V2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.2

### Custom Code

No

### OS Platform and Distribution

centos rhel fedora

### Mobile device

_No response_

### Python version

3.8.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6

### GPU model and memory

4x Tesla V100 16G 

### Current Behaviour?


As part of the effort of validating the official pretrained Keras models, I tried to measure the accuracy and try training MobileNet V2 with ImageNet2012. The measure accuracy is lower than expected, which is documented separately. The training also seems problematic with OOM errors even when the batch size is reduced to 32. 

The experiment was performed on a 4-GPU node with TF2/Keras. The ImageNet 2012 dataset is prepared using `tfds`. The multi-GPU training used `tf.distribute.MirroredStrategy()`. Please see the code and log for more details.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow import keras
import tensorflow_datasets as tfds
import numpy as np
import os
import time

from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions
from tensorflow.keras.preprocessing import image

tf.keras.backend.set_image_data_format(
    data_format='channels_last'
)

# ## MobileNet V2 Smoke Test

mbv2 = MobileNetV2(weights='imagenet')
img  = image.load_img('./dog.jpeg', target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

preds = mbv2.predict(x)
print('Predicted:', decode_predictions(preds, top=3)[0])


# ## Prepare ImageNet Train & Validation

# Get imagenet labels
labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')
imagenet_labels = np.array(open(labels_path).read().splitlines())

data_dir_val  = '/home/le_user/imagenet_dataset/'
write_dir_val = '/home/le_user/imagenet_dataset_tfds_full'

# Construct a tf.data.Dataset
download_config_val = tfds.download.DownloadConfig(
    extract_dir=os.path.join(write_dir_val, 'extracted'),
    manual_dir=data_dir_val)

download_and_prepare_kwargs_val = {
    'download_dir': os.path.join(write_dir_val, 'downloaded'),
    'download_config': download_config_val,
}

def resize_with_crop(image, label):
    i = image
    i = tf.cast(i, tf.float32)
    i = tf.image.resize_with_crop_or_pad(i, 224, 224)
    i = tf.keras.applications.mobilenet_v2.preprocess_input(i)
    return (i, label)

def resize_with_crop_v3(image, label):
    i = image
    i = tf.cast(i, tf.float32)
    i = tf.image.resize_with_crop_or_pad(i, 224, 224)
    i = tf.keras.applications.mobilenet_v3.preprocess_input(i)
    return (i, label)


ds = tfds.load('imagenet2012', 
               data_dir=os.path.join(write_dir_val, 'data'),         
               split=['train', 'validation'], 
               shuffle_files=True, 
               download=False, 
               as_supervised=True,
               download_and_prepare_kwargs=download_and_prepare_kwargs_val)

AUTOTUNE = tf.data.AUTOTUNE
BATCH_SIZE_PER_REPLICA = 128
NUM_GPUS = strategy.num_replicas_in_sync


# ## Multi-GPU MB-V2 Validation & Training
ds_val_parallel = ds[1].map(resize_with_crop)
ds_val_parallel = ds_val_parallel.batch(batch_size=BATCH_SIZE_PER_REPLICA * NUM_GPUS)
ds_val_parallel = ds_val_parallel.cache().prefetch(buffer_size=AUTOTUNE)

ds_train_parallel = ds[0].map(resize_with_crop)
ds_train_parallel = ds_train_parallel.batch(batch_size=BATCH_SIZE_PER_REPLICA * NUM_GPUS)
ds_train_parallel = ds_train_parallel.cache().prefetch(buffer_size=AUTOTUNE)

strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    mbv2_train_parallel = keras.applications.MobileNetV2(include_top=True, 
                                                        weights='imagenet', 
                                                        classifier_activation='softmax')
    mbv2_train_parallel.trainable = True
    mbv2_train_parallel.compile(optimizer='adam',
                                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                                metrics=['accuracy'])
start_time = time.time()
result_parallel = mbv2_train_parallel.evaluate(ds_val_parallel)
print(f""--- {strategy.num_replicas_in_sync}-GPU eval took {(time.time() - start_time)} seconds ---"")

print(dict(zip(mbv2_train_parallel.metrics_names, result_parallel)))

mbv2_train_parallel.fit(
    x=ds_train_parallel,
    validation_data=ds_val_parallel,
    epochs=20
)
```


### Relevant log output

```shell
2.9.2
2.9.0
channels_first
channels_last
1/1 [==============================] - 2s 2s/step
Predicted: [('n02109961', 'Eskimo_dog', 0.35159874), ('n02114548', 'white_wolf', 0.13579218), ('n02110063', 'malamute', 0.033763986)]
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')

INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).

98/98 [==============================] - 56s 394ms/step - loss: 1.7855 - accuracy: 0.6155
--- 4-GPU eval took 56.09924674034119 seconds ---
{'loss': 1.7854773998260498, 'accuracy': 0.6154599785804749}

Epoch 1/20
INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1

INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1

INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1

INFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1

 120/2503 [>.............................] - ETA: 19:42 - loss: 1.9443 - accuracy: 0.5517

---------------------------------------------------------------------------
ResourceExhaustedError                    Traceback (most recent call last)
/tmp/ipykernel_103014/1560377782.py in <cell line: 1>()
----> 1 mbv2_train_parallel.fit(
      2     x=ds_train_parallel,
      3     validation_data=ds_val_parallel,
      4     epochs=20
      5 )

~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     52   try:
     53     ctx.ensure_initialized()
---> 54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:

ResourceExhaustedError: Graph execution error:

5 root error(s) found.
  (0) RESOURCE_EXHAUSTED:  Failed to allocate memory for the batch of component 0
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[RemoteCall]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[IteratorGetNextAsOptional]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[group_deps/_681]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

  (1) RESOURCE_EXHAUSTED:  Failed to allocate memory for the batch of component 0
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[RemoteCall]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[IteratorGetNextAsOptional]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[div_no_nan_1/_655]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

  (2) RESOURCE_EXHAUSTED:  Failed to allocate memory for the batch of component 0
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[RemoteCall]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[IteratorGetNextAsOptional]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[div_no_nan/ReadVariableOp/_612]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

  (3) RESOURCE_EXHAUSTED:  Failed to allocate memory for the batch of component 0
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[RemoteCall]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[IteratorGetNextAsOptional]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[cond/output/_14/_116]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

  (4) RESOURCE_EXHAUSTED:  Failed to allocate memory for the batch of component 0
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[RemoteCall]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[IteratorGetNextAsOptional]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_77642]
```
</details>"
57619,tflite_model_maker not exporting to quantised int8,"### 1. System information
_This error happens both on my PC:_
- OS Platform and Distribution: Windows 10(amd64) v10.0
- TensorFlow installation (pip package or built from source): From pip, TF version v2.9.0-18-gd8ce9f9c301 2.9.1 on Python 3.9
- TensorFlow library: 

_AND on Google Colab_

### 2. Code
_What I am trying to do:_
Train a model and export a quantised int8 version for later use with the TPU USB accelerator. I export a non-quantized version for comparison.

_What I get:_
The model which is supposed to be quantized always exports identically to the un-quantized version, with the same message when running ""model.export()"" that ""Statistics for quantized inputs were expected, but not specified; continuing anyway.""
Link to the exported model:
https://drive.google.com/file/d/1Gt4XT1J2Ujt3dXy-vKgwESs2fciTDqFA/view?usp=sharing

```
# import
import tensorflow as tf

assert tf.__version__.startswith('2')
from tflite_model_maker.config import QuantizationConfig
from tflite_model_maker.image_classifier import DataLoader
from tflite_model_maker import model_spec
from tflite_model_maker import image_classifier

# train data folder path to my google drive, all images are .jpeg
data = DataLoader.from_folder(train_data_folder_path)
train_data, val_data = data.split(0.8)
model_spec = model_spec.get('mobilenet_v2')
model = image_classifier.create(
    train_data,
    model_spec=model_spec,
    validation_data=val_data,
)

# test data folder path goes to my google drive, all images are .jpeg
test_data = DataLoader.from_folder(test_data_folder_path)

# First export the model with default settings
tflite_filename = ""default_model.tflite""
model.export(export_dir=""."", tflite_filename=tflite_filename)

# Then, export as quantized
quant_tflite_filename = ""int8_model.tflite""
quantization_config = QuantizationConfig.for_int8(test_data)
model.export(export_dir=""."", tflite_filename=quant_tflite_filename, quantization_config=quantization_config)
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model does not produce a quantized version. The output files are of identical file size and the log below is the same for both.

### 5. Any other info / logs
>>> model._export_tflite(tflite_filepath=tflite_filename, quantization_config=quantization_config)

INFO:tensorflow:Assets written to: C:\Users\sm251\AppData\Local\Temp\tmpymme3pmk\assets
INFO:tensorflow:Assets written to: C:\Users\sm251\AppData\Local\Temp\tmpymme3pmk\assets
2022-09-06 14:35:11.483922: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2022-09-06 14:35:11.484556: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
C:\Users\sm251\AppData\Roaming\Python\Python39\site-packages\tensorflow\lite\python\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.
  warnings.warn(""Statistics for quantized inputs were expected, but not ""
2022-09-06 14:35:21.939062: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.
2022-09-06 14:35:21.939219: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.
fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3
INFO:tensorflow:Label file is inside the TFLite model with metadata.
INFO:tensorflow:Label file is inside the TFLite model with metadata.
INFO:tensorflow:Saving labels in C:\Users\sm251\AppData\Local\Temp\tmpf7kb5uh9\labels.txt
INFO:tensorflow:Saving labels in C:\Users\sm251\AppData\Local\Temp\tmpf7kb5uh9\labels.txt"
57618,Does Distributed TensorFlow can be run in independent filesystem,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf1.14

### Custom Code

Yes

### OS Platform and Distribution

Debain 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I used tensorflow version 1.14 to run distributed training and failed using the **local file system.**

I saw the exchange [post in 2018 on the Internet](https://groups.google.com/a/tensorflow.org/g/discuss/c/-NdsSrWmp8w), saying that the distributed training must use the shared file system at that time, but the local file system will be supported later
![image](https://user-images.githubusercontent.com/15689619/188060748-9bff0805-1799-4009-9cb3-248f7ec8960f.png)

Is there a version that supports running distributed training tasks on the local file system?

### Standalone code to reproduce the issue

I use tf1.14 version of the distributed training framework estimator training data, with 2 ps and 3 workers, which is all configure the local file system, and found that the operation failed.
<img width=""1085"" alt=""image"" src=""https://user-images.githubusercontent.com/15689619/188532156-463afea5-3522-4212-b9f0-25be5705996e.png"">

### Relevant log output

_No response_</details>"
57617,[Build Issue] OSError: [Errno 8] Exec format error  when doing cross compilation of TFLite C library for ARM64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

latest master, commit 71edfa2a963c1ebb624786263ab87052b39095e7)

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

Linux Ubuntu 18.04

### Python version

3.6.9

### Bazel version

_No response_

### GCC/Compiler version

Cross compilation with gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I tried to build tflite from source using cross compilation, however, I got the following issue:

Traceback (most recent call last):
  File ""scripts/generate_code.py"", line 164, in <module>
    data=""monsterdata_test.json"",
  File ""scripts/generate_code.py"", line 82, in flatc
    result = subprocess.run(cmd, cwd=str(cwd), check=True)
  File ""/usr/lib/python3.6/subprocess.py"", line 423, in run
    with Popen(*popenargs, **kwargs) as process:
  File ""/usr/lib/python3.6/subprocess.py"", line 729, in __init__
    restore_signals, start_new_session)
  File ""/usr/lib/python3.6/subprocess.py"", line 1364, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
OSError: [Errno 8] Exec format error: '/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tensorflow_src/tflite_build_arm/_deps/flatbuffers-build/flatc'
make[2]: *** [_deps/flatbuffers-build/flatc] Error 1
make[2]: *** Deleting file '_deps/flatbuffers-build/flatc'
make[1]: *** [_deps/flatbuffers-build/CMakeFiles/flatc.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
make: *** [all] Error 2
```


### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow.git tensorflow_src

cd tensorflow_src/

mkdir tflite_build_arm

cd tflite_build_arm

ARMCC_PREFIX=/home/yeverino/titools/toolchain/gcc-arm-9.2-2019.12-x86_64-aarch64-none-linux-gnu/bin/aarch64-none-linux-gnu-

ARMCC_FLAGS=""-funsafe-math-optimizations""

cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc  -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++  -DCMAKE_C_FLAGS=""${ARMCC_FLAGS}""  -DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}""  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON  -DCMAKE_SYSTEM_NAME=Linux  -DTFLITE_ENABLE_XNNPACK=ON  -DCMAKE_SYSTEM_PROCESSOR=aarch64  -DBUILD_SHARED_LIBS=ON  ../tensorflow/lite/

cmake --build . -j
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""scripts/generate_code.py"", line 164, in <module>
    data=""monsterdata_test.json"",
  File ""scripts/generate_code.py"", line 82, in flatc
    result = subprocess.run(cmd, cwd=str(cwd), check=True)
  File ""/usr/lib/python3.6/subprocess.py"", line 423, in run
    with Popen(*popenargs, **kwargs) as process:
  File ""/usr/lib/python3.6/subprocess.py"", line 729, in __init__
    restore_signals, start_new_session)
  File ""/usr/lib/python3.6/subprocess.py"", line 1364, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
OSError: [Errno 8] Exec format error: '/home/yeverino/Documents/git/fordos_tilinux/Fnv4VX/tensorflow_src/tflite_build_arm/_deps/flatbuffers-build/flatc'
make[2]: *** [_deps/flatbuffers-build/flatc] Error 1
make[2]: *** Deleting file '_deps/flatbuffers-build/flatc'
make[1]: *** [_deps/flatbuffers-build/CMakeFiles/flatc.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
make: *** [all] Error 2
```
</details>"
57615,Feature: Balanced Accuracy,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

tf 2.10.0-rc3

### Custom Code

No

### OS Platform and Distribution

Debian 11

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current Behaviour?

I suggest adding [Balanced Accuracy](https://www.statology.org/balanced-accuracy/) as additional [Keras metrics](https://keras.io/api/metrics/).

It is useful for training with an imbalanced validation dataset.

(Note: I see various suggestions on StackOverflow: [1](https://stackoverflow.com/questions/59339531/balanced-accuracy-score-in-tensorflow), [2](https://stackoverflow.com/questions/65189262/tensorflow-2-0-custom-metric-balanced-accuracy-score-for-modelcheckpoint-not))


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
metrics = [tf.keras.metrics.BalancedAccuracy()]
```


### Relevant log output

_No response_</details>"
57614,TensorBoard callback does not log `batch_` metrics since v2.8.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

Ubuntu on Colab

### Mobile device

_No response_

### Python version

3.7.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

`TensorBoard` callback doesn't log `batch_` metrics even if I set `update_freq=2`. This behavior occurs since v2.8.0 and v2.7.0 works as expected. I am not sure if this is related, but this SO question seems similar to me: https://stackoverflow.com/questions/71594268/tensorboard-not-updating-by-batch-in-google-colab


### Standalone code to reproduce the issue

This colab demonstrates the issue: https://colab.research.google.com/gist/ebraraktas/1c5040b2934c710ac797b67ed98d3622


### Relevant log output

<img width=""691"" alt=""Screen Shot 2022-09-05 at 16 37 34"" src=""https://user-images.githubusercontent.com/62459770/188462896-15418679-c265-4577-8426-554d528a298e.png"">

_No response_</details>"
57612,OnDevice prediction produces same result for all inputs in a batch," 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.2

### Custom Code

No

### OS Platform and Distribution

Mac M1, Windows x64

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


The model works well in python.  But when the model is exported as a SavedModel, and `predict` concrete function is called from C API,  all the inputs in the same batch produce exact same result.  Tested in Mac M1 and Windows x64, same behaviour.



### Standalone code to reproduce the issue

```python
import tensorflow as tf
import tensorflow_probability as tfp

print( tf.__version__ )


def create_model(board_width, board_height):

    class RenjuModel(tf.Module):
        def __init__(self):
            l2_penalty_beta = 1e-4

            # Define the tensorflow neural network
            # 1. Input:
            self.inputs = tf.keras.Input( shape=(4, board_height, board_width), dtype=tf.dtypes.float32, name=""input"")
            self.transposed_inputs = tf.keras.layers.Lambda( lambda x: tf.transpose(x, [0, 2, 3, 1]) )(self.inputs)

            # 2. Common Networks Layers
            self.conv1 = tf.keras.layers.Conv2D( name=""conv1"",
                filters=32,
                kernel_size=(3, 3),
                padding=""same"",
                data_format=""channels_last"",
                activation=tf.keras.activations.relu,
                kernel_regularizer=tf.keras.regularizers.L2(l2_penalty_beta)
                )(self.transposed_inputs)

            self.conv2 = tf.keras.layers.Conv2D( name=""conv2"", 
                filters=64, 
                kernel_size=(3, 3), 
                padding=""same"", 
                data_format=""channels_last"", 
                activation=tf.keras.activations.relu,
                kernel_regularizer=tf.keras.regularizers.L2(l2_penalty_beta)
                )(self.conv1)

            self.conv3 = tf.keras.layers.Conv2D( name=""conv3"",
                filters=128,
                kernel_size=(3, 3),
                padding=""same"",
                data_format=""channels_last"",
                activation=tf.keras.activations.relu,
                kernel_regularizer=tf.keras.regularizers.L2(l2_penalty_beta)
                )(self.conv2)

            # 3-1 Action Networks
            self.action_conv = tf.keras.layers.Conv2D( name=""action_conv"",
                filters=4,
                kernel_size=(1, 1),
                padding=""same"",
                data_format=""channels_last"",
                activation=tf.keras.activations.relu,
                kernel_regularizer=tf.keras.regularizers.L2(l2_penalty_beta)
                )(self.conv3)

            # flatten tensor
            self.action_conv_flat = tf.keras.layers.Reshape( (-1, 4 * board_height * board_width), name=""action_conv_flat"" 
            )(self.action_conv)

            # 3-2 Full connected layer, the output is the log probability of moves
            # on each slot on the board
            self.action_fc = tf.keras.layers.Dense( board_height * board_width,
                activation=tf.nn.log_softmax,
                name=""action_fc"",
                kernel_regularizer=tf.keras.regularizers.L2(l2_penalty_beta)
                )(self.action_conv_flat)

            # 4 Evaluation Networks
            self.evaluation_conv = tf.keras.layers.Conv2D( name=""evaluation_conv"",
                filters=2,
                kernel_size=(1, 1),
                padding=""same"",
                data_format=""channels_last"",
                activation=tf.keras.activations.relu,
                kernel_regularizer=tf.keras.regularizers.L2(l2_penalty_beta)
                )(self.conv3)

            self.evaluation_conv_flat = tf.keras.layers.Reshape( (-1, 2 * board_height * board_width),
                name=""evaluation_conv_flat"" 
                )(self.evaluation_conv)

            self.evaluation_fc1 = tf.keras.layers.Dense( 64,
                activation=tf.keras.activations.relu,
                name=""evaluation_fc1"",
                kernel_regularizer=tf.keras.regularizers.L2(l2_penalty_beta)
                )(self.evaluation_conv_flat)

            self.evaluation_fc2 = tf.keras.layers.Dense( 1, 
                activation=tf.keras.activations.tanh,
                name=""evaluation_fc2"",
                kernel_regularizer=tf.keras.regularizers.L2(l2_penalty_beta)
                )(self.evaluation_fc1)

            self.outputs = tf.keras.layers.Concatenate()([self.action_fc, self.evaluation_fc2])                

            self.model = tf.keras.Model(inputs=self.inputs, outputs=self.outputs, name=""renju_model"")
            self.model.summary()
 
            self.lr = tf.Variable(0.002, trainable=False, dtype=tf.dtypes.float32)

            @tf.function(input_signature=[ tf.TensorSpec([None, 1, board_height * board_width + 1], tf.float32),
                tf.TensorSpec([None, 1, board_height * board_width + 1], tf.float32)
            ])
            def custom_loss(labels, predictions):
                act_probs_labels, value_labels = tf.split(labels, [board_height*board_width, -1], axis=2)
                act_probs_predictions, value_predictions = tf.split(predictions, [board_height*board_width, -1], axis=2)

                #tf.print(act_probs_labels, summarize=-1)
                #tf.print(value_labels, summarize=-1)
                #tf.print(act_probs_predictions, summarize=-1)
                #tf.print(value_predictions, summarize=-1)
         
                value_loss = tf.reduce_mean( tf.losses.mean_squared_error(value_labels, value_predictions) )
                policy_loss = tf.negative(tf.reduce_mean(
                    tf.reduce_sum(tf.multiply(act_probs_labels, act_probs_predictions), 2)))
                total_loss = policy_loss + value_loss
                tf.print( ""value_loss="", value_loss, "" policy_loss="", policy_loss, "" total_loss="", total_loss )
                return total_loss

            self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = self.lr),
                    loss=custom_loss,
                    metrics=['accuracy'])




        @tf.function(input_signature=[
            tf.TensorSpec([None, 4, board_height, board_width], tf.float32),
        ])
        def predict(self, state_batch):
            if tf.shape(state_batch)[0] > 1:
                tf.print(state_batch, summarize=-1)
            predictions = self.model(state_batch)
            if tf.shape(state_batch)[0] > 1:
                tf.print(predictions, summarize=-1)
            return tf.split(predictions, [board_height*board_width, -1], axis=2)

        @tf.function(input_signature=[tf.TensorSpec(shape=[None, 4, board_height, board_width],  dtype=tf.float32), 
                                  tf.TensorSpec(shape=[None, 1, board_height * board_width],  dtype=tf.float32),
                                  tf.TensorSpec(shape=[None, 1, 1],  dtype=tf.float32),
                                  tf.TensorSpec(shape=[1],  dtype=tf.float32) ])
        def train(self, state_batch, prob_batch, score_batch, lr):
            labels = tf.concat( [prob_batch, score_batch], axis=2)
            self.lr.assign(tf.gather(lr, 0))
            with tf.GradientTape() as tape:
                predictions = self.model(state_batch, training=True)  # Forward pass
                # the loss function is configured in `compile()`
                loss = self.model.compiled_loss(labels, predictions, regularization_losses=self.model.losses)
 
            gradients = tape.gradient(loss, self.model.trainable_variables)
            self.model.optimizer.apply_gradients(
                zip(gradients, self.model.trainable_variables))

            entropy = tf.negative(tf.reduce_mean(
               tf.reduce_sum(tf.exp(predictions) * predictions, 2)))

            return (loss, entropy)

        

        @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
        def save(self, checkpoint_path):
            tensor_names = [weight.name for weight in self.model.weights]
            tensors_to_save = [weight.read_value() for weight in self.model.weights]
            tf.raw_ops.Save(
                filename=checkpoint_path, tensor_names=tensor_names,
                data=tensors_to_save, name='save')
            return checkpoint_path

        @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
        def restore(self, checkpoint_path):
            restored_tensors = {}
            for var in self.model.weights:
                restored = tf.raw_ops.Restore( file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype, name='restore')
                var.assign(restored)
                restored_tensors[var.name] = restored
            return checkpoint_path

        @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])
        def random_choose_with_dirichlet_noice(self, probs):
            concentration = 0.3*tf.ones(tf.size(probs))
            dist = tfp.distributions.Dirichlet(concentration)
            p = 0.75*probs + 0.25*dist.sample(1)[0]
            samples = tf.random.categorical(tf.math.log([p]), 1)
            return samples[0] # selected index


    return RenjuModel()


model = create_model( 15, 15)

model.model.save('renju_15x15_model', 
        save_format='tf', 
        signatures={
            'predict': model.predict.get_concrete_function(), 
            'train' : model.train.get_concrete_function(), 
            'save' : model.save.get_concrete_function(),
            'restore' : model.restore.get_concrete_function(),
            'random_choose_with_dirichlet_noice' : model.random_choose_with_dirichlet_noice.get_concrete_function() 
        })
```

If calling `predict` method from python, you can clearly see the outputs of each input in the same batch are different.

```python
input = [
    [
        [
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        ],
        [
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        ],
        [
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        ],
        [
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        ],
    ],


        [
        [
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        ],
        [
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        ],
        [
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        ],
        [
            [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        ],
    ]
]

model.predict.get_concrete_function()( tf.convert_to_tensor(input) )
```

But if trying to call `predict` method of SavedModel from C API. The outputs are exactly the same for multiple inputs in the same batch.



### Relevant log output


The `predict` method prints out the input and output via `tf.print`
```python
        def predict(self, state_batch):
            if tf.shape(state_batch)[0] > 1:
                tf.print(state_batch, summarize=-1)
            predictions = self.model(state_batch)
            if tf.shape(state_batch)[0] > 1:
                tf.print(predictions, summarize=-1)
            return tf.split(predictions, [board_height*board_width, -1], axis=2)
```

And the following logs are from stdout when calling from C, produced by the `tf.print` above.

```
[[[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]

  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]

  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]

  [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
   [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]]


 [[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]

  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]

  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]

  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]]]
[[[-16.6252575 -16.6252575 -16.554945 -16.6252575 -16.5581188 -16.6252575 -16.5571423 -16.6252575 -16.5570812 -16.6252575 -16.5592785 -16.6252575 -16.5568371 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.556715 -16.5604382 -16.5638561 -16.1153088 -16.5592785 -16.5548229 -16.5536633 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.5546398 -16.6252575 -16.6251965 -16.5716686 -16.3776989 -15.2738171 -15.9649181 -16.549757 -16.2080822 -15.4714489 -16.3686047 -16.5711803 -16.6251965 -16.6252575 -16.5547 -16.6252575 -16.6252575 -16.57057 -16.5050793 -16.4986095 -16.1685314 -15.5485363 -15.7727184 -16.2866344 -16.1884899 -16.2140026 -16.5062389 -16.570631 -16.6252575 -16.6252575 -16.5578747 -16.5541515 -16.4574718 -16.237318 -12.7237072 -14.7688122 -15.8670177 -3.9945817 -14.1392956 -13.9726696 -14.495863 -16.5278454 -16.3515148 -16.5543957 -16.5592785 -16.6252575 -16.5604382 -16.3097668 -16.0515881 -10.2605724 -2.71552896 -3.54908609 -5.74897623 -3.83827066 -4.30714273 -13.6384411 -16.1675549 -15.1023083 -16.5556774 -16.6252575 -16.5547 -16.5589123 -16.4617443 -16.3551769 -15.2829113 -5.34351969 -3.03394938 -1.94203043 -4.53840494 -3.27900553 -15.9859142 -16.3760509 -16.4613781 -16.5639782 -16.554945 -16.6252575 -16.4714489 -16.5536022 -15.8650646 -4.17939615 -5.1966691 -2.17225504 -2.46821451 -2.10627604 -4.61872721 -3.8862443 -15.8424816 -16.5542126 -15.9301281 -16.6252575 -16.5547 -16.5638561 -15.9878063 -16.3649426 -14.6254406 -4.10206461 -4.06068277 -3.20484781 -4.07081461 -5.61872721 -15.9856701 -16.383131 -16.4535046 -16.5583019 -16.5559216 -16.6252575 -16.5608654 -16.3747082 -16.4586315 -14.030653 -3.08790445 -3.91151285 -6.01582193 -3.30915689 -3.78974771 -11.8531628 -16.1242809 -16.1317883 -16.5604382 -16.6252575 -16.5598888 -16.5536633 -16.4726696 -16.5026379 -14.5284557 -10.9210339 -15.3974743 -4.61561441 -15.4461193 -11.3503551 -12.4599743 -16.4962292 -16.3061047 -16.5563488 -16.5615978 -16.6252575 -16.6252575 -16.57057 -16.5019054 -16.5019054 -13.358778 -16.1637096 -15.8333263 -16.3820934 -16.0954113 -16.1836071 -16.5053844 -16.5707531 -16.6252575 -16.6252575 -16.5546398 -16.6252575 -16.6251965 -16.570509 -16.4332409 -15.9699841 -16.4579 -16.4486217 -15.909193 -16.3141 -15.8626842 -16.5714855 -16.6251965 -16.6252575 -16.5547619 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.5536633 -16.5536022 -16.5640392 -15.6290417 -16.5579967 -16.5569592 -16.5561047 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.5554943 -16.6252575 -16.5639172 -16.6252575 -16.5551281 -16.6252575 -16.5547 -16.6252575 -16.5608654 -16.6252575 -16.5559216 -16.6252575 -16.6252575 0.242623821]]

 [[-16.6252575 -16.6252575 -16.554945 -16.6252575 -16.5581188 -16.6252575 -16.5571423 -16.6252575 -16.5570812 -16.6252575 -16.5592785 -16.6252575 -16.5568371 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.556715 -16.5604382 -16.5638561 -16.1153088 -16.5592785 -16.5548229 -16.5536633 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.5546398 -16.6252575 -16.6251965 -16.5716686 -16.3776989 -15.2738171 -15.9649181 -16.549757 -16.2080822 -15.4714489 -16.3686047 -16.5711803 -16.6251965 -16.6252575 -16.5547 -16.6252575 -16.6252575 -16.57057 -16.5050793 -16.4986095 -16.1685314 -15.5485363 -15.7727184 -16.2866344 -16.1884899 -16.2140026 -16.5062389 -16.570631 -16.6252575 -16.6252575 -16.5578747 -16.5541515 -16.4574718 -16.237318 -12.7237072 -14.7688122 -15.8670177 -3.9945817 -14.1392956 -13.9726696 -14.495863 -16.5278454 -16.3515148 -16.5543957 -16.5592785 -16.6252575 -16.5604382 -16.3097668 -16.0515881 -10.2605724 -2.71552896 -3.54908609 -5.74897623 -3.83827066 -4.30714273 -13.6384411 -16.1675549 -15.1023083 -16.5556774 -16.6252575 -16.5547 -16.5589123 -16.4617443 -16.3551769 -15.2829113 -5.34351969 -3.03394938 -1.94203043 -4.53840494 -3.27900553 -15.9859142 -16.3760509 -16.4613781 -16.5639782 -16.554945 -16.6252575 -16.4714489 -16.5536022 -15.8650646 -4.17939615 -5.1966691 -2.17225504 -2.46821451 -2.10627604 -4.61872721 -3.8862443 -15.8424816 -16.5542126 -15.9301281 -16.6252575 -16.5547 -16.5638561 -15.9878063 -16.3649426 -14.6254406 -4.10206461 -4.06068277 -3.20484781 -4.07081461 -5.61872721 -15.9856701 -16.383131 -16.4535046 -16.5583019 -16.5559216 -16.6252575 -16.5608654 -16.3747082 -16.4586315 -14.030653 -3.08790445 -3.91151285 -6.01582193 -3.30915689 -3.78974771 -11.8531628 -16.1242809 -16.1317883 -16.5604382 -16.6252575 -16.5598888 -16.5536633 -16.4726696 -16.5026379 -14.5284557 -10.9210339 -15.3974743 -4.61561441 -15.4461193 -11.3503551 -12.4599743 -16.4962292 -16.3061047 -16.5563488 -16.5615978 -16.6252575 -16.6252575 -16.57057 -16.5019054 -16.5019054 -13.358778 -16.1637096 -15.8333263 -16.3820934 -16.0954113 -16.1836071 -16.5053844 -16.5707531 -16.6252575 -16.6252575 -16.5546398 -16.6252575 -16.6251965 -16.570509 -16.4332409 -15.9699841 -16.4579 -16.4486217 -15.909193 -16.3141 -15.8626842 -16.5714855 -16.6251965 -16.6252575 -16.5547619 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.5536633 -16.5536022 -16.5640392 -15.6290417 -16.5579967 -16.5569592 -16.5561047 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.6252575 -16.5554943 -16.6252575 -16.5639172 -16.6252575 -16.5551281 -16.6252575 -16.5547 -16.6252575 -16.5608654 -16.6252575 -16.5559216 -16.6252575 -16.6252575 0.242623821]]]
```
"
57611,XLA bug for ReLU after convolution,"### Issue Type

Bug

### Source

binary, official 2.9.1-gpu Docker image

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux CentOS Stream release 8

### Python version

3.8.10

### CUDA/cuDNN version

11.2

### Current Behaviour?

With XLA enabled, ReLU after a convolution causes a CUDNN_STATUS_INTERNAL_ERROR. The error disappears when ReLU is replaced with LeakyReLU.

In 2019, a similar issue was resolved for JAX by updating XLA (https://github.com/google/jax/issues/1049). The bug comes up when running the below Python code in the official tensorflow/tensorflow:2.9.1-gpu Docker image from May 2022:

```shell
singularity pull docker://tensorflow/tensorflow:2.9.1-gpu
singularity exec -e --nv tensorflow_2.9.1-gpu.sif python ./relu.py
```


### Standalone code to reproduce the issue

```python
import os
import tensorflow as tf

os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2'

model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv1D(filters=1, kernel_size=1))
model.add(tf.keras.layers.ReLU())
model.compile(loss='MSE')

x = tf.ones(shape=(1, 32, 1))
model.fit(x, y=x, epochs=2)
```


### Relevant log output

```
2022-09-04 09:21:09.113717: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-04 09:21:17.262140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29503 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0
2022-09-04 09:21:17.290599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30988 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1c:00.0, compute capability: 7.0
2022-09-04 09:21:17.291712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30988 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1d:00.0, compute capability: 7.0
2022-09-04 09:21:17.292349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30988 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1e:00.0, compute capability: 7.0
2022-09-04 09:21:17.954855: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1750] (One-time warning): Not using XLA:CPU for cluster.

If you want XLA:CPU, do one of the following:

 - set the TF_XLA_FLAGS to include ""--tf_xla_cpu_global_jit"", or
 - set cpu_global_jit to true on this session's OptimizerOptions, or
 - use experimental_jit_scope, or
 - use tf.function(jit_compile=True).

To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a
proper command-line flag, not via TF_XLA_FLAGS).
Epoch 1/2
2022-09-04 09:21:18.525364: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f12e801d660 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-09-04 09:21:18.525426: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-09-04 09:21:18.525438: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-09-04 09:21:18.525449: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (2): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-09-04 09:21:18.525459: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (3): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-09-04 09:21:18.657529: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-09-04 09:21:23.648975: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100
2022-09-04 09:21:36.916665: I tensorflow/compiler/jit/xla_compilation_cache.cc:478] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/1 [==============================] - 19s 19s/step - loss: 1.0000
Epoch 2/2
Traceback (most recent call last):
  File ""./relu.py"", line 12, in <module>
    model.fit(x, y=x, epochs=2)
  File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnknownError: Graph execution error:

CUDNN_STATUS_INTERNAL_ERROR
in tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'
         [[{{node cluster_0_1/xla_run}}]] [Op:__inference_train_function_502]
```
</details>"
57610,Gather Op on GPU,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS BigSur 11.6.7 and Android 10 / 11.
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): v2.10.0-rc2


**Provide the text output from tflite_convert**

```
ERROR: Following operations are not supported by GPU delegate:
1: GATHER: Operation is not supported.
1: 351 operations will run on the GPU, and the remaining 1084 operations will run on the CPU.
```

Hi, 

I have a model that needs to run on GPU but it seems that GATHER Op is not supported by GPU delegate when using tflite C++. Some questions:

1. Is there any particular reason why it is not supported? 
2. Do you have plans to support this Op on GPU?
3. Is there any other Op (or set of Ops) that I could use to replace Gather on the GPU?
4. Is there a way to split the model into different subgraphs and run only the one that contains Gather Op on CPU? (if so, could you point me to some documentation?)

Thank you!"
57607,'tf.MaxPool3D' op is neither a custom op nor a flex op,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- TensorFlow installation (pip package or built from source): pip package (Installed Tensorflow 2.8.0)
- TensorFlow library (version, if pip package or github SHA, if built from source): NA

### 2. Code

Model Architecture :

```
Model: ""sequential_10""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv3d_30 (Conv3D)          (None, 16, 16, 16, 8)     224       
                                                                 
 conv3d_31 (Conv3D)          (None, 16, 16, 16, 16)    3472      
                                                                 
 batch_normalization_15 (Bat  (None, 16, 16, 16, 16)   64        
 chNormalization)                                                
                                                                 
 max_pooling3d_15 (MaxPoolin  (None, 8, 8, 8, 16)      0         
 g3D)                                                            
                                                                 
 =================================================================
```

While converting the above model to tflite using below code 

```
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with open(""model.tflite"",""wb"") as f:
    f.write(tflite_model)
```

Generates error:
```
69:0: error: 'tf.MaxPool3D' op is neither a custom op nor a flex op
r code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: failed while converting: 'main': 
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: MaxPool3D
Details:
	tf.MaxPool3D(tensor<?x16x16x16x16xf32>) -> (tensor<?x8x8x8x16xf32>) : {data_format = ""NDHWC"", device = """", ksize = [1, 2, 2, 2, 1], padding = ""VALID"", strides = [1, 2, 2, 2, 1]}
	tf.MaxPool3D(tensor<?x8x8x8x64xf32>) -> (tensor<?x4x4x4x64xf32>) : {data_format = ""NDHWC"", device = """", ksize = [1, 2, 2, 2, 1], padding = ""VALID"", strides = [1, 2, 2, 2, 1]}
```

Tried to fix by converter.allow_custom_ops=True
```
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.allow_custom_ops=True
tflite_model = converter.convert()
with open(""model.tflite"",""wb"") as f:
    f.write(tflite_model)
```
But the layer introduced **FlexMaxPool3D** which while running on device (embedded) failed with error
```
ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
ERROR: Node number 11 (FlexMaxPool3D) failed to prepare.
```

Is MaxPool3d supported by native tflite ?
If not is there any alternative layer/Custom implementation you can recommend for MaxPool3d ?


"
57606,Running sample image classification binary application on iMX gives unexpected results,"Hello
I am trying to running the default label_image in the i.MX board and I am encountered with the below result. The below output continues endlessly. Can someone tell me why this is happening?

## Current Result - 
```
/usr/bin/tensorflow-lite-2.4.0/examples#  ./label_image -m mobilenet_v1_1.0_224_quant.tflite -i grace_hopper.bmp -l labels.txt -t 1 -a 1
[72017] eval=0.324 < threshold=0.650  ==> probably in-order...
[72017] eval=0.324 < threshold=0.650  ==> probably in-order...
[72017] eval=0.324 < threshold=0.650  ==> probably in-order...
[72017] eval=0.324 < threshold=0.650  ==> probably in-order...
[72017] eval=0.324 < threshold=0.650  ==> probably in-order...
```

## Expected Result - 
```
Loaded model mobilenet_v1_1.0_224_quant.tflite
resolved reporter
invoked
average time: 39.271 ms
0.780392: 653 military uniform
0.105882: 907 Windsor tie
0.0156863: 458 bow tie
0.0117647: 466 bulletproof vest
0.00784314: 835 suit
```"
57605,[Question] Does Distributed TensorFlow can be run in  independent filesystem,"I used tensorflow version 1.14 to run distributed training and failed using the **local file system.**

I saw the exchange [post in 2018 on the Internet](https://groups.google.com/a/tensorflow.org/g/discuss/c/-NdsSrWmp8w), saying that the distributed training must use the shared file system at that time, but the local file system will be supported later
![image](https://user-images.githubusercontent.com/15689619/188060748-9bff0805-1799-4009-9cb3-248f7ec8960f.png)

Is there a version that supports running distributed training tasks on the local file system?
"
57604," what is this http://b/18228951 , i can not find it","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.7

### Custom Code

No

### OS Platform and Distribution

no

### Mobile device

no

### Python version

3.7

### Bazel version

no

### GCC/Compiler version

no

### CUDA/cuDNN version

no

### GPU model and memory

111

### Current Behaviour?

```shell
http://b/18228951,i find there are many http://b  in tflite code。But I can not find the url,do you send it?
```


### Standalone code to reproduce the issue

```shell
111111
```


### Relevant log output

```shell
1111
```
</details>"
57601,Memory leak: tf1 trained saved_model in tf2 for prediction,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.6

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.6.9

### Bazel version

_No response_

### GCC/Compiler version

tensorflow  docker image

### CUDA/cuDNN version

tensorflow docker image

### GPU model and memory

tensorflow docker image

### Current Behaviour?

```shell
A bug happened!

I have previously trained ssd_inception_v2 model in tensorflow 1.14.
It has frozen_inference graph and saved_model dir with protobuf files and variables.
I am running tensorflow 2.6.0. loading tf 1.14 trained saved_model into tf 2.6
is done without problem and it runs smoothly. But over the period of time, 
cpu memory keeps increasing and after some time, prediction scrip crashes 
because of memory full. I have tried to load ""frozen graph.pb"" instead of 
saved_model.pb and problem still exists. Any help would be appreciated.
Using ""htop"" command, MEM% column keep increasing over the
time with following script running.
```


### Standalone code to reproduce the issue

Any tensorflow 1 trained model with saved_model dir after training.
sample: wget http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz

use saved_model dir.

Docker image: docker pull tensorflow/tensorflow:2.6.0-gpu-jupyter
```shell
import numpy as np

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
tf.config.set_soft_device_placement(True)
output_tensor_dict_global = None

model_path = ""saved_model_path""

gpus = tf.config.experimental.list_physical_devices('GPU')

def get_output_tensor_dict():
    global output_tensor_dict_global
    if output_tensor_dict_global:
        return output_tensor_dict_global

    ops = tf.get_default_graph().get_operations()
    all_tensor_names = {output.name for op in ops for output in op.outputs}
    tensor_dict = {}
    for key in ['num_detections', 'detection_boxes', 'detection_scores', 'detection_classes', 'detection_masks']:
        tensor_name = key + ':0'
        if tensor_name in all_tensor_names:
            tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
                tensor_name)
    output_tensor_dict_global = tensor_dict
    return output_tensor_dict_global


if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

with tf.Graph().as_default() as g:
    with tf.Session() as sess:
        detection_graph = tf.saved_model.load(sess, [""serve""], model_path)

        while True:
            img_np = np.random.randn(720,1280,3)
            tensor_dict = get_output_tensor_dict()
            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')
            output_dict = sess.run(tensor_dict, feed_dict={image_tensor: np.expand_dims(img_np, axis=0)})
            print(output_dict.keys())
          
```
I have tested same code with tensorflow 2.9.0 and problem still exists.
```


### Relevant log output

```shell
Memory consumption should be constant.
```
</details>"
57597,Load a model from a memory buffer using Tensorflow 2.x C api,"Hi, 
I was wondering if there is a way to load 2.x model from memory buffer like GraphDef in v1

Thanks
"
57587,"Unncessary semicolons in tutorials terminate statements, possible documentation improvement","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The documentation found at `https://www.tensorflow.org/guide/basics` under heading 'Training loops' as well as additional documentation on that tutorial/guide page have unnecessary semicolons (`;`) at end of statements, not necessary for Python syntactically. 

For example, `plt.legend();` under 'Training loops' heading throws warning errors in certain IDEs with AI-assisted code review such as VSCode because of the superfluous `;`.

I kindly recommend discontinuation of unnecessary semicolons and revision of this page to remove all that are not needed per proper syntax or convention. Kindly let me know if any more information is needed. Thank you for consideration.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import matplotlib
from matplotlib import pyplot as plt
matplotlib.rcParams['figure-figsize'] = [9, 6]

x = tf.linspace(-2, 2, 201)
x = tf.cast(x, tf.float32)

def f(x):
  y = x**2 + 2*x - 5
  return y

y = f(x) + tf.random.normal(shape=[201])

plt.plot(x.numpy(), y.numpy(), '.', label='Data')
plt.plot(x, f(x), label='Ground truth')
plt.legend();

_excerpted from: https://www.tensorflow.org/guide/basics_
```


### Relevant log output

```shell
n/a
```
</details>"
57584,Can't build project with included TensorFlow Lite in Play Services (vision task),"Gradle version: 7.3.3
Full code: https://github.com/vladd11/ar-shop/

**Standalone code to reproduce the issue**

I include TensorFlow Lite in Play Services dependency:

```
dependencies {
    ...
    implementation 'org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03'
}
```

When syncing, Gradle gives me these warnings:
```
Failed to resolve: org.tensorflow:tensorflow-lite-api:0.0.0-nightly-20220719.054020-244
Failed to resolve: org.tensorflow:tensorflow-lite-support-api:0.0.0-nightly-20220719.045912-252
Failed to resolve: org.tensorflow:tensorflow-lite-task-base:0.0.0-nightly-20220719.045922-401
```

When I build project, I have this error:
```
Executing tasks: [:app:assembleDebug] in project /home/vladislav/AndroidStudioProjects/ARshop

> Task :app:preBuild UP-TO-DATE
> Task :app:preDebugBuild UP-TO-DATE
> Task :app:mergeDebugNativeDebugMetadata NO-SOURCE
> Task :app:compileDebugAidl NO-SOURCE
> Task :app:compileDebugRenderscript NO-SOURCE
> Task :app:generateDebugBuildConfig UP-TO-DATE
> Task :app:generateSafeArgsDebug UP-TO-DATE
> Task :app:checkDebugAarMetadata FAILED
> Task :app:generateDebugResValues UP-TO-DATE
> Task :app:generateDebugResources UP-TO-DATE
> Task :app:mergeDebugResources FAILED
> Task :app:packageDebugResources UP-TO-DATE
> Task :app:parseDebugLocalResources UP-TO-DATE
> Task :app:createDebugCompatibleScreenManifests UP-TO-DATE
> Task :app:extractDeepLinksDebug UP-TO-DATE
> Task :app:processDebugMainManifest FAILED
> Task :app:javaPreCompileDebug UP-TO-DATE
> Task :app:mergeDebugShaders UP-TO-DATE
> Task :app:compileDebugShaders NO-SOURCE
> Task :app:generateDebugAssets UP-TO-DATE
> Task :app:mergeDebugAssets FAILED
> Task :app:processDebugJavaRes NO-SOURCE
> Task :app:checkDebugDuplicateClasses FAILED
> Task :app:desugarDebugFileDependencies FAILED
> Task :app:extractNativeLibraries
> Task :app:configureCMakeDebug[arm64-v8a]
> Task :app:buildCMakeDebug[arm64-v8a]
> Task :app:configureCMakeDebug[armeabi-v7a]
> Task :app:buildCMakeDebug[armeabi-v7a]
> Task :app:configureCMakeDebug[x86]
> Task :app:buildCMakeDebug[x86]
> Task :app:externalNativeBuildDebug
> Task :app:mergeDebugJniLibFolders UP-TO-DATE
> Task :app:mergeDebugNativeLibs FAILED
> Task :app:validateSigningDebug UP-TO-DATE
> Task :app:writeDebugAppMetadata UP-TO-DATE
> Task :app:writeDebugSigningConfigVersions UP-TO-DATE

FAILURE: Build completed with 7 failures.

1: Task failed with an exception.
-----------
* What went wrong:
Execution failed for task ':app:checkDebugAarMetadata'.
> Could not resolve all files for configuration ':app:debugRuntimeClasspath'.
   > Could not find org.tensorflow:tensorflow-lite-api:0.0.0-nightly-20220719.054020-244.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-api-0.0.0-nightly-20220719.054020-244.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-api-0.0.0-nightly-20220719.054020-244.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03 > com.google.android.gms:play-services-tflite-support:16.0.0-beta03 > com.google.android.gms:play-services-tflite-java:16.0.0-beta03
   > Could not find org.tensorflow:tensorflow-lite-support-api:0.0.0-nightly-20220719.045912-252.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-support-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-support-api-0.0.0-nightly-20220719.045912-252.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-support-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-support-api-0.0.0-nightly-20220719.045912-252.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03 > com.google.android.gms:play-services-tflite-support:16.0.0-beta03
   > Could not find org.tensorflow:tensorflow-lite-task-base:0.0.0-nightly-20220719.045922-401.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-task-base/0.0.0-nightly-SNAPSHOT/tensorflow-lite-task-base-0.0.0-nightly-20220719.045922-401.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-task-base/0.0.0-nightly-SNAPSHOT/tensorflow-lite-task-base-0.0.0-nightly-20220719.045922-401.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03

* Try:
> Run with --stacktrace option to get the stack trace.
> Run with --info or --debug option to get more log output.
> Run with --scan to get full insights.
==============================================================================

2: Task failed with an exception.
-----------
* What went wrong:
Execution failed for task ':app:mergeDebugResources'.
> Could not resolve all files for configuration ':app:debugRuntimeClasspath'.
   > Could not find org.tensorflow:tensorflow-lite-api:0.0.0-nightly-20220719.054020-244.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-api-0.0.0-nightly-20220719.054020-244.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-api-0.0.0-nightly-20220719.054020-244.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03 > com.google.android.gms:play-services-tflite-support:16.0.0-beta03 > com.google.android.gms:play-services-tflite-java:16.0.0-beta03
   > Could not find org.tensorflow:tensorflow-lite-support-api:0.0.0-nightly-20220719.045912-252.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-support-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-support-api-0.0.0-nightly-20220719.045912-252.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-support-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-support-api-0.0.0-nightly-20220719.045912-252.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03 > com.google.android.gms:play-services-tflite-support:16.0.0-beta03
   > Could not find org.tensorflow:tensorflow-lite-task-base:0.0.0-nightly-20220719.045922-401.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-task-base/0.0.0-nightly-SNAPSHOT/tensorflow-lite-task-base-0.0.0-nightly-20220719.045922-401.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-task-base/0.0.0-nightly-SNAPSHOT/tensorflow-lite-task-base-0.0.0-nightly-20220719.045922-401.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03

* Try:
> Run with --stacktrace option to get the stack trace.
> Run with --info or --debug option to get more log output.
> Run with --scan to get full insights.
==============================================================================

3: Task failed with an exception.
-----------
* What went wrong:
Execution failed for task ':app:processDebugMainManifest'.
> Could not resolve all files for configuration ':app:debugRuntimeClasspath'.
   > Could not find org.tensorflow:tensorflow-lite-api:0.0.0-nightly-20220719.054020-244.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-api-0.0.0-nightly-20220719.054020-244.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-api-0.0.0-nightly-20220719.054020-244.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03 > com.google.android.gms:play-services-tflite-support:16.0.0-beta03 > com.google.android.gms:play-services-tflite-java:16.0.0-beta03
   > Could not find org.tensorflow:tensorflow-lite-support-api:0.0.0-nightly-20220719.045912-252.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-support-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-support-api-0.0.0-nightly-20220719.045912-252.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-support-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-support-api-0.0.0-nightly-20220719.045912-252.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03 > com.google.android.gms:play-services-tflite-support:16.0.0-beta03
   > Could not find org.tensorflow:tensorflow-lite-task-base:0.0.0-nightly-20220719.045922-401.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-task-base/0.0.0-nightly-SNAPSHOT/tensorflow-lite-task-base-0.0.0-nightly-20220719.045922-401.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-task-base/0.0.0-nightly-SNAPSHOT/tensorflow-lite-task-base-0.0.0-nightly-20220719.045922-401.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03

* Try:
> Run with --stacktrace option to get the stack trace.
> Run with --info or --debug option to get more log output.
> Run with --scan to get full insights.
==============================================================================

4: Task failed with an exception.
-----------
* What went wrong:
Execution failed for task ':app:mergeDebugAssets'.
> Could not resolve all files for configuration ':app:debugRuntimeClasspath'.
   > Could not find org.tensorflow:tensorflow-lite-api:0.0.0-nightly-20220719.054020-244.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-api-0.0.0-nightly-20220719.054020-244.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-api-0.0.0-nightly-20220719.054020-244.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03 > com.google.android.gms:play-services-tflite-support:16.0.0-beta03 > com.google.android.gms:play-services-tflite-java:16.0.0-beta03
   > Could not find org.tensorflow:tensorflow-lite-support-api:0.0.0-nightly-20220719.045912-252.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-support-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-support-api-0.0.0-nightly-20220719.045912-252.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-support-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-support-api-0.0.0-nightly-20220719.045912-252.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03 > com.google.android.gms:play-services-tflite-support:16.0.0-beta03
   > Could not find org.tensorflow:tensorflow-lite-task-base:0.0.0-nightly-20220719.045922-401.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-task-base/0.0.0-nightly-SNAPSHOT/tensorflow-lite-task-base-0.0.0-nightly-20220719.045922-401.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-task-base/0.0.0-nightly-SNAPSHOT/tensorflow-lite-task-base-0.0.0-nightly-20220719.045922-401.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03

* Try:
> Run with --stacktrace option to get the stack trace.
> Run with --info or --debug option to get more log output.
> Run with --scan to get full insights.
==============================================================================

5: Task failed with an exception.
-----------
* What went wrong:
Execution failed for task ':app:checkDebugDuplicateClasses'.
> Could not resolve all files for configuration ':app:debugRuntimeClasspath'.
   > Could not find org.tensorflow:tensorflow-lite-api:0.0.0-nightly-20220719.054020-244.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-api-0.0.0-nightly-20220719.054020-244.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-api-0.0.0-nightly-20220719.054020-244.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03 > com.google.android.gms:play-services-tflite-support:16.0.0-beta03 > com.google.android.gms:play-services-tflite-java:16.0.0-beta03
   > Could not find org.tensorflow:tensorflow-lite-support-api:0.0.0-nightly-20220719.045912-252.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-support-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-support-api-0.0.0-nightly-20220719.045912-252.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-support-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-support-api-0.0.0-nightly-20220719.045912-252.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03 > com.google.android.gms:play-services-tflite-support:16.0.0-beta03
   > Could not find org.tensorflow:tensorflow-lite-task-base:0.0.0-nightly-20220719.045922-401.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-task-base/0.0.0-nightly-SNAPSHOT/tensorflow-lite-task-base-0.0.0-nightly-20220719.045922-401.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-task-base/0.0.0-nightly-SNAPSHOT/tensorflow-lite-task-base-0.0.0-nightly-20220719.045922-401.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03

* Try:
> Run with --stacktrace option to get the stack trace.
> Run with --info or --debug option to get more log output.
> Run with --scan to get full insights.
==============================================================================

6: Task failed with an exception.
-----------
* What went wrong:
Execution failed for task ':app:desugarDebugFileDependencies'.
> Could not resolve all files for configuration ':app:debugRuntimeClasspath'.
   > Could not find org.tensorflow:tensorflow-lite-api:0.0.0-nightly-20220719.054020-244.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-api-0.0.0-nightly-20220719.054020-244.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-api-0.0.0-nightly-20220719.054020-244.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03 > com.google.android.gms:play-services-tflite-support:16.0.0-beta03 > com.google.android.gms:play-services-tflite-java:16.0.0-beta03
   > Could not find org.tensorflow:tensorflow-lite-support-api:0.0.0-nightly-20220719.045912-252.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-support-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-support-api-0.0.0-nightly-20220719.045912-252.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-support-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-support-api-0.0.0-nightly-20220719.045912-252.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03 > com.google.android.gms:play-services-tflite-support:16.0.0-beta03
   > Could not find org.tensorflow:tensorflow-lite-task-base:0.0.0-nightly-20220719.045922-401.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-task-base/0.0.0-nightly-SNAPSHOT/tensorflow-lite-task-base-0.0.0-nightly-20220719.045922-401.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-task-base/0.0.0-nightly-SNAPSHOT/tensorflow-lite-task-base-0.0.0-nightly-20220719.045922-401.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03

* Try:
> Run with --stacktrace option to get the stack trace.
> Run with --info or --debug option to get more log output.
> Run with --scan to get full insights.
==============================================================================

7: Task failed with an exception.
-----------
* What went wrong:
Execution failed for task ':app:mergeDebugNativeLibs'.
> Could not resolve all files for configuration ':app:debugRuntimeClasspath'.
   > Could not find org.tensorflow:tensorflow-lite-api:0.0.0-nightly-20220719.054020-244.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-api-0.0.0-nightly-20220719.054020-244.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-api-0.0.0-nightly-20220719.054020-244.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03 > com.google.android.gms:play-services-tflite-support:16.0.0-beta03 > com.google.android.gms:play-services-tflite-java:16.0.0-beta03
   > Could not find org.tensorflow:tensorflow-lite-support-api:0.0.0-nightly-20220719.045912-252.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-support-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-support-api-0.0.0-nightly-20220719.045912-252.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-support-api/0.0.0-nightly-SNAPSHOT/tensorflow-lite-support-api-0.0.0-nightly-20220719.045912-252.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03 > com.google.android.gms:play-services-tflite-support:16.0.0-beta03
   > Could not find org.tensorflow:tensorflow-lite-task-base:0.0.0-nightly-20220719.045922-401.
     Searched in the following locations:
       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-task-base/0.0.0-nightly-SNAPSHOT/tensorflow-lite-task-base-0.0.0-nightly-20220719.045922-401.pom
       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-task-base/0.0.0-nightly-SNAPSHOT/tensorflow-lite-task-base-0.0.0-nightly-20220719.045922-401.pom
     Required by:
         project :app > org.tensorflow:tensorflow-lite-task-vision-play-services:0.4.2-beta03

* Try:
> Run with --stacktrace option to get the stack trace.
> Run with --info or --debug option to get more log output.
> Run with --scan to get full insights.
==============================================================================

* Get more help at https://help.gradle.org

BUILD FAILED in 6s
28 actionable tasks: 15 executed, 13 up-to-date
```"
57583,Tensorflow customized embedding not work on GPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.3.0-rc2-23-gb36436b087 2.3.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 17.04

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

10.1 / 7.6.5

### GPU model and memory

Tesla-V100 32G

### Current Behaviour?

```shell
I made a tensorflow model like following:

class EnhancedEmbedding(tf.keras.layers.Embedding):
    def __init__(self, input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None, **kwargs):
        super().__init__(input_dim, output_dim, embeddings_initializer, embeddings_regularizer, activity_regularizer, embeddings_constraint, mask_zero, input_length, **kwargs)
        self.five=tf.constant(0.5)
        self.zero=tf.constant(0)
    
    def embedding(self,inputs):
        return super().call(inputs)
    
    def map_2(self,tokens):
        identifier=self.embedding(tokens[0])
        cur_word=self.embedding(tokens[2])
        return tf.cond(tf.equal(tf.shape(tokens[0]),self.zero),
                lambda: tf.squeeze(cur_word),
                lambda: tf.squeeze(tf.reduce_mean(identifier)*self.five+cur_word*self.five))


    def map_1(self,inputs):
        return tf.map_fn(fn=lambda x :self.map_2(x),elems=inputs,dtype=tf.float32)

    def call(self, inputs):
        final_embeddings=tf.map_fn(fn=lambda x:self.map_1(x),elems=inputs,dtype=tf.float32)

        return final_embeddings
```
```
class EnhancedModel(Model):
    def __init__(self,  embedding_dim, hidden_dim, vocab_size, label_size,seq_len, pretrained_weight):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.label_size = label_size
        self.activation = tf.keras.activations.tanh
        self.num_layers = 1

        self.embedding=EnhancedEmbedding(vocab_size,embedding_dim,embeddings_initializer=keras.initializers.Constant(pretrained_weight))
        self.encoder = Bidirectional(LSTM(hidden_dim, return_sequences=True,input_shape=(seq_len,embedding_dim)))
        self.pool=MaxPool1D(hidden_dim*2)
        self.decoder = Dense(self.label_size)

    def call(self, inputs):
        embeddings = self.embedding(inputs)
        embeddings=tf.reshape(embeddings,[-1,400,32])
        lstm_out = self.encoder(embeddings)
        lstm_out = tf.transpose(lstm_out, perm=[0,2,1])
        pool_out=self.pool(lstm_out)
        out = tf.squeeze(pool_out,[1])
        out = self.decoder(out)
        return out
```
Actually it is a model that cited from others. I only changed the embedding layer and the input.
the input is a RaggedTensor with shape(batch_size,400,3,None), and i use ``` and ```map_2``` function in ```EnhancedEmbedding``` to general final embedding for the following layers. Everything works fine except that the model is not work with GPU. I looked the GPU utilization using ```nvidia-smi``` and the result is about 5%. But CPU-Util is about 100%.

GPU can be used in the virtual env:
```
>>>tf.test.is_gpu_available()
>>>True
```
```


### Standalone code to reproduce the issue

```shell
the training code is here but it cannot be reproduced since the pretrained vectors, the training data are too large.

https://colab.research.google.com/drive/1qdo629GsnIdgbFpqTbIrwOaIHCsfwyJr?usp=sharing
```


### Relevant log output

_No response_</details>"
57582,Memory allocation,"Hi sorry for bothering. I am just wondering when using multiple streams for model inference. Will the cached memory blocks been shared by different streams? Or they are not shared like in pytorch?
"
57581,the edtion of TensorRT for TF-TRT,"<img width=""936"" alt=""image"" src=""https://user-images.githubusercontent.com/75348594/187877984-f321d97a-92a1-4953-a0fa-703626c106a4.png"">
On the website, I can know the edtion of cuda and cudnn, but how can I know the edtion of TensorRT?"
57580,Pose estimation line mismatch,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

 pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['CoreML', 'Metal']

### Custom Code

Yes

### OS Platform and Distribution

MacOS

### Mobile device

iPhone X

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I am performing some exercise pose estimation model lines display cross.
```


### Standalone code to reproduce the issue

```shell
When I am performing Squat sometimes lines display cross.
```


### Relevant log output

_No response_</details>"
57579,tf.linalg.solve produces incorrect output when compiled using XLA,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.linalg.solve when compiled with XLA produces different (incorrect) output when compared to a standalone TF execution.

In the sample code below, the solveEquation function solves for Ax = B and returns Ax. This function is run with and without XLA enabled. Both executions produce very different outputs!
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

data_type = tf.float32

def solveEquation(A, B):

    ATA = tf.matmul(A, A, transpose_a=True)
    ATB = tf.matmul(A, B, transpose_a=True)

    output = tf.linalg.solve(ATA, ATB)

    return (tf.matmul(A, output))

P = 2
Q = 3
R = 5

np.random.seed(0)
mat1 = tf.constant(np.random.random((P, Q)), dtype=data_type)
mat2 = tf.constant(np.random.random((P, R)), dtype=data_type)

output_standard = solveEquation(mat1, mat2)

tf_func_xla = tf.function(func=solveEquation, 
                          input_signature=[tf.TensorSpec(shape=(P,Q),dtype=data_type),
                                           tf.TensorSpec(shape=(P,R),dtype=data_type)], 
                          jit_compile=True)
output_xla = tf_func_xla(mat1, mat2)

tf.print(output_standard)
tf.print(output_xla)
```


### Relevant log output

```shell
# output with XLA disabled
[[0.43758738 0.891772866 0.963661969 0.383441627 0.791725397]
 [0.528894722 0.568044662 0.92559737 0.0710358918 0.0871287584]]

# output with XLA enabled
[[0.274795085 0.876262307 0.82626003 0.345206 0.581315041]
 [0.3589876 0.553595543 0.778286219 0.0325973332 -0.126585901]]
```
</details>"
57575,Make tf.image.resize gradient computation compatible with XLA compilation," ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

tf 2.11.0-dev20220831

### Custom Code

No

### Current Behaviour?

Currently, XLA compilation does not work for gradient computation of `tf.image.resize`, for example for the default method `ResizeMethod.BILINEAR`:

> ResizeBilinearGrad with align_corners=False or half_pixel_centers=True is not yet implemented

Support for forward pass has recently been added: https://github.com/tensorflow/tensorflow/issues/46447

This makes it impossible to XLA compile models for training that use resizing, such as RetinaNet.


### Standalone code to reproduce the issue

[Colab to reproduce error](https://colab.research.google.com/drive/1eevpjtJYqQ7Mkg7PujJu_I5UOfL1PnWX?usp=sharing)
"
57574,Cannot deserialize keras model using __add__,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

v2.9.1-0-gd8ce9f9c301 2.9.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

5.0.0

### GCC/Compiler version

GCC 9.2

### CUDA/cuDNN version

11.7/8.4.1.50

### GPU model and memory

A100-SXM4-40GB

### Current Behaviour?

When calling `tf.keras.models.load_model(""/tmp/my_model"")` I get

```
File ""/mnt/disks/data/src/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py"", line 1078, in op_dispatch_handler
    result = api_dispatcher.Dispatch(args, kwargs)
TypeError: Missing required positional argument
```

I expected the model to load successfully. I trained my model for 2 days and now I can't load it.


### Standalone code to reproduce the issue

```python
import tensorflow as tf
from tensorflow.keras.layers import Dense, Layer

class MyLayer(Layer):
    def __init__(self):
        super().__init__()
        self.dense = Dense(512)
    def __call__(self, X):
        short = X
        X = self.dense(X)
        X = short + X
        return X


def main():
    model = tf.keras.Sequential([MyLayer()])
    model.build([None, 512])
    model.save(""/tmp/my_model"")

    tf.keras.models.load_model(""/tmp/my_model"")

if __name__ == ""__main__"":
    main()
```


### Relevant log output

```shell
api_dispatcher.Dispatch=<bound method PyCapsule.Dispatch of <Dispatch(_add_dispatch): >, args=(<tf.Tensor 'Placeholder:0' shape=(None, 512) dtype=float32>,), kwargs={}
Traceback (most recent call last):
  File ""/mnt/disks/data/src/repro.py"", line 24, in <module>
    main()
  File ""/mnt/disks/data/src/repro.py"", line 21, in main
    tf.keras.models.load_model(""/tmp/my_model"")
  File ""/mnt/disks/data/src/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/mnt/disks/data/src/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py"", line 1078, in op_dispatch_handler
    result = api_dispatcher.Dispatch(args, kwargs)
TypeError: Missing required positional argument
```
</details>"
57573,Tried to export a function which references 'untracked' resource Tensor,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8.2

### Custom Code

Yes

### OS Platform and Distribution

Colab

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am not able to export models with a custom signature that contains preprocessing layers as part of the model graph.

This issue happens for different kinds of layers, in my case it was `Normalization`, `IntegerLookup` and `StringLookup`.
```


### Standalone code to reproduce the issue

```shell
Link for colab: https://colab.research.google.com/drive/15pPBp8uVBMpdHscljUz1kk_OylDp43-G?usp=sharing


def model_fn():
    age_input = L.Input(shape=(1,), dtype=tf.float32, name='age')
    sex_input = L.Input(shape=(1,), dtype=tf.float32, name='sex')
    cp_input = L.Input(shape=(len(cp_vocab),), dtype=tf.float32, name='cp')
    thal_input = L.Input(shape=(len(thal_vocab),), dtype=tf.float32, name='thal')

    concat_inputs = L.Concatenate()([age_input, sex_input, cp_input, thal_input])
    x = L.Dense(32, activation=""relu"")(concat_inputs)
    x = L.Dropout(0.5)(x)
    output = L.Dense(1, activation=""sigmoid"")(x)

    return tf.keras.models.Model(inputs=[age_input, sex_input, cp_input, thal_input], outputs=output)

model = model_fn()

def custom_signature(model):
    input_signature = [
        tf.TensorSpec([1,], tf.int64, name=""age""), 
        tf.TensorSpec([1,], tf.int64, name=""sex""), 
        tf.TensorSpec([1,], tf.int64, name=""cp""), 
        tf.TensorSpec([1,], tf.string, name=""thal"")
    ]
    @tf.function(input_signature=[input_signature])
    def serving_fn(input):
        age_processed = age_preprocessing(input[0])
        sex_processed = tf.cast(sex_preprocessing(input[1]), 
                                dtype=tf.float32)
        cp_processed = cp_preprocessing(input[2])
        thal_processed = thal_preprocessing(input[3])

        preprocessed_inputs = {""age"": age_processed, 
                              ""sex"": sex_processed, 
                              ""cp"": tf.expand_dims(cp_processed, 0), 
                              ""thal"": tf.expand_dims(thal_processed, 0)}
        probs = model(preprocessed_inputs)
        return {""probs"": probs}

    return serving_fn

tf.saved_model.save(
        obj=model,
        export_dir=""signature_model/"",
        signatures={
            ""serving_default"": custom_signature(model)
        },
    )
```
```


### Relevant log output

```shell
AssertionError: Tried to export a function which references 'untracked' resource Tensor(""3629:0"", shape=(), dtype=resource). TensorFlow objects (e.g. tf.Variable) captured by functions must be 'tracked' by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.

 Trackable Python objects referring to this tensor (from gc.get_referrers, limited to two hops):
<tensorflow.python.ops.lookup_ops.StaticHashTable object at 0x7f1d051b2e10>
```
</details>"
57566,Not enough flexibility to choose actual accelerator in CoreML delegate,"Only [two options exist](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/coreml_delegate.h) when creating a CoreML delegate:
```
typedef enum {
  // Create Core ML delegate only on devices with Apple Neural Engine.
  // Returns nullptr otherwise.
  TfLiteCoreMlDelegateDevicesWithNeuralEngine,
  // Always create Core ML delegate
  TfLiteCoreMlDelegateAllDevices
} TfLiteCoreMlDelegateEnabledDevices;
```

These don't quite fit what's possible with CoreML, [as documented here](https://developer.apple.com/documentation/coreml/mlcomputeunits?changes=latest_major&language=objc). Ideally, we should have a way to specify whether we want to use any accelerator (including neural engine), CPU only, or CPU and GPU (excluding neural engine). iOS 16 will also introduce a way to use CPU and neural engine, i.e. excluding GPU.

Currently the CoreML delegate always uses `MLComputeUnitsAll` in [coreml_executor.mm](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/coreml_executor.mm).
"
57565,TfLiteCoreMlDelegateOptions documentation is not up-to-date,"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/coreml

It currently shows the following structure, which is not up to date:
```
typedef struct {
 // We have dummy for now as we can't have empty struct in C.
 char dummy;
} TfLiteCoreMlDelegateOptions;
```

See current definition in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/coreml/coreml_delegate.h.
"
57561,Where can I find the list of ops from TensorFlow's Python API supported by default in TensorFlowLite,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Hi. I'm trying to figure out what from TensorFlow's Python API is supported by TensorFlowLite. I know there's a list of [supported MLIR ops](https://www.tensorflow.org/mlir/tfl_ops), but that is still one level of indirection from the Python API. I've also found other locations where the builtins are specified e.g. in the [generated schema](https://github.com/tensorflow/tensorflow/blob/681b73dd2e4216d883007139a53beb6c2f60cf94/tensorflow/lite/schema/schema_generated.h#L1231) or say the kernels specified in [tensorflow/lite/kernels/BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/BUILD#L562) but again, I'm not sure how these names map to TensorFlow's Python API.

Is there a location in the code or a script I can run to find the mapping?


### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

_No response_</details>"
57558,hlo-legalize-to-linalg crashes on convolution op,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

SHA 551852a9ea9bf4e99856ce75c63516ad6d372239

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


I found a bug in `hlo-legalize-to-linalg` where it will crash for certain convolution operators.  Here's one example of an op that causes the crash

```shell
func.func @main(%arg0: tensor<1x13x13x32xf32>, %arg1: tensor<1x11x11x64xf32>) -> tensor<3x3x32x64xf32> {
  %0 = ""mhlo.convolution""(%arg0, %arg1) {batch_group_count = 1 : i64, dimension_numbers = #mhlo.conv<[f, 0, 1, b]x[i, 0, 1, o]->[0, 1, b, f]>, feature_group_count = 1 : i64, lhs_dilation = dense<1> : tensor<2xi64>, padding = dense<0> : tensor<2x2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_strides = dense<1> : tensor<2xi64>} : (tensor<1x13x13x32xf32>, tensor<1x11x11x64xf32>) -> tensor<3x3x32x64xf32>
  return %0 : tensor<3x3x32x64xf32>
}
```

The crash appears to happen on this line https://github.com/tensorflow/tensorflow/commit/763d55e7e33ca888a2aa14fa89ec413047daf1e5#diff-524cc00a6c4fc699fbad5fe46d08167e30aecdb4d19edcded46028f17f1f4837R2247.  I printed out the expressions in all 3 arguments and it appears that dstExpr still has a null in it.  Here's what they look like.

```
srcExprs
d0
d2 + d3
d4 + d5
d6
windowExprs
d0
d3
d5
d1
dstExprs
d2
d4
<<NULL AFFINE EXPR>>
d6
```
Run command in standalone code section on the IR provided above


### Standalone code to reproduce the issue

```shell
tf-opt --hlo-legalize-to-linalg
```


### Relevant log output

```shell
Stack dump:
0.      Program arguments: tf-opt --hlo-legalize-to-linalg convolution.mlir
1.      Program arguments: tf-opt --hlo-legalize-to-linalg convolution.mlir
 #0 0x00007f089dc9a437 llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/../../../_solib_k8/_U_S_Stensorflow_Scompiler_Smlir_Ctf-opt___Utensorflow/libtensorflow_framework.so.2+0x1382437)
 #1 0x00007f089dc97a35 llvm::sys::RunSignalHandlers() (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/../../../_solib_k8/_U_S_Stensorflow_Scompiler_Smlir_Ctf-opt___Utensorflow/libtensorflow_framework.so.2+0x137fa35)
 #2 0x00007f089dc98a85 SignalHandler(int) Signals.cpp:0:0
 #3 0x00007f089c78f3c0 __restore_rt (/lib/x86_64-linux-gnu/libpthread.so.0+0x143c0)
 #4 0x00005616013e56aa mlir::AffineExpr::walk(std::function<void (mlir::AffineExpr)>) const (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10f026aa)
 #5 0x00005616013ef532 mlir::AffineMap::inferFromExprList(llvm::ArrayRef<llvm::ArrayRef<mlir::AffineExpr>>) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10f0c532)
 #6 0x00005615fbdcf44e mlir::mhlo::(anonymous namespace)::ConvolutionOpGeneralConversion::matchAndRewrite(mlir::mhlo::ConvolutionOp, mlir::mhlo::ConvolutionOpAdaptor, mlir::ConversionPatternRewriter&) const legalize_to_linalg.cc:0:0
 #7 0x00005615fbcff44b mlir::OpConversionPattern<mlir::mhlo::ConvolutionOp>::matchAndRewrite(mlir::Operation*, llvm::ArrayRef<mlir::Value>, mlir::ConversionPatternRewriter&) const (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xb81c44b)
 #8 0x000056160124d76d mlir::ConversionPattern::matchAndRewrite(mlir::Operation*, mlir::PatternRewriter&) const (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10d6a76d)
 #9 0x00005616012a7243 mlir::PatternApplicator::matchAndRewrite(mlir::Operation*, mlir::PatternRewriter&, llvm::function_ref<bool (mlir::Pattern const&)>, llvm::function_ref<void (mlir::Pattern const&)>, llvm::function_ref<mlir::LogicalResult (mlir::Pattern const&)>) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10dc4243)
#10 0x00005616012584ec (anonymous namespace)::OperationLegalizer::legalize(mlir::Operation*, mlir::ConversionPatternRewriter&) DialectConversion.cpp:0:0
#11 0x000056160125e257 (anonymous namespace)::OperationConverter::convertOperations(llvm::ArrayRef<mlir::Operation*>, llvm::function_ref<void (mlir::Diagnostic&)>) DialectConversion.cpp:0:0
#12 0x000056160125e5f1 mlir::applyPartialConversion(llvm::ArrayRef<mlir::Operation*>, mlir::ConversionTarget&, mlir::FrozenRewritePatternSet const&, llvm::DenseSet<mlir::Operation*, llvm::DenseMapInfo<mlir::Operation*, void>>*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10d7b5f1)
#13 0x000056160125e677 mlir::applyPartialConversion(mlir::Operation*, mlir::ConversionTarget&, mlir::FrozenRewritePatternSet const&, llvm::DenseSet<mlir::Operation*, llvm::DenseMapInfo<mlir::Operation*, void>>*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10d7b677)
#14 0x00005615fbdd6a98 mlir::mhlo::(anonymous namespace)::HloLegalizeToLinalgPass::runOnOperation() legalize_to_linalg.cc:0:0
#15 0x000056160138d792 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eaa792)
#16 0x000056160138ea42 mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eaba42)
#17 0x000056160138ca05 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10ea9a05)
#18 0x000056160138d3a2 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eaa3a2)
#19 0x000056160138ea42 mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eaba42)
#20 0x000056160138f5a6 mlir::PassManager::run(mlir::Operation*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eac5a6)
#21 0x00005615fcf7cfd3 performActions(llvm::raw_ostream&, bool, bool, llvm::SourceMgr&, mlir::MLIRContext*, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>) (.constprop.0) MlirOptMain.cpp:0:0
#22 0x00005615fcf7d526 processBuffer(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, bool, bool, bool, bool, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, llvm::ThreadPool*) MlirOptMain.cpp:0:0
#23 0x00005615fcf7d731 mlir::LogicalResult llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>::callback_fn<mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, bool, bool, bool, bool, bool)::'lambda'(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>(long, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&) MlirOptMain.cpp:0:0
#24 0x00005616014d9d73 mlir::splitAndProcessBuffer(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>, llvm::raw_ostream&, bool, bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10ff6d73)
#25 0x00005615fcf7ca72 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, bool, bool, bool, bool, bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xca99a72)
#26 0x00005615fcf7cb23 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::PassPipelineCLParser const&, mlir::DialectRegistry&, bool, bool, bool, bool, bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xca99b23)
#27 0x00005615fcf7dfa4 mlir::MlirOptMain(int, char**, llvm::StringRef, mlir::DialectRegistry&, bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xca9afa4)
#28 0x00005615fb7001fb main (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xb21d1fb)
#29 0x00007f089c3b00b3 __libc_start_main /build/glibc-sMfBJT/glibc-2.31/csu/../csu/libc-start.c:342:3
#30 0x00005615f3bae94e _start (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x36cb94e)
 #0 0x00007f089dc9a437 llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/../../../_solib_k8/_U_S_Stensorflow_Scompiler_Smlir_Ctf-opt___Utensorflow/libtensorflow_framework.so.2+0x1382437)
 #1 0x00007f089dc97a35 llvm::sys::RunSignalHandlers() (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/../../../_solib_k8/_U_S_Stensorflow_Scompiler_Smlir_Ctf-opt___Utensorflow/libtensorflow_framework.so.2+0x137fa35)
 #2 0x00007f089dc98a85 SignalHandler(int) Signals.cpp:0:0
 #3 0x00007f089c78f3c0 __restore_rt (/lib/x86_64-linux-gnu/libpthread.so.0+0x143c0)
 #4 0x00005616013e56aa mlir::AffineExpr::walk(std::function<void (mlir::AffineExpr)>) const (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10f026aa)
 #5 0x00005616013ef532 mlir::AffineMap::inferFromExprList(llvm::ArrayRef<llvm::ArrayRef<mlir::AffineExpr>>) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10f0c532)
 #6 0x00005615fbdcf44e mlir::mhlo::(anonymous namespace)::ConvolutionOpGeneralConversion::matchAndRewrite(mlir::mhlo::ConvolutionOp, mlir::mhlo::ConvolutionOpAdaptor, mlir::ConversionPatternRewriter&) const legalize_to_linalg.cc:0:0
 #7 0x00005615fbcff44b mlir::OpConversionPattern<mlir::mhlo::ConvolutionOp>::matchAndRewrite(mlir::Operation*, llvm::ArrayRef<mlir::Value>, mlir::ConversionPatternRewriter&) const (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xb81c44b)
 #8 0x000056160124d76d mlir::ConversionPattern::matchAndRewrite(mlir::Operation*, mlir::PatternRewriter&) const (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10d6a76d)
 #9 0x00005616012a7243 mlir::PatternApplicator::matchAndRewrite(mlir::Operation*, mlir::PatternRewriter&, llvm::function_ref<bool (mlir::Pattern const&)>, llvm::function_ref<void (mlir::Pattern const&)>, llvm::function_ref<mlir::LogicalResult (mlir::Pattern const&)>) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10dc4243)
#10 0x00005616012584ec (anonymous namespace)::OperationLegalizer::legalize(mlir::Operation*, mlir::ConversionPatternRewriter&) DialectConversion.cpp:0:0
#11 0x000056160125e257 (anonymous namespace)::OperationConverter::convertOperations(llvm::ArrayRef<mlir::Operation*>, llvm::function_ref<void (mlir::Diagnostic&)>) DialectConversion.cpp:0:0
#12 0x000056160125e5f1 mlir::applyPartialConversion(llvm::ArrayRef<mlir::Operation*>, mlir::ConversionTarget&, mlir::FrozenRewritePatternSet const&, llvm::DenseSet<mlir::Operation*, llvm::DenseMapInfo<mlir::Operation*, void>>*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10d7b5f1)
#13 0x000056160125e677 mlir::applyPartialConversion(mlir::Operation*, mlir::ConversionTarget&, mlir::FrozenRewritePatternSet const&, llvm::DenseSet<mlir::Operation*, llvm::DenseMapInfo<mlir::Operation*, void>>*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10d7b677)
#14 0x00005615fbdd6a98 mlir::mhlo::(anonymous namespace)::HloLegalizeToLinalgPass::runOnOperation() legalize_to_linalg.cc:0:0
#15 0x000056160138d792 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eaa792)
#16 0x000056160138ea42 mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eaba42)
#17 0x000056160138ca05 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10ea9a05)
#18 0x000056160138d3a2 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eaa3a2)
#19 0x000056160138ea42 mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eaba42)
#20 0x000056160138f5a6 mlir::PassManager::run(mlir::Operation*) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10eac5a6)
#21 0x00005615fcf7cfd3 performActions(llvm::raw_ostream&, bool, bool, llvm::SourceMgr&, mlir::MLIRContext*, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>) (.constprop.0) MlirOptMain.cpp:0:0
#22 0x00005615fcf7d526 processBuffer(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, bool, bool, bool, bool, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, llvm::ThreadPool*) MlirOptMain.cpp:0:0
#23 0x00005615fcf7d731 mlir::LogicalResult llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>::callback_fn<mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, bool, bool, bool, bool, bool)::'lambda'(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>(long, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&) MlirOptMain.cpp:0:0
#24 0x00005616014d9d73 mlir::splitAndProcessBuffer(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>, llvm::raw_ostream&, bool, bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x10ff6d73)
#25 0x00005615fcf7ca72 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, bool, bool, bool, bool, bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xca99a72)
#26 0x00005615fcf7cb23 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::PassPipelineCLParser const&, mlir::DialectRegistry&, bool, bool, bool, bool, bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xca99b23)
#27 0x00005615fcf7dfa4 mlir::MlirOptMain(int, char**, llvm::StringRef, mlir::DialectRegistry&, bool) (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xca9afa4)
#28 0x00005615fb7001fb main (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0xb21d1fb)
#29 0x00007f089c3b00b3 __libc_start_main /build/glibc-sMfBJT/glibc-2.31/csu/../csu/libc-start.c:342:3
#30 0x00005615f3bae94e _start (/.cache/bazel/_bazel/1a972990889a509d49bb6421d8d05ae2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tf-opt+0x36cb94e)
Segmentation fault (core dumped)
```
</details>"
57555,NNAPI delegate issue on Snapdragon 888,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

Android

### Mobile device

Snapdragon 888 (tested with Android 12)

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
We faced two major issues on Snapdragon 888, when inferring our quantized (INT8) tflite models with the NNAPI delegate:
  
1-On devices with Snapdragon 888 (tested with Android 12), the NNAPI delegate always crashes when there is a Quantize node right before a Concatenation node.

2-On devices with Snapdragon 888 (tested with Android 12), the INT8 tflite version of a model in which the kernel size in at least one of the Dense layers is larger than a specific threshold, always crashes with the NNAPI delegate.
```


### Standalone code to reproduce the issue

```shell
We have implemented a small tool (with comprehensive documentation) to reproduce the mentioned issues. Here is the link to the repository:

https://github.com/Bahar-BM/Concat_NNAPI
```


### Relevant log output

_No response_</details>"
57554,error: 'tf.Conv2D' op is neither a custom op nor a flex op,"### 1. System information

- Windows 10:
- TensorFlow 2.9.1 installed via pip

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks
https://colab.research.google.com/gist/andreaskoelsch/c5f82bca1aa4289724dd827d4d6cf89e/tensorflow-lite-debugger-colab.ipynb

### 3. Failure after conversion
Conversion of a very simple model does not work. `error: 'tf.Conv2D' op is neither a custom op nor a flex op`
If either the `GroupNormalization` or the second `Conv2D` layer is commented, the conversion works. The combination of the two seems to be a problem.
"
57553,Saving tensorflow logs to a log file in python,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

tf 2.2

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
While running a script that uses tf, it prints some logs such as info and warning that comes from inside the tensorflow library. Instead of printing this on console how can a user save it into a seperate log file and another log file for other modules. That is saving logs of tensorflow alone into one file and the python logging as per user preference.
```


### Standalone code to reproduce the issue

```shell
import logging

# get TF logger
log = logging.getLogger('tensorflow')
log.setLevel(logging.DEBUG)

# create formatter and add it to the handlers
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# create file handler which logs even debug messages
fh = logging.FileHandler('tensorflow.log')
fh.setLevel(logging.DEBUG)
fh.setFormatter(formatter)
log.addHandler(fh)
```


### Relevant log output

_No response_</details>"
57552,Why are tensors used after they are freed in a gpu sync operation?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

No

### Python version

3.8

### Current Behaviour?

Why are tensors used after they are freed in a gpu sync operation?
example in softmax_op_gpu.cu.cc
```shell
  void Compute(OpKernelContext* context) override {
      ............
      Tensor max_logits;
      Tensor sum_probs;
      OP_REQUIRES_OK(context,
                     context->allocate_temp(DataTypeToEnum<T>::value,
                                            softmax_out->shape(), &max_logits));

      typedef typename softmax_traits<T>::accumulator_type acc_type;
      OP_REQUIRES_OK(context,
                     context->allocate_temp(DataTypeToEnum<acc_type>::value,
                                            softmax_out->shape(), &sum_probs));

        .................
        TF_CHECK_OK(GpuLaunchKernel(
            GenerateNormalizedProb<T, acc_type, kUnroll>, numBlocks,
            numThreadsPerBlock, 0, cu_stream,
            reinterpret_cast<const T*>(logits_in_.flat<T>().data()),
            reinterpret_cast<const acc_type*>(
                sum_probs.flat<acc_type>().data()),
            reinterpret_cast<const T*>(max_logits.flat<T>().data()),
            const_cast<T*>(softmax_out->flat<T>().data()), rows, cols, log_));
        ....................
    }
  }
```


### Standalone code to reproduce the issue

1. ```max_logits ```and ```sum_probs``` will be free when ```Compute``` function over, because they are temporary variant.
2. ```GpuLaunchKernel``` will async use ```max_logits``` and ```sum_probs``` in kernel launch. 
So ```max_logits ```and ```sum_probs``` maybe used after they are freed.

Tensor is a ref and TensorBuffer guaranteed memory/GPU memory is used. but in the above example, when function over and temporary tensor destruct, GPU Memory in TensorBuffer will be freed to BFCAllocator and in sametime this GPU Memory maybe be allocated by other GPU op.
we move Administrative rights from TensorBuffer to BFCAllocator.  Is it the right way?


### Relevant log output

```shell
void Compute(OpKernelContext* context) override {
      ............
      Tensor max_logits;
      Tensor sum_probs;
      OP_REQUIRES_OK(context,
                     context->allocate_temp(DataTypeToEnum<T>::value,
                                            softmax_out->shape(), &max_logits));

      typedef typename softmax_traits<T>::accumulator_type acc_type;
      OP_REQUIRES_OK(context,
                     context->allocate_temp(DataTypeToEnum<acc_type>::value,
                                            softmax_out->shape(), &sum_probs));

        .................
        TF_CHECK_OK(GpuLaunchKernel(
            GenerateNormalizedProb<T, acc_type, kUnroll>, numBlocks,
            numThreadsPerBlock, 0, cu_stream,
            reinterpret_cast<const T*>(logits_in_.flat<T>().data()),
            reinterpret_cast<const acc_type*>(
                sum_probs.flat<acc_type>().data()),
            reinterpret_cast<const T*>(max_logits.flat<T>().data()),
            const_cast<T*>(softmax_out->flat<T>().data()), rows, cols, log_));
        ....................
         TensorRef ref0(max_logits);
         TensorRef ref1(sum_probs);
         auto done = [ref0, ref1](){
           ref0.Unref;
           ref1.Unref;
         };
         device->event_mgr->ThenExecute(device->stream(), done);
    }
  }
```
I found the same behavior in many GPU sync operations, is this the correct approach?
</details>"
57551,tf.image.rot90 should add a note for the case k<0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Source

source

### Tensorflow Version

TF2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The documentation only describes how the code runs when k>0, and dont mention k<0. The code shows that when k<0, the image will be rotated clockwise. I think a note should be added to explain what happens when k<0
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
results={}
try:
  arg_0 = tf.saturate_cast(tf.random.uniform([2, 2, 1], minval=-256, maxval=257, dtype=tf.int64), dtype=tf.int32)
  k = -1
  results[""res""] = tf.image.rot90(arg_0,k=k,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
```


### Relevant log output

_No response_</details>"
57550,Why is `tf.Conv2D` treated as a SELECT op?,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS (Intel)
- TensorFlow installation (pip package or built from source): `pip`
- TensorFlow library (version, if pip package or github SHA, if built from source): TensorFlow 2.9.1

### 2. Code

```py
from transformers import TFMobileViTForImageClassification
import tensorflow as tf

model = TFMobileViTForImageClassification.from_pretrained(""apple/mobilevit-xx-small"")

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
with open(""mobilevit_xxs.tflite"", ""wb"") as f:
    f.write(tflite_model)
```

Install `transformers` by running `pip install git+https://github.com/sayakpaul/transformers@feat/tf-mobilevit`.

### 3. Failure ~after~ before conversion

```bash
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TF Select ops: Conv2D
Details:
	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<1x1x64x48xf32>) -> (tensor<?x?x?x48xf32>) : {data_format = ""NHWC"", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<1x1x80x64xf32>) -> (tensor<?x?x?x64xf32>) : {data_format = ""NHWC"", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<1x1x96x80xf32>) -> (tensor<?x?x?x80xf32>) : {data_format = ""NHWC"", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}

Traceback (most recent call last):
  File ""playground_tf.py"", line 72, in <module>
    tflite_model = converter.convert()
  File ""/Users/sayakpaul/.local/bin/.virtualenvs/hf/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 929, in wrapper
    return self._convert_and_export_metrics(convert_func, *args, **kwargs)
  File ""/Users/sayakpaul/.local/bin/.virtualenvs/hf/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 908, in _convert_and_export_metrics
    result = convert_func(self, *args, **kwargs)
  File ""/Users/sayakpaul/.local/bin/.virtualenvs/hf/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 1338, in convert
    saved_model_convert_result = self._convert_as_saved_model()
  File ""/Users/sayakpaul/.local/bin/.virtualenvs/hf/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 1320, in _convert_as_saved_model
    return super(TFLiteKerasModelConverterV2,
  File ""/Users/sayakpaul/.local/bin/.virtualenvs/hf/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 1131, in convert
    result = _convert_graphdef(
  File ""/Users/sayakpaul/.local/bin/.virtualenvs/hf/lib/python3.8/site-packages/tensorflow/lite/python/convert_phase.py"", line 212, in wrapper
    raise converter_error from None  # Re-throws the exception.
  File ""/Users/sayakpaul/.local/bin/.virtualenvs/hf/lib/python3.8/site-packages/tensorflow/lite/python/convert_phase.py"", line 205, in wrapper
    return func(*args, **kwargs)
  File ""/Users/sayakpaul/.local/bin/.virtualenvs/hf/lib/python3.8/site-packages/tensorflow/lite/python/convert.py"", line 794, in convert_graphdef
    data = convert(
  File ""/Users/sayakpaul/.local/bin/.virtualenvs/hf/lib/python3.8/site-packages/tensorflow/lite/python/convert.py"", line 311, in convert
    raise converter_error
tensorflow.lite.python.convert_phase.ConverterError: /Users/sayakpaul/.local/bin/.virtualenvs/hf/lib/python3.8/site-packages/keras/engine/base_layer.py:1014:0: error: 'tf.Conv2D' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
/Users/sayakpaul/.local/bin/.virtualenvs/hf/lib/python3.8/site-packages/keras/engine/base_layer.py:1014:0: note: Error code: ERROR_NEEDS_FLEX_OPS
/Users/sayakpaul/.local/bin/.virtualenvs/hf/lib/python3.8/site-packages/keras/engine/base_layer.py:1014:0: error: 'tf.Conv2D' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
/Users/sayakpaul/.local/bin/.virtualenvs/hf/lib/python3.8/site-packages/keras/engine/base_layer.py:1014:0: note: Error code: ERROR_NEEDS_FLEX_OPS
/Users/sayakpaul/.local/bin/.virtualenvs/hf/lib/python3.8/site-packages/keras/engine/base_layer.py:1014:0: error: 'tf.Conv2D' op is neither a custom op nor a flex op
<unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
/Users/sayakpaul/.local/bin/.virtualenvs/hf/lib/python3.8/site-packages/keras/engine/base_layer.py:1014:0: note: Error code: ERROR_NEEDS_FLEX_OPS
<unknown>:0: error: failed while converting: 'main':
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select
TF Select ops: Conv2D
Details:
	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<1x1x64x48xf32>) -> (tensor<?x?x?x48xf32>) : {data_format = ""NHWC"", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<1x1x80x64xf32>) -> (tensor<?x?x?x64xf32>) : {data_format = ""NHWC"", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<1x1x96x80xf32>) -> (tensor<?x?x?x80xf32>) : {data_format = ""NHWC"", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
```

If I add the select OPS during conversion, it passes:

```py
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS,  # Enable TensorFlow ops.
]
```

With the following info:

```bash
WARNING:absl:Found untraced functions such as conv_stem_layer_call_fn, conv_stem_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, conv_1x1_exp_layer_call_fn while saving (showing 5 of 512). These functions will not be directly callable after loading.
2022-08-31 12:21:18.548394: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.
2022-08-31 12:21:18.548415: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.
2022-08-31 12:21:23.916667: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1901] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
Flex ops: FlexConv2D
Details:
	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<1x1x64x48xf32>) -> (tensor<?x?x?x48xf32>) : {data_format = ""NHWC"", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<1x1x80x64xf32>) -> (tensor<?x?x?x64xf32>) : {data_format = ""NHWC"", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
	tf.Conv2D(tensor<?x?x?x?xf32>, tensor<1x1x96x80xf32>) -> (tensor<?x?x?x80xf32>) : {data_format = ""NHWC"", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}
See instructions: https://www.tensorflow.org/lite/guide/ops_select
```

_**But my question is, why is Conv2D a select OP?**_

Attached is the SavedModel file for better investigation. 

[1.zip](https://github.com/tensorflow/tensorflow/files/9459235/1.zip)


### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
57549,M1 tf.gather_nd: InvalidArgumentError: Multiple OpKernel registrations match NodeDef at the same priority,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.2

### Custom Code

Yes

### OS Platform and Distribution

MacOS Monterey 12.5

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.gather_nd should properly have a gradient of shape (2,6) in the code below connecting the (very bad and toy) loss to the defined variable. However, it appears that the kernel sets the same priority for some operations and cannot resolve the graph.

Furthermore, I have observed the following *extremely weird* behavior:

If you change the code to `indexed_v = tf.gather_nd(v + 0.0, indices)`, there is no error.

I have also confirmed that this issue does not occur if I go back to a much older version of tensorflow: `2.4.0-rc0`

Lastly, for reproducibility, I set up my virtual environment with the following executions

>>> conda create -n tf_bug_test python=3.10
>>> conda activate tf_bug_test
>>> conda install -c apple tensorflow-deps
>>> python -m pip install tensorflow-macos

Note that if I install tensorflow-metal, the same error will happen but it will specify the computation is on GPU instead of CPU.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

time = 100
products = 6
var_dim = 2

which_parameter_to_grab = np.random.binomial(1, p=0.1,size=(time, products))
v = tf.Variable(tf.ones((2,products)),name=""test"",dtype=tf.float32,shape=(var_dim,products))
# generate random indices for grabbing a parameter 
indices = np.stack(
    (which_parameter_to_grab, np.tile(np.arange(products)[None, :], (time, 1))),
    axis=2,
).astype(int)

opt_fcn = tf.keras.optimizers.Adam(learning_rate=1e-3)

with tf.GradientTape() as tape:
    indexed_v = tf.gather_nd(v, indices)
    loss_val = tf.reduce_mean(tf.math.square(indexed_v))
grads = tape.gradient(loss_val, [v])
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/Users/ryansaxe/Documents/bug_reproduce.py"", line 21, in <module>
    grads = tape.gradient(loss_val, [v])
  File ""/Users/ryansaxe/miniforge3/envs/tf_bug_test/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py"", line 1100, in gradient
    flat_grad = imperative_grad.imperative_grad(
  File ""/Users/ryansaxe/miniforge3/envs/tf_bug_test/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py"", line 67, in imperative_grad
    return pywrap_tfe.TFE_Py_TapeGradient(
  File ""/Users/ryansaxe/miniforge3/envs/tf_bug_test/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py"", line 157, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File ""/Users/ryansaxe/miniforge3/envs/tf_bug_test/lib/python3.10/site-packages/tensorflow/python/ops/array_grad.py"", line 736, in _ResourceGatherNdGrad
    ref_shape = gen_resource_variable_ops.variable_shape(ref, indices.dtype)
  File ""/Users/ryansaxe/miniforge3/envs/tf_bug_test/lib/python3.10/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py"", line 1341, in variable_shape
    _ops.raise_from_not_ok_status(e, name)
  File ""/Users/ryansaxe/miniforge3/envs/tf_bug_test/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 7164, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Multiple OpKernel registrations match NodeDef at the same priority '{{node VariableShape}}': 'op: ""VariableShape"" device_type: ""CPU"" constraint { name: ""out_type"" allowed_values { list { type: DT_INT64 } } }' and 'op: ""VariableShape"" device_type: ""CPU"" constraint { name: ""out_type"" allowed_values { list { type: DT_INT64 } } }'
	 when instantiating VariableShape [Op:VariableShape]
```
</details>"
57540,Please make different versions of Tensorflow compatible with each other.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

All versions

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Tensorflow X.11 is completely incompatible with Tensorflow X.12.  If I try to run a colab notebook, and the notebook was written using Tensorflow X.11, but the newest version of Tensorflow is now X.12 or later, then the notebook will not work.  It would be nice to have some consistency between versions so I don't have to know the version of Tensorflow that the notebook was written in to run it.
```


### Standalone code to reproduce the issue

```shell
Literally use any notebook more than a year old.
```


### Relevant log output

_No response_</details>"
57536,Indentation bug in TensorFlow main expert tutorial,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

binary

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
There is an indentation bug in the main Tensorflow expert colab tutorial located here: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb. The code does not run because there is an indentation problem.

@tf.function
def train_step(images, labels):
  with tf.GradientTape() as tape:
    # training=True is only needed if there are layers with different
    # behavior during training versus inference (e.g. Dropout).
    predictions = model(images, training=True)
    loss = loss_object(labels, predictions)
  gradients = tape.gradient(loss, model.trainable_variables)  #PROBLEM!!! NEEDS INDENTATION!
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

  train_loss(loss)
  train_accuracy(labels, predictions)
```


### Standalone code to reproduce the issue

```shell
Run the colab all the way through, and you will see that it chokes on the last step.

https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb
```


### Relevant log output

_No response_</details>"
57535,Forward gradient does not support `axis=[0]` for `tf.concat`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Inconsistent `axis` shape check for `tf.concat`: when `axis=[0]`, direct execution and reverse gradient computation both will succeed, but forward gradient does not support it and throws error.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x = tf.random.uniform([12, 1, 1, 1], minval=0, maxval=2.0, dtype=tf.float32)
y = tf.random.uniform([12, 1, 1, 1], minval=0, maxval=2.0, dtype=tf.float32)
axis = [0]
result = tf.concat([x, y], axis)
print(result.shape) # Pass

with tf.GradientTape() as tape:
    tape.watch(x)
    result = tf.concat([x, y], axis)
print(result.shape)
print(tape.gradient(result, x).shape) # Reverse mode AD pass

tangent = tf.reshape(tf.one_hot(1, tf.size(x), dtype=x.dtype), shape=x.shape)
with tf.autodiff.ForwardAccumulator(x, tangent) as acc:
    result = tf.concat([x, y], axis) # Forward mode AD fail
```


### Relevant log output

```shell
(24, 1, 1, 1) 
(24, 1, 1, 1) 
(12, 1, 1, 1) 
Node: 'gradient_tape/ConcatOffset' 
Concat dim tensor should be a scalar integer, but got shape [1] 
 [[{{node gradient_tape/ConcatOffset}}]] [Op:__inference__jvp_helper_wrapper_78]
```
</details>"
57534,`tf.pad` fails when executing in `tf.autodiff.ForwardAccumulator`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
`tf.pad` fails when executing in `tf.autodiff.ForwardAccumulator` and throws `TypeError`. However, if we run `tf.pad` outside of `ForwardAccumulator` with the same input, it will pass.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_tensor = tf.random.uniform([3, 1, 1, 1], minval=2.0, maxval=3.0, dtype=tf.float32)
paddings = tf.random.uniform([4, 2], minval=0, maxval=5, dtype=tf.int64) 
mode = ""CONSTANT""
constant_values = 0 
result = tf.pad(input_tensor, paddings, mode=mode, constant_values=constant_values, ) 
print(result.shape) # Pass

tangent = tf.reshape(tf.one_hot(1, tf.size(input_tensor), dtype=input_tensor.dtype), shape=input_tensor.shape)
with tf.autodiff.ForwardAccumulator(input_tensor, tangent) as acc:
    result = tf.pad(input_tensor, paddings, mode=mode, constant_values=constant_values, ) # Fail
```


### Relevant log output

```shell
(8, 3, 8, 4)
TypeError: Input 'y' of 'Sub' Op has type int64 that does not match type int32 of argument 'x'.
```
</details>"
57533,I observe TfLiteXNNPackDelegateDelete is called when disable XNNPACK in Tensorflow Lite,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.4

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

ARMV7

### Python version

3.8

### Bazel version

cmake instead of bazel

### GCC/Compiler version

9.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Built tensorflow lite with -DTFLITE_ENABLE_XNNPACK=OFF in cmake, and I got tensorflow-lite library as below.
I expected no XNNPack functions be called, am I right?
It looks like Macro DTFLITE_WITHOUT_XNNPACK is not working as expected.



Thanks.
roro
```


### Standalone code to reproduce the issue

```shell
None, just building issue.
```


### Relevant log output

```shell
./recipe-sysroot-native/usr/bin/arm-rdkmllib32-linux-gnueabi/arm-rdkmllib32-linux-gnueabi-readelf -a image/usr/lib/libtensorflow-lite.so |grep XNNPack
0025cd28  00008315 R_ARM_GLOB_DAT    00000000   TfLiteXNNPackDelegateD
0025ea44  00008516 R_ARM_JUMP_SLOT   00000000   TfLiteXNNPackDelegateO
0025ea48  00008416 R_ARM_JUMP_SLOT   00000000   TfLiteXNNPackDelegateC
   131: 00000000     0 NOTYPE  GLOBAL DEFAULT  UND TfLiteXNNPackDelegateDele
   132: 00000000     0 NOTYPE  GLOBAL DEFAULT  UND TfLiteXNNPackDelegateCrea
   133: 00000000     0 NOTYPE  GLOBAL DEFAULT  UND TfLiteXNNPackDelegateOpti
  7898: 00000000     0 NOTYPE  GLOBAL DEFAULT  UND TfLiteXNNPackDelegateDele
  7899: 00000000     0 NOTYPE  GLOBAL DEFAULT  UND TfLiteXNNPackDelegateCrea
  7900: 00000000     0 NOTYPE  GLOBAL DEFAULT  UND TfLiteXNNPackDelegateOpti
```
</details>"
57532,Nonquantized tflite model using tflite_model_maker,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

model.export(export_dir='.', tflite_filename='model_fp16.tflite', quantization_config=config)
How to convert the trained model to tflite which is not quantized for models trained using tmm.

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Lite Model Colab]: https://www.tensorflow.org/lite/models/modify/model_maker/object_detection(https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

"
57531,Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

windows 10

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

8.1

### GPU model and memory

A5000 

### Current Behaviour?

```shell
After fresh installition of windows 10 and gpu driver, 

I have followed this installition -> https://www.tensorflow.org/install/pip?hl=en#windows-native

Created venv using conda create --name tf python=3.9
Then activated venv and used command conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0
Then used, pip install tensorflow

tensorflow successfully detects GPU, however when before starting to training, it gave cudart64_110.dll not found error. I confirmed that cudart64_110.dll is exist inside virtual env in local path ""Library\bin""
```


### Standalone code to reproduce the issue

```shell
conda create --name tf python=3.9
conda activate tf
conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0
pip install tensorflow

train any word2vec/lstm model and error pops
```


### Relevant log output

_No response_</details>"
57529,tf.image.convert_image_dtype dont check the validity of input,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

TF2.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Images that are represented using floating point values are expected to have values in the range [0,1) on documentation. However, when image is floating point values greater than 1, the code works. It should throw an exception.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
results={}
try:
  arg_0 = [[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]],[[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]]
  dtype = tf.uint64
  saturate = False
  results[""res""] = tf.image.convert_image_dtype(arg_0,dtype=dtype,saturate=saturate,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
'''
{'res': <tf.Tensor: shape=(2, 2, 3), dtype=uint64, numpy=
array([[[9223372036854775808, 9223372036854775808, 9223372036854775808],
        [9223372036854775808, 9223372036854775808, 9223372036854775808]],
       [[9223372036854775808, 9223372036854775808, 9223372036854775808],
        [9223372036854775808, 9223372036854775808, 9223372036854775808]]],
      dtype=uint64)>}
'''
```


### Relevant log output

_No response_</details>"
57528,"""Segmentation fault"" and ""Floating point exception"" popped up while training using Intel CPUs with oneDNN optimizations enabled","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
While I trained a 3D-UNet-like model using Intel CPUs with oneDNN optimizations enabled, 
""Segmentation fault"" and ""Floating point exception"" popped up with specific configurations.

Config 1:
  Input size: [512, 512, 7, 1] ([H, W, C, N])
  Batch size: 80
  --> Error: Segmentation fault (core dumped)

Config 2:
  Input size: [512, 512, 8, 1]
  Batch size: 80
  --> Runs without any error

Config 3:
  Input size: [512, 512, 32, 1]
  Batch size: 1, 4, 16 (any of them have the same reuslt)
  --> Error: Floating point exception (core dumped)
```


### Standalone code to reproduce the issue

```shell
This is the link to the source code:
https://drive.google.com/file/d/1NOqcAW8MJU4QnqpnVxYX_C85Kvcb7W05/view?usp=sharing
It is a zip file with 2 python scripts in it.
""train.py"" is the script that defines data loader and the train task.
""unet_3d_layers.py"" is the script that defines the model.

Note: Because the dataset we're using is private, I used numpy to generate random 
train data in the source code to reproduce the problem

To reproduce:
Config 1:
  1. Open train.py
  2. Uncomment line 17, 41, and 42
  3. Comment line 18, 19, 44, 45, 47, and 48
  4. In the terminal, run ""python train.py"" to run the code

Config 2:
  1. Open train.py
  2. Uncomment line 18, 44, and 45
  3. Comment line 17, 19, 41, 42, 47, and 48
  4. In the terminal, run ""python train.py"" to run the code

Config 3:
  1. Open train.py
  2. Uncomment line 19, 47, and 48
  3. Modify the variable ""batch_size"" in line 48
  4. Comment line 17, 18, 41, 42, 44, and 45
  5. In the terminal, run ""python train.py"" to run the code
```


### Relevant log output

Segmentation fault:
```shell
2022-08-31 10:29:25.997447: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-08-31 10:29:26.001968: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-08-31 10:29:26.001989: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2022-08-31 10:29:27.476182: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2022-08-31 10:29:27.476213: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2022-08-31 10:29:27.476230: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (user-COB-1932C): /proc/driver/nvidia/version does not exist
2022-08-31 10:29:27.476445: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/10
2022-08-31 10:29:31.296605: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_BOOL
    }
  }
}
 is neither a subtype nor a supertype of the combined inputs preceding it:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_LEGACY_VARIANT
    }
  }
}

	while inferring type of node 'dice_loss/cond/output/_11'
 2/40 [>.............................] - ETA: 29:12 - loss: 2.1932Segmentation fault (core dumped)
```
Floating point exception:
```shell
2022-08-31 10:27:02.379986: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-08-31 10:27:02.384512: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-08-31 10:27:02.384533: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2022-08-31 10:27:03.883968: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2022-08-31 10:27:03.883996: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2022-08-31 10:27:03.884012: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (user-COB-1932C): /proc/driver/nvidia/version does not exist
2022-08-31 10:27:03.884231: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/10
2022-08-31 10:27:07.807669: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_BOOL
    }
  }
}
 is neither a subtype nor a supertype of the combined inputs preceding it:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_LEGACY_VARIANT
    }
  }
}

	while inferring type of node 'dice_loss/cond/output/_11'
Floating point exception (core dumped)
```
</details>"
57527,"when dtype is tf.uint64, tf.image.convert_image_dtype throws exception","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

TF2.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.image.convert_image_dtype supports data types (for image and dtype) of uint8, uint16, uint32, uint64, int8, int16, int32, int64, float16, float32, float64, bfloat16. But when dtype is uint64, it throws exception.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
results={}
try:
  arg_0 = [[[1, 2, 3], [4, 5, 6]],[[7, 8, 9], [10, 11, 12]]]
  dtype = tf.uint64
  saturate = False
  results[""res""] = tf.image.convert_image_dtype(arg_0,dtype=dtype,saturate=saturate,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
'''
{'err': ""Error:Value for attr 'T' of uint64 is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, int64, complex64, complex128\n\t; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul]""}
'''
```


### Relevant log output

_No response_</details>"
57526,"When input is a tensor, the 'axis' is not type checked in tf.concat","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

TF2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Axis must be in the range [-rank(values), rank(values)) written on documentation [https://tensorflow.google.cn/versions/r2.4/api_docs/python/tf/concat#args]. When input is a list of tensor, axis will validate the range. However, when input is a single tensor, axis won't check the range.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
t1 = [[[1, 2], [2, 3]], [[4, 4], [5, 3]]]
t2 = [[[7, 4], [8, 4]], [[2, 10], [15, 11]]]
res = tf.concat([t1, t2], -100)
print(res)
'''
tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Expected concatenating dimensions in the range [-3, 3), but got -100 [Op:ConcatV2]
'''

Above code will check the validity of axis, but the following code wont.

import tensorflow as tf
results={}
try:
  arg_0 = tf.saturate_cast(tf.random.uniform([2, 4], minval=-256, maxval=257, dtype=tf.int64), dtype=tf.int64)
  axis = -200
  results[""res""] = tf.concat(arg_0,axis=axis,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
'''
{'res': <tf.Tensor: shape=(2, 4), dtype=int64, numpy=
array([[ 140,  236, -151,   66],
       [ 154,   28,  -82,   23]])>}
'''
```


### Relevant log output

_No response_</details>"
57525,tf.clip_by_norm documentation wrong,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

TF2.8

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
clip_norm should be a 0-D (scalar) Tensor > 0 written on documentation https://tensorflow.google.cn/versions/r2.4/api_docs/python/tf/clip_by_norm#args. However, when the clip_norm is a negative integral, the code works.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
results={}
try:
  arg_0 = tf.random.uniform([1, 5], dtype=tf.float32)
  results[""res""] = tf.clip_by_norm(arg_0,clip_norm=-1)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
'''
{'res': <tf.Tensor: shape=(1, 5), dtype=float32, numpy=
array([[-0.54458696, -0.59545624, -0.31598726, -0.20027176, -0.4570559 ]],dtype=float32)>}
'''
```


### Relevant log output

_No response_</details>"
57524,"When I an tryng to run the model for trainig, I am getting TypeError: 'numpy.float64' object cannot be interpreted as an integer for efficientdet.","I am facing TypeError: 'numpy.float64' object cannot be interpreted as an integer this while training my model.
![Screenshot (22)](https://user-images.githubusercontent.com/109948997/187413527-052e9ec7-19c7-48e3-b684-4b841e76836a.png)
"
57523,TFLite with XNNPack delegate is performing slower than TFLite when XNNPack delegate is set to false,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

Debian 10.2

### Mobile device

_No response_

### Python version

3.7.3 for python3, python 2.7.16

### Bazel version

5.1.0

### GCC/Compiler version

8.3.0

### CUDA/cuDNN version

NA

### GPU model and memory

NA

### Current Behaviour?

```shell
Expected performance benefit when XNNPack delegate is used as claimed.
But not happening. Tested with quant and float models.
```


### Standalone code to reproduce the issue

```shell
Used tensorflow/lite/examples/label_image/

Modified loop count to 100 and tested with num of threads to max and -1 and observed similar results.

% bazel-bin/tensorflow/lite/examples/label_image/label_image --tflite_model mobilenet_v2_1.0_224.tflite --labels tensorflow/lite/examples/label_image/mobilenet_v1_1.0_224/labels.txt --image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp --use_xnnpack=true
INFO: Loaded model mobilenet_v2_1.0_224.tflite
INFO: resolved reporter
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
XNNPACK delegate created.
INFO: Applied XNNPACK delegate.


INFO: invoked
INFO: average time: 16.0024 ms
INFO: 0.911344: 653 653:military uniform
INFO: 0.0144661: 835 835:suit, suit of clothes
INFO: 0.00624739: 440 440:bearskin, busby, shako
INFO: 0.00296665: 907 907:Windsor tie
INFO: 0.00269024: 753 753:racket, racquet

% bazel-bin/tensorflow/lite/examples/label_image/label_image --tflite_model mobilenet_v2_1.0_224.tflite --labels tensorflow/lite/examples/label_image/mobilenet_v1_1.0_224/labels.txt --image tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp --use_xnnpack=false
INFO: Loaded model mobilenet_v2_1.0_224.tflite
INFO: resolved reporter
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: invoked
INFO: average time: 3.64675 ms
INFO: 0.911344: 653 653:military uniform
INFO: 0.0144661: 835 835:suit, suit of clothes
INFO: 0.00624739: 440 440:bearskin, busby, shako
INFO: 0.00296665: 907 907:Windsor tie
INFO: 0.00269024: 753 753:racket, racquet
```


### Relevant log output

```shell
With XNNPack
INFO: Loaded model mobilenet_v2_1.0_224.tflite
INFO: resolved reporter
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
XNNPACK delegate created.
INFO: Applied XNNPACK delegate.
INFO: invoked
INFO: average time: 16.0024 ms
INFO: 0.911344: 653 653:military uniform
INFO: 0.0144661: 835 835:suit, suit of clothes
INFO: 0.00624739: 440 440:bearskin, busby, shako
INFO: 0.00296665: 907 907:Windsor tie
INFO: 0.00269024: 753 753:racket, racquet


Without XNNPack
INFO: Loaded model mobilenet_v2_1.0_224.tflite
INFO: resolved reporter
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: invoked
INFO: average time: 3.64675 ms
INFO: 0.911344: 653 653:military uniform
INFO: 0.0144661: 835 835:suit, suit of clothes
INFO: 0.00624739: 440 440:bearskin, busby, shako
INFO: 0.00296665: 907 907:Windsor tie
INFO: 0.00269024: 753 753:racket, racquet
```
</details>"
57522,About get_config,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi,if my custom‘s Layers like this:

class TestBlock(keras.layers.Layer):
    
    def __init__(self, numUnits=128, batch_size=8, L=-1, width=50, channel=128, causal = True, **kwargs):
        super(DprnnBlock, self).__init__(**kwargs)
        '''
        numUnits  hidden layer size in the LSTM
        batch_size 
        L         number of frames, -1 for undefined length
        width     width size output from encoder
        channel   channel size output from encoder
        causal    instant Layer Norm or global Layer Norm
        '''
        self.numUnits = numUnits
        self.batch_size = batch_size
        self.causal = causal
        self.intra_rnn = keras.layers.Bidirectional(keras.layers.GRU(units=self.numUnits//2, return_sequences=True,implementation = 2,recurrent_activation = 'hard_sigmoid'))
       
        self.intra_fc = keras.layers.Dense(units = self.numUnits,)

        if self.causal:
            self.intra_ln = keras.layers.LayerNormalization(center=True, scale=True, axis = [-1,-2])
        else:
            self.intra_ln = keras.layers.LayerNormalization(center=False, scale=False)

        self.inter_rnn = keras.layers.GRU(units=self.numUnits, return_sequences=True,implementation = 2,recurrent_activation = 'hard_sigmoid')
        self.inter_fc = keras.layers.Dense(units = self.numUnits,) 

        if self.causal:
            self.inter_ln = keras.layers.LayerNormalization(center=True, scale=True, axis = [-1,-2])
        else:
            self.inter_ln = keras.layers.LayerNormalization(center=False, scale=False)

        self.L = L
        self.width = width
        self.channel = channel
        
        
    def call(self, x):

        batch_size = self.batch_size
        L = self.L
        width = self.width
        
        intra_rnn = self.intra_rnn
        intra_fc = self.intra_fc
        intra_ln = self.intra_ln
        inter_rnn = self.inter_rnn
        inter_fc = self.inter_fc
        inter_ln = self.inter_ln
        channel = self.channel
        causal = self.causal
        
        # Intra-Chunk Processing
        # input shape (bs,T,F,C) --> (bs*T,F,C)
        intra_LSTM_input = tf.reshape(x,[-1,width,channel])
        # (bs*T,F,C)
        intra_LSTM_out = intra_rnn(intra_LSTM_input)
        
        # (bs*T,F,C) channel axis dense
        intra_dense_out = intra_fc(intra_LSTM_out)
            
        if causal:
            # (bs*T,F,C) --> (bs,T,F,C) Freq and channel norm
            intra_ln_input = tf.reshape(intra_dense_out,[batch_size,-1,width,channel])
            intra_out = intra_ln(intra_ln_input)
            
        else:       
            # (bs*T,F,C) --> (bs,T*F*C) global norm
            intra_ln_input = tf.reshape(intra_dense_out,[batch_size,-1])
            intra_ln_out = intra_ln(intra_ln_input)
            intra_out = tf.reshape(intra_ln_out,[batch_size,L,width,channel])
        # (bs,T,F,C)
        intra_out = keras.layers.Add()([x,intra_out])
        #%% Inter-Chunk Processing
        # (bs,T,F,C) --> (bs,F,T,C)
        inter_LSTM_input = tf.transpose(intra_out,[0,2,1,3])
        # (bs,F,T,C) --> (bs*F,T,C)
        inter_LSTM_input = tf.reshape(inter_LSTM_input,[batch_size*width,L,channel])
        
        inter_LSTM_out = inter_rnn(inter_LSTM_input)
        
        # (bs,F,T,C) 
        inter_dense_out = inter_fc(inter_LSTM_out)
        
        inter_dense_out = tf.reshape(inter_dense_out,[batch_size,width,L,channel])
        
        if causal:
            # (bs,F,T,C) --> (bs,T,F,C)
            inter_ln_input = tf.transpose(inter_dense_out,[0,2,1,3])
            inter_out = inter_ln(inter_ln_input)
            
        else:
            # (bs,F,T,C) --> (bs,F*T*C)
            inter_ln_input = tf.reshape(inter_dense_out,[batch_size,-1])
            inter_ln_out = inter_ln(inter_ln_input)
            inter_out = tf.reshape(inter_ln_out,[batch_size,width,L,channel])
            # (bs,F,T,C) --> (bs,T,F,C)
            inter_out = tf.transpose(inter_out,[0,2,1,3])
        # (bs,T,F,C)
        inter_out = keras.layers.Add()([intra_out,inter_out])
    
        return inter_out

    def get_config(self):
        config = super().get_config().copy()
        config.update({
               'numUnits':self.numUnits,
                ""batch_size"":self.batch_size,
                ""causal"":self.causal,
                       ""L"":self.L,
                   ""width"":self.width,
                 ""channel"":self.channel,                   
           })
        return config
```


### Standalone code to reproduce the issue

```shell
when i use ""model.save"" to save my model,must use ""get_config"" in custom‘s Layers.So,i want to ask how to use update like ""intra_ln""、""intra_fc""、“intra_ln” in ""config.update"" of custom‘s Layers
```


### Relevant log output

_No response_</details>"
57521,__lll_lock_wait_private cost lots of CPU time.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

tf 1.15

### Custom Code

No

### OS Platform and Distribution

Linux CentOS 7

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
__lll_lock_wait_private cost lots of CPU time.
```


### Standalone code to reproduce the issue

```shell
MAVEN:

- org.tensorflow:tensorflow:1.15.0
- org.tensorflow:libtensorflow:1.15.0
- org.tensorflow:proto:1.15.0

When calling Session.Runner#run
```


### Relevant log output

_No response_</details>"
57520,Convert FloorModOp to tosa,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.1

### GCC/Compiler version

9

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
from source code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/transforms/legalize_common.cc#L2457
floormod(x, y) = x / y - floor(x / y)

from doc:https://tensorflow.google.cn/api_docs/python/tf/raw_ops/FloorMod?version=nightly
floormod(x, y) = x - floor(x / y) * y

I wonder if I misunderstood
```


### Standalone code to reproduce the issue

```shell
tosa ir I get:
'
module attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 561 : i32}, tf_saved_model.semantics} {
  func.func @serving_default(%arg0: tensor<11x10xf32> {tf_saved_model.index_path = [""model_input1""]}, %arg1: tensor<f32> {tf_saved_model.index_path = [""model_input2""]}) -> (tensor<11x10xf32> {tf_saved_model.index_path = [""model_output""]}) attributes {tf.entry_function = {control_outputs = """", inputs = ""model_input1:0,model_input2:0"", outputs = ""FloorMod:0""}, tf_saved_model.exported_names = [""serving_default""]} {
    %0 = ""tosa.reciprocal""(%arg1) : (tensor<f32>) -> tensor<f32>
    %1 = ""tosa.reshape""(%0) {new_shape = [1, 1]} : (tensor<f32>) -> tensor<1x1xf32>
    %2 = ""tosa.mul""(%arg0, %1) {shift = 0 : i32} : (tensor<11x10xf32>, tensor<1x1xf32>) -> tensor<11x10xf32>
    %3 = ""tosa.floor""(%2) : (tensor<11x10xf32>) -> tensor<11x10xf32>
    %4 = ""tosa.sub""(%2, %3) : (tensor<11x10xf32>, tensor<11x10xf32>) -> tensor<11x10xf32>
    return %4 : tensor<11x10xf32>
  }
}
'
```


### Relevant log output

_No response_</details>"
57517,Reverse-mode auto differentiation didn't reject invalid `axis` input as expected for API `tf.math.reduce_logsumexp`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Input parameter `axis` for API `tf.math.reduce_logsumexp` should be at most rank 1. with input axis=`[[0,1]]`,  which is rank 2 and invalid, It should throw `ValueError` in both line 11 and line 17. Now forward-mode autodiff threw error as expected but reverse mode didn't validate the `axis` parameter and passed.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input = tf.constant([[0.44943833]])

axis = [[0,1]]

with tf.GradientTape() as g:
    g.watch(input)
    # should throw ValueError here, because axis is rank 2, which is invalid, but didn't
    res = tf.math.reduce_logsumexp(input, axis, keepdims=False)
grad = g.gradient(res,input)
print(grad)

with tf.autodiff.ForwardAccumulator(input,tf.constant(1.,shape=[1,1])) as acc:
    # throw error as expected
    res_fwd = tf.math.reduce_logsumexp(input, axis, keepdims=False)
jvp = acc.jvp(res_fwd)
print(jvp)
```


### Relevant log output

```shell
tf.Tensor([[1.]], shape=(1, 1), dtype=float32)
ValueError: Shape must be at most rank 1 but is rank 2 for '{{node gradient_tape/Sum}} = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](gradient_tape/Cast, inputs_1)' with input shapes: [1,1], [1,2].
```
</details>"
57514,tf.abs(tf.int32.min) returns a negative value,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.9.0-18-gd8ce9f9c301

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04.4 LTS

### Mobile device

n/a

### Python version

Python 3.8.10 (default, Jun 22 2022, 20:18:18) [GCC 9.4.0] on linux

### Bazel version

n/a

### GCC/Compiler version

n/a

### CUDA/cuDNN version

_No response_

### GPU model and memory

n/a

### Current Behaviour?

```shell
`tf.abs(tf.int32.min)` returns the unmodified value of `tf.int32.min`, a negative value.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.debugging.assert_non_negative(tf.abs(tf.int32.min + 1), ""Works fine."")
tf.debugging.assert_non_negative(tf.abs(tf.int32.min), ""Returns a negative value"")
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py"", line 3331, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-11-dd77e4f67efc>"", line 3, in <module>
    tf.debugging.assert_non_negative(tf.abs(tf.int32.min), ""Returns a negative value"")
  File ""/home/hosford42/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/hosford42/.local/lib/python3.8/site-packages/tensorflow/python/ops/check_ops.py"", line 407, in _binary_assert
    raise errors.InvalidArgumentError(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Returns a negative value.  
Condition x >= 0 did not hold element-wise:
x (shape=() dtype=int32) = 
-2147483648
```
</details>"
57503,Issue created for Rollback of PR #56119: Add lamba var loop test,"Merged PR #56119 is rolled back in f36b1f5d447577df3b87af59b4dd1415ac35b3ba.
    Please follow up with the reviewer and close this issue once its resolved."
57502,Check-fail in MaxPoolGrad,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9 and 2.11.0-dev20220828

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A check-fail can be triggered in MaxPoolGrad.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""GPU:0""):
    orig_input = tf.random.uniform([6, 32, 6, 13], dtype=tf.float32)
    orig_output = tf.random.uniform([6, 2, 0, 13], dtype=tf.float32)
    grad = tf.random.uniform([6, 2, 0, 13], dtype=tf.float32)
    ksize = [1, 13, 8, 1]
    strides = [1, 18, 7, 1]
    padding = ""VALID""
    data_format = ""NHWC""
    res = tf.raw_ops.MaxPoolGrad(
        orig_input=orig_input,
        orig_output=orig_output,
        grad=grad,
        ksize=ksize,
        strides=strides,
        padding=padding,
        data_format=data_format,
    )
```


### Relevant log output

```shell
2022-08-29 16:51:12.874955: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:636] Check failed: cudnnSetTensorNdDescriptor(handle_.get(), elem_type, nd, dims.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)batch_descriptor: {count: 6 feature_map_count: 13 spatial: 2 0  value_min: 0.000000 value_max: 0.000000 layout: BatchYXDepth}
Aborted (core dumped)
```
</details>"
57501,Crash due to Use of Uninitialized Value When Initializing TensorShape in MaxPoolingWithArgmaxOp,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9 and 2.11.0-dev20220828

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
In [MaxPoolingWithArgmaxOp::Compute](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/maxpooling_op.cc#L990-L1002), `PoolParameters` is initialized. Then `params.out_height` and `params.out_width` are used to build a `TensorShape` , but in some cases they are not initialized. 

Which may cause a Integer-Overflow. In most cases, the Integer-Overflow can be detected and an invlaid status can be triggered, which may lead to DoS. In some cases, the Integer-Overflow can not be detected and a memory corruption can be triggered, which may result in DoS and RCE.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""CPU""): 			# Also crash when run with GPU device
    input = tf.saturate_cast(tf.random.uniform([1, 1, 1, 3], minval=-128, maxval=129, dtype=tf.int64), dtype=tf.int16)
    ksize = [1, 1, 1, 3]
    strides = [1, 1, 1, 3]
    padding = ""SAME""
    res = tf.raw_ops.MaxPoolWithArgmax(
        input=input,
        ksize=ksize,
        strides=strides,
        padding=padding,
    )
```


### Relevant log output

```shell
# most cases
2022-08-29 16:47:46.060511: F tensorflow/core/framework/tensor_shape.cc:186] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 94050355076544 with 94050355128096, result: -1
Aborted (core dumped)
```
</details>"
57500,Floating point exception can be triggered in AvgPool3D when run with CPU and OneDNN is enbaled,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9 and 2.11.0-dev20220828

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When run with CPU and OneDNN is enabled, an FPE can be triggered in AvgPool3D, which may result in DoS.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""CPU""):
    input = tf.random.uniform([30, 19, 4, 19, 17], dtype=tf.float32)
    ksize =[1, 13, 3, 20, 1]
    strides = [1, 14, 4, 1, 1]
    padding = ""VALID""
    data_format = ""NDHWC""
    res = tf.raw_ops.AvgPool3D(
        input=input,
        ksize=ksize,
        strides=strides,
        padding=padding,
        data_format=data_format,
    )
```


### Relevant log output

```shell
Floating point exception (core dumped)
```
</details>"
57499,Memory Corruption in AvgPool3D when OneDNN is enabled,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9 and 2.11.0-dev20220828

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When run with CPU and OneDNN is enabled, a memory curruption can be triggered in AvgPool3D, which may result in DoS and RCE.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""CPU""):
    for i in range(100):
    	input = tf.random.uniform([9, 11, 14, 1, 1], dtype=tf.float32)
    	ksize = [1, 15, 10, 11, 1]
    	strides = [1, 11, 11, 2, 1]
    	padding = ""SAME""
    	data_format = ""NDHWC""
    	res = tf.raw_ops.AvgPool3D(
        	input=input,
        	ksize=ksize,
        	strides=strides,
        	padding=padding,
        	data_format=data_format,
    	)
```


### Relevant log output

```shell
double free or corruption (out)
Aborted (core dumped)
```
</details>"
57498,Floating point exception in MaxPool3DGrad,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9 and 2.11.0-dev20220828

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
An FPE can be triggered when the channel of `orig_input` is zero, which may cause a DoS.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""GPU:0""):
    orig_input = tf.random.uniform([16, 28, 30, 10, 0], dtype=tf.float32)
    orig_output = tf.random.uniform([16, 2, 1, 2, 0], dtype=tf.float32)
    grad = tf.random.uniform([16, 2, 1, 2, 0], dtype=tf.float32)
    ksize = [1, 22, 12, 17, 1]
    strides = [1, 16, 30, 7, 1]
    padding = ""SAME""
    data_format = ""NDHWC""
    res = tf.raw_ops.MaxPool3DGrad(
        orig_input=orig_input,
        orig_output=orig_output,
        grad=grad,
        ksize=ksize,
        strides=strides,
        padding=padding,
        data_format=data_format,
    )
```


### Relevant log output

```shell
Floating point exception (core dumped)
```
</details>"
57497,Check-fail in MaxPool3DGrad,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9 and 2.11.0-dev20220828

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A check-fail can be triggered in MaxPool3DGrad when some of the inputs' dimension sizes are zero.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""GPU:0""):
    orig_input = tf.random.uniform([19, 10, 0, 7, 19], dtype=tf.float32)
    orig_output = tf.random.uniform([19, 4, 0, 4, 19], dtype=tf.float32)
    grad = tf.random.uniform([19, 4, 0, 4, 19], dtype=tf.float32)
    ksize = [1, 16, 10, 31, 1]
    strides = [1, 3, 1, 2, 1]
    padding = ""SAME""
    data_format = ""NDHWC""
    res = tf.raw_ops.MaxPool3DGrad(
        orig_input=orig_input,
        orig_output=orig_output,
        grad=grad,
        ksize=ksize,
        strides=strides,
        padding=padding,
        data_format=data_format,
    )
```


### Relevant log output

```shell
2022-08-29 16:23:58.920831: F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)
Aborted (core dumped)
```
</details>"
57496,Infinite-loop in AdjustHue,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9 and 2.11.0-dev20220828

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
In the CPU implementation of AdjustHue, an infinite loop can be triggered.
Given a large `delta`, it will take forever to compute the `delta%6`, which may cause a DoS. 
Although delta is supposed to be a small value, the code dose not check it.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

with tf.device(""CPU""):
    res = tf.raw_ops.AdjustHue(
    	images=tf.random.uniform([2, 2, 2, 3]),
        delta=1e100,
    )
```


### Relevant log output

```shell
It won't stop in years.
```
</details>"
57495,Check-fail can be triggered in Conv3DBackpropInputV2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9 and 2.11.0-dev20220828

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.5

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A check-fail happens and may cause a DoS.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

input_sizes = [0, 6, 1, 1, 2]
filter = np.ones([3, 1, 1, 2, 3])
out_backprop = np.ones([0, 4, 1, 1, 3])

strides = [1, 1, 1, 1, 1]
padding = 'VALID'
data_format = 'NDHWC'
dilations = [1, 1, 1, 1, 1]
tf.raw_ops.Conv3DBackpropInputV2(input_sizes=input_sizes,\
    filter=filter,\
    out_backprop=out_backprop,\
    strides=strides,\
    padding=padding,\
    data_format=data_format,\
    dilations=dilations)
```


### Relevant log output

```shell
2022-08-29 16:10:10.644814: F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)
Aborted (core dumped)
```
</details>"
57494,Header tensorflow/stream_executor/stream.h not included with tf-nightly pip package,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.11.0.dev20220828

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Horovod does not build with tf-nightly anymore, see https://github.com/horovod/horovod/runs/8047013531?check_suite_focus=true#step:10:9615

The code includes tensorflow/stream_executor/stream.h at https://github.com/horovod/horovod/blob/master/horovod/tensorflow/mpi_ops.cc#L65. Header files in tensorflow/core are included with the pip package. That stream.h is part of Tensorflow sources, but is not included in the nightly pip package.

This is similar to #57083, but a different file this time.


### Standalone code to reproduce the issue

```shell
#include ""tensorflow/stream_executor/stream.h""
```


### Relevant log output

```shell
/tmp/pip-req-build-5xc4jhus/horovod/tensorflow/mpi_ops.cc:65:10: fatal error: tensorflow/stream_executor/stream.h: No such file or directory
   65 | #include ""tensorflow/stream_executor/stream.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
```
</details>"
57493,tf.image.ssim_multiscale has a bug as a loss function. It produces nan. The bug didn't happen before. Please fix it,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

RTX A6000

### Current Behaviour?

```shell
tf.image.ssim_multiscale has a bug as a loss function. It produces nan during training. The bug didn't happen in the previous Tensorflow versions. I think another person had a similar issue with the ssim loss.  Colab Tensorflow version is 2.8.2 and it doesn't have this issue. But I really need to use Tensorflow 2.9.1 for my project. Please help us to fix it. Thank you!
```


### Standalone code to reproduce the issue

```shell
msssim = tf.reduce_mean(tf.image.ssim_multiscale(real_images, generated_images, 2))
```


### Relevant log output

```shell
Epoch 1/10
   6/7856 [..............................] - ETA: 1:45:39 - d_loss: nan - g_loss: nan - ms_ssim: nan - l1: nan - l2: nan - entropy: nan - g_cost: nan
```
</details>"
57492,Tensor size-related conditional logic ignored in `tf.function`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.9.0-18-gd8ce9f9c301

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.4 LTS

### Mobile device

n/a

### Python version

Python 3.8.10 (default, Jun 22 2022, 20:18:18)  [GCC 9.4.0] on linux

### Bazel version

n/a

### GCC/Compiler version

n/a

### CUDA/cuDNN version

_No response_

### GPU model and memory

n/a

### Current Behaviour?

```shell
I have a tf.function with a guard in place to make it return early under certain conditions, but the guard is being ignored, which causes an exception. I have tried rearranging the logic several different ways, but it always results in the same error.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf


@tf.function
def overwrite(arrays, indices, values) -> tf.Tensor:
    arrays = tf.convert_to_tensor(arrays)
    indices = tf.convert_to_tensor(indices)
    values = tf.convert_to_tensor(values)

    # No-op for empty arrays
    if tf.size(arrays) == 0:
        return arrays

    indices = indices % tf.shape(arrays)[1]
    arrays = tf.tensor_scatter_nd_update(
        arrays,
        tf.concat([tf.range(tf.shape(arrays)[0])[:, tf.newaxis], indices[:, tf.newaxis]], axis=-1),
        values
    )

    return arrays


print()
print(""Works fine:"")
print(overwrite(tf.zeros((4, 2)), tf.range(4), tf.ones((4,))))
print()
print(""Causes an exception when it ought to exit early:"")
print(overwrite(tf.zeros((4, 0)), tf.range(4), tf.ones((4,))))
print()
```


### Relevant log output

```shell
2022-08-28 20:49:57.080725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
...
2022-08-28 20:49:57.239798: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
...
2022-08-28 20:49:59.004862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3359 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1

Works fine:
tf.Tensor(
[[1. 0.]
 [0. 1.]
 [1. 0.]
 [0. 1.]], shape=(4, 2), dtype=float32)

Causes an exception when it ought to exit early:
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py"", line 3331, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-7-0c355e479cbf>"", line 1, in <module>
    print(overwrite(tf.zeros((4, 0)), tf.range(4), tf.ones((4,))))
  File ""/home/hosford42/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/tmp/__autograph_generated_file7d47e8wb.py"", line 40, in tf__overwrite
    ag__.if_stmt((ag__.converted_call(ag__.ld(tf).size, (ag__.ld(arrays),), None, fscope) == 0), if_body, else_body, get_state, set_state, ('do_return', 'retval_', 'arrays', 'indices'), 2)
  File ""/tmp/__autograph_generated_file7d47e8wb.py"", line 33, in else_body
    arrays = ag__.converted_call(ag__.ld(tf).tensor_scatter_nd_update, (ag__.ld(arrays), ag__.converted_call(ag__.ld(tf).concat, ([ag__.converted_call(ag__.ld(tf).range, (ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(arrays),), None, fscope)[0],), None, fscope)[:, ag__.ld(tf).newaxis], ag__.ld(indices)[:, ag__.ld(tf).newaxis]],), dict(axis=(- 1)), fscope), ag__.ld(values)), None, fscope)
ValueError: in user code:
    File ""<ipython-input-3-dcfd69f4f71e>"", line 12, in overwrite  *
        arrays = tf.tensor_scatter_nd_update(
    ValueError: Indices and updates specified for empty input for '{{node cond/TensorScatterUpdate}} = TensorScatterUpdate[T=DT_FLOAT, Tindices=DT_INT32](cond/TensorScatterUpdate/arrays, cond/concat, cond/TensorScatterUpdate/values)' with input shapes: [4,0], [4,2], [4].
```
</details>"
57491,How to replace an existing op with a custom op without retraining?,"### My question

Hi there,

I have an old model whose training code is lost, but I still have the saved_model format file. I want to optimize this model by writing some custom ops and replace the original op because I know it is a BERT-like model. 

My question is, once I have written the custom op, how can I ask the original model to replace its op with my custom op? I only have a driver code written in C++, so I cannot use my custom op retrain a new model.


### Standalone code to reproduce the issue

The driver's code is written in C++. 

```C++
int main(int argc, char** argv)
{
   const string pathToGraph = PATH_TO_PBDIR;
   Status status;
   tensorflow::SessionOptions sessionOptions;
   tensorflow::RunOptions runOptions;
   tensorflow::SavedModelBundle savedModelBundle;
   auto deviceCountMap = sessionOptions.config.mutable_device_count();
   (*deviceCountMap)[string(""GPU"")] = 1;
   sessionOptions.config.set_log_device_placement(true);
   sessionOptions.config.mutable_graph_options()->mutable_optimizer_options()->set_do_function_inlining(true);
   sessionOptions.config.mutable_experimental()->set_optimize_for_static_graph(true);
   sessionOptions.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(0.5);
   sessionOptions.config.mutable_gpu_options()->set_allow_growth(true);
  string path = pathToGraph;
  Status ret = LoadSessionBundleOrSavedModelBundle(sessionOptions, runOptions, path, {""serve""}, &savedModelBundle);
  Session* session = savedModelBundle.session.get();
  
  std::vector<std::pair<string, Tensor>> input;
  prepare_inputs(input);

  std::vector<tensorflow::Tensor> normal_answer;
  string tmp = OUTPUT_TENSOR_NAME;
  status = session->Run(input, outname, {}, &normal_answer);

  return 0;
```
  
### Issue Type

Support

### Source

source

### Tensorflow Version

tf 1.15

### Custom Code

Yes


"
57490,Converting a TF model to TFLite and then to EdgeTPU,"I'm trying to convert a simple add model from TF to TFLite to EdgeTPU
However, it seems this conversion is caught in a catch 22 scenario.
If I specify int8 data type as below, it ends up wanting to use the FlexAddV2 TF op, which fails to convert to EdgeTPU.
If I specify as int32 data type instead, it is able to use regular ADD op, but the data type is incompatible with EdgeTPU and says the data type is supported so it only runs on CPU. Is there any way to get this to convert for EdgeTPU as INT8 so it can run on TPU?


### 1. System information

- OS Platform and Distribution: Linux Ubuntu 20.04.4 LTS (Focal Fossa)
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.9.1

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
input = keras.Input(shape=(32,), name=""dummy_input"", dtype=tf.int8)
output = tf.add(input, 1)
model = keras.Model(inputs=input, outputs=output)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_quant_model = converter.convert()

$ edgetpu_compiler -s testmodel.tflite
Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.
ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
ERROR: Node number 0 (FlexAddV2) failed to prepare.

Compilation failed: Model failed in Tflite interpreter. Please ensure model can be loaded/run in Tflite interpreter.
Compilation child process completed within timeout period.
Compilation failed!



### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

INT8 conversion, resulting in unsupported op:

edgetpu_compiler -s testmodel.tflite
Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.
ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
ERROR: Node number 0 (FlexAddV2) failed to prepare.

Compilation failed: Model failed in Tflite interpreter. Please ensure model can be loaded/run in Tflite interpreter.
Compilation child process completed within timeout period.
Compilation failed!



INT32 conversion, resulting in invalid data type for TPU:

$ edgetpu_compiler -s testmodel.tflite
Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.

Model compiled successfully in 0 ms.

Input model: testmodel.tflite
Input size: 936.00B
Output model: testmodel_edgetpu.tflite
Output size: 476.00B
On-chip memory used for caching model parameters: 0.00B
On-chip memory remaining for caching model parameters: 0.00B
Off-chip memory used for streaming uncached model parameters: 0.00B
Number of Edge TPU subgraphs: 0
Total number of operations: 1
Operation log: testmodel_edgetpu.log

Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.
Number of operations that will run on Edge TPU: 0
Number of operations that will run on CPU: 1

Operator                       Count      Status

ADD                            1          Operation is working on an unsupported data type
Compilation child process completed within timeout period.
Compilation succeeded!




"
57489,`tf.sparse.to_dense` lack support for qint,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
NotFoundError raises when calling `tf.sparse.to_dense` with qint input.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
num_rows = tf.random.uniform([], minval=0, maxval=5, dtype=tf.int32)

num_columns = None
dtype = tf.qint16
y = tf.sparse.eye(num_rows, num_columns=num_columns, dtype=dtype, )
print(y)
x = tf.sparse.to_dense(y)
```


### Relevant log output

```shell
SparseTensor(indices=tf.Tensor(
[[0 0]
 [1 1]
 [2 2]], shape=(3, 2), dtype=int64), values=tf.Tensor([1 1 1], shape=(3,), dtype=qint16), dense_shape=tf.Tensor([3 3], shape=(2,), dtype=int64))
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node SparseToDense}} = SparseToDense[T=DT_QINT16, Tindices=DT_INT64, validate_indices=true]
All kernels registered for op SparseToDense:
  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT32]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT32]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT64]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT64]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_BOOL]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_BOOL]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_INT32]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_INT8]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_INT8]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_UINT8]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_UINT8]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_INT16]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_INT16]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_UINT16]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_UINT16]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_UINT32]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_UINT32]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_INT64]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_INT64]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_UINT64]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_UINT64]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tindices in [DT_INT32]
 [Op:SparseToDense]
```
</details>"
57488,`tf.raw_ops.UnicodeDecodeWithOffsets` crashes,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Session crashes.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
input = ""text""
input_encoding = ""utf-8""
errors = ""replace""
replacement_char = 65533
replace_control_characters = False
Tsplits = 3.0
result =  tf.raw_ops.UnicodeDecodeWithOffsets(input=input, input_encoding=input_encoding, errors=errors, replacement_char=replacement_char, replace_control_characters=replace_control_characters, Tsplits=Tsplits, )
```


### Relevant log output

```shell
F tensorflow/core/framework/tensor.cc:725] Check failed: dtype() == expected_dtype (9 vs. 3) int32 expected, got int64
abort (core dumped)
```
</details>"
57487,`tf.math.bincount` crashes,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Session crashes when call bincount: abort (core dumped)
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
arr = tf.random.uniform([2, 4], minval=1.0, maxval=3.0, dtype=tf.float32)
weights = 4
axis = -1

r = tf.math.bincount(arr, weights=weights, axis=axis )
```


### Relevant log output

```shell
abort (core dumped)
```
</details>"
57486,`tf.linalg.sqrtm` crashes when computing gradient for empty input,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
floating point exception (core dumped) when compute gradient.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x = tf.random.uniform([3, 0, 0, 0], minval=0.2782671520237796, maxval=1.0, dtype=tf.float32)
print(tf.linalg.sqrtm(x))

with tf.GradientTape() as g:
    g.watch(x)
    res_backward = tf.linalg.sqrtm(x)
print(res_backward)
grad = g.jacobian(res_backward, [x])
print(grad)
```


### Relevant log output

```shell
tf.Tensor([], shape=(3, 0, 0, 0), dtype=float32)
tf.Tensor([], shape=(3, 0, 0, 0), dtype=float32)
floating point exception (core dumped)
```
</details>"
57485,tensorflow dataset map py_function returns <unknown> shaped tensors and results error with subclassed models with custom train_step,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

SMP Debian 4.19.249-2 GNU/Linux

### Mobile device

_No response_

### Python version

3.7.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.4

### GPU model and memory

Tested on T4 and A100

### Current Behaviour?

```shell
Using tf.data class with map and py_function results in a data generator that returns tensors with unknown shape: TensorSpec(shape=<unknown>, dtype=tf.int32, name=None).

Using this data generator to train a subclassed Model with custom train_step results in errors.

However, when this generator is iterated outside loop and tensors are obtained as below:

for X, y in tf_dataset:
    break
model(X)

The network is able to forward-propagate it.

Moreover, defining the same model with functional API does not result in error.
Yet, it does not work with Model subclassing.
```


### Standalone code to reproduce the issue

```shell

import tensorflow as tf
import numpy as np

# Lightweight data generator function
def idx_generator():
    while True:
        idx = np.random.randint(0, 111, 1024)
        yield idx

# Data preprocessor function to carry out complex processing        
def data_generator(idx):
    return (idx ** 2), 0

# Creating tf.dataset, mapping, batching, etc.
batch_size = 8
tf_dataset = tf.data.Dataset.from_generator(lambda: idx_generator(),
                                            output_signature= tf.TensorSpec(shape=1024, dtype=tf.int32))
tf_dataset = tf_dataset.map(lambda x: tf.py_function(data_generator, [x], [tf.int32, tf.int32]), num_parallel_calls=tf.data.AUTOTUNE)
tf_dataset = tf_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

# Model Class
class MoModel(tf.keras.Model):
    def __init__(self):
        super(MoModel, self).__init__()
        self.embedding_layer = tf.keras.layers.Embedding(32001, 64)
        self.lstm_layer = tf.keras.layers.LSTM(32)
        self.dense_layer = tf.keras.layers.Dense(1)
        
        self.loss = tf.keras.losses.MeanSquaredError()
        self.loss_metric = tf.keras.metrics.Mean(name=""loss"")
        
    def call(self, inputs):
        x = self.embedding_layer(inputs)
        x = self.lstm_layer(x)
        x = self.dense_layer(x)
            
        return x
    def train_step(self, inputs):
        X, y = inputs

        with tf.GradientTape() as tape:
            predictions = self(X, training = True)
            loss = self.loss_function(y, predictions)

            gradients = tape.gradient(loss, self.trainable_variables)
            self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
            self.loss_metric.update_state(loss)

            return {""loss"": self.loss_metric.result()}

# Construct and compile
model = MoModel()
model.compile(optimizer = 'adam')

# Fit results in error
model.fit(tf_dataset, epochs = 1, verbose = 1, steps_per_epoch = 100)
```


### Relevant log output

```shell
TypeError: in user code:

    File ""/opt/conda/lib/python3.7/site-packages/keras/engine/training.py"", line 1021, in train_function  *
        return step_function(self, iterator)
    File ""/opt/conda/lib/python3.7/site-packages/keras/engine/training.py"", line 1010, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/opt/conda/lib/python3.7/site-packages/keras/engine/training.py"", line 1000, in run_step  **
        outputs = model.train_step(data)
    File ""/tmp/ipykernel_5583/1915950636.py"", line 27, in train_step
        predictions = self(X, training = True)
    File ""/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None

    TypeError: Exception encountered when calling layer ""mo_model_16"" (type MoModel).
    
    in user code:
    
        File ""/tmp/ipykernel_5583/1915950636.py"", line 17, in call  *
            x = self.lstm_layer(x)
        File ""/opt/conda/lib/python3.7/site-packages/keras/layers/recurrent.py"", line 679, in __call__  **
            return super(RNN, self).__call__(inputs, **kwargs)
        File ""/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
            raise e.with_traceback(filtered_tb) from None
        File ""/opt/conda/lib/python3.7/site-packages/keras/layers/recurrent_v2.py"", line 1151, in call
            timesteps = input_shape[0] if self.time_major else input_shape[1]
    
        TypeError: Exception encountered when calling layer ""lstm_19"" (type LSTM).
        
        'NoneType' object is not subscriptable
        
        Call arguments received:
          • inputs=tf.Tensor(shape=<unknown>, dtype=float32)
          • mask=None
          • training=True
          • initial_state=None
    
    
    Call arguments received:
      • inputs=tf.Tensor(shape=<unknown>, dtype=int32)
```
</details>"
57483,Tensorflow macOS 12.5.1 ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

MacOS 12.5

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I followed this tutorial: https://www.youtube.com/watch?v=4nY5lDBXdOg to install tensorflow in my Mac. But I still do not manage to run tensorflow in my jupyter lab
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

```shell
--------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/__init__.py:61, in <module>
     60 sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)
---> 61 from tensorflow.python import pywrap_tensorflow
     62 sys.setdlopenflags(_default_dlopen_flags)

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py:28, in <module>
     27         return _mod
---> 28 _pywrap_tensorflow = swig_import_helper()
     29 del swig_import_helper

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py:24, in swig_import_helper()
     23 try:
---> 24     _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
     25 finally:

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/imp.py:243, in load_module(name, file, filename, details)
    242     else:
--> 243         return load_dynamic(name, filename, file)
    244 elif type_ == PKG_DIRECTORY:

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/imp.py:343, in load_dynamic(name, path, file)
    341 spec = importlib.machinery.ModuleSpec(
    342     name=name, loader=loader, origin=path)
--> 343 return _load(spec)

ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow.so, 0x000A): tried: '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow.so' (mach-o file, but is an incompatible architecture (have (x86_64), need (arm64e)))

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
Input In [1], in <cell line: 1>()
----> 1 import tensorflow as tf

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/__init__.py:24, in <module>
     21 from __future__ import print_function
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26 
     27 # Lazily import the `tf.contrib` module. This avoids loading all of the
     28 # dependencies of `tf.contrib` at `import tensorflow` time.
     29 class _LazyContribLoader(object):

File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/__init__.py:72, in <module>
     67 except ImportError:
     68   msg = """"""%s\n\nFailed to load the native TensorFlow runtime.\n
     69 See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\n
     70 for some common reasons and solutions.  Include the entire stack trace
     71 above this error message when asking for help."""""" % traceback.format_exc()
---> 72   raise ImportError(msg)
     74 # Protocol buffers
     75 from tensorflow.core.framework.graph_pb2 import *

ImportError: Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow.so, 0x000A): tried: '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow.so' (mach-o file, but is an incompatible architecture (have (x86_64), need (arm64e)))


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
​
```
</details>"
57482,Custom - minmax pooling - Keras - Tensorflow,"I created a custom pooling layer using tensorflow layer subclassing. Here is the code:

```
class Min_Max_Pooling(tf.keras.layers.Layer):
    
  def __init__(self, filter_size):
    super(Min_Max_Pooling, self).__init__()
    self.filter_size = filter_size

  def call(self, inputs):
    print('------------------------------------------------------')
    print(f'inputs shape = {inputs.shape[-1]}')
    print(f'filter_size = {self.filter_size}')
    result = tf.zeros((int((inputs.shape[-1]/self.filter_size)*2)))

    num_splits_length = int(inputs.shape[-1]/self.filter_size)
    print(f'num_splits_length = {num_splits_length}')
    print(f'result_length = {result.shape[-1]}')

    split_sequence = tf.split(inputs, num_or_size_splits=num_splits_length, axis=-1)

    count = 0
    index = 0

    def cond(count,result,index):
          return tf.less(count,len(split_sequence))

    def body(count,result,index):
          # print(tf.gather(split_sequence, count))
          max = tf.reduce_max(tf.gather(split_sequence, count))
          min = tf.reduce_min(tf.gather(split_sequence, count))

          index_max = tf.argmax(tf.gather(split_sequence, count))
          index_min = tf.argmin(tf.gather(split_sequence, count))

          indices = [[index], [index+1]]    
          
          if tf.cond(tf.greater(index_max , index_min), lambda: tf.constant(True), lambda: tf.constant(False)):
            updates = [min,max]

          else:
            updates = [max,min]
            
          result = tf.tensor_scatter_nd_update(result, indices, updates)
          index +=2
          count+=1
          # print(f'result_shape = {result.shape[-1]}')
          return [count,result,index]

    output = tf.while_loop(cond, body, [count,result,index])[1]

    return output
```
The motive of this layer is to downsample a timeseries something like:

original:
[![enter image description here][1]][1]
Later:
[![enter image description here][2]][2]

The layer accepts a filter size and a tensor(timeseries) then it splits the tensor in to chunks according to filter size and then it loops over them calculating the min and max values of chunk and updates it to 'result' tensor using 'tf.tensor_scatter_nd_update' method according to there index (if max value comes before the min value then it first appends the max value and then the min value so as to not disturb the sequence of time series).

I the created the model using keras functional api
here is the code:

```
input_layer = tf.keras.layers.Input(shape=(1000), name=""input_layer"")
layer_1 = Min_Max_Pooling(filter_size=4)(input_layer)

model = tf.keras.models.Model(input_layer, layer_1, name=""model"")

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss=""categorical_crossentropy"", run_eagerly=True)

print(model.summary())
```

After compiling the model i did model.predict here are the results:
```
data = pd.read_csv('/content/drive/MyDrive/stock.csv', parse_dates=False,
                   index_col=1)

tensor = data.close.head(1000).to_numpy() 
tensor = tensor / max(tensor)
tensor = tf.convert_to_tensor(tensor)
# print(tensor)
# print(model.summary())
# tensor = tf.reshape(tensor, (1000))

result = model.predict(tensor)
```
output :
```
------------------------------------------------------
inputs shape = 1000
filter_size = 4
num_splits_length = 250
result_length = 500
Model: ""model""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_layer (InputLayer)    [(None, 1000)]            0         
                                                                 
 min__max__pooling_16 (Min_M  (500,)                   0         
 ax_Pooling)                                                     
                                                                 
=================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
_________________________________________________________________
None
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 32
filter_size = 4
num_splits_length = 8
result_length = 16
------------------------------------------------------
inputs shape = 8
filter_size = 4
num_splits_length = 2
result_length = 4


```

my problem is why is input shape changing from 1000 to 32 and why is the function called multiple times with parts of input of shape 32 and also if you change the filter size to 5 it results in to error as 32 is not completely divisible by 5.
I don't understand what is the problem here.
Dose anyone know how to solve this problem.

Here is the full code to reproduce the error :

```
import tensorflow as tf
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
plt.rcParams['figure.figsize'] = [25, 5]


class Min_Max_Pooling(tf.keras.layers.Layer):
    
  def __init__(self, filter_size):
    super(Min_Max_Pooling, self).__init__()
    self.filter_size = filter_size

  def call(self, inputs):
    print('------------------------------------------------------')
    # print(f'inputs = {inputs}')
    print(f'inputs shape = {inputs.shape[-1]}')
    print(f'filter_size = {self.filter_size}')
    # print(f'remainder =  {int(inputs.shape[-1]%self.filter_size)}')
    result = tf.zeros((int((inputs.shape[-1]/self.filter_size)*2)))

    num_splits_length = int(inputs.shape[-1]/self.filter_size)
    print(f'num_splits_length = {num_splits_length}')
    print(f'result_length = {result.shape[-1]}')

    split_sequence = tf.split(inputs, num_or_size_splits=num_splits_length, axis=-1)

    count = 0
    index = 0

    def cond(count,result,index):
          return tf.less(count,len(split_sequence))

    def body(count,result,index):
          # print(tf.gather(split_sequence, count))
          max = tf.reduce_max(tf.gather(split_sequence, count))
          min = tf.reduce_min(tf.gather(split_sequence, count))

          index_max = tf.argmax(tf.gather(split_sequence, count))
          index_min = tf.argmin(tf.gather(split_sequence, count))

          indices = [[index], [index+1]]    
          
          if tf.cond(tf.greater(index_max , index_min), lambda: tf.constant(True), lambda: tf.constant(False)):
            updates = [min,max]

          else:
            updates = [max,min]
            
          result = tf.tensor_scatter_nd_update(result, indices, updates)
          index +=2
          count+=1
          # print(f'result_shape = {result.shape[-1]}')
          return [count,result,index]

    output = tf.while_loop(cond, body, [count,result,index])[1]

    return output


input_layer = tf.keras.layers.Input(shape=(1000), name=""input_layer"")
lambda_layer_1 = Min_Max_Pooling(filter_size=4)(input_layer)

model = tf.keras.models.Model(input_layer, lambda_layer_1, name=""model"")

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss=""categorical_crossentropy"", run_eagerly=True)

print(model.summary())

data = pd.read_csv('/content/drive/MyDrive/ADANIPORTS.csv', parse_dates=False,
                   index_col=1)

tensor = data.close.head(1000).to_numpy() 
tensor = tensor / max(tensor)
tensor = tf.convert_to_tensor(tensor)
print(tensor)
print(model.summary())
tensor = tf.reshape(tensor, (1000))

result = model.predict(tensor)

plt.plot(tensor)
plt.show()
plt.plot(result)
plt.show()


```
as i can't upload the stock file i will put the code to generate the tensor here or you can use any timeseries data.

```
a = [0.9921363 , 0.97597204, 0.9708752 , 0.9781564 , 0.97917577,
       0.98733071, 0.98689384, 0.98383574, 0.98470948, 0.98034076,
       0.97771953, 0.97655454, 0.97742828, 0.97728266, 0.97670016,
       0.97830202, 0.98441823, 0.98339886, 0.99009757, 0.9927188 ,
       0.98893258, 0.9860201 , 0.98733071, 0.98645697, 0.98514635,
       0.98500073, 0.98529198, 0.98616572, 0.98529198, 0.98354449,
       0.98354449, 0.97975826, 0.97801078, 0.97946702, 0.97946702,
       0.98034076, 0.97990389, 0.97961264, 0.97786515, 0.97830202,
       0.97859327, 0.9781564 , 0.97903014, 0.98150575, 0.98208825,
       0.98354449, 0.98150575, 0.98019514, 0.97859327, 0.97801078,
       0.98004951, 0.98136013, 0.981797  , 0.98019514, 0.98063201,
       0.98077763, 0.97946702, 0.98150575, 0.97961264, 0.97975826,
       0.97975826, 0.98063201, 0.97932139, 0.97801078, 0.97655454,
       0.97626329, 0.9750983 , 0.97713703, 0.981797  , 0.98106888,
       0.98034076, 0.97830202, 0.97844765, 0.97786515, 0.97247706,
       0.97291394, 0.96971021, 0.96941896, 0.97000146, 0.97364206,
       0.96869084, 0.96956458, 0.96854522, 0.96490462, 0.96577836,
       0.96708898, 0.9672346 , 0.96403087, 0.96344838, 0.96315713,
       0.963594  , 0.96373962, 0.96432212, 0.96388525, 0.96257463,
       0.96140964, 0.96490462, 0.96330275, 0.96621523, 0.96592398,
       0.96403087, 0.96184651, 0.96111839, 0.96082714, 0.96199214,
       0.96242901, 0.96257463, 0.96272026, 0.96432212, 0.96432212,
       0.96330275, 0.96315713, 0.96228338, 0.96082714, 0.96111839,
       0.96140964, 0.96068152, 0.95733217, 0.95893403, 0.95485656,
       0.95602155, 0.95514781, 0.95602155, 0.95514781, 0.95645842,
       0.95747779, 0.95602155, 0.95602155, 0.9568953 , 0.95791466,
       0.96111839, 0.95820591, 0.95776904, 0.95849716, 0.95806029,
       0.9593709 , 0.95878841, 0.96432212, 0.96140964, 0.9593709 ,
       0.95922528, 0.95864278, 0.95922528, 0.95878841, 0.95966215,
       0.95776904, 0.95485656, 0.95136158, 0.9532547 , 0.95529343,
       0.95034222, 0.94946847, 0.9532547 , 0.95529343, 0.96606961,
       0.96126402, 0.96257463, 0.96039027, 0.95820591, 0.96009902,
       0.95791466, 0.95340032, 0.95063346, 0.9484491 , 0.95310907,
       0.95296345, 0.95427406, 0.94990534, 0.94859473, 0.95121596,
       0.95063346, 0.95558468, 0.95645842, 0.96039027, 0.96636086,
       0.96461337, 0.96184651, 0.96563274, 0.96286588, 0.96170089,
       0.96330275, 0.9599534 , 0.96068152, 0.95820591, 0.95485656,
       0.95718654, 0.95602155, 0.96009902, 0.9593709 , 0.95747779,
       0.9557303 , 0.95762342, 0.95558468, 0.95529343, 0.95558468,
       0.95616718, 0.95529343, 0.95543906, 0.95704092, 0.95340032,
       0.95296345, 0.95310907, 0.94874035, 0.94946847, 0.94874035,
       0.94859473, 0.94917722, 0.95136158, 0.94801223, 0.94786661,
       0.94801223, 0.94830348, 0.94568225, 0.94349789, 0.94262414,
       0.94568225, 0.94364351, 0.94408038, 0.93956604, 0.94102228,
       0.93781855, 0.93621669, 0.93432358, 0.9314111 , 0.93607106,
       0.92223678, 0.92325615, 0.92121742, 0.9229649 , 0.92325615,
       0.92806174, 0.92908111, 0.92849862, 0.92951798, 0.93155672,
       0.93126547, 0.93199359, 0.93155672, 0.93199359, 0.93010048,
       0.92995486, 0.9308286 , 0.93111985, 0.9308286 , 0.93053735,
       0.93039173, 0.93199359, 0.93257609, 0.93126547, 0.92980923,
       0.93039173, 0.92995486, 0.9277705 , 0.92238241, 0.92442114,
       0.92267366, 0.9229649 , 0.92092617, 0.92529489, 0.92616863,
       0.92645988, 0.92471239, 0.92500364, 0.92806174, 0.92995486,
       0.92966361, 0.93010048, 0.92908111, 0.92922674, 0.92864424,
       0.92908111, 0.93053735, 0.93330421, 0.93199359, 0.93286734,
       0.93199359, 0.93199359, 0.93199359, 0.93053735, 0.9314111 ,
       0.9308286 , 0.93126547, 0.93243046, 0.93315858, 0.92995486,
       0.92878986, 0.92893549, 0.92878986, 0.92849862, 0.92427552,
       0.92194554, 0.92005242, 0.92223678, 0.9199068 , 0.91524683,
       0.91116936, 0.90563565, 0.90549002, 0.9011213 , 0.90286879,
       0.90461628, 0.90942187, 0.90913062, 0.90330566, 0.90505315,
       0.90723751, 0.90403378, 0.90214067, 0.90286879, 0.90286879,
       0.90330566, 0.90214067, 0.89325761, 0.89121887, 0.88947138,
       0.89180137, 0.88845202, 0.89267511, 0.89471385, 0.89587884,
       0.89631571, 0.89981069, 0.89718946, 0.89296636, 0.89573322,
       0.89908257, 0.89631571, 0.89515072, 0.89573322, 0.90214067,
       0.90184942, 0.90316004, 0.89995631, 0.89995631, 0.89893694,
       0.89981069, 0.9017038 , 0.90155818, 0.90126693, 0.89835445,
       0.90097568, 0.89908257, 0.90199505, 0.90126693, 0.9017038 ,
       0.9011213 , 0.89908257, 0.89602446, 0.89617009, 0.89558759,
       0.8974807 , 0.89718946, 0.8986457 , 0.89820882, 0.90010194,
       0.89995631, 0.8974807 , 0.89587884, 0.88976263, 0.89107325,
       0.88277268, 0.88015145, 0.88058832, 0.88102519, 0.88000582,
       0.8798602 , 0.88335518, 0.87956895, 0.88248143, 0.87811271,
       0.87840396, 0.8756371 , 0.87330712, 0.87083151, 0.87112276,
       0.85277414, 0.85772535, 0.85539537, 0.85976409, 0.86486093,
       0.86544343, 0.87039464, 0.87199651, 0.87636522, 0.87461774,
       0.87490899, 0.87549148, 0.87811271, 0.88204456, 0.88495704,
       0.8865589 , 0.89354886, 0.89092762, 0.89617009, 0.89311198,
       0.8913645 , 0.88903451, 0.88481142, 0.88510266, 0.88728702,
       0.88699578, 0.890782  , 0.88495704, 0.88510266, 0.88481142,
       0.88495704, 0.88248143, 0.88248143, 0.88102519, 0.88131644,
       0.8840833 , 0.88248143, 0.88102519, 0.88175331, 0.88102519,
       0.88175331, 0.88175331, 0.88524829, 0.88160769, 0.88160769,
       0.8798602 , 0.88087957, 0.88306393, 0.88189894, 0.88248143,
       0.88131644, 0.88058832, 0.8798602 , 0.88000582, 0.87913208,
       0.87956895, 0.88146206, 0.88058832, 0.88248143, 0.87956895,
       0.87956895, 0.87607398, 0.87432649, 0.87403524, 0.87869521,
       0.87738459, 0.87869521, 0.87782146, 0.87418087, 0.87476336,
       0.87592835, 0.87520023, 0.87461774, 0.87592835, 0.87374399,
       0.89252949, 0.88903451, 0.88510266, 0.8871414 , 0.89413135,
       0.89689821, 0.89922819, 0.89704383, 0.89762633, 0.89267511,
       0.89471385, 0.89660696, 0.8974807 , 0.89791758, 0.90243192,
       0.90417941, 0.90403378, 0.90403378, 0.90403378, 0.90345129,
       0.90214067, 0.90257754, 0.90316004, 0.90374254, 0.90505315,
       0.90330566, 0.90505315, 0.9059269 , 0.90738314, 0.90811126,
       0.90869375, 0.91218873, 0.91510121, 0.91422746, 0.91146061,
       0.90650939, 0.90854813, 0.90636377, 0.91422746, 0.91524683,
       0.9199068 , 0.91830494, 0.92194554, 0.92223678, 0.92311053,
       0.9193243 , 0.92107179, 0.91976118, 0.91917868, 0.91917868,
       0.91743119, 0.91480996, 0.9156837 , 0.9168487 , 0.91917868,
       0.9193243 , 0.91393622, 0.90782001, 0.90257754, 0.90199505,
       0.90650939, 0.90403378, 0.9095675 , 0.91087811, 0.91437309,
       0.91408184, 0.91815931, 0.91917868, 0.92150866, 0.92398427,
       0.92573176, 0.92398427, 0.92704238, 0.92864424, 0.93053735,
       0.93519732, 0.93709043, 0.93796418, 0.93781855, 0.93665356,
       0.93577982, 0.93374108, 0.93359546, 0.9344692 , 0.93301296,
       0.93330421, 0.93272171, 0.93476045, 0.93359546, 0.93228484,
       0.93344983, 0.93490607, 0.93476045, 0.93461482, 0.93417795,
       0.93927479, 0.945391  , 0.94728411, 0.95005097, 0.9526722 ,
       0.94801223, 0.94553662, 0.95077909, 0.94946847, 0.95034222,
       0.95092471, 0.95034222, 0.94786661, 0.95005097, 0.94975972,
       0.94990534, 0.94684724, 0.94859473, 0.95310907, 0.95645842,
       0.95441969, 0.95238095, 0.95238095, 0.95558468, 0.95558468,
       0.9532547 , 0.95441969, 0.95398282, 0.95427406, 0.95383719,
       0.95150721, 0.9484491 , 0.94975972, 0.95019659, 0.95281782,
       0.95194408, 0.95150721, 0.95063346, 0.95281782, 0.95471094,
       0.95369157, 0.95529343, 0.95776904, 0.95398282, 0.95383719,
       0.95471094, 0.95310907, 0.95369157, 0.95310907, 0.95136158,
       0.94946847, 0.94874035, 0.94699286, 0.95238095, 0.95165283,
       0.96082714, 0.96621523, 0.97014708, 0.96810834, 0.9708752 ,
       0.96839959, 0.96796272, 0.96767147, 0.96985583, 0.97233144,
       0.96956458, 0.97058395, 0.96796272, 0.97218582, 0.97058395,
       0.97174894, 0.97305956, 0.97305956, 0.97582642, 0.97742828,
       0.97670016, 0.97830202, 0.9787389 , 0.97830202, 0.9787389 ,
       0.97611766, 0.97699141, 0.97407893, 0.97597204, 0.97582642,
       0.97131207, 0.97568079, 0.97713703, 0.97568079, 0.97407893,
       0.97626329, 0.97553517, 0.9775739 , 0.97684578, 0.97466142,
       0.97495267, 0.97626329, 0.97684578, 0.97568079, 0.97597204,
       0.97655454, 0.97582642, 0.97538954, 0.97728266, 0.97728266,
       0.97713703, 0.97975826, 0.9812145 , 0.97946702, 0.98019514,
       0.9812145 , 0.9787389 , 0.97786515, 0.9787389 , 0.97742828,
       0.97670016, 0.97684578, 0.97524392, 0.97728266, 0.97801078,
       0.97917577, 0.97728266, 0.97568079, 0.97786515, 0.9781564 ,
       0.98077763, 0.98004951, 0.9787389 , 0.98310762, 0.981797  ,
       0.9812145 , 0.9702927 , 0.96650648, 0.97466142, 0.97276831,
       0.97553517, 0.97466142, 0.97335081, 0.97291394, 0.97437018,
       0.9739333 , 0.98092326, 1.        , 0.99606815, 0.99286442,
       0.9927188 , 0.99111694, 0.99315567, 0.99111694, 0.9884957 ,
       0.98470948, 0.98616572, 0.98587447, 0.98616572, 0.98864133,
       0.98674822, 0.98951507, 0.99024319, 0.99053444, 0.9890782 ,
       0.9890782 , 0.98922382, 0.98747634, 0.9890782 , 0.98820446,
       0.98514635, 0.98296199, 0.98281637, 0.98412698, 0.98267074,
       0.98456386, 0.98208825, 0.98165138, 0.9708752 , 0.9775739 ,
       0.97932139, 0.97859327, 0.98019514, 0.98034076, 0.98106888,
       0.98048638, 0.98150575, 0.98194262, 0.98296199, 0.981797  ,
       0.98296199, 0.98223387, 0.98267074, 0.9823795 , 0.98310762,
       0.98441823, 0.98354449, 0.98296199, 0.98281637, 0.98194262,
       0.98339886, 0.98339886, 0.98456386, 0.98412698, 0.98558322,
       0.98470948, 0.97888452, 0.96985583, 0.97437018, 0.97728266,
       0.9787389 , 0.98703946, 0.98034076, 0.97859327, 0.98048638,
       0.97830202, 0.97932139, 0.98980632, 0.99038882, 0.99417504,
       0.99082569, 0.9933013 , 0.99199068, 0.9890782 , 0.9921363 ,
       0.99097131, 0.99009757, 0.98936945, 0.99009757, 0.98878695,
       0.98631134, 0.98660259, 0.99038882, 0.98660259, 0.98354449,
       0.97990389, 0.97888452, 0.97932139, 0.97422455, 0.97713703,
       0.97771953, 0.97655454, 0.97684578, 0.97859327, 0.97204019,
       0.97072958, 0.97174894, 0.97320518, 0.97335081, 0.97305956,
       0.97160332, 0.97276831, 0.97276831, 0.9714577 , 0.97160332,
       0.97262269, 0.96985583, 0.96752585, 0.96854522, 0.96927334,
       0.96985583, 0.96956458, 0.97102082, 0.97072958, 0.97378768,
       0.97131207, 0.97058395, 0.97043833, 0.97335081, 0.97422455,
       0.97291394, 0.97349643, 0.97320518, 0.97218582, 0.9702927 ,
       0.96927334, 0.96636086, 0.96767147, 0.96927334, 0.96636086,
       0.96403087, 0.96126402, 0.96272026, 0.96577836, 0.96636086,
       0.97189457, 0.97014708, 0.96563274, 0.96941896, 0.96912771,
       0.97116645, 0.96912771, 0.97378768, 0.97699141, 0.97713703,
       0.97568079, 0.97480705, 0.97335081, 0.97131207, 0.97116645,
       0.97072958, 0.97189457, 0.97058395, 0.97102082, 0.96796272,
       0.96592398, 0.96898209, 0.96548711, 0.96839959, 0.96738022,
       0.96796272, 0.96912771, 0.96694335, 0.97102082, 0.96985583,
       0.96636086, 0.9672346 , 0.9672346 , 0.96548711, 0.9672346 ,
       0.96577836, 0.96825397, 0.96694335, 0.96752585, 0.96825397,
       0.96854522, 0.97058395, 0.97058395, 0.96927334, 0.97072958,
       0.96767147, 0.97276831, 0.97116645, 0.97000146, 0.97014708,
       0.96985583, 0.96373962, 0.96839959, 0.96694335, 0.96796272,
       0.96738022, 0.96752585, 0.96898209, 0.96927334, 0.9714577 ,
       0.97174894, 0.96883646, 0.96839959, 0.96854522, 0.97131207,
       0.97000146, 0.97043833, 0.96854522, 0.97174894, 0.97189457,
       0.97204019, 0.97189457, 0.97204019, 0.97233144, 0.97160332,
       0.97932139, 0.98470948, 0.98019514, 0.98427261, 0.9812145 ,
       0.98412698, 0.9884957 , 0.99097131, 0.9927188 , 0.98936945,
       0.98835008, 0.98354449, 0.98339886, 0.98412698, 0.98354449,
       0.98267074, 0.98019514, 0.97233144, 0.97626329, 0.97407893,
       0.97247706, 0.96985583, 0.96548711, 0.96898209, 0.96621523,
       0.96941896, 0.97174894, 0.96883646, 0.96883646, 0.9702927 ,
       0.96941896, 0.97058395, 0.96869084, 0.96796272, 0.96577836,
       0.96082714, 0.96286588, 0.96140964, 0.96155526, 0.96213776,
       0.96286588, 0.96592398, 0.96694335, 0.96286588, 0.96432212,
       0.96592398, 0.96199214, 0.96272026, 0.96344838, 0.96446774,
       0.96213776, 0.96315713, 0.96140964, 0.96257463, 0.96519586,
       0.96257463, 0.95587593, 0.95412844, 0.95383719, 0.95136158,
       0.94611912, 0.95383719, 0.95529343, 0.95733217, 0.95820591,
       0.96388525, 0.96606961, 0.96199214, 0.96446774, 0.97174894,
       0.97262269, 0.97364206, 0.97305956, 0.97538954, 0.97233144,
       0.97917577, 0.97742828, 0.97276831, 0.96883646, 0.97014708,
       0.97670016, 0.97713703, 0.9781564 , 0.97495267, 0.96941896,
       0.97160332, 0.97072958, 0.96941896, 0.96985583, 0.96898209,
       0.97014708, 0.97160332, 0.97072958, 0.97072958, 0.97072958,
       0.9702927 , 0.97102082, 0.96956458, 0.97043833, 0.97043833]
tensor = tf.convert_to_tensor(a,dtype=tf.float32,)
tensor

```

Or is there another way to implement this whole thing?
  [1]: https://i.stack.imgur.com/72bTt.png
  [2]: https://i.stack.imgur.com/uGw5I.png"
57481,`tf.keras.activations.swish` has wrong gradient,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

In the example, `tf.keras.activations.swish` accepts a complex input, and `tf.test.compute_gradient` checks the theoretical and numerical gradient, and the two gradients are different. For example, on [0,1], the theoretical gradient is `0.56998741`, and the numerical gradient is `-0.56998753`.

Probably related to https://github.com/tensorflow/tensorflow/issues/57357



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.complex(tf.random.uniform([1, 20, 1], maxval=0, dtype=tf.float64),tf.random.uniform([1, 20, 1], maxval=1.000000, dtype=tf.float64))

def f(x):
    return tf.keras.activations.swish(x, )
theoretical, numerical = tf.test.compute_gradient(f, [x])
print(theoretical)
print(numerical)

assert tf.experimental.numpy.allclose(theoretical, numerical, atol=1e-3, rtol=1e-3)
```


### Relevant log output

```shell
(array([[ 0.5       ,  0.56998741,  0.        , ...,  0.        ,
         0.        ,  0.        ],
       [-0.56998741,  0.5       ,  0.        , ...,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.5       , ...,  0.        ,
         0.        ,  0.        ],
       ...,
       [ 0.        ,  0.        ,  0.        , ...,  0.5       ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        , ...,  0.        ,
         0.5       ,  0.1744664 ],
       [ 0.        ,  0.        ,  0.        , ...,  0.        ,
        -0.1744664 ,  0.5       ]]),)
(array([[ 0.5       , -0.56998753,  0.        , ...,  0.        ,
         0.        ,  0.        ],
       [ 0.56998728,  0.5       ,  0.        , ...,  0.        ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.5       , ...,  0.        ,
         0.        ,  0.        ],
       ...,
       [ 0.        ,  0.        ,  0.        , ...,  0.5       ,
         0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        , ...,  0.        ,
         0.5       , -0.17446643],
       [ 0.        ,  0.        ,  0.        , ...,  0.        ,
         0.17446637,  0.5       ]]),)
AssertionError
```
</details>"
57480,Wrong gradient for `tf.atan` with complex input,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The theoretical and numerical gradient are different when input is complex for `tf.atan`
```


### Standalone code to reproduce the issue

```
import tensorflow as tf

def fn(x): return tf.atan(x)
x = tf.complex([[0.]], [[1.1262]])

value = fn(x)

theoretical, numerical = tf.test.compute_gradient(fn, [x])
print(theoretical)
print(numerical)

assert tf.experimental.numpy.allclose(theoretical, numerical)

```


### Relevant log output

```shell
(array([[-3.7268043, -0.       ],
       [ 0.       , -3.7268043]], dtype=float32),)
(array([[1604.7687   ,    0.       ],
       [   0.       ,   -3.7269287]], dtype=float32),)
AssertionError
```
</details>"
57479,Can't build Video Classification android app,"I cloned tensorflow/examples repo, 
I am opening video classification app: github.com/tensorflow/examples/tree/master/lite/examples/video_classification/android, but I got the following error:

```
Failed to apply plugin 'com.android.internal.version-check'.
Minimum supported Gradle version is 7.0.2. Current version is 6.8. If using the gradle wrapper, try to change the value in ""\gradle\wrapper\gradle-wrapper.properties"" to
distributionUrl=https\://services.gradle.org/distributions/gradle-7.0.2-bin.zip
```

I solved this by creating gradle-wrapper.properties file, because that file was missing. I am not sure if I only had this issue, or should I create a PR for this ? "
57476,Tensorflow loading RGB image from directory as 5-D tensor,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Using the function 

train_ds = image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=""training"",
  seed=42,
  image_size=(224, 224),
  crop_to_aspect_ratio=True,
  shuffle=True,
  batch_size=BATCH_SIZE)

Generates both the images and labels correctly.
However on performing any data augmentation:

rescale = tf.keras.Sequential([
  layers.Rescaling(1./255)
])
data_augmentation = tf.keras.Sequential([
  layers.RandomFlip(""horizontal_and_vertical""),
  layers.RandomRotation(0.2),
])

def prepare(ds, shuffle=False, augment=False,batch_size=18):
  # Resize and rescale all datasets.
  ds = ds.map(lambda x, y: (rescale(x), y), 
              num_parallel_calls=AUTOTUNE)

  if shuffle:
    ds = ds.shuffle(1000)

  # Batch all datasets.
  ds = ds.batch(batch_size)

  # Use data augmentation only on the training set.
  if augment:
    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), 
                num_parallel_calls=AUTOTUNE)

  # Use buffered prefetching on all datasets.
  return ds.prefetch(buffer_size=AUTOTUNE)

The error shows that image is being passed as 5-D tensor. I believe it is also passing labels with the image data during augmentation (in x instead of y) even though it is defined explicitly in the lambda function.
```


### Standalone code to reproduce the issue

```shell
#Load dataset
train_ds = image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=""training"",
  seed=42,
  image_size=(224, 224),
  crop_to_aspect_ratio=True,
  shuffle=True,
  batch_size=BATCH_SIZE)

#Data augmentation
rescale = tf.keras.Sequential([
  layers.Rescaling(1./255)
])
data_augmentation = tf.keras.Sequential([
  layers.RandomFlip(""horizontal_and_vertical""),
  layers.RandomRotation(0.2),
])

def prepare(ds, shuffle=False, augment=False,batch_size=18):
  # Resize and rescale all datasets.
  ds = ds.map(lambda x, y: (rescale(x), y), 
              num_parallel_calls=AUTOTUNE)

  if shuffle:
    ds = ds.shuffle(1000)

  # Batch all datasets.
  ds = ds.batch(batch_size)

  # Use data augmentation only on the training set.
  if augment:
    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), 
                num_parallel_calls=AUTOTUNE)

  # Use buffered prefetching on all datasets.
  return ds.prefetch(buffer_size=AUTOTUNE)

# Augment the data
train_ds = prepare(train_ds,shuffle=True,augment=True,batch_size=BATCH_SIZE)
```


### Relevant log output

```shell
2022-08-27 12:52:41.780851: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-08-27 12:52:42.109996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21676 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6
Found 540 files belonging to 18 classes.
Using 432 files for training.
Traceback (most recent call last):
  File ""c:\Users\Admin\Documents\GitHub\RoughSpecimenClassification\main.py"", line 34, in <module>
    train_ds = prepare(train_ds,shuffle=True,augment=True,batch_size=BATCH_SIZE)
  File ""c:\Users\Admin\Documents\GitHub\RoughSpecimenClassification\augmentation.py"", line 26, in prepare
    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y),
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2050, in map
    return ParallelMapDataset(
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 5284, in __init__
    self._map_func = structured_function.StructuredFunctionWrapper(
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\tensorflow\python\data\ops\structured_function.py"", line 271, in __init__
    self._function = fn_factory()
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\tensorflow\python\eager\function.py"", line 2567, in get_concrete_function
    graph_function = self._get_concrete_function_garbage_collected(
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\tensorflow\python\eager\function.py"", line 2533, in _get_concrete_function_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\tensorflow\python\eager\function.py"", line 2711, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\tensorflow\python\eager\function.py"", line 2627, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 1141, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\tensorflow\python\data\ops\structured_function.py"", line 248, in wrapped_fn
    ret = wrapper_helper(*args)
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\tensorflow\python\data\ops\structured_function.py"", line 177, in wrapper_helper
    ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 692, in wrapper
    raise e.ag_error_metadata.to_exception(e)
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 689, in wrapper
    return converted_call(f, args, kwargs, options=options)
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 439, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""C:\Users\Admin\AppData\Local\Temp\__autograph_generated_file9cp3lsbh.py"", line 5, in <lambda>
    tf__lam = lambda x, y: ag__.with_function_scope(lambda lscope: (ag__.converted_call(data_augmentation, (x,), dict(training=True), lscope), y), 'lscope', ag__.STD)
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\tensorflow\python\autograph\core\function_wrappers.py"", line 113, in with_function_scope
    return thunk(scope)
  File ""C:\Users\Admin\AppData\Local\Temp\__autograph_generated_file9cp3lsbh.py"", line 5, in <lambda>
    tf__lam = lambda x, y: ag__.with_function_scope(lambda lscope: (ag__.converted_call(data_augmentation, (x,), dict(training=True), lscope), y), 'lscope', ag__.STD)
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 377, in converted_call
    return _call_unconverted(f, args, kwargs, options)
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 458, in _call_unconverted
    return f(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\keras\layers\preprocessing\image_preprocessing.py"", line 392, in call
    raise ValueError('Image augmentation layers are expecting inputs to be '
ValueError: in user code:

    File ""c:\Users\Admin\Documents\GitHub\RoughSpecimenClassification\augmentation.py"", line 26, in None  *
        lambda x, y: (data_augmentation(x, training=True), y)
    File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler  **
        raise e.with_traceback(filtered_tb) from None
    File ""C:\ProgramData\Anaconda3\envs\tfenv\lib\site-packages\keras\layers\preprocessing\image_preprocessing.py"", line 392, in call
        raise ValueError('Image augmentation layers are expecting inputs to be '

    ValueError: Exception encountered when calling layer ""random_flip"" (type RandomFlip).

    Image augmentation layers are expecting inputs to be rank 3 (HWC) or 4D (NHWC) tensors. Got shape: (None, None, 224, 224, 3)

    Call arguments received by layer ""random_flip"" (type RandomFlip):
      • inputs=tf.Tensor(shape=(None, None, 224, 224, 3), dtype=float32)
      • training=True
```
</details>"
57475,NNAPI delegate issue with DepthwiseConv2D nodes,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

Android

### Mobile device

tested on Snapdragon 888, 865, 855

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Our experiments have revealed that DepthwiseConv2D nodes generate a multitude of problems when using NNAPI delegate. Here is a summary of our findings:

1. Inferring DepthwiseConv2D nodes with XNNPACK delegate is always faster than the NNAPI delegate (tested on Snapdragon 855 and Snapdragon 865 - Android 12).
2. On devices with Snapdragon 888 (tested with Android 12), INT8 models starting with a DepthwiseConv2D node always crash with the NNAPI delegate. 
3. After a specific threshold for the node size, the elapsed time inside DepthwiseConv2D nodes increases significantly so that one single node can even take about 20 ms to run with NNAPI delegate. 
4. On devices with Snapdragon 865 (tested with Android 12), quantized DepthwiseConv2D nodes with large kernel size (e.g. 7x7) cause accuracy loss with NNAPI delegate.
5. On devices with Snapdragon 855 (tested with Android 12), quantized DepthwiseConv2D nodes with stride size more than one always result in accuracy loss.
```


### Standalone code to reproduce the issue

```shell
We have implemented a small tool (with comprehensive documentation) to reproduce the mentioned issue. Here is the link to the repository:
https://github.com/Bahar-BM/nnapi_DWC
```


### Relevant log output

_No response_</details>"
57470,[tf.data.dataset] .take after .shuffle doesn't work as expected," ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

OSX 12.5.1 and Linux Ubuntu

### Python version

3.9.12

### Current Behaviour?

Performing .take(1).repeat() after .shuffle(N) returns N different elements.

Reproduction instructions:

```
dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4])
dataset = dataset.shuffle(4)
dataset = dataset.take(1)
dataset = dataset.repeat()
dataset = dataset.take(10)
for x in dataset:
  print(x)
```
This will print 10 different, random integers in the range 1 - 4.

Expected behavior:

I would have expected this to print only a single unique integer. It appears the early .shuffle interferes with the later .take, which is very counterintuitive.

See this colab:

[Link to colab](https://colab.research.google.com/drive/1UYu3TyzMd2nPs19YyNrViFQZxfSyw8WR?usp=sharing)


### Standalone code to reproduce the issue

[Link to colab](https://colab.research.google.com/drive/1UYu3TyzMd2nPs19YyNrViFQZxfSyw8WR?usp=sharing)
"
57467,"incorrect convert PRELU from pb to tflite when input H, W shape set to None in pb","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 20.04.4 LTS`
- TensorFlow installation (pip package or built from source): `pip install tensorflow==2.9.1`
- TensorFlow library (version, if pip package or github SHA, if built from source): `tensorflow==2.9.1`

### 2. Code

PRELU definition: 
```
pos = tf.nn.relu(inputs)
alphas = alphas_constraint(weights=alphas) if alphas_constraint is not None else alphas
neg = -alphas * tf.nn.relu(-inputs)
return pos + neg
```

### 3. Failure after conversion

1. There is no problem when converting the model to PB with set None in H, W such as [1, None, None, 3].
2. But when converting such pb to tflite there will be issue as follow:

- when use `converter.experimental_new_converter = False` gives error `ValueError: None is only supported in the 1st dimension. Tensor 'input_tensor' has invalid shape '[1, None, None, 3]'.`
- when use `converter.experimental_new_converter = True`, the conversion is successful, but the PRELU didn't fusion with conv properly in tflite. 
![image](https://user-images.githubusercontent.com/19303874/186912937-22a74194-f638-49b4-85e8-046312139e02.png)


**PS:**
1. If I convert the model to pb by setting up a fixed input shape (H, W not `None`). Then convert pb to tflite with `converter.experimental_new_converter = False`, the PRELU fusion with conv properly.
![image](https://user-images.githubusercontent.com/19303874/186913508-8a981292-847e-472e-9e99-5a50d711b772.png)

2. If I convert the model to pb by setting up a fixed input shape, Then convert pb to tflite with `converter.experimental_new_converter = True`, the PRELU fusion with conv not properly. 
![image](https://user-images.githubusercontent.com/19303874/186914192-ce4b8a3b-026e-458d-b959-ebd776256576.png)


### 4. Question
Is it possible to convert **pb** (which set up input shape H, W as None) to **tflite** with the PRELU op correctly fuse with CONV?"
57466,Tensorflow API for Carbon?,
57465,docs are incorrect for tf.raw_ops.Atan,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The docs for tf.raw_ops.Atan https://www.tensorflow.org/api_docs/python/tf/raw_ops/Atan indicate integer types are supported but this is not the case as shown below with colab code.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.raw_ops.Atan(x=tf.constant(1, dtype=tf.int16))
```


### Relevant log output

```shell
NotFoundError: Could not find device for node: {{node Atan}} = Atan[T=DT_INT16]
All kernels registered for op Atan:
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_HALF]
 [Op:Atan]
```
</details>"
57463,Bad training loop performance with many variables when enabling XLA,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

2.6

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
In case of many model variables the XLA optimization is quite slow and spends a lot of CPU.
I took a look at profiler `perf top -p PID` and see that a lot of time is spent in xla::HloInputOutputAliasConfig::GetAliasedOutput and looks like we do linear search over operation inputs/outputs here https://github.com/tensorflow/tensorflow/blob/fd23e690821ad733eb855380235eba73b38edfb1/tensorflow/compiler/xla/service/hlo_input_output_alias_config.cc#L140-L146
that is executed in loop over inputs here https://github.com/tensorflow/tensorflow/blob/dd79e90e1b8b6e82a7277495c8d6ef93ea059437/tensorflow/compiler/jit/xla_launch_util.cc#L281
that results in quadratic complexity on I/O tensor count.

Perf top output:
`
  59.05%  _pywrap_tensorflow_internal.so            [.] xla::HloInputOutputAliasConfig::GetAliasedOutput                                                                                                           
  11.60%  _pywrap_tensorflow_internal.so            [.] tensorflow::XlaComputationLaunchContext::PopulateInputs                                                                                                    
   1.11%  libc-2.27.so                              [.] malloc                                                                                                                                                     
   0.99%  libtensorflow_framework.so.2.6.2          [.] std::_Hashtable<std::pair<unsigned long, absl::debian1::string_view>, std::pair<std::pair<unsigned long, absl::debian1::string_view> const, tensorflow::Res
   0.98%  libc-2.27.so                              [.] cfree                                                                                                                                                      
   0.96%  _pywrap_tensorflow_internal.so            [.] tensorflow::GetVariableInfosFromInputs                                                                                                                     
   0.85%  libtensorflow_framework.so.2.6.2          [.] tensorflow::Tensor::RefCountIsOne                                                                                                                          
   0.70%  libc-2.27.so                              [.] 0x000000000018ea23                                                                                                                                         
   0.65%  _pywrap_tensorflow_internal.so            [.] tensorflow::TensorHandle::Type                                                                                                                             
   0.65%  _pywrap_tensorflow_internal.so            [.] std::vector<tensorflow::DtypeAndPartialTensorShape, std::allocator<tensorflow::DtypeAndPartialTensorShape> >::operator=                                    
   0.61%  libc-2.27.so                              [.] 0x0000000000094da5                                                                                                                                         
   0.57%  _pywrap_tensorflow_internal.so            [.] tensorflow::XlaComputationLaunchContext::PopulateOutputs                                                                                                   
   0.55%  libc-2.27.so                              [.] 0x000000000018acf0                                                                                                                                         
   0.52%  _pywrap_tensorflow_internal.so            [.] 0x0000000004ade8df                                                                                                                                         
   0.48%  libc-2.27.so                              [.] 0x0000000000090aae                                                                                                                                         
   0.47%  _pywrap_tensorflow_internal.so            [.] xla::gpu::BufferAllocations::TearDown                                                                                                                      
   0.45%  _pywrap_tensorflow_internal.so            [.] xla::Shape::Shape                                                                                                                                          
   0.40%  libstdc++.so.6.0.25                       [.] __dynamic_cast                                                                                                                                             
   0.40%  libc-2.27.so                              [.] 0x0000000000094711                                                                                                                                         
   0.40%  _pywrap_tensorflow_internal.so            [.] 0x0000000004ade92e                                                                                                                                         
   0.38%  libstdc++.so.6.0.25                       [.] std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_assign                                                                 
   0.37%  libc-2.27.so                              [.] 0x000000000018ea7e                                                                                                                                         
   0.35%  _pywrap_tensorflow_internal.so            [.] 0x0000000004b00431                                                                                                                                         
   0.34%  _pywrap_tensorflow_internal.so            [.] xla::MaybeOwningDeviceMemory::HasOwnership                                                                                                                 
   0.32%  libc-2.27.so                              [.] 0x0000000000095280                                                                                                                                         
   0.32%  libc-2.27.so                              [.] 0x000000000009474d                                                                                                                                         
   0.30%  _pywrap_tensorflow_internal.so            [.] xla::ShapeTree<xla::MaybeOwningDeviceMemory>::Lookup                                                                                                       
   0.28%  libc-2.27.so                              [.] 0x00000000000950bb                                                                                                                                         

`
```


### Standalone code to reproduce the issue
https://colab.research.google.com/drive/1ZDhmatXbYjcu4F7JdasEdS4C-Yklx0St?usp=sharing
```python
import tensorflow as tf
import numpy as np
vs = []
for i in range(3000):
  vs.append(tf.Variable(shape=(10, 10), trainable=True, initial_value=np.zeros((10, 10)), dtype=tf.float32))

def create_loss(x):
  result = tf.stack([tf.reduce_sum(v) for v in vs])
  return (tf.reduce_sum(result) - x) ** 2


opt = tf.keras.optimizers.Adam()

@tf.function(jit_compile=True)
def loop2():
  x = tf.random.uniform(shape=(), dtype=tf.float32)
  with tf.GradientTape() as tape:
    loss = create_loss(x)
  opt.minimize(loss, vs, tape=tape)

while True:
    %time loop2()


```


### Relevant log output

```shell
CPU times: user 9min 56s, sys: 554 ms, total: 9min 57s                                                                                                                                                             
Wall time: 10min 17s                                                                                                                                                                                               
CPU times: user 320 ms, sys: 0 ns, total: 320 ms
Wall time: 320 ms
CPU times: user 315 ms, sys: 0 ns, total: 315 ms
Wall time: 315 ms
CPU times: user 316 ms, sys: 0 ns, total: 316 ms
Wall time: 316 ms
CPU times: user 315 ms, sys: 1e+03 ns, total: 315 ms
Wall time: 315 ms
CPU times: user 313 ms, sys: 996 µs, total: 314 ms
Wall time: 314 ms
```
</details>"
57461,Linker error on Windows with shared build updating to 2.9.1,"👋 I am [Conan](https://github.com/conan-io) contributor and we've received a PR https://github.com/conan-io/conan-center-index/pull/11477 and there was new patch I'd like to contribute back.

The Windows shared build failed using the CMake build system. There's intentionally duplicate symbols with weak attribute, however I suspect this is not support on windows, so in the DLL both are added.

https://github.com/tensorflow/tensorflow/blob/4bfe5240068d57cb586f2c282c1ce19198e3483d/tensorflow/lite/simple_memory_arena.cc#L190-L191

https://github.com/tensorflow/tensorflow/blob/4bfe5240068d57cb586f2c282c1ce19198e3483d/tensorflow/lite/simple_memory_arena_debug_dump.cc#L132-L133

Our community opted to remove the extra file https://github.com/conan-io/conan-center-index/blob/master/recipes/tensorflow-lite/all/patches/remove_simple_memory_arena_debug_dump.patch

I would love to open a PR if this is acceptable fix ❤️ "
57455,Distributing dataset causes warning,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

MacOS 12.5

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
This was reported and closed in bug https://github.com/tensorflow/tensorflow/issues/42146

However, I don't think it was addressed fully. The recommended fix there is to set the option to `DATA`, but that would not remove the warning when the dataset is wrapped with `strategy.experimental_distribute_dataset` or (`strategy. distribute_datasets_from_function`). As seen in the example codes and outputs, the warning message still appears. Please note that the type of the dataset is changed from `BatchDataset` to `DistributedDataset`; I think `DistributedDataset` is a must for distributed strategy.

also this warning message is excessively long as it dumps the entire protobuf structure.
```


### Standalone code to reproduce the issue

```shell
dummy_ds = tf.data.Dataset.from_tensor_slices((tf.ones((10, 4), tf.float32), tf.zeros((10,), tf.float32)))
options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
dummy_ds.with_options(options)
dummy_ds = dummy_ds.batch(4)
print(type(dummy_ds), next(iter(dummy_ds)))
print('------ wrap with experimental_distribute_dataset ------')
dummy_ds = strategy.experimental_distribute_dataset(dummy_ds)
print(type(dummy_ds), next(iter(dummy_ds)))
```


### Relevant log output

```shell
<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'> (<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 1., 1., 1.],
       [1., 1., 1., 1.],
       [1., 1., 1., 1.],
       [1., 1., 1., 1.]], dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)
------ wrap with experimental_distribute_dataset ------
<class 'tensorflow.python.distribute.input_lib.DistributedDataset'> (<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 1., 1., 1.],
       [1., 1., 1., 1.],
       [1., 1., 1., 1.],
       [1., 1., 1., 1.]], dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)

2022-08-25 10:18:19.868107: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: ""TensorSliceDataset/_2""
...
```
</details>"
57454,error in `tf.train.import_meta_graph()`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

tf 2.3.0

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to convert ckpt to pb format of this model : https://github.com/isohrab/Pupil-locator/tree/master/models/3A4Bh-Ref25/best_loss with below code, but it reports error : 

Traceback (most recent call last):
  File ""d:/AI/PythonScripts/TF_ckpt_to_pb.py"", line 21, in <module>
    saver = tf.train.import_meta_graph(input_checkpoint + '.meta')
  File ""C:\Users\weida\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\training\saver.py"", line 1460, in import_meta_graph
    return _import_meta_graph_with_return_elements(meta_graph_or_file,
  File ""C:\Users\weida\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\training\saver.py"", line 1481, in _import_meta_graph_with_return_elements   
    meta_graph.import_scoped_meta_graph_with_return_elements(
  File ""C:\Users\weida\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\framework\meta_graph.py"", line 794, in import_scoped_meta_graph_with_return_elements
    imported_return_elements = importer.import_graph_def(
  File ""C:\Users\weida\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\weida\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\framework\importer.py"", line 400, in import_graph_def
    return _import_graph_def_internal(
  File ""C:\Users\weida\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\framework\importer.py"", line 501, in _import_graph_def_internal
    raise ValueError(str(e))
ValueError: Node 'gradients/InceptionV4/Block_B3/branch_0/batch_normalization/cond/FusedBatchNorm_1_grad/FusedBatchNormGrad' has an _output_shapes attribute inconsistent with the GraphDef for output #3: Dimension 0 in both shapes must be equal, but are 0 and 192. Shapes are [0] and [192].
```

this error occurs at step of `tf.train.import_meta_graph()`.
```


### Standalone code to reproduce the issue

```shell
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import os.path

MODEL_DIR = r""D:\AI\repos\Pupil-locator\models\3A4Bh-Ref25\best_loss""
MODEL_NAME = ""Pupil-locator.pb""
output_node_names = ['InceptionV4/y']

checkpoint = tf.train.get_checkpoint_state(MODEL_DIR) 
input_checkpoint = checkpoint.model_checkpoint_path
print(""==>> input_checkpoint: "", input_checkpoint)
output_graph = os.path.join(MODEL_DIR, MODEL_NAME)

with tf.Session() as sess:
    # Restore the graph
    saver = tf.train.import_meta_graph(input_checkpoint + '.meta')

    # Load weights
    # ckpt = tf.train.get_checkpoint_state(MODEL_DIR)
    # if ckpt:
    #     saver.restore(sess, ckpt.model_checkpoint_path)
        # saver.restore(sess, tf.train.latest_checkpoint('.'))
    # var_SRGAN_g = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)

    # # Freeze the graph
    # frozen_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, output_node_names)

    # # tf.train.write_graph(frozen_graph_def,MODEL_DIR,MODEL_NAME,as_text=False)

    # # Save the frozen graph
    # with open(output_graph, 'wb') as f:
    #     f.write(frozen_graph_def.SerializeToString())
    #     print(""%d ops in the final graph."" % len(frozen_graph_def.node))
```
```


### Relevant log output

_No response_</details>"
57450,`tf.keras.layers.Convolution2DTranspose` throws error when backprop,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I try to do back prop on Conv2dTranspose layer, strange `TypeError` is thrown. I think this bug is related to tensorflow because the error message contains tensorflow backprop ops.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

filters = 2
kernel_size = [3, 3]
strides = [1, 1]
padding = ""same""
output_padding = None
data_format = ""channels_last""
dilation_rate = [2, 2]
activation = ""linear""
use_bias = True
x = tf.random.uniform([1, 5, 6, 1], minval=0, maxval=1, dtype=tf.float32)
layer = tf.keras.layers.Convolution2DTranspose(filters, kernel_size, strides=strides, padding=padding, 
                                                                      output_padding=output_padding, data_format=data_format, dilation_rate=dilation_rate, 
                                                                      activation=activation, use_bias=use_bias)


with tf.GradientTape() as g:
    g.watch(x)
    res = layer(x)
print(res.shape) # (1, 5, 6, 2)
grad = g.jacobian(res, x) # Error
```


### Relevant log output

```shell
ERROR:tensorflow:Got error while pfor was converting op name: ""gradient_tape/conv2d_transpose_11/SpaceToBatchND""
op: ""SpaceToBatchND""
input: ""gradient_tape/Reshape""
input: ""gradient_tape/conv2d_transpose_11/SpaceToBatchND/block_shape""
input: ""gradient_tape/conv2d_transpose_11/SpaceToBatchND/paddings""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""Tblock_shape""
  value {
    type: DT_INT64
  }
}
attr {
  key: ""Tpaddings""
  value {
    type: DT_INT32
  }
}
 with inputs (<tf.Tensor 'gradient_tape/Reshape:0' shape=(1, 5, 6, 2) dtype=float32>, <tf.Tensor 'gradient_tape/conv2d_transpose_11/SpaceToBatchND/block_shape:0' shape=(2,) dtype=int64>, <tf.Tensor 'gradient_tape/conv2d_transpose_11/SpaceToBatchND/paddings:0' shape=(2, 2) dtype=int32>)
, converted inputs [WrappedTensor(t=<tf.Tensor 'gradient_tape/Reshape/pfor/Reshape:0' shape=(60, 1, 5, 6, 2) dtype=float32>, is_stacked=True, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'gradient_tape/conv2d_transpose_11/SpaceToBatchND/block_shape:0' shape=(2,) dtype=int64>, is_stacked=False, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'gradient_tape/conv2d_transpose_11/SpaceToBatchND/paddings:0' shape=(2, 2) dtype=int32>, is_stacked=False, is_sparse_stacked=False)]
Here are the pfor conversion stack traces: Input 'y' of 'Mul' Op has type int32 that does not match type int64 of argument 'x'.
ERROR:tensorflow:name: ""gradient_tape/conv2d_transpose_11/SpaceToBatchND""
op: ""SpaceToBatchND""
input: ""gradient_tape/Reshape""
input: ""gradient_tape/conv2d_transpose_11/SpaceToBatchND/block_shape""
input: ""gradient_tape/conv2d_transpose_11/SpaceToBatchND/paddings""
attr {
  key: ""T""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""Tblock_shape""
  value {
    type: DT_INT64
  }
}
attr {
  key: ""Tpaddings""
  value {
    type: DT_INT32
  }
}

created at:
    File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
    File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
    File ""/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py"", line 846, in launch_instance
    app.start()
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py"", line 612, in start
    self.io_loop.start()
    File ""/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py"", line 132, in start
    self.asyncio_loop.run_forever()
    File ""/usr/lib/python3.7/asyncio/base_events.py"", line 541, in run_forever
    self._run_once()
    File ""/usr/lib/python3.7/asyncio/base_events.py"", line 1786, in _run_once
    handle._run()
    File ""/usr/lib/python3.7/asyncio/events.py"", line 88, in _run
    self._context.run(self._callback, *self._args)
    File ""/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py"", line 758, in _run_callback
    ret = callback()
    File ""/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py"", line 300, in null_wrapper
    return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/tornado/gen.py"", line 1233, in inner
    self.run()
    File ""/usr/local/lib/python3.7/dist-packages/tornado/gen.py"", line 1147, in run
    yielded = self.gen.send(value)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py"", line 365, in process_one
    yield gen.maybe_future(dispatch(*args))
    File ""/usr/local/lib/python3.7/dist-packages/tornado/gen.py"", line 326, in wrapper
    yielded = next(result)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py"", line 268, in dispatch_shell
    yield gen.maybe_future(handler(stream, idents, msg))
    File ""/usr/local/lib/python3.7/dist-packages/tornado/gen.py"", line 326, in wrapper
    yielded = next(result)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py"", line 545, in execute_request
    user_expressions, allow_stdin,
    File ""/usr/local/lib/python3.7/dist-packages/tornado/gen.py"", line 326, in wrapper
    yielded = next(result)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py"", line 306, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py"", line 536, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py"", line 2855, in run_cell
    raw_cell, store_history, silent, shell_futures)
    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py"", line 2881, in _run_cell
    return runner(coro)
    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner
    coro.send(None)
    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py"", line 3058, in run_cell_async
    interactivity=interactivity, compiler=compiler, result=result)
    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py"", line 3249, in run_ast_nodes
    if (await self.run_code(code, result,  async_=asy)):
    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-12-d34c7de5c6ed>"", line 22, in <module>
    grad = g.jacobian(res, x) # Error
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py"", line 1183, in jacobian
    parallel_iterations=parallel_iterations)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py"", line 202, in pfor
    outputs = f()
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py"", line 150, in error_handler
    return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 915, in __call__
    result = self._call(*args, **kwds)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 963, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 786, in _initialize
    *args, **kwds))
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 2983, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 3292, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 3140, in _create_graph_function
    capture_by_value=self._capture_by_value),
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 1161, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 677, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 1143, in autograph_handler
    user_requested=True,
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py"", line 185, in f
    iters,
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py"", line 281, in _pfor_impl
    loop_fn_outputs = loop_fn(loop_var)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py"", line 1173, in loop_fn
    unconnected_gradients=unconnected_gradients)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py"", line 1087, in gradient
    unconnected_gradients=unconnected_gradients)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py"", line 73, in imperative_grad
    compat.as_str(unconnected_gradients.value))
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py"", line 156, in _gradient_function
    return grad_fn(mock_op, *out_grads)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_grad.py"", line 951, in _BatchToSpaceNDGrad
    array_ops.space_to_batch_nd(grad, op.inputs[1], op.inputs[2]), None, None
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 9995, in space_to_batch_nd
    paddings=paddings, name=name)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 742, in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 695, in _create_op_internal
    compute_device)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 3784, in _create_op_internal
    op_def=op_def)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 2175, in __init__
    self._traceback = tf_stack.extract_stack_for_node(self._c_op)

(1, 5, 6, 2)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-12-d34c7de5c6ed> in <module>
     20     res = layer(x)
     21 print(res.shape) # (1, 5, 6, 2)
---> 22 grad = g.jacobian(res, x) # Error

3 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)
   1145           except Exception as e:  # pylint:disable=broad-except
   1146             if hasattr(e, ""ag_error_metadata""):
-> 1147               raise e.ag_error_metadata.to_exception(e)
   1148             else:
   1149               raise

TypeError: in user code:

    File ""/usr/local/lib/python3.7/dist-packages/six.py"", line 703, in reraise
        raise value

    TypeError: Input 'y' of 'Mul' Op has type int32 that does not match type int64 of argument 'x'.
```
</details>"
57449,Error while using keyword arguments in tensorflow methods ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.6.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I dont know exactly what is going on, but I was trying to write a pipline by using tf.data.Dataset.from_tensor_slices. When I try to cast the image to tf.float32, it gives me the following error:

img = tf.cast(img, dtype=tf.float32)

    TypeError: 'dict' object is not callable

when I change the following line to:

img = tf.cast(img, tf.float32) 

everything works fine! The same thing happens when I tried to write a custom loss function. It seems passing keyword argument with the keyword causes the error.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf 

images = ['./temp/image1.jpg', './temp/image2.jpg']
batch_size = 64

def preprocess(img_name):
    img = tf.io.read_file(images[0])
    img = tf.io.decode_jpeg(img)
    img = tf.cast(img, dtype=tf.float32)
    img = tf.divide(img, 255.0)
    img = tf.image.resize(img, (256, 256))
    return img

data = tf.data.Dataset.from_tensor_slices(images[0:1])
data = data.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
data = data.shuffle(batch_size*4)
data = data.batch(batch_size)
data = data.prefetch(tf.data.AUTOTUNE)
```


### Relevant log output

```shell
/tmp/ipykernel_17/453503734.py in <module>
      8 
      9 data = tf.data.Dataset.from_tensor_slices(images[0:1])
---> 10 data = data.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
     11 data = data.shuffle(batch_size*4)
     12 data = data.batch(batch_size)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls, deterministic)
   1866           num_parallel_calls,
   1867           deterministic,
-> 1868           preserve_cardinality=True)
   1869 
   1870   def flat_map(self, map_func):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)
   5022         self._transformation_name(),
   5023         dataset=input_dataset,
-> 5024         use_legacy_function=use_legacy_function)
   5025     if deterministic is None:
   5026       self._deterministic = ""default""

/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)
   4216         fn_factory = trace_tf_function(defun_kwargs)
   4217 
-> 4218     self._function = fn_factory()
   4219     # There is no graph to add in eager mode.
   4220     add_to_graph &= not context.executing_eagerly()

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in get_concrete_function(self, *args, **kwargs)
   3149     """"""
   3150     graph_function = self._get_concrete_function_garbage_collected(
-> 3151         *args, **kwargs)
   3152     graph_function._garbage_collector.release()  # pylint: disable=protected-access
   3153     return graph_function

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)
   3114       args, kwargs = None, None
   3115     with self._lock:
-> 3116       graph_function, _ = self._maybe_define_function(args, kwargs)
   3117       seen_names = set()
   3118       captured = object_identity.ObjectIdentitySet(

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3461 
   3462           self._function_cache.missed.add(call_context_key)
-> 3463           graph_function = self._create_graph_function(args, kwargs)
   3464           self._function_cache.primary[cache_key] = graph_function
   3465 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3306             arg_names=arg_names,
   3307             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3308             capture_by_value=self._capture_by_value),
   3309         self._function_attributes,
   3310         function_spec=self.function_spec,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)
   1005         _, original_func = tf_decorator.unwrap(python_func)
   1006 
-> 1007       func_outputs = python_func(*func_args, **func_kwargs)
   1008 
   1009       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in wrapped_fn(*args)
   4193           attributes=defun_kwargs)
   4194       def wrapped_fn(*args):  # pylint: disable=missing-docstring
-> 4195         ret = wrapper_helper(*args)
   4196         ret = structure.to_tensor_list(self._output_structure, ret)
   4197         return [ops.convert_to_tensor(t) for t in ret]

/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in wrapper_helper(*args)
   4123       if not _should_unpack(nested_args):
   4124         nested_args = (nested_args,)
-> 4125       ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)
   4126       if _should_pack(ret):
   4127         ret = tuple(ret)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    693       except Exception as e:  # pylint:disable=broad-except
    694         if hasattr(e, 'ag_error_metadata'):
--> 695           raise e.ag_error_metadata.to_exception(e)
    696         else:
    697           raise

TypeError: in user code:

    /tmp/ipykernel_17/3009962238.py:9 preprocess  *
        img = tf.cast(img, dtype=tf.float32)

    TypeError: 'dict' object is not callable
```
</details>"
57444,No Mac M1 wheels for tensorflow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

macOS M1 (arm64)

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
$ pip install tensorflow
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```
```


### Standalone code to reproduce the issue

```shell
as above
```


### Relevant log output

_No response_</details>"
57438,tf.keras.layers.Layer.weights is not ordered in graph mode,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.9.0-18-gd8ce9f9c301 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.4

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When loading a 'saved_model' (tf.saved_model.load), the 'weights' attribute found in tf.keras.layers.Layer gives the model weights in a seemingly random order. If you convert the same 'saved_model' to a tflite model (tf.lite.TFLiteConverter.from_saved_model), the weights are instead given in an expected fixed order.

Since the 'saved_model' and the tflite model are generated from the same source, one expects the same outcome when calling the 'same' method
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1AE0dS1vJXV3D_NL29NjkVwavcszximFY?usp=sharing
```


### Relevant log output

_No response_</details>"
57430,Second-order gradient calculated in forward-mode for API `tf.linalg.det` is inaccurate relative to that in reverse-mode,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Second-order gradient calculated in forward-mode AD is different from that in reverse-mode. And the error reached 1e-1 which is not acceptable.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input = tf.constant([[0.41278124, 0.472762  ],[0.5486289,0.8008381 ]],dtype=tf.float32)

with tf.GradientTape(persistent=True) as g2:
    with tf.GradientTape(persistent=True) as g1:
        g1.watch(input)
        g2.watch(input)
        res = tf.linalg.det(input)
    grad = g1.gradient(res, input)
grad2nd = g2.gradient(grad, input)

grad2nd_fwd = []
for i in range(tf.size(input)):
    hessian_col = []
    for j in range(tf.size(input)):
        tangent_inner = tf.reshape(tf.one_hot(i, tf.size(input), dtype=input.dtype), shape=input.shape)
        tangent_outer = tf.reshape(tf.one_hot(j, tf.size(input), dtype=input.dtype), shape=input.shape)
        with tf.autodiff.ForwardAccumulator(input, tangent_outer) as acc_outer:
            with tf.autodiff.ForwardAccumulator(input, tangent_inner) as acc_inner:
                res = tf.reduce_sum(tf.linalg.det(input))
            jvp = acc_inner.jvp(res)
        hvp = acc_outer.jvp(jvp)
        hessian_col.append(hvp)
    grad2nd_fwd.append(tf.reduce_sum(hessian_col))
grad2nd_fwd = tf.reshape(grad2nd_fwd, shape=input.shape)
print(""reverse-mode\n"",grad2nd)
print(""forward-mode\n"",grad2nd_fwd)
```


### Relevant log output

```shell
reverse-mode
 tf.Tensor(
[[ 1.0000001  -1.2048287 ]
 [-0.79517186  1.0000002 ]], shape=(2, 2), dtype=float32)
forward-mode
 tf.Tensor(
[[ 1.        -1.000001 ]
 [-1.0000002  1.       ]], shape=(2, 2), dtype=float32)
```
</details>"
57424,Commits from CVE-2022-23569 - TFSA-2022-015,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I cannot find all the commits related to the security fix for CVE-2022-23569.
The tensorflow/security/advisories mentioned that the issue was fixed in multiple commits (https://github.com/tensorflow/tensorflow/blob/e5ef4b0f6da591ea71d7c54955a3607cd203060b/tensorflow/security/advisory/tfsa-2022-015.md) but since the ""Check-fails"" issue is quite common in tensorflow, it is hard to find the specific commits that fix this security issue. Could someone please help me?
```


### Standalone code to reproduce the issue

```shell
I don't see the related commits for CVE-2022-23569
```


### Relevant log output

_No response_</details>"
57423,Unable to free memory allocated by OpKernelContext,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.9

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The OpKernelContext used in custom op's compute method doesn't have an option to free the temporary allocated by it. 

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/op_kernel.h doesn't have an API to free the memory allocated by ""allocate_temp"" method.

Do we have any solution to free the memory allocated using allocate_temp?
```


### Standalone code to reproduce the issue

```shell
Its a request
```


### Relevant log output

_No response_</details>"
57422,The checkpoint is not readable in android  studio ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

1.15

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

android 12

### Python version

3.6

### Bazel version

---

### GCC/Compiler version

---

### CUDA/cuDNN version

---

### GPU model and memory

cpu 

### Current Behaviour?

```shell
I have a tf checkpoint trained in tensorflow 1.x ,the input type of the tf graph is byte, I have converted it to tflite and run it in interpreter python by this code : 

          input_details = interpreter.get_input_details()
          output_details = interpreter.get_output_details()
          image_filename='img.jpeg'
          input_data  = tf.compat.v1.gfile.FastGFile(image_filename, 'rb').read()
          input_data = np.array([input_data ])
          interpreter.set_tensor(input_details[0]['index'], input_data)
          interpreter.invoke()
          output_data = interpreter.get_tensor(output_details[0]['index'])
          print(output_data)


However when I want to load it in the android studio, it caused the error that the input type is not valid. In fact when I want to convert the image to byte array and feed it to tflite with this code:



 I face an error: 
 
Unknown image file format. One of JPEG, PNG, GIF, BMP required.

https://github.com/tensorflow/tflite-support/issues/870 

I presented the issue hear and due to suggested solution I removed the nodes from the beginning of the graph and added an input node with the float32 type which is a valid one in android studio,  but the results of the pruned network were not valid. what should I do ? 
can I add some input node at the beginning of the graph and decode the image from float32 to gfile in tensorflow graph ?
```


### Standalone code to reproduce the issue

```shell
val catBitmap = getBitmapFromAsset(""bwr.jpg"")
     val width: Int = catBitmap.getWidth()
     val height: Int = catBitmap.getHeight()
     val imageProcessor = ImageProcessor.Builder()
            .add(
                ResizeOp(
                    height,
                    width,
                    ResizeOp.ResizeMethod.BILINEAR
                )
            )
            //.add(NormalizeOp(0.0, 255.0))
            .build()
      var tensorImage = TensorImage(DataType.UINT8)
      tensorImage.load(catBitmap);
      tensorImage = imageProcessor.process(tensorImage);
      val dd1=tensorImage.getBuffer()
      val ssw=dd1.order(ByteOrder.nativeOrder())
      val input1=arrayOf(ssw.toString())
      val probabilityBuffer =
            TensorBuffer.createFixedSize(intArrayOf(1, 5000), DataType.FLOAT32)
       
     tflite.run(input1, probabilityBuffer.getBuffer());
```


### Relevant log output

```shell
Unknown image file format. One of JPEG, PNG, GIF, BMP required.
```
</details>"
57421,Does tensorflow use custom SSL for azure blob storage access?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

2.4.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


I added some custom code to tensorflow to access azure blob storage using oauth token via tfio and found that it run into 

```Problem with the SSL CA cert (path? access rights?)```

Does tensorflow use custom ssl?


### Standalone code to reproduce the issue

```shell
I hack the code here 

https://github.com/tensorflow/io/tree/master/tensorflow_io/core/filesystems/az


        std::string tenantId = """";
        std::string activeDirectoryApplicationId = """";
        std::string activeDirectoryApplicationSecret = """";
        std::string storageUrl = ""https://storage_name.blob.core.windows.net"";

        auto clientCertificateCredential
            = std::make_shared<Azure::Identity::ClientSecretCredential>(tenantId, activeDirectoryApplicationId, activeDirectoryApplicationSecret);

        std::string storageContainerUrl = ""https://storage_name.blob.core.windows.net/container_name"";

        auto blobclient = std::make_shared<Azure::Storage::Blobs::BlobContainerClient>(storageContainerUrl, clientCertificateCredential);

    	for (auto blobPage = (*blobclient).ListBlobs();
    	   blobPage.HasPage();
    	   blobPage.MoveToNextPage())
    	{
    	for (auto& blob : blobPage.Blobs)
    	{
    	  TF_VLOG(3, ""Blob %s\n"", blob.Name);
    	}}
    
```


### Relevant log output

_No response_</details>"
57420,multiple definition error [MinimalLogger::LogFormatted],"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

v2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.6.9

### Bazel version

_No response_

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
# Original CMakeLists.txt
if(NOT ""${CMAKE_SYSTEM_NAME}"" STREQUAL ""Android"")
  list(FILTER TFLITE_SRCS EXCLUDE REGEX "".*minimal_logging_android\\.cc$"")
endif()
if(NOT ""${CMAKE_SYSTEM_NAME}"" STREQUAL ""iOS"")
  list(FILTER TFLITE_SRCS EXCLUDE REGEX "".*minimal_logging_ios\\.cc$"")
endif()

But, I think cmake source needs to be the followings. cause of minimal_logging_default\\.cc file/ :

# updated CMakeLists.txt
if(NOT ""${CMAKE_SYSTEM_NAME}"" STREQUAL ""Android"")
  list(FILTER TFLITE_SRCS EXCLUDE REGEX "".*minimal_logging_android\\.cc$"")
  list(FILTER TFLITE_SRCS EXCLUDE REGEX "".*minimal_logging_default\\.cc$"")
endif()
if(NOT ""${CMAKE_SYSTEM_NAME}"" STREQUAL ""iOS"")
  list(FILTER TFLITE_SRCS EXCLUDE REGEX "".*minimal_logging_ios\\.cc$"")
  list(FILTER TFLITE_SRCS EXCLUDE REGEX "".*minimal_logging_default\\.cc$"")
endif()

if((NOT ""${CMAKE_SYSTEM_NAME}"" STREQUAL ""Android"") AND (NOT ""${CMAKE_SYSTEM_NAME}"" STREQUAL ""iOS""))
  list(FILTER TFLITE_SRCS EXCLUDE REGEX "".*minimal_logging_android\\.cc$"")
  list(FILTER TFLITE_SRCS EXCLUDE REGEX "".*minimal_logging_ios\\.cc$"")
endif()
```


### Standalone code to reproduce the issue

```shell
# cmake -DCMAKE_TOOLCHAIN_FILE=~/Android/Sdk/ndk/19.2.5345600/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON -DBUILD_SHARED_LIBS:BOOL=ON  -DCMAKE_BUILD_TYPE=Debug  ../tensorflow/tensorflow/lite
```


### Relevant log output

```shell
CMakeFiles/tensorflow-lite.dir/minimal_logging_default.cc.o: In function `tflite::logging_internal::MinimalLogger::LogFormatted(tflite::LogSeverity, char const*, std::__va_list)':
/home/fory2k/work/android/cmake_build/v2.9.1/tensorflow/tensorflow/lite/minimal_logging_default.cc:26: multiple definition of `tflite::logging_internal::MinimalLogger::LogFormatted(tflite::LogSeverity, char const*, std::__va_list)'
CMakeFiles/tensorflow-lite.dir/minimal_logging_android.cc.o:/home/fory2k/work/android/cmake_build/v2.9.1/tensorflow/tensorflow/lite/minimal_logging_android.cc:42: first defined here
```
</details>"
57411,LSTM layer not support bf16 ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

 Ubuntu 22.04.1 LTS

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I trid do mix_precision with keras policy, get the error ""Value passed to parameter 'input' has DataType bfloat16 not in list of allowed values: float16, float32, float64""

Why not support LSTM layer BF16 ? Is it a accuracy loss concern or something else?
Will it be possible to support LSTM with BF16?
If not, can we add it to something like DenyList to run LSTM with FP32 automatically rather than throw this error?
```


### Standalone code to reproduce the issue

```shell
from tensorflow.keras.models import Sequential
from tensorflow.keras import mixed_precision
from tensorflow.keras.layers import LSTM, Embedding

policy = mixed_precision.Policy('mixed_bfloat16')
mixed_precision.set_global_policy(policy)
vocab_size = 1000

# define model
def customer_model():
    model = Sequential()
    model.add(Embedding(vocab_size, 512, input_length=50))
    model.add(LSTM(128, return_sequences=True))
    return model
def test_model():
    model = Sequential()
    model.add(Embedding(vocab_size, 128, input_length=50))
    model.add(LSTM(4, dtype='float32',  return_sequences=True))    
    return model

model = customer_model()
model.summary()
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""reproduce_error.py"", line 23, in <module>
    model = customer_model()
  File ""reproduce_error.py"", line 13, in customer_model
    model.add(LSTM(128, return_sequences=True))
  File ""/home/sdp/miniconda3/envs/tensorflow_2.9.1/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py"", line 587, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/sdp/miniconda3/envs/tensorflow_2.9.1/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/sdp/miniconda3/envs/tensorflow_2.9.1/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"", line 58, in _SatisfiesTypeConstraint
    raise TypeError(
TypeError: Exception encountered when calling layer ""lstm"" (type LSTM).Value passed to parameter 'input' has DataType bfloat16 not in list of allowed values: float16, float32, float64Call arguments received by layer ""lstm"" (type LSTM):
  • inputs=tf.Tensor(shape=(None, 50, 512), dtype=bfloat16)
  • mask=None
  • training=None
  • initial_state=None
```
</details>"
57401,conv2d crashes when compute gradient,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Session crashes when computing gradient for `tf.nn.conv2d`. For the inputs in the example below, `tf.nn.conv2d` can generate a correct (empty) output tensor with shape `(1, 16, 0, 1)`. However, when I compute gradient with GradientTape, the program crashes with error message (core dumped).
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
filters = tf.random.uniform([3, 3, 1, 1], dtype=tf.float32)
strides = 2
padding = ""VALID""

input = tf.random.uniform([1, 34, 1, 1], dtype=tf.float32)

with tf.GradientTape(persistent=True,) as g:
  g.watch(input)
  res_backward = tf.nn.conv2d(input, filters, strides=strides, padding=padding)
print(res_backward)
grad = g.gradient(res_backward, input) # this will crash
```


### Relevant log output

```shell
tf.Tensor([], shape=(1, 16, 0, 1), dtype=float32)
core dumped
```
</details>"
57397,Tensorflow lite docker build issue,"TLDR:
Trying to build tflite 2.4.1 from source.
Attempting to get output: tflite_runtime-2.4.1-cp37-cp37m-linux_x86_64.whl
Getting output: tflite_runtime-2.4.1-pp37-pypy37_pp73-linux_x86_64.whl

Attempting to use same method as `https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/lite/tools/pip_package/Makefile`

Apologies if it's something dumb, I'm a complete amateur at make/python!

<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.4.1

### Custom Code

No

### OS Platform and Distribution

Docker

### Mobile device

_No response_

### Python version

3.7.4

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hey guys, 

I'm having an issue building TFL 2.4.1 from source, it seems to build, but I can't seem to import the wheel file into another environment. 

It seems that it should be outputting `tflite_runtime-2.4.1-cp37-cp37m-linux_x86_64.whl` but instead it's outputting `tflite_runtime-2.4.1-pp37-pypy37_pp73-linux_x86_64.whl`.

Not quite sure where I'm going wrong!
```


### Standalone code to reproduce the issue

```shell
# syntax = docker/dockerfile-upstream:master-labs

ARG CONDAENV='tensorflow'
ARG PYTHONENV='3.7.4'
ARG PYTHONDIR='python3'
ARG TENSORFLOWVER='2.4.1'
ARG TENSORFLOWDIR='tensorflow'
ARG GITDIR='/tensorflow/tensorflow/lite/tools'

###############
# tflite-builder

FROM condaforge/mambaforge-pypy3:4.13.0-1 as tflite-builder

# System Env
ENV PIP_ROOT_USER_ACTION='ignore'
# ENV PYTHONDONTWRITEBYTECODE='1'
# ENV PYTHONUNBUFFERED='1'
ENV DEBIAN_FRONTEND=noninteractive

# User Env
ENV CONDARC='/opt/conda/.condarc'
#ENV TENSORFLOW_TARGET='native'
ENV PYTHONVER='3.7'
ENV TZ='Europe/London'
#ENV PYTHON='python3.7'
ARG TENSORFLOWVER
ARG CONDAENV
ARG TENSORFLOWDIR
ARG GITDIR
ARG PYTHONENV

ADD --link --keep-git-dir=false https://github.com/tensorflow/tensorflow.git#v$TENSORFLOWVER /tensorflow
# COPY --link $GITDIR/make/downloads/ $GITDIR/make/downloads/

RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    <<EOF
    set -x
    rm -fv /etc/apt/apt.conf.d/docker-clean
    echo 'Binary::apt::APT::Keep-Downloaded-Packages ""true"";' > /etc/apt/apt.conf.d/keep-cache
    DEBIAN_FRONTEND=noninteractive
    apt-get update
    apt-get install -y curl \
                       debhelper \
                       dh-python \
                       git \
                       libpython3-dev \
                       pybind11-dev \
                       python3-all \
                       python3-numpy \
                       python3-pip \
                       python3-setuptools \
                       python3-wheel \
                       unzip \
                       zlib1g-dev
EOF

RUN --mount=type=cache,id=$CONDAENV,target=/opt/conda/pkgs,sharing=locked \
    --mount=type=cache,id=pip,target=/root/.cache/pip \
    <<EOF
    set -x
    cat > $CONDARC << CON
channels:
  - anaconda
  - defaults
CON
    mamba create --name $CONDAENV -y python=$PYTHONENV
    mamba init bash
    . ~/.bashrc
    . activate $CONDAENV
    python3 -m pip install -U pybind11 numpy pillow
    TENSORFLOW_TARGET=native
    PYTHON=python3.7
    BUILD_DEB=n
    VERSION_SUFFIX=
    $GITDIR/pip_package/build_pip_package.sh
EOF

###############
# envbuild

FROM condaforge/mambaforge-pypy3:4.13.0-1 AS envbuild

# System Env
ENV PIP_ROOT_USER_ACTION='ignore'
# ENV PYTHONDONTWRITEBYTECODE='1'
# ENV PYTHONUNBUFFERED='1'

# User Env
ARG CONDAENV
ARG PYTHONENV
ARG PYTHONDIR
ARG TENSORFLOWDIR
ENV ENVFILE='environment_aggregate.yml'
ARG GITDIR

# Install the package as normal and any packages in /wheel dir:
COPY --link $ENVFILE /
COPY --link wheel/*.whl /
COPY --link --from=tflite-builder $GITDIR/pip_package/gen/tflite_pip/$PYTHONDIR/dist/tflite_runtime-*.whl /

RUN --mount=type=cache,id=$CONDAENV,target=/opt/conda/pkgs,sharing=locked \
    --mount=type=cache,id=pip,target=/root/.cache/pip \
    <<EOF
    set -x
    mamba env create -v -f $ENVFILE 
    mamba run -n $CONDAENV python3 -m pip install *.whl
    # Use conda-pack to create a standalone environment in /venv
    mamba install -y -c anaconda conda-pack
    conda-pack -j -1 -n $CONDAENV -o /tmp/env.tar
    mkdir /venv && cd /venv
    tar xf /tmp/env.tar
    rm /tmp/env.tar
    # We've put venv in same path it'll be in final image, so now fix up paths
    /venv/bin/conda-unpack
EOF

###############

# The runtime-stage image; we can use Debian as the
# base image since the Conda env also includes Python
# for us.
FROM debian:buster-slim AS runtime

# Copy entrypoint script
COPY --link --chmod=755 entrypoint.sh /entrypoint.sh

# Copy /venv from the previous stage:
COPY --link --from=envbuild /venv /venv

# When image is run, run the code with the environment activated:
SHELL [""/bin/bash"", ""-c""]

#ENTRYPOINT [""/venv/bin/tini"", ""-g"", ""--""]
CMD [""/entrypoint.sh""]
```
```


### Relevant log output

```shell
#18 142.2 + mamba run -n tensorflow python3 -m pip install crate-0.27.1-py2.py3-none-any.whl tflite_runtime-2.4.1-pp37-pypy37_pp73-linux_x86_64.whl
#18 146.9 ERROR: tflite_runtime-2.4.1-pp37-pypy37_pp73-linux_x86_64.whl is not a supported wheel on this platform.
```
</details>"
57396,Problem in ML Kit task library.,"I have trained a custom object detection model using this [Colab](https://colab.research.google.com/github/khanhlvg/tflite_raspberry_pi/blob/main/object_detection/Train_custom_model_tutorial.ipynb) and exported the model without any problem and tested the model in the same colab but when I use it in my android app it doesn't work.
I am using ml kit task library in android studio for object detection.
I think the problem is in the below given code snippet.
```
CustomObjectDetectorOptions options = new CustomObjectDetectorOptions.Builder(localModel)
                .setDetectorMode(ObjectDetectorOptions.STREAM_MODE)
                .setClassificationConfidenceThreshold(0.1f)
                .setMaxPerObjectLabelCount(5)
                .enableClassification()
                .build();
```
when I use enableClassification() it shows no objects detected but when I don't use it, it detects objects.
When I don't use enableClassificatin() it goes in the onSuccessListener() shown below otherwise it goes in onFailureListener() shown 
in below code snippet.
```
objectDetector.process(image)
                        .addOnSuccessListener(
                                new OnSuccessListener<List<DetectedObject>>() {
                                    @Override
                                    public void onSuccess(List<DetectedObject> result) {
                                        Log.d(""TAG"", ""onSuccess: Object detected"");
                                        for(DetectedObject detectedObject: result){
//                                            Toast.makeText(realTime.this, """", Toast.LENGTH_SHORT).show();
                                            Rect box = detectedObject.getBoundingBox();
                                            StringBuilder builder = new StringBuilder();
                                            for(DetectedObject.Label label : detectedObject.getLabels()){
                                                String text = label.getText();
                                                builder.append(text);
                                                builder.append("" : "");
                                                int index = label.getIndex();
                                                float confidence = Math.round(label.getConfidence()*100);
                                                builder.append(confidence+""%"");
                                                builder.append(""\n"");
                                            }
                                            tv.setText(builder);
                                        }
                                        try {
                                            Toast.makeText(realTime.this, ""Detected objects:""+result.size(), Toast.LENGTH_SHORT).show();
                                        }
                                        catch (Exception e){
                                            Toast.makeText(realTime.this, ""Object detected but label problem"", Toast.LENGTH_SHORT).show();
                                        }
                                    }

                                }
                        )
                        .addOnFailureListener(
                                new OnFailureListener() {
                                    @Override
                                    public void onFailure(@NonNull Exception e) {
                                        Toast.makeText(realTime.this, ""No object detected"", Toast.LENGTH_SHORT).show();
                                        Log.d(""realTime"", ""No object detected"");
                                    }
                                }
                        )
                        .addOnCompleteListener(
                                new OnCompleteListener<List<DetectedObject>>() {
                                    @Override
                                    public void onComplete(@NonNull Task<List<DetectedObject>> task) {
                                        imageProxy.close();
                                    }
                        });
```

"
57395,when will tensorflow support FP8?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

tf2.9

### Custom Code

No

### OS Platform and Distribution

Linux ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Pytorch added FP8 support, will TF add FP8 support sometime for both E4M3 and E5M2?
```


### Standalone code to reproduce the issue

```shell
No source code, just a feature request question
```


### Relevant log output

_No response_</details>"
57392,Model improves worse with GradientTape than with fit(),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.04.1

### Mobile device

_No response_

### Python version

3.8 & 3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I already opened a question on stackoverflow for this, but I believe it could be some issue with tensorflow.

https://stackoverflow.com/questions/73440964/model-not-improving-with-gradienttape-but-with-model-fit

The same model using either custom training with GradientTape or keras' model.fit() will perform differently. The custom run, will stop improving earlier and will take longer to improve at all.

I've also included a Google Colab notebook in order to reproduce the issue:
https://colab.research.google.com/drive/1pk66rbiux5vHZcav9VNSBhdWWIhQM-nF?usp=sharing
```


### Standalone code to reproduce the issue

```shell
def build_model(kernel_regularizer=l2(0.0001), dropout=0.001, recurrent_dropout=0.):
    x1 = Input(62)
    x2 = Input((62, 3))

    x = Embedding(30, 100, mask_zero=True)(x1)
    x = Concatenate()([x, x2])

    x = Bidirectional(LSTM(500,
                           return_sequences=True,
                           kernel_regularizer=kernel_regularizer,
                           dropout=dropout,
                           recurrent_dropout=recurrent_dropout))(x)

    x = Bidirectional(LSTM(500,
                           return_sequences=False,
                           kernel_regularizer=kernel_regularizer,
                           dropout=dropout,
                           recurrent_dropout=recurrent_dropout))(x)

    x = Activation('softmax')(x)

    x = Dense(1000)(x)
    x = Dense(500)(x)
    x = Dense(250)(x)
    x = Dense(1, bias_initializer='ones')(x)

    x = tf.math.abs(x)
    return Model(inputs=[x1, x2], outputs=x)


optimizer = Adam(learning_rate=0.0001)

model = build_model()
model.compile(optimizer=optimizer, loss='mse', metrics='mse')

options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA 
dat_train = tf.data.Dataset.from_generator(
    generator= lambda: <load_function()> 
    output_types=((tf.int32, tf.float32), tf.float32)
) 
dat_train = dat_train.with_options(options) 

# keras training
model.fit(dat_train, epochs=50)


# custom training
for epoch in range(50):
    for (x1, x2), y in dat_train:
        with tf.GradientTape() as tape:
            y_pred = model((x1, x2), training=True)
            loss = model.loss(y, y_pred)
        grads = tape.gradient(loss, model.trainable_variables)
        model.optimizer.apply_gradients(zip(grads, model.trainable_variables))
```


### Relevant log output

_No response_</details>"
57391,Unable to crosscompile TFLite kernel tests with CMake,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.10.0 RC1

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

N/A

### Bazel version

N/A

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?

```shell
Unable to cross-build TensorFlow Lite Kernel tests with CMake. Issue identified on r2.10 release branch. 
The problem is tracked down to flatbuffers library update. Flatbuffers 2.0.6 attempts to generate files with built flatc compiler, if Python3 interpretter is available. At crosscompilation, this compiler is build for target archtitecture hence not working on host.

Solution is to disable the Python lookup for flatbuffers in flatbuffers.cmake file:

-DCMAKE_DISABLE_FIND_PACKAGE_Python3=TRUE
```
```


### Standalone code to reproduce the issue

```shell
$ mkdir build-native-tools && cd build-native-tools
$ cmake ../tensorflow/lite/tools/cmake/native_tools/flatbuffers/  -DCMAKE_INSTALL_PREFIX=./
$ make -j8 install
$ cd ..
$ mkdir build && cd build
$ # activate cross SDK
$ cmake -DCMAKE_TOOLCHAIN_FILE=${OE_CMAKE_TOOLCHAIN_FILE}  \
        -DTFLITE_ENABLE_XNNPACK=on -DTFLITE_ENABLE_RUY=on \
        -DTFLITE_HOST_TOOLS_DIR=../build-native-tools/ \
        -DTFLITE_KERNEL_TEST=On \
        ../tensorflow/lite/
$ make -j8 add_test
```


### Relevant log output

```shell
Running scripts/generate_code.py...
[ 37%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/f32-vbinary/gen/vmulc-minmax-neon-x8.c.o
Traceback (most recent call last):
  File ""/home/nxf39574/REPOS/tensorflow-imx/build/flatbuffers/scripts/generate_code.py"", line 148, in <module>
    flatc(
  File ""/home/nxf39574/REPOS/tensorflow-imx/build/flatbuffers/scripts/generate_code.py"", line 82, in flatc
    result = subprocess.run(cmd, cwd=str(cwd), check=True)
  File ""/opt/sdk-5.15.32_2.0.0-RC1/sysroots/x86_64-pokysdk-linux/usr/lib/python3.10/subprocess.py"", line 501, in run
    with Popen(*popenargs, **kwargs) as process:
  File ""/opt/sdk-5.15.32_2.0.0-RC1/sysroots/x86_64-pokysdk-linux/usr/lib/python3.10/subprocess.py"", line 966, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File ""/opt/sdk-5.15.32_2.0.0-RC1/sysroots/x86_64-pokysdk-linux/usr/lib/python3.10/subprocess.py"", line 1842, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
OSError: [Errno 8] Exec format error: '/home/nxf39574/REPOS/tensorflow/build/flatbuffers-flatc/src/flatbuffers-flatc-build/flatc'
make[6]: *** [CMakeFiles/flatc.dir/build.make:563: flatc] Error 1
make[6]: *** Deleting file 'flatc'
make[5]: *** [CMakeFiles/Makefile2:83: CMakeFiles/flatc.dir/all] Error 2
make[4]: *** [Makefile:136: all] Error 2
make[3]: *** [CMakeFiles/flatbuffers-flatc.dir/build.make:86: flatbuffers-flatc/src/flatbuffers-flatc-stamp/flatbuffers-flatc-build] Error 2
make[2]: *** [CMakeFiles/Makefile2:1516: CMakeFiles/flatbuffers-flatc.dir/all] Error 2
make[2]: *** Waiting for unfinished jobs....
```
</details>"
57390,Error in Kronecker product (use of ndim vs. tf.rank),"When using `tf.experimental.numpy.kron` in graph mode, one gets `AttributeError: 'Tensor' object has no attribute 'ndim'`.

The problem is the use of `a.ndim` instead of `tf.rank(a)` for a and b.

Example:
```python
@tf.function
def kron_error(a,b):
  return tf.experimental.numpy.kron(a,b)
a = tf.constant([[1.,2],[3,4]], dtype=tf.float64)
b = tf.constant([[1.,2.,3.,],[-1,4,5]], dtype=tf.float64)
kron_error(a, b)
```

One can fix it by using `tf.rank`. See this colab:
https://colab.research.google.com/drive/1quS7xqOsCKgak7pobYoG24v05nuEPvWn?usp=sharing

Code reference:
https://github.com/tensorflow/tensorflow/blob/8a20d54a3c1bfa38c03ea99a2ad3c1b0a45dfa95/tensorflow/python/ops/numpy_ops/np_math_ops.py#L383-L411"
57388,TensorFlowLiteSelectTfOps fails to link in v2.9,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

iOS 15.6.1

### Mobile device

iPhone X

### Python version

3.7.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Adding `pod 'TensorFlowLiteSelectTfOps', '2.9.1'` to my iOS app following (this guide)[https://www.tensorflow.org/lite/guide/ops_select#ios] causes linking to fail.

Note: With 2.9.0 the releases ship as `xcframework` which requires changing the path of the force_load flag in the guide to:

-force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.xcframework/ios-arm64/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps
````

Downgrading all TF pods to 2.9.0 has the same issue, and 2.8.0 does not exist.

Downgrading to 2.7.0 results in the error described in https://github.com/tensorflow/tensorflow/issues/44265#issuecomment-988734219

Finally, downgrading to 2.6.0 (and adjusting the force_load flag for non-xcframework` pods) fixes the issue.

Ref. https://github.com/tensorflow/tensorflow/issues/44265
@yishuangP @yyoon
```


### Standalone code to reproduce the issue

```shell
pod 'TensorFlowLiteC', '2.9.1'
pod 'TensorFlowLiteSwift', '2.9.1'
pod 'TensorFlowLiteSelectTfOps', '2.9.1'
```

and add the `force_load` flag:
```
-force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.xcframework/ios-arm64/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps
````
```


### Relevant log output

```shell
Undefined symbols for architecture arm64:
  ""tflite::logging_internal::MinimalLogger::LogFormatted(tflite::LogSeverity, char const*, char*)"", referenced from:
      tflite::StderrReporter::Report(char const*, char*) in TensorFlowLiteSelectTfOps(stderr_reporter.o)
  ""ruy::ScopedSuppressDenormals::ScopedSuppressDenormals()"", referenced from:
      tflite::Interpreter::Invoke() in TensorFlowLiteSelectTfOps(interpreter.o)
  ""ruy::ScopedSuppressDenormals::~ScopedSuppressDenormals()"", referenced from:
      tflite::Interpreter::Invoke() in TensorFlowLiteSelectTfOps(interpreter.o)
  ""tflite::tensor_utils::Sub1Vector(short const*, int, short*)"", referenced from:
      tflite::ops::builtin::lstm_eval::EvalInteger8x8_8(TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteLSTMParams const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, tflite::ops::builtin::lstm_eval::IntegerLstmParameter const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) in TensorFlowLiteSelectTfOps(lstm_eval.o)
      tflite::ops::builtin::lstm_eval::(anonymous namespace)::UpdateLstmCellInteger(int, int, short*, int, short const*, short*, short const*, bool, short) in TensorFlowLiteSelectTfOps(lstm_eval.o)
....
      tflite::ops::builtin::lstm_eval::(anonymous namespace)::LstmStepFloat(float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, TfLiteLSTMParams const*, int, int, int, int, int, int, float*, float*, float*, float*, float*, float*, float*) in TensorFlowLiteSelectTfOps(lstm_eval.o)
      tflite::ops::builtin::lstm_eval::(anonymous namespace)::LstmStepHybrid(float const*, signed char const*, unsigned char const*, float, signed char const*, unsigned char const*, float, signed char const*, unsigned char const*, float, signed char const*, unsigned char const*, float, float const*, signed char const*, float, signed char const*, float, signed char const*, float, signed char const*, float, signed char const*, unsigned char const*, float, signed char const*, unsigned char const*, float, signed char const*, unsigned char const*, float, signed char const*, unsigned char const*, float, signed char const*, float, signed char const*, float, signed char const*, float, float const*, float const*, float const*, float const*, float const*, float const*, float const*, float const*, signed char const*, unsigned char const*, float, float const*, TfLiteLSTMParams const*, int, int, int, int, int, int, float*, float*, float*, float*, float*, float*, float*, float*, float*, signed char*, signed char*, signed char*, signed char*, float*, float*, int*, float*, int*, int*, int*, int*, int, bool*, bool, tflite::CpuBackendContext*) in TensorFlowLiteSelectTfOps(lstm_eval.o)
      tflite::kernel_utils::RnnBatchStep(float const*, signed char const*, float, float const*, signed char const*, float, signed char const*, float, float const*, int, int, int, int, int, TfLiteFusedActivation, signed char*, signed char*, signed char*, float*, float*, float*, bool, int*, int*, int*, bool*) in TensorFlowLiteSelectTfOps(kernel_utils.o)
ld: symbol(s) not found for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```
</details>"
57387,TensorFlow building issue,"Description of the bug:
I'm trying to build TensorFlow with TensorRT support on Windows 11.
Devices specs:
Windows 11 Pro
GPU: NVIDIA Quadro P1000
RAM: 16GB
CUDA SDK Version: 11.2
cuDNN version: 8.1 (for cuda 11.2)
Build tool: MSVC build tool 2019 (latest version from VS Installer)
TensorRT version: 8.2
I've gotten no issue when configure the build:
d:\Nghich\tensorflow>python ./configure.py
You have bazel 5.0.0 installed.
Please specify the location of python. [Default is D:\Python39\python.exe]:

Found possible Python library paths:
D:\Python39\lib\site-packages
Please input the desired Python library path to use. Default is [D:\Python39\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: y
TensorRT support will be enabled for TensorFlow.

WARNING: TensorRT support on Windows is experimental

Found CUDA 11.2 in:
C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib/x64
C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/include
Found cuDNN 8 in:
C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib/x64
C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/include
Found TensorRT 8 in:
C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib
C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/include

Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 6.1

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:

Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
--config=mkl # Build with MKL support.
--config=mkl_aarch64 # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
--config=monolithic # Config for mostly static monolithic build.
--config=numa # Build with NUMA support.
--config=dynamic_kernels # (Experimental) Build kernels into separate shared objects.
--config=v1 # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
--config=nogcp # Disable GCP support.
--config=nonccl # Disable NVIDIA NCCL support.

I'm getting stuck with the following bugs:
d:\Nghich\tensorflow>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true --local_ram_resources=8192 --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from d:\nghich\tensorflow.bazelrc:
Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
'build' options: --python_path=D:/Python39/python.exe
INFO: Reading rc options for 'build' from d:\nghich\tensorflow.bazelrc:
'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false
INFO: Reading rc options for 'build' from d:\nghich\tensorflow.tf_configure.bazelrc:
'build' options: --action_env PYTHON_BIN_PATH=D:/Python39/python.exe --action_env PYTHON_LIB_PATH=D:/Python39/lib/site-packages --python_path=D:/Python39/python.exe --config=tensorrt --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --config=cuda --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from d:\nghich\tensorflow.bazelrc:
'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file d:\nghich\tensorflow.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file d:\nghich\tensorflow.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:tensorrt in file d:\nghich\tensorflow.bazelrc: --repo_env TF_NEED_TENSORRT=1
INFO: Found applicable config definition build:cuda in file d:\nghich\tensorflow.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file d:\nghich\tensorflow.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX
INFO: Found applicable config definition build:cuda in file d:\nghich\tensorflow.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:windows in file d:\nghich\tensorflow.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file d:\nghich\tensorflow.bazelrc: --define framework_shared_object=false --experimental_link_static_libraries_once=false
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/d5a6f80380113295caf442415f506e49cd015145.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/809855b56f06dd7182685f88fbbc64111df9339a.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
ERROR: D:/nghich/tensorflow/tensorflow/BUILD:1035:21: //tensorflow:libtensorflow_framework.so.2.11.0: no such attribute 'shared_lib_name' in 'cc_shared_library' rule
ERROR: D:/nghich/tensorflow/tensorflow/BUILD:1035:21: //tensorflow:libtensorflow_framework.so.2.11.0: no such attribute 'win_def_file' in 'cc_shared_library' rule
ERROR: D:/nghich/tensorflow/tensorflow/BUILD:1035:21: //tensorflow:libtensorflow_framework.2.11.0.dylib: no such attribute 'shared_lib_name' in 'cc_shared_library' rule
ERROR: D:/nghich/tensorflow/tensorflow/BUILD:1035:21: //tensorflow:libtensorflow_framework.2.11.0.dylib: no such attribute 'win_def_file' in 'cc_shared_library' rule
ERROR: D:/nghich/tensorflow/tensorflow/BUILD:1035:21: //tensorflow:tensorflow_framework.dll: no such attribute 'shared_lib_name' in 'cc_shared_library' rule
ERROR: D:/nghich/tensorflow/tensorflow/BUILD:1035:21: //tensorflow:tensorflow_framework.dll: no such attribute 'win_def_file' in 'cc_shared_library' rule
ERROR: D:/nghich/tensorflow/tensorflow/BUILD:1157:21: //tensorflow:libtensorflow_cc.so.2.11.0: no such attribute 'shared_lib_name' in 'cc_shared_library' rule
ERROR: D:/nghich/tensorflow/tensorflow/BUILD:1157:21: //tensorflow:libtensorflow_cc.so.2.11.0: no such attribute 'win_def_file' in 'cc_shared_library' rule
ERROR: D:/nghich/tensorflow/tensorflow/BUILD:1157:21: //tensorflow:libtensorflow_cc.2.11.0.dylib: no such attribute 'shared_lib_name' in 'cc_shared_library' rule
ERROR: D:/nghich/tensorflow/tensorflow/BUILD:1157:21: //tensorflow:libtensorflow_cc.2.11.0.dylib: no such attribute 'win_def_file' in 'cc_shared_library' rule
ERROR: D:/nghich/tensorflow/tensorflow/BUILD:1157:21: //tensorflow:tensorflow_cc.dll: no such attribute 'shared_lib_name' in 'cc_shared_library' rule
ERROR: D:/nghich/tensorflow/tensorflow/BUILD:1157:21: //tensorflow:tensorflow_cc.dll: no such attribute 'win_def_file' in 'cc_shared_library' rule
ERROR: D:/nghich/tensorflow/tensorflow/tools/pip_package/BUILD:278:10: errors encountered resolving select() keys for //tensorflow/tools/pip_package:build_pip_package
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted:
INFO: Elapsed time: 187.076s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (10 packages loaded, 12 targets configured)
I've followed the link https://www.tensorflow.org/install/source_windows to build and used exactly the same version of equivalent components."
57386,Model Maker Object Detection Tutorial cannot export model,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: source 
-   **TensorFlow version (use command below)**: 2.8.0
-   **Python version**: 3.7
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: same as Google Colab
-   **GPU model and memory**: same as Google Colab
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
When running the line `model.export(export_dir='.')` in the [Colab notebook](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb) of the example, the error occurs.

### Source code / logs
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
[<ipython-input-7-4c3d25dac6cc>](https://localhost:8080/#) in <module>
----> 1 model.export(export_dir='.')

8 frames
[/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/custom_model.py](https://localhost:8080/#) in export(self, export_dir, tflite_filename, label_filename, vocab_filename, saved_model_filename, tfjs_folder_name, export_format, **kwargs)
    130       tflite_filepath = os.path.join(export_dir, tflite_filename)
    131       export_tflite_kwargs, kwargs = _get_params(self._export_tflite, **kwargs)
--> 132       self._export_tflite(tflite_filepath, **export_tflite_kwargs)
    133       tf.compat.v1.logging.info(
    134           'TensorFlow Lite model exported successfully: %s' % tflite_filepath)

[/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py](https://localhost:8080/#) in _export_tflite(self, tflite_filepath, quantization_config, with_metadata, export_metadata_json_file)
    195             writer_utils.load_file(tflite_filepath),
    196             [self.model_spec.config.mean_rgb],
--> 197             [self.model_spec.config.stddev_rgb], [label_filepath])
    198         writer_utils.save_file(writer.populate(), tflite_filepath)
    199 

[/usr/local/lib/python3.7/dist-packages/tensorflow_lite_support/metadata/python/metadata_writers/object_detector.py](https://localhost:8080/#) in create_for_inference(cls, model_buffer, input_norm_mean, input_norm_std, label_file_paths, score_calibration_md)
    293         input_md=input_md,
    294         output_category_md=output_category_md,
--> 295         output_score_md=output_score_md)

[/usr/local/lib/python3.7/dist-packages/tensorflow_lite_support/metadata/python/metadata_writers/object_detector.py](https://localhost:8080/#) in create_from_metadata_info(cls, model_buffer, general_md, input_md, output_location_md, output_category_md, output_score_md, output_number_md)
    224     b = flatbuffers.Builder(0)
    225     b.Finish(
--> 226         model_metadata.Pack(b),
    227         _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)
    228 

[/usr/local/lib/python3.7/dist-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py](https://localhost:8080/#) in Pack(self, builder)
   2698             subgraphMetadatalist = []
   2699             for i in range(len(self.subgraphMetadata)):
-> 2700                 subgraphMetadatalist.append(self.subgraphMetadata[i].Pack(builder))
   2701             ModelMetadataStartSubgraphMetadataVector(builder, len(self.subgraphMetadata))
   2702             for i in reversed(range(len(self.subgraphMetadata))):

[/usr/local/lib/python3.7/dist-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py](https://localhost:8080/#) in Pack(self, builder)
   1018             inputTensorMetadatalist = []
   1019             for i in range(len(self.inputTensorMetadata)):
-> 1020                 inputTensorMetadatalist.append(self.inputTensorMetadata[i].Pack(builder))
   1021             SubGraphMetadataStartInputTensorMetadataVector(builder, len(self.inputTensorMetadata))
   1022             for i in reversed(range(len(self.inputTensorMetadata))):

[/usr/local/lib/python3.7/dist-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py](https://localhost:8080/#) in Pack(self, builder)
    256             processUnitslist = []
    257             for i in range(len(self.processUnits)):
--> 258                 processUnitslist.append(self.processUnits[i].Pack(builder))
    259             TensorMetadataStartProcessUnitsVector(builder, len(self.processUnits))
    260             for i in reversed(range(len(self.processUnits))):

[/usr/local/lib/python3.7/dist-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py](https://localhost:8080/#) in Pack(self, builder)
   2076     def Pack(self, builder):
   2077         if self.options is not None:
-> 2078             options = self.options.Pack(builder)
   2079         ProcessUnitStart(builder)
   2080         ProcessUnitAddOptionsType(builder, self.optionsType)

[/usr/local/lib/python3.7/dist-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py](https://localhost:8080/#) in Pack(self, builder)
   3013                 for i in reversed(range(len(self.mean))):
   3014                     builder.PrependFloat32(self.mean[i])
-> 3015                 mean = builder.EndVector()
   3016         if self.std is not None:
   3017             if np is not None and type(self.std) is np.ndarray:

TypeError: EndVector() missing 1 required positional argument: 'vectorNumElems'
```
"
57385,Tensorflow horovod distributed on a100 mig error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

1.15.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Error log: 
```Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA
Fatal Python error: Aborted


### Standalone code to reproduce the issue

```shell
source code:https://github.com/horovod/horovod/blob/master/examples/tensorflow/tensorflow_mnist.py

platform:
hardware: a100, enable mig
docker
env:
NVIDIA_VISIBLE_DEVICES=0:0,0:1

start script:
mpirun -allow-run-as-root -np 2 -H localhost:2 -bind-to none -map-by slot -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH python test.py
```


### Relevant log output

_No response_</details>"
57375,"official tutorial has Warning for op: ""CropAndResize""","Official tutorial is based on 2.9.0-rc1, while I reproduced the warning on both TF2.9.0 and TF2.8.2.

The warning appears on the page: https://www.tensorflow.org/hub/tutorials/object_detection#apply_module

2022-04-27 14:31:15.178364: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_FLOAT } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_FLOAT shape { dim { size: -2642 } dim { size: -2643 } dim { size: -2644 } dim { size: 1088 } } } inputs { dtype: DT_FLOAT shape { dim { size: -22 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -22 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 17 } } device { type: ""GPU"" vendor: ""NVIDIA"" model: ""Tesla P100-PCIE-16GB"" frequency: 1328 num_cores: 56 environment { key: ""architecture"" value: ""6.0"" } environment { key: ""cuda"" value: ""11020"" } environment { key: ""cudnn"" value: ""8100"" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 16038952960 bandwidth: 732160000 } outputs { dtype: DT_FLOAT shape { dim { size: -22 } dim { size: 17 } dim { size: 17 } dim { size: 1088 } } }
"
57372,tf.distribute.MultiWorkerMirroredStrategy getting stuck,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.5

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.6.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

A100- 40G/80G

### Current Behaviour?

```shell
https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#multi-worker_configuration

I am trying to follow multi worker training following mentioned tutorial.

Machine info:
Machine 1:
    Nvidia A100-40GB x 2Gpus
Machine 2:
    Nvidia A100-80GB x 2Gpus

Environment:
 tensorflow/tensorflow:2.5.0-gpu (tensorflow docker image)
```


### Standalone code to reproduce the issue

```shell
import json
import os
import sys
import tensorflow as tf

#### if it is machine 1, machine 1 tfconfig 
os.environ[""TF_CONFIG""] = json.dumps({
    ""cluster"": {
        ""chief"": [""localhost:0000""],
        ""worker"": [""localhost:1111""]
    },
   ""task"": {""type"": ""chief"", ""index"": 0}
})

#### else machine 2 tfconfig
os.environ[""TF_CONFIG""] = json.dumps({
    ""cluster"": {
        ""chief"": [""localhost:0000""],
        ""worker"": [""localhost:1111""]
    },
   ""task"": {""type"": ""worker"", ""index"": 0}
})


strategy = tf.distribute.MultiWorkerMirroredStrategy()

def mnist_dataset(batch_size):
  (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
  # The `x` arrays are in uint8 and have values in the [0, 255] range.
  # You need to convert them to float32 with values in the [0, 1] range.
  x_train = x_train / np.float32(255)
  y_train = y_train.astype(np.int64)
  train_dataset = tf.data.Dataset.from_tensor_slices(
      (x_train, y_train)).shuffle(60000).repeat().batch(batch_size)
  return train_dataset

def build_and_compile_cnn_model():
  model = tf.keras.Sequential([
      tf.keras.layers.InputLayer(input_shape=(28, 28)),
      tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
      tf.keras.layers.Conv2D(32, 3, activation='relu'),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(128, activation='relu'),
      tf.keras.layers.Dense(10)
  ])
  model.compile(
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
      metrics=['accuracy'])
  return model


per_worker_batch_size = 64
tf_config = json.loads(os.environ['TF_CONFIG'])
num_workers = len(tf_config['cluster']['worker'])

strategy = tf.distribute.MultiWorkerMirroredStrategy()

global_batch_size = per_worker_batch_size * num_workers
multi_worker_dataset = mnist_setup.mnist_dataset(global_batch_size)

with strategy.scope():
  # Model building/compiling need to be within `strategy.scope()`.
  multi_worker_model = mnist_setup.build_and_compile_cnn_model()
```

I am running same code on both the machine. I have just changed the tfconfig code section on each machine based on usage.
```


### Relevant log output

```shell
Machine 1 logs

2022-08-22 21:21:29.138865: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job chief -> {0 -> localhost:0000}  
2022-08-22 21:21:29.138884: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> localhost:1111} 
2022-08-22 21:21:29.152227: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:0000


```
Machine 2 logs
```
2022-08-22 21:21:29.152227: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:0000

```
Both machine has same logs and then they is no console output after this lines.
```"
57371,"Build TF 2.9.1 with use ROCM 5.2.3 with GCC 12.2.0 failed, but succesfull build with use ROCM 5.2.3 with GCC 11.3.0 or with GCC 12.2.0 without use ROCM","Build TF 2.9.1 with use ROCM 5.2.3 with GCC 12.2.0 failed,

but succesfull build with use ROCM 5.2.3 with GCC 11.3.0
or with GCC 12.2.0 without use ROCM

<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

Gentoo

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

5.0.2

### GCC/Compiler version

12.2.0

### CUDA/cuDNN version

11.7.1/8.5.0.96

### GPU model and memory

AMD Vega Frontier 16Gb

### Current Behaviour?

```shell
Succesfull buid with gcc 12.2.0 as this done by use old gcc 11.3.0
```


### Standalone code to reproduce the issue

```shell
# Execution platform: @local_execution_config_platform//:platform
SUBCOMMAND: # //tensorflow:libtensorflow.so.2.9.1 [action 'Linking tensorflow/libtensorflow.so.2.9.1',
configuration: bazel-out/k8-collect2: error: ld returned 1 exit status
INFO: Elapsed time: 3957.177s, Critical Path: 165.85s
```


### Relevant log output

```shell
# Execution platform: @local_execution_config_platform//:platform
SUBCOMMAND: # //tensorflow:libtensorflow.so.2.9.1 [action 'Linking tensorflow/libtensorflow.so.2.9.1', configuration: e9286302664b4ab1cd1341b866a623627f93e9b8c897b9c18eb12e802620134e, execution platform: @local_execution_config_platform//:platform]
(cd /var/tmp/portage/sci-libs/tensorflow-2.9.1-r3/work/tensorflow-2.9.1-python3_10-bazel-base/execroot/org_tensorflow && \
  exec env - \
    GLIBCXX_USE_CXX11_ABI=0 \
    HOME=/var/tmp/portage/sci-libs/tensorflow-2.9.1-r3/homedir \
    KERAS_HOME=/var/tmp/portage/sci-libs/tensorflow-2.9.1-r3/temp/.keras \
    MLIR_CRASH_REPRODUCER_DIRECTORY=/var/tmp/portage/sci-libs/tensorflow-2.9.1-r3/temp/.crash \
    PATH=/var/tmp/portage/sci-libs/tensorflow-2.9.1-r3/temp/python3.10/bin:/usr/lib/portage/python3.10/ebuild-helpers/xattr:/usr/lib/portage/python3.10/ebuild-helpers:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/bin:/usr/lib/llvm/14/bin:/usr/lib64/subversion/bin:/opt/cuda/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3.10 \
    PYTHON_LIB_PATH=/usr/lib/python3.10/site-packages \
    ROCBLAS_TENSILE_LIBPATH=/usr/lib64/rocblas/library \
    ROCM_PATH=/usr \
    TF2_BEHAVIOR=1 \
    TF_SYSTEM_LIBS=curl,cython,gif,icu,libjpeg_turbo,lmdb,nasm,png,pybind11,zlib \
  external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/libtensorflow.so.2.9.1-2.params)build.log here: https://gist.github.com/raw/c94573f1badbff781eabc359dfef20c2
# Configuration: e9286302664b4ab1cd1341b866a623627f93e9b8c897b9c18eb12e802620134e
# Execution platform: @local_execution_config_platform//:platform
ERROR: /var/tmp/portage/sci-libs/tensorflow-2.9.1-r3/work/tensorflow-2.9.1-python3_10/tensorflow/BUILD:1003:20: Linking tensorflow/libtensorflow.so.2.9.1 failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /var/tmp/portage/sci-libs/tensorflow-2.9.1-r3/work/tensorflow-2.9.1-python3_10-bazel-base/execroot/org_tensorflow && \
  exec env - \
    GLIBCXX_USE_CXX11_ABI=0 \
    HOME=/var/tmp/portage/sci-libs/tensorflow-2.9.1-r3/homedir \
    KERAS_HOME=/var/tmp/portage/sci-libs/tensorflow-2.9.1-r3/temp/.keras \
    MLIR_CRASH_REPRODUCER_DIRECTORY=/var/tmp/portage/sci-libs/tensorflow-2.9.1-r3/temp/.crash \
    PATH=/var/tmp/portage/sci-libs/tensorflow-2.9.1-r3/temp/python3.10/bin:/usr/lib/portage/python3.10/ebuild-helpers/xattr:/usr/lib/portage/python3.10/ebuild-helpers:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/bin:/usr/lib/llvm/14/bin:/usr/lib64/subversion/bin:/opt/cuda/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3.10 \
    PYTHON_LIB_PATH=/usr/lib/python3.10/site-packages \
    ROCBLAS_TENSILE_LIBPATH=/usr/lib64/rocblas/library \
    ROCM_PATH=/usr \
    TF2_BEHAVIOR=1 \
    TF_SYSTEM_LIBS=curl,cython,gif,icu,libjpeg_turbo,lmdb,nasm,png,pybind11,zlib \
  external/local_config_rocm/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/libtensorflow.so.2.9.1-2.params)
# Configuration: e9286302664b4ab1cd1341b866a623627f93e9b8c897b9c18eb12e802620134e
# Execution platform: @local_execution_config_platform//:platform
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/mat_mul_op.pic.o:mat_mul_op.cc:function tensorflow::CSRMatMulGPUOp<std::complex<float> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<float> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/mat_mul_op.pic.o:mat_mul_op.cc:function tensorflow::CSRMatMulGPUOp<double>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, double>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/mat_mul_op.pic.o:mat_mul_op.cc:function tensorflow::CSRMatMulGPUOp<std::complex<double> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<double> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/mat_mul_op.pic.o:mat_mul_op.cc:function tensorflow::CSRMatMulGPUOp<float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, float>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, double>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, double>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, double>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, double>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, std::complex<float> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<float> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, std::complex<float> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<float> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, std::complex<double> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<double> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/_objs/kernels/sparse_mat_mul_op.pic.o:sparse_mat_mul_op.cc:function tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, std::complex<double> >::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, std::complex<double> >::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
collect2: error: ld returned 1 exit status
INFO: Elapsed time: 3957.177s, Critical Path: 165.85s
```
</details>

build.log here: https://gist.github.com/raw/c94573f1badbff781eabc359dfef20c2"
57368,Shape Error when Passing Different Features for Wide and Deep Models,"**OS: ** Windows 11
**TensorFlow: **Installed from pip. Version 2.9.1
**Python Version: **  3.10.5

I am attempting to recreate the Wide and Deep model using Tensorflow's WideDeepModel library; however, I am encountering an issue when attempting to differentiate between the wide model inputs and the deep model inputs. Referenced below is the code that I am using.

```
  optimizer = tf.keras.optimizers.Ftrl(
          l1_regularization_strength=0.001,
          learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(
              initial_learning_rate=0.1, decay_steps=10000, decay_rate=0.9))

  linear_model = tf.compat.v1.keras.experimental.LinearModel()
  linear_model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])
  linear_model.fit(X_train[wideInputs], y_train, epochs=50)

  dnn_model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(1)
  ])
  dnn_model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])

  combined_model = tf.compat.v1.keras.experimental.WideDeepModel(linear_model, dnn_model)
  combined_model.compile(optimizer=[optimizer, optimizer], loss='mse', metrics=['accuracy'])

  combined_model.fit([X_train[wideInputs], X_train[wideInputs] + X_train[dnnInputs]], y_train, epochs=50)

  print('Accuracy', combined_model.evaluate(X_test, y_test, return_dict=True))
```

My goal is to fit the combined_model variable with the linear inputs only in the first model (wide) and the deep inputs (both wide and deep inputs); however, I encounter an error with mismatching shapes between the two inputs. I assume that the number of rows need to remain the same but the features can vary between the two models as we are defining two separate set of features that should be used. However, when I use a different set of features, I am returned with the following error:

`ValueError: Exception encountered when calling layer ""linear_model"" (type LinearModel).

    Input 0 of layer ""dense"" is incompatible with the layer: expected axis -1 of input shape to have value 3004, but received input with shape (None, 3009)

    Call arguments received by layer ""linear_model"" (type LinearModel):
      • inputs=tf.Tensor(shape=(None, 3009), dtype=float32)`
Any feedback would be greatly appreciated.

Just to note, when I do not differentiate the features (fit using `combined_model.fit([X_train, X_train], y_train, epochs=50)),` the model runs; however, it is not using the expected wide and deep inputs this way.

I also referenced the following pages to work with the code.

https://www.tensorflow.org/guide/migrate/canned_estimators#tf2_using_keras_widedeepmodel
https://www.tensorflow.org/api_docs/python/tf/keras/experimental/WideDeepModel

Additionally, the dataset that I am passing in looks like the following: https://i.stack.imgur.com/h1ae1.png
"
57366,TFLite build issue on windows for version 2.8.0 ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

4.2.2

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened!
Unable to install TFLite v2.8.0 on windows.


 SetEnv.bat ; bazel build -c opt //tensorflow/lite:tensorflowlite.dll --action_env PYTHON_BIN_PATH=C:\Users\hnalla\AppData\Local\Programs\Python\Python310\python.exe

Build is failing
expecting to have tflite library for v2.8.0 build for windows
```


### Standalone code to reproduce the issue

```shell
Unable to install TFLite v2.8.0 on windows.

 SetEnv.bat ; bazel build -c opt //tensorflow/lite:tensorflowlite.dll --action_env PYTHON_BIN_PATH=C:\Users\hnalla\AppData\Local\Programs\Python\Python310\python.exe
```


### Relevant log output

```shell
C:\test\tensorflow-2.8.0\tensorflow-2.8.0>set PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\Common7\IDE\VC\vcpackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\Common7\Tools;C:\Program Files (x86)\Windows Kits\10\\Bin\10.0.19041.0\x64;C:\Program Files (x86)\Windows Kits\10\\Bin\10.0.19041.0\x86;C:\Program Files (x86)\Windows Kits\10\\Bin\x64;C:\Program Files (x86)\Windows Kits\10\\Bin\x86;;;C:\Program Files\Eclipse Foundation\jdk-8.0.302.8-hotspot\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\libnvvp;C:\windows\system32;C:\windows;C:\windows\System32\Wbem;C:\windows\System32\WindowsPowerShell\v1.0\;C:\windows\System32\OpenSSH\;C:\Program Files\MathWorks\DevelTools\bin;C:\Program Files\Perforce\;C:\Program Files\Microsoft DNX\Dnvm\;C:\Program Files\Microsoft SQL Server\120\Tools\Binn\;C:\Program Files\Microsoft SQL Server\130\Tools\Binn\;C:\Program Files\dotnet\;C:\Program Files\NVIDIA Corporation\NVSMI;C:\Program Files\PuTTY\;C:\Users\chaoluo\.dnx\bin;F:\share\apps\BuildTools\MKSTK_9.2p3\mksnt;F:\share\apps\BuildTools\1.3\win32\bin;C:\windows\system32;C:\msys64\usr\bin;C:\Program Files\Microsoft SQL Server\Client SDK\ODBC\170\Tools\Binn\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\NVIDIA Corporation\Nsight Compute 2020.1.0\;C:\Program Files\NVIDIA Corporation\NvToolsExt\bin\x64;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.0\extras\CUPTI\lib64;D:\sayans\MATLAB\R2023a\bin;C:\Program Files\MATLAB\R2022b\bin;D:\R2022a\bin;D:\chaoluo\R2022a_beta\bin;C:\Program Files\MATLAB\R2021a\bin;\\mathworks\hub\share\apps\GPUTools\Nsight\NVIDIA Corporation\Nsight Systems 2021.1.1\target-windows-x64\;C:\Program Files\NVIDIA Corporation\Nsight Compute 2020.3.0\;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\bin;D:\chaoluo\mkldnn\win64\lib;D:\chaoluo\cudnn\cuda\bin;C:\Program Files\CMake\bin;C:\Program Files\Microsoft VS Code\bin;C:\Program Files\Git\cmd;C:\test;C:\Users\hnalla\AppData\Local\Programs\Python\Python310\Scripts\;C:\Users\hnalla\AppData\Local\Programs\Python\Python310\;C:\Users\hnalla\AppData\Local\Microsoft\WindowsApps;C:\Users\hnalla\.dotnet\tools

C:\test\tensorflow-2.8.0\tensorflow-2.8.0>set INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\VC\Tools\MSVC\14.29.30133\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\VC\Tools\MSVC\14.29.30133\atlmfc\include;C:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.19041.0\winrt;H:\28\hnalla.Bdeepcoder.j2041457\matlab\extern\include;

C:\test\tensorflow-2.8.0\tensorflow-2.8.0>set LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\VC\Tools\MSVC\14.29.30133\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\VC\Tools\MSVC\14.29.30133\atlmfc\lib\x64;C:\Program Files (x86)\Windows Kits\10\\Lib\10.0.19041.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\\lib\10.0.19041.0\um\x64;H:\28\hnalla.Bdeepcoder.j2041457\matlab\lib\win64;

C:\test\tensorflow-2.8.0\tensorflow-2.8.0>set LIBPATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\VC\Tools\MSVC\14.29.30133\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\VC\Tools\MSVC\14.29.30133\atlmfc\lib\x64;
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from c:\test\tensorflow-2.8.0\tensorflow-2.8.0\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/hnalla/AppData/Local/Programs/Python/Python310/python.exe
INFO: Reading rc options for 'build' from c:\test\tensorflow-2.8.0\tensorflow-2.8.0\.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'build' from c:\test\tensorflow-2.8.0\tensorflow-2.8.0\.tf_configure.bazelrc:
  'build' options: --host_force_python=PY2 --action_env PYTHON_BIN_PATH=C:/Python27/python.exe --action_env PYTHON_LIB_PATH=C:/Users/hnalla/AppData/Local/Programs/Python/Python310/Lib/site-packages --python_path=C:/Python27/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions
INFO: Reading rc options for 'build' from c:\test\tensorflow-2.8.0\tensorflow-2.8.0\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file c:\test\tensorflow-2.8.0\tensorflow-2.8.0\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\test\tensorflow-2.8.0\tensorflow-2.8.0\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:windows in file c:\test\tensorflow-2.8.0\tensorflow-2.8.0\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\test\tensorflow-2.8.0\tensorflow-2.8.0\.bazelrc: --define framework_shared_object=false
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/c3e082762b7664bbc7ffd2c39e86464928e27c0c.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/lite:tensorflowlite.dll (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: C:/users/hnalla/_bazel_hnalla/vdnqzhd6/external/XNNPACK/BUILD.bazel:6644:19: Compiling src/f32-spmm/gen/8x4-minmax-scalar.c failed: (Exit 2): cl.exe failed: error executing command
  cd C:/users/hnalla/_bazel_hnalla/vdnqzhd6/execroot/org_tensorflow
  SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\winrt;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\MSBuild\14.0\bin\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\windows\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:\Users\hnalla\AppData\Local\Programs\Python\Python310\python.exe
    SET PYTHON_LIB_PATH=C:/Users/hnalla/AppData/Local/Programs/Python/Python310/Lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\hnalla\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\hnalla\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe @bazel-out/x64_windows-opt/bin/external/XNNPACK/_objs/scalar_prod_microkernels/8x4-minmax-scalar.obj.params
Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9002 : ignoring unknown option '/experimental:preprocessor'
external/XNNPACK/src\xnnpack/params.h(2365): error C2719: 'params': formal parameter with requested alignment of 64 won't be aligned
external/XNNPACK/src\xnnpack/params.h(2372): error C2719: 'params': formal parameter with requested alignment of 64 won't be aligned
external/XNNPACK/src\xnnpack/params.h(2380): error C2719: 'params': formal parameter with requested alignment of 64 won't be aligned
external/XNNPACK/src\xnnpack/params.h(2390): error C2719: 'params': formal parameter with requested alignment of 64 won't be aligned
external/XNNPACK/src\xnnpack/params.h(2400): error C2719: 'params': formal parameter with requested alignment of 64 won't be aligned
Target //tensorflow/lite:tensorflowlite.dll failed to build
INFO: Elapsed time: 1.291s, Critical Path: 0.37s
INFO: 13 processes: 13 internal.
FAILED: Build did NOT complete successfully
```
</details>"
57365,[BUG] Gradient tape *inside* tf.function broken for tf.Variable argument.," 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.7, 2.9, nightly

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.1)

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.function (since TF 2.7 I believe) now remembers the storage of the variable and treats them as tensors (using the `__tf_retrace__` mechanism). This works fine whenever the value of the variable is needed and not it's identity. However, when taking the gradient, the identity matters, not the value.

This means when decorating a function that takes the gradient of y with respect to x (x being a parameter here), it traces the function, uses the value of the TensorLike object and **creates the gradient with respect to the parameter's identity (watching the parameter)**.
Calling the same function with a different parameter won't retrace (!) and just use the value, however since it doesn't retrace, it won't re-calculate the gradient with respect to the *identity* of the new variable given as argument but instead use the remembered gradient.

Basically, the problem arises since tf.function and tensor-like objects function functionally (i.e. gradient with respect to argument one, two etc) and tf.Variable, that are global statebased objects. Using a statebased object as a ""functional"" object breaks the correct behavior of GradientTape.

## Bug or feature?

I do understand the merits of this in a function that doesn't take gradients. However, it breaks for the logic of the gradient.

## Possible fixes

There are workarounds, i.e. to always give parameters in a dict with their name, the names triggering a retrace.
```


### Standalone code to reproduce the issue

```shell
Link to colab with the code below: https://colab.research.google.com/drive/1z26ZIoiFEBO9icGJ5bYusu4XY5u-vpkG?usp=sharing


import tensorflow as tf

x1 = tf.Variable(2.0)
x2 = tf.Variable(4.0)


def f():
    res = x1 + x2 ** 2 / 2
    return res


def grad(param):
    with tf.GradientTape(watch_accessed_variables=False) as tape:
        tape.watch(param)
        value = f()
    return tape.gradient(value, param)

jitted_grad = tf.function(grad)

y1 = grad(x1)
y1_jit = jitted_grad(x1)
assert abs(y1 - 1.0) < 1e-5  # because d x1 / dx1 = 1
assert abs(y1_jit - 1.0) < 1e-5
y2 = grad(x2)
y2_jit = jitted_grad(x2)
print(f""y2: {y2}, should be 4"")  # but is 1 because it uses the derivative of x1
print(f""y2_jit: {y2_jit}, should also be 4"")  # but is 1 because it uses the derivative of x1
assert abs(y2 - 4.0) < 1e-5  # because d / dx x**2/2 = x -> 4
assert abs(y2_jit - 4.0) < 1e-5  # fails!
```
```


### Relevant log output

_No response_</details>"
57364,XNNPACK fails to crosscompile for `elinux_armhf`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

master / v2.10.0-rc1

### Custom Code

No

### OS Platform and Distribution

elinux_armhf

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.1.1

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
XNNPACK fails to build on `master` and `v2.10.0-rc1` with `--config=elinux_armhf` which is required to build for Raspberry Pi 3. The same command worked well for `v2.9.1` but unfortunately fails with the latest version of TensorFlow and XNNPACK.

@Maratyszcza do you have any pointers in how to fix this issue or whether there are any flags missing in `.bazelrc`? It would be great if we could fix this for the `v2.10.0` stable release.
```


### Standalone code to reproduce the issue

```shell
bazelisk build  @org_tensorflow//tensorflow/lite:tflite_with_xnnpack --config=elinux_armhf
```
```


### Relevant log output

```shell
ERROR: /home/lgeiger/.cache/bazel/_bazel_lgeiger/8da1bfbfd9537ea4f19db64e1f55bee4/external/XNNPACK/BUILD.bazel:8896:19: Compiling src/qs8-vcvt/gen/vcvt-armv6simd-x8.c failed: (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command /home/lgeiger/.cache/bazel/_bazel_lgeiger/8da1bfbfd9537ea4f19db64e1f55bee4/external/armhf_linux_toolchain/bin/arm-linux-gnueabihf-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 83 arguments skipped)
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c: In function 'xnn_qs8_vcvt_ukernel__armv6simd_x8':
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:26:9: error: unknown type name 'int16x2_t'
   const int16x2_t vminus_input_zero_point = (int16x2_t) params->armv6simd.minus_input_zero_point;
         ^~~~~~~~~
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:26:46: error: 'int16x2_t' undeclared (first use in this function); did you mean 'int16_t'?
   const int16x2_t vminus_input_zero_point = (int16x2_t) params->armv6simd.minus_input_zero_point;
                                              ^~~~~~~~~
                                              int16_t
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:26:46: note: each undeclared identifier is reported only once for each function it appears in
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:26:57: error: expected ',' or ';' before 'params'
   const int16x2_t vminus_input_zero_point = (int16x2_t) params->armv6simd.minus_input_zero_point;
                                                         ^~~~~~
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:30:11: error: unknown type name 'int8x4_t'
     const int8x4_t vx0123 = (int8x4_t) unaligned_indexed_load_u32(x, 0);
           ^~~~~~~~
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:30:30: error: 'int8x4_t' undeclared (first use in this function); did you mean 'int64_t'?
     const int8x4_t vx0123 = (int8x4_t) unaligned_indexed_load_u32(x, 0);
                              ^~~~~~~~
                              int64_t
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:30:40: error: expected ',' or ';' before 'unaligned_indexed_load_u32'
     const int8x4_t vx0123 = (int8x4_t) unaligned_indexed_load_u32(x, 0);
                                        ^~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:31:20: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx4567'
     const int8x4_t vx4567 = (int8x4_t) unaligned_indexed_load_u32(x, 1);
                    ^~~~~~
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:31:20: error: 'vx4567' undeclared (first use in this function)
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:31:39: error: expected ';' before 'unaligned_indexed_load_u32'
     const int8x4_t vx4567 = (int8x4_t) unaligned_indexed_load_u32(x, 1);
                                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~
                                       ;
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:34:21: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx02'
     const int16x2_t vx02 = __sxtab16(vminus_input_zero_point, vx0123);
                     ^~~~
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:34:21: error: 'vx02' undeclared (first use in this function); did you mean 'vx0123'?
     const int16x2_t vx02 = __sxtab16(vminus_input_zero_point, vx0123);
                     ^~~~
                     vx0123
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:35:21: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx13'
     const int16x2_t vx13 = __sxtab16(vminus_input_zero_point, __ror(vx0123, 8));
                     ^~~~
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:35:21: error: 'vx13' undeclared (first use in this function); did you mean 'vx0123'?
     const int16x2_t vx13 = __sxtab16(vminus_input_zero_point, __ror(vx0123, 8));
                     ^~~~
                     vx0123
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:36:21: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx46'
     const int16x2_t vx46 = __sxtab16(vminus_input_zero_point, vx4567);
                     ^~~~
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:36:21: error: 'vx46' undeclared (first use in this function)
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:37:21: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx57'
     const int16x2_t vx57 = __sxtab16(vminus_input_zero_point, __ror(vx4567, 8));
                     ^~~~
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:37:21: error: 'vx57' undeclared (first use in this function)
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:68:20: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx0123'
     const int8x4_t vx0123 = (int8x4_t) unaligned_load_u32(x);
                    ^~~~~~
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:68:20: error: 'vx0123' undeclared (first use in this function)
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:68:39: error: expected ';' before 'unaligned_load_u32'
     const int8x4_t vx0123 = (int8x4_t) unaligned_load_u32(x);
                                       ^~~~~~~~~~~~~~~~~~~
                                       ;
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:71:21: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx02'
     const int16x2_t vx02 = __sxtab16(vminus_input_zero_point, vx0123);
                     ^~~~
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:72:21: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx13'
     const int16x2_t vx13 = __sxtab16(vminus_input_zero_point, __ror(vx0123, 8));
                     ^~~~
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:91:20: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx0123'
     const int8x4_t vx0123 = (int8x4_t) unaligned_load_u32(x);
                    ^~~~~~
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:91:39: error: expected ';' before 'unaligned_load_u32'
     const int8x4_t vx0123 = (int8x4_t) unaligned_load_u32(x);
                                       ^~~~~~~~~~~~~~~~~~~
                                       ;
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:93:21: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx02'
     const int16x2_t vx02 = __sxtab16(vminus_input_zero_point, vx0123);
                     ^~~~
external/XNNPACK/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:94:21: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx13'
     const int16x2_t vx13 = __sxtab16(vminus_input_zero_point, __ror(vx0123, 8));
                     ^~~~
Target //tensorflow/lite:tflite_with_xnnpack failed to build
```
</details>"
57363,model.save: Tried to export a function which references 'untracked' resource even though the tensor should be tracked,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.7.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7.0_516.01/8100

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I do not understand why this particular tensor (StylePredictionModelDummy/dummy_conv/kernel:0) is not tracked. As you can see my model (style_transfer_model) is created with the Tensorflow functional API and the untracked tensor in question is instantiated as part of StylePredictionModelDummy.__init__() in line 92 and assigned as a class property to self.feature_extractor which is what the error recommends to do to track the value.

The outputs of StylePredictionModelDummy(line 93) are split up(lines 100-105) and routed to the ConditionalInstanceNormalization(line 71) layers as scale and bias inside of the expand layers (line 109). So it also can't be that the tensor is just not used because it is used.

I expect the tensor to be tracked and that the model can be saved.
```


### Standalone code to reproduce the issue

https://colab.research.google.com/drive/1HFq2k12IGO6bsnbd8WWvg3yci6g12BD0?usp=sharing
```python
import numpy as np
import tensorflow as tf

class DummyModel(tf.keras.Model):
    feature_extractor = None

    def __init__(self, name=""StylePredictionModelDummy""):
        super().__init__(name=name)

        self.feature_extractor = tf.keras.layers.Conv2D(1, 9, 5, padding='same', name=""dummy_conv"")

    def call(self, inputs, training=None, mask=None):
        x = self.feature_extractor(inputs)
        return x

image_shape = (None, 960//4, 1920//4, 3)

model = DummyModel()
element = tf.convert_to_tensor(np.zeros((1, image_shape[1], image_shape[2], 3)))

# call once to build model
result = model(element)

model.save(filepath=""%TEMP%/model"", include_optimizer=False, save_format='tf')
```


### Relevant log output

```shell
C:\condaEnvs\realtime-style-transfer\python.exe C:/projects/realtime-style-transfer/save_using_checkpoint.py -C C:\projects\realtime-style-transfer\logs\2022-08-22-10-47-13.310922\last_training_checkpoint -o C:\projects\realtime-style-transfer\temp\saved.tf 
tensorflow - DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client. (tpu_cluster_resolver.py:32 @ 2022-08-22 10:55:32,983)
2022-08-22 10:55:35.933767: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
tensorflow - WARNING - `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default. (mobilenet_v3.py:256 @ 2022-08-22 10:55:35,939)
models.stylePrediction - INFO - Using 100 style parameters (stylePrediction.py:24 @ 2022-08-22 10:55:37,877)
models.stylePrediction - DEBUG - Using 192 norm parameters (stylePrediction.py:33 @ 2022-08-22 10:55:37,882)
tensorflow - WARNING - `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default. (mobilenet_v3.py:256 @ 2022-08-22 10:55:37,883)
root - INFO - Running inference to build model... (save_using_checkpoint.py:49 @ 2022-08-22 10:55:44,154)
root - INFO - Saving model... (save_using_checkpoint.py:55 @ 2022-08-22 10:55:47,613)
tensorflow - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. (saving_utils.py:328 @ 2022-08-22 10:55:47,613)
absl - WARNING - Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 67). These functions will not be directly callable after loading. (save.py:233 @ 2022-08-22 10:56:36,316)
Traceback (most recent call last):
  File ""C:\projects\realtime-style-transfer\save_using_checkpoint.py"", line 56, in <module>
    style_transfer_model.save(filepath=str(outpath), include_optimizer=False, save_format='tf')
  File ""C:\condaEnvs\realtime-style-transfer\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\condaEnvs\realtime-style-transfer\lib\site-packages\tensorflow\python\saved_model\save.py"", line 473, in _map_captures_to_created_tensors
    raise AssertionError(
AssertionError: Tried to export a function which references 'untracked' resource Tensor(""52018:0"", shape=(), dtype=resource). TensorFlow objects (e.g. tf.Variable) captured by functions must be 'tracked' by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly. See the information below:
	Function name = b'__inference_signature_wrapper_53008'
	Captured Tensor = <ResourceHandle(name=""Resource-218-at-0x231ff25da30"", device=""/job:localhost/replica:0/task:0/device:CPU:0"", container=""Anonymous"", type=""class tensorflow::Var"", dtype and shapes : ""[ DType enum: 1, Shape: [3,3,3,16] ]"")>
	Trackable referencing this tensor = <tf.Variable 'Conv/kernel:0' shape=(3, 3, 3, 16) dtype=float32>

Process finished with exit code 1
```
</details>"
57362,TypeError: stftLayer() missing 1 required positional argument: 'x',"### 1. System information

- Linux Ubuntu:
- TensorFlow installation (pip package):
- TensorFlow 1.14:

### 2. Code

Code:

# -*- coding: utf-8 -*-

import os
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Lambda, Input, Conv2D, BatchNormalization, Conv2DTranspose, Concatenate, LayerNormalization, PReLU
from tensorflow.keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping, ModelCheckpoint
from tensorflow.python.layers.convolutional import conv1d
from tensorflow.python.keras.models import Model
from numpy.linalg import pinv
from tensorflow import real,imag
from tensorflow.python.ops.signal.fft_ops import rfft
from tensorflow.python.keras.layers import concatenate

import soundfile as sf
import librosa
from random import seed
import numpy as np
from numpy import expand_dims
import tqdm

from scipy.signal import get_window 
from modules import DprnnBlock
from utils import reshape, transpose, ParallelModelCheckpoints
from data_loader import *

class DPCRN_model():
    '''
    Class to create and train the DPCRN model
    '''
    
    def __init__(self, batch_size = 1,
                       length_in_s = 5,
                       fs = 16000,
                       norm = 'iLN',
                       numUnits = 128,
                       numDP = 2,
                       block_len = 400,
                       block_shift = 200,
                       max_epochs = 200,
                       lr = 1e-3):

        # defining default cost function
        self.cost_function = self.snr_cost
        self.model = None
        # defining default parameters
        self.fs = fs
        self.length_in_s = length_in_s
        self.batch_size = batch_size
        # number of the hidden layer size in the LSTM
        self.numUnits = numUnits
        # number of the DPRNN modules
        self.numDP = numDP
        # frame length and hop length in STFT
        self.block_len = block_len
        self.block_shift = block_shift
        self.lr = lr
        self.max_epochs = max_epochs
        # window for STFT: sine win
        win = np.sin(np.arange(.5,self.block_len-.5+1)/self.block_len*np.pi)
        self.win = tf.constant(win,dtype = 'float32')

        self.L = (16000*length_in_s-self.block_len)//self.block_shift + 1
        
        self.multi_gpu = False
        # iLN for instant Layer norm and BN for Batch norm
        self.input_norm = norm
        
    @staticmethod
    def snr_cost(s_estimate, s_true):
        '''
        Static Method defining the cost function. 
        The negative signal to noise ratio is calculated here. The loss is 
        always calculated over the last dimension. 
        '''
        # calculating the SNR
        snr = tf.reduce_mean(tf.math.square(s_true), axis=-1, keepdims=True) / \
            (tf.reduce_mean(tf.math.square(s_true-s_estimate), axis=-1, keepdims=True) + 1e-8)
        # using some more lines, because TF has no log10
        num = tf.math.log(snr + 1e-8) 
        denom = tf.math.log(tf.constant(10, dtype=num.dtype))
        loss = -10*(num / (denom))

        return loss
    
    @staticmethod
    def sisnr_cost(s_hat, s):
        '''
        Static Method defining the cost function. 
        The negative signal to noise ratio is calculated here. The loss is 
        always calculated over the last dimension. 
        '''
        def norm(x):
            return tf.reduce_sum(x**2, axis=-1, keepdims=True)

        s_target = tf.reduce_sum(
            s_hat * s, axis=-1, keepdims=True) * s / norm(s)
        upp = norm(s_target)
        low = norm(s_hat - s_target)
  
        return -10 * tf.math.log(upp /low) / tf.math.log(10.0)  
    
    def spectrum_loss(self,y_true):
        '''
        spectrum MSE loss 
        '''
        enh_real = self.enh_real
        enh_imag = self.enh_imag
        enh_mag = tf.sqrt(enh_real**2 + enh_imag**2 + 1e-8)
        
        true_real,true_imag = self.stftLayer(y_true, mode='real_imag')
        true_mag = tf.sqrt(true_real**2 + true_imag**2 + 1e-8)
        
        loss_real = tf.reduce_mean((enh_real - true_real)**2,)
        loss_imag = tf.reduce_mean((enh_imag - true_imag)**2,)
        loss_mag = tf.reduce_mean((enh_mag - true_mag)**2,) 
        
        return loss_real + loss_imag + loss_mag
    
    def spectrum_loss_phasen(self, s_hat,s,gamma = 0.3):
        
        true_real,true_imag = self.stftLayer(s, mode='real_imag')
        hat_real,hat_imag = self.stftLayer(s_hat, mode='real_imag')

        true_mag = tf.sqrt(true_real**2 + true_imag**2+1e-9)
        hat_mag = tf.sqrt(hat_real**2 + hat_imag**2+1e-9)

        true_real_cprs = (true_real / true_mag )*true_mag**gamma
        true_imag_cprs = (true_imag / true_mag )*true_mag**gamma
        hat_real_cprs = (hat_real / hat_mag )* hat_mag**gamma
        hat_imag_cprs = (hat_imag / hat_mag )* hat_mag**gamma

        loss_mag = tf.reduce_mean((hat_mag**gamma - true_mag**gamma)**2,)         
        loss_real = tf.reduce_mean((hat_real_cprs - true_real_cprs)**2,)
        loss_imag = tf.reduce_mean((hat_imag_cprs - true_imag_cprs)**2,)

        return 0.7 * loss_mag + 0.3 * ( loss_imag + loss_real ) 
    
    def lossWrapper(self):
        '''
        A wrapper function which returns the loss function. This is done to
        to enable additional arguments to the loss function if necessary.
        '''
        def lossFunction(y_true,y_pred):
            # calculating loss and squeezing single dimensions away
            loss = tf.squeeze(self.cost_function(y_pred,y_true)) 
            mag_loss = tf.log(self.spectrum_loss(y_true) + 1e-8)
            # calculate mean over batches
            loss = tf.reduce_mean(loss)
            return loss + mag_loss 
        
        return lossFunction
    
    '''
    In the following some helper layers are defined.
    '''  
    def seg2frame(self, x):
        '''
        split signal x to frames
        '''
        frames = tf.signal.frame(x, self.block_len, self.block_shift)
        if self.win is not None:
            frames = self.win*frames
        return frames
    
    def stftLayer(self, x, mode ='mag_pha'):
        '''
        Method for an STFT helper layer used with a Lambda layer
        mode: 'mag_pha'   return magnitude and phase spectrogram
              'real_imag' return real and imaginary parts
        '''
        # creating frames from the continuous waveform
        frames = tf.signal.frame(x, self.block_len, self.block_shift)
        if self.win is not None:
            frames = self.win*frames
        # calculating the fft over the time frames. rfft returns NFFT/2+1 bins.
        stft_dat = tf.signal.rfft(frames)
        # calculating magnitude and phase from the complex signal
        output_list = []
        if mode == 'mag_pha':
            mag = tf.math.abs(stft_dat)
            phase = tf.math.angle(stft_dat)
            output_list = [mag, phase]
        elif mode == 'real_imag':
            real = tf.math.real(stft_dat)
            imag = tf.math.imag(stft_dat)
            output_list = [real, imag]            
        # returning magnitude and phase as list
        return output_list
    
    def fftLayer(self, x):
        '''
        Method for an fft helper layer used with a Lambda layer.
        The layer calculates the rFFT on the last dimension and returns
        the magnitude and phase of the STFT.
        '''
        # calculating the fft over the time frames. rfft returns NFFT/2+1 bins.
        stft_dat = tf.signal.rfft(x)
        # calculating magnitude and phase from the complex signal
        mag = tf.abs(stft_dat)
        phase = tf.math.angle(stft_dat)
        # returning magnitude and phase as list
        return [mag, phase]

    def ifftLayer(self, x,mode = 'mag_pha'):
        '''
        Method for an inverse FFT layer used with an Lambda layer. This layer
        calculates time domain frames from magnitude and phase information. 
        As input x a list with [mag,phase] is required.
        '''
        if mode == 'mag_pha':
        # calculating the complex representation
            s1_stft = (tf.cast(x[0], tf.complex64) * 
                        tf.exp( (1j * tf.cast(x[1], tf.complex64))))
        elif mode == 'real_imag':
            s1_stft = tf.cast(x[0], tf.complex64) + 1j * tf.cast(x[1], tf.complex64)
        # returning the time domain frames
        return tf.signal.irfft(s1_stft)  
    
    def overlapAddLayer(self, x):
        '''
        Method for an overlap and add helper layer used with a Lambda layer.
        This layer reconstructs the waveform from a framed signal.
        '''
        # calculating and returning the reconstructed waveform
        '''
        if self.move_dc:
            x = x - tf.expand_dims(tf.reduce_mean(x,axis = -1),2)
        '''
        return tf.signal.overlap_and_add(x, self.block_shift)              
     
    def mk_mask(self,x):
        '''
        Method for complex ratio mask and add helper layer used with a Lambda layer.
        '''
        [noisy_real,noisy_imag,mask] = x
        noisy_real = noisy_real[:,:,:,0]
        noisy_imag = noisy_imag[:,:,:,0]
        
        mask_real = mask[:,:,:,0]
        mask_imag = mask[:,:,:,1]
        
        enh_real = noisy_real * mask_real - noisy_imag * mask_imag
        enh_imag = noisy_real * mask_imag + noisy_imag * mask_real
        
        return [enh_real,enh_imag]

    def build_DPCRN_model(self, name = 'model0'):
       
        time_dat = Input(batch_shape=(8, 320000))
       
        # calculate STFT
     
        time_dat = tf.expand_dims(time_dat,2)
      
        real,imag = Lambda(self.stftLayer,arguments = {'mode':'real_imag'})(time_dat)
        
        real = tf.reshape(real,[self.batch_size,-1,201,1])
        imag = tf.reshape(imag,[self.batch_size,-1,201,1])

        input_complex_spec = Concatenate(axis = -1)([real,imag])
        '''encoder'''

        if self.input_norm == 'iLN':    
            input_complex_spec = LayerNormalization(axis = [-1,-2], name = 'input_norm')(input_complex_spec)
        elif self.input_norm == 'BN':    
            input_complex_spec =BatchNormalization(name = 'input_norm')(input_complex_spec)
        
        # causal padding [1,0],[0,2]
        conv_1 = Conv2D(32, (2,5),(1,2),name = name+'_conv_1',padding = [[0,0],[1,0],[0,2],[0,0]])(input_complex_spec)
        bn_1 = BatchNormalization(name = name+'_bn_1')(conv_1)
        out_1 = PReLU(shared_axes=[1,2])(bn_1)
        # causal padding [1,0],[0,1]
        conv_2 = Conv2D(32, (2,3),(1,2),name = name+'_conv_2',padding = [[0,0],[1,0],[0,1],[0,0]])(out_1)
        bn_2 = BatchNormalization(name = name+'_bn_2')(conv_2)
        out_2 = PReLU(shared_axes=[1,2])(bn_2)
        # causal padding [1,0],[1,1]
        conv_3 = Conv2D(32, (2,3),(1,1),name = name+'_conv_3',padding = [[0,0],[1,0],[1,1],[0,0]])(out_2)
        bn_3 = BatchNormalization(name = name+'_bn_3')(conv_3)
        out_3 = PReLU(shared_axes=[1,2])(bn_3)
        # causal padding [1,0],[1,1]
        conv_4 = Conv2D(64, (2,3),(1,1),name = name+'_conv_4',padding = [[0,0],[1,0],[1,1],[0,0]])(out_3)
        bn_4 = BatchNormalization(name = name+'_bn_4')(conv_4)
        out_4 = PReLU(shared_axes=[1,2])(bn_4)
        # causal padding [1,0],[1,1]
        conv_5 = Conv2D(128, (2,3),(1,1),name = name+'_conv_5',padding = [[0,0],[1,0],[1,1],[0,0]])(out_4)
        bn_5 = BatchNormalization(name = name+'_bn_5')(conv_5)
        out_5 = PReLU(shared_axes=[1,2])(bn_5)
        
        dp_in = out_5
        
        for i in range(self.numDP):
            
            dp_in = DprnnBlock(numUnits = self.numUnits, batch_size = self.batch_size, L = -1,width = 50,channel = 128, causal=True)(dp_in)#self.DPRNN_kernal(dp_in,str(i),last_dp = 0)
       
        dp_out = dp_in
        
        '''decoder'''
        skipcon_1 = Concatenate(axis = -1)([out_5,dp_out])

        deconv_1 = Conv2DTranspose(64,(2,3),(1,1),name = name+'_dconv_1',padding = 'same')(skipcon_1)
        dbn_1 = BatchNormalization(name = name+'_dbn_1')(deconv_1)
        dout_1 = PReLU(shared_axes=[1,2])(dbn_1)

        skipcon_2 = Concatenate(axis = -1)([out_4,dout_1])
        
        deconv_2 = Conv2DTranspose(32,(2,3),(1,1),name = name+'_dconv_2',padding = 'same')(skipcon_2)
        dbn_2 = BatchNormalization(name = name+'_dbn_2')(deconv_2)
        dout_2 = PReLU(shared_axes=[1,2])(dbn_2)
        
        skipcon_3 = Concatenate(axis = -1)([out_3,dout_2])
        
        deconv_3 = Conv2DTranspose(32,(2,3),(1,1),name = name+'_dconv_3',padding = 'same')(skipcon_3)
        dbn_3 = BatchNormalization(name = name+'_dbn_3')(deconv_3)
        dout_3 = PReLU(shared_axes=[1,2])(dbn_3)
        
        skipcon_4 = Concatenate(axis = -1)([out_2,dout_3])

        deconv_4 = Conv2DTranspose(32,(2,3),(1,2),name = name+'_dconv_4',padding = 'same')(skipcon_4)
        dbn_4 = BatchNormalization(name = name+'_dbn_4')(deconv_4)
        dout_4 = PReLU(shared_axes=[1,2])(dbn_4)
        
        skipcon_5 = Concatenate(axis = -1)([out_1,dout_4])
        
        deconv_5 = Conv2DTranspose(2,(2,5),(1,2),name = name+'_dconv_5',padding = 'valid')(skipcon_5)
        
        '''no activation'''        
        deconv_5 = deconv_5[:,:-1,:-2]

        #output_mask = Activation('tanh')(dbn_5)
        output_mask = deconv_5

        #enh_spec = self.mk_mask([real,imag,output_mask])
        enh_spec = Lambda(self.mk_mask)([real,imag,output_mask])
        #enh_spec = MK_M()([real,imag,output_mask])
        self.enh_real, self.enh_imag = enh_spec[0],enh_spec[1]
        
        enh_frame = ifft_Layer(name=""enh_frame"")(enh_spec)
        enh_frame = Lambda(self.ifftLayer,arguments = {'mode':'real_imag'})(enh_spec)
        #enh_frame = enh_frame * self.win
        #enh_time = Overlap_addLayer(name=""enh_time"")(enh_frame)
        enh_time = Lambda(self.overlapAddLayer,name='enhanced_time')(enh_frame)
        #enh_time = tf.signal.overlap_and_add(enh_frame, 200)
        self.model = Model(time_dat,enh_time)
        self.model.summary()

        return self.model
        
    def compile_model(self):
        '''
        Method to compile the model for training
        '''
        # use the Adam optimizer with a clipnorm of 3
        optimizerAdam = keras.optimizers.Adam(lr=self.lr, clipnorm=3.0)
        # compile model with loss function
        self.model.compile(loss=self.lossWrapper(),optimizer=optimizerAdam)

    def train_model(self, runName, data_generator):
        '''
        Method to train the model. 
        '''
        self.compile_model()
        
        # create save path if not existent
        savePath = './models_'+ runName+'/' 
        if not os.path.exists(savePath):
            os.makedirs(savePath)
        # create log file writer
        csv_logger = CSVLogger(savePath+ 'training_test' +runName+ '.log')
        # create callback for the adaptive learning rate
        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,
                              patience=5, min_lr=10**(-10), cooldown=1)
        # create callback for early stopping
        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, 
            patience=10,  mode='auto', baseline=None)
   
        # create data generator for training data

        self.model.fit_generator(data_generator.generator(batch_size = self.batch_size,validation = False), 
                                 validation_data = data_generator.generator(batch_size =self.batch_size,validation = True),
                                 epochs = self.max_epochs, 
                                 steps_per_epoch = data_generator.train_length//self.batch_size,
                                 validation_steps = self.batch_size,
                                 #use_multiprocessing=True,
                                 callbacks=[reduce_lr, csv_logger, early_stopping])
        # clear out garbage
        self.model.save(""./dpcrn.h5"")
        tf.keras.backend.clear_session()

if __name__ == '__main__':
    
    converter = tf.lite.TFLiteConverter.from_keras_model_file(""xxxxxx.h5"",custom_objects={'DprnnBlock':DprnnBlock,""stftLayer"":stftLayer,""ifftLayer"":ifftLayer,
                                                   ""overlapAddLayer"":overlapAddLayer,""mk_mask"":mk_mask})
    
    tflite_model = converter.converter()




--The error.....:

When i use ""from_keras_model_file"" ,load my model (using ""model.save"" to save the model) have a error :

""TypeError: stftLayer() missing 1 required positional argument: 'x'

Why does it prompt that the saved model needs to input parameters ""x"". can you know the reason ??

 Thanks"
57360,Stack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tf2.6.0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
./tf-opt  --tf-to-tosa-pipeline ./t.mlir -o ttosa.mlir --print-ir-after-failure --mlir-elide-elementsattrs-if-larger=4
2022-08-22 14:14:18.914738: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
TensorFlow crashed, please file a bug on https://github.com/tensorflow/tensorflow/issues with the trace below.
Stack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):
./tf-opt(+0xc34f4a1)[0x55be78dba4a1]
./tf-opt(+0xc34d3dd)[0x55be78db83dd]
./tf-opt(+0xc34daed)[0x55be78db8aed]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420)[0x7ff1cff94420]
./tf-opt(+0xc2a2617)[0x55be78d0d617]
./tf-opt(+0x7951dfb)[0x55be743bcdfb]
./tf-opt(+0xbefa7ef)[0x55be789657ef]
./tf-opt(+0xbee22f4)[0x55be7894d2f4]
./tf-opt(+0x796939d)[0x55be743d439d]
./tf-opt(+0xc1cfdf4)[0x55be78c3adf4]
./tf-opt(+0xc1d0162)[0x55be78c3b162]
./tf-opt(+0xc1d2419)[0x55be78c3d419]
./tf-opt(+0xc32dfea)[0x55be78d98fea]
./tf-opt(+0x1719c0f)[0x55be6e184c0f]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x114df)[0x7ff1cff914df]
./tf-opt(+0xc32f46f)[0x55be78d9a46f]
./tf-opt(+0xc331303)[0x55be78d9c303]
/lib/x86_64-linux-gnu/libstdc++.so.6(+0xd6de4)[0x7ff1d1950de4]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x8609)[0x7ff1cff88609]
/lib/x86_64-linux-gnu/libc.so.6(clone+0x43)[0x7ff1cfe92133]
Stack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):
./tf-opt(+0xc34f4a1)[0x55be78dba4a1]
./tf-opt(+0xc34d3dd)[0x55be78db83dd]
./tf-opt(+0xc34daed)[0x55be78db8aed]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420)[0x7ff1cff94420]
./tf-opt(+0xc2a2617)[0x55be78d0d617]
./tf-opt(+0x7951dfb)[0x55be743bcdfb]
./tf-opt(+0xbefa7ef)[0x55be789657ef]
./tf-opt(+0xbee22f4)[0x55be7894d2f4]
./tf-opt(+0x796939d)[0x55be743d439d]
./tf-opt(+0xc1cfdf4)[0x55be78c3adf4]
./tf-opt(+0xc1d0162)[0x55be78c3b162]
./tf-opt(+0xc1d2419)[0x55be78c3d419]
./tf-opt(+0xc32dfea)[0x55be78d98fea]
./tf-opt(+0x1719c0f)[0x55be6e184c0f]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x114df)[0x7ff1cff914df]
./tf-opt(+0xc32f46f)[0x55be78d9a46f]
./tf-opt(+0xc331303)[0x55be78d9c303]
/lib/x86_64-linux-gnu/libstdc++.so.6(+0xd6de4)[0x7ff1d1950de4]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x8609)[0x7ff1cff88609]
/lib/x86_64-linux-gnu/libc.so.6(clone+0x43)[0x7ff1cfe92133]
Segmentation fault
```


### Standalone code to reproduce the issue

```shell
./tf-opt  --tf-to-tosa-pipeline ./t.mlir -o ttosa.mlir --print-ir-after-failure --mlir-elide-elementsattrs-if-larger=4
2022-08-22 14:14:18.914738: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
TensorFlow crashed, please file a bug on https://github.com/tensorflow/tensorflow/issues with the trace below.
Stack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):
./tf-opt(+0xc34f4a1)[0x55be78dba4a1]
./tf-opt(+0xc34d3dd)[0x55be78db83dd]
./tf-opt(+0xc34daed)[0x55be78db8aed]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420)[0x7ff1cff94420]
./tf-opt(+0xc2a2617)[0x55be78d0d617]
./tf-opt(+0x7951dfb)[0x55be743bcdfb]
./tf-opt(+0xbefa7ef)[0x55be789657ef]
./tf-opt(+0xbee22f4)[0x55be7894d2f4]
./tf-opt(+0x796939d)[0x55be743d439d]
./tf-opt(+0xc1cfdf4)[0x55be78c3adf4]
./tf-opt(+0xc1d0162)[0x55be78c3b162]
./tf-opt(+0xc1d2419)[0x55be78c3d419]
./tf-opt(+0xc32dfea)[0x55be78d98fea]
./tf-opt(+0x1719c0f)[0x55be6e184c0f]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x114df)[0x7ff1cff914df]
./tf-opt(+0xc32f46f)[0x55be78d9a46f]
./tf-opt(+0xc331303)[0x55be78d9c303]
/lib/x86_64-linux-gnu/libstdc++.so.6(+0xd6de4)[0x7ff1d1950de4]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x8609)[0x7ff1cff88609]
/lib/x86_64-linux-gnu/libc.so.6(clone+0x43)[0x7ff1cfe92133]
Stack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):
./tf-opt(+0xc34f4a1)[0x55be78dba4a1]
./tf-opt(+0xc34d3dd)[0x55be78db83dd]
./tf-opt(+0xc34daed)[0x55be78db8aed]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420)[0x7ff1cff94420]
./tf-opt(+0xc2a2617)[0x55be78d0d617]
./tf-opt(+0x7951dfb)[0x55be743bcdfb]
./tf-opt(+0xbefa7ef)[0x55be789657ef]
./tf-opt(+0xbee22f4)[0x55be7894d2f4]
./tf-opt(+0x796939d)[0x55be743d439d]
./tf-opt(+0xc1cfdf4)[0x55be78c3adf4]
./tf-opt(+0xc1d0162)[0x55be78c3b162]
./tf-opt(+0xc1d2419)[0x55be78c3d419]
./tf-opt(+0xc32dfea)[0x55be78d98fea]
./tf-opt(+0x1719c0f)[0x55be6e184c0f]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x114df)[0x7ff1cff914df]
./tf-opt(+0xc32f46f)[0x55be78d9a46f]
./tf-opt(+0xc331303)[0x55be78d9c303]
/lib/x86_64-linux-gnu/libstdc++.so.6(+0xd6de4)[0x7ff1d1950de4]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x8609)[0x7ff1cff88609]
/lib/x86_64-linux-gnu/libc.so.6(clone+0x43)[0x7ff1cfe92133]
Segmentation fault
```


### Relevant log output

_No response_</details>"
57359,failed to set new cublas math mode: CUBLAS_STATUS_INVALID_VALUE,"### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.9.0-18-gd8ce9f9c301 2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I have a dynamic `keras.Model` named `symbol_net`. When executing forward computation (call `call` method), sometimes it crashes as follows if there's a Dense layer in the model.

I have searched on the Internet and tries so many solutions including combining them, like 

```python
import tensorflow as tf  # type: ignore
from tensorflow import keras
from keras import layers  # type: ignore
from keras import backend as K
physical_devices = tf.config.list_physical_devices(""GPU"")
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
config.gpu_options.per_process_gpu_memory_fraction = 0.333
session = tf.compat.v1.Session(config=config)
K.set_session(session)
```

But all of them don't work. I have a GPU with 12 GiB. On the multi-user machine, when I was running the code, there remains 12000 MiB for me, so it's enough. My model is quite small, like [this](https://ibb.co/DzrFqKQ) , which won't take a lot of mem.

```python
2022-08-21 23:09:42.546282: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2022-08-21 23:09:42.546307: E tensorflow/stream_executor/cuda/cuda_blas.cc:197] failed to set new cublas math mode: CUBLAS_STATUS_INVALID_VALUE
2022-08-21 23:09:42.546320: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:438 : INTERNAL: Failed initializing math mode
	outputs= (shape=(2, 2, 2, 2) dtype=<dtype: 'float32'>)
Traceback (most recent call last):
  File ""/home/colin/code/nnsmith/nnsmith/graph_gen_2.py"", line 1899, in <module>
    ic(net(*input_list))
  File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/colin/miniconda3/lib/python3.10/site-packages/tensorflow/python/eager/execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: Exception encountered when calling layer ""symbol_net"" (type SymbolNet).

Graph execution error:

Detected at node 'dense/Tensordot/MatMul' defined at (most recent call last):
    File ""/home/colin/code/nnsmith/nnsmith/graph_gen_2.py"", line 1899, in <module>
      ic(net(*input_list))
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/engine/training.py"", line 490, in __call__
      return super().__call__(*args, **kwargs)
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/engine/base_layer.py"", line 1014, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
    File ""/home/colin/code/nnsmith/nnsmith/graph_gen_2.py"", line 547, in call
      for inst, inps, outs, op, node_id in self.instructions.data:
    File ""/home/colin/code/nnsmith/nnsmith/graph_gen_2.py"", line 576, in call
      outputs = inst(*input_tensors)
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/engine/base_layer.py"", line 1014, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/layers/core/dense.py"", line 224, in call
      outputs = tf.tensordot(inputs, self.kernel, [[rank - 1], [0]])
Node: 'dense/Tensordot/MatMul'
Failed initializing math mode
	 [[{{node dense/Tensordot/MatMul}}]] [Op:__inference_call_146]

Call arguments received by layer ""symbol_net"" (type SymbolNet):
  • args=('tf.Tensor(shape=(2, 2, 2, 2), dtype=float32)', 'tf.Tensor(shape=(1, 1, 1, 1), dtype=float32)')
  • kwargs={'training': 'None'}
```


### Standalone code to reproduce the issue

```shell
Currently my code is large. Sorry.
```


### Relevant log output

```shell
2022-08-21 23:09:55.580410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:55.601460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:55.601638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:55.602081: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-08-21 23:09:55.603250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:55.603399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:55.603554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:55.915740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:55.915925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:55.916011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:55.916113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4013 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6
2022-08-21 23:09:56.068318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:56.068541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:56.068654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:56.068796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:56.068904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:56.068997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4013 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6
2022-08-21 23:09:56.183640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:56.183809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:56.183889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:56.184001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:56.184083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-21 23:09:56.184142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4013 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6

2022-08-21 23:09:57.669085: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2022-08-21 23:09:57.669107: E tensorflow/stream_executor/cuda/cuda_blas.cc:197] failed to set new cublas math mode: CUBLAS_STATUS_INVALID_VALUE
2022-08-21 23:09:57.669119: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:438 : INTERNAL: Failed initializing math mode
	outputs= (shape=(1, 1) dtype=<dtype: 'float32'>)
Traceback (most recent call last):
  File ""/home/colin/code/nnsmith/nnsmith/graph_gen_2.py"", line 1899, in <module>
    ic(net(*input_list))
  File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/colin/miniconda3/lib/python3.10/site-packages/tensorflow/python/eager/execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: Exception encountered when calling layer ""symbol_net"" (type SymbolNet).

Graph execution error:

Detected at node 'dense/MatMul' defined at (most recent call last):
    File ""/home/colin/code/nnsmith/nnsmith/graph_gen_2.py"", line 1899, in <module>
      ic(net(*input_list))
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/engine/training.py"", line 490, in __call__
      return super().__call__(*args, **kwargs)
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/engine/base_layer.py"", line 1014, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
    File ""/home/colin/code/nnsmith/nnsmith/graph_gen_2.py"", line 547, in call
      for inst, inps, outs, op, node_id in self.instructions.data:
    File ""/home/colin/code/nnsmith/nnsmith/graph_gen_2.py"", line 576, in call
      outputs = inst(*input_tensors)
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/engine/base_layer.py"", line 1014, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
    File ""/home/colin/miniconda3/lib/python3.10/site-packages/keras/layers/core/dense.py"", line 221, in call
      outputs = tf.matmul(a=inputs, b=self.kernel)
Node: 'dense/MatMul'
Failed initializing math mode
	 [[{{node dense/MatMul}}]] [Op:__inference_call_156]

Call arguments received by layer ""symbol_net"" (type SymbolNet):
  • args=('tf.Tensor(shape=(2, 2, 2, 1), dtype=float32)', 'tf.Tensor(shape=(1,), dtype=float32)')
  • kwargs={'training': 'None'}
```
"
57358,Forward-mode autodifff fails to process input in `tf.Variable` type when taking higher-order JVPs and throws `Internal error`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If the input type is `tf.Variable`, the forward-mode auto differentiation will throw internal error ""ValueError: Internal error: Tried to take gradients (or similar) of a variable without handle data"".
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

primal = tf.Variable(1.1)
with tf.autodiff.ForwardAccumulator(primal, tf.constant(1.)) as outer:
  with tf.autodiff.ForwardAccumulator(primal, tf.constant(1.)) as inner:
    primal_out = primal ** tf.constant(3.5)
inner_jvp = inner.jvp(primal_out)
outer_jvp = outer.jvp(inner_jvp)
```


### Relevant log output

```shell
raceback (most recent call last):
  File ""/Users/xxx/Documents/Github/bug/bessel_k1.py"", line 6, in <module>
    primal_out = primal ** tf.constant(3.5)
  File ""/Users/xxx/Documents/Github/venv/lib/python3.8/site-packages/tensorflow/python/ops/variables.py"", line 1074, in _run_op
    return tensor_oper(a.value(), *args, **kwargs)
  File ""/Users/xxx/Documents/Github/venv/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 582, in value
    return self._read_variable_op()
  File ""/Users/xxx/Documents/Github/venv/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 691, in _read_variable_op
    result = read_and_set_handle()
  File ""/Users/xxx/Documents/Github/venv/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 681, in read_and_set_handle
    result = gen_resource_variable_ops.read_variable_op(
  File ""/Users/xxx/Documents/Github/venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py"", line 479, in read_variable_op
    _result = pywrap_tfe.TFE_Py_FastPathExecute(
  File ""/Users/xxx/Documents/Github/venv/lib/python3.8/site-packages/tensorflow/python/eager/forwardprop.py"", line 208, in _jvp_dispatch
    return _jvp_exact_shapes(op_name, attr_tuple, inputs, outputs, tangents,
  File ""/Users/xxx/Documents/Github/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2453, in __call__
    return graph_function._call_flat(
  File ""/Users/xxx/Documents/Github/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1866, in _call_flat
    forward_function, args_with_tangents = forward_backward.forward()
  File ""/Users/xxx/Documents/Github/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1397, in forward
    forward_function = self._functions.forward(
  File ""/Users/xxx/Documents/Github/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1129, in forward
    self._forward_and_backward_functions(inference_args, input_tangents))
  File ""/Users/xxx/Documents/Github/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1280, in _forward_and_backward_functions
    return self._build_functions_for_outputs(
  File ""/Users/xxx/Documents/Github/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 893, in _build_functions_for_outputs
    forward_wrapper = self._wrap_forward_function_with_jvps(
  File ""/Users/xxx/Documents/Github/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 962, in _wrap_forward_function_with_jvps
    gradient_shape, gradient_dtype = default_gradient.shape_and_dtype(
  File ""/Users/xxx/Documents/Github/venv/lib/python3.8/site-packages/tensorflow/python/ops/default_gradient.py"", line 40, in shape_and_dtype
    raise ValueError(""Internal error: Tried to take gradients (or similar) ""
ValueError: Internal error: Tried to take gradients (or similar) of a variable without handle data:
Tensor(""Placeholder:0"", shape=(), dtype=resource)
```
</details>"
57357,`tf.nn.silu` have wrong gradient for complex input,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
`tf.nn.silu(x)` is equal to `x * sigmoid(x)`, however, when I compute the gradient with respect to a complex tensor, the gradient of the two functions are different. I believe that the `tf.nn.silu`-gradient is wrong because it is inconsistent with the numerical gradient.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
input = tf.complex([[[0.0], [0.0]]], [[[5.35], [6.61]]])
print(""input: "", input)
with tf.GradientTape() as t:
  t.watch(input)
  output1 = tf.nn.silu(input)
grad1 = t.gradient(output1, input)
with tf.GradientTape() as t:
  t.watch(input)
  output2 = input * tf.math.sigmoid(input)
grad2 = t.gradient(output2, input)

assert tf.experimental.numpy.allclose(output1, output2) # Assertion Pass
print(grad1)
print(grad2)
assert tf.experimental.numpy.allclose(grad1, grad2) # AssertionError
```


### Relevant log output

```shell
input:  tf.Tensor(
[[[0.+5.35j]
  [0.+6.61j]]], shape=(1, 2, 1), dtype=complex64)
tf.Tensor(
[[[0.5+1.4249809j]
  [0.5+1.7798613j]]], shape=(1, 2, 1), dtype=complex64)
tf.Tensor(
[[[0.5-1.4249809j]
  [0.5-1.7798615j]]], shape=(1, 2, 1), dtype=complex64)
AssertionError
```
</details>"
57356,TF golang client memory leak,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 1 and tf 2

### Custom Code

Yes

### OS Platform and Distribution

ubuntu20.04

### Python version

3.8

### GCC/Compiler version

9.3.0

### CUDA/cuDNN version

11.4

### Current Behaviour?

```shell
Action1: new a tensor and use the only one tensor to do session run will not result in memory raise.
Action2: new a tensor every time before seesion run will lead to memory rasie.

Conclusion: NewTensor api lead to memory leak

Suggestion: Release tensor obvisly instead of just set finializer
```


### Standalone code to reproduce the issue

```shell
package main

import (
	""flag""
	""fmt""
	""io/ioutil""
	""log""

	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
)

func main() {
	// An example for using the TensorFlow Go API for image recognition
	// using a pre-trained inception model (http://arxiv.org/abs/1512.00567).
	//
	// Sample usage: <program> -dir=/tmp/modeldir -image=/path/to/some/jpeg
	//
	// The pre-trained model takes input in the form of a 4-dimensional
	// tensor with shape [ BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH, 3 ],
	// where:
	// - BATCH_SIZE allows for inference of multiple images in one pass through the graph
	// - IMAGE_HEIGHT is the height of the images on which the model was trained
	// - IMAGE_WIDTH is the width of the images on which the model was trained
	// - 3 is the (R, G, B) values of the pixel colors represented as a float.
	//
	// And produces as output a vector with shape [ NUM_LABELS ].
	// output[i] is the probability that the input image was recognized as
	// having the i-th label.
	//
	// A separate file contains a list of string labels corresponding to the
	// integer indices of the output.
	//
	// This example:
	// - Loads the serialized representation of the pre-trained model into a Graph
	// - Creates a Session to execute operations on the Graph
	// - Converts an image file to a Tensor to provide as input to a Session run
	// - Executes the Session and prints out the label with the highest probability
	//
	// To convert an image file to a Tensor suitable for input to the Inception model,
	// this example:
	// - Constructs another TensorFlow graph to normalize the image into a
	//   form suitable for the model (for example, resizing the image)
	// - Creates and executes a Session to obtain a Tensor in this normalized form.
	modeldir := flag.String(""dir"", """", ""Directory containing the trained model files. The directory will be created and the model downloaded into it if necessary"")
	//imagefile := flag.String(""image"", """", ""Path of a JPEG-image to extract labels for"")
	flag.Parse()
	if *modeldir == """" {
		flag.Usage()
		return
	}
	// Load the serialized GraphDef from a file.
	// modelfile, labelsfile, err := modelFiles(*modeldir)
	//if err != nil {
	//	log.Fatal(err)
	//}
	model, err := ioutil.ReadFile(*modeldir)
	if err != nil {
		log.Fatal(err)
	}

	// Construct an in-memory graph from the serialized form.
	graph := tf.NewGraph()
	if err := graph.Import(model, """"); err != nil {
		log.Fatal(err)
	}

	// Create a session for inference over graph.
	session, err := tf.NewSession(graph, nil)
	if err != nil {
		log.Fatal(err)
	}
	defer session.Close()

	// Run inference on *imageFile.
	// For multiple images, session.Run() can be called in a loop (and
	// concurrently). Alternatively, images can be batched since the model
	// accepts batches of image data as input.
	//tensor, err := makeTensorFromImage(*imagefile)
	//if err != nil {
	//	log.Fatal(err)
	//}

	inputs := make([]string, 0)
	for i := 0; i< 8;i++ {
		inputs = append(inputs, ""测试文本输入"")
	}

	tensor, err := tf.NewTensor(inputs)
	if err != nil {
		log.Fatal(err)
	}
	for i := 0; i < 1000000; i++ {
		output, err := session.Run(
			map[tf.Output]*tf.Tensor{
				graph.Operation(""input_tensor_name"").Output(0): tensor,
			},
			[]tf.Output{
				graph.Operation(""output_tensor_name"").Output(0),
			},
			nil)
		if err != nil {
			log.Fatal(err)
		}
		probabilities := output[0].Value().([][]float32)[0]
		if i % 1000 == 0 {
			fmt.Println(""repeat "", probabilities)
		}
	}
   	// output[0].Value() is a vector containing probabilities of
	// labels for each image in the ""batch"". The batch size was 1.
	// Find the most probably label index.
	//printBestLabel(probabilities, labelsfile)
}
```"
57355,model.summary   TF 2.9.1,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

win10

### Mobile device

_No response_

### Python version

3.8.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
model = tf.keras.Sequential(name='M')
model.add( tf.keras.Input(shape=(100, 10), name=""input_arr"") )
model.add( tf.keras.layers.Flatten() )
model.add( tf.keras.layers.Dense(3, activation='softmax') )

model.summary()


missing input layer
name=""input_arr""
Only the following layers are shown

Model: ""M""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten_28 (Flatten)        (None, 1000)              0         
                                                                 
 dense_81 (Dense)            (None, 3)                 3003      
                                                                 
=================================================================
Total params: 3,003
Trainable params: 3,003
Non-trainable params: 0
_________________________________________________________________
```


### Standalone code to reproduce the issue

```shell
model.summary()  missing input layer
name=""input_arr""
Only the following layers are shown


model = tf.keras.Sequential(name='M')
model.add( tf.keras.Input(shape=(100, 10), name=""input_arr"") )
model.add( tf.keras.layers.Flatten() )
model.add( tf.keras.layers.Dense(3, activation='softmax') )

model.summary()




Model: ""M""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten_28 (Flatten)        (None, 1000)              0         
                                                                 
 dense_81 (Dense)            (None, 3)                 3003      
                                                                 
=================================================================
Total params: 3,003
Trainable params: 3,003
Non-trainable params: 0
_________________________________________________________________
```


### Relevant log output

_No response_</details>"
57354,0 derived errors ignored. [Op:__inference_test_function_5397]  Function call stack: test_function -> test_function,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.4.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.4

### GPU model and memory

NVIDIA GeForce RTX 2080 Ti

### Current Behaviour?

```shell
I am facing an issue while trying to run my model, it seems to be an unknown error with no direct understanding of what is actually causing the error. Some answers talk about increasing batch size or adding a block of code before the model training. The training suddenely stops after a few epochs which is not supposed to happen. Instead, the model will just stop through a callback after its successful training.
```


### Standalone code to reproduce the issue

```shell
feature_layers = [
    Conv2D(32, (3, 3), input_shape=(400, 400, 3,), padding='same'),
    LeakyReLU(alpha=0.1),
    # model_4.add(RandomFlip(mode='horizontal_and_vertical', seed=None))
    BatchNormalization(),
    MaxPool2D(pool_size=(2, 2), padding='same'),

    Conv2D(32, (3, 3), padding='same'),
    LeakyReLU(alpha=0.1),
    BatchNormalization(),
    MaxPool2D(pool_size=(2, 2), padding='same'),

    Conv2D(64, (3, 3), padding='same'),
    LeakyReLU(alpha=0.1),
    BatchNormalization(),
    MaxPool2D(pool_size=(2, 2), padding='same'),

    Conv2D(64, (3, 3), padding='same'),
    LeakyReLU(alpha=0.1),
    BatchNormalization(),
    MaxPool2D(pool_size=(2, 2), padding='same'),

    Conv2D(64, (3, 3), padding='same'),
    LeakyReLU(alpha=0.1),
    BatchNormalization(),
    MaxPool2D(pool_size=(2, 2), padding='same'),

    Conv2D(64, (3, 3), padding='same'),
    LeakyReLU(alpha=0.1),
    BatchNormalization(),
    MaxPool2D(pool_size=(2, 2), padding='same'),

    Conv2D(128, (3, 3), padding='same'),
    LeakyReLU(alpha=0.1),
    BatchNormalization(),
    MaxPool2D(pool_size=(2, 2), padding='same'),

    Conv2D(128, (3, 3), padding='same'),
    LeakyReLU(alpha=0.1),
    BatchNormalization(),
    MaxPool2D(pool_size=(2, 2), padding='same'),

    Conv2D(128, (3, 3), padding='same'),
    LeakyReLU(alpha=0.1),
    BatchNormalization(),
    MaxPool2D(pool_size=(2, 2), padding='same'),

    Conv2D(128, (3, 3), padding='same'),
    LeakyReLU(alpha=0.1),
    BatchNormalization(),
    MaxPool2D(pool_size=(2, 2), padding='same'),

    Conv2D(256, (3, 3), padding='same'),
    LeakyReLU(alpha=0.1),
    BatchNormalization(),
    MaxPool2D(pool_size=(2, 2), padding='same'),

    Conv2D(256, (3, 3), padding='same'),
    LeakyReLU(alpha=0.1),
    BatchNormalization(),
    MaxPool2D(pool_size=(2, 2), padding='same'),

    Conv2D(256, (3, 3), padding='same'),
    LeakyReLU(alpha=0.1),
    MaxPool2D(pool_size=(2, 2), padding='same'),
]

classification_layers = [
    Flatten(),
    Dropout(0.35),
    Dense(512, kernel_regularizer=regularizers.l2(0.1)),
    LeakyReLU(alpha=0.1),
    Dropout(0.35),
    Dense(4),
]

model = Sequential(feature_layers + classification_layers, name='Stenosis_model')

model.compile(loss='mse', optimizer=Adam(learning_rate=0.0001), metrics=[tf.keras.metrics.MeanIoU(num_classes=4), tfr.keras.metrics.MeanAveragePrecisionMetric()])

model.summary()

keras.backend.clear_session()
np.random.seed(15)
tf.random.set_seed(15)


model.fit(dataGenerator.flow(train_images, train_targets, batch_size=16),
                            validation_data=(val_images, val_targets), 
                            epochs=150, callbacks=[es], verbose=1, shuffle=True)
```
I have also added the following code before running the model:

```
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)
```
```


### Relevant log output

```shell
InvalidArgumentError                      Traceback (most recent call last)
/home/lunet/conm/Desktop/Stenosis-Project/transfer_learning_model.ipynb Cell 19 in <cell line: 6>()
      2 np.random.seed(15)
      3 tf.random.set_seed(15)
----> 6 model.fit(dataGenerator.flow(train_images, train_targets, batch_size=16),
      7                             validation_data=(val_images, val_targets), 
      8                             epochs=150, callbacks=[es], verbose=1, shuffle=True)

File ~/.conda/envs/stenosis/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1131, in Model.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1117   self._fit_frame = tf_inspect.currentframe()
   1118   self._eval_data_handler = data_adapter.DataHandler(
   1119       x=val_x,
   1120       y=val_y,
   (...)
   1129       model=self,
   1130       steps_per_execution=self._steps_per_execution)
-> 1131 val_logs = self.evaluate(
   1132     x=val_x,
   1133     y=val_y,
   1134     sample_weight=val_sample_weight,
   1135     batch_size=validation_batch_size or batch_size,
   1136     steps=validation_steps,
   1137     callbacks=callbacks,
   1138     max_queue_size=max_queue_size,
   1139     workers=workers,
   1140     use_multiprocessing=use_multiprocessing,
   1141     return_dict=True)
   1142 val_logs = {'val_' + name: val for name, val in val_logs.items()}
   1143 epoch_logs.update(val_logs)

File ~/.conda/envs/stenosis/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1389, in Model.evaluate(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)
   1387 with trace.Trace('test', step_num=step, _r=1):
   1388   callbacks.on_test_batch_begin(step)
-> 1389   tmp_logs = self.test_function(iterator)
   1390   if data_handler.should_sync:
   1391     context.async_wait()

File ~/.conda/envs/stenosis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:828, in Function.__call__(self, *args, **kwds)
    826 tracing_count = self.experimental_get_tracing_count()
    827 with trace.Trace(self._name) as tm:
--> 828   result = self._call(*args, **kwds)
    829   compiler = ""xla"" if self._experimental_compile else ""nonXla""
    830   new_tracing_count = self.experimental_get_tracing_count()

File ~/.conda/envs/stenosis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:862, in Function._call(self, *args, **kwds)
    859 self._lock.release()
    860 # In this case we have not created variables on the first call. So we can
    861 # run the first trace but we should fail if variables are created.
--> 862 results = self._stateful_fn(*args, **kwds)
    863 if self._created_variables:
    864   raise ValueError(""Creating variables on a non-first call to a function""
    865                    "" decorated with tf.function."")

File ~/.conda/envs/stenosis/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2942, in Function.__call__(self, *args, **kwargs)
   2939 with self._lock:
   2940   (graph_function,
   2941    filtered_flat_args) = self._maybe_define_function(args, kwargs)
-> 2942 return graph_function._call_flat(
   2943     filtered_flat_args, captured_inputs=graph_function.captured_inputs)

File ~/.conda/envs/stenosis/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1918, in ConcreteFunction._call_flat(self, args, captured_inputs, cancellation_manager)
   1914 possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)
   1915 if (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE
   1916     and executing_eagerly):
   1917   # No tape is watching; skip to running the function.
-> 1918   return self._build_call_outputs(self._inference_function.call(
   1919       ctx, args, cancellation_manager=cancellation_manager))
   1920 forward_backward = self._select_forward_and_backward_functions(
   1921     args,
   1922     possible_gradient_type,
   1923     executing_eagerly)
   1924 forward_function, args_with_tangents = forward_backward.forward()

File ~/.conda/envs/stenosis/lib/python3.8/site-packages/tensorflow/python/eager/function.py:555, in _EagerDefinedFunction.call(self, ctx, args, cancellation_manager)
...
0 derived errors ignored. [Op:__inference_test_function_5397]

Function call stack:
test_function -> test_function
```
</details>"
57353,tf.image.ssim produces nan gradients when utilized as a loss function,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.3/8.6

### GPU model and memory

Tesla V100

### Current Behaviour?

```shell
tf.image.ssim is intended to be used as an evaluation metric as well as a loss function.
Although normally the code works great, from time to time it produces nan gradients: this was found after ~60+ hours of debugging.
This seems to occur when certain values are too small: even Squared values can cause nan errors (more common with OneDNN optimizations enabled thanks to float errors.)

Here's our implementation of SSIM loss below that solved the nan issue.
```


### Standalone code to reproduce the issue

```shell
Nan inducing code:
 tf.GradientTape(watch_accessed_variables=False) as t:
    t.watch(self.gen_model.trainable_variables)
    gen_loss=tf.image.ssim(real,fake)
g_grads=t.gradient(gen_loss,self.gen_model.trainable_variables)

Our implementation that isn't 100% accurate but stops any nan gradients:
```
    def ssim_loss(self, img1, img2):
        window = self._tf_fspecial_gauss(size=self.WS)  
        (_, _, _, channel) = img1.shape.as_list()
        window = tf.tile(window, [1, 1, channel, 1])
        mu1 = tf.nn.depthwise_conv2d(img1, window, strides = [1, 1, 1, 1], padding = 'VALID')+1e-8
        mu2 = tf.nn.depthwise_conv2d(img2, window, strides = [1, 1, 1, 1], padding = 'VALID')
        mu1_sq = mu1 * mu1+1e-8
        mu2_sq = mu2 * mu2
        mu1_mu2 = mu1 * mu2+1e-8
        img1_2 = img1*img1+1e-8
        sigma1_sq = tf.subtract(tf.nn.depthwise_conv2d(img1_2, window, strides = [1 ,1, 1, 1], padding = 'VALID') , mu1_sq)+1e-8
        img2_2 = img2*img2
        sigma2_sq = tf.subtract(tf.nn.depthwise_conv2d(img2_2, window, strides = [1, 1, 1, 1], padding = 'VALID') ,mu2_sq)
        img12_2 = img1*img2+1e-8
        sigma1_2 = tf.subtract(tf.nn.depthwise_conv2d(img12_2, window, strides = [1, 1, 1, 1], padding = 'VALID') , mu1_mu2)+1e-8
        c1 = (self.k1*self.L)**2+1e-8
        c2 = (self.k2*self.L)**2
        ssim_map = ((2*mu1_mu2 + c1)*(2*sigma1_2 + c2)) / ((mu1_sq + mu2_sq + c1)*(sigma1_sq + sigma2_sq + c2))+1e-8
        return tf.reduce_mean(ssim_map)
```
(credits where credits due, can't remember where we borrowed the impl from.)
```


### Relevant log output

```shell
https://media.discordapp.net/attachments/769691447925669928/1009016713733480469/unknown.png
```
</details>"
57343,Computing `jacobian` or `batch_jacobian` with `experimental_pfor=True` errors when loading a `tf.saved_model` that uses an LSTM layer,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.8.2

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
An error occurs when using `experimental_use_pfor=True` for both `tf.GradientTape.jacobian` and `tf.GradientTape.batch_jacobian` on a `tf.saved_model` that uses an LSTM layer. This error does not occur when `experimental_use_pfor=False` or when the layer is something other than LSTM.

I would expect that this computation could happen with `experimental_use_pfor=True` regardless of the choice of layers in the model or using a saved_model.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1vIjQ21ixCoJ-MheucKKtog6jcMr-JHwZ?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py in jacobian(self, target, sources, unconnected_gradients, parallel_iterations, experimental_use_pfor)
   1182         output = pfor_ops.pfor(loop_fn, target_size,
-> 1183                                parallel_iterations=parallel_iterations)
   1184       except ValueError as err:

9 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py in pfor(loop_fn, iters, fallback_to_while_loop, parallel_iterations)
    201     f = def_function.function(f)
--> 202   outputs = f()
    203   if functions_run_eagerly is not None:

/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)
   1146             if hasattr(e, ""ag_error_metadata""):
-> 1147               raise e.ag_error_metadata.to_exception(e)
   1148             else:

ValueError: in user code:

    File ""/usr/local/lib/python3.7/dist-packages/six.py"", line 703, in reraise
        raise value

    ValueError: Required handle data not set for <tf.Tensor 'gradients/while_grad/gradients/grad_ys_3/pfor/Identity:0' shape=() dtype=variant>


During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-24-f6e4f092a8b4> in <module>
     14     pred = my_model(x)
     15 
---> 16 j = tape.jacobian(pred, x, experimental_use_pfor=True)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py in jacobian(self, target, sources, unconnected_gradients, parallel_iterations, experimental_use_pfor)
   1189                 ""jacobian computation. Vectorization can be disabled by setting""
   1190                 "" experimental_use_pfor to False.""),
-> 1191             sys.exc_info()[2])
   1192     else:
   1193       if context.executing_eagerly() and not self._persistent:

/usr/local/lib/python3.7/dist-packages/six.py in reraise(tp, value, tb)
    700                 value = tp()
    701             if value.__traceback__ is not tb:
--> 702                 raise value.with_traceback(tb)
    703             raise value
    704         finally:

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py in jacobian(self, target, sources, unconnected_gradients, parallel_iterations, experimental_use_pfor)
   1181       try:
   1182         output = pfor_ops.pfor(loop_fn, target_size,
-> 1183                                parallel_iterations=parallel_iterations)
   1184       except ValueError as err:
   1185         six.reraise(

/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py in pfor(loop_fn, iters, fallback_to_while_loop, parallel_iterations)
    200       def_function.run_functions_eagerly(False)
    201     f = def_function.function(f)
--> 202   outputs = f()
    203   if functions_run_eagerly is not None:
    204     def_function.run_functions_eagerly(functions_run_eagerly)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)
   1145           except Exception as e:  # pylint:disable=broad-except
   1146             if hasattr(e, ""ag_error_metadata""):
-> 1147               raise e.ag_error_metadata.to_exception(e)
   1148             else:
   1149               raise

ValueError: in user code:

    File ""/usr/local/lib/python3.7/dist-packages/six.py"", line 703, in reraise
        raise value

    ValueError: Required handle data not set for <tf.Tensor 'gradients/while_grad/gradients/grad_ys_3/pfor/Identity:0' shape=() dtype=variant>

Encountered an exception while vectorizing the jacobian computation. Vectorization can be disabled by setting experimental_use_pfor to False.
```
</details>"
57251,ERROR: Current implementation only supports equal length strides in the r.w and column dimensions.,"is there any plan to support this?

```
In [1]: import tensorflow as tf

In [2]: input_shape = (4, 256, 256, 3)

In [3]:  x = tf.random.normal(input_shape)

In [4]: y=tf.keras.layers.SeparableConv2D(2, 3, strides=(1,2),activation='relu', input_shape=input_shape[1:])(x)
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Input In [4], in <cell line: 1>()
----> 1 y=tf.keras.layers.SeparableConv2D(2, 3, strides=(1,2),activation='relu', input_shape=input_shape[1:])(x)

File ~/miniforge3/envs/tfn/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     65 except Exception as e:  # pylint: disable=broad-except
     66   filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67   raise e.with_traceback(filtered_tb) from None
     68 finally:
     69   del filtered_tb

File ~/miniforge3/envs/tfn/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7164, in raise_from_not_ok_status(e, name)
   7162 def raise_from_not_ok_status(e, name):
   7163   e.message += ("" name: "" + name if name is not None else """")
-> 7164   raise core._status_to_exception(e) from None

InvalidArgumentError: Exception encountered when calling layer ""separable_conv2d"" (type SeparableConv2D).

Current implementation only supports equal length strides in the row and column dimensions. [Op:DepthwiseConv2dNative] name: depthwise

Call arguments received by layer ""separable_conv2d"" (type SeparableConv2D):
  • inputs=tf.Tensor(shape=(4, 256, 256, 3), dtype=float32)

In [6]: tf.__version__
Out[6]: '2.9.2'
```

_Originally posted by @alexmil2019 in https://github.com/tensorflow/tensorflow/issues/33005#issuecomment-1176697401_"
57250,ERROR: Current implementation only supports equal length strides in the r.w and column dimensions!is there any plan to support this?,"is there any plan to support this?

```
In [1]: import tensorflow as tf

In [2]: input_shape = (4, 256, 256, 3)

In [3]:  x = tf.random.normal(input_shape)

In [4]: y=tf.keras.layers.SeparableConv2D(2, 3, strides=(1,2),activation='relu', input_shape=input_shape[1:])(x)
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Input In [4], in <cell line: 1>()
----> 1 y=tf.keras.layers.SeparableConv2D(2, 3, strides=(1,2),activation='relu', input_shape=input_shape[1:])(x)

File ~/miniforge3/envs/tfn/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     65 except Exception as e:  # pylint: disable=broad-except
     66   filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67   raise e.with_traceback(filtered_tb) from None
     68 finally:
     69   del filtered_tb

File ~/miniforge3/envs/tfn/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7164, in raise_from_not_ok_status(e, name)
   7162 def raise_from_not_ok_status(e, name):
   7163   e.message += ("" name: "" + name if name is not None else """")
-> 7164   raise core._status_to_exception(e) from None

InvalidArgumentError: Exception encountered when calling layer ""separable_conv2d"" (type SeparableConv2D).

Current implementation only supports equal length strides in the row and column dimensions. [Op:DepthwiseConv2dNative] name: depthwise

Call arguments received by layer ""separable_conv2d"" (type SeparableConv2D):
  • inputs=tf.Tensor(shape=(4, 256, 256, 3), dtype=float32)

In [6]: tf.__version__
Out[6]: '2.9.2'
```

_Originally posted by @alexmil2019 in https://github.com/tensorflow/tensorflow/issues/33005#issuecomment-1176697401_"
57228,tf.distribute.MirroredStrategy for asynchronous training,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Tensorflow Version

2.8.1

### Python version

3.8.13

### CUDA/cuDNN version

11.8

### Use Case

I need to run multiple asynchronous copies of the same model on different slices of the dataset (e.g. with bootstrap sampling). There's no *good* way to do this in keras api that I'm aware of, although a couple of hacks exist. Would this use case be feasible with tf.distribute?

### Feature Request

`tf.distribute.MirroredStrategy` is a synchronous, data parallel strategy for distributed training across multiple devices on a single host worker.

Would it be possible to modify this strategy to allow for asynchronous training of all model replicas, without computing the average gradient over all replicas to update weights? In this case each replica would need its own un-mirrored copy of model weights, and the update rule would depend only on the loss and gradients of each replica.

Thanks"
57226,tensorflow lite for microcontroller is slower then tensorflow lite for python.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

ubuntu20

### Python version

3.9


### Current Behaviour?

```shell
The model is runing on armv7.using tensorflow lite for microcontroller is slower than using tensorflow lite for python.My test result showing that the fullconnected layer consumes most of the computing power.
The input signals type of my model are float32 while the weights type of my model are int8.
In order to make the micro version support the hybrid model, I made some code changes.
I want to know how can I make the fullyconnected layer of micro version have the same efficiency as the fullyconnected layer of the python version.
please give me some advice.
```


### Standalone code to reproduce the issue

```shell
inline void FullyConnected(
    const FullyConnectedParams& params, const RuntimeShape& input_shape,
    const float* input_data, 			const RuntimeShape& weights_shape,
    const int8_t* weights_data, 		const RuntimeShape& bias_shape,
    const float* bias_data, 			const RuntimeShape& output_shape,
    float* output_data,    				const TfLiteTensor* filter )
{
	float output_activation_min = params.float_activation_min;
	float output_activation_max = params.float_activation_max;
	output_activation_min =output_activation_min + 0;
	output_activation_max =output_activation_max + 0;
	// TODO(b/62193649): This really should be:
	//     const int batches = ArraySize(output_dims, 1);
	// but the current --variable_batch hack consists in overwriting the 3rd
	// dimension with the runtime batch size, as we don't keep track for each
	// array of which dimension is the batch dimension in it.
	const int output_dims_count = output_shape.DimensionsCount();
	const int weights_dims_count = weights_shape.DimensionsCount();
	const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);
	const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,
					   output_shape, output_dims_count - 1);
	const int accum_depth = weights_shape.Dims(weights_dims_count - 1);


	const float filter_scale = static_cast<float>(filter->params.scale);

	for (unsigned short b = 0; b < batches; ++b)						
	{
		unsigned short index1 = b * accum_depth;
		unsigned short index3 = output_depth * b;
		for (unsigned short out_c = 0; out_c < output_depth; ++out_c)		
		{
			float total = 0.f;
			int index2 = out_c * accum_depth;
			for (unsigned short d = 0; d < accum_depth; ++d)		
			{
				total += input_data[index1 + d] * weights_data[index2 + d];
			}
			total = total * filter_scale;
			float bias_value = 0.0f;
			if (bias_data)
			{
				bias_value = bias_data[out_c];
			}
			bias_value = bias_value + 0;
			output_data[out_c + index3] = ActivationFunctionWithMinMax(total + bias_value, output_activation_min, output_activation_max);
		}
	}
}
```


### Relevant log output

```shell
python version runtime: 2.6ms
micro c++ version runtime: 7ms
```
</details>"
57225,How to calculate number of MAC's in a custom tensorflow model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.7

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.4

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

NVIDIA RTX A6000

### Current Behaviour?

```shell

Hello 

I have trained one custom tensorflow model using Conv2D's, Batch Normalization and GRU's, i want to know any tool is available in tensorflow for calculating no.of MAC's


Please suggest any tool / library for calculating no.of MACS/FLOPS required for one second for any custom tensorflow model
```


### Standalone code to reproduce the issue

```shell
Any tool or library to calculate no.of macs's required for 1 second?


Please suggest any tool / library for calculating no.of MACS/FLOPS required for one second for any custom tensorflow model
```


### Relevant log output

_No response_</details>"
57222,Saving and loading with an empty model layer,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.8.0-rc1-32-g3f878cff5b6 2.8.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When trying to load a saved keras model containing an empty sequential model, an exception is thrown.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

class TestBlock(tf.keras.Model):
  def __init__(self):
    super().__init__()
    self.shortcut = tf.keras.Sequential()
    self.dense = tf.keras.layers.Dense(5)
    self.relu = tf.keras.layers.ReLU()

  def call(self, input_vector):
    shortcut = self.shortcut(input_vector)
    dense = self.dense(input_vector)
    return self.relu(shortcut + dense)

if __name__ == ""__main__"":
  model = TestBlock()
  model.build((1,5))
  model.compile(loss=""categorical_crossentropy"")
  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=""checkpoints"",
    save_best_only=True,
    mode=""auto"",
  )
  train_data = tf.data.Dataset.from_tensor_slices((np.ones((20,5)), np.zeros((20,5)))).batch(1)
  model.fit(train_data,
    epochs=1,
    callbacks=[checkpoint_callback],
  )
  print(""Saving ..."")
  model.save(""test_model"")
  print(""Loading ..."")
  loaded_model = tf.keras.models.load_model(""test_model"")
```


### Relevant log output

```shell
2022-08-19 12:17:00.131391: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in pe
rformance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-08-19 12:17:00.599398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6666 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 206
0 SUPER, pci bus id: 0000:29:00.0, compute capability: 7.5
20/20 [==============================] - 0s 1ms/step - loss: 0.0000e+00ING:tensorflow:Can save best model only with val_loss available, skipping.
Saving ...
2022-08-19 12:17:01.354210: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
Loading ...
Traceback (most recent call last):
  File ""test_seq_model.py"", line 34, in <module>
    loaded_model = tf.keras.models.load_model(""test_model"")
  File ""/xxxxx/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/xxxxx/keras/saving/saved_model/load.py"", line 700, in _reconstruct_model
    if config['layers'][0]['class_name'] == 'InputLayer':
IndexError: list index out of range
```
</details>"
57220,Restoring a saved model does not create the same results as the same model just before saving,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9.1.

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I trained a model composed of four elements to classify images of beers by their labels. The model consists of the parts
* Convert input data from float32 tensor to uint8 tensor as object detction layer expects uint8 tensor
* Perform object detection to obtain boxes, class labels and scores of detected objects
* Crop to the highest rated detection of a bottle and a glass. Resize and concat the bottle and the glass image. Tensor shape therefore changes from (batch_size, width, height, 3) to (batch_size, width, height, 6)
* Prediction head. Consists of a CNN and a Feedforward network

Only the prediction head layer is trained.

The file test_exported_model.py performs the test between the model at the end of the training referred to as trained model and the restored model.
The model at the end of the training is recreated by restoring only the weights of the prediction head layer.
The restored model is created by exporting the complete composed model and restoring the model from this saved model.
When a prediction is performed the restored model and the trained model would be expected to produce the same results.
To perform the test one can run `python test_exported_model.py`. However the results vary significantly, for example for image 000.jpg from 0chimayblue from the evaluation dataset:
* `trained model: [[9.7456181e-01 2.0527570e-04 1.9158632e-02 1.0962408e-03 4.9780323e-03]]`
* `restored model: [[0.00636891 0.9207034  0.00868604 0.00923812 0.05500359]]`

When passing the test image into the model the trained model creates a detection box tf.tensor of shape (1, 100, 4), the restored model creates a detection box Tensor with shape=(None, 100, 4).  

The respective outputs also get printed to the console when running `python test_exported_model.py`.
```


### Standalone code to reproduce the issue

```shell
I pushed the code to the following repository. 
`https://github.com/skerres/export_model_minimal_example`
```


### Relevant log output

```shell
2022-08-18 23:48:39.756943: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2022-08-18 23:48:41.042171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-18 23:48:41.070340: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/include:/usr/local/lib:/usr/local/cuda-10.1/targets/x86_64-linux/lib:/usr/local/cuda-11.0/targets/x86_64-linux/lib:/usr/include:/usr/local/lib:/usr/local/cuda-10.1/targets/x86_64-linux/lib:/usr/local/cuda-11.0/targets/x86_64-linux/lib
2022-08-18 23:48:41.070368: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2022-08-18 23:48:41.070669: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
================================================================================
boxes: tf.Tensor(
[[[3.94008160e-01 4.05479813e+01 6.24505920e+01 6.73040085e+01]
  [6.03947639e-01 7.00556107e+01 6.71987610e+01 9.62328339e+01]
...
  [6.28376102e+00 9.83677597e+01 1.64723148e+01 1.09213722e+02]
  [5.24477730e+01 9.82576141e+01 1.43205841e+02 1.59068436e+02]]], shape=(1, 100, 4), dtype=float32)
boxes.shape: (1, 100, 4)
WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 1048). These functions will not be directly callable after loading.
INFO:tensorflow:Assets written to: output/composed_model_2022-08-18-23-48-50/assets
INFO:tensorflow:Assets written to: output/composed_model_2022-08-18-23-48-50/assets
<keras.saving.saved_model.load.ComposedModel object at 0x7f37dc791c40>
================================================================================
boxes: Tensor(""keras_layer/StatefulPartitionedCall:0"", shape=(None, 100, 4), dtype=float32)
boxes.shape: (None, 100, 4)
trained model: [[9.7456181e-01 2.0527570e-04 1.9158632e-02 1.0962408e-03 4.9780323e-03]]
served model: [[0.00636891 0.9207034  0.00868604 0.00923812 0.05500359]]
```
</details>"
57218,Can't build for iOS using CMake,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.x

### Custom Code

No

### OS Platform and Distribution

Any

### Mobile device

iOS

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
During linking, symbols are missing due to signpost_profiler.mm not being compiled when building for iOS using CMake. Patch will be provided.
```


### Standalone code to reproduce the issue

```shell
Use an iOS toolchain such as https://github.com/leetal/ios-cmake to build for iOS. Will fail due to missing symbols since signpost_profiler.mm is not compiled.
```


### Relevant log output

_No response_</details>"
57215,testTFLiteGraphDef and similar tests in lite_test.py fail on s390x arch,"### 1. System information

- OS Platform and Distribution: Ubuntu 20.04
- TensorFlow installation: Built from source
- TensorFlow library : 2.9.1

### 2. Code

bazel test -s --config=dbg --define=tensorflow_mkldnn_contraction_kernel=0 --define tflite_with_xnnpack=false --test_tag_filters=-gpu,-benchmark-test,-v1only,-no_oss,-oss_serial -k --test_timeout 300,450,1200,3600 --build_tests_only --test_output=errors //tensorflow/lite/python:lite_test

### 3. Problem
Tensorflow Lite test (e.g., `testTFLiteGraphDef`) uses `ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz` FrozenGraph model [here](https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/lite/python/lite_test.py#L1873) generated on little-endian arch.  This causes problems during import of the graph on `s390x` arch and the process fails with this error:

```
ERROR: testTFLiteGraphDef (__main__.FromFrozenGraphObjectDetection)
FromFrozenGraphObjectDetection.testTFLiteGraphDef
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/.cache/bazel/_bazel_rishi/8f98aac813e58b2da3802c4fdafe023a/execroot/org_tensorflow/bazel-out/s390x-dbg/bin/tensorflow/lite/python/lite_test.runfiles/org_tensorflow/tensorflow/python/framework/importer.py"", line 499, in _import_graph_def_internal
    results = c_api.TF_GraphImportGraphDefWithResults(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot reshape a tensor with 32 elements to shape [536870912,16777216] (9007199254740992 elements) for '{{node FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/BatchNorm_Fold/scale_reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/BatchNorm_Fold/mul, FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/BatchNorm_Fold/scale_reshape/shape)' with input shapes: [32], [2] and with input tensors computed as partial shapes: input[1] = [536870912,16777216].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/.cache/bazel/_bazel_rishi/8f98aac813e58b2da3802c4fdafe023a/execroot/org_tensorflow/bazel-out/s390x-dbg/bin/tensorflow/lite/python/lite_test.runfiles/org_tensorflow/tensorflow/lite/python/lite_test.py"", line 1899, in testTFLiteGraphDef
    converter = lite.TFLiteConverter.from_frozen_graph(self._graph_def_file,
  File ""/home/.cache/bazel/_bazel_rishi/8f98aac813e58b2da3802c4fdafe023a/execroot/org_tensorflow/bazel-out/s390x-dbg/bin/tensorflow/lite/python/lite_test.runfiles/org_tensorflow/tensorflow/lite/python/lite.py"", line 2674, in from_frozen_graph
    _import_graph_def(graph_def, name="""")
  File ""/home/.cache/bazel/_bazel_rishi/8f98aac813e58b2da3802c4fdafe023a/execroot/org_tensorflow/bazel-out/s390x-dbg/bin/tensorflow/lite/python/lite_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py"", line 561, in new_func
    return func(*args, **kwargs)
  File ""/home/.cache/bazel/_bazel_rishi/8f98aac813e58b2da3802c4fdafe023a/execroot/org_tensorflow/bazel-out/s390x-dbg/bin/tensorflow/lite/python/lite_test.runfiles/org_tensorflow/tensorflow/python/framework/importer.py"", line 403, in import_graph_def
    return _import_graph_def_internal(
  File ""/home/.cache/bazel/_bazel_rishi/8f98aac813e58b2da3802c4fdafe023a/execroot/org_tensorflow/bazel-out/s390x-dbg/bin/tensorflow/lite/python/lite_test.runfiles/org_tensorflow/tensorflow/python/framework/importer.py"", line 504, in _import_graph_def_internal
    raise ValueError(str(e))
ValueError: Cannot reshape a tensor with 32 elements to shape [536870912,16777216] (9007199254740992 elements) for '{{node FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/BatchNorm_Fold/scale_reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/BatchNorm_Fold/mul, FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/BatchNorm_Fold/scale_reshape/shape)' with input shapes: [32], [2] and with input tensors computed as partial shapes: input[1] = [536870912,16777216].

```

After examining associated `pbtxt` file I noticed that `tensor_content` fields of the model may have contents that are in little-endian order. While a simple change to byte-swapping tensor_content prior to [MakeNode](https://github.com/tensorflow/tensorflow/blob/d8ce9f9c301d021a69953134185ab728c1c248d3/tensorflow/core/common_runtime/graph_constructor.cc#L1250) fixes the immedidate issue, subsequent calls to convert function fails with similar error.

I wanted to generate this model on `s390x` to see if there is a way around this issue. From my understanding frozen inference graphs are generated using the v1.1x release version of TensorFlow. Following the [instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md) I tried to generate inference graph on `TF 2.9.1` but model generation failed with following error:

```
host:/home/tests/models/research# python ./object_detection/export_inference_graph.py     --input_type=${INPUT_TYPE}     --pipeline_config_path=${PIPELINE_CONFIG_PATH}     --trained_checkpoint_prefix=${TRAINED_CKPT_PREFIX}     --output_directory=${EXPORT_DIR}
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn(""urllib3 ({}) or chardet ({}) doesn't match a supported ""
Traceback (most recent call last):
  File ""./object_detection/export_inference_graph.py"", line 205, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""./object_detection/export_inference_graph.py"", line 193, in main
    exporter.export_inference_graph(
  File ""/home/tests/models/research/object_detection/exporter.py"", line 611, in export_inference_graph
    detection_model = model_builder.build(pipeline_config.model,
  File ""/home/tests/models/research/object_detection/builders/model_builder.py"", line 1252, in build
    return build_func(getattr(model_config, meta_architecture), is_training,
  File ""/home/tests/models/research/object_detection/builders/model_builder.py"", line 402, in _build_ssd_model
    _check_feature_extractor_exists(ssd_config.feature_extractor.type)
  File ""/home/tests/models/research/object_detection/builders/model_builder.py"", line 267, in _check_feature_extractor_exists
    raise ValueError(
ValueError: ssd_mobilenet_v1 is not supported for tf version 2. See `model_builder.py` for features extractors compatible with different versions of Tensorflow
```

Apparently, this model is no longer supported in `TF 2.x`.

I am wondering if this testcase makes sense on big-endian archs given that reference model can no longer be generated on `TF 2.x`. Are there other ways of regenerating it so it can be consumed on `big-endian` archs?  I have already tried converting it to `SavedModel` format to no avail.

Thanks for any input."
57214,Go API instrunctions no longer work,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04, in WSL2

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The first instruction for the Go API is below, and the error one gets, since Go works differently since a while now:


$ go get -d github.com/tensorflow/tensorflow/tensorflow/go
go: go.mod file not found in current directory or any parent directory.
        'go get' is no longer supported outside a module.
        To build and install a command, use 'go install' with a version,
        like 'go install example.com/cmd@latest'
        For more information, see https://golang.org/doc/go-get-install-deprecation
        or run 'go help get' or 'go help install'.
```

Are there any updated instructions on how to install the go package ?
```


### Standalone code to reproduce the issue

```shell
$ go get -d github.com/tensorflow/tensorflow/tensorflow/go
```


### Relevant log output

```shell
$ go get -d github.com/tensorflow/tensorflow/tensorflow/go
go: go.mod file not found in current directory or any parent directory.
        'go get' is no longer supported outside a module.
        To build and install a command, use 'go install' with a version,
        like 'go install example.com/cmd@latest'
        For more information, see https://golang.org/doc/go-get-install-deprecation
        or run 'go help get' or 'go help install'.
```
</details>"
57212,Can't pass random value (not tensor) in tensorflow function as a parameter.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.9.0

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I think I get unexpected behavior...

Stackoverflow source: https://stackoverflow.com/questions/73370248/passing-random-value-in-tensorflow-function-as-a-parameter/73372852#73372852

Tensorflow forum source: https://discuss.tensorflow.org/t/how-to-pass-random-value-not-tensor-in-tensorflow-function-as-a-parameter/11443

I have code in my augmentation tf.data pipeline...


    # BLURE
    filter_size = tf.random.uniform(shape=[], minval=0, maxval=5)
    image = tfa.image.mean_filter2d(image, filter_shape=filter_size)
```

But I'm constantly getting error...

```
 TypeError: The `filter_shape` argument must be a tuple of 2 integers. Received: Tensor(""filter_shape:0"", shape=(), dtype=int32)
```

I tried getting static value from random tensorflow like this...

```
    # BLURE
    filter_size = tf.get_static_value(tf.random.uniform(shape=[], minval=0, maxval=5))
    image = tfa.image.mean_filter2d(image, filter_shape=(filter_size, filter_size))
```

But I get error...

```
TypeError: The `filter_shape` argument must be a tuple of 2 integers. Received: None
```

I have tryed this also...

```
filter_size = tf.random.uniform(shape=[2], minval=0, maxval=5, dtype=tf.int32)
image = tfa.image.mean_filter2d(image, filter_shape=filter_size)
```
```
filter_size = tf.get_static_value(tf.random.uniform(shape=[], minval=0, maxval=5, dtype=tf.int32))
image = tfa.image.mean_filter2d(image, filter_shape=(filter_size, filter_size))
```

but I get...

```
TypeError: The `filter_shape` argument must be a tuple of 2 integers.
Received: Tensor(""filter_shape:0"", shape=(2,), dtype=int32)
```

And for the second…

```
ValueError: The `filter_shape` argument must be a tuple of 2 integers.
Received: (None, None) including element None of type <class 'NoneType'>
```
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow_addons as tfa
import numpy as np

def test(image):
	filter_size = tf.random.uniform(shape=[], minval=0, maxval=5, dtype=tf.int32)
	image = tfa.image.mean_filter2d(image, filter_shape=filter_size)
	return image

data = tf.data.Dataset.from_tensor_slices(tf.random.uniform([5, 100, 100, 3], 0, 255, dtype=np.int32))
data.map(test)
```


### Relevant log output

```shell
TypeError: The `filter_shape` argument must be a tuple of 2 integers. Received: Tensor(""filter_shape:0"", shape=(), dtype=int32)
```
</details>"
57211,openCL delegate issue with models that output results from their intermediate nodes,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1, nightly version

### Custom Code

No

### OS Platform and Distribution

Android

### Mobile device

tested on Snapdragon 888, 865, 855

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The openCL delegate generates all zero outputs in models that output results from their intermediate nodes. This issue does not happen with other delegates like XNNPACK.
```


### Standalone code to reproduce the issue

```shell
We have implemented a small tool (with comprehensive documentation) to reproduce the mentioned issue. Here is the link to the repository:

https://github.com/Bahar-BM/openCL_test
```


### Relevant log output

_No response_</details>"
57210,Hexagon delegate for Qualcomm snapdragon chip APQ8098 (835),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf2.7

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

Android 10

### Python version

3.6

### Bazel version

3.1.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Our product uses Qualcomm snapdragon chip 835 (APQ8098) with android 8.1. We ran our machine learning model on hexagon DSP using hexagon delegate library from Google:

v1.20.0.1

Recently, we updated our android from 8.1 to 10 and hexagon delegation stopped working (our model could only run on CPU now).

I have some question:

libhexagon_nn_skel_v66.so ==> for 'v66' based DSP
libhexagon_nn_skel_v65.so ==> for 'v65' based DSP
libhexagon_nn_skel.so ==> ??

Could you please confirm the right library to use for ADSP in APQ8098?
Our DSP version is v62 and the hexagon tools version are '8.0.08'.

Please advise how to fix this hexagon delegation issue. Thanks a lot.
```


### Standalone code to reproduce the issue

```shell
Our product uses Qualcomm snapdragon chip 835 (APQ8098) with android 8.1. We ran our machine learning model on hexagon DSP using hexagon delegate library from Google:

v1.20.0.1

Recently, we updated our android from 8.1 to 10 and hexagon delegation stopped working (our model could only run on CPU now).

I have some question:

libhexagon_nn_skel_v66.so ==> for 'v66' based DSP
libhexagon_nn_skel_v65.so ==> for 'v65' based DSP
libhexagon_nn_skel.so ==> ??

Could you please confirm the right library to use for ADSP in APQ8098?
Our DSP version is v62 and the hexagon tools version are '8.0.08'.

Please advise how to fix this hexagon delegation issue. Thanks a lot.
```


### Relevant log output

_No response_</details>"
57209,Hexagon delegate for Qualcomm snapdragon chip APQ8098 (835),"Our product uses Qualcomm snapdragon chip 835 (APQ8098) with android 8.1. We ran our machine learning model on hexagon DSP using hexagon delegate library from Google:

[v1.20.0.1](https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_v1.20.0.1.run)

Recently, we updated our android from 8.1 to 10 and hexagon delegation stopped working (our model could only run on CPU now).

I have some question:

libhexagon_nn_skel_v66.so ==> for 'v66' based DSP
libhexagon_nn_skel_v65.so ==> for 'v65' based DSP
libhexagon_nn_skel.so ==> ??

Could you please confirm the right library to use for ADSP in APQ8098?
Our DSP version is v62 and the hexagon tools version are '8.0.08'.

Please advise how to fix this hexagon delegation issue. Thanks a lot.

Regards,

David"
57208,TF Lite API Reference on Internet Page does not match Code for interpreter_options.h,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

Master Branch

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The tensorflow API reference for tf lite is showing a different name for a method for interpreter_options.h (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/interpreter_options.h).

The webpage shows the name ""SetDynamicAllocationForLargeTensors"" [https://www.tensorflow.org/lite/api_docs/cc/class/tflite/interpreter-options#setdynamicallocationforlargetensors] but the c++ code has the name ""OptimizeMemoryForLargeTensors"".

I expect that both names are equal. Either the on or the other.
```


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
57207,Bazel CI Failing for Tensorflow with Bazel@HEAD,"Tensorflow is impacted by an incompatible Bazel change: https://github.com/bazelbuild/continuous-integration/issues/1404

Tensorflow will need to migrate usages of `@bazel_tools//platforms` to `@platforms`, as described in the above bug.

Example breakage: https://buildkite.com/bazel/bazel-at-head-plus-downstream/builds/2590#0182aec8-7365-4995-81b0-a0494cafc3ea"
57206,analyzer_wrapper/model_analyzer.cc has typo causing C2001 error while compiling,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.0.0

### GCC/Compiler version

MSVC

### CUDA/cuDNN version

8.4.1

### GPU model and memory

GeForce GTX 750 Ti

### Current Behaviour?

```shell
While building, below error pop up

tensorflow/lite/python/analyzer_wrapper/model_analyzer.cc(197): error C2001: newline in constant

This is caused by non ascii quotes in the line 197 and 198.
```


### Standalone code to reproduce the issue

```shell
This is fixed by below patch. Please review.

diff --git a/tensorflow/lite/python/analyzer_wrapper/model_analyzer.cc b/tensorflow/lite/python/analyzer_wrapper/model_analyzer.cc
index 370a4eff470..d4609faab33 100644
--- a/tensorflow/lite/python/analyzer_wrapper/model_analyzer.cc
+++ b/tensorflow/lite/python/analyzer_wrapper/model_analyzer.cc
@@ -194,8 +194,8 @@ void dump_model_signature_defs(std::stringstream& out_stream,
     return;
   }
   out_stream << kSectionSplitter;
-  out_stream << ""Your TFLite model has ‘"" << signatures->Length()
-             << ""’ signature_def(s).\n\n"";
+  out_stream << ""Your TFLite model has '"" << signatures->Length()
+             << ""' signature_def(s).\n\n"";
   for (int i = 0; i < signatures->Length(); ++i) {
     auto* signature_def = signatures->Get(i);
     out_stream << ""Signature#"" << i << "" key: '""
```


### Relevant log output

_No response_</details>"
57205,It seems that `recompute_grad` is not saving GPU memory.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

conda `tensorflow-gpu` 2.10.0rc1 pypi_0 pypi
(I also tried: conda `tensorflow` 2.9.1   pypi_0    pypi)

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.5

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cudatoolkit  11.2.2   hbe64b41_10    conda-forge
cudnn          8.1.0.77   h90431f1_0    conda-forge


### GPU model and memory

NVIDIA GeForce RTX 3090 24GiB

### Current Behaviour?

(originally from this thread: https://github.com/davisyoshida/tf2-gradient-checkpointing/issues/1)

I cannot tell whether the APIs (either the official one `tf.recompute_grad`, or those from GitHub by [@pidajay](https://github.com/pidajay/tf2_gradient_checkpointing), [someone from google - @ppham27](https://github.com/tensorflow/tensorflow/issues/30418#issuecomment-719124842), [someone that crop tf-slim - @mathemakitten](https://github.com/tensorflow/tensorflow/issues/30418#issuecomment-657101055), etc) are really saving GPU memory as intended. This is because I don't know what's the right tool to tell the difference in the results.

These are my questions: (all under the assumption that tensorflow 2.x is used, not 1.x versions)

1. How to properly profile the GPU memory usage of tensorflow? I tried pidajay's method, where the unmaintained/buggy(negative memory in the report, e.g. [issue](https://github.com/pythonprofilers/memory_profiler/issues/317#issue-896895819)) [pythonprofilers/memory_profiler](https://github.com/pythonprofilers/memory_profiler) is used. From [a reply by the author](https://github.com/pythonprofilers/memory_profiler/issues/200#issuecomment-398581049), it seems that the Repo. is for profiling CPU memory only (and pidajay also said he/she hadn't done the experiment for GPU/TPU [in some PR for tensorflow](https://github.com/tensorflow/tensorflow/issues/38766#issue-604279368) - see the 2. of the link).
2. So instead, I decided to use tensorboard to profile my GPU memory usage. I followed the idea of [the ipynb-tutorial by pidajay](https://github.com/pidajay/tf2_gradient_checkpointing/blob/master/tf_recompute_grad_tutorial.ipynb) that for each block/segment(containing many existing `tf.keras.layers`) to be recomputed, it should be wrapped within a `tf.keras.Sequential` for the current `@tf.recompute_grad` API to work). Then I tried many Repo.(auhors and links mentioned above) on GitHub for the API part. The following is a pair of two different reports from thensorboard that seems to mean the official API is working, but I cannot confirm it:

    - Fig 1.: The `@tf.recompute_grad` is not applied. The report has more rows than Fig 2.
    - Fig 2.: The `@tf.recompute_grad` is applied. But I don't know whether those `Conv2DBackpropFilter` would represent those call signatures and parameters saved during the forward pass for the recomputation on the backward pass.

<img width=""1435"" alt=""image"" src=""https://user-images.githubusercontent.com/24765272/185365243-d9631f1d-0dbd-4914-8293-6470913d342a.png"">

![image](https://user-images.githubusercontent.com/24765272/185365341-7516611d-6336-49fd-bfec-5ab51b3d6cdb.png)


### Standalone code to reproduce the issue

In every experiment, I divided VGG11/13/16/19 into `Segment`s with the following code with/without the `tf.recompute_grad` wrapper.

```python
import tensorflow as tf


class Segment(tf.keras.layers.Layer):
    def __init__(self, ll, **kwargs):
        super().__init__(**kwargs)
        self._layers = tf.keras.Sequential()
        for l in (ll):
            if l == 'M':
                self._layers.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))
            else:
                self._layers.add(tf.keras.layers.Conv2D(l, kernel_size=3, padding='same'))
                self._layers.add(tf.keras.layers.BatchNormalization())
                self._layers.add(tf.keras.layers.ReLU())
        self._layers.add(tf.keras.layers.AveragePooling2D(pool_size=1, strides=1))

    def call(self, x):
        return tf.recompute_grad(self._layers)(x)
```

```python
import tensorflow as tf

from models.segment import Segment


config = {
    'vgg_new11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
    'vgg_new13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
    'vgg_new16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
    'vgg_new19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],
}



class VGG(tf.keras.Model):
    def __init__(self, vgg_name, num_classes):
        super().__init__()
        model = config[vgg_name]
        self.blocks = tf.keras.Sequential()
        _seg = []
        for i, l in enumerate(model):
            _seg.append(l)
            if (i+1) % 3 == 0:
                self.blocks.add(Segment(_seg))
                _seg = []
            if (i+1) == len(model) and len(_seg) > 0:
                self.blocks.add(Segment(_seg))

        self.flatten = tf.keras.layers.Flatten()
        self.fc = tf.keras.layers.Dense(num_classes, activation='softmax')

    def call(self, x, **kwargs):
        out = self.blocks(x)
        out = self.flatten(out)
        out = self.fc(out)
        return out
```


### Relevant log output

_No response_</details>"
57204,Metal performance shader topk not working with k>15,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.0

### Custom Code

No

### OS Platform and Distribution

macos 13

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Metal performance shader version of tf is not working properly with topk value bigger than 15. If for example we set 20 all values bigger than 15 are set to zero.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

x = np.random.rand(1,2400,1200).astype(np.float32)

a = tf.math.top_k(tf.convert_to_tensor(x), 20)[0]
b = tf.math.top_k(tf.convert_to_tensor(x), 15)[0]

with tf.device('/CPU:0'):
    acpu = tf.math.top_k(tf.convert_to_tensor(x), 20)[0]
    bcpu = tf.math.top_k(tf.convert_to_tensor(x), 15)[0]

    print(tf.experimental.numpy.allclose(b, bcpu))
    print(tf.experimental.numpy.allclose(a, acpu))
    # acpu have all proper top20 values a on mps have only top15 values and rest are zeros
```


### Relevant log output

_No response_</details>"
57203,TFlite Model not detecting any object,"I have trained a custom object detection model (Hand detection) using tflite_model_maker but it is not detecting hands in my android application.
When I try the detection in tensorflow's colab it detects hands but not in my android app.
I am using ml kit in android studio so it is not entering the onSuccessListener it is only entering onFailureLIstener.
See the below code snippet.
```
ObjectDetector objectDetector = ObjectDetection.getClient(options);
            objectDetector.process(image)
                        .addOnSuccessListener(
                                new OnSuccessListener<List<DetectedObject>>() {
                                    @Override
                                    public void onSuccess(List<DetectedObject> result) {
                                        Log.d(""TAG"", ""onSuccess: Object detected"");
                                        for(DetectedObject detectedObject: result){
                                            Rect box = detectedObject.getBoundingBox();
                                            StringBuilder builder = new StringBuilder();
                                            for(DetectedObject.Label label : detectedObject.getLabels()){
                                                String text = label.getText();
                                                builder.append(text);
                                                builder.append("" : "");
                                                int index = label.getIndex();
                                                float confidence = Math.round(label.getConfidence()*100);
                                                builder.append(confidence+""%"");
                                                builder.append(""\n"");
                                            }
                                            tv.setText(builder);
                                        }
                                        try {
                                            Toast.makeText(realTime.this, ""Detected objects:""+result.size(), Toast.LENGTH_SHORT).show();
                                        }
                                        catch (Exception e){
                                            Toast.makeText(realTime.this, ""Object detected but label problem"", Toast.LENGTH_SHORT).show();
                                        }
                                    }

                                }
                        )
                        .addOnFailureListener(
                                new OnFailureListener() {
                                    @Override
                                    public void onFailure(@NonNull Exception e) {
                                        Toast.makeText(realTime.this, ""No object detected"", Toast.LENGTH_SHORT).show();
                                        Log.d(""realTime"", ""No object detected"");
                                    }
                                }
                        )
                        .addOnCompleteListener(
                                new OnCompleteListener<List<DetectedObject>>() {
                                    @Override
                                    public void onComplete(@NonNull Task<List<DetectedObject>> task) {
                                        imageProxy.close();
                                    }
                        });
```
I is showing toast **No object detected**
"
57202,Tutorial doesnt run in CoLabs,"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb?force_kitty_mode=1&force_corgi_mode=1#scrollTo=TqOt6Sv7AsMi

This is the second Transfer learning tutorial that does not run in colabs.  I have not had an issue before.  Is there a version I need to set.  
This time the code fails at

```python
image_batch, label_batch = next(iter(train_dataset))
feature_batch = base_model(image_batch)
print(feature_batch.shape)

```"
57201,Tutorial throws error,"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning_with_hub.ipynb

at 
```python
result = classifier.predict(grace_hopper[np.newaxis, ...])
result.shape
```

throws the following error.
------------------------------

---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
[<ipython-input-14-c951ede0675b>](https://localhost:8080/#) in <module>
----> 1 result = classifier.predict(grace_hopper[np.newaxis, ...])
      2 result.shape

1 frames
[/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py](https://localhost:8080/#) in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     53     ctx.ensure_initialized()
     54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:
     57     if name is not None:

UnimplementedError: Graph execution error:

Detected at node 'predict/MobilenetV2/Conv/Conv2D' defined at (most recent call last):
    File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
      ""__main__"", mod_spec)
    File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code
      exec(code, run_globals)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py"", line 16, in <module>
      app.launch_new_instance()
    File ""/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py"", line 846, in launch_instance
      app.start()
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py"", line 612, in start
      self.io_loop.start()
    File ""/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py"", line 132, in start
      self.asyncio_loop.run_forever()
    File ""/usr/lib/python3.7/asyncio/base_events.py"", line 541, in run_forever
      self._run_once()
    File ""/usr/lib/python3.7/asyncio/base_events.py"", line 1786, in _run_once
      handle._run()
    File ""/usr/lib/python3.7/asyncio/events.py"", line 88, in _run
      self._context.run(self._callback, *self._args)
    File ""/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py"", line 758, in _run_callback
      ret = callback()
    File ""/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py"", line 300, in null_wrapper
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/tornado/gen.py"", line 1233, in inner
      self.run()
    File ""/usr/local/lib/python3.7/dist-packages/tornado/gen.py"", line 1147, in run
      yielded = self.gen.send(value)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py"", line 365, in process_one
      yield gen.maybe_future(dispatch(*args))
    File ""/usr/local/lib/python3.7/dist-packages/tornado/gen.py"", line 326, in wrapper
      yielded = next(result)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py"", line 268, in dispatch_shell
      yield gen.maybe_future(handler(stream, idents, msg))
    File ""/usr/local/lib/python3.7/dist-packages/tornado/gen.py"", line 326, in wrapper
      yielded = next(result)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py"", line 545, in execute_request
      user_expressions, allow_stdin,
    File ""/usr/local/lib/python3.7/dist-packages/tornado/gen.py"", line 326, in wrapper
      yielded = next(result)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py"", line 306, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py"", line 536, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py"", line 2855, in run_cell
      raw_cell, store_history, silent, shell_futures)
    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py"", line 2881, in _run_cell
      return runner(coro)
    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py"", line 68, in _pseudo_sync_runner
      coro.send(None)
    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py"", line 3058, in run_cell_async
      interactivity=interactivity, compiler=compiler, result=result)
    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py"", line 3249, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-12-7db51c8d2d71>"", line 4, in <module>
      hub.KerasLayer(classifier_model, input_shape=IMAGE_SHAPE+(3,))
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py"", line 153, in __init__
      self._func = load_module(handle, tags, self._load_options)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py"", line 449, in load_module
      return module_v2.load(handle, tags=tags, options=set_load_options)
    File ""/usr/local/lib/python3.7/dist-packages/tensorflow_hub/module_v2.py"", line 106, in load
      obj = tf.compat.v1.saved_model.load_v2(module_path, tags=tags)
Node: 'predict/MobilenetV2/Conv/Conv2D'
DNN library is not found.
	 [[{{node predict/MobilenetV2/Conv/Conv2D}}]] [Op:__inference_predict_function_35179]


"
57198,Colab CuDnn issue involving LSTM,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

Python 3.7.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA Version: 11.2  

### GPU model and memory

NVIDIA-SMI 460.32.03    Driver Version: 460.32.03 

### Current Behaviour?

```shell
Encountering this error suddenly on a notebook that was previously working. It seems that the forward lstm is causing issues for the graph. The lstm is in a bidirectional wrapper. I have also seen the backward lstm having the same issue. To my knowledge Google Colab did not switch GPU driver version or TF/Keras version. 

I have not tried to replicate the issue locally because I do not have a GPU. When I comment out the GPU connections I still run in to the issue so I believe it relates to either the OS Colab runs on or the TF/Keras version... or also perhaps the Python version itself.

Interestingly, I do have the trained model that I run in another notebook without GPU. It fails in the same way when running on CPU outside of any training loop.
```


### Standalone code to reproduce the issue

```shell
HERE IS A LINK TO THE COLAB WITH THE ISSUE IN CELL #2:
https://colab.research.google.com/drive/1f2L3phqOzHHpF03NgNDHDYUVyhF2NOja?usp=sharing

CODE FOR ISSUE:

~~~~

from tensorflow.keras.layers import Dense, Bidirectional, Flatten, LSTM, BatchNormalization

%tensorflow_version 2.9

LSTM_Neurons = 160
SEQ_LEN = 12
patience = 5
lr = 2.5e-5
alpha= 1
batchSize = 48
DropoutVal =.25
EPOCHS = 1000

'''Import data as dataframe'''
AllData = pickle.load(open(""BinanceAllDataForRL.pkl"", ""rb""))
InputData = pickle.load(open(""BinanceInputDataForRL.pkl"", ""rb""))

directory = os.getcwd()

!nvidia-smi
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))


# Essential Functions
def preprocess_df(df, OutOfSamplePct):
    df = df.drop(['close', 'open', 'high', 'low', 'Volume', 'Volume '+denominator], 1)
    df.dropna(inplace=True)

    sequential_data = []
    prev_days = deque(maxlen=SEQ_LEN)

    for i in df.values:
        prev_days.append([n for n in i[:-1]])
        if len(prev_days) == SEQ_LEN:
            sequential_data.append([np.array(prev_days), i[-1:]]) 

    X_train = []
    y_train = []
    X_test = []
    y_test = []
    testcutoff = np.int((1-OutOfSamplePct)*len(sequential_data))
    # For exporting
    for seq, target in sequential_data[:testcutoff]:
        X_train.append(seq)
        y_train.append(target)
    for seq, target in sequential_data[testcutoff:]:
        X_test.append(seq)
        y_test.append(target)
    ''' Output Section'''
    return np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test)

def preprocess_df_pred(df):
    df = df.drop(['close', 'open', 'high', 'low', 'Volume', 'Volume '+denominator], 1)
    df.dropna(inplace=True)
    ''' Extra Prediction Data'''
    sequential_data = []
    prev_days = deque(maxlen=SEQ_LEN)
    for i in df.values: 
        prev_days.append([n for n in i[:-1]])
        if len(prev_days) == SEQ_LEN:
            sequential_data.append([np.array(prev_days), i[-1:]])
    X_pred = []
    y_pred = []
    for seq, target in sequential_data:
        X_pred.append(seq)
        y_pred.append(target)
    return np.array(X_pred), np.array(y_pred)

# Experimental Past/Present Predictor
# Grab data & Run model
IndLength = len(AllData)
mainholder = []
Length = np.shape(AllData.close)
WasUp = np.nan * (np.zeros(Length))
WasUp[:-periods] = AllData.close.values[periods:] / AllData.open.values[1:Length[0] - periods + 1]
AllData['WasUp'] = WasUp - 1

# Test/Train data that has zero overlap w/ OOS data (InputData)
AllData = AllData[:-InputLength]

# WasUp Target Variable
Length = np.shape(InputData.close)
WasUp = np.nan * (np.zeros(Length))
WasUp[:-periods] = InputData.close.values[periods:] / InputData.open.values[1:Length[0] - periods + 1]
InputData['WasUp'] = WasUp - 1
      
InputLabels=AllData.columns
Val_InputLabels=InputData.columns
times = sorted(AllData.index.values)
last_5pct = -int(OutOfSamplePct * len(AllData))
testcutoff = times[last_5pct]

MainHolder = []  # For export data
AllData = AllData.dropna(axis=0, how=""any"")
InputData = InputData.dropna(axis=0, how=""any"")

X_train, y_train, X_test, y_test = preprocess_df(AllData, OutOfSamplePct)
X_pred, y_pred = preprocess_df_pred(InputData)

###

# Custom Loss function
def custom_loss_function(y_true, y_pred):
   positions = tf.math.multiply(y_true, y_pred)
   ## Omniscience performs the best
   sh = tf.math.reduce_mean(positions)/tf.math.reduce_std(positions)  # Sharps
   return -sh

# Custom Metric function
def custom_metric_function(y_true, y_pred):
   positions = tf.math.multiply(y_true, y_pred)
   sh = tf.math.reduce_mean(positions)/tf.math.reduce_std(positions)  # Sharps
   return -sh

# Custom Metric function 2
def custom_metric_function2(y_true, y_pred):
   positions = tf.math.multiply(y_true, y_pred)
   r = tf.math.reduce_mean(positions)
   return r

# Build Residual model
input = tf.keras.Input(shape=(SEQ_LEN, X_train.shape[2]))

### Original
x = Bidirectional(LSTM(LSTM_Neurons, activation='tanh', return_sequences=True, dropout=DropoutVal,kernel_initializer='he_uniform'))(input)
x = BatchNormalization()(x)

x_rnn = Bidirectional(LSTM(LSTM_Neurons, activation='tanh', return_sequences=True, dropout=DropoutVal,kernel_initializer='he_uniform'))(x)
x_rnn = BatchNormalization()(x_rnn)

x = tf.keras.layers.add([x, x_rnn])  ## Add this back in and take out the concat below
x = Bidirectional(LSTM(np.int(LSTM_Neurons), activation='tanh', return_sequences=True, dropout=DropoutVal,kernel_initializer='he_uniform'))(x)


x = Flatten()(x)
x = BatchNormalization()(x)

out = Dense(1, activation='tanh')(x)

model = tf.keras.models.Model(inputs=input, outputs=out)
model.summary()
opt = tf.keras.optimizers.Adam(learning_rate=lr)

# Checkpoint

filepath = 'AllStocks' + ""-{epoch:02d}-{val_custom_metric_function:.10f}""
checkpoint = ModelCheckpoint(
    ""models/{}.model"".format(filepath, monitor='val_custom_metric_function', verbose=1, save_best_only=True,
                             save_weights_only=False,
                             mode='min'))
# EarlyStopping
ES = EarlyStopping(monitor='val_custom_metric_function', mode='min', patience=patience)

model.compile(
    optimizer=opt,
    metrics=[custom_metric_function,custom_metric_function2],
    loss=custom_loss_function,
    )

history = model.fit(X_train, y_train,
                    validation_data=(X_test, y_test),
                    epochs=EPOCHS,
                    batch_size=batchSize,
                    callbacks=[ES, checkpoint],
                    verbose=1,
                    use_multiprocessing=False,
                    workers=1,
                    shuffle=True,
                    )

'''saving model and OOS metrics'''
# Identify the best model
# (Maximal Accuracy)
tix = 'AllStocks'
for root, dirs, files in os.walk(os.getcwd() + '/models/'):
    def find(s, ch):
        return [id for id, ltr in enumerate(s) if ltr == ch]
    acc0 = .0
    for name in dirs:
        if name[0:len(tix)] == tix:  # File pertains to specific industry
            dashSpot = []
            dashSpot = find(name, ""-"")
            acc = float(name[dashSpot[1] + 1:dashSpot[1] + 13])
            if acc <= acc0:
                acc0 = acc
                bestfilename = name
print('best model: ', bestfilename)

model = tf.keras.models.load_model(os.getcwd() + '/models/' + bestfilename, compile=False)
os.rename(os.getcwd() + '/models/' + bestfilename,
          os.getcwd() + ""/models/BestModel"")

# Export Model to Local drive
!zip -r /content/BestModel.zip /content/models/BestModel
from google.colab import files
files.download(""/content/BestModel.zip"")
files.download(""/content/pca.pkl"")
files.download(""/content/comps.pkl"")

opt = tf.keras.optimizers.Adam()
model.compile(
    optimizer=opt,
    loss=custom_loss_function,
    metrics='custom_metric_function')
```


### Relevant log output

```shell
UnknownError: Graph execution error:

Fail to find the dnn implementation.
	 [[{{node CudnnRNN}}]]
	 [[model/bidirectional/forward_lstm/PartitionedCall]] [Op:__inference_train_function_17056]
```
</details>"
57196,Is the estimate of how big TFLite is on ARM incorrect for iOS?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

iOS

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When trying to link against TFLite for iOS based on the CocoaPods. We noticed a nearly 3-4MB increase in our binary size as opposed to the [documentation](https://www.tensorflow.org/lite/guide) which states that 

```
The TensorFlow Lite binary is ~1MB when all 125+ supported operators are linked (for 32-bit ARM builds), and less than 300KB when using only the operators needed for supporting the common image classification models InceptionV3 and MobileNet.
```
Admittedly this was for 64-bit ARM, but I don't believe that should make a difference?

Per Bloaty, the top symbols came from
```
    FILE SIZE        VM SIZE
 --------------  --------------
  30.1%  1.27Mi  30.1%  1.27Mi    tflite::AcquireFlexDelegate()
  16.9%   728Ki  16.9%   728Ki    _TfLiteXNNPackDelegateDelete
  12.8%   548Ki  12.8%   548Ki    _TfLiteSignatureRunnerDelete
   9.4%   405Ki   9.4%   405Ki    _TfLiteOpaqueNodeGetOutput
   8.7%   372Ki   8.6%   372Ki    _TfLiteDelegateCreate
   6.4%   276Ki   6.4%   276Ki    _TfLiteTensorCopyToBuffer
```
Looking into it further, it looks like the flex delegate [is allowlisted](https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/lite/ios/allowlist_TensorFlowLiteC.txt) though I wouldn't normally expect it to be. I think this leads to an incorrect estimate of the binary size on the TFLite docs linked above.

Is there any reason why the FlexDelegate needs to be linked for iOS (similar question for XNNPack)? Can't they be extras to install like the Metal or CoreML Delegate? Making those two delegates optional would reduce the binary size by nearly 2MB, a near 50% decrease.


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
57195,AttributeError: 'Tensor' object has no attribute 'numpy',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
AttributeError: 'Tensor' object has no attribute 'numpy'
```


### Standalone code to reproduce the issue

```shell
x=tf.ones([1])
y=tf.ones([1])
@tf.function
def f(x,y):
    return (x+y).numpy()
f(x,y)
How can I get an array from this code?
```


### Relevant log output

_No response_</details>"
57194,env: python: No such file or directory,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

TF 2.8

### Custom Code

Yes

### OS Platform and Distribution

macos

### Mobile device

macbook pro

### Python version

3.10

### Bazel version

4.2.1

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
/tensorflow/core/util/BUILD:379:24: Action tensorflow/core/util/version_info.cc failed: (Exit 127): bash failed: error executing command /bin/bash -c 'bazel-out/ios-arm64-min9.0-applebin_ios-ios_arm64-opt-ST-80976841b7ce/bin/tensorflow/tools/git/gen_git_source --generate ""$@"" --git_tag_override=${GIT_TAG_OVERRIDE:-}' '' ... (remaining 4 argument(s) skipped)
env: python: No such file or directory
Target //tensorflow/lite/ios:TensorFlowLiteC_static_framework failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2.010s, Critical Path: 0.06s
INFO: 62 processes: 62 internal.
```


### Standalone code to reproduce the issue

```shell
bazel build --config=ios_fat -c opt \
  //tensorflow/lite/ios:TensorFlowLiteC_static_framework
```


### Relevant log output

_No response_</details>"
57192,Integer dtype not supported for tensorflow.math.tan,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The documentation here https://www.tensorflow.org/api_docs/python/tf/math/tan states that it supports `integers` but it doesn't
```


### Standalone code to reproduce the issue

```shell
import tensorflow

print(tensorflow.math.tan(tensorflow.constant([1,2,3],dtype='int32')))
```
### Extra
It doesn't support any integer type

### Relevant log output

```shell
File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 7164, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node Tan}} = Tan[T=DT_INT32]
All kernels registered for op Tan:
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_HALF]
 [Op:Tan]
```
</details>"
57191,WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf2.9

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.7

### Bazel version

v2.9.0-rc2-42-g8a20d54a3c1 2.9.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

2080Ti 11Gb

### Current Behaviour?

```shell
I get lots of warnings during image augmentation. Because of them, the speed is super slow.
```


### Standalone code to reproduce the issue

```shell
I'm trying to run this sample Keras code: https://keras.io/examples/vision/semantic_image_clustering
The Warning I get happens during augmentation


from collections import defaultdict
import random
import numpy as np
import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
from tqdm import tqdm

num_classes = 10
input_shape = (32, 32, 3)

(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
x_data = np.concatenate([x_train, x_test])
y_data = np.concatenate([y_train, y_test])

print(""x_data shape:"", x_data.shape, ""- y_data shape:"", y_data.shape)

classes = [
    ""airplane"",
    ""automobile"",
    ""bird"",
    ""cat"",
    ""deer"",
    ""dog"",
    ""frog"",
    ""horse"",
    ""ship"",
    ""truck"",
]


target_size = 32  # Resize the input images.
representation_dim = 512  # The dimensions of the features vector.
projection_units = 128  # The projection head of the representation learner.
num_clusters = 20  # Number of clusters.
k_neighbours = 5  # Number of neighbours to consider during cluster learning.
tune_encoder_during_clustering = False  # Freeze the encoder in the cluster learning.


data_preprocessing = keras.Sequential(
    [
        layers.Resizing(target_size, target_size),
        layers.Normalization(),
    ]
)
# Compute the mean and the variance from the data for normalization.
data_preprocessing.layers[-1].adapt(x_data)


data_augmentation = keras.Sequential(
    [
        layers.RandomTranslation(
            height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2), fill_mode=""nearest""
        ),
        layers.RandomFlip(mode=""horizontal""),
        layers.RandomRotation(
            factor=0.15, fill_mode=""nearest""
        ),
        layers.RandomZoom(
            height_factor=(-0.3, 0.1), width_factor=(-0.3, 0.1), fill_mode=""nearest""
        )
    ]
)


image_idx = np.random.choice(range(x_data.shape[0]))
image = x_data[image_idx]
image_class = classes[y_data[image_idx][0]]
plt.figure(figsize=(3, 3))
plt.imshow(x_data[image_idx].astype(""uint8""))
plt.title(image_class)
_ = plt.axis(""off"")

```
```


### Relevant log output

```shell
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x000002581AA0BAF8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x000002581BE0A288> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting Bitcast
WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2
WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3
```
```
</details>"
57189,Is there a way to see MatMul tensor shapes in TensorBoard or elsewhere ? ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
How do I find out the final MatMul shapes used by TensorFlow after all the Grappler etc. processing,
```


### Standalone code to reproduce the issue

```shell
Not applicable
```


### Relevant log output

```shell
Not applicable
```
</details>"
57185,Please publish official wheels for Apple Silicon,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

macOS 12

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

</details>

### Current Behaviour?

Users of Macs with Apple Silicon (M1, M2, etc.) cannot `pip install tensorflow` because wheels for the architecture are not published on PyPI.

The current state of this (August 2022) is that these wheels are published under a different package name, [`tensorflow-macos`](https://pypi.org/project/tensorflow-macos/), which is released separately by Apple.

This makes it more complicated to install software that uses TensorFlow on macOS; e.g., a `requirements.txt` file might have to look something like

```
tensorflow;  sys.platform != 'darwin' or platform.machine != 'arm64'
tensorflow-macos;  sys.platform == 'darwin' and platform.machine == 'arm64'
``` 

Apple's repository, [`tensorflow_macos`](https://github.com/apple/tensorflow_macos) is archived which makes it unclear where users should get support. Is it here?

Moreover, at the time of writing, `tensorflow-macos` is version 3.9.2, which is one bugfix release ahead of the latest official `tensorflow`, so comparing version numbers across the two is difficult.

And it leaves many other unanswered questions: Are these being tested in CI? How soon after an upstream release will `tensorflow-macos` get an update?

Finally, both `tensorflow` and `tensorflow-macos` support Intel-based Macs, which is another potential point of confusion—which is preferred?

cc @kulinseth @jhavukainen


### Standalone code to reproduce the issue

```shell
(.venv) % arch
arm64
(.venv) % python3 -m pip install tensorflow
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```"
57183,TensorRT conversion: tensorflow.GraphDef exceeds maximum protobuf size of 2GB,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi, we have a model that we want to convert using TensorRT as supported by TensorFlow.
The original model size is 6191909 bytes, with variable size 2308318345 bytes.

Is there anyway to enable converting this model to TensorRT optimised model?
Converting using precision FP16 doesn't seem to work.
```


### Standalone code to reproduce the issue

```shell
We can reproduce by performing TensorRT optimisation on a model larger than 2GB
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/usr/local/bin/tensord"", line 33, in <module>
    sys.exit(load_entry_point('tensord', 'console_scripts', 'tensord')())
  File ""/tensord/tensord/cli.py"", line 61, in main
    CLI()
  File ""/tensord/tensord/cli.py"", line 30, in __init__
    getattr(self, args.command)()
  File ""/tensord/tensord/cli.py"", line 46, in optimize
    optimize.cli(sys.argv[2:])
  File ""/tensord/tensord/commands/optimize.py"", line 87, in cli
    optimize(**vars(args))
  File ""/tensord/tensord/commands/optimize.py"", line 70, in optimize
    inference_optimizer.optimize()
  File ""/tensord/tensord/inference/tensorrt_optimizer.py"", line 79, in optimize
    converter.convert(input_fields=params.input_fields, unpack=params.unpack)
  File ""/tensord/tensord/inference/tensorrt.py"", line 1213, in convert
    self._saved_model, func, frozen_func = load_frozen_func(self._input_saved_model_dir, input_fields, unpack)
  File ""/tensord/tensord/inference/optimizer.py"", line 76, in load_frozen_func
    frozen_func = convert_variables_to_constants_v2(concrete_func, lower_control_flow=False)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/convert_to_constants.py"", line 1151, in convert_variables_to_constants_v2
    return _construct_concrete_function(func, output_graph_def,
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/convert_to_constants.py"", line 1076, in _construct_concrete_function
    new_func = wrap_function.function_from_graph_def(output_graph_def,
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py"", line 655, in function_from_graph_def
    wrapped_import = wrap_function(_imports_graph_def, [])
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py"", line 619, in wrap_function
    func_graph.func_graph_from_py_func(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py"", line 1161, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py"", line 83, in __call__
    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py"", line 89, in wrapped
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py"", line 649, in _imports_graph_def
    importer.import_graph_def(graph_def, name="""")
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py"", line 548, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 403, in import_graph_def
    return _import_graph_def_internal(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 497, in _import_graph_def_internal
    with c_api_util.tf_buffer(graph_def.SerializeToString()) as serialized:
ValueError: Message tensorflow.GraphDef exceeds maximum protobuf size of 2GB: 2310494883
```
</details>"
57181,Segmentation fault (core dumped),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

tf 2.6

### Custom Code

Yes

### OS Platform and Distribution

Linux 5.4.0-1089, 18.04.1-Ubuntu SMP

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

v100

### Current Behaviour?

```shell
The same code works on colab. However, when I run on VM, I get Segmentation fault (core dumped). This error is very cryptic. Please let me know how to get more detailed error for easier debugging.
BTW I have also tried the latest stable tf build 2.9 and tf-nightly build. Both crash with the same error.
Thanks.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf


class ConvBlock:
    def __init__(
        self,
        c_in,
        c_out,
        kernel_size=3,
        stride=1,
        padding=1,
        bias=False,
        K=1,
        backbone=""residual"",
    ):
        super(ConvBlock, self).__init__()

        self.f = tf.keras.layers.Conv2D(
            c_out,
            kernel_size=kernel_size,
            strides=stride,
            padding=""SAME"",
            use_bias=bias,
            activation=None,
        )
        self.g = tf.keras.layers.Conv2D(
            c_out,
            kernel_size=3,
            strides=1,
            padding=""SAME"",
            use_bias=False,
            activation=None,
        )
        self.K = K
        self.backbone = backbone

        self.bn_out = tf.keras.layers.BatchNormalization()
        self.bn_f1 = tf.keras.layers.BatchNormalization()
        self.c_out = c_out

    def __call__(self, x):
        f = self.f(tf.keras.layers.Activation(""relu"")(self.bn_f1(x)))
        h = f


        bn_g = tf.keras.layers.BatchNormalization()
        h = self.g(tf.keras.layers.Activation(""relu"")(bn_g(h)))

        return h


def net(backbone=""cnn"", K=5):

    inp = tf.keras.Input((28, 28, 1))

    conv = ConvBlock(1, 1, 3, K=K, backbone=backbone)
    x = conv(inp)
    f1 = x
    x = tf.keras.layers.Activation(""relu"")(x)

    avg_pool = tf.keras.layers.AveragePooling2D(pool_size=(4, 4))
    x = avg_pool(x)
    f2 = x
    flatten = tf.keras.layers.Flatten()
    x = flatten(x)

    fc = tf.keras.layers.Dense(10)
    out = fc(x)

    return tf.keras.Model(inp, [out, f1, f2])

if __name__==""__main__"":
    import numpy as np
    model = net(backbone=""cnn"")
    model.compile()
    x = np.zeros((28, 28, 1))
    print(model(x))
```


### Relevant log output

```shell
2022-08-16 14:01:30.779185: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-08-16 14:01:31.772277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14645 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0001:00:00.0, compute capability: 7.0
Segmentation fault (core dumped)
```
</details>"
57180,openCL crash in models converted by the newer versions of TF (e.g. TF 2.9),"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installation (pip package or built from source): source
- TensorFlow library (version, if pip package or github SHA, if built from source): TF 2.8 - TF 2.9 - TF Nightly

### 2. Code

We have implemented a small tool (with a comprehensive ReadMe) to reproduce the mentioned issue. Here is the link to the repository:
https://github.com/Bahar-BM/dense_3D_inputs

### 3. Failure after conversion
Converting models in which there is at least one Dense layer fed by a 3D input should be done using older versions of TensorFlow (we have tested v2.3 and v2.4). Otherwise, the obtained TFLite model will crash with the OpenCL delegate with this error message:

```
ERROR: TfLiteGpuDelegate Init: FULLY_CONNECTED: Amount of input data should match weights width
```"
57179,Bazel - Download from ... returned 404 Not Found,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Docker - condaforge/mambaforge-pypy3:4.13.0-1

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.1.1

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/2c3ca3b684bb2b188d977d47548e79dc559fb8ad.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found

WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/1b511f6fabad91307e7adc49a1d832b4db7c7145.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found

WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found

WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11/archive/v2.10.0.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found

WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/273292d1cfc0a94a65082ee350509af1d113344d.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found

WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pytorch/cpuinfo/archive/5e63739504f0f8e18e941bd63b2d6d42536c7d90.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found

WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/8e3d3359f9bec608e09fac1f7054a2a14b1bd73c.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
```


### Standalone code to reproduce the issue

```shell
PIP_ROOT_USER_ACTION=ignore
CONDAENV='tensorflow'
PYTHONENV='3.7'
CONDARC='/opt/conda/.condarc'
TENSORFLOWDIR='tensorflow_src/tensorflow/lite/tools/pip_package'
BAZEL_VERSION='5.1.1'
DISABLE_BAZEL_WRAPPER='1'

cat > $CONDARC << EOF
channels:
  - anaconda
  - intel
  - defaults
pip_interop_enabled: True
EOF

mamba create --name $CONDAENV -y python=$PYTHONENV \
&& mamba init bash \
&& source ~/.bashrc \
&& source activate $CONDAENV

wget -O /usr/local/bin/bazel ""https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel-$BAZEL_VERSION-linux-x86_64"" \
&& chmod a+x /usr/local/bin/bazel

git clone --depth 1 https://github.com/tensorflow/tensorflow.git /tensorflow_src

python3 -m pip install -U pip pybind11 numpy pillow

apt-get update && apt-get install -y build-essential

$TENSORFLOWDIR/build_pip_package_with_bazel.sh native
```


### Relevant log output

```
+ echo '__git_version__ = '\'''\'''
+ cd /tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
+ case ""${TENSORFLOW_TARGET}"" in
+ BAZEL_FLAGS='--copt=-O3 --copt=-march=native'
+ export CROSSTOOL_PYTHON_INCLUDE_PATH
+ case ""${TENSORFLOW_TARGET}"" in
+ LIBRARY_EXTENSION=.so
+ bazel build -c opt -s --config=monolithic --config=noaws --config=nogcp --config=nohdfs --config=nonccl --copt=-O3 --copt=-march=native //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper
Extracting Bazel installation...
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:monolithic in file /tensorflow_src/.bazelrc: --define framework_shared_object=false --experimental_link_static_libraries_once=false
INFO: Found applicable config definition build:noaws in file /tensorflow_src/.bazelrc: --define=no_aws_support=true
INFO: Found applicable config definition build:nogcp in file /tensorflow_src/.bazelrc: --define=no_gcp_support=true
INFO: Found applicable config definition build:nohdfs in file /tensorflow_src/.bazelrc: --define=no_hdfs_support=true
INFO: Found applicable config definition build:nonccl in file /tensorflow_src/.bazelrc: --define=no_nccl_support=true
INFO: Found applicable config definition build:linux in file /tensorflow_src/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow_src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/d08733f7ecc1e3ed5d75655fe47671b392f5b3a4.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/2c3ca3b684bb2b188d977d47548e79dc559fb8ad.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11/archive/v2.10.0.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/273292d1cfc0a94a65082ee350509af1d113344d.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pytorch/cpuinfo/archive/5e63739504f0f8e18e941bd63b2d6d42536c7d90.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/8e3d3359f9bec608e09fac1f7054a2a14b1bd73c.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper (97 packages loaded, 2396 targets configured).
INFO: Found 1 target...
```

Also note, that if I try to download the files without using the mirror, they work. 

E.g. 
```
https://github.com/llvm/llvm-project/archive/2c3ca3b684bb2b188d977d47548e79dc559fb8ad.tar.gz

https://github.com/tensorflow/runtime/archive/1b511f6fabad91307e7adc49a1d832b4db7c7145.tar.gz

https://github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz

https://github.com/pybind/pybind11/archive/v2.10.0.tar.gz

https://github.com/abseil/abseil-cpp/archive/273292d1cfc0a94a65082ee350509af1d113344d.tar.gz

https://github.com/pytorch/cpuinfo/archive/5e63739504f0f8e18e941bd63b2d6d42536c7d90.tar.gz

https://github.com/google/XNNPACK/archive/8e3d3359f9bec608e09fac1f7054a2a14b1bd73c.zip
```
</details>"
57177,Error while exporting efficientdet_lite0 model to tflite format,"I have trained a custom object detection model using tflite-model-maker using efficientdet_lite0 architecture but when I try to export the model in tflite format using below code snippet
```
model.export(export_dir='.', tflite_filename='Myhanddetectionmodel.tflite')
```
It gives the below given error
```
 TypeError: EndVector() missing 1 required positional argument: 'vectorNumElems'
```
I am using google colab!"
57175,how to use tfrecord data(dict format) (tf.data.dataset) in distribute training?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tf2.2.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I tried to use both mirror and multimirror in distributed training, and i found the input data (from all doucements like tf official tutorial,github and so on )to the strategy.run() must be like ((None,256,256,64)(None)), while the batched data from the fnction below ""def make_dataset_for_distribute_train(self, all_paths):"" is a dict.  strategy.run() cannot accept dict data for distributed training , so i had to transform it(the output of ""make_dataset_for_distribute_train"" function) before the distributed training using the
function ""def parse_dict2_turple(self,data_tensor"" below.
It worked, but the cpu useage is above 300%(before is 200%）and gpu_utils_average is just 10%(before is 75% one gpu). As a result, the distributed training with 4 gpus even slower than normal training. 
Can someone tell me why and how to fix it? really appreciate for your help!!!
```


### Standalone code to reproduce the issue

```shell
def make_dataset_for_distribute_train(self, all_paths):
   dataset = tf.data.TFRecordDataset(all_paths)
   dataset = dataset.map(self.parser, num_parallel_calls=tf.data.experimental.AUTOTUNE)
   dataset = dataset.map(self.make_feed_dict,num_parallel_calls=tf.data.experimental.AUTOTUNE)
   padding_dict = {""a"": [20], ""b"": [None], ""c"": [30],""d"": [13]}

   dataset_for_train = dataset.padded_batch(self.batch_size, padded_shapes=padding_dict,
                                                 drop_remainder=True).prefetch(
            buffer_size=tf.data.experimental.AUTOTUNE)
   return dataset_for_train
    



def parse_dict2_turple(self,data_tensor):

        d0 = data_tensor['a']
        d1 = data_tensor['b']
        d2 = data_tensor['c']
        d3 = data_tensor['d']
  
        return tf.data.Dataset.from_tensor_slices((d0, d1, d2, d3))

        def distributed_train_step(inputs):

            @tf.function
            def distributed_train_step_wrapped(ds):
                loss=0.0
                for k in ds:
                    # tf.print(k)
                    per_replica_average_loss = strategy.run(train_step, args=(k,))
                    loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_average_loss, axis=None)

                return loss
            return distributed_train_step_wrapped(inputs)
            for i in date:
                all_paths = self.parse_input_path(i, epoch)
                logger.info(all_paths)
                dataset = self.make_dataset_for_distribute_train(all_paths)

                total_loss = 0.0
                num_train_batches = 0
                for j in dataset:

                    dataset_parsed = self.parse_dict2_turple(j)
                    dataset_parsed = dataset_parsed.with_options(options)
                    train_dataset_distribute = strategy.experimental_distribute_dataset(dataset_parsed)
                    batch_loss = distributed_train_step(train_dataset_distribute)
                    num_train_batches += 1
                    total_loss += batch_loss
```


### Relevant log output

_No response_</details>"
57174,tf.nn.embedding_lookup result on cpu inconsistent with gpu,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

TF2.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The result on CPU should be consistent with GPU.
```


### Standalone code to reproduce the issue

```shell
results = dict()
import tensorflow as tf
try:
  try:
    with tf.device('/CPU'):
      params_tensor = tf.random.uniform([2, 256], dtype=tf.float32)
      params = tf.identity(params_tensor)
      ids_tensor = tf.saturate_cast(tf.random.uniform([2], minval=-256, maxval=257, dtype=tf.int64), dtype=tf.int32)
      ids = tf.identity(ids_tensor)
      results[""res_cpu""] = tf.nn.embedding_lookup(params=params,ids=ids,)
  except Exception as e:
    results[""err_cpu""] = ""Error:""+str(e)
  try:
    with tf.device('/GPU:0'):
      params = tf.identity(params_tensor)
      params = tf.cast(params, tf.float32)
      ids = tf.identity(ids_tensor)
      ids = tf.cast(ids, tf.int32)
      results[""res_gpu""] = tf.nn.embedding_lookup(params=params,ids=ids,)
  except Exception as e:
    results[""err_gpu""] = ""Error:""+str(e)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
'''
{'err_cpu': 'Error:indices[0] = -35 is not in [0, 2) [Op:GatherV2]', 'res_gpu': <tf.Tensor: shape=(2, 256), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],
      dtype=float32)>}
'''
CPU checks input, while GPU doesn't.
```


### Relevant log output

_No response_</details>"
57173,Inconsistent sparse_categorical_crossentropy on cpu vs gpu,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

TF2.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
the result on CPU should be consistent with the result on GPU. However, I find the result on CPU is not equal to the result on GPU on some APIs.
These APIs are:
tf.keras.losses.sparse_categorical_crossentropy
tf.keras.metrics.SparseCategoricalCrossentropy
tf.losses.sparse_categorical_crossentropy
tf.metrics.SparseCategoricalCrossentropy
```


### Standalone code to reproduce the issue

```shell
results = dict()
import tensorflow as tf
try:
  try:
    with tf.device('/CPU'):
      arg_0_0 = -56
      arg_0_1 = -47
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1_0_0 = 0.05
      arg_1_0_1 = 0.95
      arg_1_0_2 = 0
      arg_1_0 = [arg_1_0_0,arg_1_0_1,arg_1_0_2,]
      arg_1_1_0 = 0.1
      arg_1_1_1 = 0.8
      arg_1_1_2 = 0.1
      arg_1_1 = [arg_1_1_0,arg_1_1_1,arg_1_1_2,]
      arg_1 = [arg_1_0,arg_1_1,]
      results[""res_cpu""] = tf.keras.losses.sparse_categorical_crossentropy(arg_0,arg_1,)
  except Exception as e:
    results[""err_cpu""] = ""Error:""+str(e)
  try:
    with tf.device('/GPU:0'):
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1_0 = [arg_1_0_0,arg_1_0_1,arg_1_0_2,]
      arg_1_1 = [arg_1_1_0,arg_1_1_1,arg_1_1_2,]
      arg_1 = [arg_1_0,arg_1_1,]
      results[""res_gpu""] = tf.keras.losses.sparse_categorical_crossentropy(arg_0,arg_1,)
  except Exception as e:
    results[""err_gpu""] = ""Error:""+str(e)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
```
The results is {'err_cpu': 'Error:Received a label value of -56 which is outside the valid range of [0, 3). Label values: -56 -47 [Op:SparseSoftmaxCrossEntropyWithLogits]', 'res_gpu': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([nan, nan], dtype=float32)>}.

```
results = dict()
import tensorflow as tf
try:
  try:
    with tf.device('/CPU'):
      arg_0_0 = 28
      arg_0_1 = -8
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1_0_0 = -49.95
      arg_1_0_1 = -3.05
      arg_1_0_2 = False
      arg_1_0 = [arg_1_0_0,arg_1_0_1,arg_1_0_2,]
      arg_1_1_0 = -2.9
      arg_1_1_1 = -44.2
      arg_1_1_2 = -21.9
      arg_1_1 = [arg_1_1_0,arg_1_1_1,arg_1_1_2,]
      arg_1 = [arg_1_0,arg_1_1,]
      results[""res_cpu""] = tf.losses.sparse_categorical_crossentropy(arg_0,arg_1,)
  except Exception as e:
    results[""err_cpu""] = ""Error:""+str(e)
  try:
    with tf.device('/GPU:0'):
      arg_0 = [arg_0_0,arg_0_1,]
      arg_1_0 = [arg_1_0_0,arg_1_0_1,arg_1_0_2,]
      arg_1_1 = [arg_1_1_0,arg_1_1_1,arg_1_1_2,]
      arg_1 = [arg_1_0,arg_1_1,]
      results[""res_gpu""] = tf.losses.sparse_categorical_crossentropy(arg_0,arg_1,)
  except Exception as e:
    results[""err_gpu""] = ""Error:""+str(e)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
```
The result is {'err_cpu': 'Error:Received a label value of -8 which is outside the valid range of [0, 3).  Label values: 28 -8 [Op:SparseSoftmaxCrossEntropyWithLogits]', 'res_gpu': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([nan, nan], dtype=float32)>}.

CPU throws an error, while GPU runs.
```


### Relevant log output

_No response_</details>"
57172,Inconsistent tf.losses.mean_squared_error on cpu vs gpu,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

TF2.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The result on CPU should be equal to the result on GPU.
```


### Standalone code to reproduce the issue

```shell
results = dict()
import tensorflow as tf
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.saturate_cast(tf.random.uniform([2, 3], minval=-256, maxval=257, dtype=tf.int64), dtype=tf.int64)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([2, 3], dtype=tf.float16)
      arg_1 = tf.identity(arg_1_tensor)
      results[""res_cpu""] = tf.losses.mean_squared_error(arg_0,arg_1,)
  except Exception as e:
    results[""err_cpu""] = ""Error:""+str(e)
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.int64)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float16)
      results[""res_gpu""] = tf.losses.mean_squared_error(arg_0,arg_1,)
  except Exception as e:
    results[""err_gpu""] = ""Error:""+str(e)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
'''
{'res_cpu': <tf.Tensor: shape=(2,), dtype=float16, numpy=array([inf, inf], dtype=float16)>, 'res_gpu': <tf.Tensor: shape=(2,), dtype=float16, numpy=array([35650., 25060.], dtype=float16)>}
'''
The result on CPU outputs inf array, while the result on GPU outputs float array.
```


### Relevant log output

_No response_</details>"
57171,LeakyRelu in Tensorflow lite with the Hexagon Delegate not supported,"When using a tflite model (8-bits quantized via TensorFlow lite conversion framework) that includes the activation function ""LeakyRelu"", the Hexagon delegate from tensorflow framework cannot perform the DNN inference on the whole graph, but rather it falls back to the CPU/XNNPack delegate. This is due to the fact that 'LeakyRelu' operation is not supported by the Hexagon Delegate (confirmed in TensorFlow doc: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/README.md). When using Relu activation function (and Relu6 as well), we can see below that the TF Hexagon Delegate can process the whole DNN graph, unfortunately, the qualitative results I get are much worse, hence the need of having 'Leaky Relu' implemented in the Hexagon Delegate.

**System information**
- OS Platform and Distribution): Android 10, NDK R21e
- TensorFlow installed from (source or binary): from source using the Release tag '2.9.1'
- TensorFlow version (or github SHA if from source):  2.9.1


**Output of Tensorflow library when running an inference with a model that includes 'LeakyRelu' activation function**

```
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Initialized TensorFlow Lite runtime.
loaded libcdsprpc.so
INFO: TfLiteHexagonDelegate delegate: 8 nodes delegated out of 53 nodes with 1 partitions.
INFO: Replacing 8 node(s) with delegate (TfLiteHexagonDelegate) node, yielding 1 partitions.
```
As we can see below when replacing 'LeakyRelu' operation by 'Relu', then the TF Hexagon delegate can process the whole DNN graph.

**Output of Tensorflow library when running an inference with a model that includes 'Relu' activation function**

```
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Initialized TensorFlow Lite runtime.
loaded libcdsprpc.so
INFO: TfLiteHexagonDelegate delegate: 53 nodes delegated out of 53 nodes with 1 partitions.
INFO: Replacing 53 node(s) with delegate (TfLiteHexagonDelegate) node, yielding 1 partitions.
```

Would it be possible to implement 'LeakyRelu' in TF Hexagon Delegate ?"
57170,Inconsistent tf.gather_nd behavior on cpu vs gpu,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

TF2.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The result on cpu should be consistent with the result on gpu.
```


### Standalone code to reproduce the issue

```shell
results = dict()
import tensorflow as tf
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([1, 8, 1, 1], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.saturate_cast(tf.random.uniform([8, 2], minval=-256, maxval=257, dtype=tf.int64), dtype=tf.int64)
      arg_1 = tf.identity(arg_1_tensor)
      results[""res_cpu""] = tf.gather_nd(arg_0,arg_1,)
  except Exception as e:
    results[""err_cpu""] = ""Error:""+str(e)
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.int64)
      results[""res_gpu""] = tf.gather_nd(arg_0,arg_1,)
  except Exception as e:
    results[""err_gpu""] = ""Error:""+str(e)
except Exception as e:
  results[""err""] = ""Error:""+str(e)

print(results)
'''
{'err_cpu': 'Error:indices[7] = [-232, -191] does not index into param shape [1,8,1,1] [Op:GatherNd]', 'res_gpu': <tf.Tensor: shape=(8, 1, 1), dtype=float32, numpy=
array([[[0.]],
       [[0.]],
       [[0.]],
       [[0.]],
       [[0.]],
       [[0.]],
       [[0.]],
       [[0.]]], dtype=float32)>}
'''
```


### Relevant log output

_No response_</details>"
57169,how to clear tf.function's cache ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

tf 2.8.2

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
clear memory used by generating concrete functions
```


### Standalone code to reproduce the issue

```shell
# Now memory is 1.6G.
# Then, I get a concrete function by this:
def test():
    f = train_step._get_concrete_function_garbage_collected(images, labels)
    return f
f = test()

# Now, the memory is 6.4G
#Next, I tried to clear this concrete function to reduce memory. So I clear this concrete function by this:

train_step._stateless_fn._function_cache.clear()
train_step._stateful_fn._function_cache.clear()

#And start gc to flush memory:
import gc
gc.collect()

#But, the memory is still 6.4G.
```


### Relevant log output

_No response_</details>"
57167,tf.nn.conv2d error message is inconsistent with documentation,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

TF2.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
'SAME' or 'VALID' can be set to parameter 'padding' of tf.nn.conv2d  in documentation. However, the error message of tf.nn.conv2d shows that 'EXPLICIT' is also an allowd value of 'padding'. Actually, when 'padding' takes 'EXPLICIT', code doesn't work. So the error message is inconsistent with the documentation.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
results={}
try:
  arg_0 = tf.random.uniform([1, 8, 8, 1], dtype=tf.float32)
  arg_1 = tf.random.uniform([1, 1, 1, 3], dtype=tf.float32)
  strides = 1
  padding = ""valid""
  results[""res""] = tf.nn.conv2d(arg_0,arg_1,strides=strides,padding=padding,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
'''
{'err': 'Error:Value for attr \'padding\' of ""valid"" is not in the list of allowed values: ""SAME"", ""VALID"", ""EXPLICIT""\n\t; NodeDef: {{node Conv2D}}; Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[""SAME"", ""VALID"", ""EXPLICIT""]; attr=explicit_paddings:list(int),default=[]; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"", ""NCHW""]; attr=dilations:list(int),default=[1, 1, 1, 1]> [Op:Conv2D]'}
'''
```


### Relevant log output

_No response_</details>"
57166,AutoGraph cannot handle python 3.10's structural pattern matching,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.2

### Custom Code

No

### OS Platform and Distribution

macOS 12.5

### Mobile device

none

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

none

### GPU model and memory

Apple's METAL (though unlikely to be relevant here)

### Current Behaviour?

Trying to AutoGraph a function containing a [PEP 634/635/636 structural pattern matching statement](https://peps.python.org/pep-0634/) from python 3.10 (i.e. `match/case`) results in the `WARNING:tensorflow:AutoGraph could not transform <function test at 0x2c4c99120> and will run it as-is.`

### Standalone code to reproduce the issue

```python
import tensorflow as tf

@tf.function
def test(x, selector):
    match selector:
        case ""square"":
            return x**2
        case ""double"":
            return x*2.
        case _:
            raise ValueError
            
test(tf.linspace(-1,1,10),""double"")
```


### Relevant log output

```shell
INFO:tensorflow:Error transforming entity <function test at 0x2c4c99120>
Traceback (most recent call last):
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py"", line 427, in converted_call
    converted_f = _convert_actual(target_entity, program_ctx)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py"", line 269, in _convert_actual
    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 282, in transform
    return self.transform_function(obj, user_context)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 466, in transform_function
    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 359, in transform_function
    result = self.transform_ast(node, context)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py"", line 237, in transform_ast
    node = self.initial_analysis(node, ctx)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py"", line 223, in initial_analysis
    graphs = cfg.build(node)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/cfg.py"", line 970, in build
    visitor.visit(node)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ast.py"", line 410, in visit
    return visitor(node)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/cfg.py"", line 766, in visit_FunctionDef
    self._process_function_def(node, is_lambda=False)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/cfg.py"", line 757, in _process_function_def
    self.visit(stmt)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ast.py"", line 410, in visit
    return visitor(node)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ast.py"", line 414, in generic_visit
    for field, value in iter_fields(node):
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ast.py"", line 252, in iter_fields
    for field in node._fields:
AttributeError: 'NoneType' object has no attribute '_fields'
```
</details>"
57165,tf.keras.layers.Dense documentation wrong,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

TF2.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The parameter 'units' is a positive integer on documentation. However, when 'unit' is 0, code works. And when 'unit' is -1, the error message is ""Error:Dimension -1 must be >= 0"". So 'units' is not a negative integer whose value can be set to positive integer and zero.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
results={}
try:
  arg_0 = 0
  activation = ""softmax""
  arg_class = tf.keras.layers.Dense(arg_0,activation=activation,)
  arg_input = tf.random.uniform([3, 50, 1], dtype=tf.float32)
  results[""res""] = arg_class(arg_input)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
'''
results = {'res': <tf.Tensor: shape=(3, 50, 0), dtype=float32, numpy=array([], shape=(3, 50, 0), dtype=float32)>}
'''
```


### Relevant log output

_No response_</details>"
57164,filters can be set to float in tf.keras.layers.Conv1D,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

TF2.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
filters should be an integer, because it's relevant to the dimensionality of the output space. Also it must be an integer written on the document. However, I find that when the type of filters is float, code works.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
results={}
try:
  padding = ""same""
  filters = 2.5
  kernel_size = 3
  arg_class = tf.keras.layers.Conv1D(padding=padding,filters=filters,kernel_size=kernel_size,)
  arg_input = tf.random.uniform([1, 2, 2], dtype=tf.float32)
  results[""res""] = arg_class(arg_input)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
'''
results = {'res': <tf.Tensor: shape=(1, 2, 2), dtype=float32, numpy=
array([[[-0.49259394,  0.5246876 ],
        [-0.00979106,  0.42434496]]], dtype=float32)>}
'''
```


### Relevant log output

_No response_</details>"
57162,tensorflow.python.framework.errors_impl.UnknownError: JIT compilation failed. [Op:FloorMod],"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6.2

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Getting the following error.


  File ""lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 7164, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.UnknownError: JIT compilation failed. [Op:FloorMod]
```


Can be fixed by setting the following , however that implies XLA being used is some form. 

```
TF_XLA_FLAGS=""--tf_xla_auto_jit=-1"" XLA_FLAGS=""--xla_gpu_cuda_data_dir=${CUDA_PATH}""
```
```


### Standalone code to reproduce the issue

```shell
Not sure how to do that.
```


### Relevant log output

_No response_</details>"
57160,Error in spec = model_spec.get('efficientdet_lite0'),"I am making a custom object detection model.
```
spec = model_spec.get('efficientdet_lite0')
```
When I run the above line of code it gives the following error.
AttributeError: module 'keras.api._v2.keras.mixed_precision' has no attribute 'experimental'
I am using the official tensorflow colab notebook's code (in Spyder(anaconda))"
57159,The type of parameter 'prefix' in tf.keras.backend.get_uid documentation wrong,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

TF2.4

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Parameter 'prefix' must be a string on documentation. However, I find that when type of prefix is bool/tuple/integer, the code works. But when type of prefix is list, code throws an exception: ""Error:unhashable type: 'list'"". I think supported type like integer should be added to documentation.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
results={}
try:
  arg_0 = 5
  results[""res""] = tf.keras.backend.get_uid(arg_0,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
# results={'res': 1}
```


### Relevant log output

_No response_</details>"
57158,Fitting with generators and sample weights crashes when batch size varies between steps with 3D output,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

MacOS 12.5

### Mobile device

_No response_

### Python version

3.7.3

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

-

### GPU model and memory

-

### Current Behaviour?

```shell
When training a model with a generator (from dataset.from_tensor_slices) and including sample weights where the batch size varies, an error is thrown.

Note: This test for us previously worked as expected in TF<2.6!
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

x = np.random.random((200, 20, 20, 3)) * 10
y = x.dot(np.random.random((3, 3)))
x = x.astype(np.uint8)

model = tf.keras.Sequential()
model.add(
    tf.keras.layers.Conv2D(
        filters=3,
        kernel_size=3,
        activation=""relu"",
        input_shape=(20, 20, 3),
        padding=""same"",
    )
)
model.summary()
model.compile(optimizer=""adam"", loss=""mse"")

# Works
gen = tf.data.Dataset.from_tensor_slices((x, y)).batch(16).repeat()
model.fit(gen, epochs=2, steps_per_epoch=10)

# Works
gen3 = tf.data.Dataset.from_tensor_slices((x, y, np.ones((200,)))).batch(20).repeat()
model.fit(gen3, epochs=2, steps_per_epoch=10)

# Doesnt work
gen2 = tf.data.Dataset.from_tensor_slices((x, y, np.ones((200,)))).batch(16).repeat()
model.fit(gen2, epochs=2, steps_per_epoch=10)
```


### Relevant log output

```shell
grads = tape.gradient(loss, var_list, grad_loss)
Node: 'gradient_tape/mean_squared_error/weighted_loss/BroadcastGradientArgs'
Incompatible shapes: [16,20,20] vs. [16]
	 [[{{node gradient_tape/mean_squared_error/weighted_loss/BroadcastGradientArgs}}]] [Op:__inference_train_function_682]
```
</details>"
57156,"TFLite C API in Android: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter","<details><summary>Click to expand!</summary> 
 
### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

Android API 31

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

#### How I bulid android studio project?
As tutorial [here](https://www.tensorflow.org/lite/android/development?hl=de), I create a Android Studio NDK project as following steps:
1. Download arr [here](https://search.maven.org/artifact/org.tensorflow/tensorflow-lite/2.9.0/aar) and unzip it.
2. Put shared library and header files into project and link in CMakeLists.txt as following:

#### Current Behaviour
My TFLite model is for on-device training, so there are signatures including infer and train.
For infer signature, the functions defined in `c_api_experimental.h` work well and generate reasonable result.
But for train signature, I follow the same steps: set input tensors -> TfLiteSignatureRunnerInvoke -> read output tensors.
But the error log shows Select TensorFlow op(s) is not support by this interpreter when running TfLiteSignatureRunnerInvoke.

#### What I tried
1. Download tensorflow-lite-select-tf-ops shared library [here](https://search.maven.org/artifact/org.tensorflow/tensorflow-lite-select-tf-ops/2.9.0/aar), and put it into project and edit CMakeLists.txt as before.
2. I think there is no C API for flex delegate? And #53849 shows we don't need to add it by `TfLiteInterpreterOptionsAddDelegate` explictly?

#### Questions
How to apply/link TFLite select tf ops in Android Studio project in C API?


### Standalone code to reproduce the issue

CMakeLists.txt
```
cmake_minimum_required(VERSION 3.18.1)
project(""project_name"")

# ==================== For TFLite ====================
# Specify where to find the header files for TF Lite C++
set(JNI_DIR
        ${CMAKE_CURRENT_SOURCE_DIR}/../jni)
include_directories(${JNI_DIR})

add_library( tflite-lib SHARED IMPORTED )
set_target_properties( tflite-lib PROPERTIES IMPORTED_LOCATION
        ${JNI_DIR}/${ANDROID_ABI}/libtensorflowlite_jni.so)

add_library( tflite-flex-lib SHARED IMPORTED )
set_target_properties( tflite-flex-lib PROPERTIES IMPORTED_LOCATION
        ${JNI_DIR}/${ANDROID_ABI}/libtensorflowlite_flex_jni.so)

# ==================== For TFLite ====================

add_library(
        project_name
        SHARED
        tflite.c
        native-lib.cpp)
find_library( # Sets the name of the path variable.
        log-lib
        log)

target_link_libraries( # Specifies the target library.
        project_name
        tflite-lib
        tflite-flex-lib
        ${log-lib})
```

tflite-lib.c
```
// Interpreter
TfLiteModel* model = TfLiteModelCreateFromFile(infer_model_path)
TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);
```

build_model.py
```
// convert tflite model
converter = tf.lite.TFLiteConverter.from_saved_model(temp_dir, signature_keys=list(sign_dict.keys()))
converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
        tf.lite.OpsSet.SELECT_TF_OPS     # enable TensorFlow ops.
]
converter.experimental_enable_resource_variables = True
tflite_model = converter.convert()
```


### Relevant log output

```shell
2022-08-12 03:51:58.303 3854-3854/com.example.project_name E/tflite: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
2022-08-12 03:51:58.303 3854-3854/com.example.project_name E/tflite: Node number 151 (FlexRestore) failed to prepare.
```

</details>"
57155,RuntimeError: Data adapters should be mutually exclusive for handling inputs in Streamlit,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Windows 10 21H2

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

Cuda 11.2.2/ cuDNN v8.1.0.77

### GPU model and memory

GeForce NVidia RTX 3060  16GB RAM

### Current Behaviour?

```shell
I am trying to run a classifier with a keras CNN model within Streamlit and it gives me the following error.

RuntimeError: Data adapters should be mutually exclusive for handling inputs. Found multiple adapters [<class 'keras.engine.data_adapter.GenericArrayLikeDataAdapter'>, <class 'keras.engine.data_adapter.GeneratorDataAdapter'>] to handle input: <class 'streamlit.delta_generator.DeltaGenerator'>, <class 'NoneType'>

I have uploaded cnn_ltsm.h5, image20.jpg and image21.jpg to my github, if someone wants to reproduce my problem. GitHub - bluetail14/Thermal_data_images_project
```


### Standalone code to reproduce the issue

```shell
# My code runs fine in Jupyter notebook.
import os
import numpy as np
from PIL import Image 
import tensorflow as tf
from tensorflow.keras.models import load_model

def load_image(image):
    image = image.resize((224,224))
    img_array = np.array(image)/255 # a normalised 2D array                
    img_array = img_array.reshape(-1, 224, 224, 3)   # to shape as (1, 224, 224, 3)
    return img_array

im_path = 'C:\\Users\\..\\oneman\\image21.jpg'
img = load_image(Image.open(im_path))

model_cnn_ltsm = load_model(""C:/Users/.../Saved_models/cnn_ltsm_model.h5"")
model_cnn_ltsm_ = tf.keras.models.Model(model_cnn_ltsm.inputs, model_cnn_ltsm.outputs)
pred_label = model_cnn_ltsm_.predict(img)[0] 

if pred_label>0.5:
    print('Human is detected')
else:
    print(""No human is detected: "")   

#Output: 'Human is detected' (as expected)

# However, there is a problem with the same in Streamlit.
import streamlit as st
import pandas as pd
import numpy as np
from numpy import vstack
from PIL import Image 
import tensorflow as tf
from tensorflow.keras.models import load_model


st.title(""Binary Human Detection Web App"")

# loading images
def load_image(image):

    image = image.resize((224,224))
    img_array = np.array(image)/255 # a normalised 2D array                
    img_array = img_array.reshape(-1, 224, 224, 3)   # to shape as (1, 224, 224, 3)
    return img_array


uploaded_file = st.sidebar.file_uploader("" "",type=['jpg', 'jpeg'])    

if uploaded_file is not None:
    
    u_img = load_image(Image.open(uploaded_file))
    img = st.image(u_img, 'Uploaded Image', use_column_width=True)

st.sidebar.write('\n')
    
if st.sidebar.button(""Click Here to Predict""):
    
    if uploaded_file is None:
        
        st.sidebar.write(""Please upload an Image to Classify"")

    else:
       
        model_cnn = load_model(""C:/Users/.../Saved_models/cnn_ltsm_model.h5"")
        model_cnn_ltsm_ = tf.keras.models.Model(model_cnn_ltsm.inputs, model_cnn_ltsm.outputs)
        pred_label = model_cnn_ltsm_.predict(img)[0] 


        st.sidebar.header(""CNN results: "")    
        #st.write('Human is detected') if pred_label>0.5 else  st.write('No human is detected')    
        if pred_label>0.5:
            st.write('Human is detected')
        else:
            st.write('No human is detected')
```


### Relevant log output

```shell
RuntimeError: Data adapters should be mutually exclusive for handling inputs. Found multiple adapters [<class 'keras.engine.data_adapter.GenericArrayLikeDataAdapter'>, <class 'keras.engine.data_adapter.GeneratorDataAdapter'>] to handle input: <class 'streamlit.delta_generator.DeltaGenerator'>, <class 'NoneType'>
Traceback:
File ""C:\Users\maria\anaconda3\envs\tfenv\lib\site-packages\streamlit\scriptrunner\script_runner.py"", line 557, in _run_script
    exec(code, module.__dict__)
File ""app1.py"", line 46, in <module>
    pred_label = model_cnn_.predict(img)[0]
File ""C:\Users\maria\anaconda3\envs\tfenv\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
File ""C:\Users\maria\anaconda3\envs\tfenv\lib\site-packages\keras\engine\data_adapter.py"", line 990, in select_data_adapter
    raise RuntimeError(
```
</details>"
57154,"tensorflow c++ api crashed during inferencing on arm64, but same codes is running well on x86_64","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf2.6.5 or  tf2.9.1

### Custom Code

No

### OS Platform and Distribution

centos7  arm64

### Mobile device

no

### Python version

3.8.10

### Bazel version

3.7.2

### GCC/Compiler version

gcc (GCC) 7.3.1 20180303 (Red Hat 7.3.1-6)

### CUDA/cuDNN version

no

### GPU model and memory

no

### Current Behaviour?

```shell
build commands as follow:
1. configure with all default settings
2. bazel build --jobs 4  --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=noaws --config=nohdfs --config=nonccl --config=nogcp --config=opt -c opt --copt=-O3 --copt=-funsafe-math-optimizations --copt=-ftree-vectorize  //tensorflow:libtensorflow_cc.so 


crash info on arm64，
#0  0x0000ffffafe650e8 in raise () from /lib64/libc.so.6
#1  0x0000ffffafe66760 in abort () from /lib64/libc.so.6
#2  0x0000ffffb0153348 in __gnu_cxx::__verbose_terminate_handler() () from /lib64/libstdc++.so.6
#3  0x0000ffffb0150c5c in ?? () from /lib64/libstdc++.so.6
#4  0x0000ffffb0150ca8 in std::terminate() () from /lib64/libstdc++.so.6
#5  0x0000ffffb0150f64 in __cxa_throw () from /lib64/libstdc++.so.6
#6  0x0000ffffb017a9d4 in std::__throw_bad_alloc() () from /lib64/libstdc++.so.6
#7  0x0000ffffb08be850 in allocate (this=<optimized out>, __n=<optimized out>) at /usr/lib/gcc/aarch64-redhat-linux/7/../../../../include/c++/7/ext/new_allocator.h:102
#8  allocate (__a=..., __n=<optimized out>) at /usr/lib/gcc/aarch64-redhat-linux/7/../../../../include/c++/7/bits/alloc_traits.h:436
#9  Allocate (capacity=<optimized out>, this=<synthetic pointer>) at external/com_google_absl/absl/container/internal/inlined_vector.h:218
#10 absl::lts_20210324::inlined_vector_internal::Storage<long, 4ul, std::allocator<long> >::Resize<absl::lts_20210324::inlined_vector_internal::DefaultValueAdapter<std::allocator<long> > > (this=this@entry=0xffffffffa390, values=..., 
    values@entry=..., new_size=18446744073709551615) at external/com_google_absl/absl/container/internal/inlined_vector.h:636
#11 0x0000ffffb09715f8 in resize (n=<optimized out>, this=0xffffffffa390) at external/com_google_absl/absl/container/inlined_vector.h:544
#12 BuildDenseSpec<int> (dense=0xffffffffa120, sparse=<synthetic pointer>) at tensorflow/core/util/strided_slice_op.cc:84
#13 tensorflow::ValidateStridedSliceOp (begin_tensor=begin_tensor@entry=0x44e7570, end_tensor=end_tensor@entry=0x44e7590, strides_tensor=..., input_shape=..., begin_mask_spec=-24656, end_mask_spec=72251600, ellipsis_mask=<optimized out>, 
    new_axis_mask=0, shrink_axis_mask=2, processing_shape=processing_shape@entry=0xffffffffa360, final_shape=final_shape@entry=0xffffffffa378, is_identity=0xffffffffa309, is_identity@entry=0xffffffffa369, is_simple_slice=0xffffffffa30a, 
    is_simple_slice@entry=0xffffffffa36a, slice_dim0=0xffffffffa30b, slice_dim0@entry=0xffffffffa36b, begin=0xffffffffa390, begin@entry=0xffffffffa3f0, end=0xffffffffa3b8, end@entry=0xffffffffa418, strides=0xffffffffa3e0, 
    strides@entry=0xffffffffa440, shape_spec=shape_spec@entry=0x0) at tensorflow/core/util/strided_slice_op.cc:260
#14 0x0000ffffbb41a6cc in tensorflow::<lambda(tensorflow::shape_inference::InferenceContext*)>::operator()(tensorflow::shape_inference::InferenceContext *) (c=0x44e77c0, __closure=<optimized out>) at tensorflow/core/ops/array_ops.cc:1770
#15 0x0000ffffbb41a930 in std::_Function_handler<tensorflow::Status(tensorflow::shape_inference::InferenceContext*), tensorflow::<lambda(tensorflow::shape_inference::InferenceContext*)> >::_M_invoke(const std::_Any_data &, <unknown type in /root/tfso/tensorflow-2.6.5/bazel-bin/tensorflow/libtensorflow_cc.so.2, CU 0xe8e31560, DIE 0xe8ee111e>) (__functor=..., __args#0=<optimized out>) at /usr/lib/gcc/aarch64-redhat-linux/7/../../../../include/c++/7/bits/std_function.h:302
#16 0x0000ffffb09b769c in operator() (__args#0=0x44e77c0, this=<optimized out>) at /usr/lib/gcc/aarch64-redhat-linux/7/../../../../include/c++/7/bits/std_function.h:706
#17 tensorflow::shape_inference::InferenceContext::Run(std::function<tensorflow::Status (tensorflow::shape_inference::InferenceContext*)> const&) (this=0x44e77c0, fn=...) at tensorflow/core/framework/shape_inference.cc:110
#18 0x0000ffffbb3bdf84 in tensorflow::grappler::SymbolicShapeRefiner::InferShapes (this=0x4981a60, this@entry=0x438, node=..., c=0xffffffffa910, c@entry=0x46fdd90) at tensorflow/core/grappler/costs/graph_properties.cc:1914
#19 0x0000ffffbb3c44e8 in tensorflow::grappler::SymbolicShapeRefiner::UpdateNode (this=0x438, this@entry=0x4981a60, node=node@entry=0x10906c90, refined=<optimized out>) at tensorflow/core/grappler/costs/graph_properties.cc:1123
#20 0x0000ffffbb3c53dc in tensorflow::grappler::GraphProperties::UpdateShapes (this=this@entry=0xffffffffba08, shape_refiner=0x4981a60, shape_refiner@entry=0x4981e60, resource_handles=..., n=n@entry=0x10906c90, new_shapes=0xffffffffaa2f, 
    new_shapes@entry=0x4981a60) at tensorflow/core/grappler/costs/graph_properties.cc:2354
#21 0x0000ffffbb3c5544 in tensorflow::grappler::GraphProperties::PropagateShapes (this=0xffffffffba08, this@entry=0x100de6d8, shape_refiner=0x4981e60, shape_refiner@entry=0x4981a60, new_shapes=0xffffffffaf98, new_shapes@entry=0x1016a6a0, 
    resource_handles=..., num_loops=num_loops@entry=269532928) at tensorflow/core/grappler/costs/graph_properties.cc:2387
#22 0x0000ffffbb3bfe98 in tensorflow::grappler::GraphProperties::InferStatically (this=0x100de6d8, this@entry=0xffffffffba08, assume_valid_feeds=<optimized out>, aggressive_shape_inference=aggressive_shape_inference@entry=false, 
    include_input_tensor_values=255, include_input_tensor_values@entry=false, include_output_tensor_values=12, include_output_tensor_values@entry=true) at tensorflow/core/grappler/costs/graph_properties.cc:2610
#23 0x0000ffffbb348d88 in tensorflow::grappler::ConstantFolding::RunOptimizationPass (this=this@entry=0xe84a960, cluster=cluster@entry=0x0, item=item@entry=0xffffffffbe60, optimized_graph=optimized_graph@entry=0xffffffffd488)
    at tensorflow/core/grappler/optimizers/constant_folding.cc:4009
#24 0x0000ffffbb349c88 in tensorflow::grappler::ConstantFolding::Optimize (this=<optimized out>, cluster=0x0, item=..., optimized_graph=0xffffffffd488) at tensorflow/core/grappler/optimizers/constant_folding.cc:4065
#25 0x0000ffffbb258fe8 in tensorflow::grappler::MetaOptimizer::RunOptimizer (this=0x0, this@entry=0xffffffffd1c0, optimizer=0xe84a960, cluster=0x0, cluster@entry=0xe84b9e0, optimized_item=optimized_item@entry=0xffffffffd558, 
    optimized_graph=optimized_graph@entry=0xffffffffd488, optimization_result=0x0, 
    optimization_result@entry=0xffffb5ddca18 <google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::~Map()+1008>) at tensorflow/core/grappler/optimizers/meta_optimizer.cc:780
#26 0x0000ffffbb25a24c in tensorflow::grappler::MetaOptimizer::OptimizeGraph(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem&&, tensorflow::GraphDef*) (this=this@entry=0xffffffffd1c0, cluster=0xe84b9e0, 
    cluster@entry=0xffffbb38ec38 <tensorflow::grappler::OpLevelCostEstimator::PredictGatherOrSlice(tensorflow::grappler::OpContext const&, tensorflow::grappler::NodeCosts*) const>, 
    item=item@entry=<unknown type in /root/tfso/tensorflow-2.6.5/bazel-bin/tensorflow/libtensorflow_cc.so.2, CU 0xe540eabf, DIE 0xe558ad2b>, optimized_graph=0xffffffffd488, 
    optimized_graph@entry=0xffffbb3904a0 <tensorflow::grappler::OpLevelCostEstimator::PredictScatter(tensorflow::grappler::OpContext const&, tensorflow::grappler::NodeCosts*) const>) at tensorflow/core/grappler/optimizers/meta_optimizer.cc:695
#27 0x0000ffffbb25b58c in tensorflow::grappler::MetaOptimizer::OptimizeConsumeItem(tensorflow::grappler::Cluster*, tensorflow::grappler::GrapplerItem&&, tensorflow::GraphDef*) (this=this@entry=0xffffffffd1c0, 
    cluster=0xffffbb38ec38 <tensorflow::grappler::OpLevelCostEstimator::PredictGatherOrSlice(tensorflow::grappler::OpContext const&, tensorflow::grappler::NodeCosts*) const>, cluster@entry=0xffffffffd868, 
    item=item@entry=<unknown type in /root/tfso/tensorflow-2.6.5/bazel-bin/tensorflow/libtensorflow_cc.so.2, CU 0xe540eabf, DIE 0xe557c114>, 
    optimized_graph=0xffffbb3904a0 <tensorflow::grappler::OpLevelCostEstimator::PredictScatter(tensorflow::grappler::OpContext const&, tensorflow::grappler::NodeCosts*) const>, optimized_graph@entry=0xffffffffd488)
    at tensorflow/core/grappler/optimizers/meta_optimizer.cc:915
#28 0x0000ffffbb25cb28 in tensorflow::grappler::RunMetaOptimizer(tensorflow::grappler::GrapplerItem&&, tensorflow::ConfigProto const&, tensorflow::DeviceBase*, tensorflow::grappler::Cluster*, tensorflow::GraphDef*) (
    item=item@entry=<unknown type in /root/tfso/tensorflow-2.6.5/bazel-bin/tensorflow/libtensorflow_cc.so.2, CU 0xe540eabf, DIE 0xe557a753>, cfg=..., cpu_device=cpu_device@entry=0x42cd840, cluster=cluster@entry=0xffffffffd868, 
    optimized_graph=optimized_graph@entry=0xffffffffd488) at tensorflow/core/grappler/optimizers/meta_optimizer.cc:1174
#29 0x0000ffffbb24d108 in tensorflow::GraphExecutionState::OptimizeGraph (this=0xffffb1577a98 <vtable for tensorflow::GraphDef+16>, this@entry=0x4df700, options=..., graph=..., flib_def=0xe9b0e0, 
    optimized_graph=0xffffb5ddca0c <google::protobuf::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::AttrValue>::~Map()+996>, optimized_graph@entry=0xffffffffdae0, optimized_flib=0xffffffffd6f8, 
    optimized_flib@entry=0xffffffffdb88) at tensorflow/core/common_runtime/graph_execution_state.cc:810
#30 0x0000ffffbb24d8e8 in tensorflow::GraphExecutionState::BuildGraph (this=this@entry=0x4df700, options=..., out=0xffffffffdc10, out@entry=0xffffffffded8) at tensorflow/core/common_runtime/graph_execution_state.cc:869
#31 0x0000ffffbb21d6e4 in tensorflow::DirectSession::CreateGraphs (this=this@entry=0x4316050, subgraph_options=..., outputs=outputs@entry=0xffffffffe3f8, flib_def=0xffffffffe700, flib_def@entry=0xda2d760, run_state_args=0xffffffffe5f8, 
    run_state_args@entry=0x0, input_types=0x4316050, input_types@entry=0xee3940, 
    output_types=0xffffbb224034 <tensorflow::DirectSession::RunCallable(long, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, tensorflow::thread::ThreadPoolOptions const&)+740>, output_types@entry=0xee3958, collective_graph_key=collective_graph_key@entry=0xee3ad0) at tensorflow/core/common_runtime/direct_session.cc:1601
#32 0x0000ffffbb21e724 in tensorflow::DirectSession::CreateExecutors (this=0x4316050, this@entry=0x10ac11b0, callable_options=..., out_executors_and_keys=0x0, out_executors_and_keys@entry=0xffffffffe7b8, out_func_info=0x0, 
    out_func_info@entry=0xffffffffe7c0, run_state_args=0x0, run_state_args@entry=0xffffffffec48) at tensorflow/core/common_runtime/direct_session.cc:1307
#33 0x0000ffffbb2200d8 in tensorflow::DirectSession::GetOrCreateExecutors (this=0x10ac11b0, this@entry=0x4316050, inputs=..., outputs=..., target_nodes=..., executors_and_keys=0xfffffffff088, executors_and_keys@entry=0xffffffffec58, 
    run_state_args=0xffffffffec48, run_state_args@entry=0xffffb1007e6c <tensorflow::MetaGraphDef::~MetaGraphDef()+244>) at tensorflow/core/common_runtime/direct_session.cc:1549
---Type <return> to continue, or q <return> to quit---
#34 0x0000ffffbb222dbc in tensorflow::DirectSession::Run (this=0x4316050, run_options=..., inputs=std::vector of length 1, capacity 1 = {...}, output_names=std::vector of length -56468846855708674, capacity -40997270446407952 = {...}, 
    target_nodes=std::vector of length 0, capacity 0, outputs=0x1f40, run_metadata=0xf9b790, threadpool_options=...) at tensorflow/core/common_runtime/direct_session.cc:849
#35 0x0000ffffbb21261c in tensorflow::DirectSession::Run (this=this@entry=0x4316050, run_options=..., inputs=std::vector of length 1, capacity 1 = {...}, output_names=std::vector of length 1, capacity 1 = {...}, 
    target_nodes=std::vector of length 0, capacity 0, outputs=outputs@entry=0xfffffffff0c0, run_metadata=run_metadata@entry=0xffffffffef48) at tensorflow/core/common_runtime/direct_session.cc:819
#36 0x0000ffffbb220cc8 in tensorflow::DirectSession::Run (this=0x4316050, inputs=std::vector of length 1, capacity 1 = {...}, output_names=std::vector of length 1, capacity 1 = {...}, target_nodes=std::vector of length 0, capacity 0, 
    outputs=0xfffffffff0c0) at tensorflow/core/common_runtime/direct_session.cc:462
#37 0x0000ffffb082f3cc in tensorflow::(anonymous namespace)::LiteSessionWrapper::Run (this=<optimized out>, inputs=..., output_tensor_names=..., target_node_names=..., outputs=<optimized out>) at tensorflow/cc/saved_model/loader.cc:348
#38 0x0000000000403c38 in main ()
```


### Standalone code to reproduce the issue

```shell
#include <cinttypes>
#include <cstdlib>
#include <iostream>
#include <map>
#include <string>
#include <vector>

#include <tensorflow/cc/client/client_session.h>
#include <tensorflow/cc/ops/standard_ops.h>
#include <tensorflow/cc/saved_model/loader.h>
#include <tensorflow/cc/saved_model/tag_constants.h>
#include <tensorflow/core/framework/tensor.h>
#include <tensorflow/core/platform/env.h>
#include <tensorflow/core/public/session.h>

#include <npy.hpp>

std::string GetDimDescription(const std::vector<unsigned long> &shape) {
    std::ostringstream oss;
    oss << ""("";
    for (size_t i = 0; i < shape.size(); i++) {
        oss << shape[i];
        if (i + 1 != shape.size()) oss << "","";
    }
    oss << "")"";
    return oss.str();
}

int64_t GetNowMs() {
    return std::chrono::time_point_cast<std::chrono::milliseconds>(std::chrono::system_clock::now())
        .time_since_epoch()
        .count();
}

static const std::string input_mel_name = ""serving_default_mels:0"";
static const std::string output_wave_name = ""StatefulPartitionedCall:0"";

int main(int argc, char const *argv[]) {
    if (argc != 3) {
        std::cerr << ""usage: hifigan save_model_path run_time\n"";
        return -1;
    }
    std::string save_model_path(argv[1]);
    int32_t run_time = atoi(argv[2]);
    if (run_time <= 1) run_time = 1;

    tensorflow::SavedModelBundleLite bundle;
    tensorflow::SessionOptions session_options;
    tensorflow::RunOptions run_options;

    //加载模型
    auto status = tensorflow::LoadSavedModel(
        session_options, run_options, save_model_path, {tensorflow::kSavedModelTagServe}, &bundle);
    if (!status.ok()) {
        std::cout << ""load model failed, path:"" << save_model_path << "",msg:"" << status.error_message() << std::endl;
        return -1;
    }

    //预测预热
    tensorflow::Tensor input_mel(tensorflow::DT_FLOAT, tensorflow::TensorShape({1, 100, 80}));

    std::vector<std::pair<std::string, tensorflow::Tensor>> inputs;
    inputs.push_back(std::pair<std::string, tensorflow::Tensor>(input_mel_name, input_mel));

    std::vector<std::string> output_node_names = {output_wave_name};
    std::vector<tensorflow::Tensor> outputs;
    bundle.GetSession()->Run(inputs, output_node_names, {}, &outputs);

    //推理模型
    int64_t start_ms = GetNowMs();
    for (int32_t i = 0; i < run_time; i++) {
        bundle.GetSession()->Run(inputs, output_node_names, {}, &outputs);
    }
    double use_time = (GetNowMs() - start_ms) * 1.0 / run_time;
    std::cout << ""run time:"" << use_time << std::endl;

    return 0;
}
```


### Relevant log output

```shell
no
```
</details>"
57153,The type of parameter 'device_type' in tf.config.list_logical_devices wrong,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

TF2.4

### Custom Code

Yes

### OS Platform and Distribution

win11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
device_type is optional string on documentation. However, I find that when device_type is list/int/float or other type, code works. It just returns an empty list. So type restrictions should be removed from the documentation.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
results={}
try:
  arg_0 = -51
  results[""res""] = tf.config.list_logical_devices(arg_0,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
# results = {'res': []}
```


### Relevant log output

_No response_</details>"
57152,how to add callbacks without using model.fit in tf2,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf2.2.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
i want to use profilier(in tensorboard) in my code and i find it is always wrapped in the callbacks like below. while I use the custom training ""with tf.GradientTape() as tape:"" not using  
model.fit. I wonder how can I add callbacks in my code without using model.fit api. Thankyou so much.
```


### Standalone code to reproduce the issue

```shell
def train_step(inputs):
with tf.GradientTape() as tape:
                hypothesis, label_rank = cal_weights(inputs)
                local_loss = self.compute_loss(label_rank=label_rank, hypothesis=hypothesis)
            gradients = tape.gradient(local_loss, self.model.trainable_variables)
            self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
            return local_loss


callbacks = [
     tf.keras.callbacks.TensorBoard(log_dir='./logs'),
     tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,
                                        save_weights_only=True),
     tf.keras.callbacks.LearningRateScheduler(decay),
]
```


### Relevant log output

_No response_</details>"
57151,tf.clip_by_value allows clip_value_min greater than clip_value_max,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

TF2.4

### Custom Code

Yes

### OS Platform and Distribution

win11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When clip_value_min is greater than clip_value_max, the snippet code works but behaves abnormally. I think it should throw exception or add an example in documentation to illustrate what will happen when clip_value_min > clip_value_max.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
results={}
try:
  arg_0 = tf.random.uniform([2, 3], maxval=10, dtype=tf.float32)
  print(arg_0)
  '''
  arg_0 = tf.Tensor(
[[4.7769785  0.57591677 4.4995    ]
 [0.93842626 4.697542   2.146771  ]], shape=(2, 3), dtype=float32)
  '''
  clip_value_min = 1
  clip_value_max = -1
  results[""res""] = tf.clip_by_value(arg_0,clip_value_min=clip_value_min,clip_value_max=clip_value_max,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
'''
  results = {'res': <tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[1., 1., 1.],
       [1., 1., 1.]], dtype=float32)>}
'''
```


### Relevant log output

_No response_</details>"
57150,update documentation of tf.abs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

TF2.4

### Custom Code

Yes

### OS Platform and Distribution

win11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The input of tf.abs shoule be a Tensor or SparseTensor of type float16, float32, float64, int32, int64, complex64 or complex128 on documentation. That is to say, input doesn't support the type of int8 tensor. So if I input a int8 tensor, it would throw exception. But I find that when I input int8 tensor, it works well. I think it would be better to add an example to illustrate how the code works in the case of int8 or add support for int8 type on documentation.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
results={}
try:
  arg_0 = tf.saturate_cast(tf.random.uniform([], minval=0, maxval=2, dtype=tf.int64), dtype=tf.int8)
  print(arg_0)
  # tf.Tensor(1, shape=(), dtype=int8)
  results[""res""] = tf.abs(arg_0,)
except Exception as e:
  results[""err""] = ""Error:""+str(e)
print(results)
# {'res': <tf.Tensor: shape=(), dtype=int8, numpy=1>}
```


### Relevant log output

_No response_</details>"
57149,`Transformers` : Error while fitting TFBertForSequenceClassification model (via tensorflow-directml-plugin),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tensorflow-cpu 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

windows 11 PRO 64bit : 21H2

### Mobile device

_No response_

### Python version

3.8.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

tensorflow-directml-plugin 0.0.1.dev220621

### GPU model and memory

RX 6800 16Go

### Current Behaviour?

```shell
Hi,

First of all thanks a lot making directML compatible with TensorFlow > 2 !
As the Transformers lib needs TensorFlow >= 2.3 i had got hope my TFBertForSequenceClassification model was working with my AMD GPU card (Rx 6800)

Unfortuntly, this is not the case. (important detail : it takes a crazy long time but works on CPU with standard Tensorflow lib)

My env: 
windows 11 PRO 64bit : 21H2
python 3.8.13
tensorflow-cpu 2.9.1
tensorflow-directml-plugin 0.0.1.dev220621
CPU : Ryzen 5600X, 
GPU : Rx 6800


The GPU is rightly recognized: 

tf.config.list_physical_devices('GPU')
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]


I'm expecting model fit working as it does with the CPU on another similar env. but with standard tensorflow lib
```


### Standalone code to reproduce the issue

```shell
My code : 

bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,
                                                save_weights_only=True,
                                                monitor='val_loss',
                                                mode='min',
                                                save_best_only=True),
             keras.callbacks.TensorBoard(log_dir=log_dir)]

print('\nBert Model', bert_model.summary())

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08)

bert_model.compile(loss=loss, optimizer=optimizer, metrics=metric)

#OUTPUT : 
Model: ""tf_bert_for_sequence_classification_3""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 bert (TFBertMainLayer)      multiple                  109482240 
                                                                 
 dropout_151 (Dropout)       multiple                  0         
                                                                 
 classifier (Dense)          multiple                  1538      
                                                                 
=================================================================
Total params: 109,483,778
Trainable params: 109,483,778
Non-trainable params: 0
_________________________________________________________________



Above code works well but when reaching the fit : 

```python
history=bert_model.fit([X_train, Mask_Train], 
                       y_train,
                       batch_size=32,
                       epochs=EPOCHS,
                       validation_data=([X_test, Mask_test], y_test),
                       callbacks=callbacks)
```

`The GPU VRAM begins to receive data` (i'm monitoring it via Radeon Adrenalin Software) and suddenly an error message (see below) appears !


Thanks in advance for any Help !
Have a good day.
```


### Relevant log output

```shell
The Error Message : 


Epoch 1/3
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
File <timed exec>:2, in <module>

File ~\anaconda3\envs\P7_Bert_TF29_PYT38\lib\site-packages\keras\utils\traceback_utils.py:67,
 in filter_traceback.<locals>.error_handler(*args, **kwargs)
     65 except Exception as e:  # pylint: disable=broad-except
     66   filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67   raise e.with_traceback(filtered_tb) from None
     68 finally:
     69   del filtered_tb

File ~\anaconda3\envs\P7_Bert_TF29_PYT38\lib\site-packages\tensorflow\python\eager\execute.py:54, 
 in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     52 try:
     53   ctx.ensure_initialized()
---> 54   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     55                                       inputs, attrs, num_outputs)
     56 except core._NotOkStatusException as e:
     57   if name is not None:

InvalidArgumentError: Cannot assign a device for operation tf_bert_for_sequence_classification_3/bert/embeddings/Gather: 
Could not satisfy explicit device specification '' 
because the node {{colocation_node tf_bert_for_sequence_classification_3/bert/embeddings/Gather}} was colocated 
with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. 
All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. 
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=2 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' 
assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' 
resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]

StridedSlice: CPU 
Unique: GPU CPU 
Shape: GPU CPU 
_Arg: GPU CPU 
ResourceGather: GPU CPU 
Const: GPU CPU 
UnsortedSegmentSum: CPU 
Mul: GPU CPU 
ReadVariableOp: GPU CPU 
AssignVariableOp: GPU CPU 
ResourceScatterAdd: GPU CPU 
Sqrt: GPU CPU 
AddV2: GPU CPU 
RealDiv: GPU CPU 
AssignSubVariableOp: GPU CPU 
NoOp: GPU CPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  tf_bert_for_sequence_classification_3_bert_embeddings_gather_resource (_Arg)  
       framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  adam_adam_update_readvariableop_resource (_Arg)  
       framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  adam_adam_update_readvariableop_2_resource (_Arg)  
       framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  tf_bert_for_sequence_classification_3/bert/embeddings/Gather (ResourceGather) 
  Adam/Adam/update/Unique (Unique) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/Shape (Shape) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/strided_slice/stack (Const) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/strided_slice/stack_1 (Const) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/strided_slice/stack_2 (Const) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/strided_slice (StridedSlice) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/UnsortedSegmentSum (UnsortedSegmentSum) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/mul (Mul) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/ReadVariableOp (ReadVariableOp) 
  Adam/Adam/update/mul_1 (Mul) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/AssignVariableOp (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/ResourceScatterAdd (ResourceScatterAdd) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/ReadVariableOp_1 (ReadVariableOp) 
  Adam/Adam/update/mul_2 (Mul) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/mul_3 (Mul) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/ReadVariableOp_2 (ReadVariableOp) 
  Adam/Adam/update/mul_4 (Mul) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/AssignVariableOp_1 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/ResourceScatterAdd_1 (ResourceScatterAdd) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/ReadVariableOp_3 (ReadVariableOp) 
  Adam/Adam/update/Sqrt (Sqrt) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/mul_5 (Mul) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/add (AddV2) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/truediv (RealDiv) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/AssignSubVariableOp (AssignSubVariableOp) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/group_deps/NoOp (NoOp) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/group_deps/NoOp_1 (NoOp) /job:localhost/replica:0/task:0/device:GPU:0
  Adam/Adam/update/group_deps (NoOp) /job:localhost/replica:0/task:0/device:GPU:0

      [[{{node tf_bert_for_sequence_classification_3/bert/embeddings/Gather}}]] 
      [Op:__inference_train_function_57566]
```
</details>"
57148,tf.sparse.reduce_max should update the static shape,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.sparse.reduce_max(..., output_is_sparse=True) does not fill the result's static shape, although it can easily be done (like in the output_is_sparse=False case).
```


### Standalone code to reproduce the issue

```shell
@tf.function
def f(n):
  t = tf.sparse.eye(n)
  tf.print(t.shape)
  result = tf.sparse.reduce_max(t, axis=1, keepdims=True, output_is_sparse=True)
  tf.print(result.shape)


f(3)
```


### Relevant log output

```shell
TensorShape([3, 3])
TensorShape(None)
```
</details>"
57146,"Build Failure re: Intel MKL (building with ""--config=mkl"" flag); Missing ""mkl_cblas.h""?","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

Docker Image: tensorflow/tensorflow:latest-devel-gpu
TF 2.10

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

5.1.1

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

CUDA 11.2, cuDNN 8.1.0

### GPU model and memory

RTX 2070 Super 8GB

### Current Behaviour?

```shell
A bug happened!

Compiled tensorflow 2.10.0 with --config=mkl flag, expecting to compile successfully with Intel MKL libraries. System threw the following error:

In file included from tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:48:
./tensorflow/compiler/xla/service/cpu/runtime_matmul_mkl.h:22:10: fatal error: third_party/intel_mkl_ml/include/mkl_cblas.h: No such file or directory
   22 | #include ""third_party/intel_mkl_ml/include/mkl_cblas.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.

FAILED: Build did NOT complete successfully
```


### Standalone code to reproduce the issue

```shell
./configure

You have bazel 5.1.1 installed.
Please specify the location of python. [Default is /usr/bin/python3]: 


Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.8/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Found CUDA 11.2 in:
    /usr/local/cuda-11.2/targets/x86_64-linux/lib
    /usr/local/cuda-11.2/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include
Found TensorRT 7 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include/x86_64-linux-gnu


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 7.5,8.6

Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v1          	# Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=nogcp       	# Disable GCP support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished

bazel build --verbose_failures --config=opt --config=mkl --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=213
INFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false
INFO: Reading rc options for 'build' from /tensorflow_src/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=tensorrt --action_env TF_CUDA_VERSION=11.2 --action_env TF_CUDNN_VERSION=8 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5,8.6 --action_env LD_LIBRARY_PATH=/usr/local/cuda-11.0/targets/x86_64-linux/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/include/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.2/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 --config=cuda
INFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:tensorrt in file /tensorflow_src/.bazelrc: --repo_env TF_NEED_TENSORRT=1
INFO: Found applicable config definition build:cuda in file /tensorflow_src/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file /tensorflow_src/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:mkl in file /tensorflow_src/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt
INFO: Found applicable config definition build:linux in file /tensorflow_src/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow_src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Build option --cxxopt has changed, discarding analysis cache.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded, 37117 targets configured).
INFO: Found 1 target...
ERROR: /tensorflow_src/tensorflow/compiler/xla/service/cpu/BUILD:345:11: Compiling tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 \
    LD_LIBRARY_PATH=/usr/local/cuda-11.0/targets/x86_64-linux/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/include/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.2/lib64 \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-5.1.1-linux-x86_64/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=7.5,8.6 \
    TF_CUDA_VERSION=11.2 \
    TF_CUDNN_VERSION=8 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/simple_orc_jit/simple_orc_jit.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/simple_orc_jit/simple_orc_jit.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT="".so""' '-DLLVM_PLUGIN_EXT="".so""' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_DEREGISTER_FRAME=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_H=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_SETENV_R=1' '-DHAVE_STRERROR_R=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_LSEEK64=1' '-DHAVE_MALLINFO=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' '-DLLVM_NATIVE_ARCH=""X86""' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' '-DLLVM_HOST_TRIPLE=""x86_64-unknown-linux-gnu""' '-DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64-unknown-linux-gnu""' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS '-DBLAKE3_USE_NEON=0' -DBLAKE3_NO_AVX2 -DBLAKE3_NO_AVX512 -DBLAKE3_NO_SSE2 -DBLAKE3_NO_SSE41 -DGEMM_KERNEL_H '-DEIGEN_ALTIVEC_USE_CUSTOM_PACK=0' -iquote . -iquote bazel-out/k8-opt/bin -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/snappy -iquote bazel-out/k8-opt/bin/external/snappy -iquote external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/local_config_rocm -iquote bazel-out/k8-opt/bin/external/local_config_rocm -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -iquote external/llvm-project -iquote bazel-out/k8-opt/bin/external/llvm-project -iquote external/llvm_terminfo -iquote bazel-out/k8-opt/bin/external/llvm_terminfo -iquote external/llvm_zlib -iquote bazel-out/k8-opt/bin/external/llvm_zlib -iquote external/mkl_dnn_v1 -iquote bazel-out/k8-opt/bin/external/mkl_dnn_v1 -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinLocationAttributesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypeInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/FunctionInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/RegionKindInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SubElementInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorEncodingIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/InferIntRangeInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AsmParserTokenKinds -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLTypesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLInterpOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/InstCombineTableGen -Ibazel-out/k8-opt/bin/external/llvm-project/llvm/_virtual_includes/JITLinkTableGen -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_rocm/rocm -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -isystem external/llvm-project/llvm/include -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include -isystem external/llvm-project/mlir/include -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/include -isystem external/mkl_dnn_v1/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/include -isystem external/mkl_dnn_v1/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src -isystem external/mkl_dnn_v1/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/common -isystem external/mkl_dnn_v1/src/common/ittnotify -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/common/ittnotify -isystem external/mkl_dnn_v1/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu -isystem external/mkl_dnn_v1/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu/gemm -isystem external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS -Wno-sign-compare '-std=c++17' '-D_GLIBCXX_USE_CXX11_ABI=0' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_NVCC=1' '-DTENSORFLOW_USE_XLA=1' '-DGOOGLE_TENSORRT=1' -DINTEL_MKL -DENABLE_MKL -DENABLE_ONEDNN_OPENMP -msse3 -pthread -c tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc -o bazel-out/k8-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/simple_orc_jit/simple_orc_jit.pic.o)
# Configuration: 75b12b214410a34451b4da0e90e0b5419b5fdf8f4ee0dcc34cde638c3092fa7d
# Execution platform: @local_execution_config_platform//:platform
In file included from tensorflow/compiler/xla/service/cpu/simple_orc_jit.cc:48:
./tensorflow/compiler/xla/service/cpu/runtime_matmul_mkl.h:22:10: fatal error: third_party/intel_mkl_ml/include/mkl_cblas.h: No such file or directory
   22 | #include ""third_party/intel_mkl_ml/include/mkl_cblas.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1440.944s, Critical Path: 158.25s
INFO: 10493 processes: 34 internal, 10459 local.
FAILED: Build did NOT complete successfully
```
</details>"
57145,[TF2] Trying to fit an array of dictionaries into my model as prediction or training data to a DenseFeatures layer,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

v2.9.0-rc2-42-g8a20d54a3c1 2.9.0

### Custom Code

No

### OS Platform and Distribution

Windows 11 Pro

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Trying to fit an array of dictionaries into my model, which has a DenseFeature layer as first layer.
I've tried also changing the input_shape of the DenseFeature but gives an error (look at #INPUT_SHAPE error in log output) and  when giving it for prediction an array of dict [{},{}] gives another error (look at #ARRAY_DICT).

What I've expected was just like any model.fit x_train array, an array of train arrays with data in them, but it doesnt work like that and I dont know how to modify my code on this problem.
Thanks in advance.
```


### Standalone code to reproduce the issue

```shell
model = tf.keras.Sequential([
    tf.keras.layers.DenseFeatures([
        tf.feature_column.numeric_column('wf',shape=(47,)),
        tf.feature_column.numeric_column('cf',shape=(5,)),
        tf.feature_column.numeric_column('crl_avg'),
        tf.feature_column.numeric_column('crl_long'),
        tf.feature_column.numeric_column('crl_total'),
    ]),
    
    tf.keras.layers.Dense(40,activation='relu'),
    tf.keras.layers.Dense(30,activation='relu'),
    tf.keras.layers.Dense(10,activation='relu'),
    tf.keras.layers.Dense(1,activation='sigmoid'),
])
model.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer=tf.keras.optimizers.Adam(),metrics=[tf.keras.metrics.Accuracy()])

model.predict([x_set[0],x_set[1]])
```


### Relevant log output

```shell
#INPUT_SHAPE error

ValueError: Exception encountered when calling layer ""dense_features_11"" (type DenseFeatures).

We expected a dictionary here. Instead we got: 

Call arguments received by layer ""dense_features_11"" (type DenseFeatures):
  • features=tf.Tensor(shape=(None, 16), dtype=float32)
  • cols_to_output_tensors=None
  • training=False

#ARRAY_DICT error

ValueError: Exception encountered when calling layer ""dense_features_12"" (type DenseFeatures).
    
    We expected a dictionary here. Instead we got: 
    
    Call arguments received by layer ""dense_features_12"" (type DenseFeatures):
      • features=({'wf': 'tf.Tensor(shape=(None, 47), dtype=float32)', 'cf': 'tf.Tensor(shape=(None, 5), dtype=float32)', 'crl_avg': 'tf.Tensor(shape=(None,), dtype=float32)', 'crl_long': 'tf.Tensor(shape=(None,), dtype=float32)', 'crl_total': 'tf.Tensor(shape=(None,), dtype=float32)'}, {'wf': 'tf.Tensor(shape=(None, 47), dtype=float32)', 'cf': 'tf.Tensor(shape=(None, 5), dtype=float32)', 'crl_avg': 'tf.Tensor(shape=(None,), dtype=float32)', 'crl_long': 'tf.Tensor(shape=(None,), dtype=float32)', 'crl_total': 'tf.Tensor(shape=(None,), dtype=float32)'})
      • cols_to_output_tensors=None
      • training=False
```
</details>"
57144,TF2 run out of GPU memory when doing GradientTape.jacobian,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tf 2.5

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am now trying to use TF2 to caculate gradients for every single entry of a matirx with the following code:

  with tf.GradientTape() as g:
      g.watch(nn_model.trainable_variables)
      out1 = nn_model(x)
      out2 = nn_model(y)
      matrix = generate (nn_model,out1,out2)

  jac = g.jacobian(matrix,nn_model.trainable_variables)

The matrix is pretty big (56x56), and the jacobian step keeps running out of GPU memory. And I have 2 RTX3090 on the server, so is there any technique that I can use to utilize both GPU's memory? Say, I can use up to 2xRTX3090's memory to expand the total capacity.
```


### Standalone code to reproduce the issue

```shell
Here is a minimal example:

import tensorflow.compat.v2 as tf

class NN(tf.keras.Model):
  """"""The convolutional network used to compute the agent's policy.""""""

  def __init__(self,
               num_actions,
               name = None,
               dropout = 0.0,
               rand_conv = False,
               projection = True,
               **kwargs):
    super(NN, self).__init__(**kwargs)
    self.kwargs = kwargs
    self._dropout = dropout
    self._num_actions = num_actions
    self.rand_conv = None
    self._projection = projection
    # Defining layers.
    activation_fn = tf.keras.activations.relu
    self.conv0 = tf.keras.layers.Conv2D(
        32, [8, 8],
        strides=4,
        padding='same',
        activation=activation_fn,
        name='Conv')
    self.conv1 = tf.keras.layers.Conv2D(
        64, [4, 4],
        strides=2,
        padding='same',
        activation=activation_fn,
        name='Conv1')
    self.conv2 = tf.keras.layers.Conv2D(
        64, [3, 3],
        strides=1,
        padding='same',
        activation=activation_fn,
        name='Conv2')
    self.flatten = tf.keras.layers.Flatten()
    self.dense0 = tf.keras.layers.Dense(256, activation=activation_fn)
    self.dense1 = tf.keras.layers.Dense(64, activation=activation_fn)
    self.dense2 = tf.keras.layers.Dense(num_actions, name='fully_connected')

  @tf.function
  def call(self, state, training=True):
    x = self.representation(state, projection=False)
    if training:
      x = tf.nn.dropout(x, rate=self._dropout)
    x = self.dense2(x)
    if self._num_actions == 1:
      x = tf.squeeze(x, axis=-1)
    return x

  def representation(self, state, projection=True):
    x = tf.cast(state, tf.float32)
    if self.rand_conv is not None:
      x = self.rand_conv(x)
    x = self.conv0(x)
    x = self.conv1(x)
    x = self.conv2(x)
    x = self.flatten(x)
    x = self.dense0(x)
    if projection and self._projection:
      x = self.dense1(x)
    return x

nn_model = NN(
      num_actions=2, dropout=float(0.0), rand_conv=True,
      projection=True)

fake1 = tf.random.uniform([56,60,60,2])
fake2 = tf.random.uniform([56,60,60,2])

with tf.GradientTape() as g:
    g.watch(nn_model.trainable_variables)
    out1 = nn_model.representation(fake1)
    out2 = nn_model.representation(fake2)
    # Say we use a cosine-smilarity to 
    cos = tf.keras.losses.CosineSimilarity(axis=2, reduction=tf.keras.losses.Reduction.NONE)
    m = cos(tf.expand_dims(out1, 1), tf.expand_dims(out2, 0))

jac = g.jacobian(m,nn_model.trainable_variables)
```


### Relevant log output

```shell
TF reports hundreds of lines error info, I guess the main idea is:

tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[3136,56,8,8,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[node gradient_tape/ReluGrad_5/pfor/Tile (defined at /RLgen-MaaM/jumping_task/train_minimal_debug.py:82) ]]
```
```
</details>"
57142,Problem with constructor of tensor,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf2.9.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/core/framework/tensor.cc#L699

There is a problem with the constructor of tensor, passing datatype to shape.
```


### Standalone code to reproduce the issue

```shell
Tensor::Tensor(DataType type) : shape_(type), buf_(nullptr) {}

https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/core/framework/tensor.cc#L699
```


### Relevant log output

_No response_</details>"
57133,[Win] fft2d tries to link m.lib,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Cannot compile tensorflow lite on Windows using CMake build system. Fails at linker stage while trying to link m.lib - the math library is only suitable for linking on macOS and Linux.

Will provide a patch for this issue.
```


### Standalone code to reproduce the issue

```shell
Build on any 2.9.x branch on Windows using CMake - build will fail while trying to link non-existent m.lib at fft2d linking stage.
```


### Relevant log output

_No response_</details>"
57131,[Android]: Internal error: Cannot create interpreter: tensorflow/lite/core/subgraph.cc BytesRequired number of elements overflowed.,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):  
  (1) google/sdk_gphone64_arm64/emu64a:13/TPB4.220624.004/8808248:userdebug/dev-keys
  (2) Nokia/TA-1012_00WW/NB1:9/PPR1.180610.011/00WW_5_15K:user/release-keys
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`): 2.9.0
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):  
  (1) 22.18.21 (190400-453244992)
  (2) 22.26.15 (100400-461192076)

**Standalone code to reproduce the issue**
I'm trying to load a tflite model with dynamic input size of (1, None, None, 1) on _Android_ as follows:

```kotlin
import org.tensorflow.lite.Interpreter

companion object {
   private const val MODEL_PATH = ""myModel.tflite""
}

val model = FileUtil.loadMappedFile(context, MODEL_PATH)

val options = Interpreter.Options()
options.numThreads = 5
// not using GPU or NNAPI because of missing dynamic tensor support
interpreter = Interpreter(model, options)
```

When instantiating the `Interpreter` I get the following error:

```
Internal error: Cannot create interpreter: tensorflow/lite/core/subgraph.cc BytesRequired number of elements overflowed.
Node number 0 (CONV_2D) failed to prepare.
```

When I run the exact same model using _Python_ it works as intended:

```python
from tensorflow.lite.python.interpreter import Interpreter

tf_lite_model = Interpreter(model_path=model_path)
tf_lite_model.resize_tensor_input(0, image.shape, strict=True) # image.shape - e.g.: (1, 43, 85, 1)
tf_lite_model.allocate_tensors()
```

When I skip the `resize_tensor_input()` operation in _Python_ (2. line) I get the **exact same error message** as in _Android_ which leads me to believe that the problem might be linked to the _Android_ interpreter's [allocate tensors](https://github.com/tensorflow/tensorflow/blob/4ad6160cf20af654c5e9eb553408cdd606b4da18/tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java#L127) step within its [constructor](https://github.com/tensorflow/tensorflow/blob/4ad6160cf20af654c5e9eb553408cdd606b4da18/tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java#L75) before resizing the input tensor.

**Any other info / logs**
Using _Android's_ ML Binding leads to the exact same error message.

```kotlin
val model = myModel.newInstance(context)
```

TFLite model input details:

```
[
  {
    'name': 'serving_default_image',
    'index': 0,
    'shape': array([1, 1, 1, 1], dtype=int32),
    'shape_signature': array([ 1, -1, -1,  1], dtype=int32),
    'dtype': <class 'numpy.float32'>,
    'quantization': (0.0, 0),
    'quantization_parameters': {
      'scales': array([], dtype=float32), 
      'zero_points': array([], dtype=int32), 
      'quantized_dimension': 0
    },
    'sparsity_parameters': {}
  }
]
```

Extracted from _Python's_ interpreter:

```python
from tensorflow.lite.python.interpreter import Interpreter

tf_lite_model = Interpreter(model_path=model_path)
input_details = tf_lite_model.get_input_details()
```

I do however succeed to instantiate the _Android_ interpreter when I use a static input tensor size, e.g. `(1, 50, 150, 1)`. Resizing the input tensor then to the desired size works as well:

```kotlin
import org.tensorflow.lite.Interpreter

val model = FileUtil.loadMappedFile(context, MODEL_PATH)
val options = Interpreter.Options()
options.numThreads = 5
interpreter = Interpreter(model, options)

interpreter.resizeInput(0, intArrayOf(1, bitmap.height, bitmap.width, 1), false)
interpreter.allocateTensors()
```

Any idea how to solve this (maybe except using static input tensor shape)?"
57129,Tensorlow installation issue,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

latest

### Custom Code

Yes

### OS Platform and Distribution

windows

### Mobile device

_No response_

### Python version

3,9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Intel (R) UHD Graphics 630

### Current Behaviour?

```shell
@gadagashwini

ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```


### Standalone code to reproduce the issue

```shell
pip install tensorflow in virtual environment.
```


### Relevant log output

_No response_</details>"
57128,Incorrect output shape when using dilations and strides > 1 with depthwise conv,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.4 - v2.4.0-rc4-71-g582c8d236cb 2.4.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04.6 LTS

### Mobile device

_No response_

### Python version

3.8.0

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.0

### GPU model and memory

_No response_

### Current Behaviour?


I am building a model which uses `strides` > 1 and `dilation_rate` > 1 in some DepthwiseConv2D layers.

However, with certain values, it seems like the output shape is incorrect.

Note: On the documentation, I see that using dilation_rate > 1 with strides > 1 is invalid, however, no runtime error is raised, in contrast with Conv2D, where the error is raised.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input = tf.keras.layers.Input(
            shape=(136, 136, 72),
            batch_size=1,
        )
x = tf.keras.layers.DepthwiseConv2D(
      kernel_size=3,
      strides=2,
      activation=None,
      use_bias=False,
      padding='valid',
      dilation_rate=(3, 3))(input)

print(f""output shape: {x.shape}"")
```

The print statement shows: 
```
output shape: (1, 64, 64, 72)
```

Interestingly, when doing the following:
```
conv = tf.keras.layers.DepthwiseConv2D(
      kernel_size=3,
      strides=2,
      activation=None,
      use_bias=False,
      padding='valid',
      dilation_rate=(3, 3))

conv.compute_output_shape((1, 136, 136, 72))
```

The output reported is: 
```
(1, 65, 65, 72)
```

Interestingly, when using dilation_rate =(2, 2) instead, the outputs reported via these two examples, match.


Following the equation in: https://www.tensorflow.org/api_docs/python/tf/nn#notes_on_padding_2

for valid padding, the output shape is calculated as:

```
output = ceil((in_height - filter_height + 1) / stride_height)
```

where filter_height when using dilations is computed as:

```
filter_height: dilation*(filter_height - 1) + 1
```

Following these equations, the output shape as reported by `compute_output_shape` is correct. However, it doesnt match the shape reported when actually forwarding through the layer.

I also tried implementing this in Pytorch, and output shape reported matches the one reported by `compute_output_shape`

This might be related to this issue: https://github.com/keras-team/tf-keras/issues/79

Is this a bug ? I need to know how the output shape is computed, as I need to be able to calculate a certain amount of padding such to ensure the downsampling caused by the stride results in a certain shape.

Thanks in advance for your support!



### Relevant log output

_No response_</details>"
57127, Compile failed with c++17 using gcc,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.7

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.6.8

### Bazel version

4.2.1

### GCC/Compiler version

8.3.1

### CUDA/cuDNN version

11.0/8

### GPU model and memory

Nvidia A100

### Current Behaviour?

```shell
I compiled tf 2.7 with command: ""bazel build --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --cxxopt=-std=c++1z //tensorflow/tools/pip_package:build_pip_package"" and this error occured:
ImportError: /root/.cache/bazel/_bazel_root/75033a8ead962ac817396f45646bf4d8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow11GetNodeAttrERKNS_9AttrSliceEN4absl12lts_2021032411string_viewEPNS_18PartialTensorShapeE

I used c++ 17 since I needed to import rocksdb(c++17 required) in my project, and followed this 
 issue(https://github.com/tensorflow/tensorflow/issues/23561) I rebuilt tf.
Any workaround suggest?
```


### Standalone code to reproduce the issue

```shell
bazel build --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --cxxopt=-std=c++1z //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /root/git/tensorflow/tensorflow/BUILD:1258:19: Executing genrule //tensorflow:tf_python_api_gen_v2 failed (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/75033a8ead962ac817396f45646bf4d8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /root/.cache/bazel/_bazel_root/75033a8ead962ac817396f45646bf4d8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow11GetNodeAttrERKNS_9AttrSliceEN4absl12lts_2021032411string_viewEPNS_18PartialTensorShapeE

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/75033a8ead962ac817396f45646bf4d8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 26, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/root/.cache/bazel/_bazel_root/75033a8ead962ac817396f45646bf4d8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/root/.cache/bazel/_bazel_root/75033a8ead962ac817396f45646bf4d8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 80, in <module>
    f'{traceback.format_exc()}'
ImportError: Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/75033a8ead962ac817396f45646bf4d8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /root/.cache/bazel/_bazel_root/75033a8ead962ac817396f45646bf4d8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow11GetNodeAttrERKNS_9AttrSliceEN4absl12lts_2021032411string_viewEPNS_18PartialTensorShapeE


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /root/git/tensorflow/tensorflow/tools/pip_package/BUILD:70:10 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed (Exit 1): bash failed: error executing command
```
</details>"
57126,Test //tensorflow/python/tools:saved_model_cli_test fails on s390x arch due to incorrect target_triple,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubunu 18.04

### Mobile device

NA

### Python version

3.10.4

### Bazel version

5.1.1

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

NA

### GPU model and memory

NA

### Current Behaviour?

```shell
Test case //tensorflow/python/tools:saved_model_cli_test, fails with below error on s390x arch.

2022-08-10 13:20:14.560234: F tensorflow/compiler/xla/service/llvm_ir/llvm_util.cc:262] Check failed: module->getDataLayout().isLittleEndian() == tensorflow::port::kLittleEndian (1 vs. 0)
Fatal Python error: Aborted

Expected behavior
The test case should pass
```


### Standalone code to reproduce the issue

The full command used is as follows:
```shell
bazel --host_jvm_args=""-Xms1024m"" --host_jvm_args=""-Xmx5048m"" test --jobs=8  --verbose_failures --test_tag_filters=-gpu,-benchmark-test,-v1only,-no_oss,-oss_serial -k --test_timeout 300,450,1200,3600 --build_tests_only --test_output=errors --define=tensorflow_mkldnn_contraction_kernel=0 --define tflite_with_xnnpack=false    -- //tensorflow/python/tools:saved_model_cli_test
```
I checked the target_cpu and target_triple values which are getting set for s390x, it takes default values viz.
target_triple=x86_64-pc-linux
target_cpu=""""

//tensorflow/python/tools:saved_model_cli_test test passes if we set the target_cpu and target_triple values explicitly at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/saved_model_cli.py#L891

This is not the right location to add this, but this confirms that the above error is somehow bypassed when we provide the s390x arch-specific target_cpu and target_triple values. Maybe the little-endian check was encountered because the target_triple is set to x86_64-pc-linux?

Also, I observed that the above check is considered only in sub-test cases where we have variables_to_feed as an empty string.

I tried setting  -c opt --cpu s390x  as well as --copt=-mtune=z14 --copt=-mzarch --copt=-march=z14 options in bazel test command. However, it didn't help either. Due to some reason, the LLVM compiler cannot use the underlying host CPU by default and in turn, the target_triple is set incorrectly.
The subtest which is responsible for the failure of //tensorflow/python/tools:saved_model_cli_test is testAOTCompileCPUFreezesAndCompiles   https://github.com/tensorflow/tensorflow/blob/7115bc2b888e524450bed3b77c88eef960cbd7ba/tensorflow/python/tools/saved_model_cli_test.py#L785

Could you please guide me on what I'm probably missing here? It would also be beneficial if someone could help identify where this target triple is getting set via LLVM? And how is Tensorflow reading the target_triple based on host



### Relevant log output

```shell
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
Executing tests from //tensorflow/python/tools:saved_model_cli_test
-----------------------------------------------------------------------------
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
Running tests under Python 3.10.4: /usr/local/bin/python3
[ RUN      ] SavedModelCLITestCase.testAOTCompileCPUFreezesAndCompilesVariablesToFeedAll
INFO:tensorflow:Assets written to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/dummy_model/assets
I0812 10:44:28.877938 4396789761824 builder_impl.py:779] Assets written to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/dummy_model/assets
2022-08-12 10:44:29.758373: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)
2022-08-12 10:44:29.760543: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
2022-08-12 10:44:29.965128: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:903] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node StatefulPartitionedCall_1.
INFO:tensorflow:Restoring parameters from /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/dummy_model/variables/variables
I0812 10:44:30.096666 4396789761824 saver.py:1412] Restoring parameters from /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/dummy_model/variables/variables
2022-08-12 10:44:30.098594: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
WARNING:tensorflow:From /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/tools/saved_model_aot_compile.py:284: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0812 10:44:30.115664 4396789761824 deprecation.py:350] From /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/tools/saved_model_aot_compile.py:284: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/framework/convert_to_constants.py:925: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0812 10:44:30.116104 4396789761824 deprecation.py:350] From /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/framework/convert_to_constants.py:925: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Writing graph def to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/frozen_graph.pb
I0812 10:44:30.120236 4396789761824 saved_model_aot_compile.py:300] Writing graph def to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/frozen_graph.pb
INFO:tensorflow:Writing config_pbtxt to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/config.pbtxt
I0812 10:44:30.123833 4396789761824 saved_model_aot_compile.py:305] Writing config_pbtxt to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/config.pbtxt
INFO:tensorflow:Generating XLA AOT artifacts in: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/aot_compile_cpu_dir
I0812 10:44:30.124699 4396789761824 saved_model_aot_compile.py:390] Generating XLA AOT artifacts in: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/aot_compile_cpu_dir
INFO:tensorflow:time(__main__.SavedModelCLITestCase.testAOTCompileCPUFreezesAndCompilesVariablesToFeedAll): 2.12s
I0812 10:44:30.172144 4396789761824 test_util.py:2458] time(__main__.SavedModelCLITestCase.testAOTCompileCPUFreezesAndCompilesVariablesToFeedAll): 2.12s
[       OK ] SavedModelCLITestCase.testAOTCompileCPUFreezesAndCompilesVariablesToFeedAll
[ RUN      ] SavedModelCLITestCase.testAOTCompileCPUFreezesAndCompilesVariablesToFeedMyVar
INFO:tensorflow:Assets written to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/dummy_model/assets
I0812 10:44:30.570448 4396789761824 builder_impl.py:779] Assets written to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/dummy_model/assets
2022-08-12 10:44:30.797690: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)
2022-08-12 10:44:30.857804: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
2022-08-12 10:44:31.068380: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:903] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node StatefulPartitionedCall_1.
INFO:tensorflow:Restoring parameters from /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/dummy_model/variables/variables
I0812 10:44:31.165877 4396789761824 saver.py:1412] Restoring parameters from /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/dummy_model/variables/variables
INFO:tensorflow:Writing graph def to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/frozen_graph.pb
I0812 10:44:31.192195 4396789761824 saved_model_aot_compile.py:300] Writing graph def to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/frozen_graph.pb
INFO:tensorflow:Writing config_pbtxt to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/config.pbtxt
I0812 10:44:31.195412 4396789761824 saved_model_aot_compile.py:305] Writing config_pbtxt to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/config.pbtxt
INFO:tensorflow:Generating XLA AOT artifacts in: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/aot_compile_cpu_dir
I0812 10:44:31.196351 4396789761824 saved_model_aot_compile.py:390] Generating XLA AOT artifacts in: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/aot_compile_cpu_dir
INFO:tensorflow:time(__main__.SavedModelCLITestCase.testAOTCompileCPUFreezesAndCompilesVariablesToFeedMyVar): 1.11s
I0812 10:44:31.278413 4396789761824 test_util.py:2458] time(__main__.SavedModelCLITestCase.testAOTCompileCPUFreezesAndCompilesVariablesToFeedMyVar): 1.11s
[       OK ] SavedModelCLITestCase.testAOTCompileCPUFreezesAndCompilesVariablesToFeedMyVar
[ RUN      ] SavedModelCLITestCase.testAOTCompileCPUFreezesAndCompilesVariablesToFeedNone
INFO:tensorflow:Assets written to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/dummy_model/assets
I0812 10:44:31.671765 4396789761824 builder_impl.py:779] Assets written to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/dummy_model/assets
2022-08-12 10:44:31.967894: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)
2022-08-12 10:44:31.976452: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
2022-08-12 10:44:32.095856: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:903] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node StatefulPartitionedCall_1.
INFO:tensorflow:Restoring parameters from /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/dummy_model/variables/variables
I0812 10:44:32.262146 4396789761824 saver.py:1412] Restoring parameters from /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/dummy_model/variables/variables
INFO:tensorflow:Writing graph def to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/frozen_graph.pb
I0812 10:44:32.287023 4396789761824 saved_model_aot_compile.py:300] Writing graph def to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/frozen_graph.pb
INFO:tensorflow:Writing config_pbtxt to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/config.pbtxt
I0812 10:44:32.289704 4396789761824 saved_model_aot_compile.py:305] Writing config_pbtxt to: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/config.pbtxt
INFO:tensorflow:Generating XLA AOT artifacts in: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/aot_compile_cpu_dir
I0812 10:44:32.290452 4396789761824 saved_model_aot_compile.py:390] Generating XLA AOT artifacts in: /home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/_tmp/e661df874edfb1b4d54e0ac549b74c5fnnq9m32i/aot_compile_cpu_dir
2022-08-12 10:44:32.294907: F tensorflow/compiler/xla/service/llvm_ir/llvm_util.cc:262] Check failed: module->getDataLayout().isLittleEndian() == tensorflow::port::kLittleEndian (1 vs. 0)
Fatal Python error: Aborted

Current thread 0x000003ffb5178720 (most recent call first):
  File ""/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/tools/saved_model_aot_compile.py"", line 399 in aot_compile_cpu_meta_graph_def
  File ""/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/tools/saved_model_cli.py"", line 887 in aot_compile_cpu
  File ""/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/tools/saved_model_cli_test.py"", line 820 in testAOTCompileCPUFreezesAndCompiles
  File ""/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/absl_py/absl/testing/parameterized.py"", line 316 in bound_param_test
  File ""/usr/local/lib/python3.10/unittest/case.py"", line 549 in _callTestMethod
  File ""/usr/local/lib/python3.10/unittest/case.py"", line 591 in run
  File ""/usr/local/lib/python3.10/unittest/case.py"", line 650 in __call__
  File ""/usr/local/lib/python3.10/unittest/suite.py"", line 122 in run
  File ""/usr/local/lib/python3.10/unittest/suite.py"", line 84 in __call__
  File ""/usr/local/lib/python3.10/unittest/suite.py"", line 122 in run
  File ""/usr/local/lib/python3.10/unittest/suite.py"", line 84 in __call__
  File ""/usr/local/lib/python3.10/unittest/runner.py"", line 184 in run
  File ""/usr/local/lib/python3.10/unittest/main.py"", line 271 in runTests
  File ""/usr/local/lib/python3.10/unittest/main.py"", line 101 in __init__
  File ""/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/absl_py/absl/testing/absltest.py"", line 2537 in _run_and_get_tests_result
  File ""/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/absl_py/absl/testing/absltest.py"", line 2568 in run_tests
  File ""/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/absl_py/absl/testing/absltest.py"", line 2156 in _run_in_app
  File ""/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/absl_py/absl/testing/absltest.py"", line 2049 in main
  File ""/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 51 in g_main
  File ""/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/absl_py/absl/app.py"", line 258 in _run_main
  File ""/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/absl_py/absl/app.py"", line 312 in run
  File ""/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 60 in main_wrapper
  File ""/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 503 in benchmarks_main
  File ""/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 62 in main
  File ""/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 56 in main
  File ""/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/tools/saved_model_cli_test.py"", line 880 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, tensorflow.python.framework.fast_tensor_util, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.h5r, h5py.utils, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5t, h5py._conv, h5py.h5z, h5py._proxy, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, scipy._lib._ccallback_c, scipy.sparse._sparsetools, scipy.sparse._csparsetools, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.ndimage._nd_image, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg.cython_lapack, scipy.linalg._decomp_update, scipy.special._comb, scipy.special._ellip_harm_2, _ni_label, scipy.ndimage._ni_label (total: 65)
*** Received signal 6 ***
*** BEGIN MANGLED STACK TRACE ***
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/../libtensorflow_framework.so.2(+0x10d43e2)[0x3ff9d5543e2]
[0x2aa25c32bf8]
/lib/s390x-linux-gnu/libpthread.so.0(raise+0x8c)[0x3ffb5012fdc]
[0x2aa25c3363e]
/lib/s390x-linux-gnu/libc.so.6(gsignal+0x8c)[0x3ffb4c3ded4]
/lib/s390x-linux-gnu/libc.so.6(abort+0x172)[0x3ffb4c3f35a]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZTv0_n24_N10tensorflow8internal15LogMessageFatalD1Ev+0x0)[0x3ffaa5adc08]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN3xla7llvm_ir26ConvertLiteralToIrConstantERKNS_7LiteralEPN4llvm6ModuleE+0x164)[0x3ffa9e40b34]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN3xla14FusedIrEmitter14HandleConstantERKNS_14HloInstructionE+0x76)[0x3ffa8025cce]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN3xla14FusedIrEmitter15CreateGeneratorERKNS_14HloInstructionE+0x164)[0x3ffa80268ac]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN3xla14FusedIrEmitter12GetGeneratorERKNS_14HloInstructionE+0x21e)[0x3ffa8028c46]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN3xla3cpu9IrEmitter12HandleFusionEPNS_14HloInstructionE+0x2d8)[0x3ffa7f9fa28]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN3xla14HloInstruction5VisitIPS0_EEN10tensorflow6StatusEPNS_17DfsHloVisitorBaseIT_EE+0x86a)[0x3ffa9fed08a]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZNK3xla14HloComputation13AcceptOrderedIPNS_14HloInstructionEEEN10tensorflow6StatusEPNS_17DfsHloVisitorBaseIT_EEN4absl12lts_202111024SpanIKS3_EE+0x9e0)[0x3ffa1b1f1c8]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN3xla3cpu9IrEmitter15EmitComputationEPNS_14HloComputationERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbN4absl12lts_202111024SpanIKPNS_14HloInstructionEEEb+0x572)[0x3ffa7f8db72]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN3xla3cpu11CpuCompiler18CompileAheadOfTimeESt10unique_ptrINS_14HloModuleGroupESt14default_deleteIS3_EERKNS_21AotCompilationOptionsE+0x1464)[0x3ffa7f299ac]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN3xla8Compiler18CompileAheadOfTimeESt10unique_ptrINS_14HloModuleGroupESt14default_deleteIS2_EERKNS_21AotCompilationOptionsEPS1_INS_22AotCompilationMetadataES3_IS9_EE+0xa2)[0x3ffa976cada]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN3xla18CompileOnlyService18CompileAheadOfTimeEN4absl12lts_202111024SpanIKNS0_25AotXlaComputationInstanceEEERKNS_21AotCompilationOptionsEPSt10unique_ptrINS_22AotCompilationMetadataESt14default_deleteISB_EE+0x2a7a)[0x3ffa7ed9442]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN3xla17CompileOnlyClient18CompileAheadOfTimeEN4absl12lts_202111024SpanIKNS0_25AotXlaComputationInstanceEEERKNS_21AotCompilationOptionsEPSt10unique_ptrINS_22AotCompilationMetadataESt14default_deleteISB_EE+0x98)[0x3ffa7ec3ee0]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow9tfcompile12CompileGraphENS_8GraphDefERKNS_6tf2xla6ConfigERKNS0_9MainFlagsEPNS0_13CompileResultE+0x116e)[0x3ffa29c898e]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow9tfcompile4MainERKNS0_9MainFlagsE+0x306)[0x3ffa29ca21e]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tfcompile.so(+0x1588cc)[0x3ff901d88cc]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tfcompile.so(+0x15901a)[0x3ff901d901a]
/home/vibhuti/.cache/bazel/_bazel_vibhuti/41ee634ee038d354a3b395f369225651/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/tools/saved_model_cli_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tfcompile.so(+0x15e03c)[0x3ff901de03c]
/usr/local/bin/python3(+0x28e8a6)[0x2aa2220e8a6]
/usr/local/bin/python3(_PyObject_MakeTpCall+0xa6)[0x2aa21fffc16]
/usr/local/bin/python3(+0x6283c)[0x2aa21fe283c]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x592e)[0x2aa21fe87ee]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(+0x6279a)[0x2aa21fe279a]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x592e)[0x2aa21fe87ee]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(+0x6279a)[0x2aa21fe279a]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x7d94)[0x2aa21feac54]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(PyVectorcall_Call+0x6c)[0x2aa220000fc]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x38f4)[0x2aa21fe67b4]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(+0x26c94c)[0x2aa221ec94c]
/usr/local/bin/python3(+0x6279a)[0x2aa21fe279a]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x6046)[0x2aa21fe8f06]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(+0x6279a)[0x2aa21fe279a]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x4436)[0x2aa21fe72f6]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(+0x26c814)[0x2aa221ec814]
/usr/local/bin/python3(PyVectorcall_Call+0x6c)[0x2aa220000fc]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x38f4)[0x2aa21fe67b4]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(_PyObject_Call_Prepend+0xc0)[0x2aa22000cf0]
/usr/local/bin/python3(+0xef316)[0x2aa2206f316]
/usr/local/bin/python3(_PyObject_MakeTpCall+0xa6)[0x2aa21fffc16]
/usr/local/bin/python3(+0x6283c)[0x2aa21fe283c]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x6046)[0x2aa21fe8f06]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(+0x26c814)[0x2aa221ec814]
/usr/local/bin/python3(PyVectorcall_Call+0x6c)[0x2aa220000fc]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x38f4)[0x2aa21fe67b4]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(_PyObject_Call_Prepend+0xc0)[0x2aa22000cf0]
/usr/local/bin/python3(+0xef316)[0x2aa2206f316]
/usr/local/bin/python3(_PyObject_MakeTpCall+0xa6)[0x2aa21fffc16]
/usr/local/bin/python3(+0x6283c)[0x2aa21fe283c]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x6046)[0x2aa21fe8f06]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(+0x26c814)[0x2aa221ec814]
/usr/local/bin/python3(PyVectorcall_Call+0x6c)[0x2aa220000fc]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x38f4)[0x2aa21fe67b4]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(_PyObject_Call_Prepend+0xc0)[0x2aa22000cf0]
/usr/local/bin/python3(+0xef316)[0x2aa2206f316]
/usr/local/bin/python3(_PyObject_MakeTpCall+0xa6)[0x2aa21fffc16]
/usr/local/bin/python3(+0x6283c)[0x2aa21fe283c]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x6046)[0x2aa21fe8f06]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(+0x6279a)[0x2aa21fe279a]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x4436)[0x2aa21fe72f6]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(+0x6279a)[0x2aa21fe279a]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x4436)[0x2aa21fe72f6]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(_PyObject_Call_Prepend+0x1c6)[0x2aa22000df6]
/usr/local/bin/python3(+0xef488)[0x2aa2206f488]
/usr/local/bin/python3(+0xe8f5a)[0x2aa22068f5a]
/usr/local/bin/python3(PyObject_Call+0x78)[0x2aa220004b0]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x38f4)[0x2aa21fe67b4]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(+0x6279a)[0x2aa21fe279a]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x6046)[0x2aa21fe8f06]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(+0x6279a)[0x2aa21fe279a]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x6046)[0x2aa21fe8f06]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(+0x6279a)[0x2aa21fe279a]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x6046)[0x2aa21fe8f06]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(+0x6279a)[0x2aa21fe279a]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x592e)[0x2aa21fe87ee]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(+0x6279a)[0x2aa21fe279a]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x6046)[0x2aa21fe8f06]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(+0x6279a)[0x2aa21fe279a]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x6046)[0x2aa21fe8f06]
/usr/local/bin/python3(+0x149f7c)[0x2aa220c9f7c]
/usr/local/bin/python3(_PyFunction_Vectorcall+0x6a)[0x2aa220007a2]
/usr/local/bin/python3(+0x6279a)[0x2aa21fe279a]
/usr/local/bin/python3(_PyEval_EvalFrameDefault+0x592e)[0x2aa21fe87ee]
*** END MANGLED STACK TRACE ***

*** Begin stack trace ***
        tensorflow::CurrentStackTrace[abi:cxx11]()


        raise

        gsignal
        abort
        virtual thunk to tensorflow::internal::LogMessageFatal::~LogMessageFatal()
        xla::llvm_ir::ConvertLiteralToIrConstant(xla::Literal const&, llvm::Module*)
        xla::FusedIrEmitter::HandleConstant(xla::HloInstruction const&)
        xla::FusedIrEmitter::CreateGenerator(xla::HloInstruction const&)
        xla::FusedIrEmitter::GetGenerator(xla::HloInstruction const&)
        xla::cpu::IrEmitter::HandleFusion(xla::HloInstruction*)
        tensorflow::Status xla::HloInstruction::Visit<xla::HloInstruction*>(xla::DfsHloVisitorBase<xla::HloInstruction*>*)
        tensorflow::Status xla::HloComputation::AcceptOrdered<xla::HloInstruction*>(xla::DfsHloVisitorBase<xla::HloInstruction*>*, absl::lts_20211102::Span<xla::HloInstruction* const>) const
        xla::cpu::IrEmitter::EmitComputation(xla::HloComputation*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool, absl::lts_20211102::Span<xla::HloInstruction* const>, bool)
        xla::cpu::CpuCompiler::CompileAheadOfTime(std::unique_ptr<xla::HloModuleGroup, std::default_delete<xla::HloModuleGroup> >, xla::AotCompilationOptions const&)
        xla::Compiler::CompileAheadOfTime(std::unique_ptr<xla::HloModuleGroup, std::default_delete<xla::HloModuleGroup> >, xla::AotCompilationOptions const&, std::unique_ptr<xla::AotCompilationMetadata, std::default_delete<xla::AotCompilationMetadata> >*)
        xla::CompileOnlyService::CompileAheadOfTime(absl::lts_20211102::Span<xla::CompileOnlyService::AotXlaComputationInstance const>, xla::AotCompilationOptions const&, std::unique_ptr<xla::AotCompilationMetadata, std::default_delete<xla::AotCompilationMetadata> >*)
        xla::CompileOnlyClient::CompileAheadOfTime(absl::lts_20211102::Span<xla::CompileOnlyClient::AotXlaComputationInstance const>, xla::AotCompilationOptions const&, std::unique_ptr<xla::AotCompilationMetadata, std::default_delete<xla::AotCompilationMetadata> >*)
        tensorflow::tfcompile::CompileGraph(tensorflow::GraphDef, tensorflow::tf2xla::Config const&, tensorflow::tfcompile::MainFlags const&, tensorflow::tfcompile::CompileResult*)
        tensorflow::tfcompile::Main(tensorflow::tfcompile::MainFlags const&)




        _PyObject_MakeTpCall

        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall

        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall

        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall
        PyVectorcall_Call
        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall


        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall

        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall

        PyVectorcall_Call
        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall
        _PyObject_Call_Prepend

        _PyObject_MakeTpCall

        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall

        PyVectorcall_Call
        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall
        _PyObject_Call_Prepend

        _PyObject_MakeTpCall

        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall

        PyVectorcall_Call
        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall
        _PyObject_Call_Prepend

        _PyObject_MakeTpCall

        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall

        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall

        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall
        _PyObject_Call_Prepend


        PyObject_Call
        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall

        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall

        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall

        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall

        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall

        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall

        _PyEval_EvalFrameDefault

        _PyFunction_Vectorcall

*** End stack trace ***
```
</details>"
57124,Error:Could not compute output KerasTensor... when saving a model,"when saving model, identical layer instance in different level leads to an error

**code to reproduce**:
```
from tensorflow.keras import *
import tensorflow as tf

act = layers.ReLU()

model = Sequential([
  act, 
  Sequential([
    act
  ]),
  Sequential([
    act
  ])
])
model(tf.random.normal([1]))
model.save(""./model-test"")
```

**the code below works correctly**:
```
from tensorflow.keras import *
import tensorflow as tf
from copy import deepcopy

act = layers.ReLU()

model = Sequential([
  deepcopy(act),
  Sequential([
    act
  ]),
  Sequential([
    act
  ])
])
model(tf.random.normal([1]))
model.save(""./model-test"")
```

there is no error as long as not saving, I wonder if it is a bug, or some mechanism causes that

platform: win10
tf version: 2.7.0
python version: 3.9"
57123,tf.math.ceil supported dtypes,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
https://www.tensorflow.org/api_docs/python/tf/math/ceil

The dtype section in the documentation above says `int32` which I believe is a mistake.
```


### Standalone code to reproduce the issue

```shell
import tensorflow
print(tensorflow.math.ceil(tensorflow.constant([1,2,3])))
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of int32 is not in the list of allowed values: bfloat16, half, float, double
	; NodeDef: {{node Ceil}}; Op<name=Ceil; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]> [Op:Ceil]
```
</details>"
57121,CHECK failed: file != nullptr,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.6.0

### Custom Code

No

### OS Platform and Distribution

MacOS Monterey Version 12.0.1 (21A559)

### Mobile device

_No response_

### Python version

3.8.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Building and running a docker container (file provided below) on several machines with the same dockerfile and python version. Works on various Ubuntu installations, a MacOS Monterey Version 12.5 installation, but not the MacOS Monterey Version 12.0.1 (21A559) installation on a machine with an Apple M1 Pro chip. On that one, the log output below happens.
```


### Standalone code to reproduce the issue

```shell
FROM tensorflow/serving:2.6.0
#RUN mkdir /models
#WORKDIR /models
#COPY ./ /models
EXPOSE 8080
ENTRYPOINT tensorflow_model_server --rest_api_port=8080 --model_name=MyModel #--model_base_path=/models/
```


### Relevant log output

```shell
[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc:2345] CHECK failed: file != nullptr:
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  CHECK failed: file != nullptr:
qemu: uncaught target signal 6 (Aborted) - core dumped
Aborted
```
</details>"
57120,Hexagon delegate (tflite) functionality broken after rebuilding project,"I had the hexagon delegate working fine for weeks, but yesterday I clicked ""Rebuild Project"" in Android Studio, and now I cannot get it working again.
I have scrubbed every place on my disk where I can find the files `libhexagon_interface.so` and `libtensorflowlite_hexagon_jni.so`, and made sure that the only copy of those files present on my computer is one that I downloaded and extracted manually from [oss.sonatype.org](https://oss.sonatype.org/#nexus-search;gav~org.tensorflow~tensorflow-lite-hexagon~~~). I place these files into their location in `.grade/caches/...`. I'm using the latest `0.0.0-nightly-SNAPSHOT` from there, which is dated `Fri Aug 12 2022`.
I delete all intermediate build directories, and wipe Android studio caches.
Still, I get this log dump when trying to load the hexagon delegate:

```
2022-08-12 09:34:50.182 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/rpcmem_android.c:159: rpcmem_init_internal: opened ION device fd 61, configured heap IDs: system (0x2000000), contig (0x400000), secure (0x200), secure flags (0x80080000)
2022-08-12 09:34:50.182 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:3087: fastrpc_apps_user_init done with default domain:0 and &fastrpc_trace:0x73af5f9fa0
2022-08-12 09:34:50.188 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/rpcmem_android.c:159: rpcmem_init_internal: opened ION device fd 62, configured heap IDs: system (0x2000000), contig (0x400000), secure (0x200), secure flags (0x80080000)
2022-08-12 09:34:50.189 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:3087: fastrpc_apps_user_init done with default domain:3 and &fastrpc_trace:0x73af548fa0
2022-08-12 09:34:50.195 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:2636: Successfully opened /vendor/dsp/cdsp/fastrpc_shell_33
2022-08-12 09:34:50.195 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_config.c:200: Reading configuration file: com.journeyapps.visiondemo.debugconfig
2022-08-12 09:34:50.185 6374-6374/com.journeyapps.visiondemo I/apps.visiondemo: type=1400 audit(0.0:190): avc: denied { read } for name=""adsprpc-smd-secure"" dev=""tmpfs"" ino=4237 scontext=u:r:untrusted_app_27:s0:c169,c256,c512,c768 tcontext=u:object_r:vendor_xdsp_device:s0 tclass=chr_file permissive=1 app=com.journeyapps.visiondemo
2022-08-12 09:34:50.197 6374-6374/com.journeyapps.visiondemo E/ion: ioctl c0044901 failed with code -1: Inappropriate ioctl for device
2022-08-12 09:34:50.189 6374-6374/com.journeyapps.visiondemo I/apps.visiondemo: type=1400 audit(0.0:192): avc: denied { ioctl } for path=""/dev/adsprpc-smd-secure"" dev=""tmpfs"" ino=4237 ioctlcmd=0x5208 scontext=u:r:untrusted_app_27:s0:c169,c256,c512,c768 tcontext=u:object_r:vendor_xdsp_device:s0 tclass=chr_file permissive=1 app=com.journeyapps.visiondemo
2022-08-12 09:34:50.241 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:2843: Created user PD on domain 3 (attrs 0x0, debug_trace 0x0)
2022-08-12 09:34:50.245 6374-6402/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/listener_android.c:111: listener thread starting
2022-08-12 09:34:50.245 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_perf.c:273: fastrpc_perf_init: enabled systrace 0x0 and RPC traces (kernel 0, dsp 0) with frequency 1000
2022-08-12 09:34:50.246 6374-6403/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/log_config.c:345: file_watcher_thread starting for domain 3
2022-08-12 09:34:50.247 6374-6404/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_latency.c:96: FastRPC latency thread started for QoS
2022-08-12 09:34:50.247 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:1345: remote_handle_open: Successfully opened handle 0x29f0d640 for adsp_current_process on domain 3
2022-08-12 09:34:50.248 6374-6403/com.journeyapps.visiondemo E/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/log_config.c:268:Error 0x200: fopen failed for /data/app/~~1vCWmfHaESKkWu1hV7LMOg==/com.journeyapps.visiondemo-v9RYULlL0KQNWz-o9hPkYA==/lib/arm64/com.journeyapps.visiondemo.farf. (No such file or directory)
2022-08-12 09:34:50.249 6374-6374/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:1798: remote_handle_control_domain: requested QOS 1, latency 115 for domain 3 handle 0xffffffffffffffff
2022-08-12 09:34:50.250 6374-6403/com.journeyapps.visiondemo E/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/log_config.c:268:Error 0x200: fopen failed for /system/vendor/lib/rfsa/adsp/com.journeyapps.visiondemo.farf. (No such file or directory)
2022-08-12 09:34:50.252 6374-6403/com.journeyapps.visiondemo E/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/log_config.c:268:Error 0x200: fopen failed for /vendor/lib/rfsa/adsp/com.journeyapps.visiondemo.farf. (No such file or directory)
2022-08-12 09:34:50.252 6374-6403/com.journeyapps.visiondemo E/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/log_config.c:268:Error 0x200: fopen failed for /vendor/dsp/cdsp/com.journeyapps.visiondemo.farf. (No such file or directory)
2022-08-12 09:34:50.252 6374-6402/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/mod_table.c:687: open_mod_table_open_from_static: reverse module apps_std opened with handle 0xaf54b9e8 (idx 0)
2022-08-12 09:34:50.253 6374-6403/com.journeyapps.visiondemo E/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/log_config.c:268:Error 0x200: fopen failed for /vendor/dsp/com.journeyapps.visiondemo.farf. (No such file or directory)
2022-08-12 09:34:50.254 6374-6402/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/mod_table.c:464: is_reverse_handle_opened: reverse module apps_std  already found with handle 0xaf54b9e8 (idx 0)
2022-08-12 09:34:50.254 6374-6402/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/mod_table.c:687: open_mod_table_open_from_static: reverse module apps_std opened with handle 0xaf54b9e8 (idx 0)
2022-08-12 09:34:50.256 6374-6402/com.journeyapps.visiondemo E/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/listener_android.c:64::error: 512: AEE_SUCCESS == (nErr = mod_table_close(handle, errStr, errStrLen, dlErr))
2022-08-12 09:34:50.257 6374-6402/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/apps_std_imp.c:868: Successfully opened file /data/app/~~1vCWmfHaESKkWu1hV7LMOg==/com.journeyapps.visiondemo-v9RYULlL0KQNWz-o9hPkYA==/lib/arm64/libhexagon_nn_skel_v66.so
2022-08-12 09:34:50.271 6374-6402/com.journeyapps.visiondemo I/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/mod_table.c:687: open_mod_table_open_from_static: reverse module apps_mem opened with handle 0xaf54bae8 (idx 1)
2022-08-12 09:34:50.281 6374-6374/com.journeyapps.visiondemo E/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:1325: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror _rtld_map_object_ex: cannot open oemconfig.so, errno 2 (no such file
2022-08-12 09:34:50.281 6374-6374/com.journeyapps.visiondemo E/com.journeyapps.visiondemo: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:1398: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp (errno Success)
2022-08-12 09:34:50.281 6374-6374/com.journeyapps.visiondemo W/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.
2022-08-12 09:34:50.282 6374-6374/com.journeyapps.visiondemo I/tflite: Hexagon Delegate is not supported.
```
Prior to hitting ""Rebuild Project"", this worked fine for weeks. Neither my Android hardware nor my software has changed.

This is a real-time project we're working on, and without hexagon support, inference time goes from 200ms to 1600ms, which makes this project dead in the water.

Please help!"
57118,Is there a way to convert (or transfer weights from) CuDNNLSTM to TFLiteLSTMCell?,"### 1. System information

- OS Platform and Distribution: Ubuntu 18.04 (and Android for inference)
- TensorFlow installation: pip
- TensorFlow library: version 1.14.0

### 2. Code

I wish to use `CuDNNLSTM` for a bidirectional LSTM model in order to make use of GPU for training. A short code snippet is below.

```
        forward_layer = tf.compat.v1.keras.layers.CuDNNLSTM(164, return_sequences=True, time_major=True)
        backward_layer = tf.compat.v1.keras.layers.CuDNNLSTM(164, return_sequences=True, time_major=True,
                         go_backwards=True)
        output = tensorflow.keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer)(input_feature)
```
However, this architecture is too heavy to deply on android using TFLite. Also, I need to avoid the Select TF Ops which increase the size of the runtime dramatically. Therefore, I decided to use experimental `TFLiteLSTMCell` in combination with `bidirectional_dynamic_rnn` , which works very well when trained on CPU. 

```
        lstm_forward = tf.compat.v1.lite.experimental.nn.TFLiteLSTMCell(164, use_peepholes=False, name=""fw"")
        lstm_backward = tf.compat.v1.lite.experimental.nn.TFLiteLSTMCell(164, use_peepholes=False, name=""bw"")
        infer_output, infer_state =  tensorflow.lite.experimental.examples.lstm.rnn.bidirectional_dynamic_rnn(
                lstm_forward,lstm_backward,lstm_inputs, dtype='float32', time_major=True)
```

I'm using what I believe is a reasonably standard method of conversion:

```
            converter = tf.compat.v1.lite.TFLiteConverter.from_session(sess, [roi], [y_pred])

            converter.experimental_new_converter = True
            converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]

            # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS], tf.lite.OpsSet.SELECT_TF_OPS]
            # We do not want to use Select TF Ops

            tflite = converter.convert()
```


Is there any way to convert weights from `CuDNNLSTM` to `TFLiteLSTMCell`? Even a way to do it from `LSTM` (non cudnn) to `TFLiteLSTMCell` will be most helpful, becuase transferring weights `CuDNNLSTM` to `LSTM` is well documented and not a big hurdle.

Any other way to convert CuDNNLSTM to TFLite without using Select Ops is also welcome.

Thanks in advance!


"
57115,Issue created for Rollback of PR #56572: Support Cudnn Runtime Fusion (Conv+Bias+Relu6/Elu/LeakyRelu),"Merged PR #56572 is rolled back in 5fc2e25f66cfc46d451add785a43ecb663501387.
    Please follow up with the reviewer and close this issue once its resolved."
57111,"Cannot assign variable in Tensorflow-Lite: ""incompatible with expected resource""","### Summary

I cannot get variable assignment to work in TFLite. 

I've made a minimal-possible graph for just doing a cumulative sum.  

 When I try to save the graph as a TFLite model, I get 

```
Cannot assign variable in Tensorflow-Lite: ""incompatible with expected resource""
```

### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Monterey, 12.2.1
- TensorFlow installation (pip package or built from source): Conda, from `conda install -c apple tensorflow==2.8.1`
- TensorFlow library (version, if pip package or github SHA, if built from source): `2.8.1`

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

This colab notebook reproduces the issue: 
https://colab.research.google.com/drive/1KPjHhlCMVs2oFxodrFAO7YcPrfBADC2k?usp=sharing 

See also: Stackoverflow Question
https://stackoverflow.com/questions/73327824/cannot-assign-variable-in-tensorflow-lite-incompatible-with-expected-resource "
57106,Numpy version error compiled with API 0x10 installed 0xf,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Debian Bullseye in Docker on an M1 Mac (linux/aarch64 build)

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

5.1.1

### GCC/Compiler version

gcc (Debian 10.2.1-6) 10.2.1 20210110

### CUDA/cuDNN version

_No response_

### GPU model and memory

Docker container has 24GiB RAM

### Current Behaviour?

```shell
I am compiling Tensorflow and Tensorflow-Text for linux/aarch64 because there is an issue with Docker/Qemu support for Apple Silicon. The code being run works on regular linux x86_64/amd64 containers Qemu does not support AVX2 instructions.

Tensorflow builds successfully but does not import and run successfully. Since this code works in the amd64 containers on intel machines, I expect it to work on this machine with a proper build of Tensorflow.

I get an error complaining about the Numpy version:


RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf
RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf
ImportError: numpy.core._multiarray_umath failed to import
ImportError: numpy.core.umath failed to import
```
```


### Standalone code to reproduce the issue

```shell
git clone --depth 1 --branch ""v2.9.1"" --single-branch https://github.com/tensorflow/tensorflow.git
pushd tensorflow

#You have bazel 5.1.1 installed.
#Please specify the location of python. [Default is /usr/local/bin/python3]:
#Found possible Python library paths:
#/orm/
#/orm/chassis/
#/orm/service/
#/usr/local/lib/python3.10/site-packages
#Please input the desired Python library path to use.  Default is [/orm/]
#/usr/local/lib/python3.10/site-packages
#Do you wish to build TensorFlow with ROCm support? [y/N]:
#No ROCm support will be enabled for TensorFlow.
#Do you wish to build TensorFlow with CUDA support? [y/N]:
#    No CUDA support will be enabled for TensorFlow.
#Do you wish to download a fresh release of clang? (Experimental) [y/N]:
#Clang will not be downloaded.
#Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:
#Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
#Not configuring the WORKSPACE for Android builds.

echo -e ""\n/usr/local/lib/python3.10/site-packages\nN\nN\nN\n-march=native -Wno-sign-compare\nN\n""|./configure

bazel build //tensorflow/tools/pip_package:build_pip_package
./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
```


### Relevant log output

```shell
platform linux -- Python 3.10.6, pytest-7.1.2, pluggy-1.0.0
django: settings: topic_classifier_service.settings (from env)
rootdir: /orm, configfile: pytest.ini
plugins: xdist-2.5.0, django-4.5.2, forked-1.4.0, cov-3.0.0
collected 137 items / 3 errors

================================================================================================= ERRORS =================================================================================================
___________________________________________________________________ ERROR collecting chassis/core/tests/service/test_chassis_views.py ____________________________________________________________________
../chassis/core/tests/service/test_chassis_views.py:17: in <module>
    class APIDocsTestCase(TestCase):
../chassis/core/tests/service/test_chassis_views.py:18: in APIDocsTestCase
    url = reverse('chassis:api-docs')
/usr/local/lib/python3.10/site-packages/django/urls/base.py:54: in reverse
    app_list = resolver.app_dict[ns]
/usr/local/lib/python3.10/site-packages/django/urls/resolvers.py:530: in app_dict
    self._populate()
/usr/local/lib/python3.10/site-packages/django/urls/resolvers.py:464: in _populate
    for url_pattern in reversed(self.url_patterns):
/usr/local/lib/python3.10/site-packages/django/utils/functional.py:48: in __get__
    res = instance.__dict__[self.name] = self.func(instance)
/usr/local/lib/python3.10/site-packages/django/urls/resolvers.py:602: in url_patterns
    patterns = getattr(self.urlconf_module, ""urlpatterns"", self.urlconf_module)
/usr/local/lib/python3.10/site-packages/django/utils/functional.py:48: in __get__
    res = instance.__dict__[self.name] = self.func(instance)
/usr/local/lib/python3.10/site-packages/django/urls/resolvers.py:595: in urlconf_module
    return import_module(self.urlconf_name)
/usr/local/lib/python3.10/importlib/__init__.py:126: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1050: in _gcd_import
    ???
<frozen importlib._bootstrap>:1027: in _find_and_load
    ???
<frozen importlib._bootstrap>:1006: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:688: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:883: in exec_module
    ???
<frozen importlib._bootstrap>:241: in _call_with_frames_removed
    ???
topic_classifier_service/urls.py:32: in <module>
    path(""api/v1/topic-classifier-service/"", include(""api.urls"")),
/usr/local/lib/python3.10/site-packages/django/urls/conf.py:34: in include
    urlconf_module = import_module(urlconf_module)
/usr/local/lib/python3.10/importlib/__init__.py:126: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1050: in _gcd_import
    ???
<frozen importlib._bootstrap>:1027: in _find_and_load
    ???
<frozen importlib._bootstrap>:1006: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:688: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:883: in exec_module
    ???
<frozen importlib._bootstrap>:241: in _call_with_frames_removed
    ???
api/urls.py:4: in <module>
    import api.views
api/views.py:4: in <module>
    from topic_classifier_service.tasks import store_predictions
topic_classifier_service/tasks.py:10: in <module>
    from classifier.train import train
classifier/train.py:2: in <module>
    from classifier.module import TopicClassifierModule
classifier/module.py:1: in <module>
    import tensorflow as tf
/usr/local/lib/python3.10/site-packages/tensorflow/__init__.py:37: in <module>
    from tensorflow.python.tools import module_util as _module_util
/usr/local/lib/python3.10/site-packages/tensorflow/python/__init__.py:42: in <module>
    from tensorflow.python import data
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/__init__.py:21: in <module>
    from tensorflow.python.data import experimental
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/experimental/__init__.py:95: in <module>
    from tensorflow.python.data.experimental import service
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/experimental/service/__init__.py:387: in <module>
    from tensorflow.python.data.experimental.ops.data_service_ops import distribute
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py:22: in <module>
    from tensorflow.python.data.experimental.ops import compression_ops
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py:16: in <module>
    from tensorflow.python.data.util import structure
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py:22: in <module>
    from tensorflow.python.data.util import nest
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/util/nest.py:36: in <module>
    from tensorflow.python.framework import sparse_tensor as _sparse_tensor
/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/sparse_tensor.py:24: in <module>
    from tensorflow.python.framework import constant_op
/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:25: in <module>
    from tensorflow.python.eager import execute
/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:23: in <module>
    from tensorflow.python.framework import dtypes
/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/dtypes.py:29: in <module>
    _np_bfloat16 = _pywrap_bfloat16.TF_bfloat16_type()
E   TypeError: Unable to convert function return value to a Python type! The signature was
E       () -> handle
-------------------------------------------------------------------------------------------- Captured stderr ---------------------------------------------------------------------------------------------
RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf
RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf
ImportError: numpy.core._multiarray_umath failed to import
ImportError: numpy.core.umath failed to import
____________________________________________________________________________ ERROR collecting service/api/tests/test_views.py ____________________________________________________________________________
api/tests/test_views.py:3: in <module>
    from api.views import classify, dataset_csv, create_dataset_example, update_dataset_example
api/views.py:4: in <module>
    from topic_classifier_service.tasks import store_predictions
topic_classifier_service/tasks.py:10: in <module>
    from classifier.train import train
classifier/train.py:2: in <module>
    from classifier.module import TopicClassifierModule
classifier/module.py:1: in <module>
    import tensorflow as tf
/usr/local/lib/python3.10/site-packages/tensorflow/__init__.py:37: in <module>
    from tensorflow.python.tools import module_util as _module_util
/usr/local/lib/python3.10/site-packages/tensorflow/python/__init__.py:42: in <module>
    from tensorflow.python import data
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/__init__.py:21: in <module>
    from tensorflow.python.data import experimental
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/experimental/__init__.py:95: in <module>
    from tensorflow.python.data.experimental import service
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/experimental/service/__init__.py:387: in <module>
    from tensorflow.python.data.experimental.ops.data_service_ops import distribute
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py:22: in <module>
    from tensorflow.python.data.experimental.ops import compression_ops
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py:16: in <module>
    from tensorflow.python.data.util import structure
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py:22: in <module>
    from tensorflow.python.data.util import nest
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/util/nest.py:36: in <module>
    from tensorflow.python.framework import sparse_tensor as _sparse_tensor
/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/sparse_tensor.py:24: in <module>
    from tensorflow.python.framework import constant_op
/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:25: in <module>
    from tensorflow.python.eager import execute
/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:23: in <module>
    from tensorflow.python.framework import dtypes
/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/dtypes.py:29: in <module>
    _np_bfloat16 = _pywrap_bfloat16.TF_bfloat16_type()
E   TypeError: Unable to convert function return value to a Python type! The signature was
E       () -> handle
_________________________________________________________________________ ERROR collecting service/classifier/tests/test_eval.py _________________________________________________________________________
classifier/tests/test_eval.py:2: in <module>
    from classifier.metrics import mean_reciprocal_rank
classifier/metrics.py:1: in <module>
    import tensorflow as tf
/usr/local/lib/python3.10/site-packages/tensorflow/__init__.py:37: in <module>
    from tensorflow.python.tools import module_util as _module_util
/usr/local/lib/python3.10/site-packages/tensorflow/python/__init__.py:42: in <module>
    from tensorflow.python import data
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/__init__.py:21: in <module>
    from tensorflow.python.data import experimental
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/experimental/__init__.py:95: in <module>
    from tensorflow.python.data.experimental import service
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/experimental/service/__init__.py:387: in <module>
    from tensorflow.python.data.experimental.ops.data_service_ops import distribute
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py:22: in <module>
    from tensorflow.python.data.experimental.ops import compression_ops
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py:16: in <module>
    from tensorflow.python.data.util import structure
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py:22: in <module>
    from tensorflow.python.data.util import nest
/usr/local/lib/python3.10/site-packages/tensorflow/python/data/util/nest.py:36: in <module>
    from tensorflow.python.framework import sparse_tensor as _sparse_tensor
/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/sparse_tensor.py:24: in <module>
    from tensorflow.python.framework import constant_op
/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:25: in <module>
    from tensorflow.python.eager import execute
/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:23: in <module>
    from tensorflow.python.framework import dtypes
/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/dtypes.py:29: in <module>
    _np_bfloat16 = _pywrap_bfloat16.TF_bfloat16_type()
E   TypeError: Unable to convert function return value to a Python type! The signature was
E       () -> handle
============================================================================================ warnings summary ============================================================================================
../../usr/local/lib/python3.10/site-packages/kombu/utils/compat.py:82
  /usr/local/lib/python3.10/site-packages/kombu/utils/compat.py:82: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.
    for ep in importlib_metadata.entry_points().get(namespace, [])

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
======================================================================================== short test summary info =========================================================================================
ERROR ../chassis/core/tests/service/test_chassis_views.py - TypeError: Unable to convert function return value to a Python type! The signature was
ERROR api/tests/test_views.py - TypeError: Unable to convert function return value to a Python type! The signature was
ERROR classifier/tests/test_eval.py - TypeError: Unable to convert function return value to a Python type! The signature was
```
</details>"
57105,cmake build is not installing library,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

master

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

3.8.5

### Bazel version

_No response_

### GCC/Compiler version

9.4

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Using CMake and specifying a CMAKE_INSTALL_PREFIX, the source compiles library is not being installed in the specified path.
```


### Standalone code to reproduce the issue

```shell
cmake \
      -DBUILD_SHARED_LIBS=ON \
      -DCMAKE_BUILD_TYPE=$build_type\
      -DCMAKE_CXX_COMPILER:FILEPATH=$(which $CXX) \
      -DCMAKE_C_COMPILER:FILEPATH=$(which $CC) \
      -DCMAKE_INSTALL_PREFIX=${pkg_install} \
      -DABSL_PROPAGATE_CXX_STD=ON \
      -DTFLITE_ENABLE_INSTALL=OFF \
      ../tensorflow/lite
```
```


### Relevant log output

_No response_</details>"
57104,Support Workload Identity Federation authentication in GFile,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

2.9.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20:04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Workload Identity Federation is the new and preferred authentication mechanism to GCS on CI systems where long lived service account keys can pose a security risk according to https://github.com/google-github-actions/auth.

It would be great if TensorFlow gfile would be able to utilise this authentication mechanism as well.
```


### Standalone code to reproduce the issue

```shell
A full reproduction can be found at https://github.com/google-github-actions/auth/issues/210
```


### Relevant log output

_No response_</details>"
57103,Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf2.9.1

### Custom Code

No

### OS Platform and Distribution

windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Should i do something with that ?
Ofc i have GPU in my Computer
```


### Standalone code to reproduce the issue

```shell
'''import tensorflow as tf
print(""TensorFlow version:"", tf.__version__)'''
```


### Relevant log output

```shell
C:\Users\tamir\PycharmProjects\pythonProject\venv\Scripts\python.exe C:/Users/tamir/PycharmProjects/pythonProject/module_one.py
2022-08-11 14:21:29.345331: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2022-08-11 14:21:29.345439: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
TensorFlow version: 2.9.1

Process finished with exit code 0
```
</details>"
57101,crash on R2.2 libtensorflowlite_c.so and config threadnum >1,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf2.2, tf2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
https://discuss.tensorflow.org/t/r2-2-libtensorflowlite-c-so-crash-for-multithread/7128


r2.2 libtensorflowlite_c.so, set threadnum=4 and  dlopen so that interface code as below.
the crash as blow is low probability and happen when run more than 100 loops.  
and i set the threads to 0 and it is working and have not crash. or this way have not crash that don't use dlopen and dynamic link interface so in .mk
```


### Standalone code to reproduce the issue

```shell
…

#include “tensorflow/lite/c/c_api.h”
#include “tensorflow/lite/c/c_api_experimental.h”
#include “tensorflow/lite/delegates/gpu/delegate.h”
#include “log.h”

class TFLITEObject{
public:
TFLITEObject(){
model = nullptr;
options = nullptr;
interpreter = nullptr;
gpudelegate = nullptr;
}
virtual ~TFLITEObject() {
if(gpudelegate)
TfLiteGpuDelegateV2Delete(gpudelegate);
if(interpreter)
TfLiteInterpreterDelete(interpreter);
if(options)
TfLiteInterpreterOptionsDelete(options);
if(model)
TfLiteModelDelete(model);
model = nullptr;
options = nullptr;
interpreter = nullptr;
gpudelegate = nullptr;
}
public:
void init_network(UNNModel *info) {

    if(info->ld.isFromBuffer)
    {
        LOGD(""tflite buffer : 0x%x, size : %d\n"", info->ld.model_buffer, info->ld.model_size);
        model = TfLiteModelCreate((const char*)info->ld.model_buffer, info->ld.model_size);
    }
    else
    {
        LOGD(""tflite file : %s\n"", info->ld.model_name);
        model = TfLiteModelCreateFromFile(info->ld.model_name);
    }

    options = TfLiteInterpreterOptionsCreate();

    if( info->param.number_of_threads > 0)
        TfLiteInterpreterOptionsSetNumThreads(options, info->param.number_of_threads);
…

    // Create the interpreter.
    interpreter = TfLiteInterpreterCreate(model, options);

    // Allocate tensors and populate the input tensor data.
    TfLiteInterpreterAllocateTensors(interpreter);

}

void deinit_network( ) {

    LOGD(""deinit_network \n"");

    if(gpudelegate)
        TfLiteGpuDelegateV2Delete(gpudelegate);
    if(interpreter)
        TfLiteInterpreterDelete(interpreter);
    if(options)
        TfLiteInterpreterOptionsDelete(options);
    if(model)
        TfLiteModelDelete(model);

    model = nullptr;
    options = nullptr;
    interpreter = nullptr;
    gpudelegate = nullptr;
}

int run_network( UNNModelInOutBuf *inOut) {
    // SetInput
    LOGD(""Set Network Inputs.\n"");
    for (int id = 0; id < inOut->inputs_count; id++) {
        TfLiteTensor* input_tensor = TfLiteInterpreterGetInputTensor(interpreter, id);
        TfLiteTensorCopyFromBuffer(input_tensor, (void *)inOut->inputs[id].data, inOut->inputs[id].size);
    }

    // Execute
    TfLiteInterpreterInvoke(interpreter);

    // GetOutput
    LOGD(""Get Network Outputs.\n"");
    for (int id = 0; id < inOut->outputs_count; id++) {
        const TfLiteTensor* output_tensor = TfLiteInterpreterGetOutputTensor(interpreter, id);
        if(output_tensor->quantization.type == kTfLiteAffineQuantization)
        {
            TfLiteQuantizationParams param = TfLiteTensorQuantizationParams(output_tensor);
            inOut->outputs[id].isQuant     = 1;
            inOut->outputs[id].quant_zero  = param.zero_point;
            inOut->outputs[id].quant_scale = param.scale;
        }
        TfLiteTensorCopyToBuffer(output_tensor, (void *)inOut->outputs[id].data, inOut->outputs[id].size);
    }
    return 0;
}
public:
TfLiteModel *model;
TfLiteInterpreterOptions *options;
TfLiteInterpreter *interpreter;
TfLiteDelegate *gpudelegate;
};

int TFLITEEngineInit(void **handle){
int ire = -1;
TFLITEObject *magic = new TFLITEObject();
if(magic != NULL){
ire = 0;
*handle = magic;
LOGD(“TFLITEEngineInit OK”);
}
else{
*handle = NULL;
LOGE(“TFLITEEngineInit Failed!”);
}

return ire;
}

int TFLITEEngineDeInit(void handle){
int ire = -1;
if(handle != NULL){
TFLITEObject pTFLITE = static_cast<TFLITEObject *>(handle);
delete pTFLITE;
pTFLITE = NULL;
ire = 0;
}
return ire;
}

int TFLITEEngineCreateNetwork(void *handle, UNNModel info)
{
int ire = -1;
if (handle != NULL)
{
TFLITEObject pTFLITE = static_cast<TFLITEObject *>(handle);
pTFLITE->init_network(info);
ire = 0;
}
return ire;
}

int TFLITEEngineDestroyNetwork(void handle)
{
int ire = -1;
if(handle != NULL){
TFLITEObject pTFLITE = static_cast<TFLITEObject *>(handle);
pTFLITE->deinit_network();
ire = 0;
}
return ire;
}

static bool TFLITEEngineOK(void handle){
bool b = false;
if(handle != NULL){
TFLITEObject pTFLITE = static_cast<TFLITEObject *>(handle);
if (pTFLITE->model && pTFLITE->interpreter){
b = true;
}
}
return b;
}

int TFLITEEngineRunSession(void *handle, UNNModelInOutBuf inOut){
int ire = -99;
if(TFLITEEngineOK(handle)){
TFLITEObject pTFLITE = static_cast<TFLITEObject *>(handle);
LOGD(“Start Invoke \n”);
if (ire = (int)pTFLITE->run_network( inOut ) != 0) {
LOGE(“Failed to invoke TFLITE Runtime!\n”);
}
ire = 0;
LOGD(“Invoke is OK \n”);
}
return ire;
}
```


### Relevant log output

```shell
C35033A 01-04 05:29:57.710 498 12173 F libc : /buildbot/src/android/ndk-release-r18/external/libcxx/…/…/external/libcxxabi/src/abort_message.cpp:73: abort_message: assertion “cannot create thread specific key for __cxa_get_globals()” failed
C350342 01-04 05:29:57.712 498 12173 F libc : Fatal signal 6 (SIGABRT), code -1 (SI_QUEUE) in tid 12173 (Blur), pid 498 (provider@2.4-se)
C350630 01-04 05:29:57.860 12179 12179 F DEBUG : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
C350631 01-04 05:29:57.860 12179 12179 F DEBUG : Native Crash TIME: 16201049
C350632 01-04 05:29:57.860 12179 12179 F DEBUG : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
C350633 01-04 05:29:57.860 12179 12179 F DEBUG : Build fingerprint: ‘realme/RMP2105/RE87CCL1:11/RP1A.201005.001/1640606145:userdebug/test-keys’
C350634 01-04 05:29:57.861 12179 12179 F DEBUG : Revision: ‘0’
C350635 01-04 05:29:57.861 12179 12179 F DEBUG : ABI: ‘arm’
C350636 01-04 05:29:57.861 12179 12179 F DEBUG : Timestamp: 2022-01-04 05:29:57+0800
C350637 01-04 05:29:57.861 12179 12179 F DEBUG : pid: 498, tid: 12173, name: Blur >>> /vendor/bin/hw/android.hardware.camera.provider@2.4-service <<<
C350638 01-04 05:29:57.861 12179 12179 F DEBUG : uid: 1047
C350639 01-04 05:29:57.861 12179 12179 F DEBUG : signal 6 (SIGABRT), code -1 (SI_QUEUE), fault addr --------
C35063A 01-04 05:29:57.861 12179 12179 F DEBUG : Abort message: ‘/buildbot/src/android/ndk-release-r18/external/libcxx/…/…/external/libcxxabi/src/abort_message.cpp:73: abort_message: assertion “cannot create thread specific key for __cxa_get_globals()” failed’
C35063B 01-04 05:29:57.861 12179 12179 F DEBUG : r0 00000000 r1 00002f8d r2 00000006 r3 cc06d010
C35063C 01-04 05:29:57.861 12179 12179 F DEBUG : r4 cc06d024 r5 cc06d008 r6 000001f2 r7 0000016b
C35063D 01-04 05:29:57.861 12179 12179 F DEBUG : r8 cc06d010 r9 cc06d020 r10 cc06d040 r11 cc06d030
C35063E 01-04 05:29:57.861 12179 12179 F DEBUG : ip 00002f8d sp cc06cfe0 lr ee9087cd pc ee9087e0
C35064A 01-04 05:29:57.867 12179 12179 F DEBUG : backtrace:
C35064B 01-04 05:29:57.867 12179 12179 F DEBUG : #00 pc 000387e0 /apex/com.android.runtime/lib/bionic/libc.so (abort+172) (BuildId: 724f04e3eb055a58d7517ffbc7210561)
C35064C 01-04 05:29:57.867 12179 12179 F DEBUG : #01 pc 00038a87 /apex/com.android.runtime/lib/bionic/libc.so (__assert2+22) (BuildId: 724f04e3eb055a58d7517ffbc7210561)
C35064D 01-04 05:29:57.867 12179 12179 F DEBUG : #02 pc 001fde95 /vendor/lib/libtensorflowlite_c.so
C35064E 01-04 05:29:57.867 12179 12179 F DEBUG : #03 pc 001fc471 /vendor/lib/libtensorflowlite_c.so
C35064F 01-04 05:29:57.867 12179 12179 F DEBUG : #04 pc 00081cb5 /apex/com.android.runtime/lib/bionic/libc.so (pthread_once+76) (BuildId: 724f04e3eb055a58d7517ffbc7210561)
C350650 01-04 05:29:57.867 12179 12179 F DEBUG : #05 pc 001fc433 /vendor/lib/libtensorflowlite_c.so
C350651 01-04 05:29:57.867 12179 12179 F DEBUG : #06 pc 001fc3e1 /vendor/lib/libtensorflowlite_c.so
C350652 01-04 05:29:57.867 12179 12179 F DEBUG : #07 pc 001fc15b /vendor/lib/libtensorflowlite_c.so
C350653 01-04 05:29:57.867 12179 12179 F DEBUG : #08 pc 001fbf05 /vendor/lib/libtensorflowlite_c.so
C350654 01-04 05:29:57.867 12179 12179 F DEBUG : #09 pc 001fbfe5 /vendor/lib/libtensorflowlite_c.so
C350655 01-04 05:29:57.867 12179 12179 F DEBUG : #10 pc 001fbfa3 /vendor/lib/libtensorflowlite_c.so
C350656 01-04 05:29:57.867 12179 12179 F DEBUG : #11 pc 000fd855 /vendor/lib/libtensorflowlite_c.so
C350657 01-04 05:29:57.867 12179 12179 F DEBUG : #12 pc 00080973 /apex/com.android.runtime/lib/bionic/libc.so (__pthread_start(void*)+40) (BuildId: 724f04e3eb055a58d7517ffbc7210561)
C350658 01-04 05:29:57.867 12179 12179 F DEBUG : #13 pc 00039ce3 /apex/com.android.runtime/lib/bionic/libc.so (__start_thread+30) (BuildId: 724f04e3eb055a58d7517ffbc7210561)
```
</details>"
57099,TensorFlow Lite in Play Services issue,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible): Lenovo/A6020a46/A6020a46:5.1.1/LMY47V/A6020a46_S105_161124_ROW:user/release-keys
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`): play-services-tflite-java:16.0.0-beta03
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`): 22.26.15 (020408-461192076)

**Standalone code to reproduce the issue**
TfLite.initialize(this, TfLiteInitializationOptions.builder().setEnableGpuDelegateSupport(true).build());

**Any other info / logs**
Any reliable way to find if GPU Delegate is supported in given device? Currently I am relying on addOnFailureListener() callback with ""Error loading TFLite GPU delegate module"" message.

Should I go back to CompatibilityList class of org.tensorflow.lite.gpu package for this one particular requirement? "
57098,"Take the middle segment of the model as the a model, got this error ValueError: Graph disconnected:","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

tf2.6

### Custom Code

Yes

### OS Platform and Distribution

windows11 && centos7

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The code runs normally on Windows, but an error occurs on Linux
```


### Standalone code to reproduce the issue

```shell
import tensorflow.keras.applications as app
from tensorflow.keras import Model
if __name__ == '__main__':
    efficientb4 = getattr(app, 'EfficientNetB4')(include_top=False)
    model = Model(inputs=efficientb4.layers[7].input, outputs=efficientb4.layers[29].output, name=""new_model"")
    model.summary()
```


### Relevant log output

```shell
ValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='input_1'), name='input_1', description=""created by layer 'input_1'"") at layer ""rescaling"". The following previous layers were accessed without issue: []
```
</details>"
57097,CMake and Bazel documentations for compiling tensorflow-lite are incomplete and/or incorrect,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04 LTS 

### Mobile deice

_No response_

### Python version

3.10

### Bazel version

N/A

### GCC/Compiler version

gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?


Following the instructions on https://www.tensorflow.org/lite/guide/build_cmake_arm to compile the shared library for Raspberry Pi, neither 64 bit nor 32 bit libraries for ARMv8 / ARMv7 respectively can be compiled.

Using the commands as provided to build for AArch64 (ARM64), compilation fails with the provided log.

If NEON is disabled by passing `-DTFLITE_ENABLE_XNNPACK=OFF` to CMAKE, compilation fails with another error message:



```shell
.
.
.
[ 96%] Linking CXX executable flatc
cd /home/iman/tensorflow_src/tflite-buildarm/_deps/flatbuffers-build && /usr/bin/cmake -E cmake_link_script CMakeFiles/flatc.dir/link.txt --verbose=1
/home/iman/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-g++ -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -mfp16-format=ieee -std=c++0x -Wall -pedantic -Werror -Wextra -Werror=shadow -faligned-new -Werror=implicit-fallthrough=2 -Wunused-result -Werror=unused-result -Wunused-parameter -Werror=unused-parameter -fsigned-char -O3 -DNDEBUG -rdynamic CMakeFiles/flatc.dir/src/idl_parser.cpp.o CMakeFiles/flatc.dir/src/idl_gen_text.cpp.o CMakeFiles/flatc.dir/src/reflection.cpp.o CMakeFiles/flatc.dir/src/util.cpp.o CMakeFiles/flatc.dir/src/idl_gen_cpp.cpp.o CMakeFiles/flatc.dir/src/idl_gen_csharp.cpp.o CMakeFiles/flatc.dir/src/idl_gen_dart.cpp.o CMakeFiles/flatc.dir/src/idl_gen_kotlin.cpp.o CMakeFiles/flatc.dir/src/idl_gen_go.cpp.o CMakeFiles/flatc.dir/src/idl_gen_java.cpp.o CMakeFiles/flatc.dir/src/idl_gen_ts.cpp.o CMakeFiles/flatc.dir/src/idl_gen_php.cpp.o CMakeFiles/flatc.dir/src/idl_gen_python.cpp.o CMakeFiles/flatc.dir/src/idl_gen_lobster.cpp.o CMakeFiles/flatc.dir/src/idl_gen_lua.cpp.o CMakeFiles/flatc.dir/src/idl_gen_rust.cpp.o CMakeFiles/flatc.dir/src/idl_gen_fbs.cpp.o CMakeFiles/flatc.dir/src/idl_gen_grpc.cpp.o CMakeFiles/flatc.dir/src/idl_gen_json_schema.cpp.o CMakeFiles/flatc.dir/src/idl_gen_swift.cpp.o CMakeFiles/flatc.dir/src/flatc.cpp.o CMakeFiles/flatc.dir/src/flatc_main.cpp.o CMakeFiles/flatc.dir/src/bfbs_gen_lua.cpp.o CMakeFiles/flatc.dir/src/code_generators.cpp.o CMakeFiles/flatc.dir/grpc/src/compiler/cpp_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/go_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/java_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/python_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/swift_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/ts_generator.cc.o -o flatc 
Running scripts/generate_code.py...
cd /home/iman/tensorflow_src/tflite-buildarm/flatbuffers && /usr/bin/python3.10 scripts/generate_code.py --flatc /home/iman/tensorflow_src/tflite-buildarm/_deps/flatbuffers-build/flatc --skip-gen-reflection
Traceback (most recent call last):
  File ""/home/iman/tensorflow_src/tflite-buildarm/flatbuffers/scripts/generate_code.py"", line 148, in <module>
    flatc(
  File ""/home/iman/tensorflow_src/tflite-buildarm/flatbuffers/scripts/generate_code.py"", line 82, in flatc
    result = subprocess.run(cmd, cwd=str(cwd), check=True)
  File ""/usr/lib/python3.10/subprocess.py"", line 501, in run
    with Popen(*popenargs, **kwargs) as process:
  File ""/usr/lib/python3.10/subprocess.py"", line 966, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File ""/usr/lib/python3.10/subprocess.py"", line 1842, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
OSError: [Errno 8] Exec format error: '/home/iman/tensorflow_src/tflite-buildarm/_deps/flatbuffers-build/flatc'
gmake[2]: *** [_deps/flatbuffers-build/CMakeFiles/flatc.dir/build.make:566: _deps/flatbuffers-build/flatc] Error 1
gmake[2]: *** Deleting file '_deps/flatbuffers-build/flatc'
gmake[2]: Leaving directory '/home/iman/tensorflow_src/tflite-buildarm'
gmake[1]: *** [CMakeFiles/Makefile2:4601: _deps/flatbuffers-build/CMakeFiles/flatc.dir/all] Error 2
gmake[1]: Leaving directory '/home/iman/tensorflow_src/tflite-buildarm'
gmake: *** [Makefile:139: all] Error 2
```
Furthermore, trying to build tensorflow-lite with Bazel for ARMhf causes a similar problem:

```shell
iman@iman-Virtual-Machine:~/tensorflow_src$ bazel build --config=elinux_armhf -c opt //tensorflow/lite:libtensorflowlite.so
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=104
INFO: Reading rc options for 'build' from /home/iman/tensorflow_src/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/iman/tensorflow_src/.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/iman/tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/iman/tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:elinux_armhf in file /home/iman/tensorflow_src/.bazelrc: --config=elinux --cpu=armhf --distinct_host_configuration=true --copt -mfp16-format=ieee
INFO: Found applicable config definition build:elinux in file /home/iman/tensorflow_src/.bazelrc: --crosstool_top=@local_config_embedded_arm//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain
INFO: Found applicable config definition build:linux in file /home/iman/tensorflow_src/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/iman/tensorflow_src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/fe01292457fc04532c5d2eccc9d0674df4582fa6.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/7eb46bb31f4dc3ad8ec11000ca2eeece598ffe3c.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Build options --copt and --cpu have changed, discarding analysis cache.
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/8e3d3359f9bec608e09fac1f7054a2a14b1bd73c.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pytorch/cpuinfo/archive/5e63739504f0f8e18e941bd63b2d6d42536c7d90.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/lite:libtensorflowlite.so (0 packages loaded, 10635 targets configured).
INFO: Found 1 target...
ERROR: /home/iman/.cache/bazel/_bazel_iman/d6309e44857b123c13e8a171acb8b0c0/external/XNNPACK/BUILD.bazel:8896:19: Compiling src/qu8-vcvt/gen/vcvt-armv6simd-x8.c failed: (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command /home/iman/.cache/bazel/_bazel_iman/d6309e44857b123c13e8a171acb8b0c0/external/armhf_linux_toolchain/bin/arm-linux-gnueabihf-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections ... (remaining 82 arguments skipped)
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c: In function 'xnn_qu8_vcvt_ukernel__armv6simd_x8':
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:26:9: error: unknown type name 'uint16x2_t'
   const uint16x2_t vminus_input_zero_point = (uint16x2_t) params->armv6simd.minus_input_zero_point;
         ^~~~~~~~~~
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:26:47: error: 'uint16x2_t' undeclared (first use in this function); did you mean 'uint16_t'?
   const uint16x2_t vminus_input_zero_point = (uint16x2_t) params->armv6simd.minus_input_zero_point;
                                               ^~~~~~~~~~
                                               uint16_t
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:26:47: note: each undeclared identifier is reported only once for each function it appears in
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:26:59: error: expected ',' or ';' before 'params'
   const uint16x2_t vminus_input_zero_point = (uint16x2_t) params->armv6simd.minus_input_zero_point;
                                                           ^~~~~~
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:30:11: error: unknown type name 'uint8x4_t'
     const uint8x4_t vx0123 = (uint8x4_t) unaligned_indexed_load_u32(x, 0);
           ^~~~~~~~~
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:30:31: error: 'uint8x4_t' undeclared (first use in this function); did you mean 'uint64_t'?
     const uint8x4_t vx0123 = (uint8x4_t) unaligned_indexed_load_u32(x, 0);
                               ^~~~~~~~~
                               uint64_t
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:30:42: error: expected ',' or ';' before 'unaligned_indexed_load_u32'
     const uint8x4_t vx0123 = (uint8x4_t) unaligned_indexed_load_u32(x, 0);
                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:31:21: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx4567'
     const uint8x4_t vx4567 = (uint8x4_t) unaligned_indexed_load_u32(x, 1);
                     ^~~~~~
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:31:21: error: 'vx4567' undeclared (first use in this function)
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:31:41: error: expected ';' before 'unaligned_indexed_load_u32'
     const uint8x4_t vx4567 = (uint8x4_t) unaligned_indexed_load_u32(x, 1);
                                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~
                                         ;
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:34:22: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx02'
     const uint16x2_t vx02 = __uxtab16(vminus_input_zero_point, vx0123);
                      ^~~~
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:34:22: error: 'vx02' undeclared (first use in this function); did you mean 'vx0123'?
     const uint16x2_t vx02 = __uxtab16(vminus_input_zero_point, vx0123);
                      ^~~~
                      vx0123
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:35:22: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx13'
     const uint16x2_t vx13 = __uxtab16(vminus_input_zero_point, __ror(vx0123, 8));
                      ^~~~
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:35:22: error: 'vx13' undeclared (first use in this function); did you mean 'vx0123'?
     const uint16x2_t vx13 = __uxtab16(vminus_input_zero_point, __ror(vx0123, 8));
                      ^~~~
                      vx0123
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:36:22: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx46'
     const uint16x2_t vx46 = __uxtab16(vminus_input_zero_point, vx4567);
                      ^~~~
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:36:22: error: 'vx46' undeclared (first use in this function)
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:37:22: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx57'
     const uint16x2_t vx57 = __uxtab16(vminus_input_zero_point, __ror(vx4567, 8));
                      ^~~~
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:37:22: error: 'vx57' undeclared (first use in this function)
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:68:21: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx0123'
     const uint8x4_t vx0123 = (uint8x4_t) unaligned_load_u32(x);
                     ^~~~~~
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:68:21: error: 'vx0123' undeclared (first use in this function)
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:68:41: error: expected ';' before 'unaligned_load_u32'
     const uint8x4_t vx0123 = (uint8x4_t) unaligned_load_u32(x);
                                         ^~~~~~~~~~~~~~~~~~~
                                         ;
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:71:22: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx02'
     const uint16x2_t vx02 = __uxtab16(vminus_input_zero_point, vx0123);
                      ^~~~
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:72:22: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx13'
     const uint16x2_t vx13 = __uxtab16(vminus_input_zero_point, __ror(vx0123, 8));
                      ^~~~
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:91:21: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx0123'
     const uint8x4_t vx0123 = (uint8x4_t) unaligned_load_u32(x);
                     ^~~~~~
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:91:41: error: expected ';' before 'unaligned_load_u32'
     const uint8x4_t vx0123 = (uint8x4_t) unaligned_load_u32(x);
                                         ^~~~~~~~~~~~~~~~~~~
                                         ;
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:93:22: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx02'
     const uint16x2_t vx02 = __uxtab16(vminus_input_zero_point, vx0123);
                      ^~~~
external/XNNPACK/src/qu8-vcvt/gen/vcvt-armv6simd-x8.c:94:22: error: expected '=', ',', ';', 'asm' or '__attribute__' before 'vx13'
     const uint16x2_t vx13 = __uxtab16(vminus_input_zero_point, __ror(vx0123, 8));
                      ^~~~
Target //tensorflow/lite:libtensorflowlite.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 34.262s, Critical Path: 14.01s
INFO: 403 processes: 178 internal, 225 local.
FAILED: Build did NOT complete successfully

```
### Standalone code to reproduce the issue

```shell
$ sudo apt-get install cmake gcc g++ git
$ git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
$ curl -LO https://storage.googleapis.com/mirror.tensorflow.org/developer.arm.com/media/Files/downloads/gnu-a/8.3-2019.03/binrel/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz
$ mkdir -p ${HOME}/toolchains
$ tar xvf gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz -C ${HOME}/toolchains
$ cd tensorflow_src
$ mkdir tflite-buildaa64
$ cd tflite-buildaa64
$ ARMCC_PREFIX=${HOME}/toolchains/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-
$ ARMCC_FLAGS=""-funsafe-math-optimizations""
$ cmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc \
  -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++ \
  -DCMAKE_C_FLAGS=""${ARMCC_FLAGS}"" \
  -DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}"" \
  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \
  -DCMAKE_SYSTEM_NAME=Linux \
  -DCMAKE_SYSTEM_PROCESSOR=aarch64 \
  ../tensorflow/lite/
$ cmake --build . -j1
```


### Relevant log output

```shell
.
.
.
[ 12%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c.o
cd /home/iman/tensorflow_src/tflite-buildarm/_deps/xnnpack-build && /home/iman/toolchains/gcc-arm-8.3-2019.03-x86_64-arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc -DCPUINFO_SUPPORTED_PLATFORM=1 -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ARM_DOTPROD=1 -DXNN_ENABLE_ARM_FP16=1 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_GEMM_M_SPECIALIZATION=1 -DXNN_ENABLE_JIT=0 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_SPARSE=1 -DXNN_LOG_LEVEL=0 -I/home/iman/tensorflow_src/tflite-buildarm/xnnpack/include -I/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src -I/home/iman/tensorflow_src/tflite-buildarm/cpuinfo/include -I/home/iman/tensorflow_src/tflite-buildarm/pthreadpool-source/include -I/home/iman/tensorflow_src/tflite-buildarm/FXdiv-source/include -I/home/iman/tensorflow_src/tflite-buildarm/FP16-source/include -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -mfp16-format=ieee -O3 -DNDEBUG -fPIC -Wno-psabi -pthread -std=gnu99  -fno-math-errno  -marm  -march=armv6 -mfpu=vfp -munaligned-access  -O2  -MD -MT _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c.o -MF CMakeFiles/XNNPACK.dir/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c.o.d -o CMakeFiles/XNNPACK.dir/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c.o -c /home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c: In function ‘xnn_qs8_vcvt_ukernel__armv6simd_x8’:
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:26:9: error: unknown type name ‘int16x2_t’
   const int16x2_t vminus_input_zero_point = (int16x2_t) params->armv6simd.minus_input_zero_point;
         ^~~~~~~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:26:46: error: ‘int16x2_t’ undeclared (first use in this function); did you mean ‘int16_t’?
   const int16x2_t vminus_input_zero_point = (int16x2_t) params->armv6simd.minus_input_zero_point;
                                              ^~~~~~~~~
                                              int16_t
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:26:46: note: each undeclared identifier is reported only once for each function it appears in
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:26:57: error: expected ‘,’ or ‘;’ before ‘params’
   const int16x2_t vminus_input_zero_point = (int16x2_t) params->armv6simd.minus_input_zero_point;
                                                         ^~~~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:30:11: error: unknown type name ‘int8x4_t’
     const int8x4_t vx0123 = (int8x4_t) unaligned_indexed_load_u32(x, 0);
           ^~~~~~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:30:30: error: ‘int8x4_t’ undeclared (first use in this function); did you mean ‘int64_t’?
     const int8x4_t vx0123 = (int8x4_t) unaligned_indexed_load_u32(x, 0);
                              ^~~~~~~~
                              int64_t
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:30:40: error: expected ‘,’ or ‘;’ before ‘unaligned_indexed_load_u32’
     const int8x4_t vx0123 = (int8x4_t) unaligned_indexed_load_u32(x, 0);
                                        ^~~~~~~~~~~~~~~~~~~~~~~~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:31:20: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘vx4567’
     const int8x4_t vx4567 = (int8x4_t) unaligned_indexed_load_u32(x, 1);
                    ^~~~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:31:20: error: ‘vx4567’ undeclared (first use in this function)
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:31:39: error: expected ‘;’ before ‘unaligned_indexed_load_u32’
     const int8x4_t vx4567 = (int8x4_t) unaligned_indexed_load_u32(x, 1);
                                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~
                                       ;
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:34:21: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘vx02’
     const int16x2_t vx02 = __sxtab16(vminus_input_zero_point, vx0123);
                     ^~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:34:21: error: ‘vx02’ undeclared (first use in this function); did you mean ‘vx0123’?
     const int16x2_t vx02 = __sxtab16(vminus_input_zero_point, vx0123);
                     ^~~~
                     vx0123
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:34:28: warning: implicit declaration of function ‘__sxtab16’ [-Wimplicit-function-declaration]
     const int16x2_t vx02 = __sxtab16(vminus_input_zero_point, vx0123);
                            ^~~~~~~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:35:21: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘vx13’
     const int16x2_t vx13 = __sxtab16(vminus_input_zero_point, __ror(vx0123, 8));
                     ^~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:35:21: error: ‘vx13’ undeclared (first use in this function); did you mean ‘vx0123’?
     const int16x2_t vx13 = __sxtab16(vminus_input_zero_point, __ror(vx0123, 8));
                     ^~~~
                     vx0123
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:36:21: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘vx46’
     const int16x2_t vx46 = __sxtab16(vminus_input_zero_point, vx4567);
                     ^~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:36:21: error: ‘vx46’ undeclared (first use in this function)
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:37:21: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘vx57’
     const int16x2_t vx57 = __sxtab16(vminus_input_zero_point, __ror(vx4567, 8));
                     ^~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:37:21: error: ‘vx57’ undeclared (first use in this function)
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:39:21: warning: implicit declaration of function ‘__smlawb’ [-Wimplicit-function-declaration]
     int32_t vacc0 = __smlawb(vmultiplier, vx02, vbias);
                     ^~~~~~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:41:21: warning: implicit declaration of function ‘__smlawt’ [-Wimplicit-function-declaration]
     int32_t vacc2 = __smlawt(vmultiplier, vx02, vbias);
                     ^~~~~~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:48:13: warning: implicit declaration of function ‘__ssat’ [-Wimplicit-function-declaration]
     vacc0 = __ssat(math_asr_s32(vacc0, 1), 8);
             ^~~~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:68:20: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘vx0123’
     const int8x4_t vx0123 = (int8x4_t) unaligned_load_u32(x);
                    ^~~~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:68:20: error: ‘vx0123’ undeclared (first use in this function)
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:68:39: error: expected ‘;’ before ‘unaligned_load_u32’
     const int8x4_t vx0123 = (int8x4_t) unaligned_load_u32(x);
                                       ^~~~~~~~~~~~~~~~~~~
                                       ;
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:71:21: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘vx02’
     const int16x2_t vx02 = __sxtab16(vminus_input_zero_point, vx0123);
                     ^~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:72:21: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘vx13’
     const int16x2_t vx13 = __sxtab16(vminus_input_zero_point, __ror(vx0123, 8));
                     ^~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:91:20: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘vx0123’
     const int8x4_t vx0123 = (int8x4_t) unaligned_load_u32(x);
                    ^~~~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:91:39: error: expected ‘;’ before ‘unaligned_load_u32’
     const int8x4_t vx0123 = (int8x4_t) unaligned_load_u32(x);
                                       ^~~~~~~~~~~~~~~~~~~
                                       ;
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:93:21: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘vx02’
     const int16x2_t vx02 = __sxtab16(vminus_input_zero_point, vx0123);
                     ^~~~
/home/iman/tensorflow_src/tflite-buildarm/xnnpack/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c:94:21: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘vx13’
     const int16x2_t vx13 = __sxtab16(vminus_input_zero_point, __ror(vx0123, 8));
                     ^~~~
gmake[2]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:2641: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-vcvt/gen/vcvt-armv6simd-x8.c.o] Error 1
gmake[2]: Leaving directory '/home/iman/tensorflow_src/tflite-buildarm'
gmake[1]: *** [CMakeFiles/Makefile2:6161: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/all] Error 2
gmake[1]: Leaving directory '/home/iman/tensorflow_src/tflite-buildarm'
gmake: *** [Makefile:139: all] Error 2
```
</details>"
57095,Pluggable Device crash since H2D copies optimization,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf-nightly-cpu 2.10.0.dev20220604

### Custom Code

No

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
1. `pip install tf-nightly-cpu==2.10.0.dev20220604`
2. `pip install tensorflow-directml-plugin==0.0.1.dev220621`
3. Run the code included in the repro
4. Notice a crash in the `memcpy_htod`, which is a pluggable device function called by TensorFlow to copy from CPU to GPU
5. Now, `pip install tf-nightly-cpu==2.10.0.dev20220603 --force-reinstall`, run the repro code and notice the test passes

Something happened in a June 3 commit (https://github.com/tensorflow/tensorflow/commit/0416617ecae7fda2c646aeb53fa911ed6d129547) that started breaking pluggable device support for us. Somehow, TensorFlow is calling the `memcpy_htod` function with GPU-allocated data as the source, which naturally causes a crash because the function is expecting CPU-allocated data.

Note that this also happens on Linux (and WSL), but we don't have a package ready to repro the issue on those platforms just yet.
```


### Standalone code to reproduce the issue

```shell
import numpy as np

from tensorflow.python.framework import dtypes as dtypes_lib
from tensorflow.python.framework import test_util
from tensorflow.python.ops import gradient_checker_v2
from tensorflow.python.ops import math_ops
from tensorflow.python.platform import test

class HostToDeviceCopyTest(test.TestCase):
    def test_host_to_device_copy_windows_crash(self):
        np_x = np.arange(1, 3).reshape((1, 2)).astype(dtypes_lib.float32.as_numpy_dtype)

        def callback(values):
            return math_ops.unsorted_segment_sum(values, np.array([0], dtype=np.int32), 3)

        with test_util.use_gpu():
            gradient_tape_jacob_t, jacob_n = gradient_checker_v2.compute_gradient(
                callback, [np_x], delta=1.0
            )
            self.assertAllCloseAccordingToType(jacob_n, gradient_tape_jacob_t)

if __name__ == ""__main__"":
    test.main()
```


### Relevant log output

```shell
[ RUN      ] HostToDeviceCopyTest.test_host_to_device_copy_windows_crash
WARNING:tensorflow:From C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\framework\test_util.py:1941: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
W0810 21:09:22.055201 13956 deprecation.py:350] From C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\framework\test_util.py:1941: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2022-08-10 21:09:22.058549: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-08-10 21:09:22.061527: I tensorflow/c/logging.cc:34] DirectML: creating device on adapter 0 (NVIDIA GeForce GTX 1070)
2022-08-10 21:09:22.178742: I tensorflow/c/logging.cc:34] Successfully opened dynamic library Kernel32.dll
2022-08-10 21:09:22.181038: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2022-08-10 21:09:22.181211: W tensorflow/core/common_runtime/pluggable_device/pluggable_device_bfc_allocator.cc:28] Overriding allow_growth setting because force_memory_growth was requested by the device.
2022-08-10 21:09:22.181359: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 6880 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)
2022-08-10 21:09:22.183758: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2022-08-10 21:09:22.183879: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6880 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)
Windows fatal exception: access violation

Thread 0x00003684 (most recent call first):
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 12741 in zeros_like
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 3137 in zeros_like_impl
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 2972 in wrapped
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 3075 in zeros_like
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\util\dispatch.py"", line 1176 in op_dispatch_handler
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\util\traceback_utils.py"", line 141 in error_handler
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\ops\math_grad.py"", line 475 in _GatherDropNegatives
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\ops\math_grad.py"", line 518 in _UnsortedSegmentSumGrad
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\eager\backprop.py"", line 157 in _gradient_function
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\eager\imperative_grad.py"", line 67 in imperative_grad
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\eager\backprop.py"", line 586 in vjp
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\eager\backprop.py"", line 513 in decorated
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\eager\backprop.py"", line 412 in decorated
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\ops\gradient_checker_v2.py"", line 167 in <lambda>
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\ops\gradient_checker_v2.py"", line 110 in decorated_eager
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\ops\gradient_checker_v2.py"", line 172 in _compute_theoretical_jacobian
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\ops\gradient_checker_v2.py"", line 272 in _compute_gradient
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\ops\gradient_checker_v2.py"", line 288 in <listcomp>
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\ops\gradient_checker_v2.py"", line 287 in _compute_gradient_list
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\ops\gradient_checker_v2.py"", line 342 in compute_gradient
  File ""C:\Users\pavignol\projects\tensorflow-directml-plugin\test.py"", line 19 in test_host_to_device_copy_windows_crash
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\unittest\case.py"", line 550 in _callTestMethod
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\unittest\case.py"", line 592 in run
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\unittest\case.py"", line 651 in __call__
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\unittest\suite.py"", line 122 in run
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\unittest\suite.py"", line 84 in __call__
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\unittest\suite.py"", line 122 in run
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\unittest\suite.py"", line 84 in __call__
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\unittest\runner.py"", line 176 in run
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\absl\testing\_pretty_print_reporter.py"", line 82 in run
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\unittest\main.py"", line 271 in runTests
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\unittest\main.py"", line 101 in __init__
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\absl\testing\absltest.py"", line 2518 in _run_and_get_tests_result
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\absl\testing\absltest.py"", line 2549 in run_tests
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\absl\testing\absltest.py"", line 2146 in _run_in_app
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\absl\testing\absltest.py"", line 2051 in main
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\platform\googletest.py"", line 51 in g_main
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\absl\app.py"", line 254 in _run_main
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\absl\app.py"", line 308 in run
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\platform\googletest.py"", line 60 in main_wrapper
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\platform\benchmark.py"", line 503 in benchmarks_main
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\platform\googletest.py"", line 62 in main
  File ""C:\Users\pavignol\Miniconda3\envs\tfdml2\lib\site-packages\tensorflow\python\platform\test.py"", line 56 in main
  File ""C:\Users\pavignol\projects\tensorflow-directml-plugin\test.py"", line 26 in <module>
*** Received signal 11 ***
*** BEGIN STACK TRACE POINTERS ***
0x00007ffc4c0f0cdf
0x00007ffc4c0f0ec7
0x00007ffce8fe1a82
0x00007ffce8fd2319
0x00007ffce8fc5680
0x00007ffce8fc5815
0x00007ffceb3e8fcf
0x00007ffceb375e9a
0x00007ffceb3e7fde
0x00007ffc3de1cf6a
0x00007ffc3d9c0bde
0x00007ffc3d9a844a
0x00007ffc3d9a2f56
0x00007ffc4b4ce449
0x00007ffc4b6ab811
0x00007ffc4b4be6e3
0x00007ffc4b4c637a
0x00007ffc4be5ca0f
0x00007ffc4be5daee
0x00007ffc4be42bfb
0x00007ffc4be41a76
0x00007ffc4bfc89fd
0x00007ffc4be41ee8
0x00007ffc4be42200
0x00007ffc4bea4a4b
0x00007ffc4b4c7789
0x00007ffc4be27bf2
0x00007ffc4be26f14
0x00007ffc4c142501
0x00007ffc4c1429a1
0x00007ffc428f59d8
0x00007ffce8fa6c0c
0x00007ffce9fc54e0
0x00007ffceb34485b
*** END STACK TRACE POINTERS ***

0x00007FFC4C17E075      tensorflow::CurrentStackTrace
0x00007FFC4C0F0ED1      tensorflow::testing::InstallStacktraceHandler
0x00007FFCE8FE1A82      exp2f
0x00007FFCE8FD2319      _intrinsic_setjmpex
0x00007FFCE8FC5680      _C_specific_handler
0x00007FFCE8FC5815      _C_specific_handler_noexcept
0x00007FFCEB3E8FCF      _chkstk
0x00007FFCEB375E9A      RtlRestoreContext
0x00007FFCEB3E7FDE      KiUserExceptionDispatcher
0x00007FFC3DE1CF6A      TF_InitProfiler
0x00007FFC3D9C0BDE      TF_InitProfiler
0x00007FFC3D9A844A      TF_InitProfiler
0x00007FFC3D9A2F56      plugin_memcpy_htod
0x00007FFC4B4CE449      tensorflow::GPUOptions::allow_growth
0x00007FFC4B6AB811      stream_executor::StreamExecutor::EnablePeerAccessTo
0x00007FFC4B4BE6E3      tensorflow::TensorShapeBase<tensorflow::PartialTensorShape>::RemoveDim
0x00007FFC4B4C637A      tensorflow::GPUOptions::per_process_gpu_memory_fraction
0x00007FFC4BE5CA0F      absl::lts_20211102::container_internal::raw_hash_set<absl::lts_20211102::container_internal::FlatHashMapPolicy<tensorflow::data::model::Node const * __ptr64,tensorflow::data::model::ModelTiming::NodeTiming>,absl::lts_20211102::container
0x00007FFC4BE5DAEE      absl::lts_20211102::container_internal::raw_hash_set<absl::lts_20211102::container_internal::FlatHashMapPolicy<tensorflow::data::model::Node const * __ptr64,tensorflow::data::model::ModelTiming::NodeTiming>,absl::lts_20211102::container
0x00007FFC4BE42BFB      tensorflow::LocalRendezvous::LocalRendezvous
0x00007FFC4BE41A76      tensorflow::LocalRendezvous::LocalRendezvous
0x00007FFC4BFC89FD      tensorflow::LocalRendezvous::RecvAsync
0x00007FFC4BE41EE8      tensorflow::LocalRendezvous::LocalRendezvous
0x00007FFC4BE42200      tensorflow::LocalRendezvous::LocalRendezvous
0x00007FFC4BEA4A4B      tensorflow::tfdbg::SingleDebugEventFileWriter::FileName
0x00007FFC4B4C7789      tensorflow::GPUOptions::per_process_gpu_memory_fraction
0x00007FFC4BE27BF2      tensorflow::tracing::ScopedRegion::~ScopedRegion
0x00007FFC4BE26F14      tensorflow::tracing::ScopedRegion::~ScopedRegion
0x00007FFC4C142501      Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop
0x00007FFC4C1429A1      Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop
0x00007FFC428F59D8      tensorflow::random::PhiloxRandom::SkipOne
0x00007FFCE8FA6C0C      recalloc
0x00007FFCE9FC54E0      BaseThreadInitThunk
0x00007FFCEB34485B      RtlUserThreadStart
```
</details>

@penpornk "
57094,"How to fix the ""undefined reference"" problem for the TF Lite? A suggestion.","When [CMakeLists.txt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt) is used to build a shared library of TF Lite, it is impossible to link with it because of the ""undefined reference"" problem.

Those ""undefined reference"" objects reside on the `tensorflow` main branch, but the following macro `populate_tf_source_vars` has never populated a single line of main Tensorflow code for the TF Lite build, yet the main code is referenced by it. No wonder we see the ""undefine reference"". 

I did try to use `populate_tf_source_vars` to bring those missing objects to TF Lite. Unfortunately, however, the references will quickly escalate to reference a lot of main Tensorflow stuff. That defeats the very motivation of having a ""Lite"" version of Tensorflow, doesn't it?

Can you please make TF Lite stop referencing those objects because they don't seem to be important? If you do think they are important, can you make a light version of them just for TF Lite?

https://github.com/tensorflow/tensorflow/blob/d35d3a1614fc8dd34dc181f521cf1d7f472249c1/tensorflow/lite/CMakeLists.txt#L136

**_The problem:_**

```
/usr/bin/ld: /usr/local/lib/libtensorflow-lite.so: undefined reference to `tensorflow::profiler::internal::g_trace_level'
/usr/bin/ld: /usr/local/lib/libtensorflow-lite.so: undefined reference to `tensorflow::profiler::TraceMeRecorder::Record(tensorflow::profiler::TraceMeRecorder::Event&&)'
/usr/bin/ld: /usr/local/lib/libtensorflow-lite.so: undefined reference to `tensorflow::profiler::GetCurrentTimeNanos()'
/usr/bin/ld: /usr/local/lib/libtensorflow-lite.so: undefined reference to `tensorflow::profiler::ScopedMemoryDebugAnnotation::ThreadMemoryDebugAnnotation()'

```"
57093,low-level compilation,"Before tensorflow 2.0, tensorflow was a static graph mode, and it was necessary to build a graph and get an HLO IR. XLA accepts computational graphs defined in HLO for goal-independent optimization. Then, the xla backend performs further HLO-level optimizations. After that, xla backend converts the HLO IR to LLVM IR, which performs low-level optimizations and generates machine code. After tensorflow 2.0, tensorflow has become a dynamic graph mode similar to pytorch, without first building a static graph. Can you answer, what is the logic from the top to the bottom under the dynamic graph concept? "
57086,Installation of tflite-model-maker 0.4.1. takes forever,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tflite-model-maker==0.4.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 18.04.3

### Mobile device

_No response_

### Python version

3.7.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tflite-model-maker==0.4.1 is taking forever to load. A similar issue I have seen is https://github.com/tensorflow/tensorflow/issues/51031.

I am trying to use `searcher`, which is not in the 0.3.2 version so I am trying to donwload 0.4.1. I see that in the solution to the similar issue for 0.3.2, a requirements.txt was provided. Could the same be done with 0.4.1? The requirements.txt file on pypi does not seem to have been updated. Some of the packages pip does not know the version of are: python-slugify, text-unidecode. I am not sure if there are more but this is what my output has shown so far.
```


### Standalone code to reproduce the issue

```shell
! pip install tflite-model-maker==0.4.1
```


### Relevant log output

```shell
INFO: pip is looking at multiple versions of text-unidecode to determine which version is compatible with other requirements. This could take a while.
Collecting text-unidecode>=1.3
  Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)
INFO: pip is looking at multiple versions of python-slugify to determine which version is compatible with other requirements. This could take a while.
Collecting python-slugify
  Using cached python_slugify-6.1.2-py2.py3-none-any.whl (9.4 kB)
  Using cached python_slugify-6.1.1-py2.py3-none-any.whl (9.1 kB)
  Using cached python_slugify-6.1.0-py2.py3-none-any.whl (9.2 kB)
INFO: pip is looking at multiple versions of text-unidecode to determine which version is compatible with other requirements. This could take a while.
  Using cached python_slugify-6.0.1-py2.py3-none-any.whl (9.0 kB)
  Using cached python_slugify-6.0.0-py2.py3-none-any.whl (9.0 kB)
  Using cached python_slugify-5.0.2-py2.py3-none-any.whl (6.7 kB)
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking
  Using cached python_slugify-5.0.1-py2.py3-none-any.whl (6.7 kB)
INFO: pip is looking at multiple versions of python-slugify to determine which version is compatible with other requirements. This could take a while.
  Using cached python_slugify-5.0.0-py2.py3-none-any.whl (6.8 kB)
  Using cached python-slugify-4.0.1.tar.gz (11 kB)
  Using cached python-slugify-4.0.0.tar.gz (8.8 kB)
  Using cached python-slugify-3.0.6.tar.gz (8.7 kB)
  Using cached python-slugify-3.0.5.tar.gz (8.7 kB)
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking
  Using cached python-slugify-3.0.4.tar.gz (8.1 kB)
  Using cached python-slugify-3.0.3.tar.gz (8.1 kB)
Collecting text-unidecode==1.2
  Using cached text_unidecode-1.2-py2.py3-none-any.whl (77 kB)
Collecting python-slugify
  Using cached python-slugify-3.0.2.tar.gz (7.9 kB)
  Using cached python-slugify-3.0.1.tar.gz (7.9 kB)
  Using cached python-slugify-3.0.0.tar.gz (6.7 kB)
  Using cached python-slugify-2.0.1.tar.gz (6.6 kB)
  Using cached python-slugify-2.0.0.tar.gz (6.5 kB)
  Using cached python-slugify-1.2.6.tar.gz (6.8 kB)
Collecting Unidecode>=0.04.16
  Using cached Unidecode-1.3.4-py3-none-any.whl (235 kB)
INFO: pip is looking at multiple versions of unidecode to determine which version is compatible with other requirements. This could take a while.
  Using cached Unidecode-1.3.3-py3-none-any.whl (235 kB)
  Using cached Unidecode-1.3.2-py3-none-any.whl (235 kB)
  Using cached Unidecode-1.2.0-py2.py3-none-any.whl (241 kB)
  Using cached Unidecode-1.1.2-py2.py3-none-any.whl (239 kB)
  Using cached Unidecode-1.1.1-py2.py3-none-any.whl (238 kB)
  Using cached Unidecode-1.1.0.tar.gz (212 kB)
  Using cached Unidecode-1.0.23-py2.py3-none-any.whl (237 kB)
INFO: pip is looking at multiple versions of unidecode to determine which version is compatible with other requirements. This could take a while.
  Using cached Unidecode-1.0.22-py2.py3-none-any.whl (235 kB)
  Using cached Unidecode-0.04.21-py2.py3-none-any.whl (228 kB)
  Using cached Unidecode-0.04.20-py2.py3-none-any.whl (228 kB)
  Using cached Unidecode-0.04.19.tar.gz (204 kB)
  Using cached Unidecode-0.04.18.tar.gz (206 kB)
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking
  Using cached Unidecode-0.04.17.tar.gz (201 kB)
  Using cached Unidecode-0.04.16.tar.gz (200 kB)
Collecting python-slugify
  Using cached python-slugify-1.2.5.tar.gz (6.6 kB)
  Using cached python_slugify-1.2.4-py2.py3-none-any.whl (4.7 kB)
  Using cached python-slugify-1.2.3.tar.gz (8.5 kB)
  Using cached python-slugify-1.2.2.tar.gz (7.5 kB)
  Using cached python-slugify-1.2.1.tar.gz (5.3 kB)



(this is not the entire output but it keeps on going like so)
```
</details>"
57084,"TensorFlow-lite: ""ValueError: Invalid tensor size"" when output is a zero-size tensor","**Summary - when the output has zero-size, I get ""ValueError: Invalid tensor size"" when trying to retrieve the output value - even when the output is supposed to have zero-size.**

### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **MacOS Monterrey 12.2.1**

- TensorFlow installation (pip package or built from source): **`2.8.1`, installed with `conda install -c apple tensorflow==2.8.1`**

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
import tensorflow as tf

def test_tflite_fails_with_zero_size_output():

    func = tf.where  # Example of function that can potentially have zero-size output

    # With these test arguments, it passes
    # mask = tf.cast([
    #     [1, 0, 0, 1],
    #     [0, 0, 1, 0],
    #     [1, 0, 0, 0]
    # ], dtype=tf.bool)
    # expected_ixs = [(0, 0), (0, 3), (1, 2), (2, 0)]

    # With these (designed to produce zero-size output), it fails on the indicated line below
    mask = tf.cast([
        [0, 0, 0, 0],
        [0, 0, 0, 0],
        [0, 0, 0, 0]
    ], dtype=tf.bool)
    expected_ixs = np.zeros((0, 2))

    # Verify that it works in ordinary tensorflow
    result = func(mask)
    assert np.array_equal(result.numpy(), expected_ixs)

    # Save tflite model
    model_path = os.path.expanduser('~/Downloads/test_model.tflite')
    concrete_func = tf.function(
        input_signature=[tf.TensorSpec(shape=mask.shape, dtype=tf.bool)],
    )(func).get_concrete_function()
    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
    converter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS, tf.lite.OpsSet.TFLITE_BUILTINS]  # enable TensorFlow Lite ops.]
    serialized_model = converter.convert()
    with open(model_path, 'wb') as f:
        f.write(serialized_model)
    print(f'Saved model to {model_path}')

    # Load tflite model
    interpreter = tf.lite.Interpreter(model_path=model_path)
    interpreter.allocate_tensors()
    interpreter.set_tensor(0, mask)
    interpreter.invoke()
    result = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])
    # ^^^ ABOVE LINE GIVES ""ValueError: Invalid tensor size."" WHEN OUTPUT SIZE IS ZERO!
    assert np.array_equal(result, expected_ixs)

```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

The model gives the error when outputs have zero-size - even when they are expected to have zero-size.

"
57083,Header tensorflow/tsl/platform/stack_frame.h not included with tf-nightly pip package,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v1.12.1-79472-g4aaefec710e 2.11.0-dev20220810

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Horovod does not build with `tf-nightly` anymore, see https://github.com/horovod/horovod/issues/3641

The code includes `tensorflow/core/framework/op.h` at https://github.com/horovod/horovod/blob/master/horovod/tensorflow/mpi_ops.cc#L34. Header files in `tensorflow/core` are included with the pip package. However, this ultimately includes `tensorflow/tsl/platform/stack_frame.h`, which does not come with the package.

This appears to have changed with https://github.com/tensorflow/tensorflow/commit/399e4071c471f6dc47bf245f8aeb8ab0c0374fce, cc @joker-eph


### Standalone code to reproduce the issue

```shell
#include ""tensorflow/core/framework/op.h""
```


### Relevant log output

```shell
2022-08-10T16:38:11.6788601Z #44 157.3   [ 66%] Building CXX object horovod/tensorflow/CMakeFiles/tensorflow.dir/mpi_ops.cc.o
2022-08-10T16:38:11.6792823Z #44 157.3   cd /tmp/pip-req-build-ggufvm1f/build/temp.linux-x86_64-3.8/RelWithDebInfo/horovod/tensorflow && /usr/bin/c++  -DEIGEN_MPL2_ONLY=1 -DHAVE_GLOO=1 -DHAVE_MPI=1 -DTENSORFLOW_VERSION=9999999999 -Dtensorflow_EXPORTS -I/tmp/pip-req-build-ggufvm1f/third_party/HTTPRequest/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/assert/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/config/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/core/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/detail/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/iterator/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/lockfree/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/mpl/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/parameter/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/predef/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/preprocessor/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/static_assert/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/type_traits/include -I/tmp/pip-req-build-ggufvm1f/third_party/boost/utility/include -I/tmp/pip-req-build-ggufvm1f/third_party/lbfgs/include -I/tmp/pip-req-build-ggufvm1f/third_party/gloo -I/tmp/pip-req-build-ggufvm1f/third_party/flatbuffers/include -isystem /usr/local/lib/python3.8/dist-packages/tensorflow/include  -I/usr/local/lib/python3.8/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=1 -DEIGEN_MAX_ALIGN_BYTES=64  -pthread -fPIC -Wall -ftree-vectorize -mf16c -mavx -mfma -O3 -g -DNDEBUG -fPIC   -std=c++17 -o CMakeFiles/tensorflow.dir/mpi_ops.cc.o -c /tmp/pip-req-build-ggufvm1f/horovod/tensorflow/mpi_ops.cc
2022-08-10T16:38:11.6795960Z #44 157.4   In file included from /tmp/pip-req-build-ggufvm1f/horovod/common/gloo/../mpi/mpi_context.h:25,
2022-08-10T16:38:11.8295689Z #44 157.4                    from /tmp/pip-req-build-ggufvm1f/horovod/common/gloo/gloo_context.h:25,
2022-08-10T16:38:11.8296650Z #44 157.4                    from /tmp/pip-req-build-ggufvm1f/horovod/common/gloo/gloo_controller.h:19,
2022-08-10T16:38:11.8297220Z #44 157.4                    from /tmp/pip-req-build-ggufvm1f/horovod/common/gloo/gloo_controller.cc:16:
2022-08-10T16:38:11.8298063Z #44 157.4   /tmp/pip-req-build-ggufvm1f/horovod/common/gloo/../mpi/../half.h: In function ‘void horovod::common::HalfBits2Float(const short unsigned int*, float*)’:
2022-08-10T16:38:11.8298871Z #44 157.4   /tmp/pip-req-build-ggufvm1f/horovod/common/gloo/../mpi/../half.h:76:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
2022-08-10T16:38:11.8299357Z #44 157.4      76 |   *res = *reinterpret_cast<float const*>(&f);
2022-08-10T16:38:11.8299655Z #44 157.4         |           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2022-08-10T16:38:13.5852575Z #44 159.3   In file included from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/platform/status.h:32,
2022-08-10T16:38:13.7360889Z #44 159.3                    from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/lib/core/status.h:19,
2022-08-10T16:38:13.7361596Z #44 159.3                    from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/resource_base.h:20,
2022-08-10T16:38:13.7362253Z #44 159.3                    from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/resource_handle.h:21,
2022-08-10T16:38:13.7362956Z #44 159.3                    from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:32,
2022-08-10T16:38:13.7363571Z #44 159.3                    from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/op_def_builder.h:28,
2022-08-10T16:38:13.7364233Z #44 159.3                    from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/full_type_inference_util.h:23,
2022-08-10T16:38:13.7364870Z #44 159.3                    from /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/framework/op.h:24,
2022-08-10T16:38:13.7365433Z #44 159.3                    from /tmp/pip-req-build-ggufvm1f/horovod/tensorflow/mpi_ops.cc:34:
2022-08-10T16:38:13.7366144Z #44 159.3   /usr/local/lib/python3.8/dist-packages/tensorflow/include/tensorflow/core/platform/stack_frame.h:19:10: fatal error: tensorflow/tsl/platform/stack_frame.h: No such file or directory
2022-08-10T16:38:13.7366631Z #44 159.3      19 | #include ""tensorflow/tsl/platform/stack_frame.h""
2022-08-10T16:38:13.7366941Z #44 159.3         |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2022-08-10T16:38:13.7367206Z #44 159.3   compilation terminated.
2022-08-10T16:38:13.7367601Z #44 159.3   make[2]: *** [horovod/tensorflow/CMakeFiles/tensorflow.dir/build.make:453: horovod/tensorflow/CMakeFiles/tensorflow.dir/mpi_ops.cc.o] Error 1
```
</details>"
57082,"raise ValueError( ValueError: Given `time_step`: TimeStep( {'discount': array(1., dtype=float32),","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

MacOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
/Users/shinwazu/miniforge3/envs/tensorflow/bin/python /Users/shinwazu/PycharmProjects/pythonProject/WarehouseEnv(Tensorflow)/TensorflowEnv.py
[[0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0.]]
Traceback (most recent call last):
  File ""/Users/shinwazu/PycharmProjects/pythonProject/WarehouseEnv(Tensorflow)/TensorflowEnv.py"", line 5, in <module>
    utils.validate_py_environment(python_environment, episodes=5)
  File ""/Users/shinwazu/.local/lib/python3.10/site-packages/tf_agents/environments/utils.py"", line 78, in validate_py_environment
    raise ValueError(
ValueError: Given `time_step`: TimeStep(
{'discount': array(1., dtype=float32),
 'observation': array([[[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]]]),
 'reward': array(0., dtype=float32),
 'step_type': array(0, dtype=int32)}) does not match expected `time_step_spec`: TimeStep(
{'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),
 'observation': BoundedArraySpec(shape=(25,), dtype=dtype('float64'), name='observation', minimum=0.0, maximum=5.0),
 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),
 'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')})
```


### Standalone code to reproduce the issue

```shell
https://code-with-me.global.jetbrains.com/QyxA3M63JEzplQaS1hQJww#p=PC&fp=166685A1B2D9C482A51851BB0062CEB0F4674F873AEB0A62E1352D445737634E
```


### Relevant log output

_No response_</details>"
57081,Data loss: not an sstable (bad magic number),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
57080,Allow getting length when using tf.data.Dataset as numpy iterators,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

v2.9.0-18-gd8ce9f9c301 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


When using tensorflow Dataset as numpy iterator I want length/cardinality to work.

```python
import tensorflow as tf
import numpy as np
ds = tf.data.Dataset.from_tensor_slices(np.zeros(100))
ds_n = ds.as_numpy_iterator()

# this works
print(len(ds))
print(ds.cardinality().numpy())

# this does not work
print(len(ds_n))
print(ds_n.cardinality().numpy())
```


### Standalone code to reproduce the issue

[Colab Link](https://colab.research.google.com/drive/1XjDgaLCGcIOQ1_mycZh9aglYFlnXayLW?usp=sharing)


### Relevant log output

_No response_</details>"
57076,Keras Model `__call__` performance with large `StringLookup`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Debian 4.19.249-2

### Mobile device

_No response_

### Python version

3.7.3

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When running a custom Keras model with a large instance variable (for example a `StringLookup` layer with a large vocabulary), the `__call__` function is extremely slow, irrespective of what's in the custom user defined `call` function. It seems that this is happening because when running `__call__` in eager mode, `_clear_losses` is called, which eventually calls `_flatten_modules`, which recursively iterates over all trackable attributes of the model (which happens to include the large `StringLookup` table.

The table isn't touched at all in `_clear_losses`, since it's not actually a layer -- is there a way to stop these variables from being processed in that function, or am I looking at this the wrong way?
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from datetime import datetime

class LookupModel(tf.keras.Model):
    def __init__(self, table_size, name=""lookup""):
        super().__init__(name=name)

        self.kvstore = tf.keras.layers.StringLookup(
            vocabulary=[str(i) for i in range(table_size)],
            trainable=False
        )

    def call(self, input):
        return input

small = LookupModel(10)
large = LookupModel(5_000_000)

small_start = datetime.now()
small([1])
print(""Small Lookup:"", datetime.now() - small_start)

large_start = datetime.now()
large([1])
print(""Large Lookup:"", datetime.now() - large_start)
```


### Relevant log output

```shell
Small Lookup: 0:00:00.132581
Large Lookup: 0:00:04.091659
```
</details>"
57072,Docker Hub ubuntu version documentation error,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

binary

### Tensorflow Version

n/a

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
On docker hub (https://hub.docker.com/r/tensorflow/tensorflow), it says `Images built after May 20 2019 (TF nightly, plus TF versions 1.14 and onward) are based on Ubuntu 18.04. Earlier images are based on Ubuntu 16.04.`

However, if I pull `tensorflow/tensorflow:latest-gpu`, I get:

root@d98e1fb52f91:/# cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=20.04
DISTRIB_CODENAME=focal
DISTRIB_DESCRIPTION=""Ubuntu 20.04.4 LTS""
```.

The documentation should be fixed to accurately indicate which Ubuntu version you'll get for each image.
```


### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

_No response_</details>"
57070,Tensorflow on Linux with conda is loading the process on GPU memory but executes with CPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Pop!_OS 22.04 LTS (Ubuntu based)

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

NVIDIA GeForce GT 1030

### Current Behaviour?


I just installed the last stable tensorflow version following the step by step guide from the [official documentation](https://www.tensorflow.org/install/pip#step-by-step_instructions).

Well, I have a NVIDIA GeForce GT 1030 so I first checked if it was in the cuda-enabled GPU list from [wikipedia](https://en.wikipedia.org/wiki/CUDA#GPUs_supported), and there is. Then I installed the cuda toolkit from system76 site, following [this official article](https://support.system76.com/articles/cuda/). (Obviously I restarted the system after successful installation).

After this I followed the tensorflow guide.

I can successfully run the sample code, but I always get the warning as showed in the log output. And here comes the weird part: I'd like to execute the code on GPU for better performance, and in some way tensorflow seems to recognise my GPU, but it doesn't execute the training on it, when I run the script the computers loads python process in the GPU memory, but the system monitor says that my GPU performs at only its 30% and the CPU starts working (of course in a beautyful parallelised manner using its 8 cores). So the fact is that I'm not currently running the neural-network training on the graphic card.

Is there a way to bring all the computational work on the GPU?

The log says I'm running a tensorflow binary optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA. Is this the reason of the general behaviour?

At this point I don't know what's going on:

- did I mis-installed something?
- is this an issue because of bad support of NVIDIA on linux?
- why is the program load on GPU but then is executed through CPU?
- is it a feature I don't understand?

Also when I run `nvcc -V` via CLI the output is like this

```
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Mon_Nov_30_19:08:53_PST_2020
Cuda compilation tools, release 11.2, V11.2.67
Build cuda_11.2.r11.2/compiler.29373293_0
```

but if I run `nvidia-smi` it says cuda version 11.7, which is a different version from `nvcc`... did I somehow installed cuda toolkit twice?


### Standalone code to reproduce the issue

```python
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras import optimizers

import numpy as np

(train_images, train_labels), _ = mnist.load_data()

num_images, img_x, img_y = train_images.shape

# linearize images
train_images = train_images.reshape( (num_images, img_x * img_y) )
train_images = train_images.astype(""float64"") / 255.0
# one-hot-encoding of labels
train_labels = to_categorical(train_labels)


# initialize the network
model = Sequential()

# add nodes to the network
model.add( Dense(15, input_shape=(img_x*img_y,) # input size
                ) )
model.add( Activation(""sigmoid"") )

model.add( Dense(10) )
model.add( Activation(""softmax"") )

# finalize the network
model.compile( optimizer=""rmsprop"",
               loss='categorical_crossentropy',
               metrics=['acc'] )

# train the network
hist = model.fit( x=train_images, # training examples
                  y=train_labels, # desired output
                  epochs=10,      # number of training epochs 
                  verbose=1)
```


### Relevant log output

```shell
2022-08-09 21:44:21.873432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-09 21:44:21.895547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-09 21:44:21.896061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-09 21:44:21.897810: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-08-09 21:44:21.899040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-09 21:44:21.899512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-09 21:44:21.899927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-09 21:44:22.244149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-09 21:44:22.244287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-09 21:44:22.244378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-08-09 21:44:22.244453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1264 MB memory:  -> device: 0, name: NVIDIA GeForce GT 1030, pci bus id: 0000:01:00.0, compute capability: 6.1
```
</details>"
57064,Android digit_classifier not working at all,The android [digit_classifier](https://github.com/tensorflow/examples/tree/master/lite/examples/digit_classifier/android) is not detecting the correct digit.
57061,Unable to load model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The attached code works on Linux (centos), but when run on Windows 10 it fails with the attached output.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import pickle
import numpy as np

tf.random.set_seed(42)

input_x = np.random.randint(0, 50000, (10000,1))
input_y = np.random.randint(0, 50000, (10000,1))
output = input_x + input_y
input = np.concatenate((input_x, input_y), axis=1)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(2, activation = tf.keras.activations.relu, input_shape=[2]),   
    tf.keras.layers.Dense(2, activation = tf.keras.activations.relu),
    tf.keras.layers.Dense(1),
])

model.compile(loss = tf.keras.losses.mae,
              optimizer=tf.optimizers.Adam(learning_rate=0.00001),
              metrics = ['mse'])
              
model.fit(input, output, epochs = 100)

fl = open('D:/tf/tf.pkl', 'wb')
pickle.dump(model, fl)
fl.close()

fl = open('D:/tf/tf.pkl', 'rb')
model = pickle.load(fl)
print(model.predict([[2.2, 5.1]]))
fl.close()
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""D:\tf\create_model.py"", line 29, in <module>
    model = pickle.load(fl)
  File ""C:\Users\developer\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\saving\pickle_utils.py"", line 48, in deserialize_model_from_bytecode
    model = save_module.load_model(temp_dir)
  File ""C:\Users\developer\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\developer\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\saved_model\load.py"", line 915, in load_partial
    raise FileNotFoundError(
FileNotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://6f4ad55a-d5dc-49ec-8a70-d9987bcb9422/variables/variables
 You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.
```
</details>"
57058,"""The Metal Performance Shaders operations encoded on it may not have completed.""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.0

### Custom Code

No

### OS Platform and Distribution

MaxOS 12.3.1 

### Mobile device

-

### Python version

3.9

### Bazel version

-

### GCC/Compiler version

-

### CUDA/cuDNN version

-

### GPU model and memory

M1 Max 26 cores GPU 32 GB RAM

### Current Behaviour?


Sometimes during training i get the following error:
```
Error: command buffer exited with error status.
	The Metal Performance Shaders operations encoded on it may not have completed.
	Error: 
	(null)
	Internal Error (0000000e:Internal Error)
	<AGXG13XFamilyCommandBuffer: 0x1665c64e0>
    label = <none> 
    device = <AGXG13XDevice: 0x12da25600>
        name = Apple M1 Max 
    commandQueue = <AGXG13XFamilyCommandQueue: 0x106477000>
        label = <none> 
        device = <AGXG13XDevice: 0x12da25600>
            name = Apple M1 Max 
    retainedReferences = 1
```
does not seem to cause any important issue in the training process, but maybe it's worth reporting it



### Standalone code to reproduce the issue

```shell
If you need it, I can share the jupyter notebook that I'm using
```


### Relevant log output

```shell
Error: command buffer exited with error status.
	The Metal Performance Shaders operations encoded on it may not have completed.
	Error: 
	(null)
	Internal Error (0000000e:Internal Error)
	<AGXG13XFamilyCommandBuffer: 0x1665c64e0>
    label = <none> 
    device = <AGXG13XDevice: 0x12da25600>
        name = Apple M1 Max 
    commandQueue = <AGXG13XFamilyCommandQueue: 0x106477000>
        label = <none> 
        device = <AGXG13XDevice: 0x12da25600>
            name = Apple M1 Max 
    retainedReferences = 1
```
</details>"
57057,GlobalAveragePooling2D-layer in mixed-precision mode and using CPU produces incorrect results,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

v2.8.0-rc1-32-g3f878cff5b6 2.8.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Given a certain input, when applying a GlobalAveragePooling2D-layer in mixed-precision mode and using CPU, then the output is bogus (zeros and nans). When using GPU, all works fine.
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
from tensorflow.keras import mixed_precision

# download the file from https://github.com/cgebbe/random_files/blob/main/x.npy
x = np.load(""x.npy"")
assert x.shape == (1, 256, 256, 32)

correct_result = np.array(
    [
        [
            0.1556,
            0.1864,
            0.4644,
            -0.2502,
            0.4453,
            0.3118,
            0.835,
            0.4453,
            0.26,
            0.452,
            -0.0783,
            -0.10693,
            0.1621,
            0.1736,
            0.9214,
            0.0373,
            0.2294,
            0.2252,
            0.5415,
            0.1819,
            0.2678,
            0.3835,
            0.0973,
            0.815,
            -0.013306,
            -0.1713,
            0.1719,
            0.577,
            0.3047,
            0.3525,
            0.315,
            1.783,
        ]
    ],
    dtype=np.float16,
)

wrong_result = np.array(
    [
        [
            0.0,
            0.0,
            0.0,
            -0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            -0.0,
            -0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            -0.0,
            -0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            np.nan,
        ]
    ],
    dtype=np.float16,
)

policy = mixed_precision.Policy(""mixed_float16"")
mixed_precision.set_global_policy(policy)
new_layer = tf.keras.layers.GlobalAveragePooling2D()

with tf.device(""/gpu""):
    y1 = new_layer(tf.convert_to_tensor(x))
    np.testing.assert_allclose(y1, correct_result)

with tf.device(""/cpu""):
    y2 = new_layer(tf.convert_to_tensor(x))
    np.testing.assert_allclose(y2, wrong_result)
```


### Relevant log output

_No response_</details>"
57055,error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 10 Home

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

Nvidia GeForce 1060 3GB

### Current Behaviour?

Running model_main_tf2.py from object_detection halts with error output:
```shell
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice
2022-08-09 14:54:40.390320: W tensorflow/core/framework/op_kernel.cc:1733] UNKNOWN: JIT compilation failed.
```
I've looked through the relevant issues to this but so far I've only seen solutions for linux.
Adding the path C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.7 to path variables including creating a new called CUDA_DIR does nothing.


### Standalone code to reproduce the issue
Command used:

```shell
python model_main_tf2.py --model_dir=models/my_efficientdet_d4_coco17_tpu-32 --pipeline_config_path=models/my_efficientdet_d4_coco17_tpu-32/pipeline.config
```

### Relevant log output

_No response_</details>"
57052, Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When running `model.predict` I am getting the warning. What does that mean and how can I fix it ?
```


### Standalone code to reproduce the issue

```shell
The model is a keras.functional.Functional model where the first layers are from tensorflow hub pretrained models. I have two dense layers in the end. compiling, fitting do not give any warning however when running `model.predict([""examplestring""])` I get the warning.
```


### Relevant log output

```shell
2022-08-09 15:37:17.301561: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected a subtype of type_id: TFT_TENSOR
args {
  type_id: TFT_LEGACY_VARIANT
}
 for input 2 of a homogeneous container 1001, got type_id: TFT_RAGGED
args {
  type_id: TFT_INT32
}

        while inferring type of node 'model/preprocessing/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bert_pack_inputs/PartitionedCall/map/while/body/_337/map/while/TensorArrayV2Write/TensorListSetItem'
```
</details>"
57049, SIGSEGV(SEGV_MAPERR) with Tensorflow Lite on Android Platform,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.91

### Custom Code

Yes

### OS Platform and Distribution

Android 

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.2.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Got native crash when using Tensorflow lite on android.
Thes stacktrack has 2 root cause:
first:  crash atorg_tensorflow/external/ruy/ruy/context_get_ctx.cc:25 
second: crash at org_tensorflow/tensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc:2236
```


### Standalone code to reproduce the issue

```shell
Only a few user got this crash, can't be reproduced on local
```


### Relevant log output

```shell
1:
org_tensorflow/external/ruy/ruy/context_get_ctx.cc:25
org_tensorflow/external/ruy/ruy/ruy.h:47
org_tensorflow/external/ruy/ruy/ruy.h:101
org_tensorflow/./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141
org_tensorflow/./tensorflow/lite/kernels/cpu_backend_gemm.h:169
org_tensorflow/./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:299
org_tensorflow/tensorflow/lite/kernels/lstm_eval.cc:59
org_tensorflow/tensorflow/lite/kernels/lstm_eval.cc:206
org_tensorflow/tensorflow/lite/kernels/lstm_eval.cc:913
org_tensorflow/tensorflow/lite/kernels/lstm_eval.cc:1894
org_tensorflow/tensorflow/lite/kernels/unidirectional_sequence_lstm.cc:1332
org_tensorflow/tensorflow/lite/core/subgraph.cc:1013
org_tensorflow/tensorflow/lite/core/subgraph.cc:1280
org_tensorflow/tensorflow/lite/interpreter.cc:230
org_tensorflow/tensorflow/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc:539


2:
org_tensorflow/tensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc:2236
org_tensorflow/tensorflow/lite/kernels/lstm_eval.cc:893
org_tensorflow/tensorflow/lite/kernels/lstm_eval.cc:1894
org_tensorflow/tensorflow/lite/kernels/unidirectional_sequence_lstm.cc:1332
org_tensorflow/tensorflow/lite/core/subgraph.cc:1013
org_tensorflow/tensorflow/lite/core/subgraph.cc:1280
org_tensorflow/tensorflow/lite/interpreter.cc:230
org_tensorflow/tensorflow/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc:539
```
</details>"
57043,Shifted/reordered YOLO output layers when converting with TfLite 2.8 or higher,"System info:
Ubuntu 22.04 LTS
Python: Python 3.10.4
Tensorflow Lite: 2.8 and higher

Our YOLO network has three output layers.

The (correct) output from Keras (on the .h5), TfLite2.5.0, and my company’s custom-made inference engine is:

```
Output layer 0:  1.4805529   0.22662726 ...
Output layer 1:  0.42533997 -0.17623788 ...
Output layer 2: -1.0985005  -0.11061229 ...
```

Converting a YOLO network from .h5 to to .tflite with TfLite2.8 or higher, and then perform inference incorrectly outputs:

```
Output layer 0: -1.0985005  -0.11061229 ...  //== old output layer 2
Output layer 1:  1.4805529   0.22662726 ...  //== old output layer 0
Output layer 2:  0.42533997 -0.17623788 ...  //== old output layer 1
```

As if the output layers have shifted circularly or have reordered.

Work-around:
`converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(""<my_yolo_network>.h5"")`
Still, I consider downgrading TfLite or use `tf.compat.v1.lite` not a future-proof fix.

Python code is below. I cannot share the network file, because it’s company IP.

```
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'  #Notify Tensorflow there is no CUDA GPU
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  #Notify Tensorflow to not print info messages

import numpy as np

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.preprocessing.image import array_to_img

#Load trained Keras model to file
keras_file = ""<my_yolo_network>.h5""
keras_model = keras.models.load_model(keras_file, compile=False)

#Set up test image
img = load_img(""<my_image>.png"", color_mode='grayscale')
img = img.resize((128,128))
img_array = img_to_array(img)
img_array = img_array / 255
img_array = np.expand_dims(img_array, axis=0)

#Infer using Keras
keras_out = keras_model.predict(img_array)
print(""Keras output layer 0:"", keras_out[0][0][0][0][0], keras_out[0][0][0][0][1])
print(""Keras output layer 1:"", keras_out[1][0][0][0][0], keras_out[1][0][0][0][1])
print(""Keras output layer 2:"", keras_out[2][0][0][0][0], keras_out[2][0][0][0][1])

#Convert Keras model to a Tensorflow Lite model
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
#converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(""<my_yolo_network>.h5"")
tflite_model = converter.convert()

#Construct Tensorflow Lite interpreter
interpreter = tf.lite.Interpreter(model_content=tflite_model)
#interpreter = tf.lite.Interpreter(model_path=""<my_yolo_network>.tflite"")

#Set up input layer
input_details = interpreter.get_input_details()
interpreter.resize_tensor_input(input_details[0]['index'], (1,128,128,1))  #Input is one 128x128 grayscale image
interpreter.allocate_tensors()
interpreter.set_tensor(input_details[0]['index'], img_array)  #Copy image data to input layer

#Infer
interpreter.invoke()

#Print output
output_details = interpreter.get_output_details()
tflite_out = interpreter.get_tensor(output_details[0]['index'])  #Yolo has three output layers, so index at 0, 1, or 2
print(""TfLite output layer 0:"", tflite_out[0][0][0][0], tflite_out[0][0][0][1])
tflite_out = interpreter.get_tensor(output_details[1]['index'])
print(""TfLite output layer 1:"", tflite_out[0][0][0][0], tflite_out[0][0][0][1])
tflite_out = interpreter.get_tensor(output_details[2]['index'])
print(""TfLite output layer 2:"", tflite_out[0][0][0][0], tflite_out[0][0][0][1])
```

Similar issues:
https://github.com/tensorflow/tensorflow/issues/51858
https://github.com/tensorflow/tensorflow/issues/47927
https://github.com/tensorflow/tensorflow/issues/33303
"
57041,Add bazel rule to selectively build TFLite by ops instead of model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

2.9, all

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


Per Tensorflow Lite's documentation on reducing binary size (https://www.tensorflow.org/lite/guide/reduce_binary_size), one way to reduce TFLite's binary size is to build it with a reduced set of ops based on a list of models on disk. It'd be nice if instead of a list of models, users could instead specify a list of ops that they want included in the binary (for both android and iOS). This is useful in environments where you may not have the models immediately available, but know the set of ops the group of models will want.



### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>"
57040,xla && dynamic graph,what is the relationship of xla and dynamic graph？
57039,"Please update the Simplified Chinese documents, or at least set an indicator to tell readers the documents are out of date","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

NA

### Custom Code

No

### OS Platform and Distribution

NA

### Mobile device

NA

### Python version

NA

### Bazel version

NA

### GCC/Compiler version

NA

### CUDA/cuDNN version

NA

### GPU model and memory

NA

### Current Behaviour?

https://www.tensorflow.org/lite/performance/gpu_advanced?hl=zh-cn#android_cc

> bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:gl_delegate                  # for static library
> bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_gl.so  # for dynamic library

https://www.tensorflow.org/lite/performance/gpu_advanced?hl=en-us#android_cc

> bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:delegate                           # for static library
> bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so  # for dynamic library

I read documents in all 20 languages but only the Simplified Chinese version is out of date, and it doesn't have even an indicator to tell me that it is out of date.

The old version library, gl_delegate, its header gl_delegate.h shows

``` c
ABSL_DEPRECATED(""Use TfLiteGpuDelegateV2Create defined in delegate.h instead."")
TFL_CAPI_EXPORT TfLiteDelegate* TfLiteGpuDelegateCreate(
    const TfLiteGpuDelegateOptions* options);
```


### Standalone code to reproduce the issue

```shell
NA
```


### Relevant log output

```shell
NA
```
</details>"
57038,Cannot compile Hexagon delegate properly,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Android NDK r21e

### Mobile device

Android 10

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

clang 9.0.9

### CUDA/cuDNN version

_No response_

### GPU model and memory

QQcom Adreno 650

### Current Behaviour?

```shell
I'm currently trying to recompile the Hexagon DSP, when recompiling I get the following issue:
""tensorflow/lite/delegates/hexagon/builders/op_builder.h:91:44: error: use of undeclared identifier 'OP_Const' void SetConstNode() { op_node_.op_type = OP_Const; }""

It seems that the enum 'OP_const' is not available, when checking the source file ""hexagon/hexagon_nn_ops.h"" it seems indeed not defined there.
Would you have an idea how to solve this ?
```


### Standalone code to reproduce the issue

```shell
Recompile Hexagon delegate manually.
```


### Relevant log output

```shell
96%] Building CXX object CMakeFiles/tensorflowlite_hexagon_delegate.dir/mnt/c/DevRoot/git/tensorflow/tensorflow/lite/delegates/hexagon/hexagon_delegate.cc.o
[ 96%] Building CXX object CMakeFiles/tensorflowlite_hexagon_delegate.dir/mnt/c/DevRoot/git/tensorflow/tensorflow/lite/delegates/hexagon/hexagon_delegate_kernel.cc.o
[ 96%] Built target tensorflowlite_c
[ 96%] Built target tensorflowlite_gpu_delegate
In file included from /mnt/c/DevRoot/git/tensorflow/tensorflow/lite/delegates/hexagon/hexagon_delegate.cc:23:
In file included from /mnt/c/DevRoot/git/tensorflow/tensorflow/lite/delegates/hexagon/hexagon_delegate_kernel.h:30:
/mnt/c/DevRoot/git/tensorflow/tensorflow/lite/delegates/hexagon/builders/op_builder.h:91:44: error: use of undeclared identifier 'OP_Const'
  void SetConstNode() { op_node_.op_type = OP_Const; }
                                           ^
/mnt/c/DevRoot/git/tensorflow/tensorflow/lite/delegates/hexagon/builders/op_builder.h:102:57: error: use of undeclared identifier 'OP_Const'
  bool IsConstNode() const { return op_node_.op_type == OP_Const; }
                                                        ^
In file included from /mnt/c/DevRoot/git/tensorflow/tensorflow/lite/delegates/hexagon/hexagon_delegate_kernel.cc:15:
In file included from /mnt/c/DevRoot/git/tensorflow/tensorflow/lite/delegates/hexagon/hexagon_delegate_kernel.h:30:
/mnt/c/DevRoot/git/tensorflow/tensorflow/lite/delegates/hexagon/builders/op_builder.h:91:44: error: use of undeclared identifier 'OP_Const'
  void SetConstNode() { op_node_.op_type = OP_Const; }
                                           ^
/mnt/c/DevRoot/git/tensorflow/tensorflow/lite/delegates/hexagon/builders/op_builder.h:102:57: error: use of undeclared identifier 'OP_Const'
  bool IsConstNode() const { return op_node_.op_type == OP_Const; }
```
</details>"
57037,Pip install very slow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

2.6.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

Ubuntu

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
pip install --upgrade tensorflow==2.6.1
Want normal speed
Country:Belarus.
No VPN
```


### Standalone code to reproduce the issue

```shell
Speed is 80kb/s
```


### Relevant log output

_No response_</details>"
57036,"ValueError: Protocol message has no oneof ""optional_autotune_algorithm"" field.","I just start learning tensorflow object detection API. I'm using [this video](https://www.youtube.com/watch?v=yqkISICHH-U) to learn tensorflow object detection. At 8553s, I run the same code, but my command prompt said ```ValueError: Protocol message has no oneof ""optional_autotune_algorithm"" field.``` I have no idea how to solve this. I use 2.9.1 tensorflow, 3.9.0 python, and Windows os. Also, I don't use cudnn because I use Radeon rx580.

Below is all of my error:
```
Traceback (most recent call last):
  File ""C:\Tensorflow-object-detection\TFODCourse\Tensorflow\models\research\object_detection\model_main_tf2.py"", line 114, in <module>
    tf.compat.v1.app.run()
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\tensorflow\python\platform\app.py"", line 36, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\absl\app.py"", line 312, in run
    _run_main(main, args)
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\absl\app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""C:\Tensorflow-object-detection\TFODCourse\Tensorflow\models\research\object_detection\model_main_tf2.py"", line 105, in main
    model_lib_v2.train_loop(
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\object_detection-0.1-py3.9.egg\object_detection\model_lib_v2.py"", line 605, in train_loop
    load_fine_tune_checkpoint(
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\object_detection-0.1-py3.9.egg\object_detection\model_lib_v2.py"", line 401, in load_fine_tune_checkpoint
    _ensure_model_is_built(model, input_dataset, unpad_groundtruth_tensors)
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\object_detection-0.1-py3.9.egg\object_detection\model_lib_v2.py"", line 161, in _ensure_model_is_built
    features, labels = iter(input_dataset).next()
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 1431, in __iter__
    iterators = _create_iterators_per_worker(
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 1857, in _create_iterators_per_worker
    iterator = _SingleWorkerOwnedDatasetIterator(
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 1761, in __init__
    super(_SingleWorkerOwnedDatasetIterator,
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 1585, in __init__
    self._make_iterator()
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 1792, in _make_iterator
    self._create_owned_multi_device_iterator()
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\tensorflow\python\distribute\input_lib.py"", line 1783, in _create_owned_multi_device_iterator
    self._iterator = multi_device_iterator_ops.OwnedMultiDeviceIterator(
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\tensorflow\python\data\ops\multi_device_iterator_ops.py"", line 473, in __init__
    experimental_slack = dataset.options().experimental_slack
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 4010, in options
    return self._dataset.options()
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 453, in options
    options = self._options_tensor_to_options(self._options())
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 443, in _options_tensor_to_options
    options._from_proto(pb)  # pylint: disable=protected-access
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\tensorflow\python\data\ops\options.py"", line 610, in _from_proto
    self.autotune._from_proto(pb.autotune_options)  # pylint: disable=protected-access
  File ""C:\Tensorflow-object-detection\TFODCourse\tfod\lib\site-packages\tensorflow\python\data\ops\options.py"", line 241, in _from_proto
    if pb.WhichOneof(""optional_autotune_algorithm"") is not None:
ValueError: Protocol message has no oneof ""optional_autotune_algorithm"" field.
```"
57035,Cannot import a package from Tensorflow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.0.0-rc2

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

PyCharm version: 2019.3 EAP - Build #PY-193.3793.15

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Cannot import packages from Tensorflow in Pycharm. There is a confusion of whether this error had occured due to the version of IDE or the version of Tensorflow.
```


### Standalone code to reproduce the issue

```shell
CODE:
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
```


### Relevant log output

```shell
ERROR:
Cannot find reference 'keras' in '__init__.py'
Unresolved reference 'Conv2D'
```
</details>"
57034,ValueError: Quantizing a tf.keras Model inside another tf.keras Model is not supported.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

2.6.0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Quantization aware training in Keras incompatible with tf.keras.
```


### Standalone code to reproduce the issue

```shell
import tensorflow_model_optimization as tfmot

quantize_model = tfmot.quantization.keras.quantize_model
q_aware_model = quantize_model(model)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""train.py"", line 31, in <module>
    trainer.train(config)
  File ""/code/src/trainers/trainer_quant.py"", line 161, in train
    q_aware_model = quantize_model(model)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py"", line 138, in quantize_model
    annotated_model = quantize_annotate_model(to_quantize)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py"", line 211, in quantize_annotate_model
    to_annotate, input_tensors=None, clone_function=_add_quant_wrapper)
  File ""/usr/local/lib/python3.6/dist-packages/keras/models.py"", line 452, in clone_model
    model, input_tensors=input_tensors, layer_fn=clone_function)
  File ""/usr/local/lib/python3.6/dist-packages/keras/models.py"", line 192, in _clone_functional_model
    model, new_input_layers, layer_fn)
  File ""/usr/local/lib/python3.6/dist-packages/keras/models.py"", line 245, in _clone_layers_and_model_config
    model, serialize_layer_fn=_copy_layer)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/functional.py"", line 1349, in get_network_config
    layer_config = serialize_layer_fn(layer)
  File ""/usr/local/lib/python3.6/dist-packages/keras/models.py"", line 241, in _copy_layer
    created_layers[layer.name] = layer_fn(layer)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py"", line 205, in _add_quant_wrapper
    'Quantizing a tf.keras Model inside another tf.keras Model is not supported.'
ValueError: Quantizing a tf.keras Model inside another tf.keras Model is not supported.
```
</details>"
57033,multiple definition of `tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tensorflow-2.9.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04 LTS (GNU/Linux 5.15.0-43-generic x86_64)

### Mobile device

android arm64

### Python version

Python 3.10.4

### Bazel version

bazel 5.0.0

### GCC/Compiler version

ndk 21.4.7075529

### CUDA/cuDNN version

NA

### GPU model and memory

NA

### Current Behaviour?

I followed the official document to build tensorflow-lite with select ops from source, ducument url: https://www.tensorflow.org/lite/guide/ops_select?hl=zh-cn#c

The document tell me that:
> 从下面的方案中选择一个：
> 在用 bazel build 命令编译 TensorFlow Lite 时添加 --define=with_select_tf_ops=true 编译标记。
> 在编译依赖中添加 TensorFlow 运算符库依赖 tensorflow/lite/delegates/flex:delegate。

I tried the first, then I got a successful result but also a tensorflow lite shared library which didn't contain select ops, so I tried the second.
I wrote the following build script to cross-compile tensorflow 2.9.1

``` sh
#!/usr/bin/env bash

set -evx

tf=tensorflow-2.9.1

if [ -d ""$tf"" ]; then
    rm -rf $tf
fi

tar -xf tensorflow-2.9.1.tar.gz
pushd tensorflow-2.9.1
    cat > .tf_configure.bazelrc << EOF
build --action_env ANDROID_NDK_HOME=""$ANDROID_NDK_HOME""
build --action_env ANDROID_NDK_API_LEVEL=""33""
build --action_env ANDROID_BUILD_TOOLS_VERSION=""33.0.0""
build --action_env ANDROID_SDK_API_LEVEL=""33""
build --action_env ANDROID_SDK_HOME=""$ANDROID_SDK_HOME""
EOF
    cat .tf_configure.bazelrc

    sed -i '1215i ""//tensorflow/lite/delegates/flex:delegate"",' tensorflow/lite/BUILD

    bazel \
        build \
        -c opt \
        --fat_apk_cpu=arm64-v8a \
        --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
        --config=android_arm64 \
        --config=monolithic \
        --verbose_failures \
        //tensorflow/lite:libtensorflowlite.so
popd
```

and it produces the following failing result:

```
# Configuration: 5b254af6e42f0099eacf2893d40a00ee4bd5ca890db4dfcdc666730641843978
# Execution platform: @local_execution_config_platform//:platform
bazel-out/arm64-v8a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops_all_linked.pic.lo(register.pic.o): In function `~__base':
/proc/self/cwd/tensorflow/lite/kernels/register.cc:36: multiple definition of `tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'
bazel-out/arm64-v8a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o):/proc/self/cwd/tensorflow/lite/kernels/register.cc:36: first defined here
bazel-out/arm64-v8a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops_all_linked.pic.lo(register.pic.o): In function `~__base':
/proc/self/cwd/tensorflow/lite/kernels/register.cc:36: multiple definition of `tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'
bazel-out/arm64-v8a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o):/proc/self/cwd/tensorflow/lite/kernels/register.cc:36: first defined here
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/lite:libtensorflowlite.so failed to build
INFO: Elapsed time: 106.618s, Critical Path: 39.19s
INFO: 4 processes: 2 internal, 2 local.
FAILED: Build did NOT complete successfully
```

Then I took a look at tensorflow/lite/BUILD and found a workaround solution which can solve the double link error: remove the `//tensorflow/lite/kernels:builtin_ops_all_linked` dependency in the deps of `tflite_cc_shared_object`.

So the following building script works:

``` sh
#!/usr/bin/env bash

set -evx

tf=tensorflow-2.9.1

if [ -d ""$tf"" ]; then
    rm -rf $tf
fi

tar -xf tensorflow-2.9.1.tar.gz
pushd tensorflow-2.9.1
    cat > .tf_configure.bazelrc << EOF
build --action_env ANDROID_NDK_HOME=""$ANDROID_NDK_HOME""
build --action_env ANDROID_NDK_API_LEVEL=""33""
build --action_env ANDROID_BUILD_TOOLS_VERSION=""33.0.0""
build --action_env ANDROID_SDK_API_LEVEL=""33""
build --action_env ANDROID_SDK_HOME=""$ANDROID_SDK_HOME""
EOF
    cat .tf_configure.bazelrc

    sed -i '1215d' tensorflow/lite/BUILD
    sed -i '1215i ""//tensorflow/lite/delegates/flex:delegate"",' tensorflow/lite/BUILD

    bazel \
        build \
        -c opt \
        --fat_apk_cpu=arm64-v8a \
        --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
        --config=android_arm64 \
        --config=monolithic \
        --verbose_failures \
        //tensorflow/lite:libtensorflowlite.so
popd
```

### Standalone code to reproduce the issue

```shell
see the above Current Behaviour section
```


### Relevant log output

```shell
see the above Current Behaviour section
```
</details>"
57032,Bilinear layer,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
57031,Run object detection model on arduino-like board,"How can I run an object detection model on an arduino-like board using tflite micro. I can convert it to a model.cc, but I don't know how to run it from there on the microcontroller?

Can you please help with beginners instructions to use a object detection model trained and converted to tflite on a Arduino like board?

Thanks in advance for your help.
"
57030,Run audio recognition model on arduino-like board,"How can I run an audio recognition model on an arduino-like board using tflite micro. I can convert it to a model.cc, but I don't know how to run it from there on the microcontroller?

Any documentation and clear beginners instructions will be helpful.

Thanks in advance for your help."
57029,`tf.bitwise.right_shift` have different behaviors in cpu and gpu,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When `y` is negative for `tf.bitwise.right_shift`, according to the [documentation](https://www.tensorflow.org/api_docs/python/tf/bitwise/right_shift) ""the result is implementation defined"". Currently in the code example, when `y=-1`, the results on CPU and GPU are not the same, I wonder if this is a bug and the implementation on cpu and gpu should be the same?



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""gpu""):
  X = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.int32)
  Z = tf.bitwise.right_shift(X, -1)
  print(Z)
with tf.device(""cpu""):
  X = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.int32)
  Z = tf.bitwise.right_shift(X, -1)
  print(Z)
```


### Relevant log output

```shell
tf.Tensor(
[[0 0 0]
 [0 0 0]], shape=(2, 3), dtype=int32)
tf.Tensor(
[[1 2 3]
 [4 5 6]], shape=(2, 3), dtype=int32)
```
</details>"
57027,tflite.Interpretter.set_tensor does not support UINT16,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.0

### Custom Code

No

### OS Platform and Distribution

linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Attempting to pass an input tensor with type UINT16 to the tflite interpreter via set_tensor result in the error:

Cannot set tensor: Got value of type NOTYPE but expected type UINT16 for input 0, name: tensor-0

Support for UINT16 was originally introduced by commit: https://github.com/tensorflow/tensorflow/commit/d19a8c80d4cb751c2a7e117eeeb56ac09cf1daa7

This commit did not update the function: tensorflow/lite/python/interpreter_wrapper/numpy.cc:TfLiteTypeFromPyType(), hence while the internals of the tflite interpreter is UINT16 aware, it is not possible to pass tensors of type UINT16 into the interpreter.
```


### Standalone code to reproduce the issue

```shell
NA
```


### Relevant log output

_No response_</details>"
57026,Reducing Ops in Hexagon Delegate Use Wrong Axis Index When Input Tensor <4D,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04 LTS

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.0.0

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?


If there is a mean-reducing op that is executed by the Hexagon Delegate, the axis indices are unchanged from the original inputs.  This causes a problem if the tensor being reduced is of rank < 4.

Because Qualcomm's nnlib needs 4D input tensors, the TF code [adds dummy dimensions of size 1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/builders/op_builder.h#L177-L184) to the shape.  But since the reduction axis is unchanged, the nnlib implementation thinks the wrong axis is [being reduced over](https://source.codeaurora.org/quic/hexagon_nn/nnlib/tree/hexagon/ops/src/op_reducing_mean.c#n268) and so fails a sanity check on the shape of the output tensor.

I.e. on CPU the (correct) behavior is:

Input tensor shape -> [1, 48, 768]
Axis = 2
Output tensor shape -> [1, 48, 1]

but in the hexagon delegate we pad the dimensions to make it 4D so we get:

Input tensor shape -> [1, 1, 48, 768]
Axis = 2
Output tensor shape -> [1, 1, 1, 768]

I've started on a fix in https://github.com/tensorflow/tensorflow/pull/57021 but I need some help with it - this approach creates a new tensor from the original axes tensor, increments the value by the number of dummy dimensions, and then tries to add this new tensor instead of the original.

This runs (as long as the axes_tensor is a list, not a scalar), but it leaks memory, and there is an error relating to trying to add a duplicate tensor (`ERROR: Trying to add duplicate tensor without overwrite, tflite_tensor_id 1, hexagon_node_id 16, hexagon_node_output_id 0`).  I can't modify the original axes tensor directly because it has a read-only allocation type, and trying causes a segmentation fault.



### Standalone code to reproduce the issue

1. Create dummy tflite model using https://colab.research.google.com/drive/1cyjF6dx31T-NxmPgFwOdAXPQcUYD7685?usp=sharing
2. Run with `./benchmark_model_plus_flex --graph=dummy_model_int8.tflite --use_hexagon=true` on a device that can use the Hexagon delegate.


### Relevant log output

The dummy model in the final layers reduces from a tensor of shape (32, 12) to (32, 1) - but because the reduction axis is 1, the downstream code is expected to reduce over (1, 1, 32, 12) for an output of (1, 1, 32, 12).

```shell
----------------
Timestamp: Sat Aug  6 03:20:01 2022


Log
hexagon/ops/src/op_reducing_mean.c:268:Output tensor is of size 32, but expected output tensor of size 384
hexagon/src/execute.c:167:execute() failed on node id=f err=-1
hexagon/src/interface.c:1297:fail in execute_inner()

----------------
ERROR: Failed: Failed to execute graph..
ERROR: Node number 4 (TfLiteHexagonDelegate) failed to invoke.
```
</details>"
57018,StringLookup Skips Null Character at the End,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tensorflow==2.8.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The null character at the end is skipped when I run tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=vocab, output_mode='count'). 
Expected = ['[UNK]', 'ab\x00', '\x00ab', 'a\x00b']
Actual Output = ['[UNK]', 'ab', '\x00ab', 'a\x00b']
```


### Standalone code to reproduce the issue

```shell
>>> import tensorflow as tf
>>> vocab = [b'ab\x00', b'\x00ab', b'a\x00b']
>>> vocab_lookup = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=vocab, output_mode='count')
>>> print(vocab_lookup.get_vocabulary())
```


### Relevant log output

```shell
['[UNK]', 'ab', '\x00ab', 'a\x00b']
```
</details>"
57017,How to save checkpoint every 5 epoch  instead of every epoch  ,"From the following link 

https://stackoverflow.com/a/70624790/19119811

How to save checkpoints for every 5 or 10 epoch instead of every epoch ??

Refer :
```

# Include the epoch in the file name (uses `str.format`)
checkpoint_path = ""training_2/cp-{epoch:04d}.ckpt""
checkpoint_dir = os.path.dirname(checkpoint_path)

batch_size = 32

# Create a callback that saves the model's weights every 5 epochs
cp_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path, 
    verbose=1, 
    save_weights_only=True,
    save_freq=5*batch_size)

# Create a new model instance
model = create_model()

# Save the weights using the `checkpoint_path` format
model.save_weights(checkpoint_path.format(epoch=0))

# Train the model with the new callback
model.fit(train_images, 
          train_labels,
          epochs=50, 
          batch_size=batch_size, 
          callbacks=[cp_callback],
          validation_data=(test_images, test_labels),
          verbose=0)"
57016,Problems objection detection api on MacBook M1,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.9.2

### Custom Code

No

### OS Platform and Distribution

MacOs M1

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Trying run Tensorflow Object Detection API on my Mac and I did try to test my installation with this python object_detection/builders/model_builder_test.py. The API is installed with this guide https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html

!(tensorflow) sepi@Petris-MacBook-Air research % python object_detection/builders/model_builder_test.py
/Users/sepi/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/Users/sepi/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']
caused by: [""[Errno 2] The file to load file system plugin from does not exist.: '/Users/sepi/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so'""]
  warnings.warn(f""unable to load libtensorflow_io_plugins.so: {e}"")
/Users/sepi/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/Users/sepi/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']
caused by: [""dlopen(/Users/sepi/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so, 0x0006): tried: '/Users/sepi/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/sepi/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so' (no such file), '/Users/sepi/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so' (no such file)""]
  warnings.warn(f""file system plugins are not loaded: {e}"")
```


### Standalone code to reproduce the issue

```shell
python object_detection/builders/model_builder_test.py
```


### Relevant log output

_No response_</details>"
57015,"When I set unroll of LSTM to True, conversion problems occurs.","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

windows10

### Mobile device

_No response_

### Python version

3.9

### Current Behaviour?

```shell
My model uses the LSTM layer.During training,parameters of LSTM, unroll=False and stateful=False.
After training, I convert the model to tflite.
If I still set unroll=False and stateful=False during conversion, then there is a UnidirectionalSequenceLSTM layer in the tflite file. 
Unfortunately, I need to use tensorflow lite for micro. 
UnidirectionalSequenceLSTM layer is not supported in tensorflow lite for micro now.
So I need to set unroll=True and stateful=False during conversion.
This setting can avoid using UnidirectionalSequenceLSTM.
Test result displays that tensorflow lite for micro support the model with parameters unroll=True and stateful=False. 
However,the result of inference with ""unroll=True and stateful=False"" is different from that of inference with ""unroll=False and stateful=False"".
How to make the two results consistent.
```


### Standalone code to reproduce the issue

```shell
During training:
  P = Dense(units=self.nbin, activation='tanh')(P)
  P = Dense(units=self.nbin, activation='tanh')(P)
  P = LSTM(units=self.nbin, activation='tanh', return_sequences=True, stateful=False)(P)
  P = Dense(units=self.nbin, activation='softplus')(P)

  Be = Lambda(self.forward)(e)
  E = Dense(units=self.nbin, activation='linear')(Be)
  E = LSTM(units=self.nbin, activation='tanh', return_sequences=True, stateful=False)(E)
  Z = E*P
  Z = Dense(units=self.wlen_z, activation='linear')(Z)

During converting:
  P = Dense(units=self.nbin, activation='tanh')(P)
  P = Dense(units=self.nbin, activation='tanh')(P)
  P = LSTM(units=self.nbin, activation='tanh', return_sequences=True, unroll=True,stateful=False)(P)
  P = Dense(units=self.nbin, activation='softplus')(P)

  Be = Lambda(self.forward)(e)
  E = Dense(units=self.nbin, activation='linear')(Be)
  E = LSTM(units=self.nbin, activation='tanh', return_sequences=True, unroll=True,stateful=False)(E)
  Z = E*P
  Z = Dense(units=self.wlen_z, activation='linear')(Z)
```


### Relevant log output

```shell
The result of inference with ""unroll=True and stateful=False"" is different from that of inference with ""unroll=False and stateful=False"".
```
</details>"
57010,tf.constant with tf.float16 results in incorrect outputs with Mac M1 chip,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.2

### Custom Code

No

### OS Platform and Distribution

MacOS 12.3 arm64

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When using the pip wheel for tensorflow_macos==2.9.2 (tensorflow_macos-2.9.2-cp39-cp39-macosx_11_0_arm64.whl), using tf.constant in graph mode, or equivalently in tf.function() decorated functions, with the meta-optimizer enabled leads to incorrect tensor contents. Disabling the meta-optimizer results in correct behavior. Using tf.float32 also leads to correct behavior.  

Examination of the output generated with `TF_CPP_MIN_LOG_LEVEL=0 TF_CPP_MAX_VLOG_LEVEL=3` indeed shows that the binary content of the tensor is simply the value passed in, rather than the actual binary representation of float16. 

This issue https://github.com/tensorflow/tensorflow/issues/53260 also suffers from the same problem. 
Also this seems to be a Mac M1 ARM64 wheel-specific problem. The behaviour is correct for Linux-based systems and wheels.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf


def f(dtype, disable_meta_optimizer: bool):
  tf.config.optimizer.set_experimental_options(
    options={
      ""disable_meta_optimizer"": disable_meta_optimizer,
      ""min_graph_nodes"": 1, # the graph will only consist of a single node, default is 4
    }
  )
  print(f""dtype={dtype} disable_meta_optimizer={disable_meta_optimizer}"")
  with tf.Graph().as_default() as g, tf.compat.v1.Session(graph=g) as s:
    t = tf.constant(1.0, dtype=dtype)
    try:
      fetch = s.run(t)
      assert fetch.astype(np.float32) == np.full([], 1.0, np.float32)
    except AssertionError:
      print(f""Fail! Contents of fetched tensor: {fetch}"")
    else:
      print(f""Success!"")


for dtype in [tf.float32, tf.float16]:
  for disable_meta_optimizer in [True, False]:
    f(dtype, disable_meta_optimizer)
```


### Relevant log output

```shell
dtype=<dtype: 'float32'> disable_meta_optimizer=True
Success!
dtype=<dtype: 'float32'> disable_meta_optimizer=False
Success!
dtype=<dtype: 'float16'> disable_meta_optimizer=True
Success!
dtype=<dtype: 'float16'> disable_meta_optimizer=False
Fail! Contents of fetched tensor: 0.0
```
</details>"
57006,Runtime Error: Node number 93 (CONCATENATION) failed to prepare,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18 LTS
- TensorFlow installation (pip package or built from source): TF 2.9 installed using pip
- TensorFlow library (version, if pip package or github SHA, if built from source): TF 2.9 installed using pip

### 2. Code

```
# -*- coding: utf-8 -*-
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.utils import Sequence
import os
import random
from glob import glob
from PIL import Image
import numpy as np
import pathlib
import cv2
import onnx
from onnx_tf.backend import prepare
base_dir = ""/media/sf_C_DRIVE/Users/ML/Desktop/blindbot_main""
modelpath = ""/media/sf_C_DRIVE/Users/ML/Desktop/combined_model.onnx""
modelpath_keras = ""/media/sf_C_DRIVE/Users/ML/Desktop/combined_model.hdf5""

class Data_Generator(Sequence):

    def __init__(self, image_filenames, batch_size):
        self.image_filenames = image_filenames
        self.batch_size = batch_size

    def __len__(self):
        return int(np.ceil(len(self.image_filenames) / float(self.batch_size)))

    def __getitem__(self, idx):
      batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]
      image = (np.array([ np.array(Image.open(file_name).resize(size=(320, 1024)), np.float32) for file_name in batch_x])/255)
      image = np.transpose(image, (0, 3, 2, 1))
      return image

validation_filenames = glob(base_dir + ""/**/*.jpg"",recursive=True)
random.shuffle(validation_filenames)
val_generator = Data_Generator(image_filenames=validation_filenames, batch_size=1)



def representative_data_gen():
  for i in range(1000):
    imgs = val_generator.__getitem__(i)
    yield [imgs[0:1]]

converter = tf.lite.TFLiteConverter.from_saved_model(""/media/sf_C_DRIVE/Users/ML/Desktop/blindbot_main/tfmodel.pb"")
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8       # try to implement the model in tensorflow / try with int8 / try release candidate version for tensorflow / try model without concatenation layers / open issue on github of tensorflow / try monodepth tensorflow version
converter.inference_output_type = tf.int8
converter.representative_dataset = representative_data_gen
converter.inference_type = tf.int8

tflite_models_dir = pathlib.Path(""/media/sf_ML/Models/tflite_quantisized_models/"")
tflite_models_dir.mkdir(exist_ok=True, parents=True)

tflite_model_quant = converter.convert()
tflite_model_quant_file = tflite_models_dir/""model_quant.tflite""
tflite_model_quant_file.write_bytes(tflite_model_quant)
print(""done!"")
```

### 3.Error: 
The conversion fails with an error: 
```
tensorflow/lite/kernels/concatenation.cc:80 t->dims->data[d] != t0->dims->data[d] (20 != 18)Node number 93 (CONCATENATION) failed to prepare.
  File ""/media/sf_ML/Python_Files/converttflite_onnx.py"", line 71, in <module>
    tflite_model_quant = converter.convert()
```

The model is a Resnet and upscale image to image network originally saved from PyTorch.
I have read in similar issues that there might be a problem with the way that PyTorch models have a dynamic input dimension which has to be fixed during conversion, but I am not sure on how to diagnose this any further than that.

Does anyone have an idea on how to solve the issue of the dimension mismatch?

Thank you very much for any help on this.
"
57005,Training with GRU is taking more time when i replace the LSTM with same input and Parameters ? Is it Expected?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.7

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Training with GRU is taking more time when i replace the LSTM with same input and Parameters ? Is it Expected?
```


### Standalone code to reproduce the issue

```shell
self.inter_rnn1 = keras.layers.LSTM(64,activation='tanh',return_sequences=True, implementation = 2)

Able to complete in 20min/epoch and watch -n 0.1 nvidia-smi shows 100% usage of all GPU's



self.inter_rnn1 = keras.layers.GRU(64,activation='tanh',return_sequences=True, implementation = 2)

Able to complete in 4hours/epoch and watch -n 0.1 nvidia-smi shows hardly  15% usage of all GPU's

Can someone help on this?
```


### Relevant log output

_No response_</details>"
57003,"MatrixTriangularSolve error: ""InvalidArgumentError: Input matrix is not invertible.""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

v2.7.0-rc1-69-gc256c071bb2

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04.4 LTS

### Mobile device

n/a

### Python version

3.8.10

### Bazel version

n/a

### GCC/Compiler version

n/a

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

When a matrix is not invertible, I get an exception, `tensorflow.python.framework.errors_impl.InvalidArgumentError: Input matrix is not invertible. [Op:MatrixTriangularSolve]`, while computing the gradient.

I expected the gradient to be filled with NaN values in the appropriate locations, as occurs on the forward pass.

In the code sample I provide below, note that there are two sets of data stored in `x`. The first one has an ill-defined cholesky decomposition of the covariance matrix, whereas the second one is fine. I could still use the gradient of the second one to adjust my parameters, despite it being in the same batch with the first one, but I am prevented from doing so because the MatrixTriangularSolve operation raises a hard exception rather than filling the appropriate portion of the gradient tensor with NaNs (as is the typical behavior for other ops). Even if I split the data into separate batches, Keras will still choke on this hard exception and fail to complete the training epoch.

If MatrixTriangularSolve returns NaNs in the gradient, users can add their own code to assert that NaNs are not present in the gradient and raise a hard exception, if that is the desired behavior. Or, importantly, we have the option to use a function with a custom gradient on the input tensor and block the NaN gradients from corrupting our parameters while continuing to train. But as long as MatrixTriangularSolve raises an exception itself, users are left with little to no control over how the problem is handled.


### Standalone code to reproduce the issue

```python3
import tensorflow as tf
import tensorflow_probability as tfp

x = tf.Variable([[[-0.96169615, 0.03600049, -0.86897445],
                  [0.14993548, 0.32782674, 0.05467033],
                  [0.70350194, 0.45990896, 0.24882722]],
                 [[0.19966352, 0.22277904, 0.18279016],
                  [0.996189, 0.8356074, 0.32451844],
                  [0.9954922, 0.95547783, 0.7614579]]])
with tf.GradientTape() as tape:
    y = tfp.stats.cholesky_covariance(x, sample_axis=1)
    tf.print(y)
g = tape.gradient(y, x)
```


### Relevant log output

```python3
2022-08-03 15:25:26.880273: W tensorflow/core/kernels/linalg/cholesky_op.cc:56] Cholesky decomposition was not successful. Eigen::LLT failed with error code 1. Filling lower-triangular output with NaNs.
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py"", line 3331, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-106-5ae22a61c2ce>"", line 10, in <module>
    g = tape.gradient(y, x)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/backprop.py"", line 1084, in gradient
    flat_grad = imperative_grad.imperative_grad(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/imperative_grad.py"", line 71, in imperative_grad
    return pywrap_tfe.TFE_Py_TapeGradient(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/backprop.py"", line 159, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/linalg_grad.py"", line 471, in _CholeskyGrad
    l_inverse = linalg_ops.matrix_triangular_solve(l,
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 7107, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input matrix is not invertible. [Op:MatrixTriangularSolve]
[[[nan 0 -2.78880322e+11]
  [nan nan 0]
  [nan nan nan]]
 [[0.375321597 0 0]
  [0.317106605 0.049177371 0]
  [0.169663742 0.178507909 0.000427246094]]]
```
</details>"
57002,Forward-mode Autodiff gives wrong `nan` when argument `segment_ids[0]` is not zero  for API `tf.math.segment_max`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

If the first element of `segment_ids` is not zero, the forward mode autodiff will give nan as gradient, which is incorrect. It should give the same result as reverse-mode did.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

data = tf.constant(
[[0.47682607, 0.2620497, 0.22771002],
 [0.59265005, 0.62705662, 0.81160802]], shape=(2, 3), dtype=tf.float64)


with tf.GradientTape(persistent=True) as g:
 g.watch(data)
 res_backward = tf.math.segment_max(data,[1,1])
grad_backward = g.gradient(res_backward,data)
print(""reverse-mode:\n"",grad_backward)

grad_fwd_arr = []

for i in range(tf.size(data)):
    tangents = tf.reshape(tf.one_hot(i,tf.size(data),dtype=tf.float64),shape=data.shape)
    with tf.autodiff.ForwardAccumulator(data, tangents) as acc:
        res_forward = tf.math.segment_max(data,[1,1])
        jvp = acc.jvp(res_forward)
        grad_fwd_arr.append(tf.reduce_sum(jvp))

grad_fwd = tf.reshape(tf.convert_to_tensor(grad_fwd_arr),shape=data.shape)
print(""forward-mode:\n"",grad_fwd)
```


### Relevant log output

```shell
reverse-mode:
 tf.Tensor(
[[0. 0. 0.]
 [1. 1. 1.]], shape=(2, 3), dtype=float64)
forward-mode:
 tf.Tensor(
[[nan nan nan]
 [nan nan nan]], shape=(2, 3), dtype=float64)
```
</details>"
57000,Document supported CUDA/cuDNN versions,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Source

binary

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
See title
```


### Standalone code to reproduce the issue

```shell
See title
```


### Relevant log output

_No response_</details>"
56999,tflite.interpreter problem its not opening the model in raspberry pi 4," Hi guys,

I am trying to implement pose estimation with TFlite in raspberry pi so its working in my desktop but when I tried to run it in my raspberry pi its not working.

this is my original libraries:
matplotlib==3.5.2
numpy==1.19.5
opencv_python==4.6.0.66
setuptools==57.0.0
tensorflow==2.4.1

when I tried to install my dependencies and libraries using requirements, its not accepting the tensorflow 2.4.1 so I downloaded 2.8 instead .

my raspberry pi:
bullseye 11
python 3.9
64bits

this is my error:
File ""/home/pi/pose1/main.py"", line 7, in <module>
    interpreter = tf.lite.Interpreter(model_path=""thunder.tflite "")
  File ""/home/pi/pose1/pose11-env/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py"", line 456, in __init__
    _interpreter_wrapper.CreateWrapperFromFile(
ValueError: Could not open 'thunder.tflite '.


this is the code for my interpreter:


interpreter = tf.lite.Interpreter(model_path=""thunder.tflite "")
interpreter.allocate_tensors()

the model is also in my directory , again its working in my desktop but not in my raspberry pi.  please help thank you so much!!!!

"
56998,"tf.pow gives some incorrect results on cuda: tf.pow(2.0,13)==8191.996 ","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

ubuntu 18.04

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.4

### GPU model and memory

RTX 2080TI 11GB RAM

### Current Behaviour?

On Cuda only (Rtx 2080TI, Cuda 11.4, and also other gpus, driver 515.48.07)

The result of tf.pow is incorrect to some numbers, for example:

```python

print(tf.pow(2.0,13))
```
prints:

```python
tf.Tensor(8191.996, shape=(), dtype=float32)
```

`2.0**13` is well within float32 precision and is 8192.

This *does not* happen on TF 2.2


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.pow(2.0,13))
```


### Relevant log output

_No response_</details>"
56997,AttributeError: '_UserObject' object has no attribute 'add_slot',"Issue Type: Bug
Source: binary
Tensorflow Version: 2.6
Custom Code: Yes


# Current Behaviour?

I have model, saved using `SavedModel` format. The model is trained on Kaggle TPU and saved in the following ways:

```python
# in kaggle tpu, it must be saved in the following way!
save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')
final_model.save('./final_model', options=save_locally)
 ```

Now, it seems like, if I try to load this model in the following way, 

```python
model = tf.saved_model.load(""final_model"")

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/tmp/ipykernel_17/3033429325.py in <module>
----> 1 model = tf.saved_model.load(""final_model"")
...
...
AttributeError: '_UserObject' object has no attribute 'add_slot'
```

But if I do as follows, it works.

```
model = tf.keras.models.load_model(""final_model"")
```

But for some reason, I need to make `tf.saved_model.load` API work instead `tf.keras.models.load_model` API. What approach should we take here?
"
56996,How to reduce the size of dependent files?,"Hi, 
    I want to deploy my model on the edge device, but the device space is limited. And the compiled ""tensorflowlite.so"" depends on three large files (larger than 8M). Is there any way to reduce the size of dependent files?

Thanks!"
56990,MirroredStrategy does not use GPUs simultaneously,"### Issue Type

Performance

### Source

binary

### Tensorflow Version

2.6,2.7,2.8,2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

DGX1 V100 

### Current Behaviour?

Hi,
I have observed some bad performance when trying to run single machine / multi-GPU jobs. I have reported a minimum example below. When looking at the GPU performance (with NVIDIA NSight System), I can see that the GPUs run almost sequentially, i.e., GPU0 will do some work then when it is almost done, GPU1 will start. 

![image](https://user-images.githubusercontent.com/37110816/182431591-47ff8961-9987-4d91-afd1-356881078bd9.png)

I have executed a similar version of the code on PyTorch and it exhibits the expected behaviour, i.e., the GPUs run in parallel.

![image](https://user-images.githubusercontent.com/37110816/182431647-fe5f2162-3045-43e1-a444-238d9d828d7f.png)

This issue on TF dramatically slows down model training. Can someone help me with the issue? Am I not using MirroredStrategy correctly? I used `@tf.function` to disable the eager execution of the graphs. I am posting the code below and to remove any impact of data loading, each GPU creates their own tensor to work on.

Thanks!


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import time
from functools import partial

N = 2**20
D = 1

mirrored_strategy = tf.distribute.MirroredStrategy()
N_DEVICES = mirrored_strategy.num_replicas_in_sync

print(f'Found {N_DEVICES} gpus')

def per_replica_grads():
    M = tf.eye(8192)
    M = tf.linalg.inv(M)
    return M[0, :]


@tf.function
def step():
    with mirrored_strategy.scope():
        fn_replica = mirrored_strategy.run(per_replica_grads)
        fn_replica_summed = mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, fn_replica, axis=None)
    return fn_replica_summed


for it in range(10):
    time.sleep(0.5)
    print(step())
```


### Relevant log output

_No response_"
56988,Ignore certain classes in detection.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Source

binary

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

Ubuntu 16.04

### Mobile device

Android

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When we run tflite model on android device we provide all classes on trained tflite weights. 

But I dont want to detect all classes in shared weights. Is there any way to detect only required classes while running detections ?
```


### Standalone code to reproduce the issue

```shell
No issues with running the model.
```


### Relevant log output

```shell
No issues with running the model.
```
</details>"
56987,tensorflow-gpu Docker image contains dependencies that are not used,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

any

### Custom Code

No

### OS Platform and Distribution

Docker

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Some apt packages like `software-properties-common` and `unzip` are installed during the GPU Docker build but they are unnecessary since they are not used anywhere afterwards: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/gpu.Dockerfile#L59

Removing them would be beneficial, especially for the case of `software-properties-common`, since it depends on `python3-chardet` which contains a license that restricts non-commercial use.
```


### Standalone code to reproduce the issue

```shell
cd tensorflow/tools/dockerfiles
docker build -f ./dockerfiles/gpu.Dockerfile -t tf .
``
```


### Relevant log output

_No response_</details>"
56985,Tensorflow randomly changes the order of the input tensors depending on their names,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.2

### Custom Code

Yes

### OS Platform and Distribution

macOS Monterey 12.5

### Mobile device

_No response_

### Python version

3.10.5

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I have a Keras model, which has two inputs, and convert it to TFLite format. Depending on how the inputs are named, the order of the input tensors changes. Moreover, it does not seem that the order depends on the alphabetical order of the names.

This behavior is very strange and makes it difficult to integrate with another team that uses the model converted to TFLite format.

*Expected behavior*. The order of the input tensors should be preserved in the way they are ordered in code.
```


### Standalone code to reproduce the issue

```shell
import os

import tensorflow as tf

from pprint import pprint
from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Concatenate, Dense


os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


def print_input_details_for_model(input_name_1, input_name_2):
    print()
    print(""************"")
    print(""Input names: "", input_name_1, input_name_2)
    x = Input(shape=(7), name=input_name_1)
    y = Input(shape=(17), name=input_name_2)
    x_with_y = Concatenate()([x, y])
    z = Dense(27, name=""output"")(x_with_y)

    model = Model([x, y], z)

    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    tflite_model = converter.convert() 
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    pprint(interpreter.get_input_details())


if __name__ == ""__main__"":
    tf.Variable(1.0)
    # The order of tensors is preserved.
    print_input_details_for_model(""input_word_ids"", ""input_bboxes"")
    # The order of tensors is changed.
    print_input_details_for_model(""input_ids"", ""bboxes"")
```


### Relevant log output

```shell
Metal device set to: Apple M1 Pro

systemMemory: 16.00 GB
maxCacheSize: 5.33 GB


************
Input names:  input_word_ids input_bboxes
[{'dtype': <class 'numpy.float32'>,
  'index': 0,
  'name': 'serving_default_input_word_ids:0',
  'quantization': (0.0, 0),
  'quantization_parameters': {'quantized_dimension': 0,
                              'scales': array([], dtype=float32),
                              'zero_points': array([], dtype=int32)},
  'shape': array([1, 7], dtype=int32),
  'shape_signature': array([-1,  7], dtype=int32),
  'sparsity_parameters': {}},
 {'dtype': <class 'numpy.float32'>,
  'index': 1,
  'name': 'serving_default_input_bboxes:0',
  'quantization': (0.0, 0),
  'quantization_parameters': {'quantized_dimension': 0,
                              'scales': array([], dtype=float32),
                              'zero_points': array([], dtype=int32)},
  'shape': array([ 1, 17], dtype=int32),
  'shape_signature': array([-1, 17], dtype=int32),
  'sparsity_parameters': {}}]

************
Input names:  input_ids bboxes
[{'dtype': <class 'numpy.float32'>,
  'index': 0,
  'name': 'serving_default_bboxes:0',
  'quantization': (0.0, 0),
  'quantization_parameters': {'quantized_dimension': 0,
                              'scales': array([], dtype=float32),
                              'zero_points': array([], dtype=int32)},
  'shape': array([ 1, 17], dtype=int32),
  'shape_signature': array([-1, 17], dtype=int32),
  'sparsity_parameters': {}},
 {'dtype': <class 'numpy.float32'>,
  'index': 1,
  'name': 'serving_default_input_ids:0',
  'quantization': (0.0, 0),
  'quantization_parameters': {'quantized_dimension': 0,
                              'scales': array([], dtype=float32),
                              'zero_points': array([], dtype=int32)},
  'shape': array([1, 7], dtype=int32),
  'shape_signature': array([-1,  7], dtype=int32),
  'sparsity_parameters': {}}]
```
</details>"
56984,TensorFlow Lite convert BatchMatMulV2 into Split+multi FullyConnected + Pack,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ub18.04
- TensorFlow installation (pip package or built from source):tf2.9

When I convert a transformer model with BatchMatMulV2 into tflite,  the first input shape of BatchMatMulV2 is [40x1x64],the second input shape   is [64x64], the BatchMatMulV2 is converted into Split+multi FullyConnected + Pack in the tflite model.

"
56983,TensorFlow Lite gpu delegate incorrect fusion for layernorm reduce+elemwise,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
when run transformer model based on tflite gpu delegate, tflite gpu delegate will fuse reduce+multi elemwise ops in layernorm, however, when broadcast happens in elemwise ops, the fusion will be inccrect. Since fused op use broadcased output tensor shape to calculate grid size.
```


### Standalone code to reproduce the issue

```shell
use tflite gpu delegate to run transformer model with layernorm layer, which is constructed by multiple reduce + elemwise nodes.
```


### Relevant log output

_No response_</details>"
56982,Dtype of reverse-mode gradient is incorrect when input dtype is a mixture of float32 and complex128,"### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Like the case below, reverse mode AD gives the gradient in float32 dtype when the input dtype is a mixture of float32 and complex128. It should be complex128.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x = tf.constant(
[[0.69053245, 0.7255454, 0.4136572 ],
 [0.6055007, 0.5583403, 0.07835412]], shape=(2, 3), dtype=tf.float32)

y = tf.constant(
[[0.30684007+0.04178488j, 0.0126979 +0.32119822j, 0.99649133+0.13754789j],
 [0.29316908+0.50216602j, 0.11820176+0.79961702j, 0.41341467+0.78824452j]], shape=(2, 3), dtype=tf.complex128)

with tf.GradientTape(persistent=True) as g:
 g.watch(x)
 g.watch(y)
 res_backward = tf.keras.losses.MSE(x, y)
grad_backward = g.gradient(res_backward,x)
print(grad_backward)

grad_fwd_arr = []

for i in range(tf.size(x)):
    tangents = tf.reshape(tf.one_hot(i,tf.size(x)),shape=x.shape)
    with tf.autodiff.ForwardAccumulator(x, tangents) as acc:
        res_forward = tf.keras.losses.MSE(x,y)
        jvp = acc.jvp(res_forward)
        grad_fwd_arr.append(tf.reduce_sum(jvp))

grad_fwd = tf.reshape(tf.convert_to_tensor(grad_fwd_arr),shape=x.shape)
print(grad_fwd)
```


### Relevant log output

```shell
tf.Tensor(
[[ 0.2557949   0.47523168 -0.3885561 ]
 [ 0.20822108  0.2934257  -0.2233737 ]], shape=(2, 3), dtype=float32)
tf.Tensor(
[[ 0.25579492+0.02785659j  0.47523167+0.21413215j -0.38855609+0.09169859j]
 [ 0.20822108+0.33477735j  0.2934257 +0.53307801j -0.2233737 +0.52549635j]], shape=(2, 3), dtype=complex128)
```
"
56981,Is possible to measure height of person using tensorFlow pose Estimation ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

pod 'TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['CoreML', 'Metal']

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I want to performe Vertical Jump test & want to measure distance between floor to ankle.
```


### Standalone code to reproduce the issue

```shell
I want to performe Vertical Jump test & want to measure distance between floor to ankle.
```


### Relevant log output

_No response_</details>"
56980, AssertionError: Duplicate registrations for type 'optimizer'  in Keras Optimizer - Tensorflow 2.9.1,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

TF  2.9.1

### Custom Code

Yes

### OS Platform and Distribution

MACOS

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Exception Error in Keras Regualrization and Optimizer 

1) Un-installed and  re-installed Tensorflow, keras from scratch but not luck
2) Created a New ENV  and  started from scracth and used latest  TF  2.9.1 but no luck as
same issue is seen

(IOT_DL) akram@ISHERIFF-M-RBNA Data % 
(IOT_DL) akram@ISHERIFF-M-RBNA Data % python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
v2.9.0-18-gd8ce9f9c301 2.9.1
(IOT_DL) akram@ISHERIFF-M-RBNA Data %
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time

from keras.models import Sequential
from keras.layers.recurrent import LSTM
from keras.callbacks import TensorBoard
from keras.callbacks import ModelCheckpoint

from tensorflow.python.keras.layers.core import Reshape,Dense,Dropout,Activation,Flatten

epochs = 100
batch_size = 50

# Each training data point will be length 100-1,
# since the last value in each sequence is the label
sequence_length = 100

# OMP error issue fixing (some machine may show error)
import os

os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'


# Input data generation

def prepare_data(data, train_start, train_end, test_start, test_end):
    print(""Length of Data"", len(data))

    # training data
    print(""Preparaing training data..."")

    result = []
    for index in range(train_start, train_end - sequence_length):
        result.append(data[index: index + sequence_length])
    result = np.array(result)
    result, result_mean = normalize(result)

    print(""Training data shape  : "", result.shape)

    train = result[train_start:train_end, :]
    np.random.shuffle(train)
    X_train = train[:, :-1]
    y_train = train[:, -1]

    # test data
    print(""Creating test data..."")

    result = []
    for index in range(test_start, test_end - sequence_length):
        result.append(data[index: index + sequence_length])
    result = np.array(result)
    result, result_mean = normalize(result)

    print(""Test data shape  : {}"".format(result.shape))

    X_test = result[:, :-1]
    y_test = result[:, -1]

    print(""Shape X_train"", np.shape(X_train))
    print(""Shape X_test"", np.shape(X_test))

    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

    return X_train, y_train, X_test, y_test


# Model genration function

def generate_model():
    model = Sequential()

    # First LSTM layer defining the input sequence length
    model.add(LSTM(input_shape=(sequence_length - 1, 1),
                   units=32,
                   return_sequences=True))
    model.add(Dropout(0.2))

    # Second LSTM layer with 128 units
    model.add(LSTM(units=128,
                   return_sequences=True))
    model.add(Dropout(0.2))

    # Third LSTM layer with 100 units
    model.add(LSTM(units=100,
                   return_sequences=False))
    model.add(Dropout(0.2))

    # Densely-connected output layer with the linear activation function
    model.add(Dense(units=1))
    model.add(Activation('linear'))

    model.compile(loss='mean_squared_error', optimizer='rmsprop')

    return model


# Function for result normalisation
def normalize(result):
    result_mean = result.mean()
    result_std = result.std()
    result -= result_mean
    result /= result_std
    return result, result_mean


## Function for running the model

def run_model(model=None, data=None):
    # global_start_time = time.time()

    print('Loading data... ')
    data_b = pd.read_csv('/Users/akram/AKRAM_CODE_FOLDER/IOT_DL/HAR/Data/cpu-utilisation.csv',
                         parse_dates=[0], infer_datetime_format=True)
    data = data_b['cpu utilisation (%)'].as_matrix()

    # train on first 700 samples and test on next 300 samples (test set has anomaly)
    X_train, y_train, X_test, y_test = prepare_data(data, 0, 600, 400, 700)

    # tensor board setting
    tensorboard = TensorBoard(log_dir='./logs',
                              histogram_freq=0,
                              write_graph=True,
                              write_images=True)

    if model is None:
        model = generate_model()

    try:
        print(""Training..."")
        checkpointer = ModelCheckpoint(filepath=""lstm-results/checkpoint-02.hdf5"",
                                       verbose=1, save_best_only=True, monitor='loss')
        model.fit(
            X_train, y_train,
            batch_size=batch_size, epochs=epochs, callbacks=[checkpointer, tensorboard], validation_split=0.05)
        print(""Predicting..."")

        predicted = model.predict(X_test)
        # model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1, callbacks=[checkpointer, tensorboard])

        print(""Reshaping predicted"")
        predicted = np.reshape(predicted, (predicted.size,))
    except KeyboardInterrupt:
        print(""Prediction exception"")
        # print('Training duration:{}'.format(time.time() - global_start_time))
        return model, y_test, 0

    try:
        plt.figure(figsize=(20, 8))
        plt.plot(y_test[:len(y_test)], 'b', label='Observed')
        plt.plot(predicted[:len(y_test)], 'g', label='Predicted')
        plt.plot(((y_test - predicted) ** 2), 'r', label='Root-mean-square deviation')
        plt.legend()
        plt.show()
    except Exception as e:
        print(""plotting exception"")
        print(str(e))
    # print('Training duration:{}'.format(time.time() - global_start_time))

    return model, y_test, predicted


# Call the run function
model, y_test, predicted = run_model()
```


### Relevant log output

```shell
traceback (most recent call last):
  File ""/Users/akram/AKRAM_CODE_FOLDER/IOT_DL/HAR/Models/LSTM_Security_Threat.py"", line 6, in <module>
    from keras.models import Sequential
  File ""/Users/akram/opt/anaconda3/envs/IOT_DL/lib/python3.9/site-packages/keras/__init__.py"", line 25, in <module>
    from keras import models
  File ""/Users/akram/opt/anaconda3/envs/IOT_DL/lib/python3.9/site-packages/keras/models/__init__.py"", line 18, in <module>
    from keras.engine.functional import Functional
  File ""/Users/akram/opt/anaconda3/envs/IOT_DL/lib/python3.9/site-packages/keras/engine/functional.py"", line 25, in <module>
    from keras.engine import base_layer
  File ""/Users/akram/opt/anaconda3/envs/IOT_DL/lib/python3.9/site-packages/keras/engine/base_layer.py"", line 40, in <module>
    from keras.mixed_precision import loss_scale_optimizer
  File ""/Users/akram/opt/anaconda3/envs/IOT_DL/lib/python3.9/site-packages/keras/mixed_precision/loss_scale_optimizer.py"", line 20, in <module>
    from keras.optimizer_v2 import optimizer_v2
  File ""/Users/akram/opt/anaconda3/envs/IOT_DL/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py"", line 1461, in <module>
    tf.__internal__.saved_model.load.register_revived_type(
  File ""/Users/akram/opt/anaconda3/envs/IOT_DL/lib/python3.9/site-packages/tensorflow/python/saved_model/revived_types.py"", line 133, in register_revived_type
    raise AssertionError(f""Duplicate registrations for type '{identifier}'"")
AssertionError: Duplicate registrations for type 'optimizer'
```
</details>"
56979,Colab GPU runtime ignores Graph execution error with `sparse_categorical_crossentropy`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Setting output layer units to be a number that is lower than possible class labels, should make `sparse_categorical_crossentropy` throw a `Graph execution error` at the fit() step - which works as expected on normal runtime, however, switching the Google Colab runtime to GPU, the error is ignored and training proceeds as normal (with nan loss and 0 accuracy).
```


### Standalone code to reproduce the issue

```shell
GPU runtime should throw the same error as well instead of letting training proceed. I have created a well-documented notebook that can demonstrate / reproduce this issue and have [linked it here](https://colab.research.google.com/github/rajdeep-biswas/sparse-categorical-crossentropy/blob/master/sparcat.ipynb)..
```


### Relevant log output

_No response_</details>"
56976,Could not load dynamic library 'cudart64_110.dll',"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The load of cuda for tensorflow fails with the following error:

c:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin>python
Python 3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
2022-08-01 17:30:54.607827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2022-08-01 17:30:54.608106: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
>>>

-----------
This error appears to come from: 
tensorflow/tensorflow/core/platform/windows/load_library.cc
line: 43
LoadLibraryExW(ws_file_name.c_str(), NULL, LOAD_WITH_ALTERED_SEARCH_PATH)

Technically this should not be failing. Since CUDA 11.2 is loaded, the path variable is set correctly, and the file cudart64_110.dll is present. (see ""log"" output)
The load even fails when python is started from within the bin directory of CUDA\v11.2. 
Is it possible that an extremely long path to the tensorflow python module could cause LoadLibraryEx to fail?
-----------
```


### Standalone code to reproduce the issue

```shell
The issue may be related to the length of the python path:
C:\Users\uuuuuuu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages

Loaded tensorflow using pip. Loaded CUDA 11.2, loaded cnDNN 8.1 and copied the files into the CUDA 11.2 directory.

I was NOT able to reproduce it on my other machine. However windows version is home instead of pro and python is installed under a different path.
This issue is not consistently reproduceable. Works fine on home computer but not on office computer.
```


### Relevant log output

```shell
Tensor flow is in the following directory:
C:\Users\uuuuuuu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages>dir
 Volume in drive C has no label.
 Volume Serial Number is 4298-C2D1

 Directory of C:\Users\uuuuuuu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages

08/01/2022  01:59 PM    <DIR>          .
08/01/2022  01:59 PM    <DIR>          ..
06/20/2022  10:30 AM    <DIR>          absl
06/20/2022  10:30 AM    <DIR>          absl_py-1.1.0.dist-info
06/20/2022  10:30 AM    <DIR>          astunparse
06/20/2022  10:30 AM    <DIR>          astunparse-1.6.3.dist-info
06/20/2022  10:30 AM    <DIR>          cachetools
06/20/2022  10:30 AM    <DIR>          cachetools-5.2.0.dist-info
06/20/2022  10:30 AM    <DIR>          certifi
06/20/2022  10:30 AM    <DIR>          certifi-2022.6.15.dist-info
06/20/2022  10:29 AM             8,731 CHANGELOG.md
06/20/2022  10:30 AM    <DIR>          charset_normalizer
06/20/2022  10:30 AM    <DIR>          charset_normalizer-2.0.12.dist-info
06/20/2022  10:29 AM    <DIR>          clang
06/20/2022  10:29 AM    <DIR>          flatbuffers
06/20/2022  10:29 AM    <DIR>          flatbuffers-1.12.dist-info
06/20/2022  10:30 AM    <DIR>          gast
06/20/2022  10:30 AM    <DIR>          gast-0.4.0.dist-info
06/20/2022  10:30 AM    <DIR>          google
06/20/2022  10:30 AM               539 google_auth-2.8.0-py3.10-nspkg.pth
06/20/2022  10:30 AM    <DIR>          google_auth-2.8.0.dist-info
06/20/2022  10:30 AM    <DIR>          google_auth_oauthlib
06/20/2022  10:30 AM    <DIR>          google_auth_oauthlib-0.4.6.dist-info
06/20/2022  10:30 AM    <DIR>          google_pasta-0.2.0.dist-info
06/20/2022  10:30 AM    <DIR>          grpc
06/20/2022  10:30 AM    <DIR>          grpcio-1.46.3.dist-info
06/20/2022  10:30 AM    <DIR>          h5py
06/20/2022  10:30 AM    <DIR>          h5py-3.7.0.dist-info
06/20/2022  10:30 AM    <DIR>          idna
06/20/2022  10:30 AM    <DIR>          idna-3.3.dist-info
06/20/2022  10:29 AM    <DIR>          keras
06/20/2022  10:29 AM    <DIR>          keras-2.9.0.dist-info
06/20/2022  10:30 AM    <DIR>          keras_preprocessing
06/20/2022  10:30 AM    <DIR>          Keras_Preprocessing-1.1.2.dist-info
06/20/2022  10:29 AM    <DIR>          libclang-14.0.1.dist-info
06/20/2022  10:29 AM               577 LICENSE
06/20/2022  10:30 AM    <DIR>          markdown
06/20/2022  10:30 AM    <DIR>          Markdown-3.3.7.dist-info
05/26/2022  11:16 AM    <DIR>          mysql
05/26/2022  11:15 AM    <DIR>          mysqlclient-2.1.0.dist-info
05/26/2022  11:15 AM    <DIR>          MySQLdb
05/26/2022  11:16 AM    <DIR>          mysqlx
05/26/2022  11:16 AM    <DIR>          mysql_connector_python-8.0.29.dist-info
06/20/2022  10:29 AM    <DIR>          numpy
06/20/2022  10:30 AM    <DIR>          numpy-1.22.4.dist-info
06/20/2022  10:29 AM    <DIR>          oauthlib
06/20/2022  10:29 AM    <DIR>          oauthlib-3.2.0.dist-info
06/20/2022  10:30 AM    <DIR>          opt_einsum
06/20/2022  10:30 AM    <DIR>          opt_einsum-3.3.0.dist-info
06/20/2022  10:30 AM    <DIR>          packaging
06/20/2022  10:30 AM    <DIR>          packaging-21.3.dist-info
06/20/2022  10:30 AM    <DIR>          pasta
08/01/2022  12:45 PM    <DIR>          pip
08/01/2022  12:45 PM    <DIR>          pip-22.2.1.dist-info
06/20/2022  10:29 AM               540 protobuf-3.19.4-py3.10-nspkg.pth
06/20/2022  10:29 AM    <DIR>          protobuf-3.19.4.dist-info
06/20/2022  10:29 AM    <DIR>          pyasn1
06/20/2022  10:29 AM    <DIR>          pyasn1-0.4.8.dist-info
06/20/2022  10:29 AM    <DIR>          pyasn1_modules
06/20/2022  10:29 AM    <DIR>          pyasn1_modules-0.2.8.dist-info
06/20/2022  10:29 AM    <DIR>          pyparsing
06/20/2022  10:29 AM    <DIR>          pyparsing-3.0.9.dist-info
06/20/2022  10:29 AM             1,928 README.md
06/20/2022  10:30 AM    <DIR>          requests
06/20/2022  10:30 AM    <DIR>          requests-2.28.0.dist-info
06/20/2022  10:30 AM    <DIR>          requests_oauthlib
06/20/2022  10:30 AM    <DIR>          requests_oauthlib-1.3.1.dist-info
06/20/2022  10:29 AM    <DIR>          rsa
06/20/2022  10:29 AM    <DIR>          rsa-4.8.dist-info
06/20/2022  10:29 AM    <DIR>          six-1.16.0.dist-info
06/20/2022  10:29 AM            34,549 six.py
08/01/2022  01:59 PM    <DIR>          tensorboard
08/01/2022  01:59 PM    <DIR>          tensorboard-2.9.1.dist-info
08/01/2022  01:59 PM    <DIR>          tensorboard_data_server
08/01/2022  01:59 PM    <DIR>          tensorboard_data_server-0.6.1.dist-info
08/01/2022  01:59 PM    <DIR>          tensorboard_plugin_wit
08/01/2022  01:59 PM    <DIR>          tensorboard_plugin_wit-1.8.1.dist-info
08/01/2022  01:59 PM    <DIR>          tensorflow
08/01/2022  01:59 PM    <DIR>          tensorflow-2.9.1.dist-info
08/01/2022  01:59 PM    <DIR>          tensorflow_estimator
08/01/2022  01:59 PM    <DIR>          tensorflow_estimator-2.9.0.dist-info
08/01/2022  01:59 PM    <DIR>          tensorflow_io_gcs_filesystem
08/01/2022  01:59 PM    <DIR>          tensorflow_io_gcs_filesystem-0.26.0.dist-info
06/20/2022  10:29 AM    <DIR>          termcolor-1.1.0-py3.10.egg-info
01/13/2011  10:43 AM             5,044 termcolor.py
06/20/2022  10:29 AM    <DIR>          typing_extensions-4.2.0.dist-info
06/20/2022  10:29 AM            70,613 typing_extensions.py
06/20/2022  10:29 AM    <DIR>          urllib3
06/20/2022  10:29 AM    <DIR>          urllib3-1.26.9.dist-info
06/20/2022  10:29 AM    <DIR>          werkzeug
06/20/2022  10:29 AM    <DIR>          Werkzeug-2.1.2.dist-info
06/20/2022  10:29 AM    <DIR>          wheel
06/20/2022  10:29 AM    <DIR>          wheel-0.37.1.dist-info
06/20/2022  10:29 AM    <DIR>          wrapt
06/20/2022  10:29 AM    <DIR>          wrapt-1.14.1.dist-info
05/26/2022  11:16 AM         1,354,240 _mysqlxpb.cp310-win_amd64.pyd
05/26/2022  11:16 AM            61,952 _mysql_connector.cp310-win_amd64.pyd
06/20/2022  10:29 AM    <DIR>          __pycache__
              10 File(s)      1,538,713 bytes
              88 Dir(s)  473,428,164,608 bytes free
------------------------------------------------------------------------
The cuda .dll file is in the following directoy:

c:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin>dir
 Volume in drive C has no label.
 Volume Serial Number is 4298-C2D1

 Directory of c:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin

08/01/2022  01:27 PM    <DIR>          .
08/01/2022  01:27 PM    <DIR>          ..
02/15/2021  05:38 AM           205,824 bin2c.exe
01/29/2021  11:05 AM                66 compute-sanitizer.bat
08/01/2022  12:17 PM    <DIR>          crt
02/15/2021  05:38 AM           183,808 cu++filt.exe
02/15/2021  12:07 AM       107,330,560 cublas64_11.dll
02/15/2021  12:07 AM       175,706,112 cublasLt64_11.dll
02/15/2021  05:38 AM           374,784 cuda-memcheck.exe
02/15/2021  05:38 AM         4,685,824 cudafe++.exe
02/15/2021  05:38 AM           393,728 cudart32_110.dll
02/15/2021  05:38 AM           464,896 cudart64_110.dll
08/01/2022  12:37 PM           222,720 cudnn64_8.dll
08/01/2022  12:37 PM       128,429,056 cudnn_adv_infer64_8.dll
08/01/2022  12:37 PM        82,672,640 cudnn_adv_train64_8.dll
08/01/2022  12:37 PM       545,695,232 cudnn_cnn_infer64_8.dll
08/01/2022  12:37 PM        87,374,336 cudnn_cnn_train64_8.dll
08/01/2022  12:37 PM       273,139,712 cudnn_ops_infer64_8.dll
08/01/2022  12:37 PM        46,076,416 cudnn_ops_train64_8.dll
02/15/2021  05:38 AM       188,301,312 cufft64_10.dll
02/15/2021  05:38 AM           258,560 cufftw64_10.dll
02/15/2021  05:38 AM         1,314,304 cuinj64_112.dll
02/15/2021  05:38 AM         2,878,464 cuobjdump.exe
02/15/2021  05:38 AM        60,627,968 curand64_10.dll
02/15/2021  05:38 AM       396,296,704 cusolver64_11.dll
02/15/2021  05:38 AM       213,831,168 cusolverMg64_11.dll
02/15/2021  05:38 AM       228,391,424 cusparse64_11.dll
02/15/2021  05:38 AM           337,408 fatbinary.exe
02/15/2021  05:38 AM           246,272 nppc64_11.dll
02/15/2021  05:38 AM        11,890,176 nppial64_11.dll
02/15/2021  05:38 AM         5,023,744 nppicc64_11.dll
02/15/2021  05:38 AM         8,515,072 nppidei64_11.dll
02/15/2021  05:38 AM        58,631,168 nppif64_11.dll
02/15/2021  05:38 AM        29,265,920 nppig64_11.dll
02/15/2021  05:38 AM         7,227,392 nppim64_11.dll
02/15/2021  05:38 AM        26,726,912 nppist64_11.dll
02/15/2021  05:38 AM           219,648 nppisu64_11.dll
02/15/2021  05:38 AM         3,079,680 nppitc64_11.dll
02/15/2021  05:38 AM        15,545,856 npps64_11.dll
02/15/2021  12:07 AM           315,392 nvblas64_11.dll
02/15/2021  05:38 AM         4,817,408 nvcc.exe
02/15/2021  05:38 AM               334 nvcc.profile
02/15/2021  05:38 AM        33,605,120 nvdisasm.exe
02/15/2021  05:38 AM         3,424,256 nvjpeg64_11.dll
02/15/2021  05:38 AM         8,529,920 nvlink.exe
02/15/2021  05:38 AM         2,187,264 nvprof.exe
02/15/2021  05:38 AM           227,840 nvprune.exe
02/15/2021  05:38 AM         5,542,912 nvrtc-builtins64_112.dll
08/01/2022  12:17 PM    <DIR>          nvrtc-prev
02/15/2021  05:38 AM        31,991,296 nvrtc64_112_0.dll
02/15/2021  05:38 AM               129 nvvp.bat
02/15/2021  05:38 AM         8,402,944 ptxas.exe
              48 File(s)  2,810,609,681 bytes
               4 Dir(s)  473,422,077,952 bytes free

----------------------------------------------------------------
The environment variable path has been set correctly:

c:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin>echo %PATH:;=&echo.%
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\libnvvp

c:\program files\graphicsmagick-1.3.36-q16
C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common
C:\Program Files (x86)\Common Files\Oracle\Java\javapath
C:\WINDOWS\system32
C:\WINDOWS
C:\WINDOWS\System32\Wbem
C:\WINDOWS\System32\WindowsPowerShell\v1.0\
C:\WINDOWS\System32\OpenSSH\
C:\Program Files (x86)\Windows Kits\8.1\Windows Performance Toolkit\
C:\Program Files\Microsoft SQL Server\Client SDK\ODBC\170\Tools\Binn\
C:\Program Files (x86)\Microsoft SQL Server\150\Tools\Binn\
C:\Program Files\Microsoft SQL Server\150\Tools\Binn\
C:\Program Files\Microsoft SQL Server\150\DTS\Binn\
C:\Program Files (x86)\Microsoft SQL Server\150\DTS\Binn\
C:\Program Files\Azure Data Studio\bin
C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR
C:\Program Files\Git LFS
C:\Program Files\Go\bin
C:\Program Files\Git\cmd
C:\Program Files\NVIDIA Corporation\Nsight Compute 2020.3.1\
C:\Program Files\MySQL\MySQL Shell 8.0\bin\
C:\Users\uuuuuu\AppData\Local\Microsoft\WindowsApps
C:\Users\uuuuuu\Dev\opencv\build\bin
C:\Users\uuuuuu\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\Scripts
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\libnvvp
-------------
```
</details>"
56974,tf.lookup.StaticHashTable with TextFileInitializer uses a cached version of the file even after the file is changed,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.2

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.7.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When I create a tf.lookup.StaticHashTable with tf.lookup.TextFileInitializer, then change the text file and create a new tf.lookup.StaticHashTable, it uses the contents from the original text file. If this caching behavior is expected, it's not clear to me how to clear this cache.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1mJ2rTixQXxaePjkdoEfbRMfNbJjlAFSe?usp=sharing


import tensorflow as tf
import tempfile

f = tempfile.NamedTemporaryFile(delete=False)
csv_data1 = f""""""0,0
1,1
2,2
""""""
with open(f.name, ""w"") as csv_file:
  csv_file.write(csv_data1)

!cat {f.name}

table1 = tf.lookup.StaticHashTable(
    initializer=tf.lookup.TextFileInitializer(
        f.name,
        key_dtype=tf.int64, key_index=0,
        value_dtype=tf.int64, value_index=1,
        delimiter="","",
    ),
    default_value=-1,
)

csv_data2 = f""""""0,1
1,2
2,3
""""""
with open(f.name, ""w"") as csv_file:
  csv_file.write(csv_data2)

!cat {f.name}

table2 = tf.lookup.StaticHashTable(
    initializer=tf.lookup.TextFileInitializer(
        f.name,
        key_dtype=tf.int64, key_index=0,
        value_dtype=tf.int64, value_index=1,
        delimiter="","",
    ),
    default_value=-1,
)

table1.export(), table2.export()
```
```


### Relevant log output

```shell
0,0
1,1
2,2
0,1
1,2
2,3

((<tf.Tensor: shape=(3,), dtype=int64, numpy=array([0, 1, 2])>,
  <tf.Tensor: shape=(3,), dtype=int64, numpy=array([0, 1, 2])>),
 (<tf.Tensor: shape=(3,), dtype=int64, numpy=array([0, 1, 2])>,
  <tf.Tensor: shape=(3,), dtype=int64, numpy=array([0, 1, 2])>))
```
</details>"
56973,Cannot build branch r2.9 on Windows 11,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9

### Custom Code

No

### OS Platform and Distribution

Windows 11 Enterprise version 21H2 OS build 22000.795

### Mobile device

_No response_

### Python version

3.9.7

### Bazel version

5.0.0

### GCC/Compiler version

Microsoft (R) C/C++ Optimizing Compiler Version 19.29.30146 for x64

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I downloaded and installed all of the prerequisites mentioned on the [TensorFlow Build from source on Windows](https://www.tensorflow.org/install/source_windows) page, ran `python configure.py`, accepted all of the defaults, and ran `bazel build //tensorflow/tools/pip_package:build_pip_package`, as that page directed.  That produces an error as given in the ""relevant log output"" section.
```


### Standalone code to reproduce the issue

```shell
Install Python 3.9.7.
Install Bazel 5.0.0.
Install Visual Studio Build Tools 2019.
Open an elevated ""x64 Native Tools Command Prompt for VS 2019"" and run the following commands.
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git switch r2.9
python -m venv venv
venv\Scripts\activate
pip3 install -U six numpy wheel packaging
pip3 install -U keras_preprocessing --no-deps
python configure.py
bazel build //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
(venv) C:\Users\Tenchumaru\Documents\GitHub\tensorflow>python configure.py
You have bazel 5.0.0 installed.
Please specify the location of python. [Default is C:\Users\Tenchumaru\Documents\GitHub\tensorflow\venv\Scripts\python.exe]:


Found possible Python library paths:
  C:\Users\Tenchumaru\Documents\GitHub\tensorflow\venv\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\Tenchumaru\Documents\GitHub\tensorflow\venv\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]:
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.

(venv) C:\Users\Tenchumaru\Documents\GitHub\tensorflow>bazel build //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=147
INFO: Reading rc options for 'build' from c:\users\tenchumaru\documents\github\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/Tenchumaru/Documents/GitHub/tensorflow/venv/Scripts/python.exe
INFO: Reading rc options for 'build' from c:\users\tenchumaru\documents\github\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'build' from c:\users\tenchumaru\documents\github\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/Tenchumaru/Documents/GitHub/tensorflow/venv/Scripts/python.exe --action_env PYTHON_LIB_PATH=C:/Users/Tenchumaru/Documents/GitHub/tensorflow/venv/lib/site-packages --python_path=C:/Users/Tenchumaru/Documents/GitHub/tensorflow/venv/Scripts/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Reading rc options for 'build' from c:\users\tenchumaru\documents\github\tensorflow\.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file c:\users\tenchumaru\documents\github\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\users\tenchumaru\documents\github\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:windows in file c:\users\tenchumaru\documents\github\tensorflow\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\users\tenchumaru\documents\github\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Repository llvm-project instantiated at:
  C:/users/tenchumaru/documents/github/tensorflow/WORKSPACE:15:14: in <toplevel>
  C:/users/tenchumaru/documents/github/tensorflow/tensorflow/workspace2.bzl:880:21: in workspace
  C:/users/tenchumaru/documents/github/tensorflow/tensorflow/workspace2.bzl:519:15: in _tf_repositories
  C:/users/tenchumaru/documents/github/tensorflow/third_party/llvm/setup.bzl:22:19: in llvm_setup
Repository rule llvm_configure defined at:
  C:/users/tenchumaru/_bazel_tenchumaru/vswyg4al/external/llvm-raw/utils/bazel/configure.bzl:83:33: in <toplevel>
ERROR: An error occurred during the fetch of repository 'llvm-project':
   Traceback (most recent call last):
        File ""C:/users/tenchumaru/_bazel_tenchumaru/vswyg4al/external/llvm-raw/utils/bazel/configure.bzl"", line 73, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""C:/users/tenchumaru/_bazel_tenchumaru/vswyg4al/external/llvm-raw/utils/bazel/configure.bzl"", line 62, column 13, in _overlay_directories
                fail((""Failed to execute overlay script: '{cmd}'\n"" +
Error in fail: Failed to execute overlay script: 'C:/Users/Tenchumaru/Documents/GitHub/tensorflow/venv/Scripts/python.exe C:/users/tenchumaru/_bazel_tenchumaru/vswyg4al/external/llvm-raw/utils/bazel/overlay_directories.py --src C:/users/tenchumaru/_bazel_tenchumaru/vswyg4al/external/llvm-raw --overlay C:/users/tenchumaru/_bazel_tenchumaru/vswyg4al/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""C:\users\tenchumaru\_bazel_tenchumaru\vswyg4al\external\llvm-raw\utils\bazel\overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""C:\users\tenchumaru\_bazel_tenchumaru\vswyg4al\external\llvm-raw\utils\bazel\overlay_directories.py"", line 80, in main
    _symlink_abs(os.path.join(args.overlay, relpath),
  File ""C:\users\tenchumaru\_bazel_tenchumaru\vswyg4al\external\llvm-raw\utils\bazel\overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: [WinError 1314] A required privilege is not held by the client: 'C:\\users\\tenchumaru\\_bazel_tenchumaru\\vswyg4al\\external\\llvm-raw\\utils\\bazel\\llvm-project-overlay\\.bazelignore' -> 'C:\\users\\tenchumaru\\_bazel_tenchumaru\\vswyg4al\\external\\llvm-project\\.bazelignore'

ERROR: C:/users/tenchumaru/documents/github/tensorflow/WORKSPACE:15:14: fetching llvm_configure rule //external:llvm-project: Traceback (most recent call last):
        File ""C:/users/tenchumaru/_bazel_tenchumaru/vswyg4al/external/llvm-raw/utils/bazel/configure.bzl"", line 73, column 25, in _llvm_configure_impl
                _overlay_directories(repository_ctx)
        File ""C:/users/tenchumaru/_bazel_tenchumaru/vswyg4al/external/llvm-raw/utils/bazel/configure.bzl"", line 62, column 13, in _overlay_directories
                fail((""Failed to execute overlay script: '{cmd}'\n"" +
Error in fail: Failed to execute overlay script: 'C:/Users/Tenchumaru/Documents/GitHub/tensorflow/venv/Scripts/python.exe C:/users/tenchumaru/_bazel_tenchumaru/vswyg4al/external/llvm-raw/utils/bazel/overlay_directories.py --src C:/users/tenchumaru/_bazel_tenchumaru/vswyg4al/external/llvm-raw --overlay C:/users/tenchumaru/_bazel_tenchumaru/vswyg4al/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""C:\users\tenchumaru\_bazel_tenchumaru\vswyg4al\external\llvm-raw\utils\bazel\overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""C:\users\tenchumaru\_bazel_tenchumaru\vswyg4al\external\llvm-raw\utils\bazel\overlay_directories.py"", line 80, in main
    _symlink_abs(os.path.join(args.overlay, relpath),
  File ""C:\users\tenchumaru\_bazel_tenchumaru\vswyg4al\external\llvm-raw\utils\bazel\overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: [WinError 1314] A required privilege is not held by the client: 'C:\\users\\tenchumaru\\_bazel_tenchumaru\\vswyg4al\\external\\llvm-raw\\utils\\bazel\\llvm-project-overlay\\.bazelignore' -> 'C:\\users\\tenchumaru\\_bazel_tenchumaru\\vswyg4al\\external\\llvm-project\\.bazelignore'

INFO: Repository build_bazel_rules_android instantiated at:
  C:/users/tenchumaru/documents/github/tensorflow/WORKSPACE:15:14: in <toplevel>
  C:/users/tenchumaru/documents/github/tensorflow/tensorflow/workspace2.bzl:880:21: in workspace
  C:/users/tenchumaru/documents/github/tensorflow/tensorflow/workspace2.bzl:785:20: in _tf_repositories
  C:/users/tenchumaru/documents/github/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  C:/users/tenchumaru/documents/github/tensorflow/third_party/repo.bzl:81:35: in <toplevel>
INFO: Repository flatbuffers instantiated at:
  C:/users/tenchumaru/documents/github/tensorflow/WORKSPACE:15:14: in <toplevel>
  C:/users/tenchumaru/documents/github/tensorflow/tensorflow/workspace2.bzl:873:28: in workspace
  C:/users/tenchumaru/documents/github/tensorflow/tensorflow/workspace2.bzl:66:16: in _initialize_third_party
  C:/users/tenchumaru/documents/github/tensorflow/third_party/flatbuffers/workspace.bzl:6:20: in repo
  C:/users/tenchumaru/documents/github/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive
Repository rule _tf_http_archive defined at:
  C:/users/tenchumaru/documents/github/tensorflow/third_party/repo.bzl:81:35: in <toplevel>
ERROR: C:/users/tenchumaru/documents/github/tensorflow/tensorflow/tools/pip_package/BUILD:277:10: //tensorflow/tools/pip_package:build_pip_package depends on //tensorflow/compiler/mlir/tensorflow:gen_mlir_passthrough_op_py in repository @ which failed to fetch. no such package '@llvm-project//mlir': Failed to execute overlay script: 'C:/Users/Tenchumaru/Documents/GitHub/tensorflow/venv/Scripts/python.exe C:/users/tenchumaru/_bazel_tenchumaru/vswyg4al/external/llvm-raw/utils/bazel/overlay_directories.py --src C:/users/tenchumaru/_bazel_tenchumaru/vswyg4al/external/llvm-raw --overlay C:/users/tenchumaru/_bazel_tenchumaru/vswyg4al/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'
Exited with code 1
stdout:

stderr:
Traceback (most recent call last):
  File ""C:\users\tenchumaru\_bazel_tenchumaru\vswyg4al\external\llvm-raw\utils\bazel\overlay_directories.py"", line 92, in <module>
    main(parse_arguments())
  File ""C:\users\tenchumaru\_bazel_tenchumaru\vswyg4al\external\llvm-raw\utils\bazel\overlay_directories.py"", line 80, in main
    _symlink_abs(os.path.join(args.overlay, relpath),
  File ""C:\users\tenchumaru\_bazel_tenchumaru\vswyg4al\external\llvm-raw\utils\bazel\overlay_directories.py"", line 64, in _symlink_abs
    os.symlink(os.path.abspath(from_path), os.path.abspath(to_path))
OSError: [WinError 1314] A required privilege is not held by the client: 'C:\\users\\tenchumaru\\_bazel_tenchumaru\\vswyg4al\\external\\llvm-raw\\utils\\bazel\\llvm-project-overlay\\.bazelignore' -> 'C:\\users\\tenchumaru\\_bazel_tenchumaru\\vswyg4al\\external\\llvm-project\\.bazelignore'

ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed
INFO: Elapsed time: 0.685s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
    currently loading: tensorflow/lite/python ... (3 packages)
    Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_android/archive/v0.1.1.zip
    Fetching https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.12.0.tar.gz
```
</details>"
56970,Model Maker Object Detection Tutorial crashes in colab at model.export line (TypeError: EndVector() missing 1 required positional argument: 'vectorNumElems'),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

binary

### Tensorflow Version

2.8

### Custom Code

No

### OS Platform and Distribution

Google Colab

### Mobile device

_No response_

### Python version

3.7.14

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When running the Google Colab for Model Maker Object Detection (https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/object_detection.ipynb#scrollTo=Hm_UULdW7A9T) it crashes at the model.export stage.

I was running the colab because running it locally produces the same error and I was trying to tease out the exact combination of versions of libraries to make this work. Sadly it crashed in colab just the same.

From what I can figure out from comments elsewhere this might have to do with a wrong version of FlatBuffers (or a bad combination of libraries).
```


### Standalone code to reproduce the issue

```shell
Run all the preceding code in the colab. It crashes on this line:

model.export(export_dir='.')
```


### Relevant log output

```shell
---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

<ipython-input-10-4c3d25dac6cc> in <module>()
----> 1 model.export(export_dir='.')

8 frames

/usr/local/lib/python3.7/dist-packages/tensorflow_lite_support/metadata/metadata_schema_py_generated.py in Pack(self, builder)
   2562                 for i in reversed(range(len(self.mean))):
   2563                     builder.PrependFloat32(self.mean[i])
-> 2564                 mean = builder.EndVector()
   2565         if self.std is not None:
   2566             if np is not None and type(self.std) is np.ndarray:

TypeError: EndVector() missing 1 required positional argument: 'vectorNumElems'
```
</details>"
56969,Model does not train properly when explicitly applying the gradients,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

2.9

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Training the model with the fit method and with apply_gradient provided two different results (with apply_gradient seems not training).
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from time import time

tf.random.set_seed(0)
lb = tf.constant([0,0], dtype=np.float32)
ub = tf.constant([1,1], dtype=np.float32)
N_0     = 4092
Nepochs = 1500
lr      = 0.001

def fun_u_0(xx):
    c_0  = 0.5*(lb+ub)
    rr   = 0.25*tf.reduce_min(ub-lb)    
    dsq  = tf.math.reduce_sum( (xx-c_0)*(xx-c_0),axis=1)
    return(tf.where( dsq<=rr*rr, 1.0, 0.0) )

def init_model(num_hidden_layers=2, num_neurons_per_layer=64):
    model = tf.keras.Sequential()
    model.add(tf.keras.Input(shape=(2,)) )  
    #scaling_layer = tf.keras.layers.Lambda(lambda x: 2.0*(x - lb)/(ub - lb) - 1.0)
    #model.add(scaling_layer)    
    for _ in range(num_hidden_layers):
        model.add(tf.keras.layers.Dense(num_neurons_per_layer, activation=tf.keras.layers.LeakyReLU(
        ),kernel_initializer=""glorot_uniform"")  )    
    # Output is one-dimensional
    model.add(tf.keras.layers.Dense(1,kernel_initializer=""glorot_uniform""))    
    return model

X_data = tf.random.uniform((N_0,2), lb, ub, dtype=np.float32)
u_data = fun_u_0(X_data)

train_len = int(0.8*X_data.shape[0])
X_train = X_data[:train_len]
u_train = u_data[:train_len]
X_test  = X_data[train_len:]
u_test  = u_data[train_len:]

# This version works:
def my_loss(u_true, u_pred):
    return tf.math.reduce_mean(tf.math.square(u_true - u_pred))

model_0 = init_model(num_hidden_layers=2, num_neurons_per_layer=64)
optim_0 = tf.keras.optimizers.Adam(learning_rate=lr)
model_0.compile(loss=my_loss, optimizer=optim_0)
model_0.summary()
history_0 = model_0.fit(X_train,u_train,validation_data=(X_test.numpy(),u_test.numpy()), verbose=0,epochs=Nepochs,batch_size=X_train.shape[0])

# This version does not work:
def compute_loss(model, X_data, u_data):
    u_pred = model(X_data)
    loss = tf.math.reduce_mean(tf.math.square(u_data - u_pred))
    return loss

@tf.function
def training(model,optim,X_train,u_train,X_test=None,u_test=None):
    if X_test is not None:
        validation_loss  = compute_loss(model, X_test, u_test )
    else:
        validation_loss = None        
    with tf.GradientTape(persistent=True) as tape:
        theta = model.trainable_variables
        tape.watch(theta)
        loss = compute_loss(model, X_train, u_train )
    grad_theta = tape.gradient(loss, theta)
    optim.apply_gradients(zip(grad_theta, model.trainable_variables))
    return loss,validation_loss

model_G = init_model(num_hidden_layers=2, num_neurons_per_layer=64)
optim_G = tf.keras.optimizers.Adam(learning_rate=lr)
model_G.summary()

hist = {'val_loss':[],'loss':[]}
for i in range(Nepochs+1):
        loss, val_loss = training(model_G,optim_G,X_train,u_train,X_test,u_test)
        # Append current loss to hist
        hist['loss'].append(loss.numpy())
        hist['val_loss'].append(val_loss.numpy())
        # Output current loss after 50 iterates
        if val_loss is not None:
          print('It {:05d}: loss = {:10.8e}, validation loss = {:10.8e}'.format(i,loss,val_loss))
        else:
          print('It {:05d}: loss = {:10.8e}'.format(i,loss))
```


### Relevant log output

_No response_</details>"
56968,tape.gradient is returning none,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.7.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I don't understand why the code below output grads to be None.
Could you please give me some explanations ?
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
ones = tf.ones([ 1, 128, 128, 1], tf.float32)
with tf.GradientTape() as tape:

    
    class_channel = tf.constant([4.4328732],  dtype=tf.float32)
    grads = tape.gradient(class_channel, ones)

    print(""grads: "",grads)
```


### Relevant log output

_No response_</details>"
56967,dat file of model cannot be loaded.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
dat file of model cannot be loaded.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import pickle
model=tf.keras.models.Sequential([
          tf.keras.layers.Flatten(input_shape=(28, 28)),
          tf.keras.layers.Dense(128,activation='relu'),
          tf.keras.layers.Dropout(0.2),
          tf.keras.layers.Dense(10)
          ])
output_file=open('model.dat','wb')
pickle.dump(model,output_file)
output_file.close()
input_file=open('model.dat','rb')
model=pickle.load(input_file)
```


### Relevant log output

_No response_</details>"
56966,Memory Leak,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.8.2

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 16.04

### Mobile device

Linux Ubuntu 16.04

### Python version

3.8.2

### Bazel version

5.0.0

### GCC/Compiler version

9.6.0

### CUDA/cuDNN version

11.4

### GPU model and memory

3080 11g

### Current Behaviour?

```shell
When adapting a binary operator for calculation, such as mul, if two input tensors are one-dimensional, one of them will be converted into a scalar,
so that the memory of the tensor is not released, causing memory leaks (Especially when programming with opencl).
```


### Standalone code to reproduce the issue

```shell
'''
import tensorflow  as tf
a = tf.random.normal(shape=[1], dtype=tf.float32)
'''
through tensorflow/python/ops/random_ops.py:43(random_normal)
mean and stddev will be changed tensor. Next, in tensorflow/core/kernels/cwise_ops_common.h:120(Compute) tensor is seen as scalar so that it will be not released.
```


### Relevant log output

_No response_</details>"
56965,_pywrap_tensorflow_internal.so linker error when cross compiling for Aarch64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.2.0

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

10.2/8.2.1

### GPU model and memory

nVidia Xavier NX

### Current Behaviour?

```shell
I'm cross compiling Tensorflow v2.9.1 on my x86-64 machine for my aarch64 target device Xavier NX. My build machine and target device have matching GCC version of 7.5.0, and matching CUDA version, all installed from nvidia Jetpack.

Currently, build fails at linking `_pywrap_tensorflow_internal.so` because some of the static library files are x86-64 format rather than aarch64. Specifically the static libraries in `mlir_generated` directory, like `libnext_after_gpu_f32_f32_kernel_generator.a` and `libelu_gpu_f16_f16_kernel_generator.a`.

I build it with `--distinct_host_configuration=true` bazel option, and I expect the build script to compile those static libraries for the aarch64 target rather than my x86-64 host.
```


### Standalone code to reproduce the issue

```shell
bazel \
--output_base=$(@D)/build \
build --verbose_failures --config=opt --config=cuda \
--define=tflite_with_xnnpack=false \
--config=noaws \
--config=nogcp \
--config=nohdfs \
--config=nonccl \
--config=monolithic \
--config=v2 \
--copt=-ftree-vectorize \
--copt=-funsafe-math-optimizations \
--copt=-ftree-loop-vectorize \
--copt=-fomit-frame-pointer \
--subcommands \
--noincompatible_do_not_split_linking_cmdline \
--crosstool_top=//third_party/toolchains/cpus/aarch64:toolchain \
--distinct_host_configuration=true \
--cpu=aarch64 \
//tensorflow/tools/pip_package:build_pip_package
```
```


### Relevant log output

```shell
ERROR: /home/dev/tf/build/tensorflow-v2.9.1/tensorflow/python/BUILD:3248:24: Linking tensorflow/python/_pywrap_tensorflow_internal.so failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /home/dev/tf/build/tensorflow-v2.9.1/build/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/home/dev/aarch64-linux-gnu/usr/local/cuda-10.2 \
    CUDNN_INSTALL_PATH=/home/dev/aarch64-linux-gnu/usr \
    GCC_HOST_COMPILER_PATH=/home/dev/aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/dev/python3.8 \
    PYTHON_LIB_PATH=/home/dev/python3.8/site-packages \
    TENSORRT_INSTALL_PATH=/home/dev/aarch64-linux-gnu/usr \
    TF2_BEHAVIOR=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=7.2 \
    TF_CUDA_PATHS=/home/dev/aarch64-linux-gnu/usr/local/cuda-10.2 \
    TF_CUDA_VERSION=10.2 \
    TF_CUDNN_VERSION=8.2.1 \
    TF_TENSORRT_VERSION=8.0.1 \
  third_party/toolchains/cpus/aarch64/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/aarch64-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so bazel-out/aarch64-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow_internal.so/pywrap_tensorflow_internal.o -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/rpc/libgrpc_session.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/distributed_runtime/rpc/libgrpc_remote_master.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libgpu_nextafter_op.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libnext_after_gpu_f32_f32_kernel_generator.a -Wl,-no-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libgpu_relu_op.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libelu_gpu_f16_f16_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libelu_gpu_f32_f32_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libelu_gpu_f64_f64_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/librelu_gpu_f16_f16_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/librelu_gpu_f32_f32_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/librelu_gpu_f64_f64_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/librelu_gpu_i8_i8_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/librelu_gpu_i16_i16_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/librelu_gpu_i64_i64_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/librelu_gpu_ui8_ui8_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/librelu_gpu_ui16_ui16_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/librelu_gpu_ui32_ui32_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/librelu_gpu_ui64_ui64_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libselu_gpu_f16_f16_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libselu_gpu_f32_f32_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libselu_gpu_f64_f64_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/libsoftmax_op.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/libsoftmax_op_gpu.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/libsoftplus_op.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/libsoftplus_op_gpu.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libgpu_softplus_op.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libsoftplus_gpu_f16_f16_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libsoftplus_gpu_f32_f32_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libsoftplus_gpu_f64_f64_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/libsoftsign_op.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/libsoftsign_op_gpu.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libgpu_softsign_op.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libsoftsign_gpu_f16_f16_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libsoftsign_gpu_f32_f32_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libsoftsign_gpu_f64_f64_kernel_generator.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/aarch64-opt/bin/tensorflow/core/kernels/libtopk_op.lo -Wl,-no-whole-archive -Wl,-whole-archive -pthread -ldl -lm -lpthread -lm -lpthread -lm -lm -pthread -pthread -pthread -lstdc++ -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu')
# Configuration: 954605433930adc5a888fb615975817a98f0152151879f679ae1137fe14384fa
# Execution platform: @local_execution_config_platform//:platform
/home/dev/aarch64-linux-gnu/bin/../lib/gcc/aarch64-linux-gnu/7.5.0/../../../../aarch64-linux-gnu/bin/ld: bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libnext_after_gpu_f32_f32_kernel_generator.a(next_after_gpu_f32_f32_kernel_generator_kernel.o): Relocations in generic ELF (EM: 62)
/home/dev/aarch64-linux-gnu/bin/../lib/gcc/aarch64-linux-gnu/7.5.0/../../../../aarch64-linux-gnu/bin/ld: bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libnext_after_gpu_f32_f32_kernel_generator.a(next_after_gpu_f32_f32_kernel_generator_kernel.o): Relocations in generic ELF (EM: 62)
/home/dev/aarch64-linux-gnu/bin/../lib/gcc/aarch64-linux-gnu/7.5.0/../../../../aarch64-linux-gnu/bin/ld: bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libnext_after_gpu_f32_f32_kernel_generator.a(next_after_gpu_f32_f32_kernel_generator_kernel.o): error adding symbols: file in wrong format
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build



# ar x /home/dev/tf/build/tensorflow-v2.9.1/build/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/libnext_after_gpu_f32_f32_kernel_generator.a
# file next_after_gpu_f32_f32_kernel_generator_kernel.o
next_after_gpu_f32_f32_kernel_generator_kernel.o: ELF 64-bit LSB relocatable, x86-64, version 1 (SYSV), not stripped



SUBCOMMAND: # //tensorflow/core/kernels/mlir_generated:next_after_gpu_f32_f32_kernel_generator [action 'Generating kernel '//tensorflow/core/kernels/mlir_generated:next_after_gpu_f32_f32_kernel_generator'', configuration: 954605433930adc5a888fb615975817a98f0152151879f679ae1137fe14384fa, execution platform: @local_execution_config_platform//:platform]
  (cd /home/dev/tf/build/tensorflow-v2.9.1/build/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/home/dev/aarch64-linux-gnu/usr/local/cuda-10.2 \
    CUDNN_INSTALL_PATH=/home/dev/aarch64-linux-gnu/usr \
    GCC_HOST_COMPILER_PATH=/home/dev/aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/dev/python3.8 \
    PYTHON_LIB_PATH=/home/dev/python3.8/site-packages \
    TENSORRT_INSTALL_PATH=/home/dev/aarch64-linux-gnu/usr \
    TF2_BEHAVIOR=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=7.2 \
    TF_CUDA_PATHS=/home/dev/aarch64-linux-gnu/usr/local/cuda-10.2 \
    TF_CUDA_VERSION=10.2 \
    TF_CUDNN_VERSION=8.2.1 \
    TF_TENSORRT_VERSION=8.0.1 \
  bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel '--tile_sizes=1024' '--max-supported-rank=5' '--arch=compute_72' '--input=bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/next_after_gpu_f32_f32.mlir' '--output=bazel-out/aarch64-opt/bin/tensorflow/core/kernels/mlir_generated/next_after_gpu_f32_f32_kernel_generator_kernel.o' '--enable_ftz=True' '--cpu_codegen=False' '--jit=False')

```
</details>"
56964,[Question]: Where does these recommended block size come from in Mali GPU delegates?,"i found codes in GPU delegates below: https://github.com/tensorflow/tensorflow/blob/03e6e2af40f53f69e5fab1151235b0880252c21c/tensorflow/lite/delegates/gpu/common/task/util.cc#L150 

and i'm really confused how these thresholds come from，is there any doc or paper that i can learn about? seemed they are related to CU compute capacity, but i don;t know why, could any one help me? thank your so much."
56963,build failure on Debian,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

current master

### Custom Code

No

### OS Platform and Distribution

Linux Debian Unstable

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.1.1

### GCC/Compiler version

9.5

### CUDA/cuDNN version

11.4/8.2

### GPU model and memory

I configured Cuda compute level 6.1 (GTX 1060)

### Current Behaviour?

```shell
The build failed when running a bash command:


ERROR: /home/hans/src/tensorflow/tensorflow/BUILD:1396:19: Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)
```

```
ERROR: /home/hans/src/tensorflow/tensorflow/lite/python/BUILD:68:10 Middleman _middlemen/tensorflow_Slite_Spython_Stflite_Uconvert-runfiles failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)
```

See the Relevant log output below for more of the build log.
```


### Standalone code to reproduce the issue

```shell
bazel build --config=nogcp //tensorflow/tools/pip_package:build_pip_package
```
```


### Relevant log output

```shell
INFO: Found applicable config definition build:short_logs in file /home/hans/src/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/hans/src/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /home/hans/src/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:nogcp in file /home/hans/src/tensorflow/.bazelrc: --define=no_gcp_support=true
INFO: Found applicable config definition build:linux in file /home/hans/src/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/hans/src/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/021b5f0d3daea2d1666599d7d9c471a426c35f65.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/114df244ec50ce0145702974335965c3aa2c3dcc.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
DEBUG: /home/hans/.cache/bazel/_bazel_hans/e1debe52b9a35d05215b3a49132625cd/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10:
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/273292d1cfc0a94a65082ee350509af1d113344d.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pytorch/cpuinfo/archive/5e63739504f0f8e18e941bd63b2d6d42536c7d90.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/6b409ac0a3090ebe74d0cdfb494c4cd91485ad39.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/nvidia/nccl/archive/v2.13.4-1.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/NVIDIA/cudnn-frontend/archive/refs/tags/v0.6.2.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (511 packages loaded, 36686 targets configured).
INFO: Found 1 target...
ERROR: /home/hans/src/tensorflow/tensorflow/BUILD:1396:19: Executing genrule //tensorflow:tf_python_api_gen_v2 failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)
2022-07-31 23:38:36.549685: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""/home/hans/.cache/bazel/_bazel_hans/e1debe52b9a35d05215b3a49132625cd/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/hans/.cache/bazel/_bazel_hans/e1debe52b9a35d05215b3a49132625cd/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTIN6icu_678ByteSinkE

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/hans/.cache/bazel/_bazel_hans/e1debe52b9a35d05215b3a49132625cd/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 22, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/hans/.cache/bazel/_bazel_hans/e1debe52b9a35d05215b3a49132625cd/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/home/hans/.cache/bazel/_bazel_hans/e1debe52b9a35d05215b3a49132625cd/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 77, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""/home/hans/.cache/bazel/_bazel_hans/e1debe52b9a35d05215b3a49132625cd/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /home/hans/.cache/bazel/_bazel_hans/e1debe52b9a35d05215b3a49132625cd/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTIN6icu_678ByteSinkE


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/hans/src/tensorflow/tensorflow/lite/python/BUILD:68:10 Middleman _middlemen/tensorflow_Slite_Spython_Stflite_Uconvert-runfiles failed: (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument skipped)
INFO: Elapsed time: 33090.674s, Critical Path: 280.90s
INFO: 121408 processes: 98221 internal, 23187 local.
FAILED: Build did NOT complete successfully
```
</details>"
56962,INVALID_ARGUMENT:  assertion failed: [predictions must be <= 1],"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

binary

### Tensorflow Version

tf2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.7 / cuDNN 8.4

### GPU model and memory

Geforce RTX 2080TI

### Current Behaviour?

```shell
So I got a Deep Knowledge Tracing Project from GitHub which is open for use by anyone and I wrote it to my use-case. I now trying to train the model in a Jupyter Notebook environment. And when I use the Model.fit() method I get the error output displayed below.
```


### Standalone code to reproduce the issue

```shell
Here is a Link to a repo with everything needed for this error to reproduce

https://github.com/Blackbird95x/Issue_Repo.git
```


### Relevant log output

```shell
2022-07-31 22:35:39.354415: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_INT32
    }
  }
}
 is neither a subtype nor a supertype of the combined inputs preceding it:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_FLOAT
    }
  }
}

	while inferring type of node 'cond_21/output/_23'
2022-07-31 22:35:39.756638: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Input In [9], in <cell line: 1>()
----> 1 history = student_model.fit(dataset = train_set,
      2                             epochs = epochs,
      3                             verbose = verbose,
      4                             validation_data = val_set,
      5                             callbacks = [
      6                                 tf.keras.callbacks.CSVLogger(f""({log_dir}/train.log""),
      7                                 tf.keras.callbacks.ModelCheckpoint(best_model_weights,
      8                                                                    save_best_only = True,
      9                                                                    save_weights_only = True),
     10                                 tf.keras.callbacks.TensorBoard(log_dir=log_dir)
     11                             ])

File ~/project_Bachelor/deep_Knowledge_Tracing/deepkt.py:136, in DKTModel.fit(self, dataset, epochs, verbose, callbacks, validation_data, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)
     64 def fit (self,
     65         dataset,
     66         epochs = 1,
   (...)
     73         validation_steps = None,
     74         validation_freq = 1):
     75     """"""Trains the model for a fixed number of epochs(iterations on a dataset).
     76     Arguments:
     77         dataset: A tf.data.dataset. Should return a tuple of '(inputs,(skills,targets))'
   (...)
    134     
    135     """"""
--> 136     return super(DKTModel, self).fit (x=dataset,
    137                                         epochs = epochs,
    138                                         verbose = verbose,
    139                                         callbacks = callbacks,
    140                                         validation_data = validation_data,
    141                                         shuffle = shuffle,
    142                                         initial_epoch = initial_epoch,
    143                                         steps_per_epoch = steps_per_epoch,
    144                                         validation_steps = validation_steps,
    145                                         validation_freq = validation_freq)

File ~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     65 except Exception as e:  # pylint: disable=broad-except
     66   filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67   raise e.with_traceback(filtered_tb) from None
     68 finally:
     69   del filtered_tb

File ~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     52 try:
     53   ctx.ensure_initialized()
---> 54   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     55                                       inputs, attrs, num_outputs)
     56 except core._NotOkStatusException as e:
     57   if name is not None:

InvalidArgumentError: Graph execution error:

Detected at node 'assert_less_equal/Assert/AssertGuard/Assert' defined at (most recent call last):
    File ""/home/dennis/anaconda3/lib/python3.9/runpy.py"", line 197, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/home/dennis/anaconda3/lib/python3.9/runpy.py"", line 87, in _run_code
      exec(code, run_globals)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/ipykernel_launcher.py"", line 16, in <module>
      app.launch_new_instance()
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/traitlets/config/application.py"", line 846, in launch_instance
      app.start()
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/ipykernel/kernelapp.py"", line 677, in start
      self.io_loop.start()
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/tornado/platform/asyncio.py"", line 199, in start
      self.asyncio_loop.run_forever()
    File ""/home/dennis/anaconda3/lib/python3.9/asyncio/base_events.py"", line 601, in run_forever
      self._run_once()
    File ""/home/dennis/anaconda3/lib/python3.9/asyncio/base_events.py"", line 1905, in _run_once
      handle._run()
    File ""/home/dennis/anaconda3/lib/python3.9/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py"", line 471, in dispatch_queue
      await self.process_one()
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py"", line 460, in process_one
      await dispatch(*args)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py"", line 367, in dispatch_shell
      await result
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py"", line 662, in execute_request
      reply_content = await reply_content
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/ipykernel/ipkernel.py"", line 360, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/ipykernel/zmqshell.py"", line 532, in run_cell
      return super().run_cell(*args, **kwargs)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 2881, in run_cell
      result = self._run_cell(
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 2936, in _run_cell
      return runner(coro)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
      coro.send(None)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3135, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3338, in run_ast_nodes
      if await self.run_code(code, result, async_=asy):
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3398, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""/tmp/ipykernel_30263/708048511.py"", line 1, in <cell line: 1>
      history = student_model.fit(dataset = train_set,
    File ""/home/dennis/project_Bachelor/deep_Knowledge_Tracing/deepkt.py"", line 136, in fit
      return super(DKTModel, self).fit (x=dataset,
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/engine/training.py"", line 1409, in fit
      tmp_logs = self.train_function(iterator)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/engine/training.py"", line 1051, in train_function
      return step_function(self, iterator)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/engine/training.py"", line 1040, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/engine/training.py"", line 1030, in run_step
      outputs = model.train_step(data)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/engine/training.py"", line 894, in train_step
      return self.compute_metrics(x, y, y_pred, sample_weight)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/engine/training.py"", line 987, in compute_metrics
      self.compiled_metrics.update_state(y, y_pred, sample_weight)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/engine/compile_utils.py"", line 501, in update_state
      metric_obj.update_state(y_t, y_p, sample_weight=mask)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/utils/metrics_utils.py"", line 70, in decorated
      update_op = update_state_fn(*args, **kwargs)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/metrics/base_metric.py"", line 140, in update_state_fn
      return ag_update_state(*args, **kwargs)
    File ""/home/dennis/project_Bachelor/deep_Knowledge_Tracing/metrics.py"", line 16, in update_state
      super(AUC, self).update_state(y_true=true,
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/metrics/metrics.py"", line 1759, in update_state
      return metrics_utils.update_confusion_matrix_variables(
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/utils/metrics_utils.py"", line 606, in update_confusion_matrix_variables
      tf.debugging.assert_less_equal(
Node: 'assert_less_equal/Assert/AssertGuard/Assert'
Detected at node 'assert_less_equal/Assert/AssertGuard/Assert' defined at (most recent call last):
    File ""/home/dennis/anaconda3/lib/python3.9/runpy.py"", line 197, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/home/dennis/anaconda3/lib/python3.9/runpy.py"", line 87, in _run_code
      exec(code, run_globals)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/ipykernel_launcher.py"", line 16, in <module>
      app.launch_new_instance()
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/traitlets/config/application.py"", line 846, in launch_instance
      app.start()
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/ipykernel/kernelapp.py"", line 677, in start
      self.io_loop.start()
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/tornado/platform/asyncio.py"", line 199, in start
      self.asyncio_loop.run_forever()
    File ""/home/dennis/anaconda3/lib/python3.9/asyncio/base_events.py"", line 601, in run_forever
      self._run_once()
    File ""/home/dennis/anaconda3/lib/python3.9/asyncio/base_events.py"", line 1905, in _run_once
      handle._run()
    File ""/home/dennis/anaconda3/lib/python3.9/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py"", line 471, in dispatch_queue
      await self.process_one()
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py"", line 460, in process_one
      await dispatch(*args)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py"", line 367, in dispatch_shell
      await result
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py"", line 662, in execute_request
      reply_content = await reply_content
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/ipykernel/ipkernel.py"", line 360, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/ipykernel/zmqshell.py"", line 532, in run_cell
      return super().run_cell(*args, **kwargs)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 2881, in run_cell
      result = self._run_cell(
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 2936, in _run_cell
      return runner(coro)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
      coro.send(None)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3135, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3338, in run_ast_nodes
      if await self.run_code(code, result, async_=asy):
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3398, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""/tmp/ipykernel_30263/708048511.py"", line 1, in <cell line: 1>
      history = student_model.fit(dataset = train_set,
    File ""/home/dennis/project_Bachelor/deep_Knowledge_Tracing/deepkt.py"", line 136, in fit
      return super(DKTModel, self).fit (x=dataset,
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/engine/training.py"", line 1409, in fit
      tmp_logs = self.train_function(iterator)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/engine/training.py"", line 1051, in train_function
      return step_function(self, iterator)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/engine/training.py"", line 1040, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/engine/training.py"", line 1030, in run_step
      outputs = model.train_step(data)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/engine/training.py"", line 894, in train_step
      return self.compute_metrics(x, y, y_pred, sample_weight)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/engine/training.py"", line 987, in compute_metrics
      self.compiled_metrics.update_state(y, y_pred, sample_weight)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/engine/compile_utils.py"", line 501, in update_state
      metric_obj.update_state(y_t, y_p, sample_weight=mask)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/utils/metrics_utils.py"", line 70, in decorated
      update_op = update_state_fn(*args, **kwargs)
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/metrics/base_metric.py"", line 140, in update_state_fn
      return ag_update_state(*args, **kwargs)
    File ""/home/dennis/project_Bachelor/deep_Knowledge_Tracing/metrics.py"", line 16, in update_state
      super(AUC, self).update_state(y_true=true,
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/metrics/metrics.py"", line 1759, in update_state
      return metrics_utils.update_confusion_matrix_variables(
    File ""/home/dennis/anaconda3/lib/python3.9/site-packages/keras/utils/metrics_utils.py"", line 606, in update_confusion_matrix_variables
      tf.debugging.assert_less_equal(
Node: 'assert_less_equal/Assert/AssertGuard/Assert'
2 root error(s) found.
  (0) INVALID_ARGUMENT:  assertion failed: [predictions must be <= 1] [Condition x <= y did not hold element-wise:] [x (Sum_5:0) = ] [[[1][1.00000012][0.99999994]]...] [y (Cast_11/x:0) = ] [1]
	 [[{{node assert_less_equal/Assert/AssertGuard/Assert}}]]
	 [[assert_less_equal_1/Assert/AssertGuard/pivot_f/_79/_131]]
  (1) INVALID_ARGUMENT:  assertion failed: [predictions must be <= 1] [Condition x <= y did not hold element-wise:] [x (Sum_5:0) = ] [[[1][1.00000012][0.99999994]]...] [y (Cast_11/x:0) = ] [1]
	 [[{{node assert_less_equal/Assert/AssertGuard/Assert}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_7474]

​
```
</details>"
56961,Wrong gradient in TensorScatterUpdate with duplicated indices,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.2

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

`tf.tensor_scatter_nd_update()` generates wrong gradients (summation) with duplicated indices. According to the doc, on CPU, only the gradient of last update should be used.


### Standalone code to reproduce the issue

https://colab.research.google.com/drive/11esloyqJ4ysEqobGfPuKvvW1IpZirTvC


### Relevant log output

_No response_</details>"
56959,GPU detected in terminal but not while running file,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Kubuntu 22.04

### Mobile device

_No response_

### Python version

3.10/3.9

### Bazel version

_No response_

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

RTX 2070

### Current Behaviour?

```shell
When I followed the instructions to install TensorFlow on the main website everything worked in the terminal including the GPU output. When I run the code to test if the GPU is detected on a Jupyter notebook or .py file, it can't detect it. I am using the same Conda environment that I installed TensorFlow on.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.config.list_physical_devices('GPU')
```


### Relevant log output

```shell
2022-07-30 14:43:50.994118: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-07-30 14:43:50.994146: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2022-07-30 14:43:50.994170: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2022-07-30 14:43:50.994194: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2022-07-30 14:43:50.994217: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2022-07-30 14:43:50.994240: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory
2022-07-30 14:43:50.994263: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2022-07-30 14:43:50.994288: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2022-07-30 14:43:50.994292: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
```
</details>"
56958,Why the RMSE worse  on higher tensorflow versions,"
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.7.1, 2.4.1, 2.3.0, 2.2.0, 2.1.0, 2.0.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

3.7.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?


My code was tested on tensorflow 2.7.1, 2.4.1, 2.3.0, 2.2.0, 2.1.0, and 2.0.0. The outputs were evaluated using  RMSE, and I found that the code performed best at version 2.0.0. The detailed experimental results are as follows.
| RMSE | Version |
| ---- | ------- |
| 1.83 | 2.7.1   |
| 1.28 | 2.4.1   |
| 2.36 | 2.3.0   |
| 2.17 | 2.2.0   |
| 1.23 | 2.1.0   |
| 1.08 | 2.0.0   |



### Standalone code to reproduce the issue

```python
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
import tensorflow.keras.layers as L
import tensorflow.keras.models as M
import tensorflow.keras.optimizers as O
import tensorflow.keras.losses as Los

from sklearn.model_selection import KFold
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')

positive_excerpts = train_data[train_data.target.values>=0]
negative_excerpts = train_data[train_data.target.values<0]

positive_len = [len(x) for x in positive_excerpts.excerpt.values]
negative_len = [len(x) for x in negative_excerpts.excerpt.values]

tokenizer = Tokenizer(num_words=500)
tokenizer.fit_on_texts(train_data.excerpt.values)
data_seq = tokenizer.texts_to_sequences(train_data.excerpt.values)

BATCH_SIZE = 16
MAX_LEN = 172
EPOCHS = 10
pad_data_seq = tf.keras.preprocessing.sequence.pad_sequences(data_seq,maxlen=MAX_LEN,padding='post')
def build_model():
    inp = L.Input(shape=(MAX_LEN,))
    emb = L.Embedding(input_dim=500,output_dim = 62)(inp)
    X = L.Bidirectional(L.LSTM(32))(emb)
    X = L.Dense(64,activation='relu')(X)
    X = L.Dense(32,activation='relu')(X)
    out = L.Dense(1)(X)
    
    model = M.Model(inputs=inp,outputs=out)
    model.compile(loss='mse',optimizer='adam',metrics=['acc'])
    return model
model = build_model()
model.summary()
kf = KFold(n_splits=5,random_state=24,shuffle=True)

for index,(t_idx,v_idx) in enumerate(kf.split(pad_data_seq)):
    print(f""######## STEP {index+1} ########"")
    train_data_seq = pad_data_seq[t_idx]
    val_data_seq = pad_data_seq[v_idx]
    train_target = train_data.target.values[t_idx]
    val_target = train_data.target.values[v_idx]
    
    history = model.fit(train_data_seq,
                        train_target,
                        validation_data=(val_data_seq,val_target),
                        epochs=EPOCHS,
                        batch_size=BATCH_SIZE)
test_data_seq = tokenizer.texts_to_sequences(test_data.excerpt.values)
test_data_seq = tf.keras.preprocessing.sequence.pad_sequences(test_data_seq,maxlen=MAX_LEN)
pred = model.predict(pad_data_seq)
pred = model.predict(test_data_seq,verbose=1)
sampl = pd.read_csv('sample_submission.csv')
sampl.target = pred
sampl.to_csv('submission.csv',index=False)

from sklearn.metrics import mean_squared_error
import pandas as pd

y_pred = pd.read_csv('submission.csv')[['target']]
y_target = pd.read_csv('target.csv')[['target']]
print(""MSE: "", mean_squared_error(y_target, y_pred))
print(""RMSE: "",mean_squared_error(y_target,y_pred,squared=False))
```
[dataset.zip](https://github.com/tensorflow/tensorflow/files/9226863/dataset.zip)


### Relevant log output
Here are the outputs of my codes on different tensroflow versions.
[evaluation.zip](https://github.com/tensorflow/tensorflow/files/9226865/evaluation.zip)

"
56957,bazel build tflite ：xnnpack/assembly.h not found,"**System information** 
os:win11
tfv ersion:2.9.0
bazel version:5.1.1
python version:3.8.12
android sdk:31
ndk version:21.4.7075529
the build command:
bazel build -c opt  --cxxopt=""--std=c++11""  --fat_apk_cpu=arm64-v8a --config=android_arm64 //tensorflow/lite:libtensorflowlite.so --verbose_failures
log:
ERROR: C:/users/dcl/_bazel_dcl/4iqg5dqe/external/XNNPACK/BUILD.bazel:10079:19: Compiling src/f32-gemm/gen/1x12-minmax-aarch64-neonfma-cortex-a53.S failed: (Exit 1): clang failed: error executing command
  cd /d C:/users/dcl/_bazel_dcl/4iqg5dqe/execroot/org_tensorflow
  SET ANDROID_BUILD_TOOLS_VERSION=32.0.0
    SET ANDROID_NDK_API_LEVEL=21
    SET ANDROID_NDK_HOME=D:/sdk/ndk/21.4.7075529
    SET ANDROID_SDK_API_LEVEL=31
    SET ANDROID_SDK_HOME=D:/sdk
    SET PATH=D:\Program Files\msys64\usr\bin;D:\Program Files\msys64\bin;C:\Windows;C:\Windows\System32;C:\Windows\System32\WindowsPowerShell\v1.0;D:\ProgramData\Miniconda3;D:\ProgramData\Miniconda3\Library\mingw-w64\bin;D:\ProgramData\Miniconda3\Library\usr\bin;D:\ProgramData\Miniconda3\Library\bin;D:\ProgramData\Miniconda3\Scripts;D:\ProgramData\Miniconda3\bin;D:\ProgramData\Miniconda3\condabin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\libnvvp;D:\ProgramData\Miniconda3;D:\ProgramData\Miniconda3\Library\mingw-w64\bin;D:\ProgramData\Miniconda3\Library\usr\bin;D:\ProgramData\Miniconda3\Library\bin;D:\ProgramData\Miniconda3\Scripts;C:\Program Files\Microsoft\jdk-11.0.12.7-hotspot\bin;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Program Files\NVIDIA Corporation\Nsight Compute 2022.1.0;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;D:\Program Files\Java\jdk-11\bin;D:\sdk\platform-tools;D:\Program Files\Sublime Text 3;D:\Program Files\Git\cmd;D:\Program Files\CMake\bin;D:\opencv\opencv\build\install\x64\vc17\bin;F:\mysql-8.0.29-winx64\bin;F:\mysql-8.0.29-winx64;D:\Program Files\msys64;D:\Program Files\msys64\usr\bin;%USERPROFILE%\AppData\Local\Microsoft\WindowsApps;%PyCharm%;%PyCharm Community Edition%;%CLion%;D:\Program Files (x86)\Tencent\QQGameTempest\Hall.57795
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/ProgramData/Miniconda3/python.exe
    SET PYTHON_LIB_PATH=D:/ProgramData/Miniconda3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TF2_BEHAVIOR=1
  external\androidndk\ndk\toolchains\llvm\prebuilt\windows-x86_64\bin\clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64 -target aarch64-none-linux-android -fpic -isystemexternal/androidndk/ndk/sysroot/usr/include/aarch64-linux-android -D__ANDROID_API__=21 -no-canonical-prefixes -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -funwind-tables -fstack-protector-strong -fno-addrsig -Werror=return-type -Werror=int-to-pointer-cast -Werror=pointer-to-int-cast -Werror=implicit-function-declaration -O2 -g -DNDEBUG -fPIC -w -Iinclude -Isrc -march=armv8.2-a+fp16+dotprod -O2 --sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64 -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c external/XNNPACK/src/f32-gemm/gen/1x12-minmax-aarch64-neonfma-cortex-a53.S -o bazel-out/arm64-v8a-opt/bin/external/XNNPACK/_objs/asm_microkernels/0/1x12-minmax-aarch64-neonfma-cortex-a53.pic.o
# Configuration: fa4e536499f276c5e5341ebd18f72405a5590e5d00d27f6a1f39cc52524f9fa1
# Execution platform: @local_execution_config_platform//:platform
external/XNNPACK/src/f32-gemm/gen/1x12-minmax-aarch64-neonfma-cortex-a53.S:10:10: fatal error: 'xnnpack/assembly.h' file not found
#include <xnnpack/assembly.h>
         ^~~~~~~~~~~~~~~~~~~~
1 error generated.
ERROR: C:/users/dcl/_bazel_dcl/4iqg5dqe/external/XNNPACK/BUILD.bazel:10079:19: Compiling src/qs8-igemm/gen/4x8-minmax-rndnu-aarch64-neon-mlal-lane-ld64.S failed: (Exit 1): clang failed: error executing command
  cd /d C:/users/dcl/_bazel_dcl/4iqg5dqe/execroot/org_tensorflow
  SET ANDROID_BUILD_TOOLS_VERSION=32.0.0
    SET ANDROID_NDK_API_LEVEL=21
    SET ANDROID_NDK_HOME=D:/sdk/ndk/21.4.7075529
    SET ANDROID_SDK_API_LEVEL=31
    SET ANDROID_SDK_HOME=D:/sdk
    SET PATH=D:\Program Files\msys64\usr\bin;D:\Program Files\msys64\bin;C:\Windows;C:\Windows\System32;C:\Windows\System32\WindowsPowerShell\v1.0;D:\ProgramData\Miniconda3;D:\ProgramData\Miniconda3\Library\mingw-w64\bin;D:\ProgramData\Miniconda3\Library\usr\bin;D:\ProgramData\Miniconda3\Library\bin;D:\ProgramData\Miniconda3\Scripts;D:\ProgramData\Miniconda3\bin;D:\ProgramData\Miniconda3\condabin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\libnvvp;D:\ProgramData\Miniconda3;D:\ProgramData\Miniconda3\Library\mingw-w64\bin;D:\ProgramData\Miniconda3\Library\usr\bin;D:\ProgramData\Miniconda3\Library\bin;D:\ProgramData\Miniconda3\Scripts;C:\Program Files\Microsoft\jdk-11.0.12.7-hotspot\bin;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Program Files\NVIDIA Corporation\Nsight Compute 2022.1.0;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;D:\Program Files\Java\jdk-11\bin;D:\sdk\platform-tools;D:\Program Files\Sublime Text 3;D:\Program Files\Git\cmd;D:\Program Files\CMake\bin;D:\opencv\opencv\build\install\x64\vc17\bin;F:\mysql-8.0.29-winx64\bin;F:\mysql-8.0.29-winx64;D:\Program Files\msys64;D:\Program Files\msys64\usr\bin;%USERPROFILE%\AppData\Local\Microsoft\WindowsApps;%PyCharm%;%PyCharm Community Edition%;%CLion%;D:\Program Files (x86)\Tencent\QQGameTempest\Hall.57795
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=D:/ProgramData/Miniconda3/python.exe
    SET PYTHON_LIB_PATH=D:/ProgramData/Miniconda3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TF2_BEHAVIOR=1
  external\androidndk\ndk\toolchains\llvm\prebuilt\windows-x86_64\bin\clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64 -target aarch64-none-linux-android -fpic -isystemexternal/androidndk/ndk/sysroot/usr/include/aarch64-linux-android -D__ANDROID_API__=21 -no-canonical-prefixes -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -funwind-tables -fstack-protector-strong -fno-addrsig -Werror=return-type -Werror=int-to-pointer-cast -Werror=pointer-to-int-cast -Werror=implicit-function-declaration -O2 -g -DNDEBUG -fPIC -w -Iinclude -Isrc -march=armv8.2-a+fp16+dotprod -O2 --sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64 -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c external/XNNPACK/src/qs8-igemm/gen/4x8-minmax-rndnu-aarch64-neon-mlal-lane-ld64.S -o bazel-out/arm64-v8a-opt/bin/external/XNNPACK/_objs/asm_microkernels/1/4x8-minmax-rndnu-aarch64-neon-mlal-lane-ld64.pic.o
# Configuration: fa4e536499f276c5e5341ebd18f72405a5590e5d00d27f6a1f39cc52524f9fa1
# Execution platform: @local_execution_config_platform//:platform
external/XNNPACK/src/qs8-igemm/gen/4x8-minmax-rndnu-aarch64-neon-mlal-lane-ld64.S:11:10: fatal error: 'xnnpack/assembly.h' file not found
#include <xnnpack/assembly.h>
         ^~~~~~~~~~~~~~~~~~~~
1 error generated.
Target //tensorflow/lite:libtensorflowlite.so failed to build
INFO: Elapsed time: 12.082s, Critical Path: 9.22s
INFO: 45 processes: 18 internal, 27 local.
FAILED: Build did NOT complete successfully


why cant find assembly.h?"
56955,How to register an op of conv grad? ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

tf 2.5

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I try to register an op of `conv grad` with the code below:

REGISTER_OP(""ConvNewGrad"")
  .Attr(""T: {float, double}"")
  .Attr(""use_bias: bool=true"")
  .Attr(""need_pad: bool=false"")
  .Attr(""stride: int"")
#ifdef USE_SGX
  .Attr(""eid_low: int"")
  .Attr(""eid_high: int"")
#endif
  .Input(""grad: T"")
  .Input(""input: T"")
  .Input(""kernel: T"")
  .Input(""bias: T"")
  .Output(""grad_input: T"")
  .Output(""grad_kernel: T"")
  .Output(""grad_bias: T"");
```

but when i run the trainning step, there is an error:
```
tensorflow.python.framwork.errors_impl.InternalError:Recorded operation 'ConvNew' returned too few gradients. Expected 4 but received 3.

```
I wonder where I can see the Grad OP of Conv in TensorFlow，and What four gradients does it need?

Can you help me?
```


### Standalone code to reproduce the issue

```shell
The detailed code of grad hasn't been written yet.
conv_kernel_grad_cc:

REGISTER_OP(""ConvNewGrad"")
  .Attr(""T: {float, double}"")
  .Attr(""use_bias: bool=true"")
  .Attr(""need_pad: bool=false"")
  .Attr(""stride: int"")
#ifdef USE_SGX
  .Attr(""eid_low: int"")
  .Attr(""eid_high: int"")
#endif
  .Input(""grad: T"")
  .Input(""input: T"")
  .Input(""kernel: T"")
  .Input(""bias: T"")
//   .Input(""out: T"")
  .Output(""grad_input: T"")
  .Output(""grad_kernel: T"")
  .Output(""grad_bias: T"");

template <typename Device, typename T>
class ConvNewGradOp : public OpKernel {
public:
    explicit ConvNewGradOp(OpKernelConstruction* context) : OpKernel(context) {
#ifdef USE_SGX
        OP_REQUIRES_OK(context, context->GetAttr(""eid_low"", &eid_low_));
        OP_REQUIRES_OK(context, context->GetAttr(""eid_high"", &eid_high_));
        lib = dlopen(""layers_sgx.so"", RTLD_NOW);
        OP_REQUIRES(context, lib != NULL, errors::Unknown(""Unable to load layers_sgx.so!""));
#endif
        OP_REQUIRES_OK(context, context->GetAttr(""use_bias"", &use_bias_));
        OP_REQUIRES_OK(context, context->GetAttr(""stride"", &stride_));
        OP_REQUIRES_OK(context, context->GetAttr(""need_pad"", &need_pad_));
    }
  
    void Compute(OpKernelContext* context) override {
        DCHECK_EQ(context->num_inputs(),4);
    }
  
private:
    bool use_bias_;
    bool need_pad_;
    void *lib;
    int stride_;
#ifdef USE_SGX
    int64 eid_low_;
    int64 eid_high_;
#endif
};

```
conv.py:
```
@ops.RegisterGradient(""ConvNew"")
def _conv_new_grad_cc(op, grad):
    return lib.conv_new_grad(grad, op.inputs[0], op.inputs[1], op.inputs[2], use_bias=op.get_attr(""use_bias""), need_pad=op.get_attr(""need_pad""),eid_low=op.get_attr(""eid_low""), stride=op.get_attr(""stride""),eid_high=op.get_attr(""eid_high""))
```
```


### Relevant log output

```shell
tensorflow.python.framwork.errors_impl.InternalError:Recorded operation 'ConvNew' returned too few gradients. Expected 4 but received 3.

```
```
</details>"
56954,TensorRT max number of engines,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Feature Request

### Source

source

### Tensorflow Version

TF2.9.1 + TensorRT

### Custom Code

No

### OS Platform and Distribution

20.04 ubuntu

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
thank you for adding support for Tensor-RT python API. 
It seems like TF-TRT has a limit on the number of engines it can create, and it seems to be 20. 

I see that ~65% ops are converted, when I expect more to be converted. Even for larger models the number of engines is exactly 20. 

[*] Total number of TensorRT engines: 20
[*] % of OPs Converted: 65.84% [688/1045]
```

Is there some documentation on setting max number of TensorRt engines.
```


### Standalone code to reproduce the issue

```shell
Apologies, the model itself is not easy to share, but the conversion follows standard steps:



    params = params._replace(
        precision_mode=precision_mode,
        max_workspace_size_bytes=2 << 32,  # 8,589,934,592 bytes
        maximum_cached_engines=100,
        minimum_segment_size=3,
        allow_build_at_runtime=True,
    )
    converter = trt.TrtGraphConverterV2(
        input_saved_model_dir=path,
        conversion_params=params,
    )
    converter.convert()
```
```


### Relevant log output

```shell
[*] Total number of TensorRT engines: 20
[*] % of OPs Converted: 65.84% [688/1045]
```
```
</details>"
56950,"Custom op's quantized version is not called, custom op's input and output are still floating-point","_TL;DR_: All the convolution operators are quantized in the TFLite model as expected, but the custom operator is not.

I built TensorFlow from source (SHA: e0032f9d64d22e4b6f20102c35045247fe704596) after adding a new custom operator in `tensorflow/core/user_ops/dummy.cc` (see code below).

I then created the following model with a few convolutions and the custom op. The goal is to have the whole TFLite model being quantized, as showed in this diagram:

```mermaid
  flowchart
    id1(quantized input)-->conv0[""Convolution""]
    conv0-->dummy[""Dummy Op""]
    dummy-->conv1[""Convolution""]
    conv1-->id2(quantized output 0)
    conv0-->conv2[""Convolution""]
    conv2-->conv3[""Convolution""]
    conv3-->id3(quantized output 1)
    style id1 stroke-width:0px,fill-opacity:0
    style id2 stroke-width:0px,fill-opacity:0
    style id3 stroke-width:0px,fill-opacity:0
```
Instead, I'm getting a TFLite model in which all the convolutions are indeed quantized, but the custom op is still floating-point and the model includes Dequantize/Quantize operators before/after the custom op. **Are quantized custom ops supported? If so, what's missing here to make sure the quantized version of the custom operator ends up in the TFLite model?**
```mermaid
  flowchart
    id1(quantized input)-->conv0[""Convolution""]
    conv0-->dequant[""Dequantize""]
    dequant--floating-point-->dummy[""Dummy Op""]
    dummy--floating-point-->quant[""Quantize""]
    quant-->conv1[""Convolution""]
    conv1-->id2(quantized output 0)
    conv0-->conv2[""Convolution""]
    conv2-->conv3[""Convolution""]
    conv3-->id3(quantized output 1)
    style id1 stroke-width:0px,fill-opacity:0
    style id2 stroke-width:0px,fill-opacity:0
    style id3 stroke-width:0px,fill-opacity:0
```

Python code to build and convert model (three TFLite models will be created: one with no optimization, one with floating-point input/outputs with optimizations, and the last one with quantized input/outputs with optimizations, which is the one most relevant to this issue):
```
import tensorflow as tf

def quant(x):
    return tf.quantization.fake_quant_with_min_max_args(x, -1, 1, 8, False)

class QConv(tf.keras.layers.Conv2D):
    def __init__(self, filters, kernel_size = 3):
        super().__init__(filters = filters, kernel_size = kernel_size)

    def call(self, bottom):
        return quant(self.convolution_op(bottom, quant(self.kernel)))

def tflite_model(save_path, tflite_path, input_type = tf.float32, output_type = tf.float32, optimize = False):
    converter = tf.lite.TFLiteConverter.from_saved_model(save_path)
    if optimize:
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.allow_custom_ops = True
    converter.inference_input_type = input_type
    converter.inference_output_type = output_type
    tflite_model = converter.convert()
    with open(tflite_path, ""wb"") as f:
        f.write(tflite_model)
    print(f""TFLite file saved in {tflite_path}"")

input_tensor = tf.keras.Input(batch_size=1, shape=(256, 256, 4), name='input')
output = quant(input_tensor)

# First convolution
layer = QConv(filters=32)
output = layer(output)

# Split in the model: DummyOp on one side, convolution on the other
output0 = tf.dummy_op(output)
output0 = quant(output0)

layer = QConv(filters=32)
output1 = layer(output)

# Another convolution
layer = QConv(filters=16)
output0 = layer(output0)

layer = QConv(filters=16)
output1 = layer(output1)

model = tf.keras.Model(inputs=[input_tensor], outputs=[output0, output1])

save_path = ""/tmp/dummy_model""
model.save(save_path)

# Floating-point model, not optimized
tflite_model(save_path, ""/tmp/dummy_not_optimized.tflite"")

# Floating-point model, optimized
tflite_model(save_path, ""/tmp/dummy_float32.tflite"", optimize=True)

# Quantized model, optimized
tflite_model(save_path, ""/tmp/dummy_int8.tflite"", input_type=tf.int8, output_type=tf.int8, optimize=True)
```

C++ code for custom operator in `tensorflow/core/user_ops/dummy.cc`:
```
#include ""tensorflow/core/framework/common_shape_fns.h""
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/op_kernel.h""

REGISTER_OP(""DummyOp"")
    .Attr(""T: {int8, qint8, quint8, float}"")
    .Input(""input: T"")
    .Output(""output: T"")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
        c->set_output(0, c->input(0));  // Output is same shape as input's.
        return tensorflow::Status::OK();
    });

class DummyOp : public tensorflow::OpKernel {
 public:
  using tensorflow::OpKernel::OpKernel;

  void Compute(tensorflow::OpKernelContext* context) override {
    // Allocates output tensor, does not compute anything.
    const tensorflow::Tensor& input_tensor = context->input(0);
    tensorflow::Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                                                     &output_tensor));
  }
};

REGISTER_KERNEL_BUILDER(Name(""DummyOp"").Device(tensorflow::DEVICE_CPU).TypeConstraint<int8_t>(""T""), DummyOp);
REGISTER_KERNEL_BUILDER(Name(""DummyOp"").Device(tensorflow::DEVICE_CPU).TypeConstraint<tensorflow::qint8>(""T""), DummyOp);
REGISTER_KERNEL_BUILDER(Name(""DummyOp"").Device(tensorflow::DEVICE_CPU).TypeConstraint<tensorflow::quint8>(""T""), DummyOp);
REGISTER_KERNEL_BUILDER(Name(""DummyOp"").Device(tensorflow::DEVICE_CPU).TypeConstraint<float>(""T""), DummyOp);
```
"
56946,`tf.lite.OpsSet.SELECT_TF_OPS` makes Interpreter invoke function crash,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04
-   **TensorFlow installed from (source or binary)**: Binary (Anaconda)
-   **TensorFlow version (use command below)**: 2.4.1
-   **Python version**: 3.9.12
-   **cudatoolkit version**: 10.1.243
-   **cuDNN version**: 7.6.5
-   **GPU model and memory**: NVIDIA GeForce GTX 1650 Mobile / Max-Q
-  **nvidia-smi**:
```
> Fri Jul 29 21:31:26 2022       
> +-----------------------------------------------------------------------------+
> | NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |
> |-------------------------------+----------------------+----------------------+
> | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
> | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
> |                               |                      |               MIG M. |
> |===============================+======================+======================|
> |   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
> | N/A   55C    P8     2W /  N/A |      4MiB /  4096MiB |      0%      Default |
> |                               |                      |                  N/A |
> +-------------------------------+----------------------+----------------------+
>                                                                                
> +-----------------------------------------------------------------------------+
> | Processes:                                                                  |
> |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
> |        ID   ID                                                   Usage      |
> |=============================================================================|
> |    0   N/A  N/A      1185      G   /usr/lib/xorg/Xorg                  4MiB |
> +-----------------------------------------------------------------------------+

```
-   **Anaconda-env**: [anaconda-env.zip](https://github.com/tensorflow/tensorflow/files/9222799/anaconda-env.zip)


### Describe the problem
Hello, I am starting working with Tensorflow and I am facing a problem that seems similar to this issue: https://github.com/tensorflow/tensorflow/issues/25231

#### Context
I have a trained model I cannot really change and I have to deploy it on Android.  
My first thought was to use Tensorflow Lite to convert it and use it on Android. So I have converted it and I have tried to load it on my computer first (before trying to deploy it on Android). While the `invoke` function of the interpreter is called, it crashes (or the execution is aborted) with no error.  
It seems it cannot handle 5D tensor as [Oktai15](https://github.com/Oktai15) says but I cannot find any documentation that explains this constraint. Moreover, even if it cannot handle the case, it should return an error message and it should be explained in the documentation.  

In order to be sure the problem is the fact that the interpreter doesn't handle 5D tensors, I have created a simple script that trains a model on the equation `y = 2*x`.

#### Source code working with 4D tensor
Here is the code that use a 4D-tensor to predict the result for the equation `y = 2*x`

```
import pathlib
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf

print('\u2022 Using TensorFlow Version:', tf.__version__)

# Create a simple Keras model.
x = [-5, -1, 0, 1, 4]
y = [-10, -2, 0, 2, 8]

model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(units = 1, input_shape=[1,1,1,1])
])
model.compile(optimizer='sgd',
              loss='mean_squared_error')

model.fit(x, y, epochs=200)

export_dir = 'saved_model/2'
tf.saved_model.save(model, export_dir)

# Convert the model.
converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
# converter.target_spec.supported_ops = [
#   tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
# ]
tflite_model = converter.convert()

tflite_model_file = pathlib.Path('model.tflite')
tflite_model_file.write_bytes(tflite_model)

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test the TensorFlow Lite model on random input data.
input_shape = input_details[0]['shape']
inputs, outputs, outputsTFL = [], [], []
for _ in range(100):
    input_data = np.array(np.random.random_sample(input_shape)*100, dtype=np.float32)
    interpreter.set_tensor(input_details[0]['index'], input_data)
    
    interpreter.invoke()
    tflite_results = interpreter.get_tensor(output_details[0]['index'])
    
    # Test the TensorFlow model on random input data.
    tf_results = model(tf.constant(input_data))
    output_data = np.array(tf_results)
    
    inputs.append(input_data[0][0][0][0][0])
    outputs.append(output_data[0][0][0][0][0])
    outputsTFL.append(np.array(tflite_results)[0][0][0][0][0])

plt.plot(inputs, outputs, 'r')
plt.show()
plt.plot(inputs, outputsTFL, 'r')
plt.show()
```

#### Source code not working with 5D tensor
Here is the code that use a 5D-tensor to predict the result for the equation `y = 2*x`.   

```
import pathlib
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf

print('\u2022 Using TensorFlow Version:', tf.__version__)

# Create a simple Keras model.
x = [-5, -1, 0, 1, 4]
y = [-10, -2, 0, 2, 8]

model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(units = 1, input_shape=[1,1,1,1,1])
])
model.compile(optimizer='sgd',
              loss='mean_squared_error')

model.fit(x, y, epochs=200)

export_dir = 'saved_model/2'
tf.saved_model.save(model, export_dir)

# Convert the model.
converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]
tflite_model = converter.convert()

tflite_model_file = pathlib.Path('model.tflite')
tflite_model_file.write_bytes(tflite_model)

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test the TensorFlow Lite model on random input data.
input_shape = input_details[0]['shape']
inputs, outputs, outputsTFL = [], [], []
for _ in range(100):
    input_data = np.array(np.random.random_sample(input_shape)*100, dtype=np.float32)
    interpreter.set_tensor(input_details[0]['index'], input_data)
    
    interpreter.invoke()
    tflite_results = interpreter.get_tensor(output_details[0]['index'])
    
    # Test the TensorFlow model on random input data.
    tf_results = model(tf.constant(input_data))
    output_data = np.array(tf_results)
    
    inputs.append(input_data[0][0][0][0][0][0])
    outputs.append(output_data[0][0][0][0][0][0])
    outputsTFL.append(np.array(tflite_results)[0][0][0][0][0][0])

plt.plot(inputs, outputs, 'r')
plt.show()
plt.plot(inputs, outputsTFL, 'r')
plt.show()
```

**WARNING**: The converter needs the quantization that enables Tensorflow ops: `tf.lite.OpsSet.SELECT_TF_OPS`. Otherwise, this error is raised:   
```<unknown>:0: error: loc(callsite(callsite(""sequential/dense/BiasAdd@__inference__wrapped_model_1628"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1742"") at ""StatefulPartitionedCall"")): 'tf.BiasAdd' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
	tf.BiasAdd {data_format = ""NHWC"", device = """"}
````

#### Source code not working with 1D tensor
After trying a new configuration, it seems the problem comes from `tf.lite.OpsSet.SELECT_TF_OPS`  
When it is passed to the  converter even the configuration with 1D tensor is not working:
```
model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(units = 1, input_shape=[1])
])
```

**Thanks**

_Note: I will not be able to execute code until august 17th._"
56945,Broken link to model mitigation guide for variable_scope (python),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The link to the model mitigation guide is broken in the documentation for variable_scope ([https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope]).  In the second paragraph of ""Migrate to TF2,"" the current link points to [https://www.tensorflow.org/api_docs/python/tf/compat/v1/www.tensorflow.org/guide/migrate/model_mapping] instead of [https://www.tensorflow.org/guide/migrate/model_mapping].  The link on Line 2154 just needs ""https:"" added to prevent it from being treated as a relative URL.
```


### Standalone code to reproduce the issue

```shell
It's live in the current documentation.  https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope
```


### Relevant log output

_No response_</details>"
56944,build tensorflow-lite pip package arch x86_64 script error,"

### System information


-   **OS Platform and Distribution  Linux Ubuntu 22.04 LTS
-   X86_64
-   **TensorFlow installed from git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
-   
-   **Python version**: Python 3.10.4 
-   **GCC/Compiler version gcc version 11.2.0 (Ubuntu 11.2.0-19ubuntu1)
-   **CUDA/cuDNN version**: n/a
-   **GPU model and memory**: n/a
-   **Exact command to reproduce**:
- user@host:~$ cd tensorflow_src/
user@host:~/tensorflow_src$ python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native
  File ""/home/user/tensorflow_src/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh"", line 71
    armhf)
         ^
SyntaxError: unmatched ')'
user@host:~/tensorflow_src$ 

correcting this with an opening '(' on line 70 results in -

user@host:~/tensorflow_src$ python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native
  File ""/home/user/tensorflow_src/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh"", line 18
    SCRIPT_DIR=""$(cd ""$(dirname ""${BASH_SOURCE[0]}"")"" && pwd)""
                      ^
SyntaxError: invalid syntax
user@host:~/tensorflow_src$ 

trying to build tensorflow-runtime for x86 platform as package is required by pre-configured code originally destined for an Arm platform, rest of the code works ok.
I'm not a bash expert so didn't want to play further....

Grateful for any assistance you can give...
"
56938,Allow tf.GradientTape.batch_jacobian to accept containers of tensors as source,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

binary

### Tensorflow Version

tf 2.8.2

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
At the moment, containers of tensors are supported by `tf.GradientTape.jacobian`, but not `tf.GradientTape.batch_jacobian`.

If this behavior was allowed in `batch_jacobian`, I would expect it to return the same container with tensors equal to the `batch_jacobian` of `target` with respect to each tensor in the `source` container.

Example expected output shown in the colab.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1RTv34VMa-joB45x7xu-9H3pdSTnDAj72?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-7-5c1f96355dc5> in <module>()
      6     pred = my_model(x, training=False)[:, :, 0]
      7 
----> 8 j = tape.batch_jacobian(pred, x)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py in batch_jacobian(self, target, source, unconnected_gradients, parallel_iterations, experimental_use_pfor)
   1287       dim = target_shape.dims[0]
   1288     if not (target_shape.with_rank_at_least(2) and
-> 1289             source.shape.with_rank_at_least(2) and
   1290             dim.is_compatible_with(source.shape[0])):
   1291       raise ValueError(

AttributeError: 'dict' object has no attribute 'shape'
```
</details>"
56937,io_ops.py docstrings for serialize_tensor method generates different output on s390x architecture,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

5.1.1

### GCC/Compiler version

7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Currently [//tensorflow/tools/docs:tf_doctest](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docs/tf_doctest.py) triggers `doctests` in [io_ops.py](https://github.com/tensorflow/tensorflow/blob/21830d8dc775334fa86b2eb1227ff9a96b5e5c8b/tensorflow/python/ops/io_ops.py#L176) which fails on `s390x` 
arch due to values returned during `serialize_tensor` ops. This is primarily due to endianess of the underlying system. I think the current behavior of both `ParseTensor` and `SerializeTensor` ops are correct on `s390x` but the docstrings only account for little-endian results.


x86:
>>> import tensorflow as tf
>>> t = tf.constant(1)
2022-07-28 10:43:31.641256: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
>>> tf.io.serialize_tensor(t)
<tf.Tensor: shape=(), dtype=string, numpy=b'\x08\x03\x12\x00""\x04\x01\x00\x00\x00'>

s390x:
>>> import tensorflow as tf
>>> t = tf.constant(1)
>>> tf.io.serialize_tensor(t)
<tf.Tensor: shape=(), dtype=string, numpy=b'\x08\x03\x12\x00""\x04\x00\x00\x00\x01'>
```

Should a little blurb be added to [serialize_tensor](https://github.com/tensorflow/tensorflow/blob/21830d8dc775334fa86b2eb1227ff9a96b5e5c8b/tensorflow/python/ops/io_ops.py#L156) def to indicate that example docstring results will depend on system architecture on which testcase is executed?

Thanks!
```


### Standalone code to reproduce the issue

```shell
x86:
>>> import tensorflow as tf
>>> t = tf.constant(1)
2022-07-28 10:43:31.641256: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
>>> tf.io.serialize_tensor(t)
<tf.Tensor: shape=(), dtype=string, numpy=b'\x08\x03\x12\x00""\x04\x01\x00\x00\x00'>

s390x:
>>> import tensorflow as tf
>>> t = tf.constant(1)
>>> tf.io.serialize_tensor(t)
<tf.Tensor: shape=(), dtype=string, numpy=b'\x08\x03\x12\x00""\x04\x00\x00\x00\x01'>
```
```


### Relevant log output

```shell
Note that the numpy results are endian specific:
on x86:

<tf.Tensor: shape=(), dtype=string, numpy=b'\x08\x03\x12\x00""\x04\x01\x00\x00\x00'>
```

on s390x:
```
<tf.Tensor: shape=(), dtype=string, numpy=b'\x08\x03\x12\x00""\x04\x00\x00\x00\x01'>
```
```
</details>"
56934,Efficiently get an equal number of per class data point continuously with `tf.data` API. ,"[Info]
```
TensorFlow: 2.6
Environment: Kaggle / Colab
Accelerator: TPU / GPU
```

# Current Behaviour

I am trying to get equal number of sample per class within a batch of data from `tf.data` API. With `batch_size = 14` and `sample_per_class = 3`, I'm expecting to get the following output for `num_classes = 5`:

```yaml
1st batch: [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5]
2nd batch: [5, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5]
```

# Standalone code to reproduce the issue

Here is the standalone code and approach so far.

```python

def dataset_for_class(i):
    i = tf.cast(i, tf.int32)
    return dataset.filter(lambda label: label == i)

num_classes = 1000
num_contiguous_instances = 2

dataset = tf.random.uniform([num_classes], maxval=num_classes, dtype=tf.int32).numpy()
dataset = tf.data.Dataset.from_tensor_slices(dataset)
dataset = tf.data.Dataset.range(num_classes).interleave(
    dataset_for_class,
    cycle_length=num_classes,
    block_length=num_contiguous_instances,
)
dataset = dataset.prefetch(tf.data.AUTOTUNE)
list(dataset.as_numpy_iterator())
[1, 1, 2, 2, 4, 4, 6, 7, 8, 8, 10, 10, 11, 12, ...]
# 6 appears 1 time because of dummy random input.
```
```bash
%%timeit
a = next(iter(dataset))
16.6 ms ± 205 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

The issue arises when `num_classes` gets bigger, for example, **image-net** (1000 classes). It gets too slow to train the model, GPU/TPU both. I'm looking for an efficient solution with `tf.data` API. "
56932,Tensorflow lite for microcontroller. VAR_HANDLE requires resource variables.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

ubuntu20

### Python version

3.9


```shell
A bug happened!
When I run this code
TfLiteTensor* input1 = interpreter1.input(0);

the console return the following text:

VAR_HANDLE requires resource variables. Please create ResourceVariables and pass it to the interpreter.
Node VAR_HANDLE (number 1f) failed to prepare with status 1


I have two tflite models.One of them is OK, but the other cannot run and return this text.
However, I have no problem running both with tensorflow Lite
```


### Standalone code to reproduce the issue

```shell
const tflite::Model* model1 = ::tflite::GetModel(CDEC_1_1600_Zz_tflite);
  if (model1->version() != TFLITE_SCHEMA_VERSION) {
    TF_LITE_REPORT_ERROR(&micro_error_reporter,
                         ""Model provided is schema version %d not equal ""
                         ""to supported version %d.\n"",
                         model1->version(), TFLITE_SCHEMA_VERSION);
  }

  tflite::AllOpsResolver micro_op_resolver;
  // Build an interpreter to run the model with.
  tflite::MicroInterpreter interpreter1(model1, micro_op_resolver, tensor_arena,
                                       tensor_arena_size,
                                       &micro_error_reporter);
  interpreter1.AllocateTensors();

  // Get information about the memory area to use for the model's input.
  TfLiteTensor* input1 = interpreter1.input(0);
  TfLiteTensor* input2 = interpreter1.input(1);
  TfLiteTensor* input3 = interpreter1.input(2);
  TfLiteTensor* input4 = interpreter1.input(3);
  TfLiteTensor* input5 = interpreter1.input(4);
  TfLiteTensor* input6 = interpreter1.input(5);
```


### Relevant log output

```shell
VAR_HANDLE requires resource variables. Please create ResourceVariables and pass it to the interpreter.
Node VAR_HANDLE (number 1f) failed to prepare with status 1
```
</details>"
56931,TEST Failure: //tensorflow/lite/tools/optimize:quantize_model_test ,"### 1. System information
VERSION=""18.04.3 LTS (Bionic Beaver)""
ID=ubuntu

TensorFlow built from latest master https://github.com/tensorflow/tensorflow/commit/113030af3b8add4b4fd94a64d834c273b722e2bf

### 2. Code

`bazel test //tensorflow/lite/tools/optimize:quantize_model_test --test_output=all`

```
[==========] 64 tests from 29 test suites ran. (4 ms total)
[  PASSED  ] 54 tests.
[  FAILED  ] 10 tests, listed below:
[  FAILED  ] QuantizeSoftmaxTest.VerifySoftmaxQuantization
[  FAILED  ] QuantizeAvgPoolTest.VerifyAvgPoolQuantization
[  FAILED  ] QuantizeResourcesModelTest/QuantizeResourcesModelTest.GraphIsFullyQuantized/0, where GetParam() = 8-byte object <09-FF 58-BD 00-00 00-00>
[  FAILED  ] QuantizeResourcesModelTest/QuantizeResourcesModelTest.GraphIsFullyQuantized/1, where GetParam() = 8-byte object <09-FD 58-BD 01-00 00-00>
[  FAILED  ] QuantizeResourcesModelTest/QuantizeResourcesModelTest.GraphIsFullyQuantized/2, where GetParam() = 8-byte object <09-F6 58-BD 02-00 00-00>
[  FAILED  ] QuantizeResourcesModelTest/QuantizeResourcesModelTest.GraphIsFullyQuantized/3, where GetParam() = 8-byte object <09-00 00-00 03-00 00-00>
[  FAILED  ] QuantizeResourcesModelTest/QuantizeResourcesModelTest.GraphIsFullyQuantized/4, where GetParam() = 8-byte object <07-00 00-00 00-00 00-00>
[  FAILED  ] QuantizeResourcesModelTest/QuantizeResourcesModelTest.GraphIsFullyQuantized/5, where GetParam() = 8-byte object <07-5B BE-57 01-00 00-00>
[  FAILED  ] QuantizeResourcesModelTest/QuantizeResourcesModelTest.GraphIsFullyQuantized/6, where GetParam() = 8-byte object <07-FF 58-BD 02-00 00-00>
[  FAILED  ] QuantizeResourcesModelTest/QuantizeResourcesModelTest.GraphIsFullyQuantized/7, where GetParam() = 8-byte object <07-FD 58-BD 03-00 00-00>

10 FAILED TESTS
```

"
56930,Daily builds of opencl tflite delegate are DOA on older android versions,"
### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:  yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android 9 or 10, NOT reproducible on Android 11
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: Tested on Samsung Galaxy S8 and S9 with qualcomm
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: daily build
-   **Python version**: N/A
-   **Bazel version (if compiling from source)**: N/A
-   **GCC/Compiler version (if compiling from source)**: N/A
-   **CUDA/cuDNN version**: N/A
-   **GPU model and memory**: Qualcomm
-   **Exact command to reproduce**: 


### Describe the problem
I am attempting to work around https://github.com/tensorflow/tensorflow/issues/56328 by pulling daily builds as recommended in that issue. However, the daily builds are not working on Android 9 or 10. Please either use the above issue to repro OR you probably can use the tests for https://github.com/tensorflow/tensorflow/commit/35b04bb55d3afdde8ac8f8c4a3ba7a055da4217b, which I believe is the breaking commit. 

Attempting to run gives this error: 
Failed to build program executable - Build program failureBC-src-code:68:7: error: no matching builtin function for call to 'native_exp'
    src = native_exp(src);
          ^~~~~~~~~~
    BC-src-code:68:7: note: candidate function not viable: no known conversion from 'half4' to 'float' for 1st argument; 
    src = native_exp(src);
          ^          ~~~
    BC-src-code:68:7: note: candidate function not viable: no known conversion from 'half4' to 'float  __attribute__((ext_vector_type(2)))' for 1st argument; 
    src = native_exp(src);
          ^          ~~~
    BC-src-code:68:7: note: candidate function not viable: no known conversion from 'half4' to 'float  __attribute__((ext_vector_type(3)))' for 1st argument; 
    src = native_exp(src);
          ^          ~~~
    BC-src-code:68:7: note: candidate function not viable: no known conversion from 'half4' to 'float  __attribute__((ext_vector_type(4)))' for 1st argument; 
    src = native_exp(src);
          ^          ~~~
    1 diagnostic(s) generated.

### Source code / logs
See above
"
56928,Enable NNAPI Crash ,"**System information**
- Xamarin.Android Android 12.0 
- Xamarin Tensorflow Lite 2.6.0.1
- Visual Studio 2022 c#


**Standalone code to reproduce the issue**

Enableing NNAPI (options.SetUseNNAPI(true);)
interpreter = new Interpreter(mappedByteBuffer, options);
https://tfhub.dev/tensorflow/lite-model/ssd_mobilenet_v1/1/metadata/1?lite-format=tflite
That works 


all my trained models Crash 
Work when i don't Enable NNAPI
(I Comment options.SetUseNNAPI(true) but its should be slower)

Even when i take 
http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tar.gz


tflite_directory = '/content/tfliteExport'
pipeline_file = '/content/models/research/deploy/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/pipeline.config'


last_model_path = '/content/models/research/deploy/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/checkpoint'
!python /content/models/research/object_detection/export_tflite_graph_tf2.py \
    --pipeline_config_path={pipeline_file} \
    --trained_checkpoint_dir={last_model_path} \
    --output_directory={tflite_directory}
import cv2

# image_path = '/content/gdrive/MyDrive/Map/foto/Test'
image_path = '/content/img'

def representative_dataset_gen():
    for f_name in os.listdir(image_path):
      file_path = os.path.normpath(os.path.join(image_path, f_name))
      print(file_path)
      img = cv2.imread(file_path)
      img = cv2.resize(img, (640,640))
      img = img / 255.0
      img = np.reshape(img, (1, 640, 640, 3))
      image = img.astype(np.float32)
      yield [image]
converter = tf.lite.TFLiteConverter.from_saved_model('/content/tfliteExport/saved_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.float32
# converter.inference_input_type = tf.uint8
# converter.inference_output_type = tf.uint8
tflite_model = converter.convert()
with open('{}/TFmodelV3.tflite'.format('/content/tfliteExport/saved_model'), 'wb') as w:
    w.write(tflite_model)
print(""tflite convert complete! - {}/TFmodelV3.tflite"".format('/content/newTFLite/saved_model'))
**Any other info / logs**
{Java.Lang.IllegalArgumentException: Internal error: Failed to apply delegate: NN API returned error ANEURALNETWORKS_BAD_DATA at line 1053 while adding operation.

Restored original execution plan after delegate application failure.
  at Java.Interop.JniEnvironment+InstanceMethods.CallNonvirtualVoidMethod (Java.Interop.JniObjectReference instance, Java.Interop.JniObjectReference type, Java.Interop.JniMethodInfo method, Java.Interop.JniArgumentValue* args) [0x00088] in /Users/runner/work/1/s/xamarin-android/external/Java.Interop/src/Java.Interop/Java.Interop/JniEnvironment.g.cs:12324 
  at Java.Interop.JniPeerMembers+JniInstanceMethods.FinishCreateInstance (System.String constructorSignature, Java.Interop.IJavaPeerable self, Java.Interop.JniArgumentValue* parameters) [0x0003e] in /Users/runner/work/1/s/xamarin-android/external/Java.Interop/src/Java.Interop/Java.Interop/JniPeerMembers.JniInstanceMethods.cs:142 
  at Xamarin.TensorFlow.Lite.Interpreter..ctor (Java.Nio.ByteBuffer byteBuffer, Xamarin.TensorFlow.Lite.Interpreter+Options options) [0x0009d] in <d3ecc302b2d54bbb87b0b5080c9dc98b>:0 
  at CameraTF.TensorflowLiteService.Initialize () [0x00017] in X:\Snelle TF Xam\CameraTF-master\src\CameraTF\AR\TensorflowLiteService.cs:62 
  --- End of managed Java.Lang.IllegalArgumentException stack trace ---
java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: NN API returned error ANEURALNETWORKS_BAD_DATA at line 1053 while adding operation.

Restored original execution plan after delegate application failure.
	at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)
	at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:486)
	at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:88)
	at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:66)
	at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:247)
	at crc649b31f1000968ddb3.MainActivity.n_onCreate(Native Method)
	at crc649b31f1000968ddb3.MainActivity.onCreate(MainActivity.java:31)
	at android.app.Activity.performCreate(Activity.java:7994)
	at android.app.Activity.performCreate(Activity.java:7978)
	at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1309)
	at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3422)
	at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3601)
	at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:85)
	at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135)
	at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95)
	at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2066)
	at android.os.Handler.dispatchMessage(Handler.java:106)
	at android.os.Looper.loop(Looper.java:223)
	at android.app.ActivityThread.main(ActivityThread.java:7656)
	at java.lang.reflect.Method.invoke(Native Method)
	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:592)
	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:947)
}
"
56927,Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf-nightly

### Custom Code

Yes

### OS Platform and Distribution

WSL Ubuntu

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice

The error appears when setting groups > 1 in tf.keras.layers.Conv2D with tf-nightly.

The error does not appear in TensorFlow 2.8.

Seems the previous TensorFlow build is packed with libdevice.
```


### Standalone code to reproduce the issue

```shell
tf.keras.layers.Conv2D(
            512, 
            1,
            use_bias=False
            groups=5,
            )
```


### Relevant log output

```shell
2022-07-28 01:48:13.943787: W tensorflow/compiler/xla/service/gpu/nvptx_helper.cc:56] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.
Searched for CUDA in the following directories:
  ./cuda_sdk_lib
  /usr/local/cuda-11.2
  /usr/local/cuda
  .
You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-07-28 01:48:13.986332: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2022-07-28 01:48:13.986408: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version
2022-07-28 01:48:14.018602: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2022-07-28 01:48:14.018778: F tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:453] ptxas returned an error during compilation of ptx to sass: 'INTERNAL: Failed to launch ptxas'  If the error message indicates that a file could not be written, please verify that sufficient filesystem space is provided.
Fatal Python error: Aborted

Thread 0x00007f3abe7fc640 (most recent call first):
  File ""/home/edwardyehuang/miniconda3/envs/tf-nightly/lib/python3.9/threading.py"", line 316 in wait
  File ""/home/edwardyehuang/miniconda3/envs/tf-nightly/lib/python3.9/threading.py"", line 574 in wait
  File ""/home/edwardyehuang/.vscode-server/extensions/ms-python.python-2022.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/pydevd.py"", line 257 in _on_run
  File ""/home/edwardyehuang/.vscode-server/extensions/ms-python.python-2022.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_daemon_thread.py"", line 49 in run
  File ""/home/edwardyehuang/miniconda3/envs/tf-nightly/lib/python3.9/threading.py"", line 973 in _bootstrap_inner
  File ""/home/edwardyehuang/miniconda3/envs/tf-nightly/lib/python3.9/threading.py"", line 930 in _bootstrap

Thread 0x00007f3abeffd640 (most recent call first):
  File ""/home/edwardyehuang/miniconda3/envs/tf-nightly/lib/python3.9/threading.py"", line 316 in wait
  File ""/home/edwardyehuang/miniconda3/envs/tf-nightly/lib/python3.9/threading.py"", line 574 in wait
  File ""/home/edwardyehuang/.vscode-server/extensions/ms-python.python-2022.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/pydevd.py"", line 211 in _on_run
  File ""/home/edwardyehuang/.vscode-server/extensions/ms-python.python-2022.10.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_daemon_thread.py"", line 49 in run
  File ""/home/edwardyehuang/miniconda3/envs/tf-nightly/lib/python3.9/threading.py"", line 973 in _bootstrap_inner
  File ""/home/edwardyehuang/miniconda3/envs/tf-nightly/lib/python3.9/threading.py"", line 930 in _bootstrap
```
</details>"
56925,Failure to use selectively build TFLite frameworks on iOS,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.6

### Custom Code

No

### OS Platform and Distribution

iOS 14.1 - target build, MacOSX 10.15.7 and XCode 12.4 for building

### Mobile device

Apple iPad, iOS 15.3

### Python version

3.9

### Bazel version

3.4.0

### GCC/Compiler version

Apple clang version 12.0.0 (clang-1200.0.32.29)

### CUDA/cuDNN version

no CUDA used

### GPU model and memory

no GPU

### Current Behaviour?

```shell
I've successfully built TFLite frameworks for my iOS project, following the ""Selectively build"" instructions.

bash tensorflow/lite/ios/build_frameworks.sh \
  --input_models=modelX.tflite,modelY.tflite \
  --target_archs=arm64

After that, I added both frameworks (TensorFlowLiteSelectTfOps and TensorFlowLiteC) to my XCode project (using Pods) and updated ""Other linker options"" with ""-force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps"". 

Project failed to build on the linking stage.

Besides that, the custom-built TensorFlowLiteSelectTfOps.framework binary weights about 1Gb, that is too much comparing with the same custom build (with the same models) for Android which size was only 6 Mb.

Seems like the Bazel build  script for ""selective"" build with models works wrong and does not build static TensorFlowLiteSelectTfOps library as expected.

Please help!
```


### Standalone code to reproduce the issue

```shell
No custom code. All builds were made using Bazel, according the instructions https://www.tensorflow.org/lite/guide/build_ios#selectively_build_tflite_frameworks
```


### Relevant log output

```shell
Bazel build was successful. XCode build fails with 
with linker error:

Undefined symbols for architecture arm64:
  ""google::protobuf::DoubleValue::MergeFrom(google::protobuf::DoubleValue const&)"", referenced from:
      tensorflow::MetricEntry::MergeFrom(tensorflow::MetricEntry const&) in TensorFlowLiteSelectTfOps(test_log.pb.o)
  ""google::protobuf::DoubleValue::InternalSerializeWithCachedSizesToArray(unsigned char*) const"", referenced from:
      tensorflow::MetricEntry::InternalSerializeWithCachedSizesToArray(unsigned char*) const in TensorFlowLiteSelectTfOps(test_log.pb.o)
  ""google::protobuf::DoubleValue* google::protobuf::Arena::CreateMaybeMessage<google::protobuf::DoubleValue>(google::protobuf::Arena*)"", referenced from:
      tensorflow::MetricEntry::MergePartialFromCodedStream(google::protobuf::io::CodedInputStream*) in TensorFlowLiteSelectTfOps(test_log.pb.o)
      tensorflow::MetricEntry::MergeFrom(tensorflow::MetricEntry const&) in TensorFlowLiteSelectTfOps(test_log.pb.o)
 
**** AND 100+ same signature errors with C++ namespaces referencing google::protobuf library's functions. ****
```
</details>"
56923,Xcode 13.4.1 - No such module 'TensorFlowLiteCCoreML'  ,"<img width=""457"" alt=""Screenshot 2022-07-27 at 5 14 42 PM"" src=""https://user-images.githubusercontent.com/14552882/181239038-d95eff4e-be92-4021-9447-9e688efa3484.png"">
<img width=""779"" alt=""Screenshot 2022-07-27 at 5 14 54 PM"" src=""https://user-images.githubusercontent.com/14552882/181239058-f630c89d-ac1e-4000-ad67-422fd76e2e1b.png"">

No such module 'TensorFlowLiteCCoreML' getting this error. 
xcode Version 13.4.1 (13F100)
Tried pod install and opening .xcworkspace only. 

Please note that i am getting this error only when i archive the build. Any help would be appreciated!

Thanks,
Payal U."
56922,TFLite C API on Android Studio,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: No
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**:2.9.0
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

### Describe the problem

I'm currently run tflite model on Android app by C API, I download version 2.9.0 shared library from [TensorFlow Lite AAR hosted at MavenCentral](https://search.maven.org/artifact/org.tensorflow/tensorflow/tensorflow-lite) and include requied header files as [DOC](https://github.com/tensorflow/tensorflow/blob/r2.9/tensorflow/lite/g3doc/guide/android.md) mentioned.

Load tflite model and create interpreter works perfectly. And `TfLiteInterpreterGetSignatureCount` return number is also correct.

But `TfLiteInterpreterGetSignatureKey` shows **undefined reference to `TfLiteInterpreterGetSignatureKey`**.

Does C API support these two function currently?

### Source code / logs

_native-lib.cpp_
```
extern ""C"" JNIEXPORT jstring JNICALL
Java_com_example_ainl_MainActivity_loadTfliteModelJNI(
        JNIEnv* env,
        jobject /* this */) {


    TfLiteModel* model = TfLiteModelCreateFromFile(""/data/data/com.example.ainl/train.tflite"");

    TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, NULL);
    TfLiteInterpreterAllocateTensors(interpreter);

    const char* tflite_version = TfLiteVersion();
    int num_signature = TfLiteInterpreterGetSignatureCount(interpreter);

    /** Get each signature key */
    const char* signature_key = TfLiteInterpreterGetSignatureKey(interpreter, 0);
    /** Get each signature key input number */
    TfLiteSignatureRunner* runner = TfLiteInterpreterGetSignatureRunner(interpreter, ""train"");
    int num_input = TfLiteSignatureRunnerGetInputCount(runner);

    char status_string[100];
    sprintf(status_string, ""--- tflite version: %s ---\n"", tflite_version);
    sprintf(status_string + strlen(status_string), ""#Signature: %d\n"", num_signature);

    sprintf(status_string + strlen(status_string), ""0th signature key: %s\n"", signature_key);
    sprintf(status_string + strlen(status_string), ""#input of key [train]: %d\n"", num_input);

    return env->NewStringUTF(status_string);
}
```
"
56921,"Tensorflow, llvmlite, mac M1 and poetry","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Apple Mac OS 12.5

### Mobile device

_No response_

### Python version

3.9.2

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am trying to add tensorflow to my (cross-platform) project. 
I am using poetry and conda.

The ""bug"" apparently is due to the missing pre-build wheel for m1 architecture of the llvmlite dependency.

**Note**: I cannot add `tensorflow-macos` to the project as it has to be cross-platform.
```


### Standalone code to reproduce the issue

```shell
simply I run `poetry add tensorflow` and I get the following message


note: This error originates from a subprocess, and is likely not a problem with pip.
  error: legacy-install-failure
  
  × Encountered error while trying to install package.
  ╰─> llvmlite
  
  note: This is an issue with the package mentioned above, not pip.
  hint: See above for output from the failure.
```


### Relevant log output

```shell
note: This error originates from a subprocess, and is likely not a problem with pip.
  error: legacy-install-failure
  
  × Encountered error while trying to install package.
  ╰─> llvmlite
  
  note: This is an issue with the package mentioned above, not pip.
  hint: See above for output from the failure.
```
</details>"
56920,Load Lambda,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

windows

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Hi,
my code like this:
The functions:
    def stftLayer(self, x, mode='mag_pha'):

        frames = tf.signal.frame(x, self.block_len, self.block_shift)
        if self.win is not None:
            frames = self.win * frames
        stft_dat = tf.signal.rfft(frames)
        # calculating magnitude and phase from the complex signal
        output_list = []
        if mode == 'mag_pha':
            mag = tf.math.abs(stft_dat)
            phase = tf.math.angle(stft_dat)
            output_list = [mag, phase]
        elif mode == 'real_imag':
            real = tf.math.real(stft_dat)
            imag = tf.math.imag(stft_dat)
            output_list = [real, imag]
            # returning magnitude and phase as list
        return output_list

In model:
...
real,imag = Lambda(self.stftLayer,arguments = {'mode':'real_imag'})(time_dat)
...

Where the error occurred：

model = tf.keras.models.load_model(modelparh,custom_objects={""stftLayer"":stftLayer})
```


### Standalone code to reproduce the issue

```shell
When i want to load the model,have a issus:

TypeError: Exception encountered when calling layer ""lambda"" (type Lambda).

stftLayer() missing 1 required positional argument: 'x'

Call arguments received by layer ""lambda"" (type Lambda):
  • inputs=tf.Tensor(shape=(8, None), dtype=float32)
  • mask=None
  • training=None
```


### Relevant log output

_No response_</details>"
56917,How to register an op of conv?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

tf 2.5

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I wanna to registe an REIGSTER_OP(Conv2D_New).
But the `c->set_output(0,c->Matrix())` only support two dimensions.
the conv need four dimensions, i don't know how to do it
```


### Standalone code to reproduce the issue

```shell
c->set_output(0,c->Matrix());
```


### Relevant log output

_No response_</details>"
56916,How to use feed_dict in MonitoredTrainingSession,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Performance

### Source

source

### Tensorflow Version

tf 1.15

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 18

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I saw this link:https://github.com/tensorflow/tensorflow/issues/19657#issuecomment-396385448
Because tf.data.Dataset.from_tensor_slices stores the entire dataset in the TF graph, it is recommended to use placeholder and feed_dict. But I use MonitoredTrainingSession not Session, and no initialization operations can be performed after MonitoredTrainingSession , So I used scaffold to initialize the data. But an error is reported: 'GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element.'

How to use feed_dict in MonitoredTrainingSession and initialize the data correctly?
```


### Standalone code to reproduce the issue

```shell
from cProfile import label
from pyexpat import features
import tensorflow as tf
import numpy as np
import pandas as pd
import time
import collections
import random
datas = np.random.rand(16000000)
features = tf.placeholder(tf.float32)
feed_dict = {features:datas}
dataset = tf.data.Dataset.from_tensor_slices(features)
dataset = dataset.batch(100)
dataset = dataset.prefetch(2)
iterator = dataset.make_initializable_iterator()
next_element = iterator.get_next()
global_step = tf.train.get_or_create_global_step()
sess_config = tf.ConfigProto()
scaffold = tf.train.Scaffold(
        init_op=iterator.initializer,
        local_init_op=tf.local_variables_initializer(),
        init_feed_dict=feed_dict,
       )
stop_hook = tf.train.StopAtStepHook(last_step=1000)
hooks = []
hooks.append(stop_hook)
tm = time.time()
with tf.train.MonitoredTrainingSession(
            scaffold=scaffold,
            hooks=hooks,
            checkpoint_dir = './result/ddd',
            save_checkpoint_steps=1000,
            summary_dir= None,
            save_summaries_steps=1000,
            config=sess_config) as sess:
    print(f'cost session create:{time.time() - tm:.8f}s')
    t2 = time.time()
    # sess.run(iterator.initializer,feed_dict={data: tuple([datas]*40)})
    for i in range(1,100):
        sess.run(next_element)
    print(f'cost run time:{time.time() - t2:.8f}s')
    print('done')
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.FailedPreconditionError: GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element.
         [[node IteratorGetNext (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]
```
</details>"
56912,Differentiate a tuple with `tape.jacobian`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

MacOs

### Mobile device

-

### Python version

3.9

### Bazel version

-

### GCC/Compiler version

-

### CUDA/cuDNN version

-

### GPU model and memory

-

### Current Behaviour?

```shell
I expect to able to differentiate a function returning a tuple of TensorFlow tensors, but unfortunately it is not possible because tuples have no shape. Could you provide me a work around for this? Maybe something with ragged tensors is possible here?
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

def circuit(params):
    return params[0] ** 2, params[1::] ** 3

x = tf.Variable([0.1, 0.2, 0.3])

with tf.GradientTape() as tape:
    out = circuit(x)

jac = tape.jacobian(out, x)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/Users/.../test.py"", line 14, in <module>
    jac = tape.jacobian(out, x)
  File ""/Users/.../lib/python3.9/site-packages/tensorflow/python/eager/backprop.py"", line 1183, in jacobian
    target_static_shape = target.shape
AttributeError: 'tuple' object has no attribute 'shape'
```
</details>"
56910,unexpected value of binary_crossentropy loss function in network with ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8.2

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Basically I am seeing binary_crossentropy loss evaluate to a completely unexpected number when using it on a network that isn't outputting just a single logit. 

Below is a simple example of a network with two outputs which will correspond to the probability of each class. I wanted to check the calculated loss was what I would expect it to be. All this works fine for the categorical_crossentropy loss function (which is the more natural choice in this case, I accept), but given the code lets you do binary_crossentropy here too I wanted to understand what it's doing. But the meaning of the value seems hard to decipher. Is there a bug or other explanation for the difference in values???
```


### Standalone code to reproduce the issue

```shell
from tensorflow import keras
import numpy as np

loss_func = keras.losses.BinaryCrossentropy()
nn = keras.Sequential([
  keras.layers.Dense(2**8, input_shape=(1,), activation='relu'),
  keras.layers.Dense(2, activation='softmax')
])
nn.compile(loss=loss_func,optimizer='adam')
train_x = np.array([0.4])
train_y = np.array([[0,1]])
print(nn.predict(train_x))
print(""Evaluated loss = "",nn.evaluate(train_x,train_y))
print(""Function loss = "",loss_func(train_y,nn.predict(train_x)).numpy())
print(""Manual loss = "",np.average( -train_y*np.log(nn.predict(train_x)) -(1-train_y)*np.log(1. - nn.predict(train_x)) ))
```
```


### Relevant log output

```shell
[[0.5152152  0.48478484]]
1/1 [==============================] - 0s 92ms/step - loss: 0.7085
Evaluated loss =  0.7084982991218567
Function loss =  0.72405
Manual loss =  0.7240501642227173


The evaluated loss value makes no sense compared to the function value and the manually calculated value (which is just equal to -log(0.48478484) as it should be
```
</details>"
56909,"raise ValueError(f""No gradients provided for any variable: {variable}.","```
all_vars = [var for var in tf.compat.v1.trainable_variables()
            if 'g_' in var.name]
all_opt = tf.optimizers.Adam(learning_rate=0.0001).minimize(
    lossDict[""all_loss""], var_list=all_vars, tape=tf.GradientTape())
for var in tf.trainable_variables():
    print(""Listing trainable variables ... "", var)

saver = tf.train.Saver(max_to_keep=20)
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)
sess.run(tf.global_variables_initializer())
var_restore = [v for v in tf.trainable_variables()]
saver_restore = tf.train.Saver(var_restore)
ckpt = tf.train.get_checkpoint_state('result/'+task)
```

this is my code and i am facing no gradient issue..."
56907,TF-TRT failed to convert the key of the savedModel model signature_def inputs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf2.4.1

### Custom Code

Yes

### OS Platform and Distribution

Ubutun18.04

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
A bug happened

When using tf-trt for model conversion in savedModel format, the model produced by trt changes the input key value of signature_def. The original key value contains uppercase and lowercase names. After trt-tf is output, it becomes all lowercase, and the model cannot be run.

use command： saved_model_cli show

before：

MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:
signature_def['__saved_model_init_op']:
  The given SavedModel SignatureDef contains the following input(s):
  The given SavedModel SignatureDef contains the following output(s):
    outputs['__saved_model_init_op'] tensor_info:
        dtype: DT_INVALID
        shape: unknown_rank
        name: NoOp
  Method name is: 
signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['answerWilsonScore'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1)
        name: serving_default_answerWilsonScore:0

after tf-trt：

MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:
signature_def['__saved_model_init_op']:
  The given SavedModel SignatureDef contains the following input(s):
  The given SavedModel SignatureDef contains the following output(s):
    outputs['__saved_model_init_op'] tensor_info:
        dtype: DT_INVALID
        shape: unknown_rank
        name: NoOp
  Method name is: 
signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['answerwilsonscore'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1)
        name: serving_default_answerwilsonscore:0
```


### Standalone code to reproduce the issue

```shell
https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#worflow-with-savedmodel 

from tensorflow.python.compiler.tensorrt import trt_convert as trt
converter = trt.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir)
converter.convert()
converter.save(output_saved_model_dir)
```


### Relevant log output

_No response_</details>"
56906,[C API] Index error in ShapeInference C API,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell

**Error**: When using shape inference c api, `TF_ShapeInferenceContextGetInput` and `TF_ShapeInferenceContextSetOutput`, 
even the index is not out of range, there is an out of range error. 

**Cause**: Checking the source code, we can see the condition `0 < i || i >= cc_ctx->num_inputs()`, 
which is wrong obviously.

**Source code**: Here is source code [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/ops.cc#L146)

```


### Standalone code to reproduce the issue

```shell
No standalone test needed
```


### Relevant log output

```shell
Check failed: TF_OK == TF_GetCode(status) (0 vs. 3)
```
</details>"
56904,"`Error in PredictCost() for the op: op: ""CropAndResize""` when using the `tf.image.crop_and_resize` op","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

RedHat Linux Enterprise 8.4

### Mobile device

_No response_

### Python version

3.9.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA: 11.6

### GPU model and memory

V100, 32GB

### Current Behaviour?


I wanted to code the equivalent of [RandomResizedCrop](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html) from torchvision.
I used the model from [this official keras tutorial](https://keras.io/examples/vision/nnclr/#random-resized-crops) and integrated it in my data pipeline.
When using it, I got the following warning:

```
2022-07-26 14:10:05.665573: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: 40 } dim { size: 40 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 16 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 16 } dim { size: 16 } dim { size: 3 } } }
2022-07-26 14:10:05.815003: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
```

At first, I didn't care because my code was running fine, but I had a CPU memory error after about 9 epochs of ImageNet training. This suggests that there is somehow a memory leak during training.

The same behaviour (although on GPU from that I understand) was also observed in this [SO question](https://stackoverflow.com/q/72642906/4332585).



### Standalone code to reproduce the issue


Unfortunately, the warning does not appear on Colab, but here is a link with the appropriate minimal example anyway: https://colab.research.google.com/drive/1QHa4kxPLfCjkfDvmwv9wj5yq19za5SpA?usp=sharing

However, locally (on my laptop without GPU) and on my server, the warning is thrown.

The full code is the following:

```python

import tensorflow as tf

class RandomResizedCrop(tf.keras.layers.Layer):
    # taken from
    # https://keras.io/examples/vision/nnclr/#random-resized-crops
    def __init__(self, scale, ratio, crop_shape):
        super(RandomResizedCrop, self).__init__()
        self.scale = scale
        self.log_ratio = (tf.math.log(ratio[0]), tf.math.log(ratio[1]))
        self.crop_shape = crop_shape

    def call(self, images):
        batch_size = tf.shape(images)[0]

        random_scales = tf.random.uniform(
            (batch_size,),
            self.scale[0],
            self.scale[1]
        )
        random_ratios = tf.exp(tf.random.uniform(
            (batch_size,),
            self.log_ratio[0],
            self.log_ratio[1]
        ))

        new_heights = tf.clip_by_value(
            tf.sqrt(random_scales / random_ratios),
            0,
            1,
        )
        new_widths = tf.clip_by_value(
            tf.sqrt(random_scales * random_ratios),
            0,
            1,
        )
        height_offsets = tf.random.uniform(
            (batch_size,),
            0,
            1 - new_heights,
        )
        width_offsets = tf.random.uniform(
            (batch_size,),
            0,
            1 - new_widths,
        )

        bounding_boxes = tf.stack(
            [
                height_offsets,
                width_offsets,
                height_offsets + new_heights,
                width_offsets + new_widths,
            ],
            axis=1,
        )
        images = tf.image.crop_and_resize(
            images,
            bounding_boxes,
            tf.range(batch_size),
            self.crop_shape,
        )
        return images

import tensorflow_datasets as tfds


ds = tfds.load('cifar10', split='train', as_supervised=True)
image_width = 16
crop = RandomResizedCrop(
    scale=(0.08, 1.0),
    ratio=(0.75, 1.33),
    crop_shape=(image_width, image_width),
)
data_aug_list = [
    tf.keras.layers.ZeroPadding2D(padding=4),
    crop,
]
data_aug_layer = tf.keras.models.Sequential(data_aug_list)
ds = ds.map(
  lambda x, y: (data_aug_layer(x[None], training=True)[0], y),
  num_parallel_calls=tf.data.experimental.AUTOTUNE,
)
ds = ds.shuffle(
  buffer_size=1000,  # For now a hardcoded value
  reshuffle_each_iteration=True,
).batch(
  32,
  num_parallel_calls=tf.data.experimental.AUTOTUNE,
)
ds = ds.prefetch(
  buffer_size=tf.data.experimental.AUTOTUNE,
)

res = next(iter(ds))  # warning is thrown here
```


### Relevant log output

```shell
2022-07-25 15:28:12.176012: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 16:47:53.335022: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 18:07:19.352253: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 19:26:43.163855: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 20:46:07.180534: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 22:05:27.767552: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 23:24:49.840029: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-26 00:44:13.601504: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-26 02:03:37.488186: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-26 03:22:57.948498: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-26 04:42:09.083767: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=1562147.0. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: r10i0n2: task 0: Out Of Memory
srun: launch/slurm: _step_signal: Terminating StepId=1562147.0
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=1562147.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.
```
</details>"
56903,tflite_model_maker.object_detector dies with sigkill during import,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.2

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04 LTS

### Mobile device

_No response_

### Python version

3.8.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuda_11.7.r11.7

### GPU model and memory

NVIDIA TITAN X rev a1

### Current Behaviour?

```shell
I am attempting to follow the directions here (https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_efficientdet_model_maker_tf2.ipynb#scrollTo=license) to use tflite model_maker to retrain a network. I have created a virtual environment as shown below. When I import the object_detector from tflite_model_maker it pauses for a bit and then I get a sigkill. (Process finished with exit code 132 (interrupted by signal 4: SIGILL). I traced this all the way down into the file scann/scann_ops/py/scan_ops.py. It calls tf.load_op_library for the file cc/_scann_ops.so. When it tries to load this library it dies with the sigkill.
```


### Standalone code to reproduce the issue

```shell
cd my_folder
pyenv local 3.8.11
python -m venv venv --prompt defender
source venv/bin/activate
python3 -m pip install -U pip wheel setuptools
python3 -m pip install --upgrade pip

cd ..
git clone https://github.com/tensorflow/examples
cd examples/tensorflow_examples/lite/model_maker/pip_package
python3 -m pip install -e .

I then try and and run the following code (I have stripped out everything but the import that is causing the issue), I get the sigkill.

from tflite_model_maker import object_detector

output is:
Process finished with exit code 132 (interrupted by signal 4: SIGILL)
```


### Relevant log output

```shell
I am not sure what logs you would need here. If you let me know how to generate any logs I would be happy to provide them.
```
</details>"
56899,tflite QuantizationDebugger seem haven't support tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,"tflite QuantizationDebugger seem haven't support tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8

I try directly use the mlir_quantize(inference_type=dypes.int16), but it will directly convert full quantized model."
56898,Rsqrt doesn't support for tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
56897,How to build tensorflow-lite-c enabling flex option using CMake,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

Ubuntu 20.04.4 LTS

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

gcc 9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'd like to use the tensorflow-lite model created in python in C++.
But, I'm facing a problem where I can't call the relu activation included in the tflite file on C++.

The python code I wrote to create the tflite model is as follows.
I am using tensorflow 2.9.1 Version in Ubuntu environment by directly building using CMake.

By the way, when I try to use relu for keras activation, I get the below error.
I did a google search and found that it was caused by lack of flex library.
However, I couldn't find a way to build using CMake instead of bazel in the Documentation below.
- https://www.tensorflow.org/lite/guide/ops_select#cc

Is there currently any way to support this? or i can use relu activation
or Is there any other way? (using not keras relu but another relu activation)
```


### Standalone code to reproduce the issue

```shell
class PredictedDestination(tf.Module):
    def __init__(self):
        self.model = tf.keras.Sequential([
            tf.keras.layers.Dense(32, input_shape=(11, ), name='input'),
             tf.keras.layers.Dense(16, activation=tf.nn.relu, name='dense_1'),
             tf.keras.layers.Dense(8, activation=tf.nn.relu, name='dense_2'),
             tf.keras.layers.Dense(4, activation=tf.nn.relu, name='dense_3'),
            tf.keras.layers.Dense(2),
        ])
    
        self.model.compile(
            optimizer='sgd',
            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True))

    # The `train` function takes a batch of input images and labels.
    @tf.function(input_signature=[
      tf.TensorSpec([None, 11], tf.float32),
      tf.TensorSpec([None, 2], tf.float32),
    ])
    def train(self, x, y):
        epochs = 100
        for i in range(epochs):
            with tf.GradientTape() as tape:
              prediction = self.model(x)
              loss = self.model.loss(y, prediction)
            gradients = tape.gradient(loss, self.model.trainable_variables)
            self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
            result = {""loss"": loss}
        return result

    @tf.function(input_signature=[
      tf.TensorSpec([None, 11], tf.float32),
    ])
    def predict(self, x): 
        logits = self.model(x)
        probabilities = tf.nn.softmax(logits, axis=-1)
        print(probabilities)
        print(logits)
        return {
            ""output"": probabilities,
            ""logits"": logits
        }

model = PredictedDestination()

SAVED_MODEL_DIR = ""predicted_destination_model""

tf.saved_model.save(
    model,
    SAVED_MODEL_DIR,
    signatures={
        'train':
            model.train.get_concrete_function(),
        'predict':
            model.predict.get_concrete_function(),
    })

converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]
converter.experimental_enable_resource_variables = True
tflite_model = converter.convert()

open('predicted_destination.tflite', 'wb').write(tflite_model)
```


### Relevant log output

```shell
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
```
</details>"
56895,Inconsistent CUDA toolkit path: /usr vs /usr/lib,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.x (tried on 2.9 and 2.4)

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8 (probably not relevant)

### Bazel version

3.7.2, 4.2.2 (probably not relevant)

### GCC/Compiler version

9.4 .0 (probably not relevant)

### CUDA/cuDNN version

CUDA 11.7, cuDNN 8

### GPU model and memory

RTX 3060 (not relevant)

### Current Behaviour?

```shell
I occasionally need to build tensorflow_cc from source so that I can use tensorflow inference in a C++ application. I have done this several times over the last ~5 years using https://github.com/FloopCZ/tensorflow_cc to do the build.

In my most recent attempt to build I ran into this problem while configuring:


Inconsistent CUDA toolkit path: /usr vs /usr/lib
Asking for detailed CUDA configuration...
```

The exception that causes this problem is raised in `tensorflow/third_party/gpus/find_cuda_config.py` at https://github.com/tensorflow/custom-op/blob/e0e681164dc58580987971464c252529252b89e6/gpu/find_cuda_config.py#L289

There is a TODO a few lines earier:

# XLA requires the toolkit path to find ptxas and libdevice.
# TODO(csigg): pass in both directories instead.

I suspect that resolving this TODO would fix the problem.

But what is the problem, exactly?

It seems to me that there are multiple variations in how CUDA + cuDNN can be installed. Some of these variations will trigger the problem because the exact locations of CUDA/cuDNN components do not match those expected.

While searching for clues to how to resolve this problem, I came across this closed issue: https://github.com/tensorflow/tensorflow/issues/40202

In that issue the reporter wrote ""A more reliable workaround is to install the cuda toolkit using Nvidia's .run file installer."", which was apparently enough to close the issue.

I'd like to request that this issue be fully resolved by addressing the TODO.

In my case, I am no longer fully blocked by this issue, but I had hoped to be able to use the Lambda Stack to install all of the necessary dependencies. The stack installs CUDA/cuDNN in a layout that triggers this problem.
```


### Standalone code to reproduce the issue

```shell
I believe this may be the most succinct description I can provide for how to reproduce the problem:

1. Start with a fresh install of Ubuntu 20.04.
2. Install Lambda Stack (https://lambdalabs.com/lambda-stack-deep-learning-software)
3. git clone https://github.com/FloopCZ/tensorflow_cc.git 
4. Follow the instructions https://github.com/FloopCZ/tensorflow_cc#3-build-and-install-the-library 

You should see the ""Inconsistent ..."" error when running `make`.

Notes:

1. I saw the error above when building Tensorflow 2.9, which is the current default specified by https://github.com/FloopCZ/tensorflow_cc/blob/master/tensorflow_cc/PROJECT_VERSION. However, I also tried Tensorflow 2.4.3 with the same result.
```


### Relevant log output

_No response_</details>"
56894,SlurmClusterResolver cannot be configured to use no GPUs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux custom cray distribution

### Mobile device

N/A

### Python version

Conda evn installed python 3.10

### Bazel version

N/A

### GCC/Compiler version

N/A

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?

```shell
Attempting to use zero GPU's causes an error



Traceback (most recent call last):
  File ""test.py"", line 17, in <module>
    strategy = tensorflow.distribute.MultiWorkerMirroredStrategy(cluster_resolver=cluster_resolver)
  File ""/global/homes/m/markcian/.conda/envs/parallel_tensorflow/lib/python3.10/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 189, in __init__
    communication_options=communication_options))
  File ""/global/homes/m/markcian/.conda/envs/parallel_tensorflow/lib/python3.10/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 327, in __init__
    self._initialize_strategy(self._cluster_resolver)
  File ""/global/homes/m/markcian/.conda/envs/parallel_tensorflow/lib/python3.10/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 338, in _initialize_strategy
    if cluster_resolver.cluster_spec().as_dict():
  File ""/global/homes/m/markcian/.conda/envs/parallel_tensorflow/lib/python3.10/site-packages/tensorflow/python/distribute/cluster_resolver/slurm_cluster_resolver.py"", line 327, in cluster_spec
    range(num_tasks), range(0, self._gpus_per_node, self._gpus_per_task)):
ValueError: range() arg 3 must not be zero
```


If I attempt to set gpus_per_task to 1 to fix this error, 

```
Traceback (most recent call last):
  File ""test.py"", line 6, in <module>
    auto_set_gpu=False)
  File ""/global/homes/m/markcian/.conda/envs/parallel_tensorflow/lib/python3.10/site-packages/tensorflow/python/distribute/cluster_resolver/slurm_cluster_resolver.py"", line 271, in __init__
    raise RuntimeError('Requested more GPUs per node then available.')
RuntimeError: Requested more GPUs per node then available.
```

The expected behavior is that I would be able to configure tensorflow to not use GPUs and that it won't even attempt to use them if there are no GPUs on the node especially when installed with `pip install tensorflow_cpu`.
```


### Standalone code to reproduce the issue

```shell
import tensorflow

cluster_resolver = tensorflow.distribute.cluster_resolver.SlurmClusterResolver(gpus_per_node=0,
                                                            gpus_per_task=0,
                                                            tasks_per_node=1,
                                                            auto_set_gpu=False)
```


### Relevant log output

```shell
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
v2.9.0-18-gd8ce9f9c301 2.9.1
```
</details>"
56892,Test failures in //tensorflow/java module,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Others

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

Python 3.8.10

### Bazel version

5.1.1

### GCC/Compiler version

9.4.0

### CUDA/cuDNN version

NA

### GPU model and memory

NA

### Current Behaviour?

```shell
Tests fail when run locally.


Tests should pass. 
Checked a few CI builds and observed only //tensorflow/java:source_writer_test is executed. Could someone please share thoughts on this?
```


### Standalone code to reproduce the issue

```shell
bazel --host_jvm_args=""-Xms1024m"" --host_jvm_args=""-Xmx2048m"" test //tensorflow/java/...
```


### Relevant log output

```shell
java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: linux, architecture: x86_64.
.
.
java.lang.NoClassDefFoundError: Could not initialize class org.tensorflow.EagerSession
```
</details>"
56890,NaN values in tf.reduce_mean + float16 + CPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.2

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.7.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When using CPU + fp16, tf.reduce_mean produces NaN-s.
When reduce_mean placed on GPU everything works.
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1sEUNVv0knkg65JNrsHMcbdlCJGs8U8j9?usp=sharing
```


### Relevant log output

_No response_</details>"
56889,AttributeError: module 'keras.layers' has no attribute 'experimental': M1 Mac Tensorflow Metal,"I am trying to run a TensorFlow model on M1 Mac with the following settings:

1. MacBook Pro M1
2. macOS 12.4
3. tensorflow-deps **&** tensorflow-estimator --> 2.9.0
4. tensorflow-macos --> 2.9.2
5. tensorflow-metal --> 0.5.0
6. keras --> 2.9.0
7. keras-preprocessing  --> 1.1.2
8. Python 3.8.13

When resizing and rescaling from keras.layers, I got the following error:

```
resize_and_rescale = keras.Sequential([
   layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),
   layers.experimental.preprocessing.Rescaling(1./255),
])

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Input In [15], in <cell line: 1>()
      1 resize_and_rescale = keras.Sequential([
----> 2   layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),
      3   layers.experimental.preprocessing.Rescaling(1./255),
      4 ])
AttributeError: module 'keras.layers' has no attribute 'experimental'
```

Any suggestions? Thanks"
56888,Gradients calculated in reverse mode and forward mode are not equal for `tf.compat.v1.keras.layers.GlobalAvgPool2D`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The jacobian matrix calculated by reverse mode is not equal to that computed in forward mode.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

input = tf.constant(
    [[[[0.1695373]],[[0.2855878]],[[0.67758346]],[[0.2562498]],[[0.11648738]]],
     [[[0.76352525]],[[0.61349154]],[[0.12928855]],[[0.0187813]],[[0.72227335]]],
     [[[0.9399148]],[[0.3782258]],[[0.4337635]],[[0.4784329]],[[0.28710878]]]], dtype=tf.float32)

globalAvgPool2d = tf.compat.v1.keras.layers.GlobalAvgPool2D(data_format=""channels_last"", keepdims=[2,2])

with tf.GradientTape() as g:
    g.watch(input)
    res_backward = globalAvgPool2d(input)
jacob = g.jacobian(res_backward,input)

grad_fwd_arr = []

for i in range(tf.size(input)):
    tangents = tf.reshape(tf.one_hot(i,tf.size(input)),shape=input.shape)
    with tf.autodiff.ForwardAccumulator(input, tangents) as acc:
        res_forward = globalAvgPool2d(input)
        jvp = acc.jvp(res_forward)
        grad_fwd_arr.append(jvp)
jacob_fwd = tf.reshape(tf.convert_to_tensor(grad_fwd_arr),shape=jacob.shape)
np.testing.assert_allclose(jacob,jacob_fwd)
```


### Relevant log output

```shell
AssertionError: 
Not equal to tolerance rtol=1e-07, atol=0

Mismatched elements: 20 / 45 (44.4%)
Max absolute difference: 0.2
Max relative difference: 1.
 x: array([[[[[[[[0.2]],

            [[0.2]],...
 y: array([[[[[[[[0.2]],

            [[0. ]],...
```
</details>"
56887,AARCH64 builds broken,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

CentOS 7

### Mobile device

n/a

### Python version

3.8.10

### Bazel version

5.1.1

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
Build fails to complete with
""./tensorflow/compiler/xla/service/cpu/runtime_fp16.h:23:23: error: '_Float16' does not name a type""
```


### Standalone code to reproduce the issue

```shell
bazel build --config=nonccl --config=mkl_aarch64 --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --copt=""-fopenmp"" --linkopt=""-lgomp"" --jobs=100 -- //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
In file included from tensorflow/compiler/xla/service/cpu/runtime_fp16.cc:16:
./tensorflow/compiler/xla/service/cpu/runtime_fp16.h:23:23: error: '_Float16' does not name a type
   23 | using XlaF16ABIType = _Float16;
      |                       ^~~~~~~~
./tensorflow/compiler/xla/service/cpu/runtime_fp16.h:36:12: error: 'XlaF16ABIType' does not name a type
   36 | extern ""C"" XlaF16ABIType __gnu_f2h_ieee(float);
      |            ^~~~~~~~~~~~~
./tensorflow/compiler/xla/service/cpu/runtime_fp16.h:39:33: error: 'XlaF16ABIType' was not declared in this scope
   39 | extern ""C"" float __gnu_h2f_ieee(XlaF16ABIType);
      |                                 ^~~~~~~~~~~~~
./tensorflow/compiler/xla/service/cpu/runtime_fp16.h:42:12: error: 'XlaF16ABIType' does not name a type
   42 | extern ""C"" XlaF16ABIType __truncdfhf2(double);
      |            ^~~~~~~~~~~~~
tensorflow/compiler/xla/service/cpu/runtime_fp16.cc:63:1: error: 'XlaF16ABIType' does not name a type
   63 | XlaF16ABIType ABSL_ATTRIBUTE_WEAK __gnu_f2h_ieee(float float_value) {
      | ^~~~~~~~~~~~~
tensorflow/compiler/xla/service/cpu/runtime_fp16.cc:117:27: error: redefinition of 'float __gnu_h2f_ieee'
  117 | float ABSL_ATTRIBUTE_WEAK __gnu_h2f_ieee(XlaF16ABIType hf) {
      |                           ^~~~~~~~~~~~~~
In file included from tensorflow/compiler/xla/service/cpu/runtime_fp16.cc:16:
./tensorflow/compiler/xla/service/cpu/runtime_fp16.h:39:18: note: 'float __gnu_h2f_ieee' previously defined here
   39 | extern ""C"" float __gnu_h2f_ieee(XlaF16ABIType);
      |                  ^~~~~~~~~~~~~~
tensorflow/compiler/xla/service/cpu/runtime_fp16.cc:117:42: error: 'XlaF16ABIType' was not declared in this scope
  117 | float ABSL_ATTRIBUTE_WEAK __gnu_h2f_ieee(XlaF16ABIType hf) {
      |                                          ^~~~~~~~~~~~~
tensorflow/compiler/xla/service/cpu/runtime_fp16.cc:141:1: error: 'XlaF16ABIType' does not name a type
  141 | XlaF16ABIType ABSL_ATTRIBUTE_WEAK __truncdfhf2(double d) {
      | ^~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
</details>"
56886,Reverse-mode autodiff didn't throw error with invalid input shape for API `tf.reduce_max` as expected,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Reverse-mode autodiff didn't throw any error with **invalid input shape `axis.shape=(1,1)`** on line 13 in the code below. 
Forward-mode autodiff threw the `ValueError` as expected. So the behaviour for `tf.GradientTape` should be the same but not.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input = tf.constant([0.9165313, 0.9094733], shape=(2,), dtype=tf.float32)

# axis is with invalid shape
axis = tf.constant([[0]], shape=(1, 1), dtype=tf.int32)

res = tf.reduce_max(input,axis=axis,keepdims=False)
print(res)
with tf.GradientTape() as g:
    g.watch(input)
    # should threw ValueError here, but it didn't 
    res_backward = tf.reduce_max(input,axis=axis,keepdims=False)
print(g.gradient(res_backward,input))

with tf.autodiff.ForwardAccumulator(input,tf.constant([1.,0.])) as acc:
    res_forward = tf.reduce_max(input,axis=axis,keepdims=False)
print(acc.jvp(res_forward))
```


### Relevant log output

```shell
ValueError: Shape must be at most rank 1 but is rank 2 for '{{node gradient_tape/Sum}} = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](gradient_tape/Cast, inputs_1)' with input shapes: ?, [1,1].
```
</details>"
56885,collective_ops.all_reduce_v2 with ordering_token does not work correctly,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.5 or tf 2.8

### Custom Code

No

### OS Platform and Distribution

Centos 72.

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I use MultiWorkerMirroredStrategy to train a DeepRecommendation model in EasyRec. However it reports the following error:

I0725 11:56:59.386929 140272595433216 basic_session_run_hooks.py:254] lr = 0.001,step = 0,cross_entropy_loss = 1.0084826,regularization_loss = 0.003870264,total_loss = 1.0123528
2022-07-25 11:57:07.263300: E tensorflow/core/common_runtime/base_collective_executor.cc:243] BaseCollectiveExecutor::StartAbort Invalid argument: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:0/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.
2022-07-25 11:57:07.264955: E tensorflow/core/common_runtime/ring_alg.cc:276] Aborting RingGather with Invalid argument: [_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.
The error could be from a previous operation. Restart your program to reset.
Additional GRPC error information from remote target /job:worker/replica:0/task:1:
:{""created"":""@1658721427.264762538"",""description"":""Error received from peer ipv4:127.0.0.1:10838"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""[_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.\nThe error could be from a previous operation. Restart your program to reset."",""grpc_status"":3}
2022-07-25 11:57:07.265079: E tensorflow/core/common_runtime/ring_alg.cc:276] Aborting RingGather with Invalid argument: [_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.
The error could be from a previous operation. Restart your program to reset.
Additional GRPC error information from remote target /job:worker/replica:0/task:1:
:{""created"":""@1658721427.264794128"",""description"":""Error received from peer ipv4:127.0.0.1:10838"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""[_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.\nThe error could be from a previous operation. Restart your program to reset."",""grpc_status"":3}
2022-07-25 11:57:07.265118: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at collective_ops.cc:713 : Invalid argument: [_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:0/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.
The error could be from a previous operation. Restart your program to reset.

However, if I update replace the function collective_ops.all_gather_v2 with all_gather in tensorflow/python/distribute/cross_device_utils.py: 381, everything runs fine. I want to know what does ordering_token means?
```


### Standalone code to reproduce the issue

```shell
git clone https://github.com/alibaba/EasyRec.git
cd EasyRec
bash scripts/init.sh
TEST_DEVICES='' python -m easy_rec.python.test.train_eval_test TrainEvalTest.test_train_with_multi_worker_mirror
```


### Relevant log output

```shell
I0725 11:56:59.385384 140272595433216 basic_session_run_hooks.py:262] loss = 1.0119569, step = 0
INFO:tensorflow:lr = 0.001,step = 0,cross_entropy_loss = 1.0084826,regularization_loss = 0.003870264,total_loss = 1.0123528
I0725 11:56:59.386929 140272595433216 basic_session_run_hooks.py:254] lr = 0.001,step = 0,cross_entropy_loss = 1.0084826,regularization_loss = 0.003870264,total_loss = 1.0123528
2022-07-25 11:57:07.263300: E tensorflow/core/common_runtime/base_collective_executor.cc:243] BaseCollectiveExecutor::StartAbort Invalid argument: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:0/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.
2022-07-25 11:57:07.264955: E tensorflow/core/common_runtime/ring_alg.cc:276] Aborting RingGather with Invalid argument: [_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.
The error could be from a previous operation. Restart your program to reset.
Additional GRPC error information from remote target /job:worker/replica:0/task:1:
:{""created"":""@1658721427.264762538"",""description"":""Error received from peer ipv4:127.0.0.1:10838"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""[_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.\nThe error could be from a previous operation. Restart your program to reset."",""grpc_status"":3}
2022-07-25 11:57:07.265079: E tensorflow/core/common_runtime/ring_alg.cc:276] Aborting RingGather with Invalid argument: [_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.
The error could be from a previous operation. Restart your program to reset.
Additional GRPC error information from remote target /job:worker/replica:0/task:1:
:{""created"":""@1658721427.264794128"",""description"":""Error received from peer ipv4:127.0.0.1:10838"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""[_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.\nThe error could be from a previous operation. Restart your program to reset."",""grpc_status"":3}
2022-07-25 11:57:07.265118: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at collective_ops.cc:713 : Invalid argument: [_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:0/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.
The error could be from a previous operation. Restart your program to reset.
2022-07-25 11:57:07.265255: E tensorflow/core/common_runtime/ring_alg.cc:276] Aborting RingGather with Invalid argument: [_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.
The error could be from a previous operation. Restart your program to reset.
Additional GRPC error information from remote target /job:worker/replica:0/task:1:
:{""created"":""@1658721427.264813400"",""description"":""Error received from peer ipv4:127.0.0.1:10838"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""[_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.\nThe error could be from a previous operation. Restart your program to reset."",""grpc_status"":3}
2022-07-25 11:57:07.265326: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at collective_ops.cc:713 : Invalid argument: [_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:0/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.
The error could be from a previous operation. Restart your program to reset.
2022-07-25 11:57:07.268713: E tensorflow/core/common_runtime/ring_alg.cc:276] Aborting RingGather with Invalid argument: [_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.
The error could be from a previous operation. Restart your program to reset.
Additional GRPC error information from remote target /job:worker/replica:0/task:1:
:{""created"":""@1658721427.264830344"",""description"":""Error received from peer ipv4:127.0.0.1:10838"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""[_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:1/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.\nThe error could be from a previous operation. Restart your program to reset."",""grpc_status"":3}
2022-07-25 11:57:07.268788: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at collective_ops.cc:713 : Invalid argument: [_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:0/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.
The error could be from a previous operation. Restart your program to reset.
2022-07-25 11:57:07.269536: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at collective_ops.cc:713 : Invalid argument: [_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:0/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.
The error could be from a previous operation. Restart your program to reset.
Traceback (most recent call last):
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1375, in _do_call
    return fn(*args)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1360, in _run_fn
    target_list, run_metadata)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1453, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:worker/replica:0/task:0:
[_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:0/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.
The error could be from a previous operation. Restart your program to reset.
	 [[{{node CollectiveGatherV2_16}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/apsarapangu/disk3/mengli.cml/easy_rec_outer/EasyRec/easy_rec/python/train_eval.py"", line 145, in <module>
    tf.app.run()
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/apsarapangu/disk3/mengli.cml/easy_rec_outer/EasyRec/easy_rec/python/train_eval.py"", line 139, in main
    FLAGS.check_mode)
  File ""/apsarapangu/disk3/mengli.cml/easy_rec_outer/EasyRec/easy_rec/python/main.py"", line 330, in _train_and_evaluate_impl
    estimator_train.train_and_evaluate(estimator, train_spec, eval_spec)
  File ""/apsarapangu/disk3/mengli.cml/easy_rec_outer/EasyRec/easy_rec/python/compat/estimator_train.py"", line 75, in train_and_evaluate
    _TrainingExecutor)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/estimator_training.py"", line 290, in train_and_evaluate
    session_config=run_config.session_config)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 861, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 360, in _run_single_worker
    return worker_fn(strategy)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/estimator_training.py"", line 252, in _worker_fn
    hooks=hooks)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1173, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1235, in _train_model_distributed
    self._config._train_distribute, input_fn, hooks, saving_listeners)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1349, in _actual_train_model_distributed
    saving_listeners)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1427, in _train_with_estimator_spec
    estimator_spec, worker_hooks, saving_listeners)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1374, in _train_with_estimator_spec_distributed
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 779, in run
    run_metadata=run_metadata)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1284, in run
    run_metadata=run_metadata)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1385, in run
    raise six.reraise(*original_exc_info)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/six.py"", line 703, in reraise
    raise value
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1370, in run
    return self._sess.run(*args, **kwargs)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1443, in run
    run_metadata=run_metadata)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1201, in run
    return self._sess.run(*args, **kwargs)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 968, in run
    run_metadata_ptr)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1191, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1369, in _do_run
    run_metadata)
  File ""/apsarapangu/disk3/mengli.cml/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1394, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:worker/replica:0/task:0:
[_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 244. Op at device /job:worker/replica:0/task:0/device:CPU:0 expected shape [98] but another member in the group expected shape [102]. This is likely due to different input shapes at different members of the collective op.
The error could be from a previous operation. Restart your program to reset.
	 [[node CollectiveGatherV2_16 (defined at /anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py:1319) ]]

Original stack trace for 'CollectiveGatherV2_16':
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/easy_rec_outer/EasyRec/easy_rec/python/train_eval.py"", line 145, in <module>
    tf.app.run()
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/easy_rec_outer/EasyRec/easy_rec/python/train_eval.py"", line 139, in main
    FLAGS.check_mode)
  File ""/easy_rec_outer/EasyRec/easy_rec/python/main.py"", line 330, in _train_and_evaluate_impl
    estimator_train.train_and_evaluate(estimator, train_spec, eval_spec)
  File ""/easy_rec_outer/EasyRec/easy_rec/python/compat/estimator_train.py"", line 75, in train_and_evaluate
    _TrainingExecutor)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/estimator_training.py"", line 290, in train_and_evaluate
    session_config=run_config.session_config)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 861, in run_distribute_coordinator
    task_id, session_config, rpc_layer)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_coordinator.py"", line 360, in _run_single_worker
    return worker_fn(strategy)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/estimator_training.py"", line 252, in _worker_fn
    hooks=hooks)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1173, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1235, in _train_model_distributed
    self._config._train_distribute, input_fn, hooks, saving_listeners)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1319, in _actual_train_model_distributed
    self.config))
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 2833, in call_for_each_replica
    return self._call_for_each_replica(fn, args, kwargs)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 679, in _call_for_each_replica
    self._container_strategy(), fn, args, kwargs)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_run.py"", line 104, in call_for_each_replica
    return _call_for_each_replica(strategy, fn, args, kwargs)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_run.py"", line 239, in _call_for_each_replica
    **merge_kwargs)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 597, in wrapper
    return func(*args, **kwargs)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/training/optimizer.py"", line 676, in _distributed_apply
    ds_reduce_util.ReduceOp.SUM, grads_and_vars)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 2402, in batch_reduce_to
    return self._batch_reduce_to(reduce_op, value_destination_pairs, options)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 770, in _batch_reduce_to
    options=self._communication_options.merge(options))
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_ops.py"", line 447, in batch_reduce
    options)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_ops.py"", line 1270, in batch_reduce_implementation
    for value, dest in value_destination_pairs
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_ops.py"", line 1270, in <listcomp>
    for value, dest in value_destination_pairs
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_ops.py"", line 1225, in reduce_implementation
    options)[0]
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_ops.py"", line 1212, in _all_reduce_per_replica_values
    self._all_reduce(reduce_op, values_by_device[i], i, options))
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_ops.py"", line 1175, in _all_reduce
    options.timeout_seconds))
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_utils.py"", line 566, in all_reduce_indexed_slices
    length, communication_hint, timeout=timeout)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_utils.py"", line 388, in _all_gather
    ordering_token=ordering_token)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/ops/collective_ops.py"", line 200, in all_gather_v2
    ordering_token=ordering_token or [])
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/ops/gen_collective_ops.py"", line 545, in collective_gather_v2
    timeout_seconds=timeout_seconds, name=name)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 750, in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3565, in _create_op_internal
    op_def=op_def)
  File ""/anaconda3/envs/tf_py3_20/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__
    self._traceback = tf_stack.extract_stack_for_node(self._c_op)
```
</details>"
56884,"message ""convert_variables_to_constants is deprecated""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Documentation Bug

### Source

binary

### Tensorflow Version

2.8.2

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

`tf.compat.v1.graph_util.convert_variables_to_constants` gives a warning message:

```
WARNING:tensorflow:From <ipython-input-2-d9eca9909494>:2: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
```

It's confusing here: I did use `tf.compat.v1.graph_util.convert_variables_to_constants`. Why did it still give a warning message?


### Standalone code to reproduce the issue

```py
import tensorflow as tf
tf.compat.v1.graph_util.convert_variables_to_constants(tf.compat.v1.Session(), tf.compat.v1.GraphDef(), [])
```

https://colab.research.google.com/gist/njzjz/6cd1af05016aea60e1c4ec258d5897d6/tf-compat-v1-graph_util-convert_variables_to_constants.ipynb


### Relevant log output

_No response_</details>"
56883,Are there on-device training with Tensorflow lite in Arduino IDEs???,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Source

source

### Tensorflow Version

tf2.8

### Custom Code

No

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Is there a way to perform on-device training with Tensorflow lite in Arduino IDE? I see an option for Java and C++ with android but can't find the C++ doc... Seems like C ain't supported too.
```


### Standalone code to reproduce the issue

```shell
No code
```


### Relevant log output

_No response_</details>"
56882,"TypeError: ('Keyword argument not understood:', 'keepdims')","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.8

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I have saved model in H5 file. I want to convert this file into .pb and then convert into IR using OPENVINO toolkit.
I have this .pb file which I converted last month and it is getting converted to IR format using OPENVINO currently. But if I now convert the same .h5 file now to .pb then it will convert but while converting to IR it is giving error.

All this situation tells me that it is causing problem due to .PB file.

I downgraded TensorFlow version to 2.5.3 then it gave me error as 
TypeError: ('Keyword argument not understood:', 'keepdims')

Could you please help me with this ?
```


### Standalone code to reproduce the issue

```shell
Below code I am using to convert .h5 file to .pb

from tensorflow.keras.models import load_model
model_path = data_path + '/MobileNetV3_New_Pruning.h5'

model = load_model(model_path)
print(model.outputs)
# [<tf.Tensor 'dense_2/Softmax:0' shape=(?, 10) dtype=float32>]
print(model.inputs)
# [<tf.Tensor 'conv2d_1_input:0' shape=(?, 28, 28, 1) dtype=float32>]
#


from keras.models import load_model
from keras import backend as K
import tensorflow as tf
from tensorflow.python.framework.ops import disable_eager_execution
from tensorflow.python.framework.graph_util import convert_variables_to_constants

tf.compat.v1.disable_eager_execution()


from keras.models import Model, load_model
# model = load_model('Prediction_Test/Classification_model.h5')
# model.save('Prediction_Test/Classification_model.h5')


# # tfd = tfp.distributions

tf.compat.v1.global_variables_initializer()

disable_eager_execution()

def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):
    """"""
    Freezes the state of a session into a pruned computation graph.

    Creates a new computation graph where variable nodes are replaced by
    constants taking their current value in the session. The new graph will be
    pruned so subgraphs that are not necessary to compute the requested
    outputs are removed.
    @param session The TensorFlow session to be frozen.
    @param keep_var_names A list of variable names that should not be frozen,
                          or None to freeze all the variables in the graph.
    @param output_names Names of the relevant graph outputs.
    @param clear_devices Remove the device directives from the graph for better portability.
    @return The frozen graph definition.
    """"""
    from tensorflow.python.framework.graph_util import convert_variables_to_constants
    graph = session.graph
    with graph.as_default():
        freeze_var_names = list(set(v.op.name for v in tf.compat.v1.global_variables()).difference(keep_var_names or []))
        output_names = output_names or []
        output_names += [v.op.name for v in tf.compat.v1.global_variables()]
        # Graph -> GraphDef ProtoBuf
        input_graph_def = graph.as_graph_def()
        if clear_devices:
            for node in input_graph_def.node:
                node.device = """"
        frozen_graph = convert_variables_to_constants(session, input_graph_def,
                                                      output_names, freeze_var_names)
        return frozen_graph


frozen_graph = freeze_session(K.get_session(),
                              output_names=[out.op.name for out in model.outputs])


import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

# Save to ./model/tf_model.pb
tf.train.write_graph(frozen_graph, data_path, ""tf_model.pb"", as_text=False)
```


### Relevant log output

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-46-637c54ecae2e> in <module>()
      2 model_path = data_path + '/MobileNetV3_New_Pruning.h5'
      3 
----> 4 model = load_model(model_path)
      5 print(model.outputs)
      6 # [<tf.Tensor 'dense_2/Softmax:0' shape=(?, 10) dtype=float32>]

14 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py in validate_kwargs(kwargs, allowed_kwargs, error_message)
   1135   for kwarg in kwargs:
   1136     if kwarg not in allowed_kwargs:
-> 1137       raise TypeError(error_message, kwarg)
   1138 
   1139 

TypeError: ('Keyword argument not understood:', 'keepdims')
```
</details>"
56880,"[No fix, use horovod.] Manual device placement for distributed training isn't async like TF1","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.12

### Custom Code

No

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.6

### GPU model and memory

3090, 24GB

### Current Behaviour?

```shell
As I am not able to use a tensorflow dataset for my specific application (migrated from v1, would rather keep the custom loader that works in parallel very well, tuned to specific machines.), I was looking into V1-esque distributed training strategies.
In V1, we would simply loop over gpus and redo the computation while splitting the inputs in quarters.
That is what I did in tf2, and that is what the manual placement guide says.
.debugging.set_log_device_placement(True)

gpus = tf.config.list_logical_devices('GPU')
if gpus:
  # Replicate your computation on multiple GPUs
  c = []
  for gpu in gpus:
    with tf.device(gpu.name):
      a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
      b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
      c.append(tf.matmul(a, b))

  with tf.device('/CPU:0'):
    matmul_sum = tf.add_n(c)

  print(matmul_sum)
However, the amount of time the computation takes on multiple gpus is simply longer than the time that takes on a single gpu.
So I printed what happens after each gpu's computations are finished, and as I expected, the processes are not running async.
I even accumulated the gradients to compile them later, but it did not fix the problem.

Is there any way to make distributed training work without a tf.dataset? I've already gone through so many issues to make this work, including the following (not even a distributed issue)
https://github.com/tensorflow/tensorflow/issues/56878
```


### Standalone code to reproduce the issue

```shell
.debugging.set_log_device_placement(True)

gpus = tf.config.list_logical_devices('GPU')
if gpus:
  # Replicate your computation on multiple GPUs
  c = []
  for gpu in gpus:
    with tf.device(gpu.name):
      a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
      b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
      c.append(tf.matmul(a, b))

  with tf.device('/CPU:0'):
    matmul_sum = tf.add_n(c)

  print(matmul_sum)
```


### Relevant log output

_No response_</details>"
56879,Does tensorflow have a detach method like in pytorch to interrupt backforward?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tell me the Method to interrupt gradient propagation
```


### Standalone code to reproduce the issue

```shell
nothing
```


### Relevant log output

_No response_</details>"
56878,using tf.function on a model call will not utilize gpu nor VRAM but log placement to gpu,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Windows 11

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.6

### GPU model and memory

3090 24GB

### Current Behaviour?

```shell
Utilize tf.function on a function that calls model(a,b) (aka model.call)
Although eager tensors have all been allocated to the gpu, actual training doesnt happen on the gpu, rather the cpu. Notice 20x slower training speeds than when utilizing nested tf.functions in model.call, which is not allowed in MirroredStrategy.
```


### Standalone code to reproduce the issue

```shell
Model.call:

def call(self,x,y):
    vals=self.disc_gradients(x,y)
    self.gen_gradients(x,y)
    return vals
```
model call wrapper:
```
@tf.function#this is the problematic tf.function, if this tf.function is moved to the below tf.functions it works fine.
def run_batch(self,batch_x,batch_y):
    val=self.model(batch_x,batch_y)
    return val
```
gradients functions:
```
#@tf.function
def get_disc_gradients(self,x,y):
    #calc val
    self.disc_optimizer.apply_gradients(disc_gradients,vars)
    return disc_gradients
```
Obviously this code currently does nothing, it's a minimal reproducible example.
```


### Relevant log output

```shell
WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.
```
The above log is relevant as it tries to make us wrap the entire model.call in a tf.function, which is where I discovered this entire issue. This issue is replicateable w/o using MirroredStrategy: all that matters is that tf.function is wrapping a model call.
No other relevant log output could be found.
Only way to notice the issues are the extremely long batch times, along with
the GPU only utilizing 3GB of VRAM compared to a full 20gb when the tf.function is moved.
</details>"
56875,keras.metrics.Mean does not support cross-replica context,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubunto 18.04.6 LTS

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Trying to call `update_state` on a `keras.metrics.Mean` object within a cross-replica context (specifically `tf.distribute.MirroredStrategy`) raises `ValueError: SyncOnReadVariable does not support `assign_add` in cross-replica context when aggregation is set to `tf.VariableAggregation.SUM`.`.

Reproducible in TF 2.8.0 as well.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow import keras

with tf.distribute.MirroredStrategy().scope():
    metric = keras.metrics.Mean()
    metric.update_state(0)
```


### Relevant log output

```shell
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Input In [8], in <cell line: 1>()
      1 with tf.distribute.MirroredStrategy().scope():
      2     metric = keras.metrics.Mean()
----> 3     metric.update_state(0)

File ~/anaconda3/envs/tf-2.9.0/lib/python3.9/site-packages/keras/utils/metrics_utils.py:70, in update_state_wrapper.<locals>.decorated(metric_obj, *args, **kwargs)
     64     raise ValueError(
     65         'Trying to run metric.update_state in replica context when '
     66         'the metric was not created in TPUStrategy scope. '
     67         'Make sure the keras Metric is created in TPUstrategy scope. ')
     69 with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):
---> 70   update_op = update_state_fn(*args, **kwargs)
     71 if update_op is not None:  # update_op will be None in eager execution.
     72   metric_obj.add_update(update_op)

File ~/anaconda3/envs/tf-2.9.0/lib/python3.9/site-packages/keras/metrics/base_metric.py:140, in Metric.__new__.<locals>.update_state_fn(*args, **kwargs)
    137 control_status = tf.__internal__.autograph.control_status_ctx()
    138 ag_update_state = tf.__internal__.autograph.tf_convert(
    139     obj_update_state, control_status)
--> 140 return ag_update_state(*args, **kwargs)

File ~/anaconda3/envs/tf-2.9.0/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:689, in convert.<locals>.decorator.<locals>.wrapper(*args, **kwargs)
    687 try:
    688   with conversion_ctx:
--> 689     return converted_call(f, args, kwargs, options=options)
    690 except Exception as e:  # pylint:disable=broad-except
    691   if hasattr(e, 'ag_error_metadata'):

File ~/anaconda3/envs/tf-2.9.0/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:331, in converted_call(f, args, kwargs, caller_fn_scope, options)
    329 if conversion.is_in_allowlist_cache(f, options):
    330   logging.log(2, 'Allowlisted %s: from cache', f)
--> 331   return _call_unconverted(f, args, kwargs, options, False)
    333 if ag_ctx.control_status_ctx().status == ag_ctx.Status.DISABLED:
    334   logging.log(2, 'Allowlisted: %s: AutoGraph is disabled in context', f)

File ~/anaconda3/envs/tf-2.9.0/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:458, in _call_unconverted(f, args, kwargs, options, update_cache)
    455   return f.__self__.call(args, kwargs)
    457 if kwargs is not None:
--> 458   return f(*args, **kwargs)
    459 return f(*args)

File ~/anaconda3/envs/tf-2.9.0/lib/python3.9/site-packages/keras/metrics/base_metric.py:465, in Reduce.update_state(self, values, sample_weight)
    463 value_sum = tf.reduce_sum(values)
    464 with tf.control_dependencies([value_sum]):
--> 465   update_total_op = self.total.assign_add(value_sum)
    467 # Exit early if the reduction doesn't have a denominator.
    468 if self.reduction == metrics_utils.Reduction.SUM:

File ~/anaconda3/envs/tf-2.9.0/lib/python3.9/site-packages/tensorflow/python/distribute/values.py:1313, in SyncOnReadVariable.assign_add(self, value, use_locking, name, read_value)
   1310 if (ds_context.in_cross_replica_context() and
   1311     not values_util.in_replica_update_context()):
   1312   values_util.mark_as_unsaveable()
-> 1313   return values_util.on_read_assign_add_cross_replica(
   1314       self, value, read_value=read_value)
   1315 else:
   1316   return super(SyncOnReadVariable,
   1317                self).assign_add(value, use_locking, name, read_value)

File ~/anaconda3/envs/tf-2.9.0/lib/python3.9/site-packages/tensorflow/python/distribute/values_util.py:197, in on_read_assign_add_cross_replica(var, value, read_value)
    195 if ds_context.in_cross_replica_context():
    196   if var.aggregation == vs.VariableAggregation.SUM:
--> 197     raise ValueError(
    198         ""SyncOnReadVariable does not support `assign_add` in ""
    199         ""cross-replica context when aggregation is set to ""
    200         ""`tf.VariableAggregation.SUM`."")
    201   return assign_on_each_device(var, assign_add_on_device,
    202                                value, read_value)

ValueError: SyncOnReadVariable does not support `assign_add` in cross-replica context when aggregation is set to `tf.VariableAggregation.SUM`.
```
</details>"
56874,Limit max results ,"How to limit max results shown on screen in Android object detection JAVA, in latest version its written in kotlin so i just can't understand, so how to set limit of results in JAVA"
56873,Wrong gradient for `tf.math.xlog1py` when x is `0`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The gradient of `tf.math.xlog1py` at `x=0` should be `log1p(y)`. In the example below, `y=1.0`, so the gradient should be `0.6931472`. However, the output gradient is `0.0`, which is wrong!
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.zeros([1])
y = 1.0
print(""x = "", x)
print(""y = "", y)

with tf.GradientTape() as g:
  g.watch(x)
  res = tf.math.xlog1py(x, y)
print(""gradient: "", g.gradient(res, x))
theoretical, numerical = tf.test.compute_gradient(lambda x: tf.math.xlog1py(x, y), [x])
print(""theoretical: "", theoretical)
print(""numerical: "", numerical)
print(""log1p(y): "", tf.math.log1p(y))
```


### Relevant log output

```shell
x =  tf.Tensor([0.], shape=(1,), dtype=float32)
y =  1.0
gradient:  tf.Tensor([0.], shape=(1,), dtype=float32)
theoretical:  (array([[0.]], dtype=float32),)
numerical:  (array([[0.6931472]], dtype=float32),)
log1p(y):  tf.Tensor(0.6931472, shape=(), dtype=float32)
```
</details>"
56872,Building to wasm,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

v2.9.0+incompatible

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04.4 LTS

### Mobile device

_No response_

### Python version

N/A

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Applications depending on tensorflow do not build to js operating system and wasm architecture. Does the Tensorflow for golang have wasm support?
```


### Standalone code to reproduce the issue

```shell
Using the following code:


package main

import (
	""fmt""

	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
)

func main() {
	var t tf.Tensor
	fmt.Println(t)
}
```
```


### Relevant log output

```shell
Build results:

$GOOS=js GOARCH=wasm go build main.go 
# command-line-arguments
./main.go:10:11: undefined: tensorflow.Tensor
```
```
</details>"
56871,Bilinear upsampling layer in tflite cannot be 8-bit quantized correctly,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04
- TensorFlow installation (pip package or built from source):  pip 
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0
- TensorFlow Model Optimization version (installed from source or binary): pip  0.7.2

### Abstract
Hello, I want to do full 8-bit quantization(input, weight all 8-bit) to the network with a bilinear upsampling layer. The fake QAT result in validation set is closed to FP32 result, but when I converted the QAT model to full 8-bit tflite model, the result in validation set decreased significantly, almost no accuracy. So I wonder whether I made a mistake or tflite doesn't support int8 bilinear upsampling correctly. The details are as follows.

### 2. Code

I aim to do 8-bit quantization to the network below, which has a bilinear upsample branch

```
def upsambiskip(scale=3, in_channels=3, num_fea=28, m=4, out_channels=3):
    inp = Input(shape=(None, None, 3)) 
    upsampled_inp=UpSampling2D(size=(3,3),data_format=None,interpolation='bilinear')(inp)

    x = Conv2D(num_fea, 3, padding='same', activation='relu', kernel_initializer=glorot_normal(), bias_initializer='zeros')(inp)

    for i in range(m):
        x = Conv2D(num_fea, 3, padding='same', activation='relu', kernel_initializer=glorot_normal(), bias_initializer='zeros')(x)

    x = Conv2D(out_channels*(scale**2), 3, padding='same', kernel_initializer=glorot_normal(), bias_initializer='zeros')(x)
    
    depth_to_space = Lambda(lambda x: tf.nn.depth_to_space(x, scale))
    out = depth_to_space(x)
    x = Add()([upsampled_inp, out])
    x = Conv2D(3, 3, padding='same', kernel_initializer=glorot_normal(), bias_initializer='zeros')(x)

    clip_func = Lambda(lambda x: K.clip(x, 0., 255.))
    out = clip_func(x)
    
    return Model(inputs=inp, outputs=out)
```
I had done FP32 training before, then I loaded FP32 checkpoint and did Quantization-aware training like below
```
class NoOpQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):
    def get_weights_and_quantizers(self, layer):
        return []
    def get_activations_and_quantizers(self, layer):
        return []
    def set_quantize_weights(self, layer, quantize_weights):
        pass
    def set_quantize_activations(self, layer, quantize_anctivations):
        pass
    def get_output_quantizers(self, layer):
        return []
    def get_config(self):
        return {}

def ps_quantization(self, layer):
    # lambda not quantization
    if 'lambda' in layer.name :
        return tfmot.quantization.keras.quantize_annotate_layer(layer, quantize_config=NoOpQuantizeConfig())
    return layer


# create fp32 model and load weight
p_model = create_model(args['networks'])
    
lg.info('Start copying weights and annotate Lambda layer...')
annotate_model = tf.keras.models.clone_model(
            p_model,
            clone_function=self.ps_quantization
            )
lg.info('Start annotating other parts of model...')
annotate_model = tfmot.quantization.keras.quantize_annotate_model(annotate_model)
lg.info('Creating quantize-aware model...')
depth_to_space = Lambda(lambda x: tf.nn.depth_to_space(x, 3))
with tfmot.quantization.keras.quantize_scope({'NoOpQuantizeConfig': NoOpQuantizeConfig, 'depth_to_space': depth_to_space, 'tf': tf}):
    self.model = tfmot.quantization.keras.quantize_apply(annotate_model)
    
# training...
```

the QAT model was correct, and the performance in validation set was closed to FP32's. Then I converted QAT model to tflite

```
def qat_quantize(model_path,output_path):

    fakeqmodel=tf.keras.models.load_model(model_path)
    print('fake quantize model validate..')

    # validate fake QAT model, and the performance is OK
    load_validate(fakeqmodel,save_path=None)

    # convert QAT model and store int8 tflite model
    converter = tf.lite.TFLiteConverter.from_keras_model(fakeqmodel)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    ### converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.experimental_new_converter=True
    converter.experimental_new_quantizer=True
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8
    quant_tf_model = converter.convert()
    with open(output_path, 'wb') as f:
        f.write(quant_tf_model)
```

I loaded tflite model and evaluated as below, but the performance decrease significantly. 
```
def evaluate(quantized_model_path, save_path):

    interpreter = tf.lite.Interpreter(model_path=quantized_model_path,num_threads=32)
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    IS, IZ = input_details[0]['quantization']
    OS, OZ = output_details[0]['quantization']
    print('Input Scale: {}, Zero Point: {}'.format(IS, IZ))
    print('Output Scale: {}, Zero Point: {}'.format(OS, OZ))
    psnr = 0.0
    for i in range(801, 901):
        lr_path = 'data/DIV2K/DIV2K_train_LR_bicubic/X3_pt/0{}x3.pt'.format(i)
        with open(lr_path, 'rb') as f:
            lr = pickle.load(f)
        h, w, c = lr.shape
        lr = np.expand_dims(lr, 0).astype(np.float32)
        # ##lr = np.round(lr/IS+IZ).astype(np.uint8)
        lr = lr.astype(np.uint8)

        hr_path = 'data/DIV2K/DIV2K_train_HR_pt/0{}.pt'.format(i)
        with open(hr_path, 'rb') as f:
            hr = pickle.load(f)
        hr = np.expand_dims(hr, 0).astype(np.float32)
        interpreter.resize_tensor_input(input_details[0]['index'], lr.shape)
        interpreter.allocate_tensors()
        interpreter.set_tensor(input_details[0]['index'], lr)
        interpreter.invoke()

        sr = interpreter.get_tensor(output_details[0]['index'])
        #sr = np.clip(np.round((sr.astype(np.float32)-OZ)*OS), 0, 255)
        sr = np.clip(sr, 0, 255)
        b, h, w, c = sr.shape
        # save image
        if save_path is not None:
            save_name = osp.join(save_path, '{:04d}x3.png'.format(i))
            cv2.imwrite(save_name, cv2.cvtColor(sr.squeeze().astype(np.uint8), cv2.COLOR_RGB2BGR))

        mse = np.mean((sr[:, 1:h-1, 1:w-1, :].astype(np.float32) - hr[:, 1:h-1, 1:w-1, :].astype(np.float32)) ** 2)
        singlepsnr =  20. * math.log10(255. / math.sqrt(mse))
        print('[{}]/[100]: {}'.format(i, singlepsnr))
        psnr += singlepsnr
    print(psnr / 100)

```

### 3. Failure after conversion
the conversion is successful, but the int8 tflite model has a low accuracy performance.


### 5. (optional) Any other info / logs
the tflite model is below, though there are some unexpected op before bilinear upsampling because of the dynamic input Tensor, I thought it has nothing to do with performance degradation, beacause the psnr performance is still low when the input size is solid. 
![image](https://user-images.githubusercontent.com/28351316/180609889-a4c05f4c-717c-48a0-aefb-75683cc719c4.png)

"
56867,Loss stagnates after first episode on random dataset,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

#### TL:DR

Loss stagnated after first episode on real-world dataset. Tried same model on a random dataset. Loss stagnated as well. Seems to be related to kernel size. Why is the network seemingly unable to learn anything past the first episode when the kernel size increases above a certain threshold?

Test code/notebook link below.

[Notebook link](https://colab.research.google.com/drive/1sC7KQJwbTPiQ9ZBWZBCzY3if4bSuRzNq?usp=sharing)

#### Context

I'm trying to solve a real-world problem with a simple 1D CNN implemented in TensorFlow. However, the loss ""converged""/stayed constant after the first episode for some reason.

After tinkering around with the actual dataset, I tried training the model with a randomly created dataset. And, low and behold, the loss stagnated at the exact same value as it would with the actual dataset.

This seems to be related to the kernel size. However, I can't really make sense of why it results in the exact same constant loss despite being fed two completely different datasets.

#### Question

What is the intuition behind this? Why is the network seemingly unable to learn anything past the first episode when the kernel size increases above a certain threshold?

Note that I get the exact same loss for a real-world dataset, so the problem isn't related to the fact that the example dataset is made up and random.

Thank you for taking the time.


### Standalone code to reproduce the issue

[Notebook link](https://colab.research.google.com/drive/1sC7KQJwbTPiQ9ZBWZBCzY3if4bSuRzNq?usp=sharing)

```python
import numpy as np
import tensorflow as tf

def network(seq_len, n_features):
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=128, activation='relu',
                                     input_shape=(seq_len, n_features)))
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))
    return model


def make_dataset(n, seq_len, n_features, classes):
    x = np.random.random((n, seq_len, n_features))
    y = np.random.randint(0, classes, (n,))

    return x, y, seq_len, n_features

# create random dataset with
# - 10000 sequences, each of which with 2400 timesteps and one feature
# - two classes (0 and 1)
x, y, seq_len, n_features = make_dataset(n=10000, seq_len=2400, n_features=1, classes=2)

model = network(seq_len, n_features)
model.compile(optimizer=tf.keras.optimizers.Adam(),
                  loss=tf.keras.losses.BinaryCrossentropy())
model.fit(x, y, epochs=10, batch_size=64)
```


### Relevant log output

```shell
Epoch 1/10
157/157 [==============================] - 28s 179ms/step - loss: 0.7059
Epoch 2/10
157/157 [==============================] - 27s 172ms/step - loss: 0.6932
Epoch 3/10
157/157 [==============================] - 26s 168ms/step - loss: 0.6931
Epoch 4/10
157/157 [==============================] - 29s 182ms/step - loss: 0.6931
Epoch 5/10
157/157 [==============================] - 28s 176ms/step - loss: 0.6931
Epoch 6/10
157/157 [==============================] - 28s 180ms/step - loss: 0.6931
Epoch 7/10
157/157 [==============================] - 27s 173ms/step - loss: 0.6931
Epoch 8/10
157/157 [==============================] - 27s 172ms/step - loss: 0.6931
Epoch 9/10
157/157 [==============================] - 27s 172ms/step - loss: 0.6931
Epoch 10/10
157/157 [==============================] - 27s 173ms/step - loss: 0.6931
```
</details>"
56866,Loss is significantly worse when calculated manually in train_step compared to passed into model.compile,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.8.2

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04.5

### Mobile device

_No response_

### Python version

Python 3.7.13

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I am following the steps here for customizing what happens in `train_step` when you customize a model:

https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit

The issue is, I am getting very different results for the loss when I am using the exact same tensorflow.keras.losses.mean_squared_error function in different locations.

When I use mean_squared_error ""manually"" during train_step (I am calling this the non-compiled mse), the loss is significantly worse than if I  pass it into the compile method, e.g. `model.compile(optimizer=""adam"", loss=mean_squared_error)` (I am calling this the compiled mse).

tensorflow.keras.metrics.MeanSquaredError yields roughly the same result as the compiled version of mse, but it is still not exactly the same.

My expectation is that the loss should be the same whether it is passed into `.compile`, defined during `train_step`, or used as a metric. But it is significantly worse if I do not pass it into `.compile` and I don't know why. I am following the documentation exactly and as far as I can tell, I am not doing anything incorrectly.

Why is the loss so different when it is and is not passed into `.compile`?
```


### Standalone code to reproduce the issue

```shell
I have reproduced this behavior in this google colab notebook, but will also paste the code below. The plot shows that non-compiled mse is systematically worse than compiled mse.  

https://colab.research.google.com/drive/1c1L8KJYQbAldphKvFvmw5cHi9EyBQP0X?usp=sharing


import tensorflow as tf
from tensorflow.keras.metrics import Mean, MeanSquaredError
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import mean_squared_error
import matplotlib.pyplot as plt


seed = 113
epochs = 30
batch_size = 10

n = 5
history_dict = {}

# load dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(
    path=""boston_housing.npz"", test_split=0.2, seed=seed,
)
n_validate = int(0.1 * len(x_train))
train_dataset = tf.data.Dataset.from_tensor_slices(
    (x_train[:-n_validate], y_train[:-n_validate])
).batch(batch_size)
valid_dataset = tf.data.Dataset.from_tensor_slices(
    (x_train[-n_validate:], y_train[-n_validate:])
).batch(batch_size)
test_dataset = tf.data.Dataset.from_tensor_slices(
    (x_test, y_test)
).batch(batch_size)

for i in range(n):
    
    tf.keras.backend.clear_session()
    
    mse_tracker = Mean(name=""non_compiled_mse"")
    mse_tracker_metric = MeanSquaredError(name=""mse_metric"")
    
    class CustomModel(Model):
        
        @tf.function
        def train_step(self, data):
            x, y_true = data
            with tf.GradientTape() as tape:
                y_pred = self(x, training=True)
                compiled_mse = self.compiled_loss(y_true, y_pred)
                non_compiled_mse = mean_squared_error(y_true, y_pred)
                mse_tracker.update_state(non_compiled_mse)
                mse_tracker_metric.update_state(y_true, y_pred)
            gradients = tape.gradient(compiled_mse, self.trainable_variables)
            self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
            return {m.name: m.result() for m in self.metrics}
        
        @tf.function()
        def test_step(self, data):
            x, y_true = data
            y_pred = self(x, training=False)
            compiled_mse = self.compiled_loss(y_true, y_pred)
            non_compiled_mse = mean_squared_error(y_true, y_pred)
            mse_tracker.update_state(non_compiled_mse)
            mse_tracker_metric.update_state(y_true, y_pred)
            return {m.name: m.result() for m in self.metrics}

        @property
        def metrics(self):
            return super().metrics + [mse_tracker, mse_tracker_metric]
    
    input_layer = Input(shape=x_train.shape[1:])
    x = Dense(8, activation='linear')(input_layer)
    x = Dense(8, activation='linear')(x)
    output_layer = Dense(units=1, activation='linear')(x)
    model = CustomModel(input_layer, output_layer)
    model.compile(optimizer=Adam(learning_rate=1e-3), loss=mean_squared_error)
    history = model.fit(
        train_dataset,
        validation_data=valid_dataset,
        epochs=epochs,
        verbose=0,
    )
    history_dict[f""compiled_mse_{i}""] = history.history[""loss""]
    history_dict[f""non_compiled_mse_{i}""] = history.history[""non_compiled_mse""]
    history_dict[f""mse_metric_{i}""] = history.history[""mse_metric""]


plot_start_epoch = 5
plt.figure(figsize=(12, 6))
for i in range(n):
    plt.plot(
        range(epochs)[plot_start_epoch:],
        history_dict[f""compiled_mse_{i}""][plot_start_epoch:],
        label=f""compiled_mse"" if i == 0 else None,
        color=""black"",
    )
    plt.plot(
        range(epochs)[plot_start_epoch:],
        history_dict[f""non_compiled_mse_{i}""][plot_start_epoch:],
        label=f""non_compiled_mse"" if i == 0 else None,
        color=""orange"",
    )
    plt.plot(
        range(epochs)[plot_start_epoch:],
        history_dict[f""mse_metric_{i}""][plot_start_epoch:],
        label=f""mse_metric"" if i == 0 else None,
        color=""red"",
        linestyle=""dashed"",
    )
plt.ylabel(""loss"")
plt.xlabel(""epoch"")
plt.title(""compiled vs non-compiled mse vs mse metric"")
plt.legend()
plt.show()
```


### Relevant log output

_No response_</details>"
56865,WARNING:tensorflow:AutoGraph could not transform... Unable to locate the source code of...,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 12.4
- TensorFlow installation (pip package or built from source): 2.7.0
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.7.0 (shown by `conda list`)

### 2. Code
first jupyter cell:
```
import  tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.preprocessing import MinMaxScaler
NUM_POINTS = 2000
SPLIT = 0.9
EPOCHS = 50
def gen(N):
    theta = np.sqrt(np.random.rand(N))*4*np.pi

    r_a = 2*theta + np.pi
    data_a = np.array([np.cos(theta)*r_a, np.sin(theta)*r_a]).T
    tuples_0 = data_a + np.random.randn(N,2)

    r_b = -2*theta - np.pi
    data_b = np.array([np.cos(theta)*r_b, np.sin(theta)*r_b]).T
    tuples_1 = data_b + np.random.randn(N,2)
    return tuples_0, tuples_1

t0,t1 = gen(int(NUM_POINTS * (1-SPLIT)))
x_train = MinMaxScaler().fit_transform(np.concatenate((t0, t1)))
y_train = np.concatenate((np.repeat(0, int(NUM_POINTS * (1-SPLIT))), np.repeat(1, int(NUM_POINTS * (1-SPLIT))))) * 2 - 1

t0,t1 = gen(int(NUM_POINTS * SPLIT))
x_test = MinMaxScaler().fit_transform(np.concatenate((t0, t1)))
y_test = np.concatenate((np.repeat(0, int(NUM_POINTS * (SPLIT))), np.repeat(0, int(NUM_POINTS * (SPLIT)))))
```

second jupyter cell:
```
scores_lab_unlab = np.exp(-1000 * euclidean_distances(x_test, x_train)**2)
scores_unlab_unlab = np.exp(-1000 * euclidean_distances(x_test, x_test)**2)
vars = [tf.Variable(0.) for _ in x_test]
optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
```

third jupyter cell:
```
%%time
@tf.function
def calc_loss(vars):
    global scores_lab_unlab, y_train, scores_unlab_unlab
    loss_lab_unlab = tf.subtract(tf.expand_dims(vars,1), y_train)**2 * scores_lab_unlab
    loss_unlab_unlab = tf.subtract(tf.expand_dims(vars,1), vars)**2 * scores_unlab_unlab / 2
    return tf.reduce_sum(loss_lab_unlab) + tf.reduce_sum(loss_unlab_unlab)

with tf.device(""/device:CPU:0""):
    for e in range(EPOCHS):
        print(f""Epoch: {e} / {EPOCHS}"", end="""")
        with tf.GradientTape(persistent=True) as tape:
            loss = calc_loss(vars)
        grad = tape.gradient(loss, vars)
        print(f"" - Grad: {np.linalg.norm(grad, 1)}"")
        optimizer.apply_gradients(zip(grad, vars))
```

### 3. (optional) Any other info / logs
 > WARNING:tensorflow:AutoGraph could not transform <function calc_loss at 0x44e3cfc10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Unable to locate the source code of <function calc_loss at 0x44e3cfc10>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function calc_loss at 0x44e3cfc10> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Unable to locate the source code of <function calc_loss at 0x44e3cfc10>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

"
56863,Gradients calculated by reverse mode and forward mode are not equal for API `tf.keras.layers.Softmax`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The jacobian matrix calculated in reverse mode are not equal to that in forward mode using tf.autodiff.ForwardAccumulator.
```


### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

input0 = tf.constant([[[[[[0.56601346]]],
   [[[0.35367298]]],
   [[[0.5609504 ]]]],
  [[[[0.43669665]]],
   [[[0.82903767]]],
   [[[0.57900476]]]]],
 [[[[[0.13786423]]],
   [[[0.17976725]]],
   [[[0.35320067]]]],
  [[[[0.34501767]]],
   [[[0.82709   ]]],
   [[[0.8386754 ]]]]],
 [[[[[0.8424399 ]]],
   [[[0.12519908]]],
   [[[0.41379738]]]],
  [[[[0.88551676]]],
   [[[0.26824057]]],
   [[[0.06636572]]]]]], shape=(3, 2, 3, 1, 1, 1), dtype=tf.float32)
input1 = tf.constant(
[[[[[[0.88215363],
     [0.56112957],
     [0.47048628]],
    [[0.23962319],
     [0.24418604],
     [0.68752027]],
    [[0.8854921 ],
     [0.8750253 ],
     [0.43920374]],
    [[0.6869767 ],
     [0.9971782 ],
     [0.21735716]]],
   [[[0.7472261 ],
     [0.7923174 ],
     [0.99001765]],
    [[0.23535097],
     [0.47414947],
     [0.53421795]],
    [[0.46127486],
     [0.31279147],
     [0.41679263]],
    [[0.40748405],
     [0.8575851 ],
     [0.62180364]]],
   [[[0.09478486],
     [0.8094814 ],
     [0.7278038 ]],
    [[0.36277568],
     [0.14143586],
     [0.6791742 ]],
    [[0.48797262],
     [0.34706163],
     [0.211653  ]],
    [[0.49032676],
     [0.37094796],
     [0.7821864 ]]]]]], shape=(1, 1, 3, 4, 3, 1), dtype=tf.float32)

softmax = tf.keras.layers.Softmax(axis=0)

input = [input0,input1]

with tf.GradientTape() as g:
    g.watch(input1)
    res_backward = softmax(*input)
grad = g.jacobian(res_backward,input1)

grad_fwd_arr = []

for i in range(tf.size(input1)):
    tangents = tf.reshape(tf.one_hot(i,tf.size(input1)),shape=input1.shape)
    with tf.autodiff.ForwardAccumulator(input1, tangents) as acc:
        res_forward = softmax(*input)
        jvp = acc.jvp(res_forward)
        grad_fwd_arr.append(jvp)

grad_fwd = tf.reshape(tf.convert_to_tensor(grad_fwd_arr),shape=grad.shape)

np.testing.assert_allclose(grad,grad_fwd)
```


### Relevant log output

```shell
AssertionError: 
Not equal to tolerance rtol=1e-07, atol=0

Mismatched elements: 215 / 7776 (2.76%)
Max absolute difference: 32.
Max relative difference: 1.
 x: array([[[[[[[[[[[[-29.802322],
                 [  0.      ],
                 [  0.      ]],...
 y: array([[[[[[[[[[[[  0.],
                 [  0.],
                 [  0.]],...
```
</details>"
56862,TF v2.9 Failed to build from source Error: crosstool_wrapper_driver_is_not_gcc failed: error executing command ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

tf 2.9

### Custom Code

No

### OS Platform and Distribution

20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

5.0

### GCC/Compiler version

9.4

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

3090Ti

### Current Behaviour?

```shell
When compiling following the build from source instructions I get the following error
```


### Standalone code to reproduce the issue

```shell
I fun ./configure choose the correct fields. No to ROC, No to Android build & No to CLang. I then goto build with.
bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package

I have Cuda 11.2
```


### Relevant log output

```shell
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/093ed77f7d50f75b376f40a71ea86e08cedb8b80.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
DEBUG: /home/heales/.cache/bazel/_bazel_heales/30fcb60da9c2874c98674721e029dab7/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10:
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/11b2812d64e49bab9b6c489f79067fc94e69db9f.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /home/heales/tensorflow/WORKSPACE:23:14: in <toplevel>
  /home/heales/tensorflow/tensorflow/workspace0.bzl:107:34: in workspace
  /home/heales/.cache/bazel/_bazel_heales/30fcb60da9c2874c98674721e029dab7/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories
Repository rule git_repository defined at:
  /home/heales/.cache/bazel/_bazel_heales/30fcb60da9c2874c98674721e029dab7/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /home/heales/.cache/bazel/_bazel_heales/30fcb60da9c2874c98674721e029dab7/external/llvm-project/mlir/BUILD.bazel:6997:11: Compiling mlir/lib/Dialect/Linalg/IR/LinalgDialect.cpp failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/_objs/LinalgOps/LinalgDialect.d ... (remaining 136 arguments skipped)
x86_64-linux-gnu-gcc-9: fatal error: Killed signal terminated program cc1plus
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1503.995s, Critical Path: 280.70s
INFO: 11667 processes: 6515 internal, 5152 local.
FAILED: Build did NOT complete successfully
```
</details>"
56861,Unit test quantization_ops:quantization_ops_test fails on mkl_aarch64,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

CentOS 7

### Mobile device

n/a

### Python version

3.8.10

### Bazel version

5.1.1

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

```shell
Unit test //tensorflow/python/kernel_tests/quantization_ops:quantization_ops_test fails with segfault introduced by https://github.com/tensorflow/tensorflow/commit/7cdf9d4d2083b739ec81cfdace546b0c99f50622
```


### Standalone code to reproduce the issue

```shell
bazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --config=nonccl --config=mkl_aarch64 --copt=""-mtune=generic"" --copt=""-march=armv8-a"" --copt=""-O3"" --test_env=TF_ENABLE_ONEDNN_OPTS=1 --copt=""-fopenmp"" --linkopt=""-lgomp"" --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --verbose_failures --build_tests_only --jobs=75 -- //tensorflow/python/kernel_tests/quantization_ops:quantization_ops_test
```


### Relevant log output

```shell
==================== Test output for //tensorflow/python/kernel_tests/quantization_ops:quantization_ops_test:
2022-07-22 11:25:14.622796: I tensorflow/core/util/util.cc:175] Experimental oneDNN custom operations are on. If you experience issues, please turn them off by setting the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Running tests under Python 3.8.13: /tmp/workspace/venv-cp38-cp38/bin/python3
[ RUN      ] FakeQuantWithMinMaxVarsOpTest.test_invalid_inputs
INFO:tensorflow:Running test_invalid_inputs in GRAPH mode.
I0722 11:25:15.747941 281472890588256 test_util.py:1490] Running test_invalid_inputs in GRAPH mode.
WARNING:tensorflow:From /opt/python/cp38-cp38/lib/python3.8/contextlib.py:83: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
W0722 11:25:15.748297 281472890588256 deprecation.py:350] From /opt/python/cp38-cp38/lib/python3.8/contextlib.py:83: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
INFO:tensorflow:time(__main__.FakeQuantWithMinMaxVarsOpTest.test_invalid_inputs): 0.08s
I0722 11:25:15.824039 281472890588256 test_util.py:2460] time(__main__.FakeQuantWithMinMaxVarsOpTest.test_invalid_inputs): 0.08s
INFO:tensorflow:Running test_invalid_inputs in EAGER mode.
I0722 11:25:15.824797 281472890588256 test_util.py:1499] Running test_invalid_inputs in EAGER mode.
INFO:tensorflow:time(__main__.FakeQuantWithMinMaxVarsOpTest.test_invalid_inputs): 0.05s
I0722 11:25:15.870929 281472890588256 test_util.py:2460] time(__main__.FakeQuantWithMinMaxVarsOpTest.test_invalid_inputs): 0.05s
[       OK ] FakeQuantWithMinMaxVarsOpTest.test_invalid_inputs
[ RUN      ] FakeQuantWithMinMaxVarsOpTest.test_session
[  SKIPPED ] FakeQuantWithMinMaxVarsOpTest.test_session
[ RUN      ] FakeQuantWithMinMaxVarsPerChannelOpTest.test_invalid_inputs
INFO:tensorflow:Running test_invalid_inputs in GRAPH mode.
I0722 11:25:15.872137 281472890588256 test_util.py:1490] Running test_invalid_inputs in GRAPH mode.
INFO:tensorflow:time(__main__.FakeQuantWithMinMaxVarsPerChannelOpTest.test_invalid_inputs): 0.01s
I0722 11:25:15.883851 281472890588256 test_util.py:2460] time(__main__.FakeQuantWithMinMaxVarsPerChannelOpTest.test_invalid_inputs): 0.01s
INFO:tensorflow:Running test_invalid_inputs in EAGER mode.
I0722 11:25:15.884400 281472890588256 test_util.py:1499] Running test_invalid_inputs in EAGER mode.
INFO:tensorflow:time(__main__.FakeQuantWithMinMaxVarsPerChannelOpTest.test_invalid_inputs): 0.01s
I0722 11:25:15.890146 281472890588256 test_util.py:2460] time(__main__.FakeQuantWithMinMaxVarsPerChannelOpTest.test_invalid_inputs): 0.01s
[       OK ] FakeQuantWithMinMaxVarsPerChannelOpTest.test_invalid_inputs
[ RUN      ] FakeQuantWithMinMaxVarsPerChannelOpTest.test_session
[  SKIPPED ] FakeQuantWithMinMaxVarsPerChannelOpTest.test_session
[ RUN      ] QuantizeDownAndShrinkRangeOpTest.test_invalid_inputs
INFO:tensorflow:Running test_invalid_inputs in GRAPH mode.
I0722 11:25:15.891172 281472890588256 test_util.py:1490] Running test_invalid_inputs in GRAPH mode.
INFO:tensorflow:time(__main__.QuantizeDownAndShrinkRangeOpTest.test_invalid_inputs): 0.0s
I0722 11:25:15.895756 281472890588256 test_util.py:2460] time(__main__.QuantizeDownAndShrinkRangeOpTest.test_invalid_inputs): 0.0s
INFO:tensorflow:Running test_invalid_inputs in EAGER mode.
I0722 11:25:15.896286 281472890588256 test_util.py:1499] Running test_invalid_inputs in EAGER mode.
INFO:tensorflow:time(__main__.QuantizeDownAndShrinkRangeOpTest.test_invalid_inputs): 0.02s
I0722 11:25:15.911795 281472890588256 test_util.py:2460] time(__main__.QuantizeDownAndShrinkRangeOpTest.test_invalid_inputs): 0.02s
[       OK ] QuantizeDownAndShrinkRangeOpTest.test_invalid_inputs
[ RUN      ] QuantizeDownAndShrinkRangeOpTest.test_session
[  SKIPPED ] QuantizeDownAndShrinkRangeOpTest.test_session
[ RUN      ] QuantizedAddOpTest.test_invalid_inputs
INFO:tensorflow:Running test_invalid_inputs in GRAPH mode.
I0722 11:25:15.912965 281472890588256 test_util.py:1490] Running test_invalid_inputs in GRAPH mode.
INFO:tensorflow:time(__main__.QuantizedAddOpTest.test_invalid_inputs): 0.01s
I0722 11:25:15.919473 281472890588256 test_util.py:2460] time(__main__.QuantizedAddOpTest.test_invalid_inputs): 0.01s
INFO:tensorflow:Running test_invalid_inputs in EAGER mode.
I0722 11:25:15.920017 281472890588256 test_util.py:1499] Running test_invalid_inputs in EAGER mode.
INFO:tensorflow:time(__main__.QuantizedAddOpTest.test_invalid_inputs): 0.01s
I0722 11:25:15.933510 281472890588256 test_util.py:2460] time(__main__.QuantizedAddOpTest.test_invalid_inputs): 0.01s
[       OK ] QuantizedAddOpTest.test_invalid_inputs
[ RUN      ] QuantizedAddOpTest.test_session
[  SKIPPED ] QuantizedAddOpTest.test_session
[ RUN      ] QuantizedAvgPoolingOpTest.test_invalid_inputs
INFO:tensorflow:Running test_invalid_inputs in GRAPH mode.
I0722 11:25:15.934696 281472890588256 test_util.py:1490] Running test_invalid_inputs in GRAPH mode.
INFO:tensorflow:time(__main__.QuantizedAvgPoolingOpTest.test_invalid_inputs): 0.01s
I0722 11:25:15.941932 281472890588256 test_util.py:2460] time(__main__.QuantizedAvgPoolingOpTest.test_invalid_inputs): 0.01s
INFO:tensorflow:Running test_invalid_inputs in EAGER mode.
I0722 11:25:15.942471 281472890588256 test_util.py:1499] Running test_invalid_inputs in EAGER mode.
Fatal Python error: Segmentation fault

Current thread 0x0000ffff83a84c60 (most recent call first):
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/eager/execute.py"", line 54 in quick_execute
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_nn_ops.py"", line 6987 in quantized_avg_pool_eager_fallback
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_nn_ops.py"", line 6934 in quantized_avg_pool
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.py"", line 170 in test_invalid_inputs
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1504 in run_eagerly
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1520 in decorated
  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod
  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/case.py"", line 676 in run
  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/case.py"", line 736 in __call__
  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/suite.py"", line 122 in run
  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/suite.py"", line 84 in __call__
  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/suite.py"", line 122 in run
  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/suite.py"", line 84 in __call__
  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/runner.py"", line 176 in run
  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/main.py"", line 271 in runTests
  File ""/opt/python/cp38-cp38/lib/python3.8/unittest/main.py"", line 101 in __init__
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/absl_py/absl/testing/absltest.py"", line 2537 in _run_and_get_tests_result
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/absl_py/absl/testing/absltest.py"", line 2568 in run_tests
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/absl_py/absl/testing/absltest.py"", line 2156 in _run_in_app
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/absl_py/absl/testing/absltest.py"", line 2049 in main
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 51 in g_main
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/absl_py/absl/app.py"", line 258 in _run_main
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/absl_py/absl/app.py"", line 312 in run
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 60 in main_wrapper
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 503 in benchmarks_main
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 62 in main
  File ""/root/.cache/bazel/_bazel_root/7043a081cadd05f91bd91c35f2a2c120/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.py"", line 347 in <module>
================================================================================
Target //tensorflow/python/kernel_tests/quantization_ops:quantization_ops_test up-to-date:
  bazel-bin/tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test
INFO: Elapsed time: 156.845s, Critical Path: 120.79s
INFO: 216 processes: 1 internal, 215 local.
INFO: Build completed, 1 test FAILED, 216 total actions
//tensorflow/python/kernel_tests/quantization_ops:quantization_ops_test  FAILED in 3 out of 3 in 2.7s
```
</details>"
56860,How to define a simple LSTM layer?,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Support

### Source

source

### Tensorflow Version

tf 2.10.0

### Custom Code

No

### OS Platform and Distribution

ubuntu 20.04

### Mobile device

none

### Python version

3.8

### Bazel version

5.1.1

### GCC/Compiler version

9.3.0

### CUDA/cuDNN version

none

### GPU model and memory

none

### Current Behaviour?

```shell
I want to build a single LSTM-layer model, but got unidirectional-LSTM instead.
```


### Standalone code to reproduce the issue

```shell
#define.py
inputs = layers.Input(shape=(5, 4),batch_size=3)
lstm = layers.LSTM(units=3,return_sequences=True,return_state=True,time_major=True)
out,_,_ = lstm(inputs)
print(out.shape)
model = keras.models.Model(inputs = inputs, outputs = out)
model.save('model')

#tflite_convert 
tflite_convert --output_file /mnt/e/lstm.tflite --saved_model_dir ./model/
```


### Relevant log output

_No response_</details>"
56859,Cross compile Tensorflow with host protoc,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Build/Install

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
I'm cross compiling Tensorflow for aarch64 target on my x86-64 host.
Bazel pulls in protobuf and uses the aarch64 compiler to compile protoc and then tries to execute it on my x86-64 host which fails.
```


### Standalone code to reproduce the issue

```shell
How do I tell it to use my host protoc?
This is my current build command.
`bazel build --crosstool_top=//third_party/toolchains/cpus/aarch64:toolchain --cpu=arm //tensorflow/tools/pip_package:build_pip_package`
```


### Relevant log output

_No response_</details>"
